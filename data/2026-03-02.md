<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [CACTUSDB: Unlock Co-Optimization Opportunities for SQL and AI/ML Inferences](https://arxiv.org/abs/2602.23469)
*Lixi Zhou,Kanchan Chowdhury,Lulu Xie,Jaykumar Tandel,Hong Guan,Zhiwei Fan,Xinwei Fu,Jia Zou*

Main category: cs.DB

TL;DR: CactusDB是一个新型数据库系统，支持SQL与AI/ML模型推理的联合查询优化，通过三级中间表示和MCTS优化器实现高达441倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前数据库系统需要支持SQL与AI/ML模型推理的联合查询，以避免数据去规范化、减少数据传输、便于管理和缓解隐私问题。现有系统无法同时支持所有四种协同优化技术，导致性能不理想。

Method: 基于Meta开源的Velox高性能UDF中心化数据库引擎，构建CactusDB系统，采用三级中间表示（关系运算符、表达式运算符、ML函数）支持灵活优化，并提出基于蒙特卡洛树搜索（MCTS）的优化器，结合查询嵌入技术实现跨查询的优化知识共享。

Result: 在MovieLens和TPCx-AI等知名数据集上，对12个代表性推理工作负载和2000个随机生成的推理查询进行评估，CactusDB相比替代系统实现了高达441倍的加速。

Conclusion: CactusDB成功解决了统一多种协同优化技术的挑战，通过创新的三级中间表示和MCTS优化器，显著提升了SQL与AI/ML模型推理联合查询的性能，填补了现有系统的空白。

Abstract: There is a growing demand for supporting inference queries that combine Structured Query Language (SQL) and Artificial Intelligence / Machine Learning (AI/ML) model inferences in database systems, to avoid data denormalization and transfer, facilitate management, and alleviate privacy concerns. Co-optimization techniques for executing inference queries in database systems without accuracy loss fall into four categories: (O1) Relational algebra optimization treating AI/ML models as black-box user-defined functions (UDFs); (O2) Factorized AI/ML inferences; (O3) Tensor-relational transformation; and (O4) General cross-optimization techniques. However, we found none of the existing database systems support all these techniques simultaneously, resulting in suboptimal performance. In this work, we identify two key challenges to address the above problem: (1) the difficulty of unifying all co-optimization techniques that involve disparate data and computation abstractions in one system; and (2) the lack of an optimizer that can effectively explore the exponential search space. To address these challenges, we present CactusDB, a novel system built atop Velox - a high-performance, UDF-centric database engine, open-sourced by Meta. CactusDB features a three-level Intermediate Representations (IR) that supports relational operators, expression operators, and ML functions to enable flexible optimization of arbitrary sub-computations. Additionally, we propose a novel Monte-Carlo Tree Search (MCTS)-based optimizer with query embedding, co-designed with our unique three-level IR, enabling shared and reusable optimization knowledge across different queries. Evaluation of 12 representative inference workloads and 2,000 randomly generated inference queries on well-known datasets, such as MovieLens and TPCx-AI, shows that CactusDB achieves up to 441 times speedup compared to alternative systems.

</details>


### [2] [OceanBase Bacchus: a High-Performance and Scalable Cloud-Native Shared Storage Architecture for Multi-Cloud](https://arxiv.org/abs/2602.23571)
*Quanqing Xu,Mingqiang Zhuang,Chuanhui Yang,Quanwei Wan,Fusheng Han,Fanyu Kong,Hao Liu,Hu Xu,Junyu Ye*

Main category: cs.DB

TL;DR: OceanBase Bacchus：针对云对象存储优化的LSM-tree架构，通过共享日志服务实现存储计算分离，在保持高性能的同时大幅降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 当前共享存储架构在成本与性能之间尚未达到最优平衡。B+-tree架构难以高效处理高并发读写场景中的频繁原地更新，而现有LSM-tree存储分离设计又受限于复杂的跨节点共享日志机制实现。

Method: 采用LSM-tree架构适配云对象存储，通过共享的PALF（基于Paxos的仅追加日志文件系统）日志服务和异步后台服务使计算节点无状态化，使用共享块缓存服务灵活利用缓存资源，将日志同步功能放入共享服务中。

Result: 在SysBench和TPC-H等基准测试中，OceanBase Bacchus在OLTP场景下性能与HBase相当或更优，在OLAP场景下显著优于StarRocks。支持多云部署，在保持高可用性和竞争力的性能的同时，OLTP场景存储成本降低59%，OLAP场景降低89%。

Conclusion: OceanBase Bacchus为存储计算分离数据库提供了新颖的日志共享解决方案，通过模块化功能解耦实现弹性扩展，在云对象存储环境下实现了成本与性能的良好平衡。

Abstract: Although an increasing number of databases now embrace shared-storage architectures, current storage-disaggregated systems have yet to strike an optimal balance between cost and performance. In high-concurrency read/write scenarios, B+-tree-based shared storage struggles to efficiently absorb frequent in-place updates. Existing LSM-tree-backed disaggregated storage designs are hindered by the intricate implementation of cross-node shared-log mechanisms, where no satisfactory solution yet exists.
  This paper presents OceanBase Bacchus, an LSM-tree architecture tailored for object storage provided by cloud vendors. The system sustains high-performance reads and writes while rendering compute nodes stateless through shared service-oriented PALF (Paxos-backed Append-only Log File system) logging and asynchronous background services. We employ a Shared Block Cache Service to flexibly utilize cache resources. Our design places log synchronization into a shared service, providing a novel solution for log sharing in storage-compute-separated databases. The architecture decouples functionality across modules, enabling elastic scaling where compute, cache, and storage resources can be resized rapidly and independently. Through experimental evaluation using multiple benchmark tests, including SysBench and TPC-H, we confirm that OceanBase Bacchus achieves performance comparable to or superior to that of HBase in OLTP scenarios and significantly outperforms StarRocks in OLAP workloads. Leveraging Bacchus's support for multi-cloud deployment and consistent performance, we not only retain high availability and competitive performance but also achieve substantial reductions in storage costs by 59% in OLTP scenarios and 89% in OLAP scenarios.

</details>


### [3] [GPU-Native Approximate Nearest Neighbor Search with IVF-RaBitQ: Fast Index Build and Search](https://arxiv.org/abs/2602.23999)
*Jifan Shi,Jianyang Gao,James Xia,Tamás Béla Fehér,Cheng Long*

Main category: cs.DB

TL;DR: IVF-RaBitQ：一种GPU原生的近似最近邻搜索方案，结合IVF聚类和RaBitQ量化，在GPU上实现快速索引构建、高吞吐量搜索、高召回率和低存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有GPU上的ANNS方法存在权衡：基于图的方法召回率高但构建时间和存储成本高；基于聚类的方法构建效率高但需要多次探测才能达到高召回，消耗内存带宽和计算资源。需要同时实现快速索引构建、高吞吐量搜索、高召回率和低存储需求。

Method: 1. 将基于聚类的方法IVF与RaBitQ量化集成到GPU原生的索引构建/搜索流水线中；2. 开发可扩展的GPU原生RaBitQ量化方法，实现快速准确的低比特编码；3. 开发GPU原生的RaBitQ码距离计算方案和融合搜索内核，实现高吞吐量和高召回率。

Result: 在cuVS Bench多个数据集上的实验显示：IVF-RaBitQ在召回率、吞吐量、索引构建时间和存储占用方面表现优异。在召回率约0.95时，比最先进的基于图的方法CAGRA的QPS高2.2倍，索引构建速度快7.7倍；比基于聚类的方法IVF-PQ吞吐量高2.7倍以上，且无需访问原始向量进行重排序。

Conclusion: IVF-RaBitQ为GPU上的近似最近邻搜索提供了一个强大的性能前沿，成功平衡了索引构建速度、搜索吞吐量、召回率和存储需求，已集成到NVIDIA cuVS库中。

Abstract: Approximate nearest neighbor search (ANNS) on GPUs is gaining increasing popularity for modern retrieval and recommendation workloads that operate over massive high-dimensional vectors. Graph-based indexes deliver high recall and throughput but incur heavy build-time and storage costs. In contrast, cluster-based methods build and scale efficiently yet often need many probes for high recall, straining memory bandwidth and compute. Aiming to simultaneously achieve fast index build, high-throughput search, high recall, and low storage requirement for GPUs, we present IVF-RaBitQ (GPU), a GPU-native ANNS solution that integrates the cluster-based method IVF with RaBitQ quantization into an efficient GPU index build/search pipeline. Specifically, for index build, we develop a scalable GPU-native RaBitQ quantization method that enables fast and accurate low-bit encoding at scale. For search, we develop GPU-native distance computation schemes for RaBitQ codes and a fused search kernel to achieve high throughput with high recall. With IVF-RaBitQ implemented and integrated into the NVIDIA cuVS Library, experiments on cuVS Bench across multiple datasets show that IVF-RaBitQ offers a strong performance frontier in recall, throughput, index build time, and storage footprint. For Recall approximately equal to 0.95, IVF-RaBitQ achieves 2.2x higher QPS than the state-of-the-art graph-based method CAGRA, while also constructing indices 7.7x faster on average. Compared to the cluster-based method IVF-PQ, IVF-RaBitQ delivers on average over 2.7x higher throughput while avoiding accessing the raw vectors for reranking.

</details>


### [4] [NSHEDB: Noise-Sensitive Homomorphic Encrypted Database Query Engine](https://arxiv.org/abs/2602.24271)
*Boram Jung,Yuliang Li,Hung-Wei Tseng*

Main category: cs.DB

TL;DR: NSHEDB是一个基于同态加密的安全查询处理引擎，采用BFV方案的词级分层同态加密，通过纯同态计算支持等式、范围和聚合操作，无需方案转换或可信硬件，实现了比现有系统20-1370倍加速和73倍存储减少。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）虽然理论上强大，但在数据库系统中的实际应用受限，主要因为密文扩展严重、内存开销大以及重置噪声的自举操作计算成本高。

Method: 使用基于BFV方案的词级分层同态加密（LHE）最小化密文扩展并避免昂贵的自举操作；引入纯同态计算技术执行等式、范围和聚合操作，无需在不同HE方案间转换或依赖可信硬件；采用噪声感知查询规划器扩展计算深度同时保持安全保证。

Result: 在真实数据库工作负载（TPC-H）上评估显示，相比最先进的HE系统，NSHEDB实现了20-1370倍加速和73倍存储减少，同时在半诚实模型中保持128位安全性，无需密钥释放或可信组件。

Conclusion: NSHEDB通过系统架构层面的创新，解决了同态加密在数据库系统中的实际应用障碍，为安全隐私保护的数据存储和查询处理提供了高效可行的解决方案。

Abstract: Homomorphic encryption (HE) enables computations directly on encrypted data, offering strong cryptographic guarantees for secure and privacy-preserving data storage and query execution. However, despite its theoretical power, practical adoption of HE in database systems remains limited due to extreme cipher-text expansion, memory overhead, and the computational cost of bootstrapping, which resets noise levels for correctness.
  This paper presents NSHEDB, a secure query processing engine designed to address these challenges at the system architecture level. NSHEDB uses word-level leveled HE (LHE) based on the BFV scheme to minimize ciphertext expansion and avoid costly bootstrapping. It introduces novel techniques for executing equality, range, and aggregation operations using purely homomorphic computation, without transciphering between different HE schemes (e.g., CKKS/BFV/TFHE) or relying on trusted hardware. Additionally, it incorporates a noise-aware query planner to extend computation depth while preserving security guarantees.
  We implement and evaluate NSHEDB on real-world database workloads (TPC-H) and show that it achieves 20x-V1370x speedup and a 73x storage reduction compared to state-of-the-art HE-based systems, while upholding 128-bit security in a semi-honest model with no key release or trusted components.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [QoSFlow: Ensuring Service Quality of Distributed Workflows Using Interpretable Sensitivity Models](https://arxiv.org/abs/2602.23598)
*Md Hasanur Rashid,Jesun Firoz,Nathan R. Tallent,Luanzheng Guo,Meng Tang,Dong Dai*

Main category: cs.DC

TL;DR: QoSFlow是一种性能建模方法，通过将工作流执行配置空间划分为具有相似行为的区域，实现高效的QoS驱动调度，相比最佳标准启发式方法性能提升27.38%。


<details>
  <summary>Details</summary>
Motivation: 随着分布式科学工作流的重要性日益增加，需要确保服务质量约束，但工作流行为的不可预测性使得提供QoS保证变得困难。

Method: QoSFlow将工作流的执行配置空间划分为具有相似行为的区域，每个区域根据给定的统计敏感性将具有可比执行时间的配置分组，通过分析推理而非穷举测试实现高效调度。

Result: 在三个不同工作流上的评估显示，QoSFlow的执行建议比最佳标准启发式方法性能提升27.38%，经验验证确认其推荐配置在不同QoS约束下与实测结果一致。

Conclusion: QoSFlow通过配置空间分区方法有效解决了分布式科学工作流的QoS调度问题，提供了一种可靠的分析推理框架来保证服务质量约束。

Abstract: With the increasing importance of distributed scientific workflows, there is a critical need to ensure Quality of Service (QoS) constraints, such as minimizing time or limiting execution to resource subsets. However, the unpredictable nature of workflow behavior, even with similar configurations, makes it difficult to provide QoS guarantees. For effective reasoning about QoS scheduling, we introduce QoSFlow, a performance modeling method that partitions a workflow's execution configuration space into regions with similar behavior. Each region groups configurations with comparable execution times according to a given statistical sensitivity, enabling efficient QoS-driven scheduling through analytical reasoning rather than exhaustive testing. Evaluation on three diverse workflows shows that QoSFlow's execution recommendations outperform the best-performing standard heuristic by 27.38%. Empirical validation confirms that QoSFlow's recommended configurations consistently match measured execution outcomes across different QoS constraints.

</details>


### [6] [Hestia: Hyperthread-Level Scheduling for Cloud Microservices with Interference-Aware Attention](https://arxiv.org/abs/2602.23758)
*Dingyu Yang,Fanyong Kong,Jie Dai,Shiyou Qian,Shuangwei Li,Jian Cao,Guangtao Xue,Gang Chen*

Main category: cs.DC

TL;DR: Hestia是一个基于自注意力的超线程级干扰感知调度框架，通过建模共享核心和共享插槽两种主要争用模式，显著降低微服务延迟并提升CPU效率。


<details>
  <summary>Details</summary>
Motivation: 现代云服务器通常共置多个延迟敏感的微服务实例以提高资源效率，但微服务行为的多样性加上SMT下的相互性能干扰，使得大规模调度变得复杂。现有方法依赖粗粒度的核心级分析或静态资源分区，未能充分建模超线程级异构性和SMT争用动态。

Method: Hestia包含：(1) 基于自注意力的CPU使用率预测器，建模SC/SS争用和硬件异构性；(2) 干扰评分模型，估计成对争用风险以指导调度决策。框架基于对32,408个实例和3,132台服务器的生产跟踪分析，识别出两种主要争用模式。

Result: Hestia将第95百分位服务延迟降低高达80%，相同工作负载下总体CPU消耗降低2.3%，在多样化争用场景中超越五种最先进调度器达30.65%。

Conclusion: Hestia通过超线程级干扰感知调度，有效解决了微服务共置中的SMT争用问题，显著提升了云服务器的资源效率和微服务性能。

Abstract: Modern cloud servers routinely co-locate multiple latency-sensitive microservice instances to improve resource efficiency. However, the diversity of microservice behaviors, coupled with mutual performance interference under simultaneous multithreading (SMT), makes large-scale placement increasingly complex. Existing interference aware schedulers and isolation techniques rely on coarse core-level profiling or static resource partitioning, leaving asymmetric hyperthread-level heterogeneity and SMT contention dynamics largely unmodeled. We present Hestia, a hyperthread-level, interference-aware scheduling framework powered by self-attention. Through an extensive analysis of production traces encompassing 32,408 instances across 3,132 servers, we identify two dominant contention patterns -- sharing-core (SC) and sharing-socket (SS) -- and reveal strong asymmetry in their impact. Guided by these insights, Hestia incorporates (1) a self-attention-based CPU usage predictor that models SC/SS contention and hardware heterogeneity, and (2) an interference scoring model that estimates pairwise contention risks to guide scheduling decisions. We evaluate Hestia through large-scale simulation and a real production deployment. Hestia reduces the 95th-percentile service latency by up to 80\%, lowers overall CPU consumption by 2.3\% under the same workload, and surpasses five state-of-the-art schedulers by up to 30.65\% across diverse contention scenarios.

</details>


### [7] [Mixed Choice in Asynchronous Multiparty Session Types](https://arxiv.org/abs/2602.23927)
*Laura Bocchi,Raymond Hu,Adriana Laura Voinea,Simon Thompson*

Main category: cs.DC

TL;DR: 提出支持异步混合选择的多方会话类型框架，确保分布式参与者最终达成一致状态，并实现Erlang工具链验证


<details>
  <summary>Details</summary>
Motivation: 现有多方会话类型框架在处理异步混合选择时存在限制，需要支持分布式参与者间的暂时不一致但最终一致的状态管理

Method: 设计核心构造支持异步混合选择，允许协议状态的暂时不一致但确保最终一致性；建立全局类型与分布式本地类型投影的操作对应关系；实现Erlang/OTP工具链用于协议规范和验证

Result: 证明了系统的正确性（进展属性和操作对应性）；实现了实用的工具链；成功应用于RabbitMQ的amqp_client部分重新实现

Conclusion: 提出的异步混合选择多方会话类型框架有效支持分布式协议规范与验证，为Erlang/OTP应用提供了实用的会话类型工具链

Abstract: We present a multiparty session type (MST) framework with asynchronous mixed choice (MC). We propose a core construct for MC that allows transient inconsistencies in protocol state between distributed participants, but ensures all participants can always eventually reach a mutually consistent state. We prove the correctness of our system by establishing a progress property and an operational correspondence between global types and distributed local type projections. Based on our theory, we implement a practical toolchain for specifying and validating asynchronous MST protocols featuring MC, and programming compliant gen_statem processes in Erlang/OTP. We test our framework by using our toolchain to specify and reimplement part of the amqp_client of the RabbitMQ broker for Erlang.

</details>


### [8] [Green or Fast? Learning to Balance Cold Starts and Idle Carbon in Serverless Computing](https://arxiv.org/abs/2602.23935)
*Bowen Sun,Christos D. Antonopoulos,Evgenia Smirni,Bin Ren,Nikolaos Bellas,Spyros Lalis*

Main category: cs.DC

TL;DR: LACE-RL：基于深度强化学习的服务器无服务器计算管理框架，动态调整函数实例保活时间，在降低冷启动延迟和减少碳排放之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 服务器无服务器计算简化了云部署，但带来了服务延迟和碳排放管理的新挑战。减少冷启动延迟需要保留热函数实例，而最小化碳排放则倾向于回收空闲资源。这种平衡在时变的电网碳强度和不同工作负载模式下变得更加复杂，静态保活策略效率低下。

Method: 提出LACE-RL框架，将服务器无服务器Pod保留问题建模为序列决策问题。使用深度强化学习动态调整保活持续时间，联合建模冷启动概率、函数特定延迟成本和实时碳强度。

Result: 基于华为公共云追踪数据，LACE-RL相比华为静态策略减少51.69%的冷启动和77.08%的空闲保活碳排放，在延迟-碳排放权衡方面优于最先进的启发式和单目标基线方法，接近Oracle性能。

Conclusion: LACE-RL框架通过深度强化学习有效解决了服务器无服务器计算中延迟和碳排放的权衡问题，实现了动态、智能的资源管理策略。

Abstract: Serverless computing simplifies cloud deployment but introduces new challenges in managing service latency and carbon emissions. Reducing cold-start latency requires retaining warm function instances, while minimizing carbon emissions favors reclaiming idle resources. This balance is further complicated by time-varying grid carbon intensity and varying workload patterns, under which static keep-alive policies are inefficient. We present LACE-RL, a latency-aware and carbon-efficient management framework that formulates serverless pod retention as a sequential decision problem. LACE-RL uses deep reinforcement learning to dynamically tune keep-alive durations, jointly modeling cold-start probability, function-specific latency costs, and real-time carbon intensity. Using the Huawei Public Cloud Trace, we show that LACE-RL reduces cold starts by 51.69% and idle keep-alive carbon emissions by 77.08% compared to Huawei's static policy, while achieving better latency-carbon trade-offs than state-of-the-art heuristic and single-objective baselines, approaching Oracle performance.

</details>


### [9] [Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving](https://arxiv.org/abs/2602.24044)
*Ferran Agullo,Joan Oliveras,Chen Wang,Alberto Gutierrez-Torre,Olivier Tardieu,Alaa Youssef,Jordi Torres,Josep Ll. Berral*

Main category: cs.DC

TL;DR: 提出一个数据驱动的流水线，通过精准性能预测和贪心放置算法，在分布式LLM适配器服务中最大化GPU效率，减少所需GPU数量


<details>
  <summary>Details</summary>
Motivation: LLM适配器虽然能低成本实现模型专业化，但在分布式服务系统中引入复杂的缓存和调度挑战。现有研究主要关注延迟最小化，而通过吞吐量最大化实现资源效率的研究不足

Method: 提出三组件流水线：1) 针对LLM适配器服务的数字孪生(DT)；2) 基于DT生成数据训练的蒸馏机器学习模型；3) 利用ML性能估计的贪心放置算法。DT高保真模拟真实系统动态，ML模型加速性能估计

Result: DT实现低于5%的吞吐量估计误差，执行速度比完整LLM基准测试快90倍。实验显示该流水线显著提高GPU效率，减少维持目标工作负载所需的GPU数量

Conclusion: 该流水线能有效优化LLM适配器服务的GPU效率，并可适应其他目标如延迟最小化，展示了未来大规模LLM服务基础设施的通用性

Abstract: Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.

</details>


### [10] [nvidia-pcm: A D-Bus-Driven Platform Configuration Manager for OpenBMC Environments](https://arxiv.org/abs/2602.24237)
*Harinder Singh*

Main category: cs.DC

TL;DR: nvidia-pcm是一个平台配置管理器，通过查询硬件身份数据并导出环境变量，使单个固件镜像能够服务多个GPU服务器平台变体，消除了为微小硬件差异维护单独固件镜像的开销。


<details>
  <summary>Details</summary>
Motivation: GPU加速服务器平台硬件架构相似但存在微小差异（如组件标识符、热配置文件、互连拓扑），这导致需要维护多个单独的固件镜像，增加了开发和维护开销。

Method: nvidia-pcm在启动时通过D-Bus查询硬件身份数据，将正确的平台特定配置导出为环境变量。下游服务读取这些变量而无需关心具体硬件变体。平台差异完全通过声明式JSON文件捕获，而非单独的构建工件。

Result: 实现了单个固件镜像服务多个平台变体的能力，平台差异完全通过JSON文件管理，简化了固件分发和维护，提高了开发效率。

Conclusion: 通过以最小抽象级别解决平台身份问题，优先考虑采用简单性而非全面的硬件建模，nvidia-pcm成功简化了多平台固件管理，为类似硬件变体管理问题提供了实用解决方案。

Abstract: GPU-accelerated server platforms that share most of their hardware architecture often require separate firmware images due to minor hardware differences--different component identifiers, thermal profiles, or interconnect topologies. I built nvidia-pcm to eliminate that overhead. nvidia-pcm is a platform configuration manager for NVBMC, NVIDIA's OpenBMC-based firmware distribution, that enables a single firmware image to serve multiple platform variants. At boot, nvidia-pcm queries hardware identity data over D-Bus and exports the correct platform-specific configuration as environment variables. Downstream services read those variables without knowing or caring which hardware variant they are running on. The result is that platform differences are captured entirely in declarative JSON files, not in separate build artifacts. This paper describes the architecture, implementation, and deployment impact of nvidia-pcm, and shares lessons learned from solving the platform-identity problem at a deliberately minimal level of abstraction--prioritizing adoption simplicity over comprehensive hardware modeling.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [SGAgent: Suggestion-Guided LLM-Based Multi-Agent Framework for Repository-Level Software Repair](https://arxiv.org/abs/2602.23647)
*Quanjun Zhang,Chengyu Gao,Yu Han,Ye Shang,Chunrong Fang,Zhenyu Chen,Liang Xiao*

Main category: cs.SE

TL;DR: SGAgent是一个基于建议引导的多智能体软件修复框架，采用定位-建议-修复范式，通过知识图谱增强全局上下文理解，在SWE-Bench上达到51.3%的修复准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体的软件修复方法通常采用定位-修复范式，直接从"bug在哪里"跳到"如何修复"，存在根本性的推理鸿沟。需要加强从定位到修复的过渡过程。

Method: 提出SGAgent框架，采用定位-建议-修复三阶段范式：1) 定位器识别bug位置；2) 建议器从bug位置逐步检索相关上下文，理解bug本质后提供可操作的修复建议；3) 修复器基于建议生成补丁。同时构建知识图谱增强全局上下文感知。

Result: 在SWE-Bench上，SGAgent使用Claude-3.5达到51.3%的修复准确率，81.2%的文件级和52.4%的函数级定位准确率，平均每个实例成本1.48美元。在VUL4J和VJBench漏洞修复任务上达到48%准确率，展示了跨任务和编程语言的强泛化能力。

Conclusion: SGAgent通过引入建议阶段和知识图谱增强，有效弥合了定位与修复之间的推理鸿沟，在软件修复任务上取得了最先进的性能，并展示了良好的泛化能力。

Abstract: The rapid advancement of Large Language Models (LLMs) has led to the emergence of intelligent agents capable of autonomously interacting with environments and invoking external tools. Recently, agent-based software repair approaches have received widespread attention, as repair agents can automatically analyze and localize bugs, generate patches, and achieve state-of-the-art performance on repository-level benchmarks. However, existing approaches usually adopt a localize-then-fix paradigm, jumping directly from "where the bug is" to "how to fix it", leaving a fundamental reasoning gap. To this end, we propose SGAgent, a Suggestion-Guided multi-Agent framework for repository-level software repair, which follows a localize-suggest-fix paradigm. SGAgent introduces a suggestion phase to strengthen the transition from localization to repair. The suggester starts from the buggy locations and incrementally retrieves relevant context until it fully understands the bug, and then provides actionable repair suggestions. Moreover, we construct a Knowledge Graph from the target repository and develop a KG-based toolkit to enhance SGAgent's global contextual awareness and repository-level reasoning. Three specialized sub-agents (i.e., localizer, suggester, and fixer) collaborate to achieve automated end-to-end software repair. Experimental results on SWE-Bench show that SGAgent with Claude-3.5 achieves 51.3% repair accuracy, 81.2% file-level and 52.4% function-level localization accuracy with an average cost of $1.48 per instance, outperforming all baselines using the same base model. Furthermore, SGAgent attains 48% accuracy on VUL4J and VJBench for vulnerability repair, demonstrating strong generalization across tasks and programming languages.

</details>


### [12] [Peeling Off the Cocoon: Unveiling Suppressed Golden Seeds for Mutational Greybox Fuzzing](https://arxiv.org/abs/2602.23736)
*Ruixiang Qian,Chunrong Fang,Zengxu Chen,Youxin Fu,Zhenyu Chen*

Main category: cs.SE

TL;DR: PoCo通过逐步移除障碍条件语句并进行更深入的种子选择来增强基于覆盖率的种子选择技术


<details>
  <summary>Details</summary>
Motivation: 现有的基于覆盖率的种子选择技术（如afl-cmin）在处理复杂条件语句时存在局限性，无法充分探索深层代码路径

Method: 采用渐进式方法：1) 识别并移除障碍条件语句；2) 进行更深入的种子选择；3) 逐步优化种子集合

Result: PoCo能够更有效地选择种子，提高代码覆盖率，发现更多深层漏洞

Conclusion: PoCo技术显著改进了传统CSS方法，特别是在处理复杂条件分支时表现出色

Abstract: PoCo is a technique that aims to enhance modern coverage-based seed selection (CSS) techniques (such as afl-cmin) by gradually removing obstacle conditional statements and conducting deeper seed selection.

</details>


### [13] [SWE-rebench V2: Language-Agnostic SWE Task Collection at Scale](https://arxiv.org/abs/2602.23866)
*Ibragim Badertdinov,Maksim Nekrashevich,Anton Shevtsov,Alexander Golubev*

Main category: cs.SE

TL;DR: SWE-rebench V2：一个语言无关的自动化流程，用于大规模收集可执行的真实世界软件工程任务并构建RL训练环境，包含32,000+个任务和120,000+个扩展任务。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程智能体的强化学习训练受限于大规模任务集的稀缺性，现有数据集规模有限、多样性不足，且主要针对高资源语言生态系统。

Method: 开发了一个语言无关的自动化流程，通过交互式设置代理合成仓库特定的安装和测试程序，使用LLM评委集合过滤无效实例，并基于人类验证的SWE-bench标注进行验证。

Result: 构建了包含32,000+个任务的数据集，涵盖20种语言和3,600+个仓库，并提供预构建镜像用于可重复执行；额外发布了120,000+个带有安装说明、测试和元数据的任务。

Conclusion: 该工作提供了大规模、多样化的软件工程任务数据集和工具，能够支持跨多种语言和仓库的软件工程智能体的大规模训练。

Abstract: Software engineering agents (SWE) are improving rapidly, with recent gains largely driven by reinforcement learning (RL). However, RL training is constrained by the scarcity of large-scale task collections with reproducible execution environments and reliable test suites. Although a growing number of benchmarks have emerged, datasets suitable for training remain limited in scale and diversity or often target a limited set of high-resource language ecosystems. We introduce SWE-rebench V2, a language-agnostic automated pipeline for harvesting executable real-world SWE tasks and constructing RL training environments at scale. The pipeline synthesizes repository-specific installation and test procedures via an interactive setup agent, and filters unsound instances using an ensemble of LLM judges, validated against human-verified SWE-bench annotations. Using this pipeline, we construct a dataset of 32,000+ tasks spanning 20 languages and 3,600+ repositories, with pre-built images for reproducible execution. To further scale training data, we additionally release 120,000+ tasks with installation instructions, fail-to-pass tests and rich metadata, where the problem statement is generated based on the original pull request description. We validate the collected instances through a diagnostic study that covers a subset of tasks in five programming languages across seven popular models, and provide instance-level metadata that flags common confounders such as overly restrictive tests and underspecified descriptions. We release the datasets, the collection and execution code, and associated artifacts to enable large-scale training of SWE agents across diverse languages and repositories.

</details>


### [14] [Novice Developers Produce Larger Review Overhead for Project Maintainers while Vibe Coding](https://arxiv.org/abs/2602.23905)
*Syed Ammar Asdaque,Imran Haider,Muhammad Umar Malik,Maryam Abdul Ghafoor,Abdul Ali Bangash*

Main category: cs.SE

TL;DR: 研究AI辅助开发中开发者经验是否仍重要，发现低经验开发者提交更大代码量但接受率更低、审查负担更重，经验开发者无法被低经验AI开发者安全替代


<details>
  <summary>Details</summary>
Motivation: 探究在AI编码代理快速生成代码的时代，项目管理者能否用经验较少的AI开发者替代经验丰富的开发者，验证开发者经验在AI辅助开发中是否仍然重要

Method: 分析AIDev数据集中22,953个PR，将1,719名AI开发者分为低经验和高经验两组，比较贡献量、PR接受率、审查评论数、解决时间等指标

Result: 低经验开发者提交PR包含2.15倍更多提交和1.47倍更多文件更改，收到4.52倍更多审查评论，接受率低31%，解决时间长5.16倍

Conclusion: 低经验AI开发者专注于生成更多代码但将验证负担转移给审查者，无法安全替代经验开发者，需结合新手培训和自适应PR审查周期

Abstract: AI coding agents allow software developers to generate code quickly, which raises a practical question for project managers and open source maintainers: can vibe coders with less development experience substitute for expert developers? To explore whether developer experience still matters in AI-assisted development, we study $22,953$ Pull Requests (PRs) from $1,719$ vibe coders in the GitHub repositories of the AIDev dataset. We split vibe coders into lower experience vibe coders ($\mathit{Exp}_{Low}$) and higher experience vibe coders ($\mathit{Exp}_{High}$) and compare contribution magnitude and PR acceptance rates across PR categories. We find that $\mathit{Exp}_{Low}$ submits PRs with larger volume ($2.15\times$ more commits and $1.47\times$ more files changed) than $\mathit{Exp}_{High}$. Moreover, $\mathit{Exp}_{Low}$ PRs, when compared to $\mathit{Exp}_{High}$, receive $4.52\times$ more review comments, and have $31\%$ lower acceptance rates, and remain open $5.16\times$ longer before resolution. Our results indicate that low-experienced vibe coders focus on generating more code while shifting verification burden onto reviewers. For practice, project managers may not be able to safely replace experienced developers with low-experience vibe coders without increasing review capacity. Development teams should therefore combine targeted training for novices with adaptive PR review cycles.

</details>


### [15] [Invariant-Driven Automated Testing](https://arxiv.org/abs/2602.23922)
*Ana Catarina Ribeiro*

Main category: cs.SE

TL;DR: APOSTL是一种基于一阶逻辑的API规范语言，用于扩展现有API描述语言以支持测试目的，结合PETIT工具实现微服务测试自动化


<details>
  <summary>Details</summary>
Motivation: 当前微服务架构迁移缺乏有效的自动化测试流程，现有API规范语言表达能力不足以支持测试需求，需要扩展API规范语言来创建有用的测试工件

Method: 提出APOSTL规范语言，基于一阶逻辑扩展现有API描述语言，添加可用于测试的属性；开发PETIT工具，通过分析OpenAPI规范文档（用APOSTL注解）来自动化测试微服务

Result: APOSTL能够为API规范添加语义信息，PETIT工具能够在不依赖源代码的情况下分析微服务，实现微服务测试过程的完全自动化

Conclusion: 通过APOSTL扩展API规范语言并结合PETIT工具，可以解决微服务测试自动化问题，为行业提供有效的微服务测试解决方案

Abstract: Microservice architectures are an emergent technology that builds business logic into a suite of small services. Each microservice runs in its process and the communication is made through lightweight mechanisms, usually HTTP resource API. These architectures are built upon independently deployable and, supposedly, reliable pieces of software that may, or may not, have been developed by the team using it. Nowadays, industries are dangerously migrating into microservice architectures without an effective and automatic process for testing the software being used. Furthermore, current API specification languages are not expressive enough to be used for testing purposes. To solve this problem it is necessary to extend currently broadly used API specification languages. APOSTL is a specification language to annotate APIs specifications based on first-order logic, with some restrictions. It has the purpose of extending the currently used API description languages with properties that can be useful for testing purposes, transforming these description documents into useful testing artefacts. Besides providing information needed for testing an application, APOSTL also provides an API with semantic. This additional information is then leveraged to automate microservice testing. The work developed in this thesis aims to fully automate the microservice testing process. It is achieved by the implementation of PETIT a tool able to test microservices when provided with an OpenAPI Specification document, written in JSON and properly annotated with the previously proposed specification language, APOSTL. The tool is able to analyze microservices independently from the source code availability.

</details>


### [16] [The Vocabulary of Flaky Tests in the Context of SAP HANA](https://arxiv.org/abs/2602.23957)
*Alexander Berndt,Zoltán Nochta,Thomas Bach*

Main category: cs.SE

TL;DR: 该研究在大型工业项目SAP HANA中评估了基于测试代码中源代码标识符识别不稳定测试的方法，复制了先前研究并扩展了特征提取和分类技术，取得了更高的F1分数，但发现这些方法在实际中对开发者的可操作性有限。


<details>
  <summary>Details</summary>
Motivation: 不稳定测试会随机失败，影响软件质量评估。先前研究提出基于测试代码中的源代码标识符识别不稳定测试，但这些方法尚未在大型工业环境中进行大规模评估。本研究旨在在SAP HANA这一大型工业项目中评估这些方法及其识别不稳定测试根本原因的能力。

Method: 首先在SAP HANA环境中复制Pinto等人的先前研究；其次评估不同的特征提取技术（TF-IDF和TF-IDFC-RF）；第三评估CodeBERT和XGBoost作为分类模型；使用先前研究的数据集和两个SAP HANA数据集进行对比分析。

Result: 复制研究在原始数据集和一个SAP HANA数据集上取得了相似结果（原始方法F1分数分别为0.94和0.92）。扩展方法取得了更好的性能（F1分数分别为0.96和0.99）。发现外部数据源依赖是SAP HANA中测试不稳定的常见根本原因。

Conclusion: 大型工业项目的词汇在具体术语上略有不同，但术语类别（如远程依赖）与先前实证发现相似。尽管获得了较高的F1分数，但基于源代码标识符识别不稳定测试和黑盒预测在实际中应用有限，因为结果对开发者不够可操作。

Abstract: Background. Automated test execution is an important activity to gather information about the quality of a software project. So-called flaky tests, however, negatively affect this process. Such tests fail seemingly at random without changes to the code and thus do not provide a clear signal. Previous work proposed to identify flaky tests based on the source code identifiers in the test code. So far, these approaches have not been evaluated in a large-scale industrial setting. Aims. We evaluate approaches to identify flaky tests and their root causes based on source code identifiers in the test code in a large-scale industrial project. Method. First, we replicate previous work by Pinto et al. in the context of SAP HANA. Second, we assess different feature extraction techniques, namely TF-IDF and TF-IDFC-RF. Third, we evaluate CodeBERT and XGBoost as classification models. For a sound comparison, we utilize both the data set from previous work and two data sets from SAP HANA. Results. Our replication shows similar results on the original data set and on one of the SAP HANA data sets. While the original approach yielded an F1-Score of 0.94 on the original data set and 0.92 on the SAP HANA data set, our extensions achieve F1-Scores of 0.96 and 0.99, respectively. The reliance on external data sources is a common root cause for test flakiness in the context of SAP HANA. Conclusions. The vocabulary of a large industrial project seems to be slightly different with respect to the exact terms, but the categories for the terms, such as remote dependencies, are similar to previous empirical findings. However, even with rather large F1-Scores, both finding source code identifiers for flakiness and a black box prediction have limited use in practice as the results are not actionable for developers.

</details>


### [17] [Context-Aware Functional Test Generation via Business Logic Extraction and Adaptation](https://arxiv.org/abs/2602.24108)
*Yakun Zhang,Zihan Wang,Xinzhi Peng,Zihao Xie,Xiaodong Wang,Xutao Li,Dan Hao,Lu Zhang,Yunming Ye*

Main category: cs.SE

TL;DR: LogiDroid：一种两阶段方法，通过提取业务逻辑并将其适配到目标应用来生成移动应用功能测试用例，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动应用功能测试对验证业务逻辑符合用户需求至关重要，但目前仍严重依赖人工。主要面临两大挑战：1) 从非结构化需求中获取和复用复杂业务逻辑困难；2) 将业务逻辑适配到多样化GUI环境时存在语义鸿沟。

Method: 提出LogiDroid两阶段方法：1) 知识检索与融合阶段：构建数据集检索相关案例并提取目标功能的业务逻辑；2) 上下文感知测试生成阶段：联合分析提取的业务逻辑和实时GUI环境生成功能测试用例，包含验证断言。

Result: 在覆盖28个真实应用和190个功能需求的两个数据集上评估，LogiDroid在FrUITeR数据集上成功测试了40%的功能需求（比SOTA提升48%以上），在Lin数据集上成功测试了65%（比SOTA提升55%以上）。

Conclusion: LogiDroid通过准确理解应用语义并利用领域专业知识生成完整测试用例，在功能测试生成方面表现出显著有效性，解决了业务逻辑提取和GUI适配的挑战。

Abstract: Functional testing is essential for verifying that the business logic of mobile applications aligns with user requirements, serving as the primary methodology for quality assurance in software development. Despite its importance, functional testing remains heavily dependent on manual effort due to two core challenges. First, acquiring and reusing complex business logic from unstructured requirements remains difficult, which hinders the understanding of specific functionalities. Second, a significant semantic gap exists when adapting business logic to the diverse GUI environments, which hinders the generation of test cases for specific mobile applications. To address the preceding challenges, we propose LogiDroid, a two-stage approach that generates individual functional test cases by extracting business logic and adapting it to target applications. First, in the Knowledge Retrieval and Fusion stage, we construct a dataset to retrieve relevant cases and extract business logic for the target functionality. Second, in the Context-Aware Test Generation stage, LogiDroid jointly analyzes the extracted business logic and the real-time GUI environment to generate functional test cases. This design allows LogiDroid to accurately understand application semantics and use domain expertise to generate complete test cases with verification assertions. We assess the effectiveness of LogiDroid using two widely-used datasets that cover 28 real-world applications and 190 functional requirements. Experimental results show that LogiDroid successfully tested 40% of functional requirements on the FrUITeR dataset (an improvement of over 48% compared to the state-of-the-art approaches) and 65% on the Lin dataset (an improvement of over 55% compared to the state-of-the-art approaches). These results demonstrate the significant effectiveness of LogiDroid in functional test generation.

</details>
