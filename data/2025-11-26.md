<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 19]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Opt4GPTQ: Co-Optimizing Memory and Computation for 4-bit GPTQ Quantized LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2511.19438)
*Yaozheng Zhang,Wei Wang,Jie Kong,Jiehan Zhou,Huanqing Cui*

Main category: cs.DC

TL;DR: Opt4GPTQ是一种针对4位GPTQ量化大语言模型在异构AI加速器上推理的优化方法，通过共享内存缓冲、向量化内存加载和内联汇编优化，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在异构计算平台上推理效率低下的问题，特别是在不同异构AI加速器上的性能挑战。

Method: 基于vLLM服务系统，集成三种平台级优化策略：共享内存缓冲优化、向量化内存加载优化和内联汇编优化，直接利用硬件原生指令。

Result: 有效提升不同模型的推理性能，最高实现84.42%的吞吐量提升和51.35%的延迟降低。

Conclusion: 平台级工程优化对于在异构AI加速架构上实现高效LLM推理至关重要，为未来异构平台适配提供了有价值的部署经验和方法论。

Abstract: The increasing adoption of large language model (LLMs) on heterogeneous computing platforms poses significant challenges for achieving high inference efficiency. To address the low inference efficiency of LLMs across diverse heterogeneous platforms, this paper proposes a practical optimization method, Opt4GPTQ, designed for 4-bit GPTQ quantized LLMs inference on heterogeneous AI accelerators. Built upon the vLLM serving system, Opt4GPTQ integrates three platform-level optimization strategies: Shared Memory Buffering optimization (SMB-Opt), which caches data in shared memory and employs single-threaded writes; Vectorized Memory Loading optimization (VML-Opt), which utilizes vectorized memory operations for efficient data loading; and Inline Assembly optimization (ILAOpt), which directly leverages hardware-native vector halfprecision addition and fused multiply-accumulate instructions for efficient execution. Experimental results show that Opt4GPTQ effectively improves inference performance across different models, achieving up to 84.42% throughput improvement and up to 51.35% latency reduction. This work highlights the critical role of platform-level engineering optimizations in enabling efficient LLMs inference on emerging heterogeneous AI acceleration architectures and provides valuable deployment experience and methodologies for future heterogeneous platform adaptation.

</details>


### [2] [Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution](https://arxiv.org/abs/2511.19445)
*Luca Accorsi,Demetrio Laganà,Federico Michelotto,Roberto Musmanno,Daniele Vigo*

Main category: cs.DC

TL;DR: 提出了FILO2^x并行共享内存方案，用于协同优化容量约束车辆路径问题，无需显式分解且同步开销最小。这是FILO2算法的单轨迹并行适配版本，通过并发异步优化多个可能不相关的解区域来提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 为了充分利用可用计算资源，在保持最终解质量的同时显著减少求解时间，特别是对于从数百到数十万客户的大规模实例。

Method: 基于FILO2算法的局部性特性，设计并行共享内存架构，多个求解器同时异步优化同一底层解的不同区域，形成基于迭代的并行性。

Result: 计算结果表明，FILO2^x相比原始方法能大幅提升求解时间，同时保持相似的最终解质量，适用于数百到数十万客户规模的实例。

Conclusion: FILO2^x通过更好地利用计算资源，在保持解质量的同时显著提升了容量约束车辆路径问题的求解效率，证明了并行化策略的有效性。

Abstract: We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.

</details>


### [3] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: PSAP是一个动态智能分片分配协议，通过预测性工作负载分配解决区块链分片中的负载不均衡问题，相比现有方法提升2倍吞吐量、降低35%延迟和20%跨分片开销。


<details>
  <summary>Details</summary>
Motivation: 静态或启发式分片分配导致工作负载倾斜、拥塞和过多跨分片通信，削弱了分片技术的可扩展性优势。

Method: 整合时序工作负载预测模型和安全约束强化学习控制器，实现多区块前瞻预测和自适应分片重配置，通过同步量化运行时和安全门机制确保确定性推理。

Result: 在以太坊、NEAR和Hyperledger Fabric等异构数据集上实验，相比现有动态分片基线，吞吐量提升2倍，延迟降低35%，跨分片开销减少20%。

Conclusion: 预测性、确定性和安全感知的分片分配是下一代可扩展区块链系统的有前景方向。

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [4] [AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles](https://arxiv.org/abs/2511.19453)
*Yuxin Wang,Yuankai He,Weisong Shi*

Main category: cs.DC

TL;DR: AVS是一个自主车辆存储系统，通过分层设计实现高效的数据存储和检索，包括模态感知的压缩、热冷数据分层和轻量级元数据索引，在嵌入式硬件上验证了实时数据摄入和快速选择性检索能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆生成海量异构数据（如每天14TB），但现有数据记录器和存储系统无法提供高效的数据存储和检索功能，需要支持第三方应用的可查询车载存储系统。

Method: 设计了AVS系统，采用分层架构：模态感知的降维和压缩、热冷数据分层与每日归档、轻量级元数据层索引。在嵌入式硬件上使用真实L4自动驾驶轨迹进行验证。

Result: 原型系统在适度资源预算下实现了可预测的实时数据摄入、快速选择性检索和显著的空间占用减少。

Conclusion: 该工作为将存储作为AV堆栈中的一等组件提供了基础，并提出了向更可扩展和长期部署发展的观察和下一步方向。

Abstract: Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.

</details>


### [5] [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)
*Anton Reinhard,Simeon Ehrig,René Widera,Michael Bussmann,Uwe Hernandez Acosta*

Main category: cs.DC

TL;DR: 提出一个Julia软件框架，能自动动态生成静态调度和编译代码，用于优化复杂科学计算问题的硬件调度


<details>
  <summary>Details</summary>
Motivation: 科学计算中的复杂问题通常包含具有不同计算需求的子任务，需要根据每个子任务的特点在最适合的硬件上调度，同时考虑并行性、任务依赖性和数据传输速度

Method: 使用有向无环图表示计算问题，结合领域特定信息扩展DAG调度理论，实现自动动态生成静态调度和编译代码的框架

Result: 开发了Julia软件框架，并在量子电动力学中多粒子散射过程的矩阵元计算中进行了应用验证

Conclusion: 通过理论扩展和领域特定信息的结合，实现了传统DAG调度无法做到的优化，为复杂科学计算问题提供了高效的硬件调度解决方案

Abstract: Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.

</details>


### [6] [SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference](https://arxiv.org/abs/2511.19457)
*Ziyang Zhang,Jie Liu,Luca Mottola*

Main category: cs.DC

TL;DR: SparOA是一个CPU-GPU混合推理框架，通过利用稀疏性和计算强度来优化算子调度，在边缘设备上实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络模型在资源受限的边缘设备上部署时面临性能挑战，现有解决方案如模型压缩会牺牲精度，而专用硬件成本高且不灵活。

Method: 包含三个关键组件：阈值预测器确定最优稀疏性和计算强度阈值；基于强化学习的调度器动态优化资源分配；混合推理引擎通过异步执行和批量大小优化提高效率。

Result: 相比所有基线方法平均加速1.22-1.31倍，比CPU-Only最高加速50.7倍，能耗比最先进的协同执行基线降低7%-16%。

Conclusion: SparOA通过有效利用稀疏性和计算强度，在边缘设备上实现了高效的CPU-GPU混合推理，显著提升了性能和能效。

Abstract: The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size optimization.Extensive results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\%-16\% less energy than the SOTA co-execution baseline.

</details>


### [7] [Systemic approach for modeling a generic smart grid](https://arxiv.org/abs/2511.19460)
*Sofiane Ben Amor,Guillaume Guerard,Loup-Noé Levy*

Main category: cs.DC

TL;DR: 提出了一种智能电网骨干模型，用于测试电网的替代场景，通过分布式优化子系统实现生产和消费调度，同时保持灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 智能电网技术进步带来了复杂的跨学科建模问题，传统计算方法难以解决，需要系统性的集成建模方法来模拟电力系统、能源市场、需求侧管理等资源。

Method: 开发智能电网骨干模型，采用分布式优化子系统的方法，模拟不同系统以验证假设，实现生产和消费调度。

Result: 该工具能够模拟不同的系统，在人类规模模型之前验证假设，通过分布式优化实现灵活和可扩展的生产消费调度。

Conclusion: 提出的智能电网骨干模型为解决复杂智能电网模拟问题提供了有效工具，通过分布式优化方法实现了系统的灵活性和可扩展性。

Abstract: Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.

</details>


### [8] [Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna](https://arxiv.org/abs/2511.19463)
*Aldo Canfora,Eleonora Bergamaschi,Riccardo Mioli,Federico Battini,Mirko Degli Esposti,Giorgio Pedrazzi,Chiara Dellacasa*

Main category: cs.DC

TL;DR: 开发了一个集成EnergyPlus模拟、高性能计算和开放地理数据集的UBEM管道，用于估算意大利博洛尼亚的建筑能源需求，在30分钟内模拟了约25,000栋建筑。


<details>
  <summary>Details</summary>
Motivation: 城市建筑能源建模(UBEM)在理解和预测城市尺度能源消耗中发挥核心作用，需要高效准确的建模方法来支持城市能源规划。

Method: 结合EnergyPlus模拟、高性能计算和开放地理数据集，从博洛尼亚开放数据门户获取几何信息，从区域建筑法规和TABULA数据库获取非几何属性，在Leonardo超级计算机上执行计算。

Result: 成功构建了博洛尼亚建筑能源需求估算模型，在30分钟内完成了约25,000栋建筑的模拟计算。

Conclusion: 该UBEM管道展示了将高性能计算与开放数据源结合的高效性，为城市尺度能源建模提供了可行的技术方案。

Abstract: Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.

</details>


### [9] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: 评估本地小型语言模型(SLMs)在自动化事件分类中的可行性，替代基于云的LLMs以解决成本、延迟和保密性问题


<details>
  <summary>Details</summary>
Motivation: SOC和CSIRT面临自动化事件分类的压力，但使用基于云的LLMs存在成本、延迟和保密性风险，需要探索本地执行的SLMs是否能满足需求

Method: 评估21个参数范围从1B到20B的模型，调整温度超参数，测量执行时间和精度，比较两种不同架构

Result: 温度对性能影响很小，而参数数量和GPU容量是决定性因素

Conclusion: 本地执行的SLMs在自动化事件分类中具有可行性，关键因素在于模型参数规模和硬件能力

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [10] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 该论文提出了一种在太空中构建可扩展机器学习计算系统的方案，使用配备太阳能电池板、光学星间链路和TPU芯片的卫星群，通过近距离编队飞行实现高带宽低延迟通信。


<details>
  <summary>Details</summary>
Motivation: AI计算需求持续增长，而太阳是太阳系最大的能源，需要考虑如何高效利用太阳能为未来AI基础设施供电。

Method: 使用配备太阳能电池板、光学星间链路和TPU加速器芯片的卫星群，在1公里半径内进行81颗卫星的编队飞行，采用高精度ML模型控制大规模星座。

Result: Trillium TPU经过辐射测试，在相当于5年任务寿命的总电离剂量下无永久故障，并对位翻转错误进行了表征。发射成本分析显示到2030年代中期LEO发射成本可能降至≤200美元/公斤。

Conclusion: 太空中的太阳能驱动AI计算系统是可行的，通过卫星编队飞行和光学通信可以实现高效的大规模机器学习计算。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

</details>


### [11] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: 提出了一个在混合HPC和云环境中高效运行的联邦学习框架，解决了系统异构性、通信开销和资源调度等关键挑战，同时保持模型准确性和数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展和隐私感知AI系统的需求增长，联邦学习成为有前景的解决方案，同时HPC和云基础设施的结合带来了新的复杂性，特别是在处理异构硬件、通信限制和非均匀数据时。

Method: 开发了一个专门为混合HPC和云环境设计的联邦学习框架，通过解决系统异构性、通信开销和资源调度等关键挑战来实现高效运行。

Result: 在混合测试平台上的实验表明，即使在非IID数据分布和不同硬件条件下，系统在可扩展性、容错性和收敛性方面都表现出色。

Conclusion: 这些结果突显了联邦学习作为在现代分布式计算环境中构建可扩展AI系统的实用方法的潜力。

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [12] [Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures](https://arxiv.org/abs/2511.19832)
*Aurelio Vivas,Harold Castro*

Main category: cs.DC

TL;DR: nFlows是一个NUMA感知的工作流执行运行时系统，专门为NUMA架构的HPC系统设计，支持数据密集型工作流的建模、裸机执行、模拟和调度算法验证。


<details>
  <summary>Details</summary>
Motivation: 现代HPC系统普遍采用NUMA架构，包含多个NUMA域和异构内存区域，但现有工作流调度策略大多针对Grid或Cloud环境设计，缺乏NUMA感知能力，导致数据访问延迟变化大，任务和数据放置复杂。

Method: 开发nFlows系统，支持构建模拟模型并在物理系统上直接执行，能够研究NUMA对调度的影响、设计NUMA感知算法、分析数据移动行为、识别性能瓶颈，并探索内存内工作流执行。

Result: 提出了nFlows系统的设计、实现和验证方法，该系统能够有效处理NUMA架构下的工作流调度挑战。

Conclusion: nFlows填补了NUMA感知工作流调度系统的空白，为数据密集型科学工作流在NUMA架构HPC系统上的高效执行提供了重要工具。

Abstract: Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.

</details>


### [13] [Batch Denoising for AIGC Service Provisioning in Wireless Edge Networks](https://arxiv.org/abs/2511.19847)
*Jinghang Xu,Kun Guo,Wei Teng,Chenxi Liu,Wei Feng*

Main category: cs.DC

TL;DR: 提出了一种用于无线边缘网络中AIGC服务的批量去噪框架和联合优化方法，旨在在端到端延迟约束下最大化AIGC服务质量。


<details>
  <summary>Details</summary>
Motivation: 基于两个经验观察：(i)批量去噪通过增强并行性有效降低每步去噪延迟；(ii)早期去噪步骤对生成质量的影响比后期步骤更大。

Method: 开发了STACKING算法来优化批量去噪，该算法独立于任何特定的内容质量函数形式，并实现了较低的计算复杂度。在批量解决方案基础上，进一步优化了AIGC服务间的带宽分配。

Result: 仿真结果表明该算法在提供高质量、低延迟AIGC服务方面表现出优越性能。

Conclusion: 提出的批量去噪框架和联合优化方法能够有效提升无线边缘网络中AIGC服务的质量和延迟性能。

Abstract: Artificial intelligence-generated content (AIGC) service provisioning in wireless edge networks involves two phases: content generation on edge servers and content transmission to mobile devices. In this paper, we take image generation as a representative application and propose a batch denoising framework, followed by a joint optimization of content generation and transmission, with the objective of maximizing the average AIGC service quality under an end-to-end service delay constraint. Motivated by the empirical observations that (i) batch denoising effectively reduces per-step denoising delay by enhancing parallelism and (ii) early denoising steps have a greater impact on generation quality than later steps, we develop the STACKING algorithm to optimize batch denoising. The STACKING operates independently of any specific form of the content quality function and achieves lower computational complexity. Building on the batch solution, we further optimize bandwidth allocation across AIGC services. Simulation results demonstrate the superior performance of our algorithm in delivering high-quality, lower-latency AIGC services.

</details>


### [14] [PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases](https://arxiv.org/abs/2511.19949)
*Qingda Hu,Xinjun Yang,Feifei Li,Junru Li,Ya Lin,Yuqi Zhou,Yicong Zhu,Junwei Zhang,Rongbiao Xie,Ling Zhou,Bin Wu,Wenchao Zhou*

Main category: cs.DC

TL;DR: PolarStore是一个用于云原生关系数据库的压缩共享存储系统，通过软硬件结合的压缩机制实现高压缩比和低性能开销。


<details>
  <summary>Details</summary>
Motivation: 云原生RDBMS虽然提供了弹性计算资源，但存储成本仍是关键问题。现有压缩方法存在性能开销大或灵活性不足的问题。

Method: 采用双层级压缩机制：在PolarCSD硬件中进行存储内压缩，同时在软件层进行轻量级压缩，并结合数据库导向的优化。

Result: 部署在数千台存储服务器上，管理超过100PB数据，压缩比达3.55，存储成本降低约60%，性能与未压缩集群相当。

Conclusion: PolarStore成功解决了云原生RDBMS中存储成本与性能的平衡问题，通过软硬件协同压缩实现了高效存储方案。

Abstract: In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.

</details>


### [15] [Improved Linear-Time Construction of Minimal Dominating Set via Mobile Agents](https://arxiv.org/abs/2511.19880)
*Prabhat Kumar Chand,Anisur Rahaman Molla*

Main category: cs.DC

TL;DR: 本文提出了在匿名图中使用移动代理计算最小支配集(mDS)的线性时间算法，在同步设置下仅需O(n)轮次和O(log n)位内存，无需全局参数知识。


<details>
  <summary>Details</summary>
Motivation: 移动代理作为解决分布式图问题的强大框架，能够在匿名图中高效计算最小支配集等经典问题，但现有方法在时间复杂度和内存使用方面仍有改进空间。

Method: 基于最近提出的最优分散算法，设计了两种新算法，在同步移动代理模型中实现线性时间解决方案，支持根配置和任意初始配置。

Result: 在连通n节点图中，无论初始配置如何，都能在O(n)轮次内计算最小支配集，仅需O(log n)位内存，且无需全局参数知识，改进了文献中的最佳复杂度结果。

Conclusion: 所提算法不仅高效解决了最小支配集问题，还自然地构建了生成树并选举了唯一领导者，这些成果在移动代理框架中具有独立的研究价值。

Abstract: Mobile agents have emerged as a powerful framework for solving fundamental graph problems in distributed settings in recent times. These agents, modelled as autonomous physical or software entities, possess local computation power, finite memory and have the ability to traverse a graph, offering efficient solutions to a range of classical problems. In this work, we focus on the problem of computing a \emph{minimal dominating set} (mDS) in anonymous graphs using mobile agents. Building on the recently proposed optimal dispersion algorithm on the synchronous mobile agent model, we design two new algorithms that achieve a \emph{linear-time} solution for this problem in the synchronous setting. Specifically, given a connected $n$-node graph with $n$ agents initially placed in either rooted or arbitrary configurations, we show that an mDS can be computed in $O(n)$ rounds using only $O(\log n)$ bits of memory per agent, without using any prior knowledge of any global parameters. This improves upon the best-known complexity results in the literature over the same model. In addition, as natural by-products of our methodology, our algorithms also construct a spanning tree and elect a unique leader in $O(n)$ rounds, which are also important results of independent interest in the mobile-agent framework.

</details>


### [16] [SwitchDelta: Asynchronous Metadata Updating for Distributed Storage with In-Network Data Visibility](https://arxiv.org/abs/2511.19978)
*Junru Li,Qing Wang,Zhe Yang,Shuo Liu,Jiwu Shu,Youyou Lu*

Main category: cs.DC

TL;DR: SwitchDelta通过将元数据更新移出关键路径来加速分布式存储系统的有序写入，利用可编程交换机缓冲元数据更新，在网络中实现数据可见性并保持强一致性。


<details>
  <summary>Details</summary>
Motivation: 分布式存储系统通常采用有序写入来保持数据节点和元数据节点之间的强一致性，但这种顺序操作会带来性能开销。需要一种方法来加速有序写入过程。

Method: 在可编程交换机中缓冲正在进行的元数据更新，使数据在网络中可见；采用尽力而为的数据平面设计克服交换机资源限制；设计新颖的元数据更新协议来利用网络内数据可见性的优势。

Result: 在三种分布式内存存储系统（日志结构键值存储、文件系统和二级索引）中评估，SwitchDelta将写操作延迟降低高达52.4%，在写密集型工作负载下吞吐量提升高达126.9%。

Conclusion: SwitchDelta通过将元数据更新移出关键路径，有效加速了分布式存储系统的有序写入操作，显著提升了系统性能。

Abstract: Distributed storage systems typically maintain strong consistency between data nodes and metadata nodes by adopting ordered writes: 1) first installing data; 2) then updating metadata to make data visible.We propose SwitchDelta to accelerate ordered writes by moving metadata updates out of the critical path. It buffers in-flight metadata updates in programmable switches to enable data visibility in the network and retain strong consistency. SwitchDelta uses a best-effort data plane design to overcome the resource limitation of switches and designs a novel metadata update protocol to exploit the benefits of in-network data visibility. We evaluate SwitchDelta in three distributed in-memory storage systems: log-structured key-value stores, file systems, and secondary indexes. The evaluation shows that SwitchDelta reduces the latency of write operations by up to 52.4% and boosts the throughput by up to 126.9% under write-heavy workloads.

</details>


### [17] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: MTMC是一个分层框架，通过将优化策略与实现细节解耦来解决GPU内核生成的挑战。它使用强化学习指导轻量级LLM探索语义优化策略，同时利用通用LLM逐步实现优化提案，在准确性和运行时间上都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 开发高性能GPU内核对AI和科学计算至关重要，但现有基于LLM的方法面临正确性和效率的冲突限制，因为需要探索包含优化策略和实现代码的极大空间。

Method: 提出Macro Thinking Micro Coding (MTMC)分层框架：Macro Thinking使用强化学习指导轻量级LLM学习最大化硬件利用的语义优化策略；Micro Coding利用通用LLM逐步实现优化提案，避免全内核生成错误。

Result: 在KernelBench上，MTMC在Levels 1-2达到近100%准确率，Level 3达70%，比SOTA方法提升50%以上，速度比LLM快7.3倍，比专家优化的PyTorch Eager内核快2.2倍。在TritonBench上达到59.64%准确率和34倍加速。

Conclusion: MTMC通过分层方法有效导航优化空间和实现细节，使LLM能够生成高性能GPU内核，在准确性和运行时间上都显著优于现有方法。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [18] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: Beluga是一个基于CXL技术的新型内存架构，通过CXL交换机让GPU和CPU共享大规模内存池，显著降低LLM推理中的内存访问延迟。


<details>
  <summary>Details</summary>
Motivation: LLM模型规模快速增长和对长上下文推理的需求使得内存成为GPU加速服务系统的关键瓶颈。现有RDMA解决方案存在高延迟、复杂协议和同步开销等问题。

Method: 提出Beluga架构，通过CXL交换机实现GPU和CPU对共享内存池的原生load/store访问，设计Beluga-KVCache系统专门管理LLM推理中的大规模KVCache。

Result: 与基于RDMA的解决方案相比，Beluga-KVCache在vLLM推理引擎中实现了89.6%的TTFT降低和7.35倍的吞吐量提升。

Conclusion: Beluga是首个通过CXL交换机让GPU直接访问大规模内存池的系统，为实现GPU对海量内存资源的低延迟共享访问迈出了重要一步。

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>


### [19] [Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi](https://arxiv.org/abs/2511.20391)
*Anton Ivashkevich,Matija Piškorec,Claudio J. Tessone*

Main category: cs.DC

TL;DR: 开发了一个在多个树莓派上运行的完整以太坊PoW区块链网络原型，用于教育和演示区块链基本概念


<details>
  <summary>Details</summary>
Motivation: 为教育和演示目的创建一个易于设置的独立区块链系统，让用户能够直观理解区块链工作原理和共识机制

Method: 使用多个树莓派计算机构建本地WiFi网络，配备LCD屏幕显示区块链状态，提供基于Web的配置界面

Result: 成功实现了功能完整的以太坊PoW区块链网络原型，能够可视化展示共识过程和网络参数变化的影响

Conclusion: 该原型系统有效地展示了区块链核心概念，特别适合教育用途，能够直观呈现PoW共识机制及其影响因素

Abstract: We describe a prototype of a fully capable Ethereum Proof-of-Work (PoW) blockchain network running on multiple Raspberry Pi (RPi) computers. The prototype is easy to set up and is intended to function as a completely standalone system, using a local WiFi router for connectivity. It features LCD screens for visualization of the local state of blockchain ledgers on each RPi, making it ideal for educational purposes and to demonstrate fundamental blockchain concepts to a wide audience. For example, a functioning PoW consensus is easily visible from the LCD screens, as well as consensus degradation which might arise from various factors, including peer-to-peer topology and communication latency - all parameters which can be configured from the central web-based interface.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [20] [Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization](https://arxiv.org/abs/2511.19830)
*Junhao Zhu,Lu Chen,Xiangyu Ke,Ziquan Fang,Tianyi Li,Yunjun Gao,Christian S. Jensen*

Main category: cs.DB

TL;DR: Nirvana是一个多模态数据分析框架，通过可编程语义算子和LLM驱动的语义查询处理，结合逻辑和物理查询优化策略，显著提升多模态数据分析效率。


<details>
  <summary>Details</summary>
Motivation: 传统关系查询算子在捕捉查询语义方面能力有限，阻碍了多模态分析在现实世界的应用。基础模型特别是大语言模型的出现为开发灵活、语义感知的数据分析系统提供了新机遇。

Method: 1) 基于自然语言转换规则和随机游走搜索的代理逻辑优化器；2) 使用改进分数指标选择最有效LLM后端的成本感知物理优化器；3) 计算重用和基于模型能力假设的评估下推技术。

Result: 在三个真实世界基准测试中，Nirvana能够将端到端运行时间减少10%-85%，系统处理成本平均降低76%，在效率和可扩展性方面均优于最先进系统。

Conclusion: Nirvana框架通过结合语义感知查询处理和优化策略，成功克服了传统关系算子的局限性，为多模态数据分析的实际应用提供了可行解决方案。

Abstract: Multi-modal analytical processing has the potential to transform applications in e-commerce, healthcare, entertainment, and beyond. However, real-world adoption remains elusive due to the limited ability of traditional relational query operators to capture query semantics. The emergence of foundation models, particularly the large language models (LLMs), opens up new opportunities to develop flexible, semantic-aware data analytics systems that transcend the relational paradigm.
  We present Nirvana, a multi-modal data analytics framework that incorporates programmable semantic operators while leveraging both logical and physical query optimization strategies, tailored for LLM-driven semantic query processing. Nirvana addresses two key challenges. First, it features an agentic logical optimizer that uses natural language-specified transformation rules and random-walk-based search to explore vast spaces of semantically equivalent query plans -- far beyond the capabilities of conventional optimizers. Second, it introduces a cost-aware physical optimizer that selects the most effective LLM backend for each operator using a novel improvement-score metric. To further enhance efficiency, Nirvana incorporates computation reuse and evaluation pushdown techniques guided by model capability hypotheses. Experimental evaluations on three real-world benchmarks demonstrate that Nirvana is able to reduce end-to-end runtime by 10%--85% and reduces system processing costs by 76% on average, outperforming state-of-the-art systems at both efficiency and scalability.

</details>


### [21] [Updatable Balanced Index for Fast On-device Search with Auto-selection Model](https://arxiv.org/abs/2511.20049)
*Yushuai Ji,Sheng Wang,Zhiyu Chen,Yuan Sun,Zhiyong Peng*

Main category: cs.DB

TL;DR: UnIS是一个针对边缘设备上多路KD树索引的优化系统，通过预测分割超平面加速构建、选择性子树重建加速插入、自动选择搜索策略提升查询性能，在构建、插入和搜索方面相比BMKD-tree都有显著加速。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的平衡多路KD树(BMKD-tree)在构建开销大、不支持实时插入、查询性能不稳定等方面存在限制，影响了边缘数据分析的效率。

Method: 1) 利用数据集分布预测分割超平面加速构建；2) 选择性子树重建方案减少插入时重新平衡的数据点数量；3) 自动选择模型为任意查询任务选择最优搜索策略。

Result: 相比BMKD-tree，UnIS在索引构建上平均加速17.96倍，插入加速1.60倍，kNN搜索加速7.15倍，半径搜索加速1.09倍，在数据集简化任务中比Lloyd算法快217倍。

Conclusion: UnIS有效解决了BMKD-tree在边缘设备上的性能瓶颈，显著提升了索引构建、数据插入和查询操作的效率。

Abstract: Diverse types of edge data, such as 2D geo-locations and 3D point clouds, are collected by sensors like lidar and GPS receivers on edge devices. On-device searches, such as k-nearest neighbor (kNN) search and radius search, are commonly used to enable fast analytics and learning technologies, such as k-means dataset simplification using kNN. To maintain high search efficiency, a representative approach is to utilize a balanced multi-way KD-tree (BMKD-tree). However, the index has shown limited gains, mainly due to substantial construction overhead, inflexibility to real-time insertion, and inconsistent query performance. In this paper, we propose UnIS to address the above limitations. We first accelerate the construction process of the BMKD-tree by utilizing the dataset distribution to predict the splitting hyperplanes. To make the continuously generated data searchable, we propose a selective sub-tree rebuilding scheme to accelerate rebalancing during insertion by reducing the number of data points involved. We then propose an auto-selection model to improve query performance by automatically selecting the optimal search strategy among multiple strategies for an arbitrary query task. Experimental results show that UnIS achieves average speedups of 17.96x in index construction, 1.60x in insertion, 7.15x in kNN search, and 1.09x in radius search compared to the BMKD-tree. We further verify its effectiveness in accelerating dataset simplification on edge devices, achieving a speedup of 217x over Lloyd's algorithm.

</details>


### [22] [Mobility Stream Processing on NebulaStream and MEOS](https://arxiv.org/abs/2511.20084)
*Mariana M. Garcez Duarte,Dwi P. A. Nugroho,Georges Tod,Evert Bevernage,Pieter Moelans,Emine Tas,Esteban Zimanyi,Mahmoud Sakr,Steffen Zeuch,Volker Markl*

Main category: cs.DB

TL;DR: NebulaMEOS结合MEOS时空处理库与NebulaStream物联网数据管理系统，实现实时时空流数据处理，应用于比利时铁路公司的列车数据流分析。


<details>
  <summary>Details</summary>
Motivation: 物联网传感器在移动物体上的广泛应用产生了大量时空流数据，但现有流处理系统缺乏时空处理能力，而时空库主要针对历史数据分析，这使得实时时空分析面临挑战。

Method: 将MEOS时空处理库集成到NebulaStream物联网数据管理系统中，利用时空功能实时处理和分析流数据。

Result: 系统能够处理比利时铁路公司列车边缘设备的数据流，支持地理围栏和地理空间复杂事件处理，可视化实时列车运行和环境影响。

Conclusion: NebulaMEOS成功填补了实时时空流数据分析的空白，为物联网环境下的实时时空分析提供了可行解决方案。

Abstract: The increasing use of Internet-of-Things (IoT) sensors in moving objects has resulted in vast amounts of spatiotemporal streaming data. To analyze this data in situ, real-time spatiotemporal processing is needed. However, current stream processing systems designed for IoT environments often lack spatiotemporal processing capabilities, and existing spatiotemporal libraries primarily focus on analyzing historical data. This gap makes performing real-time spatiotemporal analytics challenging. In this demonstration, we present NebulaMEOS, which combines MEOS (Mobility Engine Open Source), a spatiotemporal processing library, with NebulaStream, a scalable data management system for IoT applications. By integrating MEOS into NebulaStream, NebulaMEOS utilizes spatiotemporal functionalities to process and analyze streaming data in real-time. We demonstrate NebulaMEOS by querying data streamed from edge devices on trains by the Société Nationale des Chemins de fer Belges (SNCB). Visitors can experience demonstrations of geofencing and geospatial complex event processing, visualizing real-time train operations and environmental impacts.

</details>


### [23] [N2E: A General Framework to Reduce Node-Differential Privacy to Edge-Differential Privacy for Graph Analytics](https://arxiv.org/abs/2511.20125)
*Yihua Hu,Hao Ding,Wei Dong*

Main category: cs.DB

TL;DR: 提出了N2E框架，将节点差分隐私图分析任务转化为边差分隐私任务，通过距离保持剪裁和最大度近似机制，显著提升了节点差分隐私的实用性。


<details>
  <summary>Details</summary>
Motivation: 节点差分隐私比边差分隐私提供更强的隐私保护，但由于技术挑战研究较少。需要一种通用框架将节点差分隐私任务转化为边差分隐私任务，以复用现有边差分隐私机制。

Method: N2E框架包含两个关键技术：距离保持剪裁机制，在剪裁后保持邻接图之间的边距离；首个节点差分隐私的最大度近似机制，实现紧密的隐私保护剪裁阈值。

Result: 实验结果显示，在边计数任务中误差减少2.5倍，在度分布估计中误差减少80倍。理论分析表明边计数误差与最优方法匹配。

Conclusion: N2E框架成功地将节点差分隐私任务转化为边差分隐私任务，显著提升了节点差分隐私的实用性，为图分析提供了更强的隐私保护方案。

Abstract: Differential privacy (DP) has been widely adopted to protect sensitive information in graph analytics. While edge-DP, which protects privacy at the edge level, has been extensively studied, node-DP, offering stronger protection for entire nodes and their incident edges, remains largely underexplored due to its technical challenges. A natural way to bridge this gap is to develop a general framework for reducing node-DP graph analytical tasks to edge-DP ones, enabling the reuse of existing edge-DP mechanisms. A straightforward solution based on group privacy divides the privacy budget by a given degree upper bound, but this leads to poor utility when the bound is set conservatively large to accommodate worst-case inputs. To address this, we propose node-to-edge (N2E), a general framework that reduces any node-DP graph analytical task to an edge-DP one, with the error dependency on the graph's true maximum degree. N2E introduces two novel techniques: a distance-preserving clipping mechanism that bounds edge distance between neighboring graphs after clipping, and the first node-DP mechanism for maximum degree approximation, enabling tight, privacy-preserving clipping thresholds. By instantiating N2E with existing edge-DP mechanisms, we obtain the first node-DP solutions for tasks such as maximum degree estimation. For edge counting, our method theoretically matches the error of the state-of-the-art, which is provably optimal, and significantly outperforms existing approaches for degree distribution estimation. Experimental results demonstrate that our framework achieves up to a 2.5x reduction in error for edge counting and up to an 80x reduction for degree distribution estimation.

</details>


### [24] [An experimental study of existing tools for outlier detection and cleaning in trajectories](https://arxiv.org/abs/2511.20139)
*Mariana M Garcez Duarte,Mahmoud Sakr*

Main category: cs.DB

TL;DR: 本文评估了10个开源异常值检测库在轨迹数据中的表现，比较了它们的效率和准确性，并对现有算法进行了分类和调查。


<details>
  <summary>Details</summary>
Motivation: 异常值检测和清理是数据预处理的关键步骤，但现有工具在轨迹数据中的表现缺乏系统评估，需要为实际应用提供选择指导。

Method: 实验评估10个开源库在轨迹异常值检测中的表现，建立真实基准方法，并对现有算法按统计方法、滑动窗口、聚类、图论和启发式方法进行分类。

Result: 提供了各库在效率和准确性方面的比较结果，为实际应用中的工具选择提供了依据。

Conclusion: 研究为轨迹数据异常值检测提供了实用的库性能评估和算法分类，推动了数据预处理方法的发展。

Abstract: Outlier detection and cleaning are essential steps in data preprocessing to ensure the integrity and validity of data analyses. This paper focuses on outlier points within individual trajectories, i.e., points that deviate significantly inside a single trajectory. We experiment with ten open-source libraries to comprehensively evaluate available tools, comparing their efficiency and accuracy in identifying and cleaning outliers. This experiment considers the libraries as they are offered to end users, with real-world applicability. We compare existing outlier detection libraries, introduce a method for establishing ground-truth, and aim to guide users in choosing the most appropriate tool for their specific outlier detection needs. Furthermore, we survey the state-of-the-art algorithms for outlier detection and classify them into five types: Statistic-based methods, Sliding window algorithms, Clustering-based methods, Graph-based methods, and Heuristic-based methods. Our research provides insights into these libraries' performance and contributes to developing data preprocessing and outlier detection methodologies.

</details>


### [25] [Forgetting by Pruning: Data Deletion in Join Cardinality Estimation](https://arxiv.org/abs/2511.20293)
*Chaowei He,Yuanjun Liu,Qingzhi Ma,Shenyuan Ren,Xizhao Luo,Lei Zhao,An Liu*

Main category: cs.DB

TL;DR: 提出了首个针对多表学习基数估计系统的遗忘框架CEP，通过分布敏感剪枝和域剪枝解决数据删除带来的挑战，在IMDB和TPC-H数据集上优于完全重训练。


<details>
  <summary>Details</summary>
Motivation: 学习基数估计系统中的机器遗忘面临独特挑战：属性级敏感性、表间传播和域消失导致多表连接严重高估。

Method: CEP框架包含分布敏感剪枝（构建半连接删除结果并计算敏感度分数指导参数剪枝）和域剪枝（移除被删除完全消除的值域支持）。

Result: 在NeuroCard和FACE架构上，CEP在多表场景下获得最低Q-error，特别是在高删除率下常优于完全重训练，收敛迭代显著减少，计算开销仅为微调时间的0.3%-2.5%。

Conclusion: CEP是首个专门为多表学习基数估计系统设计的遗忘框架，能有效解决数据删除带来的挑战，性能优于完全重训练且计算开销极小。

Abstract: Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.

</details>


### [26] [The Case for Intent-Based Query Rewriting](https://arxiv.org/abs/2511.20419)
*Gianna Lisa Nicolai,Patrick Hansert,Sebastian Michel*

Main category: cs.DB

TL;DR: 本文提出了基于意图的查询重写概念和INQURE系统，通过LLM理解查询意图并生成语义等效但结构不同的查询，解决传统查询重写无法处理的数据访问限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统查询重写仅关注查询优化，无法解决因访问控制、隐私或成本限制导致的数据不可访问问题。需要一种能保持洞察力但改变查询结构和语法的重写方法。

Method: INQURE系统利用LLM进行查询理解和替代查询生成，结合前置表过滤、候选重写剪枝和排序技术，实现意图保持的查询重写。

Result: 在900多个数据库表模式的基准测试中进行了评估，通过用户研究分析了不同方法在运行时间和重写质量方面的优缺点。

Conclusion: 基于意图的查询重写是可行的解决方案，INQURE系统能够有效生成语义等效的替代查询，解决数据访问限制问题。

Abstract: With this work, we describe the concept of intent-based query rewriting and present a first viable solution. The aim is to allow rewrites to alter the structure and syntactic outcome of an original query while keeping the obtainable insights intact. This drastically differs from traditional query rewriting, which typically aims to decrease query evaluation time by using strict equivalence rules and optimization heuristics on the query plan. Rewriting queries to queries that only provide a similar insight but otherwise can be entirely different can remedy inaccessible original data tables due to access control, privacy, or expensive data access regarding monetary cost or remote access. In this paper, we put forward INQURE, a system designed for INtent-based QUery REwriting. It uses access to a large language model (LLM) for the query understanding and human-like derivation of alternate queries. Around the LLM, INQURE employs upfront table filtering and subsequent candidate rewrite pruning and ranking. We report on the results of an evaluation using a benchmark set of over 900 database table schemas and discuss the pros and cons of alternate approaches regarding runtime and quality of the rewrites of a user study.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [27] [Building Browser Agents: Architecture, Security, and Practical Solutions](https://arxiv.org/abs/2511.19477)
*Aram Vardanyan*

Main category: cs.SE

TL;DR: 本文分析了生产环境中浏览器代理的可靠性和安全性挑战，指出模型能力不是限制因素，架构决策才是关键。研究发现通用自主操作存在根本性安全隐患，主张开发具有程序约束的专用工具而非通用浏览智能。


<details>
  <summary>Details</summary>
Motivation: 解决浏览器代理在生产环境中的可靠性和安全问题，分析当前方法失败的原因以及阻碍安全自主操作的因素。

Method: 采用混合上下文管理（结合可访问性树快照和选择性视觉）、全面的浏览器工具集（匹配人类交互能力）以及智能提示工程。

Result: 在WebGames基准测试的53个多样化挑战中，代理成功率约达85%，而先前浏览器代理约为50%，人类基线为95.7%。

Conclusion: 反对开发通用浏览智能，主张开发具有程序约束的专用工具，通过代码而非大语言模型推理来强制执行安全边界。

Abstract: Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).

</details>


### [28] [Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation](https://arxiv.org/abs/2511.19483)
*Qingsong He,Jing Nan,Jiayu Jiao,Liangjie Tang,Xiaodong Xu,Mengmeng Sun,Qingyao Wang,Minghui Yan*

Main category: cs.SE

TL;DR: Z-Space是一个面向数据生成的多智能体协作工具调用框架，通过意图解析、工具过滤和推理执行三个模块，解决企业级MCP服务中工具匹配的效率和准确性问题。


<details>
  <summary>Details</summary>
Motivation: 随着企业级MCP服务的快速增长，在数千个异构工具中高效准确地匹配目标功能成为限制系统实用性的核心挑战。现有方法存在用户查询与工具描述语义脱节、LLM输入上下文膨胀和高推理延迟等问题。

Method: 建立多智能体协作架构和工具过滤算法：1) 通过意图解析模型实现用户查询的结构化语义理解；2) 基于融合子空间加权算法的工具过滤模块实现意图与工具的细粒度语义对齐；3) 构建推理执行智能体支持多步骤任务的动态规划和容错执行。

Result: 系统在饿了么平台技术部门部署，服务于淘天、高德、盒马等多个业务单元的大规模测试数据生成场景。生产数据显示，系统将工具推理的平均token消耗降低96.26%，同时达到92%的工具调用准确率。

Conclusion: Z-Space框架显著提升了智能测试数据生成系统的效率和可靠性，为企业级MCP服务中的工具调用问题提供了有效解决方案。

Abstract: Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\% while achieving a 92\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.

</details>


### [29] [stable-pretraining-v1: Foundation Model Research Made Simple](https://arxiv.org/abs/2511.19484)
*Randall Balestriero,Hugues Van Assel,Sami BuGhanem,Lucas Maes*

Main category: cs.SE

TL;DR: stable-pretraining是一个用于基础模型自监督学习的模块化、可扩展库，旨在解决复杂代码库、冗余实现和实验扩展工程负担的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型和自监督学习研究受到复杂代码库、冗余重新实现以及扩展实验的沉重工程负担的阻碍，需要更灵活、高效的开发工具。

Method: 基于PyTorch、Lightning、Hugging Face和TorchMetrics构建模块化库，统一了SSL核心工具（探针、崩溃检测指标、增强管道、可扩展评估例程），采用全面日志记录设计原则。

Result: 验证了库能够以最小开销生成新的研究见解，包括深度表示探针和CLIP在合成数据微调下的退化分析。

Conclusion: stable-pretraining通过降低入门门槛同时保持大规模实验的可扩展性，旨在加速基础模型研究的发现和扩展可能性。

Abstract: Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.

</details>


### [30] [Evolution without an Oracle: Driving Effective Evolution with LLM Judges](https://arxiv.org/abs/2511.19489)
*Zhe Zhao,Yuheng Yang,Haibin Wen,Xiaojie Qiu,Zaixi Zhang,Qingfu Zhang*

Main category: cs.SE

TL;DR: MADE框架通过问题分解将主观LLM评估转化为稳定选择压力，在无客观适应度函数的情况下实现进化优化，在多个基准测试中性能提升超过50%。


<details>
  <summary>Details</summary>
Motivation: 打破传统进化计算依赖客观适应度函数的限制，探索在纯主观LLM评判下的进化优化可能性。

Method: 引入MADE框架，通过问题规范将模糊指令分解为可验证的子需求，降低主观评估的噪声。

Result: 在DevAI和InfoBench基准测试中，软件需求满足率从39.9%提升至61.9%，复杂指令遵循的完美通过率达到95%。

Conclusion: 验证了从优化可计算指标到优化可描述质量的根本范式转变，为无真实标签的开放领域解锁了进化优化潜力。

Abstract: The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.

</details>


### [31] [CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem](https://arxiv.org/abs/2511.19510)
*Asif Zaman,Kallol Naha,Khalid Belhajjame,Hasan M. Jamil*

Main category: cs.SE

TL;DR: CodeR³系统利用生成式AI将过时的Taverna工作流迁移到Snakemake和VisFlow等现代技术，通过自动化分析、服务替换和人机验证来修复和重用衰败的工作流。


<details>
  <summary>Details</summary>
Motivation: 科学工作流包含宝贵的领域专业知识，但大量已发布工作流会随时间衰败，特别是像Taverna这样的传统系统，因服务停止、依赖过时和系统退役导致工作流无法使用。

Method: 开发CodeR³系统，结合生成式AI分析衰败工作流特征，将其转换为现代工作流技术，集成逐步工作流分析可视化、自动化服务替换和人机验证循环。

Result: 通过Taverna工作流复兴案例研究证明该方法可行性，自动化显著减少了工作流解析和服务识别的手动工作，但服务替换和数据验证仍需领域专业知识。

Conclusion: 提出了一个平衡自动化效率与必要人工判断的工作流复兴框架，将开发众包平台供社区协作复兴衰败工作流并验证功能正确性。

Abstract: Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.

</details>


### [32] [Agint: Agentic Graph Compilation for Software Engineering Agents](https://arxiv.org/abs/2511.19635)
*Abhi Chivukula,Jay Somasundaram,Vijay Somasundaram*

Main category: cs.SE

TL;DR: Agint是一个代理图编译器、解释器和运行时，通过分层增量编译将自然语言指令转换为类型化、效果感知的代码DAG，解决了LLM编码代理在上下文管理、延迟、可靠性和可扩展性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的编码代理面临上下文管理、延迟、可靠性、可重现性和可扩展性等挑战，需要一种能够动态优化、可重现执行且与现有开发工具互操作的系统。

Method: 引入显式类型层次（文本→数据→规范→代码），基于语义图变换和混合LLM与函数的JIT运行时，支持动态图优化、推测性评估和并发组合。

Result: 提高了可靠性，支持使用更小更快的模型进行加速开发，降低延迟，提高吞吐量，支持可重现和高效并行生成。

Conclusion: Agint通过连接自然语言、编译器方法和开发工具，实现了可组合、团队中心的编码代理规模化，支持从原型到生产代码的无缝过渡。

Abstract: LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.

</details>


### [33] [CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection](https://arxiv.org/abs/2511.19875)
*Qingyu Zhang,Puzhuo Liu,Peng Di,Chenxiong Qian*

Main category: cs.SE

TL;DR: CODEFUSE-COMMITEVAL是首个专门用于评估LLM检测提交消息与代码不一致性(MCI)的基准，通过规则引导的突变生成七种不一致消息类型，评估显示模型能可靠检测不一致提交但存在类型差异。


<details>
  <summary>Details</summary>
Motivation: 提交消息与代码不一致(MCI)会误导审查者、阻碍维护、污染研究数据集并可能掩盖安全补丁，但目前缺乏专门的基准来评估MCI检测模型。

Method: 基于ApacheCM数据集，通过规则引导的突变生成七种不一致消息类型，使用双重验证确保样本质量，评估六种开源LLM在原始设置和三种增强策略下的表现。

Result: 模型检测不一致提交比一致提交更可靠(平均召回率85.95%，精确率80.28%，特异性63.8%)；gpt-oss-20B表现最佳但token消耗是其他模型的两倍以上；增强策略效果各异。

Conclusion: CODEFUSE-COMMITEVAL为MCI检测提供了严谨的评估基础，强调需要更丰富的上下文和平衡的数据来捕捉高级语义差异。

Abstract: Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

</details>


### [34] [LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework](https://arxiv.org/abs/2511.20403)
*Andrea Lops,Fedelucio Narducci,Azzurra Ragone,Michelantonio Trizio,Claudio Barto*

Main category: cs.SE

TL;DR: AgoneTest是一个用于评估LLM生成的Java单元测试的自动化框架，包含Classes2Test数据集和综合评估指标，实验表明编译通过的LLM生成测试在覆盖率和缺陷检测方面可媲美甚至优于人工编写测试。


<details>
  <summary>Details</summary>
Motivation: 单元测试是软件开发中重要但资源密集的步骤，需要标准化框架来评估不同LLM和提示策略在真实条件下的测试生成能力。

Method: 提出AgoneTest评估框架，包含Classes2Test数据集（Java类与对应测试类的映射）和集成变异分数、测试异味等高级评估指标的综合评估方法。

Result: 对于能够编译通过的测试子集，LLM生成的测试在覆盖率和缺陷检测方面可以匹配甚至超越人工编写的测试，增强的提示策略有助于提高测试质量。

Conclusion: AgoneTest阐明了LLM在软件测试中的潜力，并为模型设计、提示工程和测试实践的改进提供了见解。

Abstract: Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.

</details>
