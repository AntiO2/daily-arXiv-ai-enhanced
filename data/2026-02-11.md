<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [RuleFlow : Generating Reusable Program Optimizations with LLMs](https://arxiv.org/abs/2602.09051)
*Avaljot Singh,Dushyant Bharadwaj,Stefanos Baziotis,Kaushik Varadharajan,Charith Mendis*

Main category: cs.SE

TL;DR: RuleFlow提出了一种混合方法，通过三阶段流程优化Pandas程序：发现特定程序优化、转换为通用重写规则、集成到编译器中自动应用，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有Pandas优化方法存在局限性：系统/编译器方法要么笨重，要么支持有限优化；基于LLM的逐程序优化方法虽然能合成非平凡优化，但不可靠、昂贵且产出率低。

Method: 采用三阶段混合方法：1) 发现阶段：使用LLM发现特定程序的优化；2) 桥接阶段：将发现的优化转换为通用重写规则；3) 部署阶段：将规则集成到编译器中，自动应用规则而无需重复依赖LLM。

Result: 在PandasBench基准测试中，RuleFlow成为新的SOTA Pandas优化框架，相比之前的编译器SOTA（Dias）最高加速4.3倍，相比之前的系统SOTA（Modin）最高加速1914.9倍。

Conclusion: RuleFlow通过将LLM驱动的优化发现与编译器驱动的规则应用相结合，提供了一种可靠、高效且可扩展的Pandas程序优化方法，克服了现有方法的局限性。

Abstract: Optimizing Pandas programs is a challenging problem. Existing systems and compiler-based approaches offer reliability but are either heavyweight or support only a limited set of optimizations. Conversely, using LLMs in a per-program optimization methodology can synthesize nontrivial optimizations, but is unreliable, expensive, and offers a low yield. In this work, we introduce a hybrid approach that works in a 3-stage manner that decouples discovery from deployment and connects them via a novel bridge. First, it discovers per-program optimizations (discovery). Second, they are converted into generalised rewrite rules (bridge). Finally, these rules are incorporated into a compiler that can automatically apply them wherever applicable, eliminating repeated reliance on LLMs (deployment). We demonstrate that RuleFlow is the new state-of-the-art (SOTA) Pandas optimization framework on PandasBench, a challenging Pandas benchmark consisting of Python notebooks. Across these notebooks, we achieve a speedup of up to 4.3x over Dias, the previous compiler-based SOTA, and 1914.9x over Modin, the previous systems-based SOTA.
  Our code is available at https://github.com/ADAPT-uiuc/RuleFlow.

</details>


### [2] [Predicting Open Source Software Sustainability with Deep Temporal Neural Hierarchical Architectures and Explainable AI](https://arxiv.org/abs/2602.09064)
*S M Rakib Ul Karim,Wenyi Lu,Enock Kasaadha,Sean Goggins*

Main category: cs.SE

TL;DR: 提出一个分层预测框架，将开源软件项目建模为基于社会技术分类的不同生命周期阶段，使用特征工程和时间序列数据实现高精度分类，强调贡献活动和社区参与是可持续性的核心信号。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖静态或聚合指标（如项目年龄或累计活动），无法深入理解开源软件可持续性如何随时间展开。需要一种能够捕捉项目组织演变和健康状况的动态分析方法。

Method: 提出分层预测框架：1) 基于社会技术分类定义不同的生命周期阶段；2) 结合工程化表格指标和24个月时间活动序列；3) 采用多阶段分类管道区分不同协调和参与机制的生命周期阶段；4) 集成可解释AI技术分析特征类别对预测的相对贡献。

Result: 在大型开源软件仓库语料库上评估，该方法在生命周期阶段分类中达到超过94%的整体准确率。归因分析一致识别出贡献活动和社区相关特征为主导信号。

Conclusion: 该框架成功地将可持续性操作化为多维构造，强调集体参与动态在开源软件项目生命周期中的核心作用。贡献活动和社区参与是预测项目发展阶段的关键指标。

Abstract: Open Source Software (OSS) projects follow diverse lifecycle trajectories shaped by evolving patterns of contribution, coordination, and community engagement. Understanding these trajectories is essential for stakeholders seeking to assess project organization and health at scale. However, prior work has largely relied on static or aggregated metrics, such as project age or cumulative activity, providing limited insight into how OSS sustainability unfolds over time. In this paper, we propose a hierarchical predictive framework that models OSS projects as belonging to distinct lifecycle stages grounded in established socio-technical categorizations of OSS development. Rather than treating sustainability solely as project longevity, these lifecycle stages operationalize sustainability as a multidimensional construct integrating contribution activity, community participation, and maintenance dynamics. The framework combines engineered tabular indicators with 24-month temporal activity sequences and employs a multi-stage classification pipeline to distinguish lifecycle stages associated with different coordination and participation regimes. To support transparency, we incorporate explainable AI techniques to examine the relative contribution of feature categories to model predictions. Evaluated on a large corpus of OSS repositories, the proposed approach achieves over 94\% overall accuracy in lifecycle stage classification. Attribution analyses consistently identify contribution activity and community-related features as dominant signals, highlighting the central role of collective participation dynamics.

</details>


### [3] [DRAGON: Robust Classification for Very Large Collections of Software Repositories](https://arxiv.org/abs/2602.09071)
*Stefano Balla,Stefano Zacchiroli,Thomas Degueule,Jean-Rémy Falleri,Romain Robbes*

Main category: cs.SE

TL;DR: DRAGON：基于轻量信号（文件名/目录名）的代码仓库分类器，在README缺失时仍保持鲁棒性，在大规模软件集合中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有代码仓库分类方法过度依赖README等元数据，但这些文件经常缺失，限制了在大规模真实场景中的应用。需要一种更鲁棒的分类方法，能够在文档稀疏或不一致的情况下有效工作。

Method: DRAGON仅使用版本控制系统中常见的轻量信号：文件和目录名称，可选地使用README（当可用时）。该方法设计用于处理大规模和多样化的软件集合，不依赖完整的文档。

Result: 在大规模仓库分类中，DRAGON将F1@5从54.8%提升到60.8%，超越了现有技术。即使README完全缺失，性能仅下降6%。许多分类错误是"近似命中"，预测标签在语义上接近正确主题。

Conclusion: DRAGON提供了一种实用的大规模代码仓库分类解决方案，在文档稀疏的真实场景中保持鲁棒性。同时发布了迄今为止最大的开源仓库分类数据集（82.5万个仓库），为未来大规模、语言无关的软件仓库理解研究奠定基础。

Abstract: The ability to automatically classify source code repositories with ''topics'' that reflect their content and purpose is very useful, especially when navigating or searching through large software collections. However, existing approaches often rely heavily on README files and other metadata, which are frequently missing, limiting their applicability in real-world large-scale settings. We present DRAGON, a repository classifier designed for very large and diverse software collections. It operates entirely on lightweight signals commonly stored in version control systems: file and directory names, and optionally the README when available. In repository classification at scale, DRAGON improves F1@5 from 54.8% to 60.8%, surpassing the state of the art. DRAGON remains effective even when README files are absent, with performance degrading by only 6% w.r.t. when they are present. This robustness makes it practical for real-world settings where documentation is sparse or inconsistent. Furthermore, many of the remaining classification errors are near misses, where predicted labels are semantically close to the correct topics. This property increases the practical value of the predictions in real-world software collections, where suggesting a few related topics can still guide search and discovery. As a byproduct of developing DRAGON, we also release the largest open dataset to date for repository classification, consisting of 825 thousand repositories with associated ground-truth topics, sourced from the Software Heritage archive, providing a foundation for future large-scale and language-agnostic research on software repository understanding.

</details>


### [4] [AIDev: Studying AI Coding Agents on GitHub](https://arxiv.org/abs/2602.09185)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AIDev是一个大规模数据集，收集了932,791个由AI编码代理在真实GitHub项目中创建的Pull Request，涵盖5种主流AI代理，为研究AI在软件开发中的采用、生产力和人机协作提供基础。


<details>
  <summary>Details</summary>
Motivation: AI编码代理正在快速改变软件工程，但研究社区缺乏全面数据集来捕捉这些代理在真实项目中的使用情况。为了填补这一空白，需要创建能够反映AI代理在实际开发中应用的数据集。

Method: 从GitHub收集AI代理创建的Pull Request数据，包括5种主流AI代理（OpenAI Codex、Devin、GitHub Copilot、Cursor、Claude Code）。数据集包含932,791个Agentic-PR，涉及116,211个仓库和72,189名开发者。还创建了一个精选子集，包含33,596个来自2,807个高星仓库的PR，附带评论、审查、提交和相关问题等详细信息。

Result: 成功构建了AIDev数据集，这是首个专注于AI代理在真实GitHub项目中创建Pull Request的大规模数据集。数据集规模庞大，覆盖了多种主流AI代理和大量开发项目，为后续研究提供了丰富的数据基础。

Conclusion: AIDev数据集为研究AI在软件工程中的采用、开发者生产力以及人机协作的新时代提供了重要基础，填补了该领域缺乏真实世界数据的空白。

Abstract: AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering

</details>


### [5] [Towards an OSF-based Registered Report Template for Software Engineering Controlled Experiments](https://arxiv.org/abs/2602.09292)
*Ana B. M. Bett,Thais S. Nepomuceno,Edson OliveiraJr,Maria Teresa Baldassarre,Valdemar V. Graciano Neto,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 该论文分析了在软件工程控制实验中采用注册报告（RR）的现状，发现现有OSF RR模板无法完全满足SE实验文档指南，建议建立SE专用的RR指南。


<details>
  <summary>Details</summary>
Motivation: 尽管经验软件工程社区多年来改进了实验方法，但控制实验的描述仍然缺乏严谨性，影响了可重复性和透明度。注册报告（RR）被提出作为解决方案，但需要评估其在SE领域的适用性。

Method: 分析Open Science Framework（OSF）中选定的RR模板，对照软件工程控制实验的文档指南进行评估。

Result: 分析发现：1）虽然有一个RR类型与许多文档建议相符，但没有一个能全面覆盖指南要求；2）OSF RR模板定制存在局限性；3）当前可用的RR类型无法完全满足SE实验的文档需求。

Conclusion: 尽管ESE领域有进步，但实验规划和文档仍缺乏严谨性，损害了可重复性。建议采用基于OSF的RR，但需要建立专门针对软件工程的RR指南，因为现有RR类型无法完全满足需求。

Abstract: Context: The empirical software engineering (ESE) community has contributed to improving experimentation over the years. However, there is still a lack of rigor in describing controlled experiments, hindering reproducibility and transparency. Registered Reports (RR) have been discussed in the ESE community to address these issues. A RR registers a study's hypotheses, methods, and/or analyses before execution, involving peer review and potential acceptance before data collection. This helps mitigate problematic practices such as p-hacking, publication bias, and inappropriate post hoc analysis. Objective: This paper presents initial results toward establishing an RR template for Software Engineering controlled experiments using the Open Science Framework (OSF). Method: We analyzed templates of selected OSF RR types in light of documentation guidelines for controlled experiments. Results: The observed lack of rigor motivated our investigation of OSF-based RR types. Our analysis showed that, although one of the RR types aligned with many of the documentation suggestions contained in the guidelines, none of them covered the guidelines comprehensively. The study also highlights limitations in OSF RR template customization. Conclusion: Despite progress in ESE, planning and documenting experiments still lack rigor, compromising reproducibility. Adopting OSF-based RRs is proposed. However, no currently available RR type fully satisfies the guidelines. Establishing RR-specific guidelines for SE is deemed essential.

</details>


### [6] [Cross-Project Flakiness: A Case Study of the OpenStack Ecosystem](https://arxiv.org/abs/2602.09311)
*Tao Xiao,Dong Wang,Shane McIntosh,Hideaki Hata,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 对OpenStack生态系统中测试不稳定性的实证研究，发现跨项目不稳定性影响55%的项目，显著增加审查时间和计算成本，70%的单元测试也受影响，挑战了单元测试隔离性的假设。


<details>
  <summary>Details</summary>
Motivation: 自动化回归测试是现代软件开发的核心，但不稳定的测试结果会破坏开发者信任、浪费计算资源并损害CI可靠性。先前研究主要关注单个项目内的测试不稳定性，但其在更广泛生态系统中的影响尚未得到充分探索。

Method: 对649个OpenStack项目进行实证研究，重点关注：(1)跨项目不稳定性——不稳定的测试影响多个项目；(2)不一致的不稳定性——测试在某些项目中不稳定但在其他项目中稳定。通过分析识别出1,535个跨项目不稳定测试和1,105个不一致不稳定测试。

Result: 跨项目不稳定性影响55%的OpenStack项目，显著增加审查时间和计算成本。令人惊讶的是，70%的单元测试表现出跨项目不稳定性，挑战了单元测试天生隔离于跨模块问题的假设。定性分析显示，CI中的竞争条件、不一致的构建配置和依赖不匹配是不一致不稳定性的主要原因。

Conclusion: 研究结果强调了在复杂生态系统中需要更好的跨项目协调、标准化的CI配置和改进的测试隔离策略。测试不稳定性不仅是单个项目问题，而是影响整个生态系统的系统性挑战。

Abstract: Automated regression testing is a cornerstone of modern software development, often contributing directly to code review and Continuous Integration (CI). Yet some tests suffer from flakiness, where their outcomes vary non-deterministically. Flakiness erodes developer trust in test results, wastes computational resources, and undermines CI reliability. While prior research has examined test flakiness within individual projects, its broader ecosystem-wide impact remains largely unexplored. In this paper, we present an empirical study of test flakiness in the OpenStack ecosystem, which focuses on (1) cross-project flakiness, where flaky tests impact multiple projects, and (2) inconsistent flakiness, where a test exhibits flakiness in some projects but remains stable in others. By analyzing 649 OpenStack projects, we identify 1,535 cross-project flaky tests and 1,105 inconsistently flaky tests. We find that cross-project flakiness affects 55% of OpenStack projects and significantly increases both review time and computational costs. Surprisingly, 70% of unit tests exhibit cross-project flakiness, challenging the assumption that unit tests are inherently insulated from issues that span modules like integration and system-level tests. Through qualitative analysis, we observe that race conditions in CI, inconsistent build configurations, and dependency mismatches are the primary causes of inconsistent flakiness. These findings underline the need for better coordination across complex ecosystems, standardized CI configurations, and improved test isolation strategies.

</details>


### [7] [SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents](https://arxiv.org/abs/2602.09447)
*Zhirui Zhang,Hongbo Zhang,Haoxiang Fei,Zhiyuan Bao,Yubin Chen,Zhengyu Lei,Ziyue Liu,Yixuan Sun,Mingkun Xiao,Zihang Ye,Yu Zhang,Hongcheng Zhu,Yuxiang Wen,Heung-Yeung Shum*

Main category: cs.SE

TL;DR: SWE-AGI是一个开源基准测试，用于评估LLM基于明确规范自主构建生产级软件系统的能力，使用MoonBit语言，要求实现解析器、解释器等复杂系统，测试结果显示GPT-5.3-Codex表现最佳，但随着任务难度增加性能急剧下降。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在编码方面表现出色，但它们能否根据明确规范自主构建生产级软件系统仍是一个未解决的问题。需要评估LLM在端到端、规范驱动的软件系统构建中的实际能力。

Method: 引入SWE-AGI基准测试，使用MoonBit语言编写，要求LLM代理根据权威标准和RFC严格实现解析器、解释器、二进制解码器和SAT求解器，每个任务涉及1,000-10,000行核心逻辑代码，利用新兴的MoonBit生态系统最小化数据泄漏。

Result: GPT-5.3-Codex表现最佳（解决19/22任务，86.4%），优于Claude-Opus-4.6（15/22，68.2%），Kimi-2.5在开源模型中表现最强。随着任务难度增加，性能急剧下降，特别是在规范密集的困难系统上。行为分析显示，随着代码库规模扩大，代码阅读而非编写成为AI辅助开发的主要瓶颈。

Conclusion: 虽然规范驱动的自主软件工程越来越可行，但在能够可靠支持生产级开发之前，仍然存在重大挑战，特别是在处理大规模、复杂规范的系统时。

Abstract: Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.

</details>


### [8] [AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms](https://arxiv.org/abs/2602.09464)
*Haoyu Zhao,Ziran Yang,Jiawei Li,Deyuan He,Zenan Li,Chi Jin,Venugopal V. Veeravalli,Aarti Gupta,Sanjeev Arora*

Main category: cs.SE

TL;DR: AlgoVeri是一个评估AI模型在Dafny、Verus和Lean三种验证系统中生成形式化验证代码能力的基准测试，包含77个经典算法，揭示了不同验证系统间的性能差异和语言设计对AI模型能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的验证代码生成基准测试各自为政，只测试单个语言/工具（如Dafny、Verus、Lean），且任务差异很大，导致性能数据无法直接比较。缺乏一个统一的跨范式评估方法。

Method: 创建AlgoVeri基准测试，包含77个经典算法，在Dafny、Verus和Lean三种验证系统中使用相同的功能契约进行评估。通过强制使用相同的功能规范，揭示验证系统的关键能力差距。

Result: 前沿模型在Dafny中表现最佳（Gemini-3 Flash达40.3%），但在Verus中性能大幅下降（24.7%），在Lean中表现最差（7.8%）。Gemini-3能有效利用迭代修复提升性能（如在Dafny中通过率提高三倍），而GPT-OSS则早期饱和。语言设计影响细化轨迹：Dafny让模型专注于逻辑正确性，而Verus和Lean则让模型陷入持久的语法和语义障碍。

Conclusion: AlgoVeri为验证代码生成提供了首个统一的跨范式评估基准，揭示了不同验证系统对AI模型能力的显著影响，特别是系统级内存约束和显式证明构造带来的挑战。该基准测试有助于推动更健壮的验证代码生成研究。

Abstract: Vericoding refers to the generation of formally verified code from rigorous specifications. Recent AI models show promise in vericoding, but a unified methodology for cross-paradigm evaluation is lacking. Existing benchmarks test only individual languages/tools (e.g., Dafny, Verus, and Lean) and each covers very different tasks, so the performance numbers are not directly comparable. We address this gap with AlgoVeri, a benchmark that evaluates vericoding of $77$ classical algorithms in Dafny, Verus, and Lean. By enforcing identical functional contracts, AlgoVeri reveals critical capability gaps in verification systems. While frontier models achieve tractable success in Dafny ($40.3$% for Gemini-3 Flash), where high-level abstractions and SMT automation simplify the workflow, performance collapses under the systems-level memory constraints of Verus ($24.7$%) and the explicit proof construction required by Lean (7.8%). Beyond aggregate metrics, we uncover a sharp divergence in test-time compute dynamics: Gemini-3 effectively utilizes iterative repair to boost performance (e.g., tripling pass rates in Dafny), whereas GPT-OSS saturates early. Finally, our error analysis shows that language design affects the refinement trajectory: while Dafny allows models to focus on logical correctness, Verus and Lean trap models in persistent syntactic and semantic barriers. All data and evaluation code can be found at https://github.com/haoyuzhao123/algoveri.

</details>


### [9] [Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository](https://arxiv.org/abs/2602.09467)
*Sota Nakashima,Masanari Kondo,Mahmoud Alfadel,Aly Ahmad,Toshihiro Nakae,Hidenori Matsuzaki*

Main category: cs.SE

TL;DR: 首个研究将拒绝的贡献（如GitHub issue）与相关源代码建立可追溯链接，使用LLM驱动管道在Go仓库上验证，准确率0.836，平均精度0.643


<details>
  <summary>Details</summary>
Motivation: 开源项目中拒绝的贡献包含有价值的设计原理和决策标准，但现有研究主要关注已接受的贡献，缺乏对拒绝贡献的可追溯性链接研究

Method: 提出LLM驱动的管道方法，在Go官方仓库的GitHub issue数据集上，将拒绝的提案链接到相关源代码，并进行失败分析

Result: 管道在正确粒度选择上准确率0.836，在相应粒度下生成正确链接的平均精度0.643；失败案例中讨论冗余且缺乏具体实现信息

Conclusion: 首次建立了拒绝贡献与源代码的可追溯链接，验证了方法的可行性，但讨论质量影响链接生成，为软件决策知识挖掘提供了新途径

Abstract: Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).

</details>


### [10] [SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?](https://arxiv.org/abs/2602.09540)
*Muxin Tian,Zhe Wang,Blair Yang,Zhenwei Tang,Kunlun Zhu,Honghua Dong,Hanchen Li,Xinni Xie,Guangjing Wang,Jiaxuan You*

Main category: cs.SE

TL;DR: SWE-Bench Mobile是一个评估编码代理在真实iOS代码库上开发能力的基准测试，结果显示当前最佳配置仅达到12%任务成功率，表明现有代理与工业级移动应用开发需求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注孤立问题或bug修复，无法评估编码代理在真实工业开发环境中的能力，特别是面对多模态输入、大规模混合代码库和完整测试套件的复杂场景。

Method: 创建SWE-Bench Mobile基准测试，包含来自生产iOS代码库的真实软件工程任务，支持多模态输入（PRD和Figma设计），使用大规模混合Swift/Objective-C代码库和完整测试套件。评估了22种代理-模型配置，涵盖三种商业代理（Cursor、Codex、Claude Code）和一种开源代理（OpenCode）。

Result: 最佳配置仅达到12%任务成功率；发现代理设计重要性不亚于模型能力（相同模型在不同代理上性能差距可达6倍）；商业代理始终优于开源替代品；简单的"防御性编程"提示比复杂提示性能高7.4%。

Conclusion: 当前编码代理能力与工业级移动应用开发需求存在显著差距，但研究为从业者和研究者提供了可操作的见解。通过发布托管基准挑战来防止数据污染并确保公平评估。

Abstract: Can large language model agents develop industry-level mobile applications? We introduce \textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.

</details>


### [11] [Generative AI Adoption in an Energy Company: Exploring Challenges and Use Cases](https://arxiv.org/abs/2602.09846)
*Malik Abdul Sami,Zeeshan Rasheed,Meri Olenius,Muhammad Waseem,Kai-Kristian Kemell,Jussi Rasku,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 能源公司员工访谈研究，探讨AI采用理解及LLM代理工作流在日常工作中的潜在应用领域


<details>
  <summary>Details</summary>
Motivation: 组织正在探索生成式AI如何支持运营工作和决策过程，需要了解员工对AI采用的理解以及识别AI和LLM代理工作流可协助日常活动的领域

Method: 在能源公司进行为期四周的研究，通过16个半结构化访谈覆盖9个部门，辅以内部文件和研究者观察

Result: 识别出员工认为AI有用的领域：报告工作、预测、数据处理、维护相关任务和异常检测；参与者描述了如何通过增量步骤引入GenAI和LLM工具以符合现有工作流程

Conclusion: 研究提供了能源行业AI采用概况，为实际实施的切入点识别和跨行业比较研究提供了结构化基础

Abstract: Organisations are examining how generative AI can support their operational work and decision-making processes. This study investigates how employees in a energy company understand AI adoption and identify areas where AI and LLMs-based agentic workflows could assist daily activities. Data was collected in four weeks through sixteen semi-structured interviews across nine departments, supported by internal documents and researcher observations. The analysis identified areas where employees positioned AI as useful, including reporting work, forecasting, data handling, maintenance-related tasks, and anomaly detection. Participants also described how GenAI and LLM-based tools could be introduced through incremental steps that align with existing workflows. The study provides an overview view of AI adoption in the energy sector and offers a structured basis for identifying entry points for practical implementation and comparative research across industries.

</details>


### [12] [Immersion in the GitHub Universe: Scaling Coding Agents to Mastery](https://arxiv.org/abs/2602.09892)
*Jiale Zhao,Guoxin Chen,Fanzhe Meng,Minghao Li,Jie Chen,Hui Xu,Yongshuai Sun,Xin Zhao,Ruihua Song,Yuan Zhang,Peng Wang,Cheng Chen,Jirong Wen,Kai Jia*

Main category: cs.SE

TL;DR: ScaleSWE：一个自动化、沙箱化的多智能体工作流，用于大规模构建高质量软件工程数据，从600万个PR中生成10万个已验证实例，是目前最大的真实世界SWE数据集


<details>
  <summary>Details</summary>
Motivation: 现实世界软件工程任务的掌握受到大规模高质量训练数据稀缺的根本性瓶颈限制，现有数据扩展受到环境设置、单元测试生成和问题描述构建复杂性的限制

Method: 提出ScaleSWE系统，采用自动化沙箱化多智能体工作流，协调三个专门智能体：环境设置、测试创建和问题描述合成，处理5200个仓库中的600万个拉取请求

Result: 生成Scale SWE Data：10万个已验证的SWE实例，是目前最大的此类数据集；通过微调Qwen30BA3BInstruct创建ScaleSWE Agent，在SWE Bench Verified上达到64%解决率，比基础模型提升近三倍

Conclusion: ScaleSWE提供了一个可扩展、可复现的数据构建方法，用于推进基于LLM的软件工程，该数据集将公开可用

Abstract: Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.

</details>


### [13] [Operationalizing Human Values in the Requirements Engineering Process of Ethics-Aware Autonomous Systems](https://arxiv.org/abs/2602.09921)
*Everaldo Silva Júnior,Lina Marsso,Ricardo Caldas,Marsha Chechik,Genaína Nunes Rodrigues*

Main category: cs.SE

TL;DR: 提出一种需求工程方法，将人类价值作为规范性目标与功能和适应目标对齐，并系统化为SLEEC需求，支持自动化检查和冲突检测


<details>
  <summary>Details</summary>
Motivation: 人类价值具有模糊性、多元性和情境依赖性，难以与功能和适应需求协同操作，需要明确表示来支持价值冲突的识别、分析和协商

Method: 提出一种面向伦理感知自主系统的需求工程方法，将人类价值捕获为规范性目标，与功能和适应目标对齐，并系统化为社会、法律、伦理、共情和文化(SLEEC)需求，支持自动化良好性检查、冲突检测和早期设计时协商

Result: 通过医疗体感网络案例研究证明了该方法的可行性，能够实现自动化良好性检查、冲突检测和早期设计协商

Conclusion: 该方法能够有效操作化人类价值，超越传统软件工程抽象，为伦理感知自主系统的需求工程提供了可行方案

Abstract: Operationalizing human values alongside functional and adaptation requirements remains challenging due to their ambiguous, pluralistic, and context-dependent nature. Explicit representations are needed to support the elicitation, analysis, and negotiation of value conflicts beyond traditional software engineering abstractions. In this work, we propose a requirements engineering approach for ethics-aware autonomous systems that captures human values as normative goals and aligns them with functional and adaptation goals. These goals are systematically operationalized into Social, Legal, Ethical, Empathetic, and Cultural (SLEEC) requirements, enabling automated well-formedness checking, conflict detection, and early design-time negotiation. We demonstrate the feasibility of the approach through a medical Body Sensor Network case study.

</details>


### [14] [JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)](https://arxiv.org/abs/2602.09930)
*Nishil Amin,Zhiwei Fei,Xiang Li,Justyna Petke,He Ye*

Main category: cs.SE

TL;DR: 构建了JMigBench基准测试评估LLM在Java 8到11代码迁移任务中的表现，发现Mistral Codestral能处理简单API替换但难以应对复杂迁移场景


<details>
  <summary>Details</summary>
Motivation: 需要评估大型语言模型在源代码迁移任务中的实际能力，特别是Java版本升级场景，为开发者提供自动化迁移工具的有效性评估

Method: 1) 从开源仓库收集函数对数据集；2) 构建包含8类废弃API的精细化数据集；3) 使用Mistral Codestral模型评估；4) 采用CodeBLEU和基于关键词的指标衡量词汇、语义相似度和迁移正确性

Result: Mistral Codestral能中等程度处理简单的一对一API替换（11.11%案例实现完全相同的迁移），但在CORBA、JAX-WS等复杂迁移场景表现不佳

Conclusion: 该模型能部分减少开发者的重复性迁移工作，但尚不能完全替代人工；基准测试为未来扩展数据集、优化提示策略和改进LLM迁移性能提供了基础

Abstract: We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.

</details>


### [15] [QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs](https://arxiv.org/abs/2602.09942)
*Junjie Luo,Shangzhou Xia,Fuyuan Zhang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出量子EMI测试方法，通过生成包含死代码的量子程序并移除死代码创建变体，检测量子软件栈中的错误


<details>
  <summary>Details</summary>
Motivation: 量子软件栈测试面临oracle问题（缺乏可靠的真值基准），现有方法主要依赖等效电路变换、后端修改或参数调优，需要新的测试技术

Method: 基于EMI思想提出量子EMI方法：1）生成包含死代码的随机量子程序；2）通过移除死代码创建程序变体；3）比较变体行为检测错误

Result: 在Qiskit、Q#和Cirq三个量子软件栈中成功发现11个崩溃错误和1个行为不一致错误

Conclusion: QEMI扩展了量子软件栈测试技术集，超越了结构变换，将语义保持变换纳入量子程序分析

Abstract: As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.

</details>


### [16] [Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents](https://arxiv.org/abs/2602.09944)
*Xiang Li,Zhiwei Fei,Ying Ma,Jerry Zhang,Sarro Federica,He Ye*

Main category: cs.SE

TL;DR: 论文指出当前代码迁移研究主要关注代码本身，而忽视了环境交互的自动化，这导致迁移效率低下。作者提出需要将代码迁移与环境构建紧密结合的框架。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统需要持续升级代码以增强功能、安全性和性能，LLM在代码迁移任务中表现出色。然而，当前研究主要集中在代码迁移本身（重构、API适配、依赖更新），而对必须伴随的环境交互自动化探索不足。代码与环境紧密交织，仅依赖静态环境分析会导致对目标环境理解不足、反馈周期延长，从而造成大量返工和项目延迟，降低整体效率。

Method: 首先概述了自动化环境构建的现状，然后提出了一个将自动化环境设置与代码迁移工作流紧密结合的新框架范式，最后探讨了代码迁移领域中自动化环境交互的挑战和未来方向。

Result: 研究发现，没有自动化环境交互，代码迁移的自动化只完成了一半。成功的软件演进需要采用整合代码和环境迁移的整体视角。

Conclusion: 论文强调软件演进需要代码迁移与环境交互自动化的紧密结合，提出了相应的框架范式，并指出了该领域未来的研究方向和挑战。

Abstract: Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.

</details>


### [17] [Artisan: Agentic Artifact Evaluation](https://arxiv.org/abs/2602.10046)
*Doehyun Baek,Michael Pradel*

Main category: cs.SE

TL;DR: Artisan：一个基于LLM的自动化代理，用于生成可复现研究结果的脚本，相比基线提升3.14倍效果，成本仅0.45美元/任务


<details>
  <summary>Details</summary>
Motivation: 当前软件工程领域的工件评估依赖人工，耗时费力且只能覆盖部分论文，需要自动化工具来支持研究结果的可复现性验证

Method: 将复现问题构建为代码生成任务，设计自动化判断机制防止简单复制结果，创建Artisan-Bench基准包含60个软件工程任务

Result: Artisan在60个任务中成功生成44个复现脚本，比最佳基线(mini-swe-agent)提升3.14倍，平均成本0.45美元/48分钟，发现20个新错误

Conclusion: Artisan证明了自动化研究结果复现的可行性，显著提高效率并降低成本，有助于发现论文或工件中的隐藏错误

Abstract: Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System](https://arxiv.org/abs/2602.09174)
*Marzieh Barkhordar,Alireza Tabatabaeian,Mohammad Sadrosadati,Christina Giannoula,Juan Gomez Luna,Izzat El Hajj,Onur Mutlu,Alaa R. Alameldeen*

Main category: cs.DC

TL;DR: 在真实PIM系统上实现图算法，分析性能瓶颈并与CPU/GPU对比，为未来PIM硬件设计提供指导


<details>
  <summary>Details</summary>
Motivation: 传统CPU/GPU架构处理大规模图数据时面临数据移动瓶颈，导致性能受限和能效低下。虽然已有PIM方案提出，但缺乏对真实PIM系统的实际评估。

Method: 在UPMEM通用PIM架构上实现代表性图算法，分析性能特征和瓶颈，与CPU/GPU基准对比，并推导硬件设计洞察

Result: 研究发现选择最优的数据分区策略对PIM性能至关重要，同时识别出现有PIM架构的关键硬件限制，包括计算、内存和通信子系统

Conclusion: 未来PIM硬件需要在指令级并行性、非阻塞DMA引擎和PIM核心间直接互连网络等方面进行改进，以减少数据传输开销

Abstract: Processing large-scale graph datasets is computationally intensive and time-consuming. Processor-centric CPU and GPU architectures, commonly used for graph applications, often face bottlenecks caused by extensive data movement between the processor and memory units due to low data reuse. As a result, these applications are often memory-bound, limiting both performance and energy efficiency due to excessive data transfers. Processing-In-Memory (PIM) offers a promising approach to mitigate data movement bottlenecks by integrating computation directly within or near memory. Although several previous studies have introduced custom PIM proposals for graph processing, they do not leverage real-world PIM systems.
  This work aims to explore the capabilities and characteristics of common graph algorithms on a real-world PIM system to accelerate data-intensive graph workloads. To this end, we (1) implement representative graph algorithms on UPMEM's general-purpose PIM architecture; (2) characterize their performance and identify key bottlenecks; (3) compare results against CPU and GPU baselines; and (4) derive insights to guide future PIM hardware design.
  Our study underscores the importance of selecting optimal data partitioning strategies across PIM cores to maximize performance. Additionally, we identify critical hardware limitations in current PIM architectures and emphasize the need for future enhancements across computation, memory, and communication subsystems. Key opportunities for improvement include increasing instruction-level parallelism, developing improved DMA engines with non-blocking capabilities, and enabling direct interconnection networks among PIM cores to reduce data transfer overheads.

</details>


### [19] [LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms](https://arxiv.org/abs/2602.09323)
*Jie Kong,Wei Wang,Jiehan Zhou,Chen Yu*

Main category: cs.DC

TL;DR: LLM-CoOpt是一个算法-硬件协同设计框架，通过KV缓存优化、分组查询注意力改进和分页注意力三种策略，提升LLM推理的吞吐量和降低延迟，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理中的内存带宽瓶颈、计算冗余和长序列处理效率低下的问题，为实际应用提供高性能优化方案。

Method: 提出三合一优化策略：1) Opt-KV优化KV缓存读写路径并引入FP8量化；2) Opt-GQA将多头自注意力重构为分组查询注意力；3) Opt-Pa采用两步法处理长序列，先分块再懒加载。

Result: 在LLaMa-13B-GPTQ模型上，推理吞吐量提升最高13.43%，延迟降低最高16.79%，同时保持模型精度。

Conclusion: LLM-CoOpt为大规模语言模型的实际推理提供了一条实用、高性能的优化路径，有效解决了内存和计算效率问题。

Abstract: Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.

</details>


### [20] [The Coordination Criterion](https://arxiv.org/abs/2602.09435)
*Joseph M. Hellerstein*

Main category: cs.DC

TL;DR: 论文提出了一个协调准则，用于判断分布式规范何时本质上需要协调，而非特定协议或实现策略强加的。该准则表明：在异步消息传递模型中，规范允许无协调实现当且仅当它在适当可观察结果顺序下相对于历史扩展是单调的。


<details>
  <summary>Details</summary>
Motivation: 研究分布式系统中协调的本质需求，区分规范本身固有的协调需求与特定实现策略引入的协调，为理解何时协调不可避免提供理论基础。

Method: 在异步消息传递模型中，基于Lamport历史（偏序执行下的happens-before关系）和规范定义的可观察结果，提出协调准则：规范允许无协调实现当且仅当它在历史扩展顺序下是单调的。

Result: 建立了协调的清晰边界，统一解释了CAP不可能性、CALM无协调性、一致性协议、快照任务、事务隔离级别和不变式汇合等一系列经典结果，表明它们都是同一底层语义现象的不同实例。

Conclusion: 协调准则为分布式规范是否需要协调提供了通用判断标准，基于最小假设，不依赖特定编程语言、对象实现或协议结构，揭示了协调需求的本质语义特性。

Abstract: When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.
  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.

</details>


### [21] [It's not a lie if you don't get caught: simplifying reconfiguration in SMR through dirty logs](https://arxiv.org/abs/2602.09441)
*Allen Clement,Natacha Crooks,Neil Giridharan,Alex Shamis*

Main category: cs.DC

TL;DR: Gauss是一个重新配置引擎，将共识协议视为可互换模块，允许独立升级成员资格、故障阈值和共识协议本身，最小化全局停机时间。


<details>
  <summary>Details</summary>
Motivation: 现有的生产状态机复制实现是复杂的多层架构，现有研究共识协议很少讨论重新配置，即使讨论也将成员变更与特定算法紧密耦合，这阻碍了独立升级单个构建块，并在过渡到新协议实现时导致昂贵的停机时间。

Method: 通过引入共识协议内部日志和暴露给RSM节点的净化外部日志之间的区别，Gauss允许工程师独立升级成员资格、故障阈值和共识协议本身。

Result: 在Rialo区块链上的初步评估表明，这种关注点分离使得SMR堆栈能够在多种不同协议实现之间无缝演进。

Conclusion: 模块化对于生产部署中的可维护性和系统演进至关重要，Gauss通过将共识协议视为可互换模块，实现了SMR堆栈的独立升级和最小化停机时间。

Abstract: Production state-machine replication (SMR) implementations are complex, multi-layered architectures comprising data dissemination, ordering, execution, and reconfiguration components. Existing research consensus protocols rarely discuss reconfiguration. Those that do tightly couple membership changes to a specific algorithm. This prevents the independent upgrade of individual building blocks and forces expensive downtime when transitioning to new protocol implementations. Instead, modularity is essential for maintainability and system evolution in production deployments. We present Gauss, a reconfiguration engine designed to treat consensus protocols as interchangeable modules. By introducing a distinction between a consensus protocol's inner log and a sanitized outer log exposed to the RSM node, Gauss allows engineers to upgrade membership, failure thresholds, and the consensus protocol itself independently and with minimal global downtime. Our initial evaluation on the Rialo blockchain shows that this separation of concerns enables a seamless evolution of the SMR stack across a sequence of diverse protocol implementations.

</details>


### [22] [High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors](https://arxiv.org/abs/2602.09604)
*Ruimin Shi,Gabin Schieffer,Pei-Hung Lin,Maya Gokhale,Andreas Herten,Ivy Peng*

Main category: cs.DC

TL;DR: 该论文研究如何在ARM SVE和RISC-V RVV等支持可变向量长度的新兴架构上实现量子态向量模拟的高性能可移植性，提出了VLA设计和优化技术，在三种ARM处理器上实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: 量子态向量模拟是量子计算的重要工作负载，而ARM SVE和RISC-V RVV是新兴的支持可变向量长度的高端处理器架构。研究目标是探索在这些向量长度无关（VLA）架构上能否实现高性能的可移植性。

Method: 提出了VLA设计和关键优化技术：VLEN自适应的内存布局调整、加载缓冲、细粒度循环控制、基于门融合的算术强度适配。在Google的Qsim中实现，并在三种ARM处理器（NVIDIA Grace、AWS Graviton3、Fujitsu A64FX）上评估了最多36个量子比特的五个量子电路。

Result: 定义了新的指标和PMU事件来量化向量化活动，为未来VLA设计提供了通用见解。单源VLA量子模拟实现：在A64FX上达到4.5倍加速，在Grace上达到2.5倍加速，在Graviton上达到1.5倍加速。

Conclusion: 研究表明在向量长度无关架构上可以实现量子态向量模拟的高性能可移植性，提出的VLA设计和优化技术有效，为未来VLA架构设计提供了重要见解。

Abstract: ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.

</details>


### [23] [Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems](https://arxiv.org/abs/2602.09721)
*Guowei Liu,Hongming Li,Yaning Guo,Yongxi Lyu,Mo Zhou,Yi Liu,Zhaogeng Li,Yanpeng Wang*

Main category: cs.DC

TL;DR: AFD架构在特定硬件-模型组合下表现良好，但非通用解决方案，受限于带宽和负载不均衡问题


<details>
  <summary>Details</summary>
Motivation: 探索Attention-FFN解耦架构(AFD)与标准专家并行(EP)的性能边界，分析AFD在内存容量和带宽方面的潜在优势

Method: 将屋顶线模型扩展到通信层面，关联互连带宽、算术强度和硬件浮点运算利用率(HFU)，系统分析AFD架构

Result: 发现标准集群存在"死区"：增加FFN实例数无法提升HFU；AFD的离散节点级扩展比EP的连续批次调整产生更高不均衡惩罚；但在Superpod级硬件和粗粒度专家模型下AFD表现更好

Conclusion: AFD是特定硬件-模型组合下的有前景方法，而非通用解决方案，需要充足互连带宽和合适模型特性才能发挥优势

Abstract: Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.

</details>


### [24] [Efficient Remote Prefix Fetching with GPU-native Media ASICs](https://arxiv.org/abs/2602.09725)
*Liang Mi,Weijun Wang,Jinghan Chen,Ting Cao,Haipeng Dai,Yunxin Liu*

Main category: cs.DC

TL;DR: KVFetcher：利用GPU原生视频编解码器实现高效的远程KV缓存重用，在带宽受限场景下显著提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 现有远程KV缓存重用方案在高速网络中表现良好，但在带宽受限场景下性能显著下降。虽然已有研究通过压缩传输KV缓存来应对，但重量级的解压缩过程抵消了KV重用的优势。

Method: 提出KVFetcher系统，采用两种关键技术：1) 编解码友好的张量布局，将KV缓存压缩为高度紧凑的视频格式；2) 高效的KV提取器，以流水线方式协调压缩KV缓存的传输、解码和恢复，消除资源争用并掩盖网络波动。

Result: 在高、中、低端GPU上原型实现，实验表明相比最先进方法，KVFetcher将首次令牌时间(TTFT)最多降低3.51倍，同时保持无损精度。

Conclusion: KVFetcher提供了一种高效且广泛可部署的远程KV缓存重用解决方案，利用GPU原生视频编解码器在带宽受限场景下显著加速LLM推理。

Abstract: Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [25] [Scaling GraphLLM with Bilevel-Optimized Sparse Querying](https://arxiv.org/abs/2602.09038)
*Yangzhe Peng,Haiquan Qiu,Quanming Yao,Kun He*

Main category: cs.DB

TL;DR: BOSQ框架通过双层优化稀疏查询策略，选择性调用LLM生成解释特征，大幅降低计算成本，在文本属性图的节点级任务上实现数量级加速并保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的文本属性图节点任务方法存在严重计算成本和金钱成本问题，例如在中等规模数据集上生成所有节点的解释特征需要数天时间，限制了实际应用。

Method: 提出BOSQ框架，采用自适应稀疏查询策略，通过双层优化选择性地决定何时调用LLM，避免冗余或低增益查询，显著减少计算开销。

Result: 在六个真实世界文本属性图数据集上的实验表明，BOSQ相比现有GraphLLM方法实现了数量级的加速，同时在节点级任务上保持相当或更优的性能。

Conclusion: BOSQ框架有效解决了LLM在文本属性图节点任务中的计算效率问题，为实际应用提供了可行的解决方案。

Abstract: LLMs have recently shown strong potential in enhancing node-level tasks on text-attributed graphs (TAGs) by providing explanation features. However, their practical use is severely limited by the high computational and monetary cost of repeated LLM queries. To illustrate, naively generating explanations for all nodes on a medium-sized benchmark like Photo (48k nodes) using a representative method (e.g., TAPE) would consume days of processing time. In this paper, we propose Bilevel-Optimized Sparse Querying (BOSQ), a general framework that selectively leverages LLM-derived explanation features to enhance performance on node-level tasks on TAGs. We design an adaptive sparse querying strategy that selectively decides when to invoke LLMs, avoiding redundant or low-gain queries and significantly reducing computation overhead. Extensive experiments on six real-world TAG datasets involving two types of node-level tasks demonstrate that BOSQ achieves orders of magnitude speedups over existing GraphLLM methods while consistently delivering on-par or superior performance.

</details>


### [26] [Efficient Distance Pruning for Process Suffix Comparison in Prescriptive Process Monitoring](https://arxiv.org/abs/2602.09039)
*Sarra Madad*

Main category: cs.DB

TL;DR: 提出一种基于三角不等式的有效检索方法，通过优化枢轴的距离边界来剪枝冗余比较，显著降低计算成本，同时保持检索结果的准确性。


<details>
  <summary>Details</summary>
Motivation: 规范过程监控需要推荐改善过程结果的行动，但大规模后缀比较的计算成本很高，随着日志大小的增长而迅速增加，这成为关键障碍。

Method: 利用三角不等式原理，通过到一组优化枢轴的距离定义边界，剪枝冗余比较。该方法完全可并行化，且剪枝是精确的。

Result: 该方法显著减少了运行时间，检索到的后缀与穷举比较的结果完全相同，保持了准确性。证明基于度量的剪枝可以加速后缀比较。

Conclusion: 基于度量的剪枝方法能够加速后缀比较，支持可扩展的规范过程监控系统，解决了大规模后缀比较的计算效率问题。

Abstract: Prescriptive process monitoring seeks to recommend actions that improve process outcomes by analyzing possible continuations of ongoing cases. A key obstacle is the heavy computational cost of large-scale suffix comparisons, which grows rapidly with log size. We propose an efficient retrieval method exploiting the triangle inequality: distances to a set of optimized pivots define bounds that prune redundant comparisons. This substantially reduces runtime and is fully parallelizable. Crucially, pruning is exact: the retrieved suffixes are identical to those from exhaustive comparison, thereby preserving accuracy. These results show that metric-based pruning can accelerate suffix comparison and support scalable prescriptive systems.

</details>


### [27] [SciDataCopilot: An Agentic Data Preparation Framework for AGI-driven Scientific Discovery](https://arxiv.org/abs/2602.09132)
*Jiyong Rao,Yicheng Qiu,Jiahui Zhang,Juntao Deng,Shangquan Sun,Fenghua Ling,Hao Chen,Nanqing Dong,Zhangyang Gao,Siqi Sun,Yuqiang Li,Dongzhan Zhou,Guangyu Wang,Lijun Wu,Conghui He,Xuhong Wang,Jing Shao,Xiang Liu,Yu Zhu,Mianxin Liu,Qihao Zheng,Yinghui Zhang,Jiamin Wu,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Bo Zhang,Wanli Ouyang,Runkai Zhao,Chunfeng Song,Lei Bai,Chi Zhang*

Main category: cs.DB

TL;DR: 提出SciDataCopilot框架，将AI-Ready概念扩展到科学数据领域，解决原始实验数据异构性高、难以与AI系统对接的问题，实现端到端的数据准备自动化。


<details>
  <summary>Details</summary>
Motivation: 当前AI for Science主要基于文本语料，但原始实验数据存在极端异构性、高特异性、需要深度领域知识等问题，缺乏与语言表示的语义对齐和统一嵌入空间的结构同质性，阻碍了AGI4S与实验物理现实的对接。

Method: 提出Scientific AI-Ready数据范式，将科学数据在计算工作流中的规范、结构和组合方式形式化。实现SciDataCopilot自主代理框架，处理数据摄取、科学意图解析和多模态集成，以端到端方式运作。

Result: 在三个异构科学领域的广泛评估显示，SciDataCopilot相比手动流程提高了效率、可扩展性和一致性，数据准备速度提升高达30倍。

Conclusion: 通过将数据准备就绪性作为核心操作原语，该框架为可重用、可转移的系统提供了原则性基础，实现了向实验驱动的科学通用智能的过渡。

Abstract: The current landscape of AI for Science (AI4S) is predominantly anchored in large-scale textual corpora, where generative AI systems excel at hypothesis generation, literature search, and multi-modal reasoning. However, a critical bottleneck for accelerating closed-loop scientific discovery remains the utilization of raw experimental data. Characterized by extreme heterogeneity, high specificity, and deep domain expertise requirements, raw data possess neither direct semantic alignment with linguistic representations nor structural homogeneity suitable for a unified embedding space. The disconnect prevents the emerging class of Artificial General Intelligence for Science (AGI4S) from effectively interfacing with the physical reality of experimentation. In this work, we extend the text-centric AI-Ready concept to Scientific AI-Ready data paradigm, explicitly formalizing how scientific data is specified, structured, and composed within a computational workflow. To operationalize this idea, we propose SciDataCopilot, an autonomous agentic framework designed to handle data ingestion, scientific intent parsing, and multi-modal integration in a end-to-end manner. By positioning data readiness as a core operational primitive, the framework provides a principled foundation for reusable, transferable systems, enabling the transition toward experiment-driven scientific general intelligence. Extensive evaluations across three heterogeneous scientific domains show that SciDataCopilot improves efficiency, scalability, and consistency over manual pipelines, with up to 30$\times$ speedup in data preparation.

</details>


### [28] [Predictive Query Language: A Domain-Specific Language for Predictive Modeling on Relational Databases](https://arxiv.org/abs/2602.09572)
*Vid Kocijan,Jinu Sunil,Jan Eric Lenssen,Viman Deb,Xinwei Xe,Federco Reyes Gomez,Matthias Fey,Jure Leskovec*

Main category: cs.DB

TL;DR: 提出Predictive Query Language (PQL)，一种类似SQL的声明式语言，用于在关系数据库上定义预测任务，自动生成机器学习训练标签。


<details>
  <summary>Details</summary>
Motivation: 关系数据上的预测建模需要从数据库中手动提取训练样本（预测实体和目标标签），这个过程缓慢、费力且容易出错。

Method: 设计PQL语言，允许通过单个声明式查询指定预测任务，自动计算各种机器学习任务（回归、分类、时间序列预测、推荐系统）的训练标签。

Result: PQL已成功集成到预测AI平台中，在金融欺诈、商品推荐、工作负载预测等多个用例中得到应用，并实现了两种实现方案：小规模低延迟和大规模数据库处理。

Conclusion: PQL提供了一种高效、自动化的方式来定义关系数据库上的预测任务，解决了手动提取训练数据的痛点，具有广泛的适用性。

Abstract: The purpose of predictive modeling on relational data is to predict future or missing values in a relational database, for example, future purchases of a user, risk of readmission of the patient, or the likelihood that a financial transaction is fraudulent. Typically powered by machine learning methods, predictive models are used in recommendations, financial fraud detection, supply chain optimization, and other systems, providing billions of predictions every day. However, training a machine learning model requires manual work to extract the required training examples - prediction entities and target labels - from the database, which is slow, laborious, and prone to mistakes. Here, we present the Predictive Query Language (PQL), a SQL-inspired declarative language for defining predictive tasks on relational databases. PQL allows specifying a predictive task in a single declarative query, enabling the automatic computation training labels for a large variety of machine learning tasks, such as regression, classification, time-series forecasting, and recommender systems. PQL is already successfully integrated and used in a collection of use cases as part of a predictive AI platform. The versatility of the language can be demonstrated through its many ongoing use cases, including financial fraud, item recommendations, and workload prediction. We demonstrate its versatile design through two implementations; one for small-scale, low-latency use and one that can handle large-scale databases.

</details>


### [29] [Optimal Bounds-Only Pruning for Spatial AkNN Joins](https://arxiv.org/abs/2602.10027)
*Dominik Winecki*

Main category: cs.DB

TL;DR: 提出了一种仅使用边界的三界邻近性测试方法，用于在分区空间数据集上进行精确欧几里得AkNN连接，通过更精确地捕捉方向语义来减少不必要的分区加载。


<details>
  <summary>Details</summary>
Motivation: 数据仓库通常对大型表进行分区并存储行组统计信息以加速搜索和连接，而不是维护索引。现有的剪枝方法对于仅边界空间数据过于保守，因为它们没有完全捕捉其方向语义，从而在连接的最早阶段错过了跳过不需要分区的机会。

Method: 提出了一种三界邻近性测试方法，用于确定分区内所有点是否在一个分区中有比另一个可能被遮挡的分区更近的邻居。该方法利用分区边界统计信息，在构建空间索引之前将连接评估本地化到少数分区。

Result: 该算法既是最优的又是高效的，能够更有效地跳过不需要的分区，从而减少数据加载和计算开销。

Conclusion: 提出的三界邻近性测试方法改进了分区空间数据集上的AkNN连接性能，通过更好地利用边界统计信息的方向语义，在连接早期阶段实现更精确的剪枝。

Abstract: We propose a bounds-only pruning test for exact Euclidean AkNN joins on partitioned spatial datasets. Data warehouses commonly partition large tables and store row group statistics for them to accelerate searches and joins, rather than maintaining indexes. AkNN joins can benefit from such statistics by constructing bounds and localizing join evaluations to a few partitions before loading them to build spatial indexes. Existing pruning methods are overly conservative for bounds-only spatial data because they do not fully capture its directional semantics, thereby missing opportunities to skip unneeded partitions at the earliest stages of a join. We propose a three-bound proximity test to determine whether all points within a partition have a closer neighbor in one partition than in another, potentially occluded partition. We show that our algorithm is both optimal and efficient.

</details>
