{"id": "2511.08607", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08607", "abs": "https://arxiv.org/abs/2511.08607", "authors": ["Yongxin Zhao", "Shenglin Zhang", "Yujia Wu", "Yuxin Sun", "Yongqian Sun", "Dan Pei", "Chetan Bansal", "Minghua Ma"], "title": "Triage in Software Engineering: A Systematic Review of Research and Practice", "comment": null, "summary": "As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.", "AI": {"tldr": "\u672c\u6587\u5bf92004\u5e74\u81f3\u4eca\u7684234\u7bc7\u8bba\u6587\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u6df1\u5165\u5206\u6790\u4e86\u8f6f\u4ef6\u7cfb\u7edf\u6545\u969c\u5206\u8bca\u7684\u57fa\u672c\u6982\u5ff5\u3001\u7cfb\u7edf\u67b6\u6784\u548c\u95ee\u9898\u9648\u8ff0\uff0c\u603b\u7ed3\u4e86\u5f00\u6e90\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u590d\u6742\u5ea6\u7684\u589e\u957f\uff0c\u6545\u969c\u5206\u8bca\u5df2\u6210\u4e3a\u7cfb\u7edf\u8fd0\u7ef4\u7684\u57fa\u672c\u6d41\u7a0b\u3002\u6d77\u91cf\u5f02\u6784\u6570\u636e\u4f7f\u5f97\u6709\u6548\u7684\u6545\u969c\u5206\u8bca\u5bf9\u4e8e\u7ef4\u62a4\u7cfb\u7edf\u53ef\u9760\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u5feb\u901f\u54cd\u5e94\u95ee\u9898\u53d8\u5f97\u4e0d\u53ef\u6216\u7f3a\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5b66\u672f\u548c\u5de5\u4e1a\u7814\u7a76\u7684\u4e0d\u540c\u76ee\u6807\uff0c\u5206\u6790\u5de5\u4e1a\u5b9e\u8df5\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc6\u522b\u9650\u5236\u6545\u969c\u5206\u8bca\u7cfb\u7edf\u5b9e\u9645\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\u3002\u603b\u7ed3\u5e7f\u6cdb\u91c7\u7528\u7684\u5f00\u6e90\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u8bc6\u522b\u4e86\u9650\u5236\u6545\u969c\u5206\u8bca\u7cfb\u7edf\u5b9e\u9645\u90e8\u7f72\u7684\u4e3b\u8981\u969c\u788d\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u8bca\u6709\u6548\u6027\u6d4b\u91cf\u89c6\u89d2\uff0c\u6240\u6709\u7efc\u8ff0\u8bba\u6587\u548c\u9879\u76ee\u53ef\u5728GitHub\u4ed3\u5e93\u83b7\u53d6\u3002", "conclusion": "\u672c\u6587\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9009\u62e9\u548c\u6027\u80fd\u8bc4\u4f30\u7684\u6307\u5bfc\uff0c\u5e76\u6307\u51fa\u4e86\u4fc3\u8fdb\u5b66\u672f\u521b\u65b0\u4e0e\u5de5\u4e1a\u5e94\u7528\u66f4\u7d27\u5bc6\u7ed3\u5408\u7684\u672a\u6765\u65b9\u5411\u548c\u65b0\u5174\u673a\u4f1a\u3002"}}
{"id": "2511.08644", "categories": ["cs.SE", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.08644", "abs": "https://arxiv.org/abs/2511.08644", "authors": ["Punit Kumar", "Asif Imran", "Tevfik Kosar"], "title": "Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis", "comment": null, "summary": "This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.", "AI": {"tldr": "\u6bd4\u8f83Pandas\u3001Polars\u548cDask\u4e09\u4e2aPython\u6570\u636e\u5904\u7406\u5e93\u5728\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u548c\u63a8\u7406\u7ba1\u9053\u4e2d\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u7814\u7a76\u8fd9\u4e9b\u5e93\u5728GPU\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u4ea4\u4e92\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u6587\u732e\u5728\u6570\u636e\u52a0\u8f7d\u3001\u9884\u5904\u7406\u548c\u6279\u6b21\u4f9b\u7ed9\u7b49\u5173\u952e\u9636\u6bb5\u7684\u6027\u80fd\u5206\u6790\u7a7a\u767d", "method": "\u6d4b\u91cf\u8fd0\u884c\u65f6\u3001\u5185\u5b58\u4f7f\u7528\u3001\u78c1\u76d8\u4f7f\u7528\u548c\u80fd\u8017\u7b49\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u4f7f\u7528\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5", "result": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u679c", "conclusion": "\u672a\u5728\u6458\u8981\u4e2d\u660e\u786e\u8bf4\u660e\u5177\u4f53\u7ed3\u8bba"}}
{"id": "2511.09000", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09000", "abs": "https://arxiv.org/abs/2511.09000", "authors": ["Jarin Tasnim", "Debasish Chakroborti", "Chanchal K. Roy", "Kevin A. Schneider"], "title": "An insight into the technical debt-fix trade off in software backporting", "comment": "10 Pages", "summary": "Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e8687\u4e2a\u4ed3\u5e93\u4e2d105,396\u4e2a\u56de\u79fb\u690d\u63d0\u4ea4\u7684\u6280\u672f\u503a\u52a1\uff0c\u53d1\u73b0\u7ea64.3%\u7684\u56de\u79fb\u690d\u5f15\u5165\u4e86\u65b0\u7684\u6280\u672f\u503a\u52a1\uff0c\u4e0d\u540c\u751f\u6001\u7cfb\u7edf\u548c\u5f00\u53d1\u9636\u6bb5\u7684\u6280\u672f\u503a\u52a1\u6a21\u5f0f\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u56de\u79fb\u690d\u8fc7\u7a0b\u4e2d\u6280\u672f\u503a\u52a1\u7684\u4ea7\u751f\uff0c\u8bc6\u522b\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u5728\u7a33\u5b9a\u6e90\u4ee3\u7801\u7684\u56de\u79fb\u690d\u8fc7\u7a0b\u4e2d\u4f1a\u4ea7\u751f\u65b0\u7684\u6280\u672f\u503a\u52a1\u3002", "method": "\u5206\u6790\u4e86\u6765\u81eaApache\u3001Eclipse\u548cPython\u4e09\u4e2a\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d87\u4e2a\u4ed3\u5e93\u768431,076\u4e2a\u56de\u79fb\u690d\u6e90\u5934\u7684105,396\u4e2a\u63d0\u4ea4\u3002", "result": "\u7ea64.3%\u7684\u56de\u79fb\u690d\u5f15\u5165\u4e86\u65b0\u7684\u6280\u672f\u503a\u52a1\uff1bApache\u8d21\u732e\u4e86\u6700\u591a\u7684\u7edd\u5bf9\u5b9e\u4f8b\uff0c\u800cPython\u548cEclipse\u7684\u503a\u52a1\u4e0e\u63d0\u4ea4\u6bd4\u7387\u51e0\u4e4e\u662fApache\u7684\u4e09\u500d\uff1b\u4e0d\u540c\u751f\u6001\u7cfb\u7edf\u5728\u4e0d\u540c\u53d1\u5e03\u9636\u6bb5\u7684\u6280\u672f\u503a\u52a1\u6a21\u5f0f\u4e0d\u540c\u3002", "conclusion": "\u56de\u79fb\u690d\u8fc7\u7a0b\u4e2d\u786e\u5b9e\u4f1a\u4ea7\u751f\u6280\u672f\u503a\u52a1\uff0c\u7279\u522b\u662f\u5728\u7ecf\u9a8c\u4e0d\u8db3\u3001\u5de5\u4f5c\u8d1f\u8377\u9ad8\u6216\u975e\u6240\u6709\u8005\u7684\u5f00\u53d1\u8005\u8fdb\u884c\u56de\u79fb\u690d\u65f6\u66f4\u5bb9\u6613\u5f15\u5165\u6280\u672f\u503a\u52a1\u3002"}}
{"id": "2511.09038", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09038", "abs": "https://arxiv.org/abs/2511.09038", "authors": ["Oussama Jebbar", "Ferhat Khendek", "Maria Toeroe"], "title": "Test Plan Generation for Live Testing of Cloud Services", "comment": null, "summary": "Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u6d4b\u8bd5\u8ba1\u5212\u751f\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u51cf\u5c11\u751f\u4ea7\u73af\u5883\u4e2d\u6d4b\u8bd5\u6d3b\u52a8\u53ef\u80fd\u5f15\u8d77\u7684\u670d\u52a1\u4e2d\u65ad", "motivation": "\u624b\u52a8\u8bbe\u8ba1\u6d4b\u8bd5\u8ba1\u5212\u7e41\u7410\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u7279\u522b\u662f\u5728\u5927\u578b\u590d\u6742\u7cfb\u7edf\u4e2d\u66f4\u4e3a\u56f0\u96be\u3002\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6d4b\u8bd5\u9700\u8981\u5728\u4e0d\u5bf9\u751f\u4ea7\u6d41\u91cf\u9020\u6210\u4e0d\u53ef\u63a5\u53d7\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c", "method": "\u63d0\u51fa\u81ea\u52a8\u5316\u6d4b\u8bd5\u8ba1\u5212\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u62ec\u6d4b\u8bd5\u914d\u7f6e\u9009\u62e9/\u751f\u6210\u3001\u6d4b\u8bd5\u914d\u7f6e\u90e8\u7f72\u89c4\u5212\u3001\u521b\u5efa\u6d4b\u8bd5\u8fd0\u884c\u8ba1\u5212\u3001\u9009\u62e9\u51cf\u5c11\u5e72\u6270\u98ce\u9669\u7684\u7b56\u7565\u7b49", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u7684\u4e0d\u540c\u65b9\u9762", "conclusion": "\u81ea\u52a8\u5316\u6d4b\u8bd5\u8ba1\u5212\u751f\u6210\u53ef\u4ee5\u51cf\u5c11\u751f\u4ea7\u73af\u5883\u4e2d\u6d4b\u8bd5\u6d3b\u52a8\u5f15\u8d77\u7684\u670d\u52a1\u4e2d\u65ad"}}
{"id": "2511.08713", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.08713", "abs": "https://arxiv.org/abs/2511.08713", "authors": ["Gabriel Rodriguez-Canal", "David Katz", "Nick Brown"], "title": "An MLIR pipeline for offloading Fortran to FPGAs via OpenMP", "comment": "Author accepted version of paper published in SC25 LLVM workshop", "summary": "With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.", "AI": {"tldr": "\u9996\u4e2a\u901a\u8fc7MLIR\u4e2d\u7684OpenMP\u76ee\u6807\u6307\u4ee4\u5b9e\u73b0\u9009\u62e9\u6027\u4ee3\u7801\u5378\u8f7d\u5230FPGA\u7684\u5b9e\u73b0\uff0c\u7ed3\u5408MLIR OpenMP\u65b9\u8a00\u548cHLS\u65b9\u8a00\u63d0\u4f9b\u53ef\u79fb\u690d\u7684FPGA\u7f16\u8bd1\u6d41\u7a0b\u3002", "motivation": "\u968f\u7740\u6469\u5c14\u5b9a\u5f8b\u653e\u7f13\uff0cFPGA\u7b49\u5f02\u6784\u8ba1\u7b97\u5e73\u53f0\u5728\u52a0\u901fHPC\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684FPGA\u52a0\u901f\u65b9\u6848\u3002", "method": "\u5c06MLIR OpenMP\u65b9\u8a00\u4e0e\u9ad8\u7ea7\u7efc\u5408(HLS)\u65b9\u8a00\u7ed3\u5408\uff0c\u652f\u6301\u4efb\u4f55MLIR\u517c\u5bb9\u524d\u7aef(\u5982Flang)\uff0c\u5229\u7528\u73b0\u6709MLIR\u6784\u5efa\u6a21\u5757\u51cf\u5c11\u5f00\u53d1\u5de5\u4f5c\u91cf\u3002", "result": "\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6307\u4ee4\u7684FPGA\u52a0\u901f\uff0c\u652f\u6301\u901a\u8fc7\u6807\u51c6OpenMP\u6307\u4ee4\u624b\u52a8\u4f18\u5316\u5378\u8f7d\u5185\u6838\uff0c\u5c55\u793a\u4e86MLIR\u751f\u6001\u7cfb\u7edf\u7684\u53ef\u7ec4\u5408\u6027\u4f18\u52bf\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u4e8e\u6307\u4ee4\u7684FPGA\u52a0\u901f\u8def\u5f84\uff0c\u96c6\u6210\u5728MLIR\u751f\u6001\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2511.08826", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.08826", "abs": "https://arxiv.org/abs/2511.08826", "authors": ["Zonglin Guo", "Tony Givargis"], "title": "FlashMap: A Flash Optimized Key-Value Store", "comment": "6 pages, 2 figures, 3 tables", "summary": "Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.", "AI": {"tldr": "FlashMap\u662f\u4e00\u4e2a\u9488\u5bf9\u95ea\u5b58\u56fa\u6001\u786c\u76d8\u4f18\u5316\u7684\u9ad8\u6027\u80fd\u952e\u503c\u5b58\u50a8\u7cfb\u7edf\uff0c\u5728\u5355\u53f0\u6570\u636e\u4e2d\u5fc3\u7ea7\u670d\u52a1\u5668\u4e0a\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u541e\u5410\u91cf\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u73b0\u4ee3\u8ba1\u7b97\u5bf9\u54cd\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u9700\u6c42\u7684\u589e\u957f\uff0c\u952e\u503c\u5b58\u50a8\u5df2\u6210\u4e3a\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u7684\u5173\u952e\u7ec4\u4ef6\u3002\u95ea\u5b58\u56fa\u6001\u786c\u76d8\u7684\u666e\u53ca\u4e3a\u4f18\u5316\u952e\u503c\u5b58\u50a8\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u5f00\u53d1\u4e86FlashMap\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u95ea\u5b58\u56fa\u6001\u786c\u76d8\u4f18\u5316\u7684\u952e\u503c\u5b58\u50a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u9488\u5bf9SSD\u7279\u6027\u8fdb\u884c\u8bbe\u8ba1\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cFlashMap\u5728100\u5b57\u8282\u8d1f\u8f7d\u4e0b\u5e73\u5747\u5b9e\u73b0\u4e861980\u4e07\u6b21\u63d2\u5165\u548c2380\u4e07\u6b21\u968f\u673a\u67e5\u627e\u6bcf\u79d2\u7684\u541e\u5410\u91cf\u3002", "conclusion": "FlashMap\u8bc1\u660e\u4e86\u9488\u5bf9\u95ea\u5b58\u56fa\u6001\u786c\u76d8\u4e13\u95e8\u4f18\u5316\u7684\u952e\u503c\u5b58\u50a8\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6027\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09122", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09122", "abs": "https://arxiv.org/abs/2511.09122", "authors": ["Joschka Kersting", "Michael Rummel", "Gesa Benndorf"], "title": "Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation", "comment": null, "summary": "Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u9488\u5bf9\u5de5\u4e1aPLC\u7f16\u7a0b\u7684\u4f4e\u6570\u636e\u9886\u57df\u4ee3\u7801\u52a9\u624b\uff0c\u901a\u8fc7RAG\u3001\u591a\u6a21\u578b\u7ade\u4e89\u548c\u5373\u65f6\u7f16\u8bd1\u9a8c\u8bc1\uff0c\u65e0\u9700\u5fae\u8c03\u5927\u6a21\u578b\u5373\u53ef\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801", "motivation": "PLC\u4f7f\u7528\u4e13\u6709\u4ee3\u7801\u65b9\u8a00\uff0c\u96be\u4ee5\u8bad\u7ec3\u4ee3\u7801\u52a9\u624b\uff1b\u73b0\u6709LLM\u4e0d\u4e86\u89e3\u7279\u5b9a\u529f\u80fd\u5757\u548c\u9879\u76ee\u4ee3\u7801\uff1b\u4f01\u4e1a\u5bf9\u4e91\u670d\u52a1\u4e0d\u4fe1\u4efb\uff0c\u9700\u8981\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u3001\u591a\u6a21\u578b\u7ade\u4e89\u3001\u63a8\u7406\u548c\u81ea\u52a8\u7ea0\u9519\uff0c\u901a\u8fc7\u5373\u65f6\u7f16\u8bd1\u9a8c\u8bc1\u4ee3\u7801\u6709\u6548\u6027\uff0c\u5bf9\u5c0f\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u63d0\u793a\u5de5\u7a0b\u548c\u5b9a\u5411\u68c0\u7d22\uff0c\u5728\u4f4e\u6570\u636e\u9886\u57df\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u751f\u6210\uff0c\u63d0\u4f9b\u4e86\u4ee3\u7801\u7f16\u8bd1\u7edf\u8ba1\u548c\u7528\u6237\u8bc4\u5206\u8bc4\u4f30", "conclusion": "RAG\u652f\u6301\u7684\u4ee3\u7801\u52a9\u624b\u53ef\u4ee5\u5728\u4f4e\u6570\u636e\u9886\u57df\u5de5\u4f5c\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u5b9a\u5411\u68c0\u7d22\u5b9e\u73b0\u6709\u6548\u4ee3\u7801\u751f\u6210"}}
{"id": "2511.08936", "categories": ["cs.DC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.08936", "abs": "https://arxiv.org/abs/2511.08936", "authors": ["Liuzixuan Lin", "Andrew A. Chien"], "title": "Distribution and Management of Datacenter Load Decoupling", "comment": null, "summary": "The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.\n  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.\n  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.", "AI": {"tldr": "\u6570\u636e\u4e2d\u5fc3\u901a\u8fc7\u80fd\u6e90\u8d44\u6e90\u89e3\u8026\u7535\u529b\u5bb9\u91cf\u548c\u7535\u7f51\u8d1f\u8f7d\u6765\u521b\u5efa\u7075\u6d3b\u6027\uff0c\u4f18\u5316\u5206\u5e03\u548c\u7ba1\u7406\u53ef\u663e\u8457\u964d\u4f4e\u7535\u7f51\u78b3\u6392\u653e\uff0c\u7ecf\u6d4e\u4e0a\u53ef\u884c\u4f46\u9700\u8981\u7535\u7f51\u5e72\u9884\u3002", "motivation": "AI\u548c\u4e91\u6570\u636e\u4e2d\u5fc3\u7684\u9ad8\u80fd\u8017\u52a0\u5267\u4e86\u78b3\u8db3\u8ff9\u95ee\u9898\uff0c\u6570\u636e\u4e2d\u5fc3\u6052\u5b9a\u7535\u529b\u9700\u6c42\u4e0e\u6ce2\u52a8\u6027\u53ef\u518d\u751f\u80fd\u6e90\u7684\u77db\u76fe\u963b\u788d\u7535\u7f51\u8131\u78b3\uff0c\u9700\u8981\u6570\u636e\u4e2d\u5fc3\u7075\u6d3b\u6027\u6765\u6539\u5584\u53ef\u518d\u751f\u80fd\u6e90\u6d88\u7eb3\u3002", "method": "\u5b9a\u4e49\u548c\u8ba1\u7b97\u6570\u636e\u4e2d\u5fc3\u8d1f\u8f7d\u89e3\u8026\u7684\u529f\u7387\u548c\u80fd\u91cf\u9700\u6c42\uff0c\u8bc4\u4f30\u89e3\u8026\u8d44\u6e90\u7684\u5206\u5e03\u548c\u7ba1\u7406\u65b9\u6cd5\uff0c\u5305\u62ec\u7ad9\u70b9\u5dee\u5f02\u548c\u7535\u7f51\u5408\u4f5c\uff08\u5355\u5411\u4fe1\u606f\u5171\u4eab\u4e0e\u53cc\u5411\u5171\u4eab\u63a7\u5236\uff09\u3002", "result": "\u4f18\u5316\u5206\u5e03\u53ef\u5b9e\u73b098%\u7684\u6f5c\u5728\u7535\u7f51\u78b3\u51cf\u6392\uff0c\u4ec5\u970070%\u7684\u603b\u89e3\u8026\u9700\u6c42\uff1b\u7535\u7f51\u5408\u4f5c\u7ba1\u7406\u4f7f\u78b3\u51cf\u6392\u6548\u679c\u63d0\u53471.4\u500d\uff1b\u7ecf\u6d4e\u4e0a\u5e73\u5747\u6536\u76ca\u5927\u4e8e\u672c\u5730\u89e3\u8026\u6210\u672c\uff0c\u4f46\u7ad9\u70b9\u95f4\u5dee\u5f02\u9700\u8981\u7535\u7f51\u5e72\u9884\u3002", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u8d1f\u8f7d\u89e3\u8026\u662f\u964d\u4f4e\u78b3\u8db3\u8ff9\u7684\u6709\u6548\u7b56\u7565\uff0c\u4f18\u5316\u5206\u5e03\u548c\u7535\u7f51\u5408\u4f5c\u80fd\u6700\u5927\u5316\u6548\u76ca\uff0c\u7ecf\u6d4e\u53ef\u884c\u6027\u9ad8\u4f46\u9700\u8981\u653f\u7b56\u652f\u6301\u89e3\u51b3\u7ad9\u70b9\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2511.09001", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.09001", "abs": "https://arxiv.org/abs/2511.09001", "authors": ["Yuka Haruki", "Shigeru Ishikura", "Kazuya Demachi", "Teruaki Hayashi"], "title": "Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration", "comment": "10 pages", "summary": "As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8868\u683c\u7ed3\u6784\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u5143\u7d20\uff08\u5217\u63cf\u8ff0\u3001\u5916\u90e8\u77e5\u8bc6\uff09\u7684\u56fe\u5d4c\u5165\u6280\u672f\uff0c\u7528\u4e8e\u6570\u636e\u96c6\u6210\u4e2d\u7684\u6a21\u5f0f\u5339\u914d\u548c\u5b9e\u4f53\u89e3\u6790\u4efb\u52a1\uff0c\u5728\u591a\u79cd\u6570\u636e\u96c6\u7279\u6027\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6570\u636e\u96c6\u6210\u9700\u6c42\u589e\u957f\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5bf9\u6570\u636e\u96c6\u7279\u6027\u7684\u5f71\u54cd\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u65b9\u6cd5\u7ec4\u5408\u6709\u9650\u3002\u9700\u8981\u66f4\u6709\u6548\u5904\u7406\u4e0d\u540c\u6570\u636e\u96c6\u7279\u6027\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e0a\u4e0b\u6587\u56fe\u5d4c\u5165\u6280\u672f\uff0c\u6574\u5408\u8868\u683c\u6570\u636e\u7684\u7ed3\u6784\u7ec6\u8282\u548c\u4e0a\u4e0b\u6587\u5143\u7d20\uff08\u5217\u63cf\u8ff0\u3001\u5916\u90e8\u77e5\u8bc6\uff09\uff0c\u5728\u5177\u6709\u4e0d\u540c\u7279\u6027\uff08\u9886\u57df\u7279\u5f02\u6027\u3001\u6570\u636e\u89c4\u6a21\u3001\u7f3a\u5931\u7387\u3001\u91cd\u53e0\u7387\uff09\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u7279\u6027\u4e0b\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u56fe\u57fa\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6570\u503c\u6bd4\u4f8b\u9ad8\u6216\u7f3a\u5931\u6570\u636e\u591a\u7684\u56f0\u96be\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u4f46\u5728\u8bed\u4e49\u76f8\u4f3c\u4f46\u4e0d\u540c\u7684\u5217\u4e0a\u4ecd\u5b58\u5728\u5931\u8d25\u6848\u4f8b\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5d4c\u5165\u63d0\u9ad8\u4e86\u5339\u914d\u53ef\u9760\u6027\uff0c\u6570\u636e\u96c6\u7279\u6027\u663e\u8457\u5f71\u54cd\u96c6\u6210\u7ed3\u679c\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u5f00\u53d1\u652f\u6301\u5b9e\u9645\u4f01\u4e1a\u5e94\u7528\u7684\u5b9e\u7528\u6570\u636e\u96c6\u6210\u7cfb\u7edf\u3002"}}
{"id": "2511.09212", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09212", "abs": "https://arxiv.org/abs/2511.09212", "authors": ["Zeru Cheng", "Yanjing Yang", "He Zhang", "Lanxin Yang", "Jinghao Hu", "Jinwei Xu", "Bohan Liu", "Haifeng Shen"], "title": "Leveraging Self-Paced Learning for Software Vulnerability Detection", "comment": null, "summary": "Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.", "AI": {"tldr": "\u63d0\u51faSPLVD\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6b65\u5b66\u4e60\u4ece\u6613\u5230\u96be\u52a8\u6001\u9009\u62e9\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u6cd5\u51c6\u786e\u7387\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u4f4e", "method": "SPLVD\u4f7f\u7528\u4e13\u95e8\u8bbe\u8ba1\u7684\u6570\u636e\u9009\u62e9\u5668\uff0c\u6839\u636e\u8bad\u7ec3\u9636\u6bb5\u52a8\u6001\u9009\u62e9\u6e90\u4ee3\u7801\uff0c\u6a21\u62df\u4eba\u7c7b\u4ece\u6613\u5230\u96be\u7684\u5b66\u4e60\u8fc7\u7a0b", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523089.2%\u300168.7%\u548c43.5%\u7684F1\u5206\u6570\uff0c\u5728OpenHarmony\u9879\u76ee\u4e2d\u8fbe\u523090.9%\u7684\u7cbe\u786e\u7387", "conclusion": "SPLVD\u80fd\u6709\u6548\u63d0\u5347\u8f6f\u4ef6\u6f0f\u6d1e\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2511.08948", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.08948", "abs": "https://arxiv.org/abs/2511.08948", "authors": ["Jay Tharwani", "Shobhit Aggarwal", "Arnab A Purkayastha"], "title": "Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures", "comment": "7 pages", "summary": "This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u5927\u4e91\u63d0\u4f9b\u5546\u5728HPC CPU\u6027\u80fd\u548c\u6210\u672c\u65b9\u9762\u7684\u8868\u73b0\uff0cAWS\u6027\u80fd\u6700\u4f73\u4f46\u4ef7\u683c\u6602\u8d35\uff0cOCI\u6700\u7ecf\u6d4e\u4f46\u901f\u5ea6\u8f83\u6162\uff0cARM\u5b9e\u4f8b\u5728\u4e0d\u540c\u63d0\u4f9b\u5546\u95f4\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u6bd4\u8f83\u865a\u62df\u5316\u4e91\u57fa\u7840\u8bbe\u65bd\u4e2dHPC\u98ce\u683c\u7684CPU\u6027\u80fd\u548c\u6210\u672c\uff0c\u5e2e\u52a9\u7528\u6237\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5148\u7ea7\u9009\u62e9\u6700\u4f18\u5b9e\u4f8b\u7c7b\u578b\u3002", "method": "\u4f7f\u7528SPEC ACCEL\u5957\u4ef6\u4e2d\u7684OpenMP\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5728AWS\u3001Azure\u3001GCP\u548cOCI\u56db\u5927\u4e91\u63d0\u4f9b\u5546\u4e0a\u6d4b\u8bd5Intel\u3001AMD\u548cARM\u5b9e\u4f8b\u7c7b\u578b\uff0c\u5bf9\u6bd4\u6309\u9700\u548c\u4e00\u5e74\u6298\u6263\u5b9a\u4ef7\u3002", "result": "AWS\u5728\u6240\u6709\u4e09\u79cd\u5b9e\u4f8b\u7c7b\u578b\u4e2d\u8fd0\u884c\u65f6\u95f4\u6700\u77ed\u4f46\u6536\u8d39\u6700\u9ad8\uff1bOCI\u5728\u6240\u6709CPU\u7cfb\u5217\u4e2d\u6700\u7ecf\u6d4e\u4f46\u8fd0\u884c\u8f83\u6162\uff1bAzure\u8868\u73b0\u4e2d\u7b49\uff1bGCP\u4eceIntel\u5207\u6362\u5230AMD\u65f6\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f46ARM\u5b9e\u4f8b\u6bd4\u5176AMD\u5b9e\u4f8b\u6162\u4e24\u500d\u4e14\u66f4\u8d35\uff1bAWS\u7684ARM\u5b9e\u4f8b\u6bd4\u5176Intel\u548cAMD\u5b9e\u4f8b\u5feb49%\u3002", "conclusion": "\u5b9e\u4f8b\u9009\u62e9\u548c\u63d0\u4f9b\u5546\u9009\u62e9\u5728\u8fd0\u884c\u65f6\u95f4\u548c\u4ef7\u683c\u4e0a\u4ea7\u751f\u663e\u8457\u5dee\u5f02\uff0c\u5e94\u6839\u636e\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5148\u7ea7\uff08\u539f\u59cb\u901f\u5ea6\u6216\u6210\u672c\u6700\u5c0f\u5316\uff09\u6307\u5bfc\u5b9e\u4f8b\u7c7b\u578b\u51b3\u7b56\u3002"}}
{"id": "2511.09052", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.09052", "abs": "https://arxiv.org/abs/2511.09052", "authors": ["Yu Wang", "Hui Wang", "Jiake Ge", "Xin Wang"], "title": "Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking", "comment": "10 pages", "summary": "Exact subgraph matching on large-scale graphs remains a chal- lenging problem due to high computational complexity and dis- tributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed envi- ronments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, mem- ory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves \"minimum edge cut + load balancing + non-interruptible queries\" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e09\u79cd\u6838\u5fc3\u521b\u65b0\u6765\u6269\u5c55GNN-PE\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\uff1a\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u3001\u591aGPU\u534f\u4f5c\u7f13\u5b58\u7b56\u7565\u548c\u67e5\u8be2\u8ba1\u5212\u6392\u5e8f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u5b50\u56fe\u5339\u914d\u6548\u7387\u3002", "motivation": "\u73b0\u6709GNN-PE\u6846\u67b6\u5728\u5355\u673a\u4e0a\u80fd\u5b9e\u73b0\u9ad8\u6548\u7cbe\u786e\u5339\u914d\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5206\u5e03\u5f0f\u73af\u5883\u7684\u53ef\u6269\u5c55\u6027\u548c\u4f18\u5316\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u8ba1\u7b97\u9700\u6c42\u3002", "method": "1) \u8f7b\u91cf\u7ea7\u52a8\u6001\u76f8\u5173\u6027\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u548c\u70ed\u8fc1\u79fb\u673a\u5236\uff1b2) \u57fa\u4e8e\u5728\u7ebf\u589e\u91cf\u5b66\u4e60\u7684\u591aGPU\u534f\u4f5c\u52a8\u6001\u7f13\u5b58\u7b56\u7565\uff1b3) \u57fa\u4e8e\u652f\u914d\u5d4c\u5165\u526a\u679d\u6f5c\u529b\u7684\u67e5\u8be2\u8ba1\u5212\u6392\u5e8f\u65b9\u6cd5\uff1b\u7ed3\u5408METIS\u5206\u533a\u548c\u5e76\u884c\u79bb\u7ebf\u9884\u5904\u7406\u3002", "result": "\u5728\u5206\u5e03\u5f0f\u573a\u666f\uff08\u6570\u5341\u53f0\u673a\u5668\uff09\u4e2d\u5b9e\u73b0\u4e86\"\u6700\u5c0f\u8fb9\u5207\u5272+\u8d1f\u8f7d\u5747\u8861+\u4e0d\u95f4\u65ad\u67e5\u8be2\"\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5e03\u5f0f\u5b50\u56fe\u5339\u914d\u7684\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u901a\u8fc7\u4e09\u9879\u6838\u5fc3\u6280\u672f\u521b\u65b0\uff0c\u6210\u529f\u5c06GNN-PE\u6846\u67b6\u6269\u5c55\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u56fe\u4e0a\u7cbe\u786e\u5b50\u56fe\u5339\u914d\u7684\u6311\u6218\u3002"}}
{"id": "2511.09223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09223", "abs": "https://arxiv.org/abs/2511.09223", "authors": ["Panya Trakoolgerntong", "Tao Xiao", "Masanari Kondo", "Chaiyong Ragkhitwetsagul", "Morakot Choetkiertikul", "Pattaraporn Sangaroonsilp", "Yasutaka Kamei"], "title": "AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews", "comment": null, "summary": "Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.\n  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.", "AI": {"tldr": "AILINKPREVIEWER\u5de5\u5177\u5229\u7528LLM\u751f\u6210PR\u4e2d\u94fe\u63a5\u7684\u9884\u89c8\uff0c\u901a\u8fc7\u5bf9\u6bd4\u4e09\u79cd\u65b9\u6cd5\u53d1\u73b0\u4e0a\u4e0b\u6587\u6458\u8981\u65b9\u6cd5\u5728\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u7528\u6237\u7814\u7a76\u663e\u793a\u5927\u591a\u6570\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u975e\u4e0a\u4e0b\u6587\u6458\u8981\u3002", "motivation": "\u4ee3\u7801\u5ba1\u67e5\u4e2d\u94fe\u63a5\u901a\u5e38\u88ab\u4e22\u5f03\uff0c\u9650\u5236\u4e86\u4fe1\u606f\u7684\u4e30\u5bcc\u6027\u5e76\u589e\u52a0\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u9700\u8981\u5de5\u5177\u6765\u63d0\u4f9b\u94fe\u63a5\u9884\u89c8\u4ee5\u589e\u5f3a\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\u3002", "method": "\u5f00\u53d1AILINKPREVIEWER\u5de5\u5177\uff0c\u5229\u7528LLM\u57fa\u4e8ePR\u5143\u6570\u636e\uff08\u6807\u9898\u3001\u63cf\u8ff0\u3001\u8bc4\u8bba\u548c\u94fe\u63a5\u5185\u5bb9\uff09\u751f\u6210\u94fe\u63a5\u9884\u89c8\uff0c\u6bd4\u8f83\u4e86\u4e0a\u4e0b\u6587LLM\u6458\u8981\u3001\u975e\u4e0a\u4e0b\u6587LLM\u6458\u8981\u548c\u57fa\u4e8e\u5143\u6570\u636e\u7684\u9884\u89c8\u4e09\u79cd\u65b9\u6cd5\u3002", "result": "\u4e0a\u4e0b\u6587\u6458\u8981\u5728BLEU\u3001BERTScore\u548c\u538b\u7f29\u6bd4\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u7528\u6237\u7814\u7a76\u4e2d\u5927\u591a\u6570\u53c2\u4e0e\u8005\u66f4\u559c\u6b22\u975e\u4e0a\u4e0b\u6587\u6458\u8981\uff0c\u8868\u660e\u6307\u6807\u6027\u80fd\u4e0e\u611f\u77e5\u53ef\u7528\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u94fe\u63a5\u9884\u89c8\u6709\u6f5c\u529b\u63d0\u9ad8\u4ee3\u7801\u5ba1\u67e5\u6548\u7387\uff0c\u4e3a\u5f00\u53d1\u4eba\u5458\u548c\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2511.08998", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08998", "abs": "https://arxiv.org/abs/2511.08998", "authors": ["Zilinghan Li", "Aditya Sinha", "Yijiang Li", "Kyle Chard", "Kibaek Kim", "Ravi Madduri"], "title": "Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science", "comment": null, "summary": "Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4f01\u4e1a\u7ea7\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u6846\u67b6APPFL\u7684\u8bbe\u8ba1\u613f\u666f\uff0c\u65e8\u5728\u89e3\u51b3\u4ece\u672c\u5730\u539f\u578b\u5230\u5206\u5e03\u5f0f\u90e8\u7f72\u7684\u89c4\u6a21\u5316\u6311\u6218\uff0c\u652f\u6301\u8de8\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u7684\u65e0\u7f1d\u6269\u5c55\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u79d1\u5b66\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u7528\u6237\u53cb\u597d\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u8981\u6c42\uff0c\u7279\u522b\u662f\u5728\u4ece\u672c\u5730\u539f\u578b\u5230\u5206\u5e03\u5f0f\u90e8\u7f72\u7684\u8fc7\u6e21\u4e2d\u5b58\u5728\u6311\u6218\u3002", "method": "\u57fa\u4e8eAPPFL\u6846\u67b6\u5f00\u53d1\u7ecf\u9a8c\uff0c\u63d0\u51fa\u4f01\u4e1a\u7ea7FL\u6846\u67b6\u7684\u5173\u952e\u80fd\u529b\uff1a\u53ef\u6269\u5c55\u672c\u5730\u4eff\u771f\u3001\u65e0\u7f1d\u90e8\u7f72\u8fc7\u6e21\u3001\u8de8\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u3001\u591a\u7ea7\u62bd\u8c61\u3001\u5168\u9762\u9690\u79c1\u5b89\u5168\u4fdd\u62a4\u3002", "result": "\u63d0\u51fa\u4e86\u5b9e\u73b0\u4f01\u4e1a\u7ea7\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u80fd\u591f\u652f\u6301\u4ece\u4e2a\u4eba\u8bbe\u5907\u5230\u4e91\u96c6\u7fa4\u548cHPC\u7cfb\u7edf\u7684\u5206\u5e03\u5f0f\u90e8\u7f72\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u5f25\u5408\u7814\u7a76\u539f\u578b\u4e0e\u4f01\u4e1a\u7ea7\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u4eba\u5de5\u667a\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09262", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09262", "abs": "https://arxiv.org/abs/2511.09262", "authors": ["Jiaping Cao", "Ting Sun", "Man Lung Yiu", "Xiao Yan", "Bo Tang"], "title": "CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System", "comment": null, "summary": "Spatial data analytics systems are widely studied in both the academia and industry. However, existing systems are limited when handling a large number of moving objects and real time spatial queries. In this work, we architect a scalable and efficient system CheetahGIS to process streaming spatial queries over massive moving objects. In particular, CheetahGIS is built upon Apache Flink Stateful Functions (StateFun), an API for building distributed streaming applications with an actor-like model. CheetahGIS enjoys excellent scalability due to its modular architecture, which clearly decomposes different components and allows scaling individual components. To improve the efficiency and scalability of CheetahGIS, we devise a suite of optimizations, e.g., lightweight global grid-based index, metadata synchroniza tion strategies, and load balance mechanisms. We also formulate a generic paradigm for spatial query processing in CheetahGIS, and verify its generality by processing three representative streaming queries (i.e., object query, range count query, and k nearest neighbor query). We conduct extensive experiments on both real and synthetic datasets to evaluate CheetahGIS.", "AI": {"tldr": "CheetahGIS\u662f\u4e00\u4e2a\u57fa\u4e8eApache Flink Stateful Functions\u6784\u5efa\u7684\u53ef\u6269\u5c55\u9ad8\u6548\u7cfb\u7edf\uff0c\u7528\u4e8e\u5904\u7406\u6d77\u91cf\u79fb\u52a8\u5bf9\u8c61\u4e0a\u7684\u6d41\u5f0f\u7a7a\u95f4\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u5904\u7406\u5927\u91cf\u79fb\u52a8\u5bf9\u8c61\u548c\u5b9e\u65f6\u7a7a\u95f4\u67e5\u8be2\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u7cfb\u7edf\u6765\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u57fa\u4e8eApache Flink Stateful Functions\u6784\u5efa\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5168\u5c40\u7f51\u683c\u7d22\u5f15\u3001\u5143\u6570\u636e\u540c\u6b65\u7b56\u7565\u548c\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u7b49\u4f18\u5316\u6280\u672f\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4f18\u79c0\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u4e09\u79cd\u4ee3\u8868\u6027\u6d41\u5f0f\u67e5\u8be2\uff08\u5bf9\u8c61\u67e5\u8be2\u3001\u8303\u56f4\u8ba1\u6570\u67e5\u8be2\u548ck\u8fd1\u90bb\u67e5\u8be2\uff09\u3002", "conclusion": "CheetahGIS\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u4f18\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6d77\u91cf\u79fb\u52a8\u5bf9\u8c61\u5b9e\u65f6\u7a7a\u95f4\u67e5\u8be2\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2511.09231", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09231", "abs": "https://arxiv.org/abs/2511.09231", "authors": ["Tobias Eisenreich", "Nicholas Friedlaender", "Stefan Wagner"], "title": "Leveraging Large Language Models for Use Case Model Generation from Software Requirements", "comment": "Accepted at the Intelligent Software Engineering Workshop (ISE 2025) at ASE 2025", "summary": "Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u7528\u4f8b\u5efa\u6a21\uff0c\u901a\u8fc7\u96c6\u6210\u5f00\u6e90LLM\u548c\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u4ece\u8f6f\u4ef6\u9700\u6c42\u4e2d\u63d0\u53d6\u53c2\u4e0e\u8005\u548c\u7528\u4f8b\uff0c\u5b9e\u9a8c\u663e\u793a\u5efa\u6a21\u65f6\u95f4\u51cf\u5c1160%\u4e14\u8d28\u91cf\u76f8\u5f53\u3002", "motivation": "\u624b\u52a8\u521b\u5efa\u7528\u4f8b\u6a21\u578b\u8017\u65f6\u8d39\u529b\uff0c\u5b9e\u8df5\u4e2d\u5e38\u88ab\u8df3\u8fc7\uff0c\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u96c6\u6210\u5f00\u6e90LLM\uff0c\u91c7\u7528\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u4ece\u8f6f\u4ef6\u9700\u6c42\u4e2d\u7cfb\u7edf\u63d0\u53d6\u53c2\u4e0e\u8005\u548c\u7528\u4f8b\uff0c\u5e76\u4e0e\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "\u5efa\u6a21\u65f6\u95f4\u51cf\u5c1160%\uff0c\u6a21\u578b\u8d28\u91cf\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u53c2\u4e0e\u8005\u8ba4\u4e3a\u8be5\u65b9\u6cd5\u5728\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\u3002", "conclusion": "LLM\u8f85\u52a9\u7684\u7528\u4f8b\u5efa\u6a21\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u81ea\u52a8\u5316\u652f\u6301\u3002"}}
{"id": "2511.09143", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09143", "abs": "https://arxiv.org/abs/2511.09143", "authors": ["Myungsu Kim", "Ikjun Yeom", "Younghoon Kim"], "title": "Flex-MIG: Enabling Distributed Execution on MIG", "comment": "13 pages, 11 figures, under review for MLSys 2026", "summary": "GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.", "AI": {"tldr": "Flex-MIG\u662f\u4e00\u4e2a\u8f6f\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06MIG\u7684\u4e00\u5bf9\u4e00\u5206\u914d\u6a21\u578b\u6539\u4e3a\u4e00\u5bf9\u591a\uff0c\u5229\u7528\u4e3b\u673a\u5171\u4eab\u5185\u5b58\u5b9e\u73b0\u8de8MIG\u5b9e\u4f8b\u7684\u96c6\u5408\u901a\u4fe1\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\uff0c\u663e\u8457\u63d0\u5347GPU\u96c6\u7fa4\u5229\u7528\u7387\u3002", "motivation": "\u591a\u79df\u6237GPU\u96c6\u7fa4\u4e2d\uff0cNVIDIA MIG\u6280\u672f\u7684\u786c\u4ef6\u521a\u6027\u548c\u4f20\u7edf\u4e00\u5bf9\u4e00\u5206\u914d\u6a21\u578b\u5bfc\u81f4\u4e25\u91cd\u7684\u8d44\u6e90\u788e\u7247\u5316\u548c\u96c6\u7fa4\u5229\u7528\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "\u63d0\u51faFlex-MIG\u8f6f\u4ef6\u6846\u67b6\uff0c\u91c7\u7528\u4e00\u5bf9\u591a\u5206\u914d\u6a21\u578b\uff0c\u901a\u8fc7\u4e3b\u673a\u5171\u4eab\u5185\u5b58\u5b9e\u73b0\u8de8MIG\u5b9e\u4f8b\u7684\u96c6\u5408\u901a\u4fe1\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\u3002", "result": "\u6d88\u9664\u4e86\u9700\u8981\u6392\u7a7a\u7684\u91cd\u914d\u7f6e\u64cd\u4f5c\uff0c\u51cf\u5c11\u4e86\u788e\u7247\u5316\uff0c\u5728\u591a\u6837\u5316\u8ddf\u8e2a\u6d4b\u8bd5\u4e2d\u4f7fmakespan\u63d0\u5347\u9ad8\u8fbe17%\u3002", "conclusion": "\u5c06MIG\u7684\u64cd\u4f5c\u6a21\u578b\u91cd\u65b0\u8bbe\u8ba1\u4e3a\u8f6f\u4ef6\u534f\u8c03\u5c42\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u96c6\u7fa4\u6548\u7387\u3002"}}
{"id": "2511.09268", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09268", "abs": "https://arxiv.org/abs/2511.09268", "authors": ["Helio Victor F. Santos", "Vitor Costa", "Joao Eduardo Montandon", "Marco Tulio Valente"], "title": "Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects", "comment": null, "summary": "Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.", "AI": {"tldr": "\u5bf9Claude Code\u914d\u7f6e\u6587\u4ef6\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86328\u4e2a\u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5173\u6ce8\u70b9\u548c\u5b9e\u8df5\u6a21\u5f0f\u3002", "motivation": "\u867d\u7136\u667a\u80fd\u4ee3\u7801\u52a9\u624b\u627f\u8bfa\u63d0\u5347\u751f\u4ea7\u529b\uff0c\u4f46\u5176\u884c\u4e3a\u548c\u6548\u679c\u4e25\u91cd\u4f9d\u8d56\u914d\u7f6e\u6587\u4ef6\uff0c\u800c\u5bf9\u8fd9\u4e9b\u914d\u7f6e\u5de5\u4ef6\u7684\u7ed3\u6784\u548c\u5185\u5bb9\u4e86\u89e3\u751a\u5c11\u3002", "method": "\u6536\u96c6\u5e76\u5206\u6790328\u4e2a\u516c\u5f00Claude Code\u9879\u76ee\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u8bc6\u522b\u5176\u4e2d\u6307\u5b9a\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5173\u6ce8\u70b9\u3001\u5b9e\u8df5\u4ee5\u53ca\u8fd9\u4e9b\u5173\u6ce8\u70b9\u5728\u5355\u4e2a\u6587\u4ef6\u4e2d\u7684\u5171\u73b0\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u667a\u80fd\u4f53\u914d\u7f6e\u6587\u4ef6\u4e2d\u5b9a\u4e49\u5e7f\u6cdb\u5173\u6ce8\u70b9\u548c\u5b9e\u8df5\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u6307\u5b9a\u667a\u80fd\u4f53\u5e94\u9075\u5faa\u7684\u67b6\u6784\u3002", "conclusion": "\u914d\u7f6e\u6587\u4ef6\u4e2d\u9700\u8981\u660e\u786e\u5b9a\u4e49\u5404\u79cd\u8f6f\u4ef6\u5de5\u7a0b\u5173\u6ce8\u70b9\uff0c\u5176\u4e2d\u67b6\u6784\u89c4\u8303\u5c24\u4e3a\u91cd\u8981\u3002"}}
{"id": "2511.09194", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09194", "abs": "https://arxiv.org/abs/2511.09194", "authors": ["Simon K\u00f6nig", "Lukas Epple", "Christian Becker"], "title": "Minimize Your Critical Path with Combine-and-Exchange Locks", "comment": "19 pages, 15 figures", "summary": "Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u6237\u7a7a\u95f4\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5CES\uff0c\u901a\u8fc7\u7ed3\u5408\u4ea4\u6362\u8c03\u5ea6\u6765\u4f18\u5316\u534f\u7a0b\u540c\u6b65\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u5e26\u67653-8\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u7f16\u7a0b\u8bed\u8a00\u5e7f\u6cdb\u652f\u6301\u534f\u7a0b\u8fdb\u884c\u9ad8\u5e76\u884c\u6216\u5f02\u6b65\u5e94\u7528\uff0c\u7528\u6237\u7a7a\u95f4\u540c\u6b65\u907f\u514d\u4e86\u91cd\u91cf\u7ea7\u7cfb\u7edf\u8c03\u7528\uff0c\u4f46\u73b0\u6709\u540c\u6b65\u539f\u8bed\u4ecd\u91c7\u7528\u5185\u6838\u7ea7\u8c03\u5ea6\u89c6\u89d2\uff0c\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u541e\u5410\u91cf\u3002", "method": "\u5f00\u53d1\u4e86Combine-and-Exchange Scheduling (CES)\u65b9\u6cd5\uff0c\u786e\u4fdd\u7ade\u4e89\u4e34\u754c\u533a\u4fdd\u6301\u5728\u540c\u4e00\u4e2a\u6267\u884c\u7ebf\u7a0b\u4e0a\uff0c\u540c\u65f6\u5c06\u53ef\u5e76\u884c\u5de5\u4f5c\u5747\u5300\u5206\u914d\u5230\u5176\u4ed6\u7ebf\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u591a\u79cd\u73b0\u6709\u8bed\u8a00\u548c\u5e93\uff0c\u5728\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e863\u500d\u6027\u80fd\u63d0\u5347\uff0c\u5728\u5fae\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e868\u500d\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CES\u65b9\u6cd5\u91cd\u65b0\u601d\u8003\u4e86\u7528\u6237\u7a7a\u95f4\u4efb\u52a1\u7684\u540c\u6b65\u673a\u5236\uff0c\u901a\u8fc7\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u534f\u7a0b\u540c\u6b65\u6027\u80fd\u3002"}}
{"id": "2511.09373", "categories": ["cs.SE", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.09373", "abs": "https://arxiv.org/abs/2511.09373", "authors": ["Adam \u0160torek", "Vikas Upadhyay", "Marianne Menglin Liu", "Daniel W. Peterson", "Anshul Mittal", "Sujeeth Bharadwaj", "Fahad Shah", "Dan Roth"], "title": "Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks", "comment": null, "summary": "LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.", "AI": {"tldr": "Routesplain\u662f\u9996\u4e2a\u9488\u5bf9\u8f6f\u4ef6\u76f8\u5173\u4efb\u52a1\u7684LLM\u8def\u7531\u5668\uff0c\u901a\u8fc7\u63d0\u53d6\u53ef\u89e3\u91ca\u6982\u5ff5\u8fdb\u884c\u8def\u7531\u51b3\u7b56\uff0c\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\uff0c\u5e76\u8fbe\u5230\u6216\u8d85\u8fc7\u6240\u6709\u9ed1\u76d2\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709LLM\u5728\u8f6f\u4ef6\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u901a\u8fc7\u8def\u7531\u67e5\u8be2\u5230\u5408\u9002\u7684LLM\u53ef\u4ee5\u63d0\u9ad8\u54cd\u5e94\u8d28\u91cf\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u901a\u7528LLM\u8def\u7531\uff0c\u7f3a\u4e4f\u9488\u5bf9\u8f6f\u4ef6\u4efb\u52a1\u7684\u4e13\u7528\u8def\u7531\u5668\u3002", "method": "Routesplain\u4ece\u6bcf\u4e2a\u67e5\u8be2\u4e2d\u63d0\u53d6\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u6982\u5ff5\uff08\u5982\u4efb\u52a1\u7c7b\u578b\u3001\u9886\u57df\u3001\u63a8\u7406\u590d\u6742\u5ea6\uff09\uff0c\u4ec5\u57fa\u4e8e\u8fd9\u4e9b\u6982\u5ff5\u8fdb\u884c\u8def\u7531\u51b3\u7b56\uff0c\u63d0\u4f9b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u4f9d\u636e\u3002", "result": "\u572816\u4e2a\u6700\u5148\u8fdbLLM\u548c8\u4e2a\u8f6f\u4ef6\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRoutesplain\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u65b9\u9762\u5747\u4f18\u4e8e\u5355\u4e2a\u6a21\u578b\uff0c\u4e14\u7b49\u4e8e\u6216\u8d85\u8fc7\u6240\u6709\u9ed1\u76d2\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Routesplain\u8bc1\u660e\u4e86\u57fa\u4e8e\u53ef\u89e3\u91ca\u6982\u5ff5\u7684\u8def\u7531\u65b9\u6cd5\u5728\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6982\u5ff5\u7ea7\u5e72\u9884\u4e3a\u8def\u7531\u5668\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.09410", "categories": ["cs.DC", "cs.DS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.09410", "abs": "https://arxiv.org/abs/2511.09410", "authors": ["Yusuf Motiwala"], "title": "No Cords Attached: Coordination-Free Concurrent Lock-Free Queues", "comment": "10 pages, 2 figures, 3 tables. Lock-free concurrent queue with coordination-free memory reclamation", "summary": "The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.\n  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.", "AI": {"tldr": "\u63d0\u51fa\u4e86Cyclic Memory Protection (CMP)\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4e25\u683cFIFO\u8bed\u4e49\u3001\u65e0\u754c\u5bb9\u91cf\u548c\u9501\u81ea\u7531\u8fdb\u5ea6\u7684\u540c\u65f6\uff0c\u6d88\u9664\u4e86\u534f\u8c03\u5f00\u9500\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5e76\u53d1\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65e0\u9501\u961f\u5217\u5b9e\u73b0\u5728\u5e76\u53d1\u73af\u5883\u4e0b\u53d8\u5f97\u5f02\u5e38\u590d\u6742\uff0c\u534f\u8c03\u673a\u5236\u5f80\u5f80\u4e3b\u5bfc\u8bbe\u8ba1\uff0c\u4fdd\u62a4\u5f00\u9500\u5728\u9ad8\u5e76\u53d1AI\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u91c7\u7528\u5faa\u73af\u5185\u5b58\u4fdd\u62a4(CMP)\u673a\u5236\uff0c\u901a\u8fc7\u6709\u754c\u4fdd\u62a4\u7a97\u53e3\u63d0\u4f9b\u5b9e\u7528\u7684\u56de\u6536\u4fdd\u8bc1\uff0c\u65e0\u9700\u534f\u8c03\u5373\u53ef\u5b9e\u73b0\u4e25\u683cFIFO\u961f\u5217\u3002", "result": "CMP\u5728\u9ad8\u5ea6\u7ade\u4e89\u73af\u5883\u4e0b\u6bd4\u6700\u5148\u8fdb\u7684\u65e0\u9501\u961f\u5217\u6027\u80fd\u63d0\u53471.72-4\u500d\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u6570\u767e\u4e2a\u7ebf\u7a0b\u3002", "conclusion": "\u9ad8\u5e76\u53d1\u961f\u5217\u53ef\u4ee5\u5728\u4e0d\u524a\u5f31\u961f\u5217\u8bed\u4e49\u7684\u60c5\u51b5\u4e0b\u56de\u5f52\u5176\u57fa\u672c\u7b80\u5355\u6027\uff0cCMP\u4e3a\u6b64\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.09447", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09447", "abs": "https://arxiv.org/abs/2511.09447", "authors": ["Lukas Gianinazzi", "Tal Ben-Nun", "Torsten Hoefler"], "title": "SPADA: A Spatial Dataflow Architecture Programming Language", "comment": null, "summary": "Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.\n  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.\n  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.", "AI": {"tldr": "SPADA\u662f\u4e00\u4e2a\u9488\u5bf9\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u7684\u7f16\u7a0b\u8bed\u8a00\uff0c\u901a\u8fc7\u62bd\u8c61\u5e95\u5c42\u67b6\u6784\u7ec6\u8282\uff0c\u63d0\u4f9b\u5bf9\u6570\u636e\u653e\u7f6e\u3001\u6570\u636e\u6d41\u6a21\u5f0f\u548c\u5f02\u6b65\u64cd\u4f5c\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u663e\u8457\u51cf\u5c11\u4ee3\u7801\u91cf\u5e76\u5b9e\u73b0\u8fd1\u7406\u60f3\u7684\u5f31\u6269\u5c55\u6027\u3002", "motivation": "\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u5728AI\u548c\u79d1\u5b66\u8ba1\u7b97\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f16\u7a0b\u56f0\u96be\uff0c\u9700\u8981\u663e\u5f0f\u534f\u8c03\u6570\u636e\u79fb\u52a8\u548c\u5f02\u6b65\u8ba1\u7b97\u3002\u73b0\u6709\u7f16\u7a0b\u6a21\u578b\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u67b6\u6784\u7684\u72ec\u7279\u80fd\u529b\uff0c\u7279\u522b\u662f\u9ad8\u6548\u7684\u6570\u636e\u6d41\u548c\u590d\u6742\u8def\u7531\u7ba1\u7406\u3002", "method": "\u63d0\u51faSPADA\u7f16\u7a0b\u8bed\u8a00\uff0c\u5efa\u7acb\u4e25\u683c\u7684\u6570\u636e\u6d41\u8bed\u4e49\u6846\u67b6\uff0c\u5b9a\u4e49\u8def\u7531\u6b63\u786e\u6027\u3001\u6570\u636e\u7ade\u4e89\u548c\u6b7b\u9501\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9Cerebras CSL\u7684\u591a\u7ea7\u964d\u4f4e\u7f16\u8bd1\u5668\u3002", "result": "SPADA\u5c06\u590d\u6742\u5e76\u884c\u6a21\u5f0f\u7684\u4ee3\u7801\u91cf\u51cf\u5c116-8\u500d\uff0c\u5728\u4e09\u4e2a\u6570\u91cf\u7ea7\u4e0a\u5b9e\u73b0\u8fd1\u7406\u60f3\u7684\u5f31\u6269\u5c55\u6027\uff0c\u5e76\u4f5c\u4e3aGT4Py stencil DSL\u7684\u9ad8\u7ea7\u7f16\u7a0b\u63a5\u53e3\u548c\u4e2d\u95f4\u8868\u793a\u3002", "conclusion": "SPADA\u901a\u8fc7\u7edf\u4e00\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u7684\u7f16\u7a0b\u6a21\u578b\uff0c\u63a8\u8fdb\u4e86\u8fd9\u4e9b\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e73\u53f0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u53ef\u7528\u6027\u3002"}}
{"id": "2511.09485", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09485", "abs": "https://arxiv.org/abs/2511.09485", "authors": ["Miroslav Popovic", "Marko Popovic", "Pavle Vasiljevic", "Miodrag Djukic"], "title": "Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links", "comment": "4 pages, 1 figure, 3 tables", "summary": "The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).", "AI": {"tldr": "\u4f7f\u7528CSP\u8fc7\u7a0b\u4ee3\u6570\u5bf9Python\u8054\u90a6\u5b66\u4e60\u6d4b\u8bd5\u5e8a\u4e2d\u7684\u7b2c\u4e09\u4e2a\u901a\u7528\u7b97\u6cd5\uff08TDM\u901a\u4fe1\uff09\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u901a\u8fc7\u6a21\u578b\u68c0\u67e5\u5668PAT\u8bc1\u660e\u5176\u6b7b\u9501\u81ea\u7531\u6027\u548c\u6210\u529f\u7ec8\u6b62\u6027", "motivation": "\u4e3aPython\u8054\u90a6\u5b66\u4e60\u6d4b\u8bd5\u5e8a\u6846\u67b6\u4e2d\u7684\u7b2c\u4e09\u4e2a\u901a\u7528\u7b97\u6cd5\uff08TDM\u901a\u4fe1\uff09\u63d0\u4f9b\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u786e\u4fdd\u5176\u6b63\u786e\u6027\u548c\u53ef\u9760\u6027", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u6784\u5efa\u5fe0\u5b9e\u53cd\u6620Python\u4ee3\u7801\u7684CSP\u6a21\u578b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6a21\u578b\u68c0\u67e5\u5668PAT\u81ea\u52a8\u9a8c\u8bc1\u7b97\u6cd5\u7684\u5b89\u5168\u6027\u548c\u6d3b\u6027\u5c5e\u6027", "result": "\u6210\u529f\u8bc1\u660e\u4e86\u7b2c\u4e09\u4e2a\u901a\u7528\u7b97\u6cd5\u7684\u6b7b\u9501\u81ea\u7531\u6027\uff08\u5b89\u5168\u6027\uff09\u548c\u6210\u529f\u7ec8\u6b62\u6027\uff08\u6d3b\u6027\uff09", "conclusion": "\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\u8bc1\u5b9e\u4e86Python\u8054\u90a6\u5b66\u4e60\u6d4b\u8bd5\u5e8a\u4e2dTDM\u901a\u4fe1\u7b97\u6cd5\u7684\u6b63\u786e\u6027"}}
