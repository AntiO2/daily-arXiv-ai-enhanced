<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.SE](#cs.SE) [Total: 23]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Hojabr: Towards a Theory of Everything for AI and Data Analytics](https://arxiv.org/abs/2512.23925)
*Amir Shaikhha*

Main category: cs.DB

TL;DR: Hojabr是一个统一的声明性中间语言，集成了关系代数、张量代数和基于约束的推理，通过高阶代数框架统一表达连接、聚合、张量收缩和递归计算，支持跨数据库系统、机器学习框架和编译器基础设施的优化技术复用。


<details>
  <summary>Details</summary>
Motivation: 现代数据分析管道将关系查询、图处理和Tensor计算结合在单个应用中，但现有系统在不同范式、执行模型和研究社区间存在碎片化，导致重复优化工作、有限互操作性以及逻辑抽象与物理执行策略之间的严格分离。

Method: 提出Hojabr作为统一的声明性中间语言，在单个高阶代数框架中集成关系代数、张量代数和基于约束的推理，将连接算法、执行模型和稀疏/密集张量表示等物理选择作为约束特化决策处理，支持与现有声明性语言的双向翻译。

Result: Hojabr通过使语义、结构和代数属性显式化，并支持编译栈的可扩展性，实现了跨数据库系统、机器学习框架和编译器基础设施的系统化推理和优化技术复用。

Conclusion: Hojabr为解决数据分析管道中跨范式碎片化问题提供了一个统一的中间语言框架，通过集成多种代数系统和支持双向翻译，实现了优化技术的系统化复用和跨领域互操作性。

Abstract: Modern data analytics pipelines increasingly combine relational queries, graph processing, and tensor computation within a single application, but existing systems remain fragmented across paradigms, execution models, and research communities. This fragmentation results in repeated optimization efforts, limited interoperability, and strict separation between logical abstractions and physical execution strategies.
  We propose Hojabr as a unified declarative intermediate language to address this problem. Hojabr integrates relational algebra, tensor algebra, and constraint-based reasoning within a single higher-order algebraic framework, in which joins, aggregations, tensor contractions, and recursive computations are expressed uniformly. Physical choices, such as join algorithms, execution models, and sparse versus dense tensor representations, are handled as constraint-specialization decisions rather than as separate formalisms. Hojabr supports bidirectional translation with existing declarative languages, enabling programs to be both lowered into Hojabr for analysis and optimization and lifted back into their original declarative form. By making semantic, structural, and algebraic properties explicit, and by supporting extensibility across the compilation stack, Hojabr enables systematic reasoning and reuse of optimization techniques across database systems, machine learning frameworks, and compiler infrastructures.

</details>


### [2] [High-dimensional Regret Minimization](https://arxiv.org/abs/2512.24078)
*Junyu Liao,Ashwin Lall,Mitsunori Ogihara,Raymond Wong*

Main category: cs.DB

TL;DR: FHDR框架通过快速高维降维技术，在少于30轮交互和0.01秒内解决大规模高维数据集上的交互式查询问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代应用（如住房、金融产品市场）的数据集通常包含数百个属性，现有交互式算法要么无法扩展到高维数据，要么需要过多用户交互（常超过1000轮），这在实际应用中不可行。

Method: 提出FHDR（快速高维降维）框架，通过高效的降维技术减少交互轮数，在保持查询质量的同时大幅降低计算复杂度。

Result: 实验表明FHDR在运行时间上比现有最佳算法至少快一个数量级，在交互轮数上快几个数量级，仅需少于30轮交互和不到0.01秒即可完成查询。

Conclusion: FHDR为可扩展的交互式遗憾最小化建立了新的技术标准，是交互式查询领域的重要突破，首次实现了对高维数据集的高效处理。

Abstract: Multi-criteria decision making in large databases is very important in real world applications. Recently, an interactive query has been studied extensively in the database literature with the advantage of both the top-k query (with limited output size) and the skyline query (which does not require users to explicitly specify their preference function). This approach iteratively asks the user to select the one preferred within a set of options. Based on rounds of feedback, the query learns the implicit preference and returns the most favorable as a recommendation.
  However, many modern applications in areas like housing or financial product markets feature datasets with hundreds of attributes. Existing interactive algorithms either fail to scale or require excessive user interactions (often exceeding 1000 rounds). Motivated by this, we propose FHDR (Fast High-Dimensional Reduction), a novel framework that takes less than 0.01s with fewer than 30 rounds of interaction. It is considered a breakthrough in the field of interactive queries since most, if not all, existing studies are not scalable to high-dimensional datasets.
  Extensive experiments demonstrate that FHDR outperforms the best-known algorithms by at least an order of magnitude in execution time and up to several orders of magnitude in terms of the number of interactions required, establishing a new state of the art for scalable interactive regret minimization.

</details>


### [3] [LMG Index: A Robust Learned Index for Multi-Dimensional Performance Balance](https://arxiv.org/abs/2512.24824)
*Yuzhen Chen,Bin Yao*

Main category: cs.DB

TL;DR: LMIndex是一个鲁棒的机器学习索引框架，通过高效查询/更新顶层结构和最优误差阈值训练算法，以及LMG变体的间隙分配策略，在多维度性能上取得突破性平衡。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习索引大多只优化查询延迟或空间使用等有限目标，忽略了更新效率和稳定性等实际评估维度，且依赖数据分布假设，缺乏理论保证，限制了在真实系统中的通用性。

Method: 提出LMIndex框架：1) 高效查询/更新顶层结构（理论上O(1)复杂度）；2) 高效最优误差阈值训练算法（实践中接近O(1))；3) 开发LMG变体，采用新颖的间隙分配策略提升更新性能和动态工作负载下的稳定性。

Result: LMG在多维度性能上表现优异：批量加载快8.25倍，点查询快1.49倍，范围查询比B+树快4.02倍，读写工作负载更新快1.5倍，稳定性提升82.59倍（变异系数降低），空间使用减少1.38倍。

Conclusion: LMG有效突破了现有方法在多维度性能上的权衡，提供了一个平衡且通用的框架，在查询性能、更新效率、稳定性和空间使用等方面实现了全面优化。

Abstract: Index structures are fundamental for efficient query processing on large-scale datasets. Learned indexes model the indexing process as a prediction problem to overcome the inherent trade-offs of traditional indexes. However, most existing learned indexes optimize only for limited objectives like query latency or space usage, neglecting other practical evaluation dimensions such as update efficiency and stability. Moreover, many learned indexes rely on assumptions about data distributions or workloads, lacking theoretical guarantees when facing unknown or evolving scenarios, which limits their generality in real-world systems.
  In this paper, we propose LMIndex, a robust framework for learned indexing that leverages a efficient query/update top-layer structure (theoretically $O(1)$ when the key type is fixed) and a efficient optimal error threshold training algorithm (approach $O(1)$ in practice). Building upon this, we develop LMG (LMIndex with gaps), a variant employing a novel gap allocation strategy to enhance update performance and maintain stability under dynamic workloads. Extensive evaluations show that LMG achieves competitive or leading performance, including bulk loading (up to 8.25$\times$ faster), point queries (up to 1.49$\times$ faster), range queries (up to 4.02$\times$ faster than B+Tree), update (up to 1.5$\times$ faster on read-write workloads), stability (up to 82.59$\times$ lower coefficient of variation), and space usage (up to 1.38$\times$ smaller). These results demonstrate that LMG effectively breaks the multi-dimensional performance trade-offs inherent in state-of-the-art approaches, offering a balanced and versatile framework.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [AgenticTCAD: A LLM-based Multi-Agent Framework for Automated TCAD Code Generation and Device Optimization](https://arxiv.org/abs/2512.23742)
*Guangxi Fan,Tianliang Ma,Xuguang Sun,Xun Wang,Kain Lu Low,Leilai Shao*

Main category: cs.SE

TL;DR: 提出AgenticTCAD框架，通过多智能体系统实现自然语言驱动的自动化TCAD设备设计与优化，相比人工专家大幅提升效率


<details>
  <summary>Details</summary>
Motivation: 随着先进技术节点的持续微缩，设计-技术协同优化(DTCO)变得日益关键，但TCAD仿真领域缺乏开源资源阻碍了语言模型生成有效TCAD代码

Method: 构建专家策划的开源TCAD数据集，微调领域特定模型用于TCAD代码生成，并在此基础上提出AgenticTCAD——自然语言驱动的多智能体框架，实现端到端自动化设备设计与优化

Result: 在2纳米纳米片FET设计验证中，AgenticTCAD在4.2小时内达到IRDS-2024设备规格，而人类专家使用商业工具需要7.1天

Conclusion: AgenticTCAD框架显著提升了TCAD设备设计与优化的效率和自动化水平，为解决DTCO挑战提供了有效解决方案

Abstract: With the continued scaling of advanced technology nodes, the design-technology co-optimization (DTCO) paradigm has become increasingly critical, rendering efficient device design and optimization essential. In the domain of TCAD simulation, however, the scarcity of open-source resources hinders language models from generating valid TCAD code. To overcome this limitation, we construct an open-source TCAD dataset curated by experts and fine-tune a domain-specific model for TCAD code generation. Building on this foundation, we propose AgenticTCAD, a natural language - driven multi-agent framework that enables end-to-end automated device design and optimization. Validation on a 2 nm nanosheet FET (NS-FET) design shows that AgenticTCAD achieves the International Roadmap for Devices and Systems (IRDS)-2024 device specifications within 4.2 hours, whereas human experts required 7.1 days with commercial tools.

</details>


### [5] [Hybrid-Code: A Privacy-Preserving, Redundant Multi-Agent Framework for Reliable Local Clinical Coding](https://arxiv.org/abs/2512.23743)
*Yunguo Yu*

Main category: cs.SE

TL;DR: Hybrid-Code是一个混合神经符号多智能体框架，用于本地临床编码，结合语言模型语义推理、确定性回退和符号验证，确保生产可靠性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 基于云的LLM临床编码自动化存在隐私风险和延迟瓶颈，不适合本地医疗部署，需要一种可靠且保护隐私的解决方案。

Method: 采用混合神经符号多智能体框架：Coder智能体使用BioMistral-7B进行语义推理，失败时回退到确定性关键词匹配；Auditor智能体验证代码是否符合257个代码知识库和临床证据。

Result: 在1000个MIMIC-III出院摘要上评估：知识库内接受输出无幻觉代码，验证率24.47%，覆盖率34.11%（95% CI: 31.2%-37.0%），语言模型利用率86%+，拒绝率75.53%，确保患者数据不离开医院防火墙。

Conclusion: 混合架构结合语言模型语义理解、确定性回退和符号验证，确保可靠性和隐私保护。在生产医疗系统中，通过冗余实现的可靠性比纯模型性能更有价值。

Abstract: Clinical coding automation using cloud-based Large Language Models (LLMs) poses privacy risks and latency bottlenecks, rendering them unsuitable for on-premise healthcare deployment. We introduce Hybrid-Code, a hybrid neuro-symbolic multi-agent framework for local clinical coding that ensures production reliability through redundancy and verification. Our system comprises two agents: a Coder that attempts language model-based semantic reasoning using BioMistral-7B but falls back to deterministic keyword matching when model output is unreliable, ensuring pipeline completion; and an Auditor that verifies codes against a 257-code knowledge base and clinical evidence. Evaluating on 1,000 MIMIC-III discharge summaries, we demonstrate no hallucinated codes among accepted outputs within the knowledge base, 24.47% verification rate, and 34.11% coverage (95% CI: 31.2%--37.0%) with 86%+ language model utilization. The Auditor filtered invalid format codes and provided evidence-based quality control (75.53% rejection rate) while ensuring no patient data leaves the hospital firewall. The hybrid architecture -- combining language model semantic understanding (when successful), deterministic fallback (when the model fails), and symbolic verification (always active) -- ensures both reliability and privacy preservation, addressing critical barriers to AI adoption in healthcare. Our key finding is that reliability through redundancy is more valuable than pure model performance in production healthcare systems, where system failures are unacceptable.

</details>


### [6] [DEFT: Differentiable Automatic Test Pattern Generation](https://arxiv.org/abs/2512.23746)
*Wei Li,Yan Zou,Yixin Liang,José Moura,Shawn Blanton*

Main category: cs.SE

TL;DR: DEFT将离散ATPG问题转化为连续优化任务，通过可微分的数学重参数化实现梯度驱动的测试模式生成，显著提升难检测故障的覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代IC复杂性导致测试模式数量激增，其中大部分模式针对少数难检测故障，需要新的ATPG算法专门提高对这类故障的测试效果。

Method: 提出DEFT方法，将离散ATPG问题重新表述为连续优化任务，引入数学基础的重参数化使连续目标与离散故障检测语义对齐，支持梯度驱动模式生成。为应对深度电路图的可扩展性和稳定性问题，集成定制CUDA内核实现高效前向-反向传播，并应用梯度归一化缓解梯度消失问题。

Result: 在两个工业基准测试中，与领先商业工具相比，在相同模式预算和可比运行时间下，DEFT将难检测故障检测率平均提高21.1%和48.9%。在部分赋值模式生成等实际ATPG设置中，DEFT生成的模式减少了19.3%的0/1比特，同时仍能检测出35%更多的故障。

Conclusion: DEFT是一种有前景且有效的ATPG引擎，为现有启发式方法提供了有价值的补充，通过可微分优化方法显著提升了难检测故障的测试效果。

Abstract: Modern IC complexity drives test pattern growth, with the majority of patterns targeting a small set of hard-to-detect (HTD) faults. This motivates new ATPG algorithms to improve test effectiveness specifically for HTD faults. This paper presents DEFT (Differentiable Automatic Test Pattern Generation), a new ATPG approach that reformulates the discrete ATPG problem as a continuous optimization task. DEFT introduces a mathematically grounded reparameterization that aligns the expected continuous objective with discrete fault-detection semantics, enabling reliable gradient-based pattern generation. To ensure scalability and stability on deep circuit graphs, DEFT integrates a custom CUDA kernel for efficient forward-backward propagation and applies gradient normalization to mitigate vanishing gradients. Compared to a leading commercial tool on two industrial benchmarks, DEFT improves HTD fault detection by 21.1% and 48.9% on average under the same pattern budget and comparable runtime. DEFT also supports practical ATPG settings such as partial assignment pattern generation, producing patterns with 19.3% fewer 0/1 bits while still detecting 35% more faults. These results indicate DEFT is a promising and effective ATPG engine, offering a valuable complement to existing heuristic.

</details>


### [7] [State-of-the-art Small Language Coder Model: Mify-Coder](https://arxiv.org/abs/2512.23747)
*Abhinav Parmar,Abhisek Panigrahi,Abhishek Kumar Dwivedi,Abhishek Bhattacharya,Adarsh Ramachandra,Aditya Choudhary,Aditya Garg,Aditya Raj,Alankrit Bhatt,Alpesh Yadav,Anant Vishnu,Ananthu Pillai,Ankush Kumar,Aryan Patnaik,Aswatha Narayanan S,Avanish Raj Singh,Bhavya Shree Gadda,Brijesh Pankajbhai Kachhadiya,Buggala Jahnavi,Chidurala Nithin Krishna,Chintan Shah,Chunduru Akshaya,Debarshi Banerjee,Debrup Dey,Deepa R.,Deepika B G,Faiz ur Rahman,Gagan Gayari,Gudhi Jagadeesh Kumar Naidu,Gursimar Singh,Harshal Tyagi,Harshini K,James Mani Vathalloor,Jayarama Nettar,Jayashree Gajjam,Joe Walter Sugil George,Kamalakara Sri Krishna Tadepalli,Kamalkumar Rathinasamy,Karan Chaurasia,Karthikeyan S,Kashish Arora,Kaushal Desai,Khushboo Buwade,Kiran Manjrekar,Malikireddy Venkata Sai Likhitha,Manjunath A,Mitali Mahavir Bedmutha,Mohammed Rafee Tarafdar,Nikhil Tiwari,Nikitha K Gigi,Pavan Ravikumar,Pendyala Swarnanjali,Piyush Anand,Prakash Chandrasekar,Prasanna Bhalchandra Gawade,Prasanth Sivan,Preeti Khurana,Priyanshi Babbar,Rajab Ali Mondal,Rajesh Kumar Vissapragada,Rajeshwari Ganesan,Rajeswari Koppisetti,Ramjee R.,Ramkumar Thiruppathisamy,Rani G. S.,S Reka,Samarth Gupta,Sandeep Reddy Kothakota,Sarathy K,Sathyanarayana Sampath Kumar,Saurabh Kumar,Shashank Khasare,Shenbaga Devi Venkatesh Kumar,Shiva Rama Krishna Parvatham,Shoeb Shaikh,Shrishanmathi A,Shubham Pathak,Sree Samhita Koppaka,Sreenivasa Raghavan K S,Sreeram Venkatasubramanian,Suprabha Desai Bojja,Swetha R,Syed Ahmed,Chinmai Harshitha Thota,Tushar Yadav,Veeravelly Kusumitha,V V S S Prasanth Patnaik,Vidya Sri Sesetti,Vijayakeerthi K,Vikram Raj Bakshi,Vinay K K,Vinoth Kumar Loganathan,Vipin Tiwari,Vivek Kumar Shrivastav,V Venkata Sri Datta Charan,Wasim Akhtar Khan*

Main category: cs.SE

TL;DR: Mify-Coder是一个2.5B参数的代码模型，通过计算最优策略在4.2T token上训练，在代码生成和函数调用基准测试中超越更大模型，证明紧凑模型能达到前沿性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过数据质量和计算效率的优化，让更小的模型在代码生成和智能体驱动工作流中达到与前沿大模型相当的性能，同时降低部署成本。

Method: 基于Mify-2.5B基础模型，采用计算最优训练策略，结合高质量人工标注数据和通过智能体设计提示生成的合成数据，使用LLM质量过滤增强数据密度，探索CPT-SFT目标、数据混合和采样动态。

Result: Mify-Coder在标准代码和函数调用基准测试中显著超越更大的基线模型，达到可比的准确性和安全性，量化变体可在标准桌面环境部署而无需专用硬件。

Conclusion: 通过原则性的数据和计算纪律，小型模型能够实现有竞争力的准确性、效率和安全合规性，为代码智能提供经济高效的解决方案。

Abstract: We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.

</details>


### [8] [Uncovering Discrimination Clusters: Quantifying and Explaining Systematic Fairness Violations](https://arxiv.org/abs/2512.23769)
*Ranit Debnath Akash,Ashish Kumar,Verya Monjezi,Ashutosh Trivedi,Gang,Tan,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 论文提出"歧视聚类"概念，扩展个体公平性检测，不仅能发现单个反事实差异，还能识别输入空间中受保护特征微小变化导致多个不同结果聚类的系统性歧视模式。


<details>
  <summary>Details</summary>
Motivation: 传统个体公平性只关注相似个体应获得相似结果，但仅能检测孤立的公平性违反，无法捕捉影响整个子群的系统性或聚类歧视模式。需要更全面的方法来揭示算法偏见的结构性模式。

Method: 提出HyFair混合技术：结合形式符号分析（通过SMT和MILP求解器）来验证个体公平性，以及随机搜索来发现歧视聚类。引入新颖的解释方法，为高k-不公平性输入生成可解释的决策树风格解释。

Result: 实验表明HyFair在公平性验证和局部解释方法方面优于现有技术。该方法既能提供形式化保证（当无反例时），又能检测符号方法单独难以计算的严重违反情况。

Conclusion: 歧视聚类概念扩展了个体公平性框架，能揭示算法决策中更广泛的系统性偏见模式。HyFair混合方法在公平性验证和解释方面表现出色，为检测和理解算法歧视提供了更全面的工具。

Abstract: Fairness in algorithmic decision-making is often framed in terms of individual fairness, which requires that similar individuals receive similar outcomes. A system violates individual fairness if there exists a pair of inputs differing only in protected attributes (such as race or gender) that lead to significantly different outcomes-for example, one favorable and the other unfavorable. While this notion highlights isolated instances of unfairness, it fails to capture broader patterns of systematic or clustered discrimination that may affect entire subgroups. We introduce and motivate the concept of discrimination clustering, a generalization of individual fairness violations. Rather than detecting single counterfactual disparities, we seek to uncover regions of the input space where small perturbations in protected features lead to k-significantly distinct clusters of outcomes. That is, for a given input, we identify a local neighborhood-differing only in protected attributes-whose members' outputs separate into many distinct clusters. These clusters reveal significant arbitrariness in treatment solely based on protected attributes that help expose patterns of algorithmic bias that elude pairwise fairness checks. We present HyFair, a hybrid technique that combines formal symbolic analysis (via SMT and MILP solvers) to certify individual fairness with randomized search to discover discriminatory clusters. This combination enables both formal guarantees-when no counterexamples exist-and the detection of severe violations that are computationally challenging for symbolic methods alone. Given a set of inputs exhibiting high k-unfairness, we introduce a novel explanation method to generate interpretable, decision-tree-style artifacts. Our experiments demonstrate that HyFair outperforms state-of-the-art fairness verification and local explanation methods.

</details>


### [9] [Test Case Specification Techniques and System Testing Tools in the Automotive Industry: A Review](https://arxiv.org/abs/2512.23780)
*Denesa Zyberaj,Pascal Hirmer,Marco Aiello,Stefan Wagner*

Main category: cs.SE

TL;DR: 论文提出汽车软件测试方法学挑战，通过系统文献综述和行业经验分析，建立技术/工具选择目录，推荐模型化规划、可互操作工具链等解决方案。


<details>
  <summary>Details</summary>
Motivation: 汽车领域向软件中心开发转型，导致嵌入式系统复杂性增加，测试能力紧张。现有标准下缺乏跨异构、遗留约束工具链的连贯系统测试方法学，实践依赖个人经验而非系统策略。

Method: 通过系统文献综述(SLR)结合行业经验，识别挑战和需求，映射到测试用例规范技术和测试工具，使用PRISMA评估其在汽车测试中的适用性。

Result: 建立了支持技术/工具选择的精选目录，识别出生命周期中的九个重复挑战领域（如需求质量、可变性管理、工具链碎片化），提供优先标准目录。

Conclusion: 推荐模型化规划、可互操作和可追溯工具链、需求提升、实用自动化和虚拟化、针对性AI和形式化方法、可操作指标、轻量级组织实践作为解决方案。

Abstract: The automotive domain is shifting to software-centric development to meet regulation, market pressure, and feature velocity. This shift increases embedded systems' complexity and strains testing capacity. Despite relevant standards, a coherent system-testing methodology that spans heterogeneous, legacy-constrained toolchains remains elusive, and practice often depends on individual expertise rather than a systematic strategy. We derive challenges and requirements from a systematic literature review (SLR), complemented by industry experience and practice. We map them to test case specification techniques and testing tools, evaluating their suitability for automotive testing using PRISMA. Our contribution is a curated catalog that supports technique/tool selection and can inform future testing frameworks and improvements. We synthesize nine recurring challenge areas across the life cycle, such as requirements quality and traceability, variability management, and toolchain fragmentation. We then provide a prioritized criteria catalog that recommends model-based planning, interoperable and traceable toolchains, requirements uplift, pragmatic automation and virtualization, targeted AI and formal methods, actionable metrics, and lightweight organizational practices.

</details>


### [10] [A Systematic Mapping on Software Fairness: Focus, Trends and Industrial Context](https://arxiv.org/abs/2512.23782)
*Kessia Nepomuceno,Fabio Petrillo*

Main category: cs.SE

TL;DR: 本文通过系统文献映射，分析了软件工程中公平性研究的现状，发现研究主要集中在算法方法、后处理和群体公平性，缺乏早期干预、个体公平性指标和工业应用。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程领域的发展，系统公平性已成为关键关注点。尽管已有一些指导原则，但对软件系统中确保公平性的研究解决方案仍缺乏全面理解。本文旨在探索和分类软件工程中公平性解决方案的当前进展。

Method: 采用系统文献映射方法，开发了一个分类框架，从新视角组织软件公平性研究，应用于95篇选定研究，并分析其工业采用潜力。

Result: 研究发现：软件公平性研究正在扩展，但主要集中在方法和算法上；主要关注后处理和群体公平性，较少关注早期干预、个体公平性指标和偏见根源理解；研究主要在学术界，工业合作有限，技术就绪水平低到中等，工业可转移性仍遥远。

Conclusion: 研究结果强调需要在软件开发生命周期的所有阶段纳入公平性考虑，并促进学术界与工业界之间更紧密的合作。该分析提供了该领域的全面概述，为未来研究和软件系统中公平性的实际应用奠定了基础。

Abstract: Context: Fairness in systems has emerged as a critical concern in software engineering, garnering increasing attention as the field has advanced in recent years. While several guidelines have been proposed to address fairness, achieving a comprehensive understanding of research solutions for ensuring fairness in software systems remains challenging. Objectives: This paper presents a systematic literature mapping to explore and categorize current advancements in fairness solutions within software engineering, focusing on three key dimensions: research trends, research focus, and viability in industrial contexts. Methods: We develop a classification framework to organize research on software fairness from a fresh perspective, applying it to 95 selected studies and analyzing their potential for industrial adoption. Results: Our findings reveal that software fairness research is expanding, yet it remains heavily focused on methods and algorithms. It primarily focuses on post-processing and group fairness, with less emphasis on early-stage interventions, individual fairness metrics, and understanding bias root causes. Additionally fairness research remains largely academic, with limited industry collaboration and low to medium Technology Readiness Level (TRL), indicating that industrial transferability remains distant. Conclusion: Our results underscore the need to incorporate fairness considerations across all stages of the software development life-cycle and to foster greater collaboration between academia and industry. This analysis provides a comprehensive overview of the field, offering a foundation to guide future research and practical applications of fairness in software systems.

</details>


### [11] [From Correctness to Collaboration: Toward a Human-Centered Framework for Evaluating AI Agent Behavior in Software Engineering](https://arxiv.org/abs/2512.23844)
*Tao Dong,Harini Sampath,Ja Young Lee,Sherry Y. Shi,Andrew Macvean*

Main category: cs.SE

TL;DR: 该论文提出了一种评估LLM作为软件工程协作伙伴的新框架，包括行为分类法和上下文自适应行为框架，以弥补当前仅关注代码正确性的评估不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从代码生成器演变为软件工程师的协作伙伴，当前的评估方法滞后。现有基准测试仅关注代码正确性，未能捕捉成功人机协作所需的细微交互行为。

Method: 1. 分析91组用户定义的代理规则，提出企业软件工程中理想代理行为的基础分类法；2. 引入上下文自适应行为框架，揭示行为期望如何沿两个经验推导的轴变化：时间范围和任务类型。

Result: 建立了包含四个关键期望的行为分类法：遵循标准和流程、确保代码质量和可靠性、有效解决问题、与用户协作。同时开发了CAB框架，显示行为期望如何随上下文动态变化。

Conclusion: 该研究为设计和评估下一代AI代理提供了以人为本的基础，将领域焦点从生成代码的正确性转向真正的协作智能动态，填补了当前评估方法的空白。

Abstract: As Large Language Models (LLMs) evolve from code generators into collaborative partners for software engineers, our methods for evaluation are lagging. Current benchmarks, focused on code correctness, fail to capture the nuanced, interactive behaviors essential for successful human-AI partnership. To bridge this evaluation gap, this paper makes two core contributions. First, we present a foundational taxonomy of desirable agent behaviors for enterprise software engineering, derived from an analysis of 91 sets of user-defined agent rules. This taxonomy defines four key expectations of agent behavior: Adhere to Standards and Processes, Ensure Code Quality and Reliability, Solving Problems Effectively, and Collaborating with the User.
  Second, recognizing that these expectations are not static, we introduce the Context-Adaptive Behavior (CAB) Framework. This emerging framework reveals how behavioral expectations shift along two empirically-derived axes: the Time Horizon (from immediate needs to future ideals), established through interviews with 15 expert engineers, and the Type of Work (from enterprise production to rapid prototyping, for example), identified through a prompt analysis of a prototyping agent. Together, these contributions offer a human-centered foundation for designing and evaluating the next generation of AI agents, moving the field's focus from the correctness of generated code toward the dynamics of true collaborative intelligence.

</details>


### [12] [From Illusion to Insight: Change-Aware File-Level Software Defect Prediction Using Agentic AI](https://arxiv.org/abs/2512.23875)
*Mohsen Hesamolhokama,Behnam Rohani,Amirahmad Shafiee,MohammadAmin Fazli,Jafar Habibi*

Main category: cs.SE

TL;DR: 该论文指出传统文件级软件缺陷预测存在准确率幻觉，提出基于代码变更的预测方法和LLM驱动的多智能体辩论框架，显著提升对缺陷引入的敏感性。


<details>
  <summary>Details</summary>
Motivation: 传统软件缺陷预测方法存在严重问题：大多数文件在多个版本中持续存在并保持缺陷标签，标准评估方法奖励的是标签持续性偏差而非对代码变更的推理，导致报告的性能提升实际上是准确率幻觉。

Method: 1. 将软件缺陷预测重新定义为变更感知的预测任务，模型需要在连续项目版本中推理文件的代码变更，而非依赖静态文件快照；2. 提出LLM驱动的、变更感知的多智能体辩论框架。

Result: 在多个PROMISE项目上的实验表明：传统模型在F1分数上表现虚高，但在关键的缺陷转换案例上失败；而变更感知推理和多智能体辩论框架在不同演化子集上表现更均衡，显著提高了对缺陷引入的敏感性。

Conclusion: 当前软件缺陷预测评估实践存在根本性缺陷，需要在实践中采用变更感知的推理方法。提出的变更感知多智能体辩论框架为解决这一问题提供了有效方案。

Abstract: Much of the reported progress in file-level software defect prediction (SDP) is, in reality, nothing but an illusion of accuracy. Over the last decades, machine learning and deep learning models have reported increasing performance across software versions. However, since most files persist across releases and retain their defect labels, standard evaluation rewards label-persistence bias rather than reasoning about code changes. To address this issue, we reformulate SDP as a change-aware prediction task, in which models reason over code changes of a file within successive project versions, rather than relying on static file snapshots. Building on this formulation, we propose an LLM-driven, change-aware, multi-agent debate framework. Our experiments on multiple PROMISE projects show that traditional models achieve inflated F1, while failing on rare but critical defect-transition cases. In contrast, our change-aware reasoning and multi-agent debate framework yields more balanced performance across evolution subsets and significantly improves sensitivity to defect introductions. These results highlight fundamental flaws in current SDP evaluation practices and emphasize the need for change-aware reasoning in practical defect prediction. The source code is publicly available.

</details>


### [13] [Coding With AI: From a Reflection on Industrial Practices to Future Computer Science and Software Engineering Education](https://arxiv.org/abs/2512.23982)
*Hung-Fu Chang,MohammadShokrolah Shirazi,Lizhou Cao,Supannika Koolmanojwong Mobasser*

Main category: cs.SE

TL;DR: 该研究通过分析57个YouTube视频，探讨了LLM编码工具在专业实践中的应用、相关风险以及对软件开发工作流程的影响，特别关注对计算机教育的启示。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注个体层面或教育环境中的AI编码，缺乏对工业从业者视角的深入探索。本研究旨在填补这一空白，了解LLM编码工具在专业实践中的实际应用、相关风险和担忧，以及开发工作流程的转变。

Method: 对2024年末至2025年间发布的57个精选YouTube视频进行定性分析，这些视频记录了从业者的反思和经验。经过筛选和质量评估后，分析比较了基于LLM的编程与传统编程，识别了新兴风险，并描述了不断演变的工作流程。

Result: 研究发现：1）定义了AI编码实践；2）显著的生产力提升和进入门槛降低；3）开发瓶颈转向代码审查；4）从业者担忧代码质量、可维护性、安全漏洞、伦理问题、基础问题解决能力侵蚀以及初级工程师准备不足。

Conclusion: 研究为计算机科学和软件工程教育提供了启示，主张课程改革应转向问题解决、架构思维、代码审查以及早期整合LLM工具的项目式学习，使教育实践与快速演变的专业现实保持一致。

Abstract: Recent advances in large language models (LLMs) have introduced new paradigms in software development, including vibe coding, AI-assisted coding, and agentic coding, fundamentally reshaping how software is designed, implemented, and maintained. Prior research has primarily examined AI-based coding at the individual level or in educational settings, leaving industrial practitioners' perspectives underexplored. This paper addresses this gap by investigating how LLM coding tools are used in professional practice, the associated concerns and risks, and the resulting transformations in development workflows, with particular attention to implications for computing education. We conducted a qualitative analysis of 57 curated YouTube videos published between late 2024 and 2025, capturing reflections and experiences shared by practitioners. Following a filtering and quality assessment process, the selected sources were analyzed to compare LLM-based and traditional programming, identify emerging risks, and characterize evolving workflows. Our findings reveal definitions of AI-based coding practices, notable productivity gains, and lowered barriers to entry. Practitioners also report a shift in development bottlenecks toward code review and concerns regarding code quality, maintainability, security vulnerabilities, ethical issues, erosion of foundational problem-solving skills, and insufficient preparation of entry-level engineers. Building on these insights, we discuss implications for computer science and software engineering education and argue for curricular shifts toward problem-solving, architectural thinking, code review, and early project-based learning that integrates LLM tools. This study offers an industry-grounded perspective on AI-based coding and provides guidance for aligning educational practices with rapidly evolving professional realities.

</details>


### [14] [Developing controlled natural language for formal specification patterns using AI assistants](https://arxiv.org/abs/2512.24159)
*Natalia Garanina,Vladimir Zyubin,Igor Anureev*

Main category: cs.SE

TL;DR: 提出一种基于AI助手的方法，通过形式化规范模式构建受控自然语言需求，包含三个阶段：编译通用模板、生成语料库、形式化语法。


<details>
  <summary>Details</summary>
Motivation: 传统需求工程中自然语言需求存在歧义和不一致问题，而形式化方法又难以被领域专家理解。需要一种既能保持形式化精确性，又能让非技术专家理解的中间表示方法。

Method: 三阶段方法：1) 编译包含形式化模板所有属性的通用自然语言需求模式；2) 使用AI助手通过部分属性评估生成自然语言需求模式语料库；3) 基于结果模式的语法结构分析，形式化受控自然语言的语法。

Result: 该方法已针对事件驱动的时序需求进行了测试，能够系统性地构建受控自然语言，将形式化规范转化为更易理解的自然语言表示。

Conclusion: AI辅助的受控自然语言构建方法为需求工程提供了有效的桥梁，既能保持形式化精确性，又能提高可理解性，特别适用于事件驱动的时序需求场景。

Abstract: Using an AI assistant, we developed a method for systematically constructing controlled natural language for requirements based on formal specification patterns containing logical attributes. The method involves three stages: 1) compiling a generalized natural language requirement pattern that utilizes all attributes of the formal specification template; 2) generating, using the AI assistant, a corpus of natural language requirement patterns, reduced by partially evaluating attributes (the developed prompt utilizes the generalized template, attribute definitions, and specific formal semantics of the requirement patterns); and 3) formalizing the syntax of the controlled natural language based on an analysis of the grammatical structure of the resulting patterns. The method has been tested for event-driven temporal requirements.

</details>


### [15] [CoHalLo: code hallucination localization via probing hidden layer vector](https://arxiv.org/abs/2512.24183)
*Nan Jia,Wangchao Sang,Pengfei Lin,Xiangping Chen,Yuan Huang,Yi Liu,Mingliang Li*

Main category: cs.SE

TL;DR: CoHalLo：一种通过探测幻觉检测模型的隐藏层向量来实现行级代码幻觉定位的新方法，通过比较预测抽象语法树和原始抽象语法树来定位幻觉代码行。


<details>
  <summary>Details</summary>
Motivation: 现有代码幻觉检测方法大多是粗粒度的，缺乏专门的细粒度幻觉定位技术，难以帮助开发者高效提高AI生成代码的可靠性。

Method: 首先在人工标注数据集上微调幻觉检测模型，使其学习代码语法特征；然后设计探针网络将高维潜在向量投影到低维语法子空间，生成向量元组并重构预测抽象语法树；通过比较预测AST和原始AST来识别与幻觉相关的关键语法结构，从而定位幻觉代码行。

Result: CoHalLo在手动收集的代码幻觉数据集上表现优异：Top-1准确率0.4253，Top-3准确率0.6149，Top-5准确率0.7356，Top-10准确率0.8333，IFA为5.73，Recall@1% Effort为0.052721，Effort@20% Recall为0.155269，均优于基线方法。

Conclusion: CoHalLo通过探测幻觉检测模型的隐藏层向量，成功实现了行级代码幻觉定位，为开发者提供了更精细的代码可靠性改进工具。

Abstract: The localization of code hallucinations aims to identify specific lines of code containing hallucinations, helping developers to improve the reliability of AI-generated code more efficiently. Although recent studies have adopted several methods to detect code hallucination, most of these approaches remain limited to coarse-grained detection and lack specialized techniques for fine-grained hallucination localization. This study introduces a novel method, called CoHalLo, which achieves line-level code hallucination localization by probing the hidden-layer vectors from hallucination detection models. CoHalLo uncovers the key syntactic information driving the model's hallucination judgments and locates the hallucinating code lines accordingly. Specifically, we first fine-tune the hallucination detection model on manually annotated datasets to ensure that it learns features pertinent to code syntactic information. Subsequently, we designed a probe network that projects high-dimensional latent vectors onto a low-dimensional syntactic subspace, generating vector tuples and reconstructing the predicted abstract syntax tree (P-AST). By comparing P-AST with the original abstract syntax tree (O-AST) extracted from the input AI-generated code, we identify the key syntactic structures associated with hallucinations. This information is then used to pinpoint hallucinated code lines. To evaluate CoHalLo's performance, we manually collected a dataset of code hallucinations. The experimental results show that CoHalLo achieves a Top-1 accuracy of 0.4253, Top-3 accuracy of 0.6149, Top-5 accuracy of 0.7356, Top-10 accuracy of 0.8333, IFA of 5.73, Recall@1% Effort of 0.052721, and Effort@20% Recall of 0.155269, which outperforms the baseline methods.

</details>


### [16] ["Game Changer" or "Overenthusiastic Drunk Acquaintance"? Generative AI Use by Blind and Low Vision Software Professionals in the Workplace](https://arxiv.org/abs/2512.24462)
*Yoonha Cha,Victoria Jackson,Lauren Shu,Stacy Branham,André van der Hoek*

Main category: cs.SE

TL;DR: 研究通过39位视障软件专业人士访谈，探讨生成式AI对其软件开发工作的影响，发现虽然能提高生产力和可访问性，但也面临幻觉风险和组织政策限制，呈现高风险高回报的权衡


<details>
  <summary>Details</summary>
Motivation: 软件开发工作场所对视障专业人士存在技术和协作可访问性挑战，虽然生成式AI在软件开发行业日益普及，但尚未咨询视障专业人士的独特视角

Method: 对39位视障软件专业人士进行半结构化访谈的定性研究，探讨生成式AI引入对其工作的意义

Result: 视障专业人士将生成式AI用于多种开发任务，获得生产力提升和可访问性改善，但比明眼同事更容易受到AI幻觉影响，有时组织政策也限制使用

Conclusion: 视障专业人士在使用生成式AI工具时需要仔细权衡高风险和高回报，研究揭示了这一特殊群体面临的独特挑战和机遇

Abstract: The software development workplace poses numerous technical and collaborative accessibility challenges for blind and low vision software professionals (BLVSPs). Though Generative AI (GenAI) is increasingly adopted within the software development industry and has been a rapidly growing topic of interest in research, to date, the unique perspectives of BLVSPs have yet to be consulted. We report on a qualitative study involving 39 semi-structured interviews with BLVSPs about what the introduction of GenAI has meant for their work. We found that BLVSPs used GenAI for many software development tasks, resulting in benefits such as increased productivity and accessibility. However, significant costs were also accompanied by GenAI use as they were more vulnerable to hallucinations than their sighted colleagues. Sometimes, organizational policies prevented use. Based on our findings, we discuss the higher-risks and higher-returns that BLVSPs had to carefully weigh when deciding whether and when to use GenAI tools for work.

</details>


### [17] [A Magnified View into Heterogeneous-ISA Thread Migration Performance without State Transformation](https://arxiv.org/abs/2512.24530)
*Nikolaos Mavrogeorgis,Christos Vasiladiotis,Pei Mu,Amir Khordadi,Björn Franke,Antonio Barbalace*

Main category: cs.SE

TL;DR: Unifico是一个多ISA编译器，通过保持跨架构的相同栈布局来消除运行时栈转换开销，相比现有方案大幅减少二进制大小开销。


<details>
  <summary>Details</summary>
Motivation: 异构ISA处理器设计需要软件支持来桥接ISA异构性，但缺乏支持异构ISA目标的编译工具链阻碍了该领域研究。现有方案中运行时栈转换成本过高。

Method: 设计开发Unifico编译器，生成在两种架构上执行时保持相同栈布局的二进制文件，避免运行时栈转换，维护统一的ABI和虚拟地址空间。基于LLVM实现，针对x86-64和ARMv8 ISA。

Result: 在NAS基准测试中，Unifico引入的平均开销小于6%（高端处理器）和10%（低端处理器）。相比现有Popcorn编译器，将二进制大小开销从~200%降低到~10%，同时消除了ISA迁移时的栈转换开销。

Conclusion: Unifico通过消除运行时栈转换开销，为异构ISA处理器提供了高效的编译支持，显著降低了二进制大小开销，其设计特性可进一步优化以减轻性能影响。

Abstract: Heterogeneous-ISA processor designs have attracted considerable research interest. However, unlike their homogeneous-ISA counterparts, explicit software support for bridging ISA heterogeneity is required. The lack of a compilation toolchain ready to support heterogeneous-ISA targets has been a major factor hindering research in this exciting emerging area. For any such compiler, "getting right" the mechanics involved in state transformation upon migration and doing this efficiently is of critical importance. In particular, any runtime conversion of the current program stack from one architecture to another would be prohibitively expensive. In this paper, we design and develop Unifico, a new multi-ISA compiler that generates binaries that maintain the same stack layout during their execution on either architecture. Unifico avoids the need for runtime stack transformation, thus eliminating overheads associated with ISA migration. Additional responsibilities of the Unifico compiler backend include maintenance of a uniform ABI and virtual address space across ISAs. Unifico is implemented using the LLVM compiler infrastructure, and we are currently targeting the x86-64 and ARMv8 ISAs. We have evaluated Unifico across a range of compute-intensive NAS benchmarks and show its minimal impact on overall execution time, where less than 6% (10%) overhead is introduced on average for high-end (low-end) processors. We also analyze the performance impact of Unifico's key design features and demonstrate that they can be further optimized to mitigate this impact. When compared against the state-of-the-art Popcorn compiler, Unifico reduces binary size overhead from ~200% to ~10%, whilst eliminating the stack transformation overhead during ISA migration.

</details>


### [18] [Localized Calibrated Uncertainty in Code Language Models](https://arxiv.org/abs/2512.24560)
*David Gros,Prem Devanbu*

Main category: cs.SE

TL;DR: 该研究开发了定位LLM生成代码中与用户意图不符部分的技术，通过创建"最小意图对齐补丁"数据集，比较了白盒探测与黑盒反射方法在预测代码编辑位置上的效果。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码可能偏离用户意图，需要监督和编辑。为了支持这一过程，需要技术来定位生成代码中可能不匹配用户意图的部分。

Method: 首先创建包含"最小意图对齐补丁"的数据集，使用测试用例验证正确性。然后比较多种技术：白盒探测（提出高效任意跨度查询技术）、黑盒反射和基于自一致性的方法，评估它们为代码编辑部分分配校准概率的能力。

Result: 研究发现，使用小型监督模型的探针能够实现较低的校准误差，Brier技能得分约为0.2，能够有效预测由大模型生成的代码中需要编辑的行。探针仅通过代码训练就能在一定程度上泛化到自然语言错误。

Conclusion: 该技术为AI监督和控制提供了新方法，小型监督模型能够有效定位大模型生成代码中的错误，且显示出一定的跨领域泛化能力。

Abstract: Large Language models (LLMs) can generate complicated source code from natural language prompts. However, LLMs can generate output that deviates from what the user wants, requiring supervision and editing. To support this process, we offer techniques to localize where generations might be misaligned from user intent. We first create a dataset of "Minimal Intent Aligning Patches" of repaired LLM generated programs. Each program uses test cases to verify correctness. After creating a dataset of programs, we measure how well various techniques can assign a well-calibrated probability to indicate which parts of code will be edited in a minimal patch (i.e., give a probability that corresponds with empirical odds it is edited). We compare white-box probing (where we propose a technique for efficient arbitrary-span querying), against black-box reflective and self-consistency based approaches. We find probes with a small supervisor model can achieve low calibration error and Brier Skill Score of approx 0.2 estimating edited lines on code generated by models many orders of magnitude larger. We discuss the generalizability of the techniques, and the connections to AI oversight and control, finding a probe trained only on code shows some signs of generalizing to natural language errors if new probability scaling is allowed.

</details>


### [19] [On the Effectiveness of Training Data Optimization for LLM-based Code Generation: An Empirical Study](https://arxiv.org/abs/2512.24570)
*Shiqi Kuang,Zhao Tian,Tao Xiao,Dong Wang,Junjie Chen*

Main category: cs.SE

TL;DR: 本文首次大规模实证研究了五种常用训练数据优化技术及其组合对LLM代码生成的影响，发现数据合成在功能正确性上最有效，而数据合成与数据重构的组合表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多种训练数据优化技术被提出以提升代码生成质量，但这些技术的整体效果尚未得到系统评估。为了填补这一空白，本文旨在通过大规模实证研究，为LLM代码生成的训练数据优化提供系统理解和实践指导。

Method: 对五种广泛使用的训练数据优化技术及其两两组合进行大规模实证研究，涵盖三个基准测试和四个大型语言模型，通过细粒度分析深入探讨各项技术及其组合对代码生成效果的影响。

Result: 数据合成在提升功能正确性和减少代码异味方面最有效，但在代码可维护性上相对较差；大多数组合不能进一步提升功能正确性，但能有效提升代码质量；数据合成与数据重构的组合在所有组合中表现最强。

Conclusion: 本研究为训练数据优化和组合策略提供了系统理解，为未来LLM代码生成的研究和部署提供了实用指导，是迈向系统化训练数据优化的第一步。

Abstract: Large language models (LLMs) have achieved remarkable progress in code generation, largely driven by the availability of high-quality code datasets for effective training. To further improve data quality, numerous training data optimization techniques have been proposed; however, their overall effectiveness has not been systematically evaluated. To bridge this gap, we conduct the first large-scale empirical study, examining five widely-used training data optimization techniques and their pairwise combinations for LLM-based code generation across three benchmarks and four LLMs. Our results show that data synthesis is the most effective technique for improving functional correctness and reducing code smells, although it performs relatively worse on code maintainability compared to data refactoring, cleaning, and selection. Regarding combinations, we find that most combinations do not further improve functional correctness but can effectively enhance code quality (code smells and maintainability). Among all combinations, data synthesis combined with data refactoring achieves the strongest overall performance. Furthermore, our fine-grained analysis reinforces these findings and provides deeper insights into how individual techniques and their combinations influence code generation effectiveness. Overall, this work represents a first step toward a systematic understanding of training data optimization and combination strategies, offering practical guidance for future research and deployment in LLM-based code generation.

</details>


### [20] [A Tale of 1001 LoC: Potential Runtime Error-Guided Specification Synthesis for Verifying Large-Scale Programs](https://arxiv.org/abs/2512.24594)
*Zhongyi Wang,Tengjie Lin,Mingshuai Chen,Haokun Li,Mingqi Yang,Xiao Yi,Shengchao Qin,Yixing Luo,Xiaofeng Li,Bin Gu,Liqiang Lu,Jianwei Yin*

Main category: cs.SE

TL;DR: Preguss是一个模块化、细粒度的框架，通过结合静态分析和演绎验证，自动化生成和精化形式化规约，显著减少了大规模程序验证的人工工作量。


<details>
  <summary>Details</summary>
Motivation: 大规模软件和硬件系统的全自动验证是形式化方法的终极目标。虽然大语言模型在增强形式化验证自动化方面显示出潜力，但由于长上下文推理限制和推断复杂过程间规约的困难，其可扩展性较差。

Method: Preguss采用分治策略，结合静态分析和演绎验证：1) 基于潜在运行时错误指导构建和优先处理验证单元；2) 在单元级别使用LLM辅助合成过程间规约。

Result: Preguss显著优于最先进的基于LLM的方法，能够对超过千行代码的实际程序进行高度自动化的RTE无错误验证，减少了80.6%~88.9%的人工验证工作量。

Conclusion: Preguss通过模块化、细粒度的框架有效解决了LLM在形式化验证中的可扩展性问题，为实现大规模系统自动化验证提供了有前景的解决方案。

Abstract: Fully automated verification of large-scale software and hardware systems is arguably the holy grail of formal methods. Large language models (LLMs) have recently demonstrated their potential for enhancing the degree of automation in formal verification by, e.g., generating formal specifications as essential to deductive verification, yet exhibit poor scalability due to long-context reasoning limitations and, more importantly, the difficulty of inferring complex, interprocedural specifications. This paper presents Preguss -- a modular, fine-grained framework for automating the generation and refinement of formal specifications. Preguss synergizes between static analysis and deductive verification by steering two components in a divide-and-conquer fashion: (i) potential runtime error-guided construction and prioritization of verification units, and (ii) LLM-aided synthesis of interprocedural specifications at the unit level. We show that Preguss substantially outperforms state-of-the-art LLM-based approaches and, in particular, it enables highly automated RTE-freeness verification for real-world programs with over a thousand LoC, with a reduction of 80.6%~88.9% human verification effort.

</details>


### [21] [How Do Agentic AI Systems Address Performance Optimizations? A BERTopic-Based Analysis of Pull Requests](https://arxiv.org/abs/2512.24630)
*Md Nahidul Islam Opu,Shahidul Islam,Muhammad Asaduzzaman,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 该论文通过实证研究分析了AI代理生成的性能相关PR，识别了52个性能主题和10个高层类别，发现AI代理在不同软件栈层次应用性能优化，且优化类型显著影响PR接受率和审查时间。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的软件工程影响现代软件开发，除了正确性外，先前研究也考察了AI代理生成的软件工件的性能。然而，目前尚不清楚代理式AI系统在实践中如何具体处理性能问题，因此需要实证研究AI代理生成的性能相关PR。

Method: 采用LLM辅助检测和基于BERTopic的主题建模方法，识别性能相关的pull requests，将52个性能相关主题分组为10个高层类别。

Result: 研究发现：1) AI代理在软件栈的不同层次应用性能优化；2) 优化类型显著影响PR接受率和审查时间；3) AI代理的性能优化主要发生在开发阶段，维护阶段关注较少。

Conclusion: 研究结果为评估和改进代理式AI系统的性能优化行为和审查结果提供了实证证据，有助于更好地理解和优化AI代理在软件性能方面的实践。

Abstract: LLM-based software engineering is influencing modern software development. In addition to correctness, prior studies have also examined the performance of software artifacts generated by AI agents. However, it is unclear how exactly the agentic AI systems address performance concerns in practice. In this paper, we present an empirical study of performance-related pull requests generated by AI agents. Using LLM-assisted detection and BERTopic-based topic modeling, we identified 52 performance-related topics grouped into 10 higher-level categories. Our results show that AI agents apply performance optimizations across diverse layers of the software stack and that the type of optimization significantly affects pull request acceptance rates and review times. We also found that performance optimization by AI agents primarily occurs during the development phase, with less focus on the maintenance phase. Our findings provide empirical evidence that can support the evaluation and improvement of agentic AI systems with respect to their performance optimization behaviors and review outcomes.

</details>


### [22] [DynaFix: Iterative Automated Program Repair Driven by Execution-Level Dynamic Information](https://arxiv.org/abs/2512.24635)
*Zhili Huang,Ling Xu,Chao Liu,Weifeng Sun,Xu Zhang,Yan Lei,Meng Yan,Hongyu Zhang*

Main category: cs.SE

TL;DR: DynaFix：一种基于执行级动态信息的迭代式程序修复方法，通过运行时信息（变量状态、控制流路径、调用栈）指导LLM生成补丁，显著提升修复效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动程序修复方法主要依赖静态分析，忽略运行时行为；即使尝试融入动态信号，也仅限于训练阶段或单次提示注入，无法有效利用细粒度执行信息。现有迭代修复框架通常依赖粗粒度反馈（如通过/失败结果），无法模拟人类逐步调试过程，限制了多步推理和复杂bug修复能力。

Method: DynaFix是一种执行级动态信息驱动的APR方法，在每轮修复中捕获运行时信息（变量状态、控制流路径、调用栈），将其转化为结构化提示来指导LLM生成候选补丁。如果补丁验证失败，重新执行修改后的程序收集新的执行信息进行下一轮尝试，形成类似人类逐步调试的迭代循环。

Result: 在Defects4J v1.2和v2.0基准测试中，DynaFix修复了186个单函数bug，比最先进基线提升10%，其中包括38个之前未修复的bug。最多35次尝试内获得正确补丁，相比现有方法将补丁搜索空间减少70%，在修复复杂bug方面展现出有效性和效率。

Conclusion: DynaFill通过迭代利用细粒度执行级动态信息，显著提升了自动程序修复的效果，能够更好地模拟人类调试过程，在复杂bug修复和多步推理方面表现出色，为基于LLM的APR方法提供了新的方向。

Abstract: Automated Program Repair (APR) aims to automatically generate correct patches for buggy programs. Recent approaches leveraging large language models (LLMs) have shown promise but face limitations. Most rely solely on static analysis, ignoring runtime behaviors. Some attempt to incorporate dynamic signals, but these are often restricted to training or fine-tuning, or injected only once into the repair prompt, without iterative use. This fails to fully capture program execution. Current iterative repair frameworks typically rely on coarse-grained feedback, such as pass/fail results or exception types, and do not leverage fine-grained execution-level information effectively. As a result, models struggle to simulate human stepwise debugging, limiting their effectiveness in multi-step reasoning and complex bug repair.
  To address these challenges, we propose DynaFix, an execution-level dynamic information-driven APR method that iteratively leverages runtime information to refine the repair process. In each repair round, DynaFix captures execution-level dynamic information such as variable states, control-flow paths, and call stacks, transforming them into structured prompts to guide LLMs in generating candidate patches. If a patch fails validation, DynaFix re-executes the modified program to collect new execution information for the next attempt. This iterative loop incrementally improves patches based on updated feedback, similar to the stepwise debugging practices of human developers. We evaluate DynaFix on the Defects4J v1.2 and v2.0 benchmarks. DynaFix repairs 186 single-function bugs, a 10% improvement over state-of-the-art baselines, including 38 bugs previously unrepaired. It achieves correct patches within at most 35 attempts, reducing the patch search space by 70% compared with existing methods, thereby demonstrating both effectiveness and efficiency in repairing complex bugs.

</details>


### [23] [How Do Agentic AI Systems Deal With Software Energy Concerns? A Pull Request-Based Study](https://arxiv.org/abs/2512.24636)
*Tanjum Motin Mitul,Md. Masud Mazumder,Md Nahidul Islam Opu,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 研究分析了AI编码代理在生成软件拉取请求时的能源意识，发现虽然AI代理本身能耗高，但在生成代码时确实表现出能源意识，不过能源优化相关的PR接受率较低。


<details>
  <summary>Details</summary>
Motivation: 随着软件工程进入SE 3.0时代，AI编码代理日益自动化软件开发流程，但尚不清楚这些代理如何识别和处理软件能源问题。考虑到大规模数据中心、能耗高的语言模型和电池受限设备，软件能源问题变得越来越重要。

Method: 使用公开数据集分析代理生成的拉取请求（PRs），识别出216个明确涉及能源的PRs，进行主题分析并构建能源意识工作的分类法，进一步分析应用的优化技术。

Result: 大多数优化技术与现有研究建议一致；虽然构建和运行AI代理本身能耗很高，但它们在生成软件工件时表现出能源意识；然而，能源优化相关的PRs接受率低于其他PRs，主要因为对可维护性产生负面影响。

Conclusion: AI编码代理在生成软件时确实具有能源意识，但能源优化相关的代码修改往往因可维护性问题而被拒绝，需要在能源效率和代码质量之间找到平衡。

Abstract: As Software Engineering enters its new era (SE 3.0), AI coding agents increasingly automate software development workflows. However, it remains unclear how exactly these agents recognize and address software energy concerns-an issue growing in importance due to large-scale data centers, energy-hungry language models, and battery-constrained devices. In this paper, we examined the energy awareness of agent-authored pull requests (PRs) using a publicly available dataset. We identified 216 energy-explicit PRs and conducted a thematic analysis, deriving a taxonomy of energy-aware work. Our further analysis of the applied optimization techniques shows that most align with established research recommendations. Although building and running these agents is highly energy intensive, encouragingly, the results indicate that they exhibit energy awareness when generating software artifacts. However, optimization-related PRs are accepted less frequently than others, largely due to their negative impact on maintainability.

</details>


### [24] [Characterizing Bugs and Quality Attributes in Quantum Software: A Large-Scale Empirical Study](https://arxiv.org/abs/2512.24656)
*Mir Mohammad Yousuf,Shabir Ahmad Sofi*

Main category: cs.SE

TL;DR: 对123个开源量子项目（2012-2024年）的32,296个已验证bug报告进行首次生态系统规模纵向分析，揭示了量子软件缺陷的特征、分布和演化规律。


<details>
  <summary>Details</summary>
Motivation: 量子软件工程(QSE)对确保混合量子-经典系统的可靠性和可维护性至关重要，但关于真实量子项目中bug如何出现及影响质量的实证证据仍然有限。

Method: 采用混合方法：结合仓库挖掘、静态代码分析、问题元数据提取和经过验证的基于规则的分类框架，分析8个功能类别（全栈库、模拟器、退火、算法、编译器、汇编、密码学、实验计算）的量子项目。

Result: 全栈库和编译器是最容易出错的类别（电路、门和转译相关问题）；模拟器主要受测量和噪声建模错误影响。经典bug主要影响可用性和互操作性，而量子特定bug不成比例地降低性能、可维护性和可靠性。缺陷密度在2017-2021年达到峰值后下降。采用自动化测试的仓库能检测更多缺陷并更快解决问题。

Conclusion: 这是首次大规模数据驱动的量子软件缺陷特征描述，为改进QSE中的测试、文档和维护实践提供了实证指导。负二项回归显示自动化测试与预期缺陷发生率降低约60%相关。

Abstract: Quantum Software Engineering (QSE) is essential for ensuring the reliability and maintainability of hybrid quantum-classical systems, yet empirical evidence on how bugs emerge and affect quality in real-world quantum projects remains limited. This study presents the first ecosystem-scale longitudinal analysis of software defects across 123 open source quantum repositories from 2012 to 2024, spanning eight functional categories, including full-stack libraries, simulators, annealing, algorithms, compilers, assembly, cryptography, and experimental computing. Using a mixed method approach combining repository mining, static code analysis, issue metadata extraction, and a validated rule-based classification framework, we analyze 32,296 verified bug reports. Results show that full-stack libraries and compilers are the most defect-prone categories due to circuit, gate, and transpilation-related issues, while simulators are mainly affected by measurement and noise modeling errors. Classical bugs primarily impact usability and interoperability, whereas quantum-specific bugs disproportionately degrade performance, maintainability, and reliability. Longitudinal analysis indicates ecosystem maturation, with defect densities peaking between 2017 and 2021 and declining thereafter. High-severity defects cluster in cryptography, experimental computing, and compiler toolchains. Repositories employing automated testing detect more defects and resolve issues faster. A negative binomial regression further shows that automated testing is associated with an approximate 60 percent reduction in expected defect incidence. Overall, this work provides the first large-scale data-driven characterization of quantum software defects and offers empirical guidance for improving testing, documentation, and maintainability practices in QSE.

</details>


### [25] [Feature Slice Matching for Precise Bug Detection](https://arxiv.org/abs/2512.24858)
*Ke Ma,Jianjun Huang,Wei You,Bin Liang,Jingzheng Wu,Yanjun Wu,Yuanjun Gong*

Main category: cs.SE

TL;DR: MATUS通过特征切片和端到端目标噪声抑制技术，提升基于相似性测量的bug检测精度，在Linux内核中发现31个未知bug


<details>
  <summary>Details</summary>
Motivation: 现有基于函数相似性的bug检测方法受无关语句（噪声）干扰，特别是目标代码中的噪声难以消除，影响检测性能

Method: 1. 从buggy查询和目标代码中提取特征切片表示语义特征；2. 利用buggy代码的先验知识指导目标切片，端到端确定切片标准；3. 所有特征切片嵌入向量并基于相似性比较；4. 审计候选bug确认未知漏洞

Result: MATUS在真实项目中具有bug检测优势且效率可接受，在Linux内核中发现31个未知bug，全部得到内核开发者确认，其中11个被分配CVE编号

Conclusion: MATUS通过抑制目标噪声实现了更精确的基于相似性的bug检测，在真实世界项目中验证了有效性

Abstract: Measuring the function similarity to detect bugs is effective, but the statements unrelated to the bugs can impede the performance due to the noise interference. Suppressing the noise interference in existing works does not manage the tough job, i.e., eliminating the noise in the targets. In this paper, we propose MATUS to mitigate the target noise for precise bug detection based on similarity measurement. Feature slices are extracted from both the buggy query and the targets to represent the semantic feature of (potential) bug logics. In particular, MATUS guides the target slicing with the prior knowledge from the buggy code, in an end-to-end way to pinpoint the slicing criterion in the targets. All feature slices are embedded and compared based on the vector similarity. Buggy candidates are audited to confirm unknown bugs in the targets. Experiments show that MATUS holds advantages in bug detection for real-world projects with acceptable efficiency. In total, MATUS has spotted 31 unknown bugs in the Linux kernel. All of them have been confirmed by the kernel developers, and 11 have been assigned CVEs.

</details>


### [26] [Securing High-Concurrency Ticket Sales: A Framework Based on Microservice](https://arxiv.org/abs/2512.24941)
*Zhiyong Zhang,Xiaoyan Zhang,Xiaoqi Li*

Main category: cs.SE

TL;DR: 基于Spring Cloud微服务架构的铁路购票系统，通过B/S架构和多种安全设计，在高并发场景下保持稳定性和可靠性，实现从查询到支付的全流程在线服务。


<details>
  <summary>Details</summary>
Motivation: 传统聚合架构在节假日等高并发场景下存在容错性不足和处理能力低的问题，无法满足峰值用户需求，需要采用微服务架构来提升系统的稳定性、数据一致性和响应速度。

Method: 采用B/S架构和Spring Cloud微服务框架进行系统设计与开发，集成多种中间件组件，制定多重安全设计方法，实现实时车次查询、动态座位更新、在线选座、购票等功能。

Result: 系统开发完成后对核心接口进行测试，测试数据证明系统在高并发场景下具有良好的处理能力和稳定性，能够快速响应用户请求。

Conclusion: 基于微服务架构的铁路购票系统能够有效解决传统线下购票的排队时间长、信息延迟等问题，通过精心设计的架构和中间件集成，实现了安全、稳定、高性能的在线购票服务。

Abstract: The railway ticketing system is one of the most important public service infrastructure. In peak periods such as holidays, it is often faced with the challenge of high concurrency scenarios because of a large number of users accessing at the same time. The traditional aggregation architecture can not meet the peak user requirements because of its insufficient fault tolerance and low ability. Therefore, the system needs to use microservice architecture for development, and add multiple security methods to ensure that the system can have good stability and data consistency under high concurrency scenarios, and can respond quickly to user requests. This paper introduces the use of B/S architecture and Spring Cloud to design and develop a railway ticket purchase system that can maintain stability and reliability under high concurrency scenarios, and formulate multiple security design methods for the system. This system integrates a range of functions, such as real-time train inquiries, dynamic seat updates, online seat selection, and ticket purchasing, effectively addressing common problems associated with offline ticket purchasing, such as long queues and delayed information. It enables a complete online process from inquiry and booking to payment and refunds. Furthermore, the "add passenger" function allows users to purchase tickets for others, extending the convenience of online ticketing to people with limited internet access. The system design prioritizes security and stability, while also focusing on high performance, and achieves these goals through a carefully designed architecture and the integration of multiple middleware components. After the completion of the system development, the core interface of the system is tested, and then the results are analyzed. The test data proves that the system has good ability and stability under high concurrency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [Governing Cloud Data Pipelines with Agentic AI](https://arxiv.org/abs/2512.23737)
*Aswathnarayan Muthukrishnan Kirubakaran,Adithya Parthasarathy,Nitin Saksena,Ram Sekhar Bodala,Akshay Deshpande,Suhas Malempati,Shiva Carimireddy,Abhirup Mazumder*

Main category: cs.DC

TL;DR: 论文提出Agentic Cloud Data Engineering平台，通过策略感知的AI代理架构实现云数据管道的自主治理，相比静态编排可减少45%恢复时间、降低25%运营成本、减少70%人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前云数据管道面临动态工作负载、模式演化、成本约束和严格治理要求，但大多数生产管道依赖静态配置和被动操作实践，导致恢复时间长、资源利用率低、人工开销大。

Method: 提出策略感知的控制架构，将有限AI代理集成到云数据管道的治理和控制平面中。专门代理分析管道遥测和元数据，基于声明性成本和合规策略进行推理，提出受限操作行动（如自适应资源重配置、模式协调、自动故障恢复），所有代理行动都经过治理策略验证以确保可预测和可审计行为。

Result: 使用代表性批处理和流分析工作负载进行实验，结果显示：相比静态编排，平台可将平均管道恢复时间减少45%，运营成本降低约25%，手动干预事件减少70%以上，同时保持数据新鲜度和策略合规性。

Conclusion: 策略约束的代理控制为企业在云数据管道治理方面提供了有效实用的方法，能够显著提升自动化水平和运营效率。

Abstract: Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45%, lowers operational cost by approximately 25%, and decreases manual intervention events by over 70% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.

</details>


### [28] [Squeezing Edge Performance: A Sensitivity-Aware Container Management for Heterogeneous Tasks](https://arxiv.org/abs/2512.23952)
*Yongmin Zhang,Pengyu Huang,Mingyi Dong,Jing Yao*

Main category: cs.DC

TL;DR: 提出基于测量的容器资源管理框架CRMS，通过非线性拟合模型和两阶段优化，在边缘服务器上联合最小化延迟和功耗，相比基线降低14%延迟并提升能效。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中任务异构性和资源有限性对高效编排带来挑战，需要针对单边缘服务器上多异构应用的资源管理方案，以优化延迟和能耗。

Method: 1) 通过大量剖析实验建立CPU/内存分配与处理延迟的非线性拟合模型；2) 基于队列延迟公式构建MINLP问题；3) 将NP难问题分解为可处理的凸子问题；4) 提出两阶段容器资源管理方案CRMS，结合凸优化和贪婪细化。

Result: CRMS相比启发式和基于搜索的基线方法，延迟降低超过14%，能效提升，具有多项式时间复杂度和准动态执行能力，适用于动态工作负载的异构边缘环境。

Conclusion: 提出的测量驱动容器资源管理框架为异构边缘环境提供了实用可扩展的解决方案，通过非线性建模和两阶段优化有效平衡延迟和功耗，支持动态工作负载特性。

Abstract: Edge computing enables latency-critical applications to process data close to end devices, yet task heterogeneity and limited resources pose significant challenges to efficient orchestration. This paper presents a measurement-driven, container-based resource management framework for intra-node optimization on a single edge server hosting multiple heterogeneous applications. Extensive profiling experiments are conducted to derive a nonlinear fitting model that characterizes the relationship among CPU/memory allocations and processing latency across diverse workloads, enabling reliable estimation of performance under varying configurations and providing quantitative support for subsequent optimization. Using this model and a queueing-based delay formulation, we formulate a mixed-integer nonlinear programming (MINLP) problem to jointly minimize system latency and power consumption, which is shown to be NP-hard. The problem is decomposed into tractable convex subproblems and solved through a two-stage container-based resource management scheme (CRMS) combining convex optimization and greedy refinement. The proposed scheme achieves polynomial-time complexity and supports quasi-dynamic execution under global resource constraints. Simulation results demonstrate that CRMS reduces latency by over 14\% and improves energy efficiency compared with heuristic and search-based baselines, offering a practical and scalable solution for heterogeneous edge environments with dynamic workload characteristics.

</details>


### [29] [Data Heterogeneity-Aware Client Selection for Federated Learning in Wireless Networks](https://arxiv.org/abs/2512.24286)
*Yanbing Yang,Huiling Zhu,Wenchi Cheng,Jingqing Wang,Changrun Chen,Jiangzhou Wang*

Main category: cs.DC

TL;DR: 提出联合客户端选择与资源分配(CSRA)方案，解决联邦学习中数据异构性导致的泛化误差、延迟和能耗问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线网络中的效率受限于通信计算资源和客户端数据异构性，特别是大规模网络中数据异构性会导致重复训练、能耗增加和延迟延长

Method: 首先理论分析客户端数据异构性对全局模型泛化误差的影响，然后构建联合最小化学习延迟和能耗的优化问题，提出基于凸优化和松弛技术的联合客户端选择与资源分配(CSRA)方法

Result: 仿真结果表明，相比不考虑数据异构性的基线方法，CSRA方案能获得更高的测试精度、更低的学习延迟和更低的能耗

Conclusion: 通过联合考虑客户端选择和资源分配，并基于数据异构性理论分析，可以有效提升联邦学习在无线网络中的效率和性能

Abstract: Federated Learning (FL) enables mobile edge devices, functioning as clients, to collaboratively train a decentralized model while ensuring local data privacy. However, the efficiency of FL in wireless networks is limited not only by constraints on communication and computational resources but also by significant data heterogeneity among clients, particularly in large-scale networks. This paper first presents a theoretical analysis of the impact of client data heterogeneity on global model generalization error, which can result in repeated training cycles, increased energy consumption, and prolonged latency. Based on the theoretical insights, an optimization problem is formulated to jointly minimize learning latency and energy consumption while constraining generalization error. A joint client selection and resource allocation (CSRA) approach is then proposed, employing a series of convex optimization and relaxation techniques. Extensive simulation results demonstrate that the proposed CSRA scheme yields higher test accuracy, reduced learning latency, and lower energy consumption compared to baseline methods that do not account for data heterogeneity.

</details>


### [30] [PackKV: Reducing KV Cache Memory Footprint through LLM-Aware Lossy Compression](https://arxiv.org/abs/2512.24449)
*Bo Jiang,Taolue Yang,Youyuan Liu,Xubin He,Sheng Di,Sian Jin*

Main category: cs.DC

TL;DR: PackKV是一个针对长上下文生成的KV缓存管理框架，通过专门设计的无损压缩技术，在保持精度的同时显著减少内存占用并提升计算吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型的长上下文推理面临KV缓存内存占用过大的挑战，随着序列长度和批次大小增加，KV缓存可达数GB，限制了实际应用。

Method: 提出PackKV框架，针对KV缓存数据特性设计专门的压缩算法和系统架构协同设计，支持动态增长的KV缓存，保持高计算效率。

Result: 在相同精度损失下，相比现有量化方法，PackKV平均实现K缓存153.2%、V缓存179.6%的内存减少率；在A100和RTX Pro 6000 GPU上，相比cuBLAS矩阵向量乘法，平均吞吐量提升K为75.7%、V为171.7%。

Conclusion: PackKV是一个高效通用的KV缓存管理框架，能有效解决长上下文生成中的内存瓶颈问题，同时提供高计算吞吐量，为实际部署提供了可行方案。

Abstract: Transformer-based large language models (LLMs) have demonstrated remarkable potential across a wide range of practical applications. However, long-context inference remains a significant challenge due to the substantial memory requirements of the key-value (KV) cache, which can scale to several gigabytes as sequence length and batch size increase. In this paper, we present \textbf{PackKV}, a generic and efficient KV cache management framework optimized for long-context generation. %, which synergistically supports both latency-critical and throughput-critical inference scenarios. PackKV introduces novel lossy compression techniques specifically tailored to the characteristics of KV cache data, featuring a careful co-design of compression algorithms and system architecture. Our approach is compatible with the dynamically growing nature of the KV cache while preserving high computational efficiency. Experimental results show that, under the same and minimum accuracy drop as state-of-the-art quantization methods, PackKV achieves, on average, \textbf{153.2}\% higher memory reduction rate for the K cache and \textbf{179.6}\% for the V cache. Furthermore, PackKV delivers extremely high execution throughput, effectively eliminating decompression overhead and accelerating the matrix-vector multiplication operation. Specifically, PackKV achieves an average throughput improvement of \textbf{75.7}\% for K and \textbf{171.7}\% for V across A100 and RTX Pro 6000 GPUs, compared to cuBLAS matrix-vector multiplication kernels, while demanding less GPU memory bandwidth. Code available on https://github.com/BoJiang03/PackKV

</details>


### [31] [Understanding LLM Checkpoint/Restore I/O Strategies and Patterns](https://arxiv.org/abs/2512.24511)
*Mikaila J. Gossman,Avinash Maurya,Bogdan Nicolae,Jon C. Calhoun*

Main category: cs.DC

TL;DR: 本文研究了LLM训练中检查点/恢复的I/O性能问题，通过liburing库优化实现了比现有方案最高7.6倍的吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 随着LLM和基础模型规模扩大，检查点/恢复成为训练和推理的关键模式。在3D并行（张量、流水线、数据）架构下，检查点涉及大量进程管理各种形状大小的张量，需要频繁持久化到稳定存储，这带来了大数据I/O问题（数据量大、种类多、频率高）。存储栈各层级性能差异巨大，即使使用异步刷新/预取也会在并发下产生瓶颈。

Method: 开发微基准测试来量化使用liburing库时的权衡，评估聚合、对齐和I/O合并在不同I/O模式下的交互效果。研究文件系统感知的聚合策略，并与现有LLM检查点引擎进行比较。

Result: 未合并的小缓冲区操作使吞吐量相比合成工作负载减半，而文件系统感知的聚合恢复了带宽并减少了元数据开销。相比最先进的LLM检查点引擎，该方法实现了比DataStates-LLM高3.9倍的写入吞吐量，比TorchSnapshot高7.6倍。

Conclusion: 研究结果表明需要开发与现代文件系统和I/O后端对齐的聚合和合并策略，以优化LLM检查点性能。liburing等内核加速I/O库在LLM检查点场景中具有显著优势。

Abstract: As LLMs and foundation models scale, checkpoint/restore has become a critical pattern for training and inference. With 3D parallelism (tensor, pipeline, data), checkpointing involves many processes, each managing numerous tensors of varying shapes and sizes, that must be persisted frequently to stable storage (e.g., parallel file systems). This turns checkpoint/restore into a big-data I/O problem characterized by volume, variety, and velocity. The workflow must traverse the full storage stack -- from GPU memory through host memory and local storage to external repositories -- whose tiers differ by orders of magnitude in performance, creating bottlenecks under concurrency even with asynchronous flush/prefetch. Kernel-accelerated I/O libraries such as \texttt{liburing} may mitigate these issues versus POSIX, but their effectiveness for LLM checkpointing remains underexplored. We develop microbenchmarks to quantify trade-offs when using \texttt{liburing}, evaluating how aggregation, alignment, and I/O coalescing interact under buffered and direct I/O. We find that uncoalesced small-buffer operations halve throughput relative to synthetic workloads, while file system-aware aggregation restores bandwidth and reduces metadata overhead. Compared to state-of-the-art LLM checkpointing engines, our approach achieves up to $3.9\times$ higher write throughput than DataStates-LLM and $7.6\times$ higher than TorchSnapshot. These results highlight the need for aggregation and coalescing strategies that align with modern file systems and I/O backends.

</details>


### [32] [Distributed Bilevel Optimization with Dual Pruning for Resource-limited Clients](https://arxiv.org/abs/2512.24667)
*Mingyi Li,Xiao Zhang,Ruisheng Zheng,Hongjian Shi,Yuan Yuan,Xiuzhen Cheng,Dongxiao Yu*

Main category: cs.DC

TL;DR: 提出首个资源自适应的分布式双层优化框架RABO/RAFBO，通过二阶自由超梯度估计器，使客户端能根据可用资源优化子模型，达到渐近最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型发展，传统分布式双层优化算法无法直接应用于低资源客户端，主要原因是同时优化上下层函数涉及过多计算。

Method: 提出资源自适应分布式双层优化框架，包含二阶自由超梯度估计器，允许客户端根据可用资源优化子模型。由于部分外层参数x和内层参数y的耦合影响，理论分析全局平均超梯度上界具有挑战性。

Result: 理论证明RABO和RAFBO都能达到渐近最优收敛率$O(1/\sqrt{C_x^{\ast}Q})$，其中$C_x^{\ast}$为外层参数的最小覆盖度。在两个不同任务上的实验证明了方法的有效性和计算效率。

Conclusion: 提出的资源自适应分布式双层优化框架解决了传统方法在低资源环境下的计算瓶颈，通过理论分析和实验验证了其有效性和高效性。

Abstract: With the development of large-scale models, traditional distributed bilevel optimization algorithms cannot be applied directly in low-resource clients. The key reason lies in the excessive computation involved in optimizing both the lower- and upper-level functions. Thus, we present the first resource-adaptive distributed bilevel optimization framework with a second-order free hypergradient estimator, which allows each client to optimize the submodels adapted to the available resources. Due to the coupled influence of partial outer parameters x and inner parameters y, it's challenging to theoretically analyze the upper bound regarding the globally averaged hypergradient for full model parameters. The error bound of inner parameter also needs to be reformulated since the local partial training. The provable theorems show that both RABO and RAFBO can achieve an asymptotically optimal convergence rate of $O(1/\sqrt{C_x^{\ast}Q})$, which is dominated by the minimum coverage of the outer parameter $C_x^{\ast}$. Extensive experiments on two different tasks demonstrate the effectiveness and computation efficiency of our proposed methods.

</details>


### [33] [AI-Driven Cloud Resource Optimization for Multi-Cluster Environments](https://arxiv.org/abs/2512.24914)
*Vinoth Punniyamoorthy,Akash Kumar Agarwal,Bikesh Kumar,Abhirup Mazumder,Kabilan Kannan,Sumit Saha*

Main category: cs.DC

TL;DR: 提出基于AI的多集群云系统自适应资源优化框架，通过预测学习和协调管理提升资源效率与系统稳定性


<details>
  <summary>Details</summary>
Motivation: 现代云原生系统依赖多集群部署实现可扩展性和弹性，但现有资源管理方法多为反应式和集群中心化，导致资源利用率低、适应延迟和运维开销大

Method: 集成预测学习、策略感知决策和持续反馈的AI驱动框架，通过分析跨集群遥测数据和历史执行模式，动态调整资源分配以平衡性能、成本和可靠性目标

Result: 原型实现显示相比传统反应式方法，资源效率提升、工作负载波动时稳定更快、性能变异性降低

Conclusion: 智能自适应基础设施管理是实现可扩展和弹性云平台的关键使能技术

Abstract: Modern cloud-native systems increasingly rely on multi-cluster deployments to support scalability, resilience, and geographic distribution. However, existing resource management approaches remain largely reactive and cluster-centric, limiting their ability to optimize system-wide behavior under dynamic workloads. These limitations result in inefficient resource utilization, delayed adaptation, and increased operational overhead across distributed environments. This paper presents an AI-driven framework for adaptive resource optimization in multi-cluster cloud systems. The proposed approach integrates predictive learning, policy-aware decision-making, and continuous feedback to enable proactive and coordinated resource management across clusters. By analyzing cross-cluster telemetry and historical execution patterns, the framework dynamically adjusts resource allocation to balance performance, cost, and reliability objectives. A prototype implementation demonstrates improved resource efficiency, faster stabilization during workload fluctuations, and reduced performance variability compared to conventional reactive approaches. The results highlight the effectiveness of intelligent, self-adaptive infrastructure management as a key enabler for scalable and resilient cloud platforms.

</details>


### [34] [Reliable and Resilient Collective Communication Library for LLM Training and Serving](https://arxiv.org/abs/2512.25059)
*Wei Wang,Nengneng Yu,Sixian Xiong,Zaoxing Liu*

Main category: cs.DC

TL;DR: R²CCL是一个容错通信库，利用多NIC硬件实现无损、低开销的故障转移，显著减少GPU训练和推理中的网络故障恢复时间。


<details>
  <summary>Details</summary>
Motivation: 现代ML训练和推理涉及大量GPU，网络故障会导致10-15%的GPU时间浪费在缓慢的恢复上。常见的网络错误和链路波动会触发超时，导致整个作业终止，迫使训练时进行昂贵的检查点回滚和推理时请求重新处理。

Method: R²CCL通过利用多NIC硬件，执行快速连接迁移、带宽感知负载重新分配和弹性集体算法，在故障下保持进展。它提供无损、低开销的故障转移能力。

Result: R²CCL对NIC故障具有高度鲁棒性，训练开销小于1%，推理开销小于3%。在8-GPU H100 InfiniBand服务器和模拟数百个GPU的大规模ML模拟器上评估，R²CCL分别比基线AdapCC和DejaVu快12.18倍和47倍。

Conclusion: R²CCL通过高效的故障转移机制显著减少了ML训练和推理中的网络故障恢复开销，提高了GPU利用率和系统可靠性。

Abstract: Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. We evaluate R$^2$CCL on two 8-GPU H100 InfiniBand servers and via large-scale ML simulators modeling hundreds of GPUs with diverse failure patterns. Experiments show that R$^2$CCL is highly robust to NIC failures, incurring less than 1\% training and less than 3\% inference overheads. R$^2$CCL outperforms baselines AdapCC and DejaVu by 12.18$\times$ and 47$\times$, respectively.

</details>
