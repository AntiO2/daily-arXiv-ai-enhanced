<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: PennyLane是一个用于混合量子-经典机器学习的Python框架，支持量子电路构建、自动微分和混合优化工作流，并与PyTorch、TensorFlow等经典ML框架集成。


<details>
  <summary>Details</summary>
Motivation: 结合量子计算的优势与经典优化技术，为研究人员提供构建、优化和部署变分量子算法的工具，推动量子增强数据科学的发展。

Method: 通过Python框架实现量子电路与经典机器学习的无缝连接，支持量子核方法、变分量子本征求解器、投资组合优化等应用场景。

Result: 展示了PennyLane在量子机器学习、优化和量子化学应用中的多功能性，提供了与scikit-learn、pandas等流行库集成的具体Python示例。

Conclusion: PennyLane作为量子计算和机器学习之间的桥梁，为基于Python的研究提供了混合量子-经典工作流程的标准工具和引用参考。

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [2] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: 提出了一个用于区域供热站早期故障检测的开源框架，包含公开数据集、评估方法和基准结果，能够提前检测60%的故障，平均提前3.9天。


<details>
  <summary>Details</summary>
Motivation: 区域供热站故障的早期检测对于降低回水温度和提高效率至关重要，但该领域进展受到公开标记数据集有限的阻碍。

Method: 开发了EnergyFaultDetector开源Python框架，结合服务报告验证的公共数据集，使用准确性、可靠性和早期性三个指标进行评估，并支持使用ARCANA进行根本原因分析。

Result: 模型实现了高正常行为准确性（0.98）和事件F分数（0.83），在客户报告问题前检测到60%的故障，平均提前3.9天。

Conclusion: 该框架通过整合公开数据集、指标、开源代码和基准，为区域供热站早期故障检测和诊断方法建立了可重现的、以故障为中心的基准，支持一致比较和方法开发。

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [3] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: irace-evo是irace的扩展版本，通过集成大语言模型实现参数和代码空间的联合探索，在可变尺寸装箱问题上发现了优于现有CMSA实现的新算法变体，且成本低于2欧元。


<details>
  <summary>Details</summary>
Motivation: 现有自动算法配置工具只能调优参数值而无法修改算法代码，限制了算法优化的潜力。

Method: 扩展irace框架，集成LLM进行代码演化，支持多语言，采用渐进式上下文管理和Always-From-Original原则确保稳健的代码演化。

Result: 在VSBPP问题上，irace-evo发现的算法变体优于最先进的CMSA实现，使用轻量级模型（如Claude Haiku 3.5）且总成本低于2欧元。

Conclusion: 将自动配置与LLM驱动的代码演化相结合，为启发式设计和元启发式优化提供了强大且成本高效的途径。

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [4] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: 本文比较了两种基于AI的编程作业评分方法：直接评分法和反向评分法，评估它们在编程课程自动评分中的效果和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统编程作业人工评分耗时且不一致，单元测试只能提供二元结果。大语言模型的发展为自动化、可扩展且更客观的评分提供了可能。

Method: 比较两种AI评分技术：直接法（AI直接应用评分标准）和反向法（AI先修复错误，再根据修复情况推断分数）。在原始评分标准和10倍扩展标准上评估，并与人类评分对比。

Result: 初步发现直接法更快更直接，但反向法通过关注修复努力提供更细粒度的评估。两种方法都需要精心设计提示词，特别是在分配部分分数和处理逻辑错误时。

Conclusion: 讨论了两种方法的优缺点，提示设计的实际考虑，以及未来混合人-AI评分系统的方向，旨在提高CS课程评分的一致性、效率和公平性。

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [5] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型的日志分析工具，能够在CPU上高效处理海量日志数据，实现自动化问题诊断和洞察生成，显著节省时间和人力成本。


<details>
  <summary>Details</summary>
Motivation: IT环境产生大量日志数据，手动检查不切实际，需要自动化日志分析解决方案来监控系统健康状态和检测问题。

Method: 利用大语言模型进行日志数据处理和问题诊断，开发了在CPU上高效运行LLM的新方法，能够快速处理海量日志而不影响输出质量。

Result: 自2024年3月在生产环境中部署，已扩展到70个软件产品，处理超过2000个工单进行问题诊断，节省300+人工小时，每月估计节省15,444美元人力成本。

Conclusion: 基于LLM的日志分析工具能够有效自动化IT支持流程，显著提高效率并降低成本，证明了在CPU上高效运行LLM处理大规模日志数据的可行性。

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [6] [Towards Continuous Assurance with Formal Verification and Assurance Cases](https://arxiv.org/abs/2511.14805)
*Dhaminda B. Abeywickrama,Michael Fisher,Frederic Wheeler,Louise Dennis*

Main category: cs.SE

TL;DR: 提出了一个统一的持续保证框架，将设计时、运行时和演进时保证集成到可追溯的模型驱动工作流中，以解决传统保证方法在系统更新和运行时变化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 自主系统需要在整个生命周期中维持对其正确性和安全性的合理信心，传统保证方法将开发时保证与运行时保证分离，导致无法适应运行时变化或系统更新的碎片化论证。

Method: 使用RoboChart进行功能正确性验证和PRISM进行概率风险分析，实现设计时保证阶段；开发了作为Eclipse插件的模型驱动转换管道，当形式化规范或其验证结果变化时自动重新生成结构化保证论证。

Result: 在核检查机器人场景中演示了该方法，展示了框架与三边AI原则的一致性，反映了监管机构认可的最佳实践。

Conclusion: 提出的持续保证框架通过集成设计时、运行时和演进时保证，为自主系统的可信性提供了可追溯的模型驱动解决方案。

Abstract: Autonomous systems must sustain justified confidence in their correctness and safety across their operational lifecycle-from design and deployment through post-deployment evolution. Traditional assurance methods often separate development-time assurance from runtime assurance, yielding fragmented arguments that cannot adapt to runtime changes or system updates - a significant challenge for assured autonomy. Towards addressing this, we propose a unified Continuous Assurance Framework that integrates design-time, runtime, and evolution-time assurance within a traceable, model-driven workflow as a step towards assured autonomy. In this paper, we specifically instantiate the design-time phase of the framework using two formal verification methods: RoboChart for functional correctness and PRISM for probabilistic risk analysis. We also propose a model-driven transformation pipeline, implemented as an Eclipse plugin, that automatically regenerates structured assurance arguments whenever formal specifications or their verification results change, thereby ensuring traceability. We demonstrate our approach on a nuclear inspection robot scenario, and discuss its alignment with the Trilateral AI Principles, reflecting regulator-endorsed best practices.

</details>


### [7] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: 探索自动流水线配置的优势及其应用方式


<details>
  <summary>Details</summary>
Motivation: 研究自动流水线配置在软件工程项目中的益处，重点关注CI流水线，并认为CD流水线也会有类似效果

Method: 定义自动流水线配置为快速部署软件工程项目流水线的过程，主要研究CI流水线

Result: 未提供具体研究结果

Conclusion: 未提供具体结论

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [8] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: 提出了MermaidSeqBench基准测试，用于评估LLM从文本提示生成Mermaid序列图的能力，包含132个样本，采用混合方法扩展，并使用LLM作为评判模型进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏系统性的基准测试来评估LLM在生成结构化图表（特别是Mermaid序列图）方面的正确性，需要填补这一空白。

Method: 采用混合方法：人工标注、上下文LLM提示和基于规则的变体生成，构建包含132个样本的基准测试，并使用LLM作为评判模型进行多维度评估。

Result: 评估结果显示不同模型之间存在显著能力差距，基准测试为结构化图表生成研究提供了基础。

Conclusion: MermaidSeqBench为结构化图表生成研究提供了基础，有助于开发更严谨、细粒度的评估方法。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [9] [FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor](https://arxiv.org/abs/2511.15007)
*Shehan I Pranto,Brett Fassler,Md Rafi Islam,Ashley Schenkel,Larry W Hawk,Edward Sazonov*

Main category: cs.SE

TL;DR: 开发了FRIENDS GUI，这是一个基于Python的开源工具，用于提取、解码和可视化FRIENDS设备收集的24小时电子烟使用数据，提高了数据的可访问性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 理解电子烟使用行为（包括抽吸持续时间、抽吸间隔和每次会话的抽吸次数）对于评估电子烟使用、有毒物质暴露和制定监管决策至关重要。

Method: 开发了基于Python的开源FRIENDS GUI工具，能够提取、解码和可视化FRIENDS设备记录的24小时抽吸和触摸事件数据。

Result: 使用24小时实验数据进行验证，确认了时间戳转换的准确性、事件解码的可靠性以及行为可视化的有效性。

Conclusion: 该软件已在GitHub上免费提供供公众使用，为电子烟使用行为研究提供了有效的工具支持。

Abstract: Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.

</details>


### [10] [Effective Code Membership Inference for Code Completion Models via Adversarial Prompts](https://arxiv.org/abs/2511.15107)
*Yuan Jiang,Zehao Li,Shan Huang,Christoph Treude,Xiaohong Su,Tiantian Wang*

Main category: cs.SE

TL;DR: AdvPrompt-MIA是一种针对代码补全模型的成员推理攻击方法，通过代码特定的对抗性提示和深度学习来检测训练数据成员资格。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒和灰盒成员推理攻击依赖昂贵的代理模型或手动设计的启发式规则，难以捕捉过参数化代码语言模型的细微记忆模式。

Method: 设计一系列对抗性提示，诱导受害者代码模型输出变化，通过比较输出与真实补全构建特征向量，训练分类器自动区分成员和非成员样本。

Result: 在Code Llama 7B等模型上评估，AUC提升高达102%，优于现有最佳基线方法，且在不同模型和数据集间具有强迁移性。

Conclusion: AdvPrompt-MIA能有效捕捉丰富的记忆模式，准确推断训练集成员资格，具有实际应用价值和通用性。

Abstract: Membership inference attacks (MIAs) on code completion models offer an effective way to assess privacy risks by inferring whether a given code snippet was part of the training data. Existing black- and gray-box MIAs rely on expensive surrogate models or manually crafted heuristic rules, which limit their ability to capture the nuanced memorization patterns exhibited by over-parameterized code language models. To address these challenges, we propose AdvPrompt-MIA, a method specifically designed for code completion models, combining code-specific adversarial perturbations with deep learning. The core novelty of our method lies in designing a series of adversarial prompts that induce variations in the victim code model's output. By comparing these outputs with the ground-truth completion, we construct feature vectors to train a classifier that automatically distinguishes member from non-member samples. This design allows our method to capture richer memorization patterns and accurately infer training set membership. We conduct comprehensive evaluations on widely adopted models, such as Code Llama 7B, over the APPS and HumanEval benchmarks. The results show that our approach consistently outperforms state-of-the-art baselines, with AUC gains of up to 102%. In addition, our method exhibits strong transferability across different models and datasets, underscoring its practical utility and generalizability.

</details>


### [11] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种训练大语言模型生成高质量Selenium测试用例的新方法，专门针对表单交互测试，在语法正确性、脚本可执行性和输入字段覆盖率方面显著优于GPT-4o等基线模型。


<details>
  <summary>Details</summary>
Motivation: 自动化Web应用测试是现代软件开发的关键组成部分，但在大语言模型背景下，表单交互生成任务仍未被充分探索，且缺乏公开的基准数据集来系统评估LLMs的表单交互生成能力。

Method: 提出了一种训练LLMs生成Selenium测试用例的新方法，构建了合成和人工标注的数据集用于训练和评估，涵盖多样化的真实世界表单和测试场景，并定义了语法正确性、脚本可执行性和输入字段覆盖率的清晰指标。

Result: 实证研究表明，该方法在所有评估指标上都显著优于包括GPT-4o在内的强基线模型和其他流行LLMs。

Conclusion: 这项工作为基于LLM的Web测试未来研究奠定了基础，并为该领域的持续进展提供了资源支持。

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [12] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: 该研究系统识别了导致机器学习应用中资源泄漏的代码异味，在PyTorch中发现了30种，在TensorFlow/Keras中发现了16种，并提出了50个最佳实践来减少资源泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有ML研究主要关注模型性能指标，而对长期可持续性和资源效率的关注有限。高效资源管理对于稳健部署同样至关重要。

Method: 对PyTorch、TensorFlow和Keras的开发者讨论和实际代码片段进行实证调查，采用三阶段验证流程（三位作者独立分析后达成共识）。

Result: 识别了46种与资源泄漏相关的代码异味，按根本原因分类，并为每种异味推导出至少一个最佳实践。

Conclusion: 这是首个全面研究主要ML框架中导致资源泄漏的代码异味的研究，为开发更高效、可持续的ML应用提供了结构化视图和可操作的最佳实践。

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [13] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: 本文介绍了M工具链和建模语言，旨在支持复杂、并发和时间感知系统的建模和多目标编译，解决现有建模语言目标单一的问题。


<details>
  <summary>Details</summary>
Motivation: 现有建模语言通常只针对仿真或实现等单一目标设计，难以实现多目标编译。复杂软件驱动系统需要从高层次系统模型生成多种专用模型用于不同目的。

Method: 开发了基于参与者模型并扩展了离散事件调度语义的文本化、语法驱动的M语言，提供系统实体建模、基于消息的交互以及时间或状态触发反应等构造。

Result: M语言能够从模型系统性地生成多样化目标工件，同时保持与原始模型的语义一致性，并可作为中间语言让其他建模语言受益于其编译框架。

Conclusion: M语言和工具链为复杂、并发和时间感知系统的模型驱动工程提供了有效的多目标编译支持，解决了现有建模语言目标单一的问题。

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [14] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: 提出AutoSW愿景——一种迭代式端到端自动化软件开发范式，通过分析-规划-实现-交付循环，让AI系统作为人类合作伙伴将自然语言意图转化为可执行软件


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，现有方法要么将AI视为需要大量人工参与的辅助工具，要么采用'氛围编码'让AI反复修改代码。这两种路径将汇聚于AI参与整个软件开发生命周期的目标

Method: 设计AutoSW范式，采用分析-规划-实现-交付的迭代循环，AI系统作为一等参与者，将自然语言表达的人类意图转化为可执行软件，并构建轻量级原型验证

Result: 原型成功执行了各种代表性案例，能够交付可执行软件

Conclusion: AutoSW为真正端到端自动化软件开发提供了可行方向，展示了AI系统作为人类合作伙伴参与整个软件开发生命周期的潜力

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [15] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: 该研究探讨了如何从机器学习文档（如ModelCards和DataSheets）中提取需求工程相关信息，并评估了三种需求工程表示方法将这些信息转化为结构化需求的有效性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习系统的软件工程过程中，集成和验证ML组件是一个主要挑战。传统需求工程过程在指定ML组件需求（包括模型和数据）方面面临新障碍，而机器学习文档作为需求相关信息来源尚未充分探索。

Method: 首先分析了20个公开可用的ModelCards和DataSheets中RE相关信息的数量和性质；然后评估了三种已建立的需求工程表示方法（EARS、Rupp模板和Volere）将这些知识结构化为需求的有效性。

Result: 研究表明这些文档包含大量潜在的RE相关信息，并且存在将ML特定知识转化为结构化需求的途径，可以将ML文档整合到ML系统的软件工程过程中。

Conclusion: 机器学习文档可以作为需求工程的重要信息来源，通过适当的需求工程表示方法，能够将ML特定知识转化为结构化需求，从而支持ML系统的软件工程过程。

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [16] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: MutDafny工具通过变异测试检测Dafny形式化规范中的弱点，使用32个变异算子，在794个真实Dafny程序中平均每241行代码发现一个需要加强的规范弱点。


<details>
  <summary>Details</summary>
Motivation: 在验证感知编程语言如Dafny中，规范与实现一样容易出错，规范缺陷可能导致形式化验证的程序偏离预期行为。

Method: 采用变异测试方法，将故障（变异）引入代码，依赖形式化规范检测这些变异。如果带变异体的程序仍能验证，则表明规范存在弱点。从流行工具中分析变异算子，并从GitHub上的Dafny项目错误修复提交中合成新算子。

Result: 在794个真实Dafny程序数据集上评估MutDafny，手动分析未检测到的变异体子集，识别出5个需要加强的真实世界规范弱点，平均每241行代码发现一个弱点。

Conclusion: MutDafny能有效提高Dafny规范的可靠性，通过自动标记潜在弱点来增强形式化验证的置信度。

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


### [17] [EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode](https://arxiv.org/abs/2511.15589)
*Qian Zhu,Yuxuan Liu,Ziyuan Zhu,Shangqing Liu,Lei Bu*

Main category: cs.SE

TL;DR: EPSO是一个基于缓存的eBPF超级优化器，通过离线超级优化发现重写规则并重用，实现高质量优化且运行时开销最小


<details>
  <summary>Details</summary>
Motivation: eBPF程序受限于内核验证器的严格安全约束，现有编译器优化支持有限，手工优化规则设计困难且效果有限，而传统超级优化计算成本高难以扩展

Method: 提出EPSO方法，通过离线超级优化发现重写规则并缓存，在运行时重用这些规则来优化eBPF程序

Result: EPSO发现了795条重写规则，相比Clang输出平均减少24.37%程序大小，最高达68.87%，在所有基准测试中优于K2，在92.68%测试中优于Merlin，平均降低6.60%程序运行时间

Conclusion: EPSO通过缓存式超级优化方法有效解决了eBPF程序优化问题，在程序大小和运行性能方面都取得了显著提升

Abstract: Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [18] [Castle: Causal Cascade Updates in Relational Databases with Large Language Models](https://arxiv.org/abs/2511.14762)
*Yongye Su,Yucheng Zhang,Zeru Shi,Bruno Ribeiro,Elisa Bertino*

Main category: cs.DB

TL;DR: Castle是首个使用大语言模型进行仅模式级联更新生成的框架，能够通过自然语言指令生成多列、因果一致的SQL UPDATE语句，而无需向模型暴露表内容。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在Text2SQL代码生成方面主要关注SELECT查询，忽视了SQL更新操作及其连锁效应的挑战。传统的CASCADE UPDATE约束是静态的，不适合需要动态、上下文感知更新的现代非规范化数据库。

Method: 将UPDATE SQL生成构建为分治任务，利用LLM的推理能力，通过嵌套查询和子结构确定哪些列需要直接更新以及这些更新如何在模式中传播，确保数据机密性。

Result: 在真实世界因果更新场景中评估，证明Castle能够生成准确的SQL更新，展示了LLM在自动化DBMS中的推理能力。

Conclusion: Castle框架成功解决了现代非规范化数据库中动态级联更新的挑战，为LLM在数据库管理系统中的推理应用开辟了新方向。

Abstract: This work introduces Castle, the first framework for schema-only cascade update generation using large language models (LLMs). Despite recent advances in LLMs for Text2SQL code generation, existing approaches focus primarily on SELECT queries, neglecting the challenges of SQL update operations and their ripple effects. Traditional CASCADE UPDATE constraints are static and unsuitable for modern, denormalized databases, which demand dynamic, context-aware updates. Castle enables natural language instructions to trigger multi-column, causally consistent SQL UPDATE statements, without revealing table content to the model. By framing UPDATE SQL generation as a divide-and-conquer task with LLMs' reasoning capacity, Castle can determine not only which columns must be directly updated, but also how those updates propagate through the schema, causing cascading updates -- all via nested queries and substructures that ensure data confidentiality. We evaluate it on real-world causal update scenarios, demonstrating its ability to produce accurate SQL updates, and thereby highlighting the reasoning ability of LLMs in automated DBMS.

</details>


### [19] [BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer](https://arxiv.org/abs/2511.15090)
*Wenhan Yu,Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Lei Sha,Deguo Xia,Jizhou Huang*

Main category: cs.DB

TL;DR: 提出了BBox DocVQA数据集，这是一个大规模、基于边界框的文档视觉问答数据集，旨在增强视觉文档中的空间推理和证据定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有的DocVQA数据集大多局限于页面级别，缺乏细粒度的空间定位，限制了视觉语言模型的解释性和推理能力。

Method: 开发了自动构建流水线Segment Judge and Generate，集成了区域分割模型、用于语义判断的VLM和用于问题答案生成的高级VLM，并经过人工验证确保质量。

Result: 构建的数据集包含3.6K个多样化文档和32K个QA对，涵盖单区域和多区域以及单页和多页场景。基准测试显示现有VLMs在空间定位和推理准确性方面仍面临挑战，但在BBox DocVQA上微调可显著改善边界框定位和答案生成。

Conclusion: BBox DocVQA数据集有效提升了VLMs的推理能力，为可解释和空间定位的视觉语言推理研究提供了重要资源。

Abstract: Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

</details>


### [20] [B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index](https://arxiv.org/abs/2511.15557)
*Selim Furkan Tekin,Rajesh Bordawekar*

Main category: cs.DB

TL;DR: 提出了一种基于磁盘的近似最近邻索引B+ANN，解决了HNSW算法的内存设计、随机内存访问、有限加速范围等问题，通过数据分块和B+树变体实现混合遍历，在质量和性能上均优于HNSW。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库主要使用基于图的HNSW算法，但存在内存设计、随机内存访问导致缓存性能下降、加速范围有限以及仅支持语义相似性查询等问题。

Method: 首先将输入数据分区为包含语义相似项的块，然后构建B+树变体在内存和磁盘上存储块，最后启用基于边和块的混合内存遍历。

Result: B+ANN在质量（召回率）和执行性能（每秒查询数）上均优于HNSW，缓存未命中相对改善19.23%，内存消耗和基于磁盘的构建时间比DiskANN算法减少24倍。

Conclusion: B+ANN不仅解决了HNSW的主要问题，还支持相似性导向ANN索引不支持的相异性查询。

Abstract: Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.

</details>


### [21] [A Decade of Systems for Human Data Interaction](https://arxiv.org/abs/2511.15585)
*Eugene Wu,Yiru Chen,Haneen Mohammed,Zezhou Huang*

Main category: cs.DB

TL;DR: HDI系统与传统数据管理有本质不同，需要满足源于可用性而非查询语义的延迟、正确性和一致性需求。接口与系统紧密耦合，必须协同设计。


<details>
  <summary>Details</summary>
Motivation: HDI系统面临与传统数据管理不同的挑战，需要满足用户交互体验的延迟、正确性和一致性需求，这些需求源于可用性而非查询语义。

Method: 通过调研实验室十年工作，采用接口与系统协同设计的方法，将系统创新和数据库理论应用于交互和可视化设计。

Result: HDI系统为可靠、交互式、AI驱动的应用奠定了基础，系统创新可以启发新的交互和可视化设计。

Conclusion: HDI系统是可靠、交互式、AI驱动应用的基础，接口与系统的紧密耦合既是挑战也是研究机会，需要协同设计方法。

Abstract: Human-data interaction (HDI) presents fundamentally different challenges from traditional data management. HDI systems must meet latency, correctness, and consistency needs that stem from usability rather than query semantics; failing to meet these expectations breaks the user experience. Moreover, interfaces and systems are tightly coupled; neither can easily be optimized in isolation, and effective solutions demand their co-design. This dependence also presents a research opportunity: rather than adapt systems to interface demands, systems innovations and database theory can also inspire new interaction and visualization designs. We survey a decade of our lab's work that embraces this coupling and argue that HDI systems are the foundation for reliable, interactive, AI-driven applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [22] [PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants](https://arxiv.org/abs/2511.14852)
*Mingkun Yu,Heming Zhong,Dan Huang,Yutong Lu,Jiazhi Jiang*

Main category: cs.DC

TL;DR: PolyKAN是一个GPU加速的Kolmogorov-Arnold网络算子库，通过四种优化技术实现比现有实现快1.2-12倍的推理和训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有的KAN并行实现GPU利用率低，阻碍了KAN在实际应用中的采用。

Method: 使用四种正交技术：查找表线性插值、2D分块、两阶段归约和系数布局重排序，将多项式KAN层的前向和反向传播融合到优化的CUDA内核中。

Result: 在高端GPU和消费级GPU上，对语音、音频增强和表格回归任务，PolyKAN比Triton + cuBLAS基线快1.2-10倍推理和1.4-12倍训练，且精度相同。

Conclusion: PolyKAN是首个通用的开源KAN实现，显著提升了KAN的GPU计算效率，推动了KAN在AI科学领域的实际应用。

Abstract: Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\times$ faster inference and $1.4$--$12\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.

</details>


### [23] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 提出了一种名为RemoteOptiGraph的分布式优化建模抽象，扩展了Plasmo.jl中的OptiGraph模型，支持在分布式内存环境中处理大规模优化问题，通过Benders分解实现了7.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 为分布式内存系统提供统一的优化建模方法，避免特定建模方法，并为开发通用元算法（如Benders或拉格朗日分解）提供基础。

Method: 扩展OptiGraph模型，引入InterWorkerEdges管理跨工作节点的链接约束，在Plasmo.jl中实现该抽象。

Result: 解决了美国西部混合整数容量扩展模型，包含超过1200万个变量和约束，使用Benders分解比无分解方法快7.5倍。

Conclusion: RemoteOptiGraph抽象为分布式优化提供了灵活统一的建模框架，显著提升了大规模问题的求解效率。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo.jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo.jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [24] [GPU-Initiated Networking for NCCL](https://arxiv.org/abs/2511.15076)
*Khaled Hamidouche,John Bachan,Pak Markthub,Peter-Jan Gootzen,Elena Agostini,Sylvain Jeaugey,Aamir Shafi,Georgios Theodorakis,Manjunath Gorentla Venkata*

Main category: cs.DC

TL;DR: NCCL 2.28引入了设备API，包括GPU发起的网络通信(GIN)功能，支持从CUDA内核直接进行远程内存操作，显著降低了MoE架构中的通信延迟。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载特别是MoE架构需要低延迟、细粒度的GPU间通信，传统的主机发起通信模式存在CPU协调开销，限制了计算与通信的紧密集成。

Method: GIN采用三层架构：1）NCCL核心主机端API用于设备通信器设置；2）设备端API支持从CUDA内核调用远程内存操作；3）网络插件架构提供两种语义（GPUDirect异步内核发起和代理），支持广泛硬件。

Result: 通过集成DeepEP MoE通信库的全面基准测试显示，GIN在NCCL统一运行时中提供设备发起通信，结合了低延迟操作与NCCL的集体算法和生产基础设施。

Conclusion: GIN成功实现了设备发起的GPU间通信，为需要紧密集成计算和通信的应用提供了显著的性能优势，特别是在MoE架构中。

Abstract: Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.
  NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.

</details>


### [25] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle是一个双层共识架构，通过BB-Core层实现低延迟高吞吐，BB-Guard层提供安全保护和恢复机制，在保持强安全性和活跃性的同时实现亚秒级最终性


<details>
  <summary>Details</summary>
Motivation: 解决区块链共识在安全性、延迟和去中心化之间的三难困境，高吞吐系统往往需要牺牲去中心化或对强对手的鲁棒性

Method: 采用双层架构：BB-Core是n=5f+1协议，用部分容错性换取低最终性延迟；BB-Guard提供去中心化时间戳、主动不当行为检测和同步恢复路径

Result: 实验显示BB-Core相比Mysticeti减少20-25%延迟，在温和同步假设下保持强安全性和活跃性，实现亚秒级最终性和高吞吐

Conclusion: BlueBottle双层架构成功平衡了共识三难困境，在保持去中心化和安全性的同时实现了高性能

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


### [26] [Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies](https://arxiv.org/abs/2511.15388)
*Lucianna Kiffer,Lioba Heimbach,Dennis Trautwein,Yann Vonlanthen,Oliver Gasser*

Main category: cs.DC

TL;DR: 对36个公共区块链网络进行首次纵向跨网络测量研究，揭示了网络规模、IP协议使用、地理分布等关键差异，并提出可扩展的区块链网络测量框架。


<details>
  <summary>Details</summary>
Motivation: 当前大多数区块链生态系统的对等网络层仍然不透明，缺乏系统性的网络层面测量研究，无法评估这些去中心化系统的网络弹性和去中心化程度。

Method: 部署15个主动爬虫，结合社区爬虫数据，进行9个月的纵向测量；利用以太坊发现协议推断辅助网络；开发基于端口扫描的通用测量方法，无需为每个区块链实现定制发现逻辑。

Result: 发现网络规模差异巨大（从少于10个到超过1万个活跃节点）；IPv4/IPv6使用趋势；自治系统和地理集中度分析；节点流失率、昼夜行为模式；发现协议的覆盖范围和冗余度。

Conclusion: 研究揭示了区块链网络在弹性、去中心化和可观测性方面的关键差异，提出的方法论为大规模去中心化网络测量提供了通用框架，支持持续监控和基准测试。

Abstract: Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.
  Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.

</details>


### [27] [When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit](https://arxiv.org/abs/2511.15421)
*Ethan Hicks,Joseph Oglio,Mikhail Nesterenko,Gokarna Sharma*

Main category: cs.DC

TL;DR: 分析比特币交易确认最终性与交易金额和用户风险承受能力的关系，提出基于区块深度和网络延迟的确认概率模型。


<details>
  <summary>Details</summary>
Motivation: 传统比特币交易确认采用固定区块深度（如6个区块），但未考虑交易金额和用户风险偏好的差异，需要更精细化的确认策略。

Method: 通过模拟不同网络延迟下的分叉情况和分析实际比特币数据，建立区块深度与确认撤销概率的关系，并应用前景理论将确认概率与交易金额和风险承受能力关联。

Result: 建立了交易确认概率与区块深度的定量关系，发现分叉概率随时间递减但永不归零，确认深度应根据交易特征动态调整。

Conclusion: 比特币交易确认应采用基于交易金额和用户风险偏好的动态确认策略，而非固定区块深度，以提高交易效率并控制风险。

Abstract: We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.

</details>


### [28] [Proving there is a leader without naming it](https://arxiv.org/abs/2511.15491)
*Laurent Feuilloley,Josef Erik Sedláček,Martin Slávik*

Main category: cs.DC

TL;DR: 该论文研究在特定图类中实现亚对数级别的领导者唯一性本地认证，探讨了在无环图等特殊拓扑结构中是否仍需要节点标识符。


<details>
  <summary>Details</summary>
Motivation: 传统领导者唯一性认证需要O(log n)位证书，但在某些特殊图类中可能实现更高效的认证。研究旨在探索网络结构如何影响本地认证复杂度，特别是在不含环图的拓扑中是否可能实现亚对数认证。

Method: 通过分析小直径图、弦图、网格图和稠密图等特殊图类，研究在这些拓扑结构中领导者唯一性认证的证书复杂度。

Result: 在特定图类中确实可以实现亚对数级别的领导者认证，且在某些情况下可能不需要节点标识符。

Conclusion: 网络拓扑结构对本地认证复杂度有显著影响，在特定图类中可以突破传统认证的下界限制，实现更高效的认证方案。

Abstract: Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.
  Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\log n)$ and $O(\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).
  A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $Θ(n)$ bits in general graphs, but only $O(\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]

</details>
