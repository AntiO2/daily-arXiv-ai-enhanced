{"id": "2602.15204", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15204", "abs": "https://arxiv.org/abs/2602.15204", "authors": ["Kevin Garner", "Polykarpos Thomadakis", "Nikos Chrisochoides"], "title": "Distributed Semi-Speculative Parallel Anisotropic Mesh Adaptation", "comment": "52 pages, 19 figures, 13 tables", "summary": "This paper presents a distributed memory method for anisotropic mesh adaptation that is designed to avoid the use of collective communication and global synchronization techniques. In the presented method, meshing functionality is separated from performance aspects by utilizing a separate entity for each - a multicore cc-NUMA-based (shared memory) mesh generation software and a parallel runtime system that is designed to help applications leverage the concurrency offered by emerging high-performance computing (HPC) architectures. First, an initial mesh is decomposed and its interface elements (subdomain boundaries) are adapted on a single multicore node (shared memory). Subdomains are then distributed among the nodes of an HPC cluster so that their interior elements are adapted while interface elements (already adapted) remain frozen to maintain mesh conformity. Lessons are presented regarding some re-designs of the shared memory software and how its speculative execution model is utilized by the distributed memory method to achieve good performance. The presented method is shown to generate meshes (of up to approximately 1 billion elements) with comparable quality and performance to existing state-of-the-art HPC meshing software.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u907f\u514d\u96c6\u4f53\u901a\u4fe1\u548c\u5168\u5c40\u540c\u6b65\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u5404\u5411\u5f02\u6027\u7f51\u683c\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u7f51\u683c\u751f\u6210\u4e0e\u6027\u80fd\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u8fbe10\u4ebf\u5143\u7d20\u7684\u5927\u89c4\u6a21\u7f51\u683c\u751f\u6210\u3002", "motivation": "\u4f20\u7edfHPC\u7f51\u683c\u81ea\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u96c6\u4f53\u901a\u4fe1\u548c\u5168\u5c40\u540c\u6b65\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u80fd\u5145\u5206\u5229\u7528\u65b0\u5174HPC\u67b6\u6784\u5e76\u53d1\u6027\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u79bb\u67b6\u6784\uff1a\u591a\u6838cc-NUMA\u5171\u4eab\u5185\u5b58\u7f51\u683c\u751f\u6210\u8f6f\u4ef6\u8d1f\u8d23\u8ba1\u7b97\uff0c\u5e76\u884c\u8fd0\u884c\u65f6\u7cfb\u7edf\u7ba1\u7406\u6027\u80fd\u3002\u5148\u5728\u5355\u8282\u70b9\u4e0a\u5206\u89e3\u521d\u59cb\u7f51\u683c\u5e76\u81ea\u9002\u5e94\u63a5\u53e3\u5143\u7d20\uff0c\u7136\u540e\u5c06\u5b50\u57df\u5206\u53d1\u5230\u96c6\u7fa4\u8282\u70b9\uff0c\u81ea\u9002\u5e94\u5185\u90e8\u5143\u7d20\u65f6\u51bb\u7ed3\u5df2\u9002\u5e94\u7684\u63a5\u53e3\u5143\u7d20\u4ee5\u4fdd\u6301\u7f51\u683c\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u9ad8\u8fbe\u7ea610\u4ebf\u5143\u7d20\u7684\u7f51\u683c\uff0c\u5728\u8d28\u91cf\u548c\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684HPC\u7f51\u683c\u8f6f\u4ef6\u76f8\u5f53\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u79bb\u5173\u6ce8\u70b9\u548c\u5229\u7528\u63a8\u6d4b\u6267\u884c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u96c6\u4f53\u901a\u4fe1\u548c\u5168\u5c40\u540c\u6b65\u7684\u9ad8\u6548\u5206\u5e03\u5f0f\u5185\u5b58\u7f51\u683c\u81ea\u9002\u5e94\uff0c\u4e3a\u5927\u89c4\u6a21HPC\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15356", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15356", "abs": "https://arxiv.org/abs/2602.15356", "authors": ["Patrick G. Bridges", "Derek Schafer", "Jack Lange", "James B. White", "Anthony Skjellum", "Evan Suggs", "Thomas Hines", "Purushotham Bangalore", "Matthew G. F. Dosanjh", "Whit Schonbein"], "title": "Co-Design and Evaluation of a CPU-Free MPI GPU Communication Abstraction and Implementation", "comment": null, "summary": "Removing the CPU from the communication fast path is essential to efficient GPU-based ML and HPC application performance. However, existing GPU communication APIs either continue to rely on the CPU for communication or rely on APIs that place significant synchronization burdens on programmers. In this paper we describe the design, implementation, and evaluation of an MPI-based GPU communication API enabling easy-to-use, high-performance, CPU-free communication. This API builds on previously proposed MPI extensions and leverages HPE Slingshot 11 network card capabilities. We demonstrate the utility and performance of the API by showing how the API naturally enables CPU-free gather/scatter halo exchange communication primitives in the Cabana/Kokkos performance portability framework, and through a performance comparison with Cray MPICH on the Frontier and Tuolumne supercomputers. Results from this evaluation show up to a 50% reduction in medium message latency in simple GPU ping-pong exchanges and a 28% speedup improvement when strong scaling a halo-exchange benchmark to 8,192 GPUs of the Frontier supercomputer.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMPI\u7684GPU\u901a\u4fe1API\uff0c\u5b9e\u73b0\u65e0\u9700CPU\u53c2\u4e0e\u7684\u9ad8\u6027\u80fdGPU\u901a\u4fe1\uff0c\u5728Frontier\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5c55\u793a\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709GPU\u901a\u4fe1API\u8981\u4e48\u4f9d\u8d56CPU\u53c2\u4e0e\u901a\u4fe1\uff0c\u8981\u4e48\u7ed9\u7a0b\u5e8f\u5458\u5e26\u6765\u6c89\u91cd\u7684\u540c\u6b65\u8d1f\u62c5\uff0c\u8fd9\u5f71\u54cd\u4e86GPU\u5728ML\u548cHPC\u5e94\u7528\u4e2d\u7684\u901a\u4fe1\u6548\u7387", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u57fa\u4e8eMPI\u7684GPU\u901a\u4fe1API\uff0c\u5229\u7528HPE Slingshot 11\u7f51\u5361\u529f\u80fd\uff0c\u6784\u5efaCPU-free\u901a\u4fe1\u673a\u5236\uff0c\u5e76\u5728Cabana/Kokkos\u6027\u80fd\u53ef\u79fb\u690d\u6846\u67b6\u4e2d\u5b9e\u73b0gather/scatter halo\u4ea4\u6362\u539f\u8bed", "result": "\u5728Frontier\u548cTuolumne\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u4e0eCray MPICH\u5bf9\u6bd4\uff0c\u4e2d\u7b49\u6d88\u606f\u5ef6\u8fdf\u964d\u4f4e50%\uff0c\u57288,192\u4e2aGPU\u4e0a\u8fd0\u884chalo\u4ea4\u6362\u57fa\u51c6\u6d4b\u8bd5\u65f6\u83b7\u5f9728%\u7684\u52a0\u901f\u63d0\u5347", "conclusion": "\u63d0\u51fa\u7684MPI-based GPU\u901a\u4fe1API\u80fd\u591f\u5b9e\u73b0\u6613\u7528\u3001\u9ad8\u6027\u80fd\u3001\u65e0\u9700CPU\u53c2\u4e0e\u7684\u901a\u4fe1\uff0c\u663e\u8457\u63d0\u5347GPU\u901a\u4fe1\u6548\u7387\u548c\u5e94\u7528\u7a0b\u5e8f\u6027\u80fd"}}
{"id": "2602.15379", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.15379", "abs": "https://arxiv.org/abs/2602.15379", "authors": ["Zhihao Shu", "Md Musfiqur Rahman Sanim", "Hangyu Zheng", "Kunxiong Zhu", "Miao Yin", "Gagan Agrawal", "Wei Niu"], "title": "FlashMem: Supporting Modern DNN Workloads on Mobile with GPU Memory Hierarchy Optimizations", "comment": null, "summary": "The increasing size and complexity of modern deep neural networks (DNNs) pose significant challenges for on-device inference on mobile GPUs, with limited memory and computational resources. Existing DNN acceleration frameworks primarily deploy a weight preloading strategy, where all model parameters are loaded into memory before execution on mobile GPUs. We posit that this approach is not adequate for modern DNN workloads that comprise very large model(s) and possibly execution of several distinct models in succession. In this work, we introduce FlashMem, a memory streaming framework designed to efficiently execute large-scale modern DNNs and multi-DNN workloads while minimizing memory consumption and reducing inference latency. Instead of fully preloading weights, FlashMem statically determines model loading schedules and dynamically streams them on demand, leveraging 2.5D texture memory to minimize data transformations and improve execution efficiency. Experimental results on 11 models demonstrate that FlashMem achieves 2.0x to 8.4x memory reduction and 1.7x to 75.0x speedup compared to existing frameworks, enabling efficient execution of large-scale models and multi-DNN support on resource-constrained mobile GPUs.", "AI": {"tldr": "FlashMem\u662f\u4e00\u4e2a\u5185\u5b58\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6d41\u5f0f\u52a0\u8f7d\u6a21\u578b\u6743\u91cd\u800c\u975e\u5b8c\u5168\u9884\u52a0\u8f7d\uff0c\u663e\u8457\u51cf\u5c11\u79fb\u52a8GPU\u4e0a\u7684\u5185\u5b58\u5360\u7528\u5e76\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u652f\u6301\u5927\u89c4\u6a21DNN\u548c\u591aDNN\u5de5\u4f5c\u8d1f\u8f7d\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u5e9e\u5927\u4e14\u590d\u6742\uff0c\u5728\u79fb\u52a8GPU\u4e0a\u8fdb\u884c\u63a8\u7406\u9762\u4e34\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\u3002\u73b0\u6709DNN\u52a0\u901f\u6846\u67b6\u91c7\u7528\u6743\u91cd\u9884\u52a0\u8f7d\u7b56\u7565\uff0c\u5c06\u6240\u6709\u6a21\u578b\u53c2\u6570\u5728\u6267\u884c\u524d\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u4e8e\u5305\u542b\u8d85\u5927\u6a21\u578b\u548c\u53ef\u80fd\u9700\u8981\u8fde\u7eed\u6267\u884c\u591a\u4e2a\u4e0d\u540c\u6a21\u578b\u7684\u73b0\u4ee3DNN\u5de5\u4f5c\u8d1f\u8f7d\u6765\u8bf4\u4e0d\u591f\u5145\u5206\u3002", "method": "FlashMem\u91c7\u7528\u5185\u5b58\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u9759\u6001\u786e\u5b9a\u6a21\u578b\u52a0\u8f7d\u8c03\u5ea6\u548c\u52a8\u6001\u6309\u9700\u6d41\u5f0f\u4f20\u8f93\u6743\u91cd\uff0c\u5229\u75282.5D\u7eb9\u7406\u5185\u5b58\u6765\u6700\u5c0f\u5316\u6570\u636e\u8f6c\u6362\u5e76\u63d0\u9ad8\u6267\u884c\u6548\u7387\uff0c\u800c\u4e0d\u662f\u5b8c\u5168\u9884\u52a0\u8f7d\u6743\u91cd\u3002", "result": "\u572811\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6846\u67b6\u76f8\u6bd4\uff0cFlashMem\u5b9e\u73b0\u4e862.0\u500d\u52308.4\u500d\u7684\u5185\u5b58\u51cf\u5c11\u548c1.7\u500d\u523075.0\u500d\u7684\u52a0\u901f\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8GPU\u4e0a\u9ad8\u6548\u6267\u884c\u5927\u89c4\u6a21\u6a21\u578b\u5e76\u652f\u6301\u591aDNN\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "FlashMem\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u6d41\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8GPU\u4e0a\u5927\u89c4\u6a21DNN\u63a8\u7406\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.15794", "categories": ["cs.DC", "cs.ET", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.15794", "abs": "https://arxiv.org/abs/2602.15794", "authors": ["Boris Sedlak", "V\u00edctor Casamayor Pujol", "Ildefons Magrans de Abril", "Praveen Kumar Donta", "Adel N. Toosi", "Schahram Dustdar"], "title": "Service Orchestration in the Computing Continuum: Structural Challenges and Vision", "comment": null, "summary": "The Computing Continuum (CC) integrates different layers of processing infrastructure, from Edge to Cloud, to optimize service quality through ubiquitous and reliable computation. Compared to central architectures, however, heterogeneous and dynamic infrastructure increases the complexity for service orchestration. To guide research, this article first summarizes structural problems of the CC, and then, envisions an ideal solution for autonomous service orchestration across the CC. As one instantiation, we show how Active Inference, a concept from neuroscience, can support self-organizing services in continuously interpreting their environment to optimize service quality. Still, we conclude that no existing solution achieves our vision, but that research on service orchestration faces several structural challenges. Most notably: provide standardized simulation and evaluation environments for comparing the performance of orchestration mechanisms. Together, the challenges outline a research roadmap toward resilient and scalable service orchestration in the CC.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u8ba1\u7b97\u8fde\u7eed\u4f53\uff08CC\uff09\u4e2d\u670d\u52a1\u7f16\u6392\u7684\u7ed3\u6784\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u81ea\u4e3b\u670d\u52a1\u7f16\u6392\u7684\u7406\u60f3\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4ee5\u4e3b\u52a8\u63a8\u7406\u4e3a\u4f8b\u8bf4\u660e\u5982\u4f55\u5b9e\u73b0\u81ea\u7ec4\u7ec7\u670d\u52a1\uff0c\u6700\u540e\u6307\u51fa\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u6311\u6218\u548c\u8def\u7ebf\u56fe\u3002", "motivation": "\u8ba1\u7b97\u8fde\u7eed\u4f53\u6574\u5408\u4e86\u4ece\u8fb9\u7f18\u5230\u4e91\u7684\u4e0d\u540c\u5904\u7406\u5c42\uff0c\u4ee5\u4f18\u5316\u670d\u52a1\u8d28\u91cf\u3002\u7136\u800c\uff0c\u76f8\u6bd4\u4e2d\u5fc3\u5316\u67b6\u6784\uff0c\u5f02\u6784\u548c\u52a8\u6001\u7684\u57fa\u7840\u8bbe\u65bd\u589e\u52a0\u4e86\u670d\u52a1\u7f16\u6392\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u7814\u7a76\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9996\u5148\u603b\u7ed3\u8ba1\u7b97\u8fde\u7eed\u4f53\u7684\u7ed3\u6784\u6027\u95ee\u9898\uff0c\u7136\u540e\u8bbe\u60f3\u81ea\u4e3b\u670d\u52a1\u7f16\u6392\u7684\u7406\u60f3\u89e3\u51b3\u65b9\u6848\u3002\u4ee5\u4e3b\u52a8\u63a8\u7406\uff08\u6765\u81ea\u795e\u7ecf\u79d1\u5b66\u7684\u6982\u5ff5\uff09\u4e3a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u652f\u6301\u81ea\u7ec4\u7ec7\u670d\u52a1\u6301\u7eed\u89e3\u91ca\u73af\u5883\u4ee5\u4f18\u5316\u670d\u52a1\u8d28\u91cf\u3002", "result": "\u76ee\u524d\u6ca1\u6709\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u5b8c\u5168\u5b9e\u73b0\u4f5c\u8005\u7684\u613f\u666f\uff0c\u670d\u52a1\u7f16\u6392\u7814\u7a76\u9762\u4e34\u591a\u4e2a\u7ed3\u6784\u6027\u6311\u6218\uff0c\u6700\u91cd\u8981\u7684\u662f\u9700\u8981\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u6a21\u62df\u548c\u8bc4\u4f30\u73af\u5883\u6765\u6bd4\u8f83\u4e0d\u540c\u7f16\u6392\u673a\u5236\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u6311\u6218\u5171\u540c\u52fe\u52d2\u51fa\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u5b9e\u73b0\u5f39\u6027\u548c\u53ef\u6269\u5c55\u670d\u52a1\u7f16\u6392\u7684\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002"}}
{"id": "2602.15360", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15360", "abs": "https://arxiv.org/abs/2602.15360", "authors": ["Boyan Wang", "Zhuochen Fan", "Dayu Wang", "Fangcheng Fu", "Zeyu Luan", "Lei Zou", "Qing Li", "Tong Yang"], "title": "Crane: An Accurate and Scalable Neural Sketch for Graph Stream Summarization", "comment": null, "summary": "Graph streams are rapidly evolving sequences of edges that convey continuously changing relationships among entities, playing a crucial role in domains such as networking, finance, and cybersecurity. Their massive scale and high dynamism make obtaining accurate statistics challenging with limited memory constraints. Traditional methods summarize graph streams through hand-crafted sketches, while recent studies have begun to replace these sketches with neural counterparts to improve adaptability and accuracy. However, this shift faces a major challenge: under limited memory, dominant frequent items tend to overshadow rare ones, hindering the neural network's ability to recover accurate statistics. To address this, we propose Crane, a hierarchical neural sketch architecture for graph stream summarization. Crane uses a hierarchical carry mechanism that automatically elevates frequent items to higher memory layers, reducing interference between frequent and infrequent items within the same layer. To better accommodate real-world deployment, Crane further adopts an adaptive memory expansion strategy that dynamically adds new layers once the occupancy of the top layer exceeds a threshold, enabling scalability across diverse data magnitudes. Extensive experiments on various datasets ranging from 20K to 60M edges demonstrate that Crane reduces estimation error by roughly 10x compared to state-of-the-art methods.", "AI": {"tldr": "Crane\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56fe\u6d41\u6458\u8981\u7684\u5206\u5c42\u795e\u7ecf\u8349\u56fe\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u643a\u5e26\u673a\u5236\u51cf\u5c11\u9891\u7e41\u9879\u4e0e\u7a00\u6709\u9879\u4e4b\u95f4\u7684\u5e72\u6270\uff0c\u5728\u6709\u9650\u5185\u5b58\u4e0b\u663e\u8457\u63d0\u5347\u7edf\u8ba1\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u56fe\u6d41\u6570\u636e\u89c4\u6a21\u5e9e\u5927\u4e14\u52a8\u6001\u53d8\u5316\uff0c\u4f20\u7edf\u624b\u5de5\u8349\u56fe\u65b9\u6cd5\u5728\u6709\u9650\u5185\u5b58\u7ea6\u675f\u4e0b\u96be\u4ee5\u51c6\u786e\u7edf\u8ba1\u3002\u867d\u7136\u795e\u7ecf\u8349\u56fe\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\uff0c\u4f46\u5728\u6709\u9650\u5185\u5b58\u4e0b\uff0c\u9891\u7e41\u9879\u4f1a\u538b\u5236\u7a00\u6709\u9879\uff0c\u963b\u788d\u795e\u7ecf\u7f51\u7edc\u6062\u590d\u51c6\u786e\u7edf\u8ba1\u3002", "method": "Crane\u91c7\u7528\u5206\u5c42\u795e\u7ecf\u8349\u56fe\u67b6\u6784\uff1a1\uff09\u5206\u5c42\u643a\u5e26\u673a\u5236\u81ea\u52a8\u5c06\u9891\u7e41\u9879\u63d0\u5347\u5230\u66f4\u9ad8\u5185\u5b58\u5c42\uff0c\u51cf\u5c11\u540c\u5c42\u5185\u9891\u7e41\u9879\u4e0e\u7a00\u6709\u9879\u7684\u5e72\u6270\uff1b2\uff09\u81ea\u9002\u5e94\u5185\u5b58\u6269\u5c55\u7b56\u7565\u5728\u9876\u5c42\u5360\u7528\u8d85\u8fc7\u9608\u503c\u65f6\u52a8\u6001\u6dfb\u52a0\u65b0\u5c42\uff0c\u9002\u5e94\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u3002", "result": "\u572820K\u523060M\u8fb9\u7684\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCrane\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5c06\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u7ea610\u500d\u3002", "conclusion": "Crane\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6d41\u6458\u8981\u4e2d\u9891\u7e41\u9879\u538b\u5236\u7a00\u6709\u9879\u7684\u95ee\u9898\uff0c\u5728\u6709\u9650\u5185\u5b58\u4e0b\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u7edf\u8ba1\u4f30\u8ba1\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u9002\u5e94\u6027\u3002"}}
{"id": "2602.15037", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15037", "abs": "https://arxiv.org/abs/2602.15037", "authors": ["Mayank Ravishankara"], "title": "CircuChain: Disentangling Competence and Compliance in LLM Circuit Analysis", "comment": null, "summary": "As large language models (LLMs) advance toward expert-level performance in engineering domains, reliable reasoning under user-specified constraints becomes critical. In circuit analysis, for example, a numerically correct solution is insufficient if it violates established methodological conventions such as mesh directionality or polarity assignments, errors that can propagate in safety-critical systems. Yet it remains unclear whether frontier models truly apply first-principles reasoning or rely on entrenched training priors that conflict with explicit instructions. We introduce CircuChain, a diagnostic benchmark designed to disentangle instruction compliance from physical reasoning competence in electrical circuit analysis. CircuChain consists of counterbalanced Control/Trap problem pairs across five canonical circuit topologies, augmented with systematic variations in sign conventions, current orientations, and polarity definitions. A multi-stage verification pipeline, combining symbolic solvers, SPICE simulation, and an LLM-based error taxonomy, enables fine-grained attribution of failures to convention errors, physics errors, arithmetic mistakes, or hallucinations. Across 100 tasks per model, we observe a consistent Compliance-Competence Divergence. The strongest model evaluated exhibits near-perfect physical reasoning but a high rate of convention violations when Trap conditions deliberately invert natural sign patterns. Conversely, weaker models display lower physical fidelity yet superior adherence to explicit instructions. These results suggest that increased model capability does not guarantee improved constraint alignment and highlight the need for new evaluation frameworks that stress instruction-following under mathematically rigid domains. CircuChain provides one such framework and offers actionable insights for both engineering education and AI alignment research.", "AI": {"tldr": "CircuChain\u662f\u4e00\u4e2a\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u5728\u7535\u8def\u5206\u6790\u4e2d\u533a\u5206\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u548c\u7269\u7406\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6700\u5f3a\u6a21\u578b\u7269\u7406\u63a8\u7406\u8fd1\u4e4e\u5b8c\u7f8e\u4f46\u6307\u4ee4\u9075\u5faa\u5dee\uff0c\u800c\u8f83\u5f31\u6a21\u578b\u7269\u7406\u63a8\u7406\u5dee\u4f46\u6307\u4ee4\u9075\u5faa\u597d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u7a0b\u9886\u57df\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\uff0c\u5728\u7528\u6237\u6307\u5b9a\u7ea6\u675f\u4e0b\u7684\u53ef\u9760\u63a8\u7406\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u4e0d\u6e05\u695a\u524d\u6cbf\u6a21\u578b\u662f\u771f\u6b63\u5e94\u7528\u57fa\u672c\u539f\u7406\u63a8\u7406\uff0c\u8fd8\u662f\u4f9d\u8d56\u4e0e\u660e\u786e\u6307\u4ee4\u51b2\u7a81\u7684\u8bad\u7ec3\u5148\u9a8c\u3002", "method": "CircuChain\u57fa\u51c6\u5305\u542b\u4e94\u4e2a\u5178\u578b\u7535\u8def\u62d3\u6251\u7684Control/Trap\u95ee\u9898\u5bf9\uff0c\u7cfb\u7edf\u53d8\u5316\u7b26\u53f7\u7ea6\u5b9a\u3001\u7535\u6d41\u65b9\u5411\u548c\u6781\u6027\u5b9a\u4e49\u3002\u91c7\u7528\u591a\u9636\u6bb5\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u7ed3\u5408\u7b26\u53f7\u6c42\u89e3\u5668\u3001SPICE\u4eff\u771f\u548c\u57fa\u4e8eLLM\u7684\u9519\u8bef\u5206\u7c7b\u6cd5\u3002", "result": "\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\"\u9075\u5faa-\u80fd\u529b\u5206\u6b67\"\uff1a\u6700\u5f3a\u6a21\u578b\u7269\u7406\u63a8\u7406\u8fd1\u4e4e\u5b8c\u7f8e\uff0c\u4f46\u5728Trap\u6761\u4ef6\u4e0b\u8fdd\u53cd\u7ea6\u5b9a\u7387\u9ad8\uff1b\u8f83\u5f31\u6a21\u578b\u7269\u7406\u4fdd\u771f\u5ea6\u4f4e\u4f46\u6307\u4ee4\u9075\u5faa\u66f4\u597d\u3002\u6a21\u578b\u80fd\u529b\u589e\u5f3a\u4e0d\u4fdd\u8bc1\u7ea6\u675f\u5bf9\u9f50\u6539\u5584\u3002", "conclusion": "\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u5f3a\u8c03\u6570\u5b66\u4e25\u683c\u9886\u57df\u4e0b\u7684\u6307\u4ee4\u9075\u5faa\u3002CircuChain\u63d0\u4f9b\u4e86\u8fd9\u6837\u7684\u6846\u67b6\uff0c\u4e3a\u5de5\u7a0b\u6559\u80b2\u548cAI\u5bf9\u9f50\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2602.15488", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15488", "abs": "https://arxiv.org/abs/2602.15488", "authors": ["Yuanhang Yu", "Dawei Cheng", "Ying Zhang", "Lu Qin", "Wenjie Zhang", "Xuemin Lin"], "title": "Efficient Approximate Nearest Neighbor Search under Multi-Attribute Range Filter", "comment": null, "summary": "Nearest neighbor search on high-dimensional vectors is fundamental in modern AI and database systems. In many real-world applications, queries involve constraints on multiple numeric attributes, giving rise to range-filtering approximate nearest neighbor search (RFANNS). While there exist RFANNS indexes for single-attribute range predicates, extending them to the multi-attribute setting is nontrivial and often ineffective. In this paper, we propose KHI, an index for multi-attribute RFANNS that combines an attribute-space partitioning tree with HNSW graphs attached to tree nodes. A skew-aware splitting rule bounds the tree height by $O(\\log n)$, and queries are answered by routing through the tree and running greedy search on the HNSW graphs. Experiments on four real-world datasets show that KHI consistently achieves high query throughput while maintaining high recall. Compared with the state-of-the-art RFANNS baseline, KHI improves QPS by $2.46\\times$ on average and up to $16.22\\times$ on the hard dataset, with larger gains for smaller selectivity, larger $k$, and higher predicate cardinality.", "AI": {"tldr": "KHI\uff1a\u4e00\u79cd\u7ed3\u5408\u5c5e\u6027\u7a7a\u95f4\u5212\u5206\u6811\u548cHNSW\u56fe\u7684\u591a\u5c5e\u6027\u8303\u56f4\u8fc7\u6ee4\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u7d22\u5f15\uff0c\u663e\u8457\u63d0\u5347\u67e5\u8be2\u6027\u80fd", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u67e5\u8be2\u5e38\u6d89\u53ca\u591a\u4e2a\u6570\u503c\u5c5e\u6027\u7684\u7ea6\u675f\uff08\u591a\u5c5e\u6027\u8303\u56f4\u8fc7\u6ee4\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22RFANNS\uff09\uff0c\u73b0\u6709\u5355\u5c5e\u6027RFANNS\u7d22\u5f15\u96be\u4ee5\u6709\u6548\u6269\u5c55\u5230\u591a\u5c5e\u6027\u573a\u666f", "method": "\u63d0\u51faKHI\u7d22\u5f15\uff0c\u7ed3\u5408\u5c5e\u6027\u7a7a\u95f4\u5212\u5206\u6811\uff08\u4f7f\u7528\u504f\u659c\u611f\u77e5\u5206\u5272\u89c4\u5219\u63a7\u5236\u6811\u9ad8\u4e3aO(log n)\uff09\u548c\u8282\u70b9\u4e0a\u9644\u52a0\u7684HNSW\u56fe\uff0c\u901a\u8fc7\u6811\u8def\u7531\u548cHNSW\u8d2a\u5a6a\u641c\u7d22\u56de\u7b54\u67e5\u8be2", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cKHI\u76f8\u6bd4\u6700\u5148\u8fdbRFANNS\u57fa\u7ebf\u5e73\u5747\u63d0\u5347QPS 2.46\u500d\uff0c\u5728\u56f0\u96be\u6570\u636e\u96c6\u4e0a\u6700\u9ad8\u63d0\u534716.22\u500d\uff0c\u5bf9\u5c0f\u9009\u62e9\u6027\u3001\u5927k\u503c\u548c\u9ad8\u8c13\u8bcd\u57fa\u6570\u60c5\u51b5\u63d0\u5347\u66f4\u663e\u8457", "conclusion": "KHI\u4e3a\u591a\u5c5e\u6027RFANNS\u95ee\u9898\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u67e5\u8be2\u541e\u5410\u91cf"}}
{"id": "2602.15090", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.15090", "abs": "https://arxiv.org/abs/2602.15090", "authors": ["Sebastian Lobentanzer"], "title": "The Agentic Automation Canvas: a structured framework for agentic AI project design", "comment": "10 pages, 1 figure", "summary": "Agentic AI prototypes are being deployed across domains with increasing speed, yet no methodology for their structured design, governance, and prospective evaluation has been established. Existing AI documentation practices and guidelines - Model Cards, Datasheets, or NIST AI RMF - are either retrospective or lack machine-readability and interoperability. We present the Agentic Automation Canvas (AAC), a structured framework for the prospective design of agentic systems and a tool to facilitate communication between their users and developers. The AAC captures six dimensions of an automation project: definition and scope; user expectations with quantified benefit metrics; developer feasibility assessments; governance staging; data access and sensitivity; and outcomes. The framework is implemented as a semantic web-compatible metadata schema with controlled vocabulary and mappings to established ontologies such as Schema.org and W3C DCAT. It is made accessible through a privacy-preserving, fully client-side web application with real-time validation. Completed canvases export as FAIR-compliant RO-Crates, yielding versioned, shareable, and machine-interoperable project contracts between users and developers. We describe the schema design, benefit quantification model, and prospective application to diverse use cases from research, clinical, and institutional settings. The AAC and its web application are available as open-source code and interactive web form at https://aac.slolab.ai", "AI": {"tldr": "\u63d0\u51fa\u4e86Agentic Automation Canvas (AAC)\u6846\u67b6\uff0c\u7528\u4e8e\u524d\u77bb\u6027\u8bbe\u8ba1\u81ea\u4e3b\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u7f51\u517c\u5bb9\u7684\u5143\u6570\u636e\u6a21\u5f0f\u548c\u9690\u79c1\u4fdd\u62a4\u7684Web\u5e94\u7528\u5b9e\u73b0\u673a\u5668\u53ef\u8bfb\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u9879\u76ee\u5951\u7ea6\u3002", "motivation": "\u81ea\u4e3b\u667a\u80fd\u4ee3\u7406\u5728\u5404\u9886\u57df\u5feb\u901f\u90e8\u7f72\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5316\u8bbe\u8ba1\u3001\u6cbb\u7406\u548c\u524d\u77bb\u6027\u8bc4\u4f30\u7684\u65b9\u6cd5\u8bba\u3002\u73b0\u6709AI\u6587\u6863\u5b9e\u8df5\uff08\u5982Model Cards\u3001Datasheets\uff09\u8981\u4e48\u662f\u56de\u987e\u6027\u7684\uff0c\u8981\u4e48\u7f3a\u4e4f\u673a\u5668\u53ef\u8bfb\u6027\u548c\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u5f00\u53d1\u4e86Agentic Automation Canvas (AAC)\u6846\u67b6\uff0c\u5305\u542b\u516d\u4e2a\u7ef4\u5ea6\uff1a\u5b9a\u4e49\u4e0e\u8303\u56f4\u3001\u7528\u6237\u671f\u671b\u4e0e\u91cf\u5316\u6548\u76ca\u6307\u6807\u3001\u5f00\u53d1\u8005\u53ef\u884c\u6027\u8bc4\u4f30\u3001\u6cbb\u7406\u9636\u6bb5\u3001\u6570\u636e\u8bbf\u95ee\u4e0e\u654f\u611f\u6027\u3001\u7ed3\u679c\u3002\u5b9e\u73b0\u4e3a\u8bed\u4e49\u7f51\u517c\u5bb9\u7684\u5143\u6570\u636e\u6a21\u5f0f\uff0c\u5305\u542b\u53d7\u63a7\u8bcd\u6c47\u8868\u548cSchema.org\u3001W3C DCAT\u7b49\u672c\u4f53\u6620\u5c04\uff0c\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u7684\u5ba2\u6237\u7aefWeb\u5e94\u7528\u63d0\u4f9b\u5b9e\u65f6\u9a8c\u8bc1\u3002", "result": "AAC\u6846\u67b6\u80fd\u591f\u751f\u6210FAIR\u517c\u5bb9\u7684RO-Crates\uff0c\u521b\u5efa\u7248\u672c\u5316\u3001\u53ef\u5171\u4eab\u3001\u673a\u5668\u53ef\u4e92\u64cd\u4f5c\u7684\u9879\u76ee\u5951\u7ea6\u3002\u5df2\u5e94\u7528\u4e8e\u7814\u7a76\u3001\u4e34\u5e8a\u548c\u673a\u6784\u8bbe\u7f6e\u4e2d\u7684\u591a\u6837\u5316\u7528\u4f8b\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u4ee3\u7801\u548c\u4ea4\u4e92\u5f0fWeb\u8868\u5355\u63d0\u4f9b\u3002", "conclusion": "AAC\u4e3a\u81ea\u4e3b\u667a\u80fd\u7cfb\u7edf\u7684\u7ed3\u6784\u5316\u8bbe\u8ba1\u3001\u6cbb\u7406\u548c\u524d\u77bb\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709AI\u6587\u6863\u5b9e\u8df5\u7684\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4e86\u7528\u6237\u4e0e\u5f00\u53d1\u8005\u4e4b\u95f4\u7684\u6c9f\u901a\u548c\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2602.15681", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15681", "abs": "https://arxiv.org/abs/2602.15681", "authors": ["Eldar Hacohen", "Yuval Moskovitch", "Amit Somech"], "title": "A universal LLM Framework for General Query Refinements", "comment": null, "summary": "Numerous studies have explored the SQL query refinement problem, where the objective is to minimally modify an input query so that it satisfies a specified set of constraints. However, these works typically target restricted classes of queries or constraints. We present OmniTune, a general framework for refining arbitrary SQL queries using LLM-based optimization by prompting (OPRO). OmniTune employs a two-step OPRO scheme that explores promising refinement subspaces and samples candidates within them, supported by concise history and skyline summaries for effective feedback.\n  Experiments on a comprehensive benchmark demonstrate that OmniTune handles both previously studied refinement tasks and more complex scenarios beyond the scope of existing solutions.", "AI": {"tldr": "OmniTune\u662f\u4e00\u4e2a\u901a\u7528\u7684SQL\u67e5\u8be2\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528LLM\u57fa\u4e8e\u63d0\u793a\u7684\u4f18\u5316(OPRO)\u6765\u7cbe\u70bc\u4efb\u610fSQL\u67e5\u8be2\uff0c\u901a\u8fc7\u4e24\u6b65\u9aa4\u63a2\u7d22\u6709\u524d\u666f\u7684\u7cbe\u70bc\u5b50\u7a7a\u95f4\u5e76\u91c7\u6837\u5019\u9009\u65b9\u6848\u3002", "motivation": "\u73b0\u6709SQL\u67e5\u8be2\u7cbe\u70bc\u7814\u7a76\u901a\u5e38\u9488\u5bf9\u53d7\u9650\u7684\u67e5\u8be2\u7c7b\u522b\u6216\u7ea6\u675f\uff0c\u7f3a\u4e4f\u5904\u7406\u4efb\u610fSQL\u67e5\u8be2\u548c\u590d\u6742\u7ea6\u675f\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u7684\u63d0\u793a\u4f18\u5316(OPRO)\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u6b65\u9aa4\u65b9\u6848\uff1a1)\u63a2\u7d22\u6709\u524d\u666f\u7684\u7cbe\u70bc\u5b50\u7a7a\u95f4\uff1b2)\u5728\u5b50\u7a7a\u95f4\u5185\u91c7\u6837\u5019\u9009\u65b9\u6848\uff0c\u5e76\u4f7f\u7528\u7b80\u6d01\u7684\u5386\u53f2\u548c\u5929\u9645\u7ebf\u6458\u8981\u63d0\u4f9b\u6709\u6548\u53cd\u9988\u3002", "result": "\u5728\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOmniTune\u65e2\u80fd\u5904\u7406\u5148\u524d\u7814\u7a76\u7684\u7cbe\u70bc\u4efb\u52a1\uff0c\u4e5f\u80fd\u5904\u7406\u8d85\u51fa\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8303\u56f4\u7684\u66f4\u590d\u6742\u573a\u666f\u3002", "conclusion": "OmniTune\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684SQL\u67e5\u8be2\u7cbe\u70bc\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u5404\u79cd\u67e5\u8be2\u548c\u7ea6\u675f\u7c7b\u578b\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.15228", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.15228", "abs": "https://arxiv.org/abs/2602.15228", "authors": ["Zaiyu Cheng", "Antonio Mastropaolo"], "title": "An Empirical Study on the Effects of System Prompts in Instruction-Tuned Models for Code Generation", "comment": "34 pages, 12 tables, 3 figures", "summary": "Instruction-tuned Language Models (ILMs) have become essential components of modern AI systems, demonstrating exceptional versatility across natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs -- commonly referred to as Code Language Models (CLMs) -- translate human intent into executable programs. While progress has been driven by advances in scaling and training methodologies, one critical aspect remains underexplored: the impact of system prompts on both general-purpose ILMs and specialized CLMs for code generation. We systematically evaluate how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect code assistant. Our experimental setting spans 360 configurations across four models, five system prompts, three prompting strategies, two languages, and two temperature settings. We find that (1) increasing system-prompt constraint specificity does not monotonically improve correctness -- prompt effectiveness is configuration-dependent and can help or hinder based on alignment with task requirements and decoding context; (2) for larger code-specialized models, few-shot examples can degrade performance relative to zero-shot generation, contrary to conventional wisdom; and (3) programming language matters, with Java exhibiting significantly greater sensitivity to system prompt variations than Python, suggesting language-specific prompt engineering strategies may be necessary.", "AI": {"tldr": "\u7cfb\u7edf\u63d0\u793a\u5bf9\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u5f71\u54cd\u7814\u7a76\uff1a\u53d1\u73b0\u63d0\u793a\u7ea6\u675f\u7279\u5f02\u6027\u4e0d\u603b\u662f\u63d0\u5347\u6b63\u786e\u6027\uff0cfew-shot\u793a\u4f8b\u53ef\u80fd\u635f\u5bb3\u5927\u6a21\u578b\u6027\u80fd\uff0cJava\u6bd4Python\u5bf9\u63d0\u793a\u53d8\u5316\u66f4\u654f\u611f\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7cfb\u7edf\u63d0\u793a\u5bf9\u901a\u7528ILMs\u548c\u4e13\u7528CLMs\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u7cfb\u7edf\u8bc4\u4f30\u7cfb\u7edf\u63d0\u793a\u7684\u8be6\u7ec6\u7a0b\u5ea6\u3001\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u7b56\u7565\u548c\u7f16\u7a0b\u8bed\u8a00\u7b49\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u4ee3\u7801\u52a9\u624b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5728360\u79cd\u914d\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff1a\u6db5\u76d64\u4e2a\u6a21\u578b\u30015\u79cd\u7cfb\u7edf\u63d0\u793a\u30013\u79cd\u63d0\u793a\u7b56\u7565\u30012\u79cd\u7f16\u7a0b\u8bed\u8a00\uff08Python\u548cJava\uff09\u548c2\u79cd\u6e29\u5ea6\u8bbe\u7f6e\u3002\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u6cd5\u5206\u6790\u5404\u56e0\u7d20\u5bf9\u4ee3\u7801\u751f\u6210\u6b63\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "1. \u589e\u52a0\u7cfb\u7edf\u63d0\u793a\u7684\u7ea6\u675f\u7279\u5f02\u6027\u5e76\u4e0d\u603b\u662f\u63d0\u9ad8\u6b63\u786e\u6027\uff0c\u63d0\u793a\u6548\u679c\u53d6\u51b3\u4e8e\u914d\u7f6e\uff0c\u53ef\u80fd\u5e2e\u52a9\u6216\u963b\u788d\u6027\u80fd\uff1b2. \u5bf9\u4e8e\u5927\u578b\u4ee3\u7801\u4e13\u7528\u6a21\u578b\uff0cfew-shot\u793a\u4f8b\u53ef\u80fd\u6bd4zero-shot\u751f\u6210\u8868\u73b0\u66f4\u5dee\uff1b3. \u7f16\u7a0b\u8bed\u8a00\u5f71\u54cd\u663e\u8457\uff0cJava\u5bf9\u7cfb\u7edf\u63d0\u793a\u53d8\u5316\u7684\u654f\u611f\u6027\u8fdc\u9ad8\u4e8ePython\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u5bf9\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u5f71\u54cd\u590d\u6742\u4e14\u914d\u7f6e\u4f9d\u8d56\uff0c\u9700\u8981\u9488\u5bf9\u5177\u4f53\u4efb\u52a1\u3001\u6a21\u578b\u548c\u7f16\u7a0b\u8bed\u8a00\u8fdb\u884c\u5b9a\u5236\u5316\u63d0\u793a\u5de5\u7a0b\u3002\u4f20\u7edffew-shot\u7b56\u7565\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u5927\u578b\u4ee3\u7801\u6a21\u578b\uff0c\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u9700\u8981\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u3002"}}
{"id": "2602.15739", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15739", "abs": "https://arxiv.org/abs/2602.15739", "authors": ["Humam Kourani", "Gyunam Park", "Wil M. P. van der Aalst"], "title": "Hierarchical Decomposition of Separable Workflow-Nets", "comment": "arXiv admin note: text overlap with arXiv:2503.20363", "summary": "The Partially Ordered Workflow Language (POWL) has recently emerged as a process modeling notation, offering strong quality guarantees and high expressiveness. While early versions of POWL relied on strict block-structured operators for choices and loops, the language has recently evolved into POWL 2.0, introducing choice graphs to enable the modeling of non-block-structured decisions and cycles. To bridge the gap between the theoretical advantages of POWL and the practical need for compatibility with established notations, robust model transformations are required. This paper presents a novel algorithm for transforming safe and sound workflow nets (WF-nets) into equivalent POWL 2.0 models. The algorithm recursively identifies structural patterns within the WF-net and translates them into their POWL representation. Unlike the previous approach that required separate detection strategies for exclusive choices and loops, our new algorithm utilizes choice graphs to capture generalized decision and cyclic patterns. We formally prove the correctness of our approach, showing that the generated POWL model preserves the language of the input WF-net. Furthermore, we prove the completeness of our algorithm on the class of separable WF-nets, which corresponds to nets constructed via the hierarchical nesting of state machines and marked graphs. We evaluate our algorithm on large-scale process models to demonstrate its high scalability. Furthermore, to test its practical expressiveness, we applied it to a benchmark of 1,493 industrial and synthetic process models. Our algorithm successfully transformed all models in this benchmark, suggesting that POWL 2.0's expressive power is generally sufficient to capture the complex logic found in real-world business processes. This work paves the way for broader adoption of POWL in practical process analysis and improvement applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u5b89\u5168\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u7f51\uff08WF-nets\uff09\u8f6c\u6362\u4e3aPOWL 2.0\u6a21\u578b\u7684\u65b0\u7b97\u6cd5\uff0c\u5229\u7528\u9009\u62e9\u56fe\u6355\u83b7\u5e7f\u4e49\u51b3\u7b56\u548c\u5faa\u73af\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u6b63\u786e\u6027\u548c\u5b8c\u5907\u6027\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "POWL 2.0\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u8fc7\u7a0b\u5efa\u6a21\u8bed\u8a00\uff0c\u5177\u6709\u5f3a\u5927\u7684\u8d28\u91cf\u4fdd\u8bc1\u548c\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u9700\u8981\u4e0e\u73b0\u6709\u5de5\u4f5c\u6d41\u7f51\uff08WF-nets\uff09\u7b49\u6210\u719f\u8868\u793a\u6cd5\u517c\u5bb9\u3002\u4e3a\u4e86\u5f25\u5408POWL\u7684\u7406\u8bba\u4f18\u52bf\u4e0e\u5b9e\u9645\u517c\u5bb9\u6027\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u7a33\u5065\u7684\u6a21\u578b\u8f6c\u6362\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7b97\u6cd5\uff0c\u9012\u5f52\u8bc6\u522bWF-net\u4e2d\u7684\u7ed3\u6784\u6a21\u5f0f\u5e76\u5c06\u5176\u8f6c\u6362\u4e3aPOWL\u8868\u793a\u3002\u4e0e\u4e4b\u524d\u9700\u8981\u5206\u522b\u68c0\u6d4b\u4e92\u65a5\u9009\u62e9\u548c\u5faa\u73af\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u65b0\u7b97\u6cd5\u5229\u7528\u9009\u62e9\u56fe\u6355\u83b7\u5e7f\u4e49\u7684\u51b3\u7b56\u548c\u5faa\u73af\u6a21\u5f0f\u3002\u7b97\u6cd5\u5728\u53ef\u5206\u79bbWF-nets\u7c7b\u4e0a\u5177\u6709\u5b8c\u5907\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u5927\u89c4\u6a21\u8fc7\u7a0b\u6a21\u578b\u4e0a\u5c55\u793a\u4e86\u9ad8\u53ef\u6269\u5c55\u6027\u3002\u57281,493\u4e2a\u5de5\u4e1a\u548c\u5408\u6210\u8fc7\u7a0b\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7b97\u6cd5\u6210\u529f\u8f6c\u6362\u4e86\u6240\u6709\u6a21\u578b\uff0c\u8868\u660ePOWL 2.0\u7684\u8868\u8fbe\u80fd\u529b\u8db3\u4ee5\u6355\u83b7\u73b0\u5b9e\u4e16\u754c\u4e1a\u52a1\u6d41\u7a0b\u4e2d\u7684\u590d\u6742\u903b\u8f91\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aPOWL\u5728\u5b9e\u9645\u8fc7\u7a0b\u5206\u6790\u548c\u6539\u8fdb\u5e94\u7528\u4e2d\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002\u901a\u8fc7\u63d0\u4f9b\u4eceWF-nets\u5230POWL 2.0\u7684\u53ef\u9760\u8f6c\u6362\uff0c\u4fc3\u8fdb\u4e86POWL\u4e0e\u73b0\u6709\u8fc7\u7a0b\u5efa\u6a21\u5b9e\u8df5\u7684\u96c6\u6210\u3002"}}
{"id": "2602.15241", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.15241", "abs": "https://arxiv.org/abs/2602.15241", "authors": ["Arya Tschand", "Chenyu Wang", "Zishen Wan", "Andrew Cheng", "Ioana Cristescu", "Kevin He", "Howard Huang", "Alexander Ingare", "Akseli Kangaslahti", "Sara Kangaslahti", "Theo Lebryk", "Hongjin Lin", "Jeffrey Jian Ma", "Alexandru Meterez", "Clara Mohri", "Depen Morwani", "Sunny Qin", "Roy Rinberg", "Paula Rodriguez-Diaz", "Alyssa Mia Taliotis", "Pernille Undrum Fathi", "Rosie Zhao", "Todd Zhou", "Vijay Janapa Reddi"], "title": "GenAI for Systems: Recurring Challenges and Design Principles from Software to Silicon", "comment": null, "summary": "Generative AI is reshaping how computing systems are designed, optimized, and built, yet research remains fragmented across software, architecture, and chip design communities. This paper takes a cross-stack perspective, examining how generative models are being applied from code generation and distributed runtimes through hardware design space exploration to RTL synthesis, physical layout, and verification. Rather than reviewing each layer in isolation, we analyze how the same structural difficulties and effective responses recur across the stack. Our central finding is one of convergence. Despite the diversity of domains and tools, the field keeps encountering five recurring challenges (the feedback loop crisis, the tacit knowledge problem, trust and validation, co-design across boundaries, and the shift from determinism to dynamism) and keeps arriving at five design principles that independently emerge as effective responses (embracing hybrid approaches, designing for continuous feedback, separating concerns by role, matching methods to problem structure, and building on decades of systems knowledge). We organize these into a challenge--principle map that serves as a diagnostic and design aid, showing which principles have proven effective for which challenges across layers. Through concrete cross-stack examples, we show how systems navigate this map as they mature, and argue that the field needs shared engineering methodology, including common vocabularies, cross-layer benchmarks, and systematic design practices, so that progress compounds across communities rather than being rediscovered in each one. Our analysis covers more than 275 papers spanning eleven application areas across three layers of the computing stack, and distills open research questions that become visible only from a cross-layer vantage point.", "AI": {"tldr": "\u8bba\u6587\u4ece\u8de8\u6808\u89c6\u89d2\u5206\u6790\u751f\u6210\u5f0fAI\u5728\u8ba1\u7b97\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u8bc6\u522b\u51fa\u4e94\u4e2a\u91cd\u590d\u51fa\u73b0\u7684\u6311\u6218\u548c\u4e94\u4e2a\u6709\u6548\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u63d0\u51fa\u4e86\u5171\u4eab\u5de5\u7a0b\u65b9\u6cd5\u8bba\u7684\u9700\u6c42\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u5728\u91cd\u5851\u8ba1\u7b97\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3001\u4f18\u5316\u548c\u6784\u5efa\u65b9\u5f0f\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u5728\u8f6f\u4ef6\u3001\u67b6\u6784\u548c\u82af\u7247\u8bbe\u8ba1\u793e\u533a\u4e4b\u95f4\u662f\u788e\u7247\u5316\u7684\u3002\u9700\u8981\u4ece\u8de8\u6808\u89c6\u89d2\u7406\u89e3\u751f\u6210\u5f0f\u6a21\u578b\u5982\u4f55\u5e94\u7528\u4e8e\u4ece\u4ee3\u7801\u751f\u6210\u5230\u786c\u4ef6\u8bbe\u8ba1\u7684\u5404\u4e2a\u5c42\u9762\uff0c\u5e76\u8bc6\u522b\u5171\u540c\u7684\u6a21\u5f0f\u548c\u6311\u6218\u3002", "method": "\u91c7\u7528\u8de8\u6808\u5206\u6790\u65b9\u6cd5\uff0c\u5ba1\u67e5\u4e86\u6db5\u76d6\u8ba1\u7b97\u6808\u4e09\u4e2a\u5c42\u6b21\u300111\u4e2a\u5e94\u7528\u9886\u57df\u7684275\u591a\u7bc7\u8bba\u6587\u3002\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u5c42\u6b21\u4e2d\u53cd\u590d\u51fa\u73b0\u7684\u7ed3\u6784\u6027\u95ee\u9898\uff0c\u8bc6\u522b\u51fa\u5171\u540c\u7684\u6311\u6218\u548c\u8bbe\u8ba1\u539f\u5219\uff0c\u5e76\u6784\u5efa\u4e86\u6311\u6218-\u539f\u5219\u6620\u5c04\u56fe\u4f5c\u4e3a\u8bca\u65ad\u548c\u8bbe\u8ba1\u5de5\u5177\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e94\u4e2a\u91cd\u590d\u51fa\u73b0\u7684\u6311\u6218\uff1a\u53cd\u9988\u5faa\u73af\u5371\u673a\u3001\u9690\u6027\u77e5\u8bc6\u95ee\u9898\u3001\u4fe1\u4efb\u4e0e\u9a8c\u8bc1\u3001\u8de8\u8fb9\u754c\u534f\u540c\u8bbe\u8ba1\u3001\u4ece\u786e\u5b9a\u6027\u5230\u52a8\u6001\u6027\u7684\u8f6c\u53d8\u3002\u540c\u65f6\u8bc6\u522b\u51fa\u4e94\u4e2a\u6709\u6548\u7684\u8bbe\u8ba1\u539f\u5219\uff1a\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u3001\u8bbe\u8ba1\u6301\u7eed\u53cd\u9988\u673a\u5236\u3001\u6309\u89d2\u8272\u5206\u79bb\u5173\u6ce8\u70b9\u3001\u65b9\u6cd5\u5339\u914d\u95ee\u9898\u7ed3\u6784\u3001\u57fa\u4e8e\u6570\u5341\u5e74\u7cfb\u7edf\u77e5\u8bc6\u6784\u5efa\u3002\u8fd9\u4e9b\u539f\u5219\u5728\u4e0d\u540c\u5c42\u6b21\u4e2d\u72ec\u7acb\u51fa\u73b0\u5e76\u8bc1\u660e\u6709\u6548\u3002", "conclusion": "\u8be5\u9886\u57df\u9700\u8981\u5171\u4eab\u7684\u5de5\u7a0b\u65b9\u6cd5\u8bba\uff0c\u5305\u62ec\u901a\u7528\u8bcd\u6c47\u8868\u3001\u8de8\u5c42\u57fa\u51c6\u6d4b\u8bd5\u548c\u7cfb\u7edf\u5316\u8bbe\u8ba1\u5b9e\u8df5\uff0c\u4ee5\u4fbf\u8fdb\u5c55\u80fd\u591f\u5728\u4e0d\u540c\u793e\u533a\u95f4\u79ef\u7d2f\u800c\u975e\u5728\u6bcf\u4e2a\u793e\u533a\u4e2d\u91cd\u65b0\u53d1\u73b0\u3002\u4ece\u8de8\u5c42\u89c6\u89d2\u53ef\u4ee5\u8bc6\u522b\u51fa\u4ec5\u5728\u8be5\u89c6\u89d2\u4e0b\u53ef\u89c1\u7684\u5f00\u653e\u7814\u7a76\u95ee\u9898\u3002"}}
{"id": "2602.15773", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.15773", "abs": "https://arxiv.org/abs/2602.15773", "authors": ["Jiaxin Jiang", "Yunxiang Zhao", "Lyu Xu", "Byron Choi", "Bingsheng He", "Shixuan Sun", "Jia Chen"], "title": "Efficient Densest Flow Queries in Transaction Flow Networks (Complete Version)", "comment": null, "summary": "Transaction flow networks are crucial in detecting illicit activities such as wash trading, credit card fraud, cashback arbitrage fraud, and money laundering. \\revise{Our collaborator, Grab, a leader in digital payments in Southeast Asia, faces increasingly sophisticated fraud patterns in its transaction flow networks. In industry settings such as Grab's fraud detection pipeline, identifying fraudulent activities heavily relies on detecting dense flows within transaction networks. Motivated by this practical foundation,} we propose the \\emph{\\(S\\)-\\(T\\) densest flow} (\\SDMF{}) query. Given a transaction flow network \\( G \\), a source set \\( \\Src \\), a sink set \\( \\Dst \\), and a size threshold \\( k \\), the query outputs subsets \\( \\Src' \\subseteq \\Src \\) and \\( \\Dst' \\subseteq \\Dst \\) such that the maximum flow from \\( \\Src' \\) to \\( \\Dst' \\) is densest, with \\(|\\Src' \\cup \\Dst'| \\geq k\\). Recognizing the NP-hardness of the \\SDMF{} query, we develop an efficient divide-and-conquer algorithm, CONAN. \\revise{Driven by industry needs for scalable and efficient solutions}, we introduce an approximate flow-peeling algorithm to optimize the performance of CONAN, enhancing its efficiency in processing large transaction networks. \\revise{Our approach has been integrated into Grab's fraud detection scenario, resulting in significant improvements in identifying fraudulent activities.} Experiments show that CONAN outperforms baseline methods by up to three orders of magnitude in runtime and more effectively identifies the densest flows. We showcase CONAN's applications in fraud detection on transaction flow networks from our industry partner, Grab, and on non-fungible tokens (NFTs).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faS-T\u6700\u5bc6\u6d41\u67e5\u8be2\uff08SDMF\uff09\u7528\u4e8e\u68c0\u6d4b\u4ea4\u6613\u7f51\u7edc\u4e2d\u7684\u6b3a\u8bc8\u6d3b\u52a8\uff0c\u5f00\u53d1\u4e86\u9ad8\u6548\u7684CONAN\u7b97\u6cd5\uff0c\u5e76\u5728Grab\u7684\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u6210\u529f\u5e94\u7528\u3002", "motivation": "Grab\u7b49\u6570\u5b57\u652f\u4ed8\u5e73\u53f0\u9762\u4e34\u65e5\u76ca\u590d\u6742\u7684\u6b3a\u8bc8\u6a21\u5f0f\uff0c\u4f20\u7edf\u6b3a\u8bc8\u68c0\u6d4b\u4f9d\u8d56\u8bc6\u522b\u4ea4\u6613\u7f51\u7edc\u4e2d\u7684\u5bc6\u96c6\u6d41\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u68c0\u6d4b\u6b3a\u8bc8\u6d3b\u52a8\u3002", "method": "\u63d0\u51faS-T\u6700\u5bc6\u6d41\u67e5\u8be2\uff08SDMF\uff09\uff0c\u5f00\u53d1CONAN\u7b97\u6cd5\uff08\u57fa\u4e8e\u5206\u6cbb\u7b56\u7565\uff09\uff0c\u5f15\u5165\u8fd1\u4f3c\u6d41\u5265\u79bb\u7b97\u6cd5\u4f18\u5316\u6027\u80fd\uff0c\u5904\u7406\u5927\u89c4\u6a21\u4ea4\u6613\u7f51\u7edc\u3002", "result": "CONAN\u7b97\u6cd5\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5feb\u4e09\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u66f4\u6709\u6548\u5730\u8bc6\u522b\u6700\u5bc6\u6d41\uff0c\u5df2\u6210\u529f\u96c6\u6210\u5230Grab\u7684\u6b3a\u8bc8\u68c0\u6d4b\u7cfb\u7edf\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b3a\u8bc8\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "SDMF\u67e5\u8be2\u548cCONAN\u7b97\u6cd5\u4e3a\u4ea4\u6613\u7f51\u7edc\u6b3a\u8bc8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.15342", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.15342", "abs": "https://arxiv.org/abs/2602.15342", "authors": ["Hanyu Zhang", "Tomoji Kishi"], "title": "SACS: A Code Smell Dataset using Semi-automatic Generation Approach", "comment": "arXiv admin note: substantial text overlap with arXiv:2511.12069", "summary": "Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past of decades, the research on code smell has received extensive attention. Especially the researches applied machine learning-technique have become a popular topic in recent studies. However, one of the biggest challenges to apply machine learning-technique is the lack of high-quality code smell datasets. Manually constructing such datasets is extremely labor-intensive, as identifying code smells requires substantial development expertise and considerable time investment. In contrast, automatically generated datasets, while scalable, frequently exhibit reduced label reliability and compromised data quality. To overcome this challenge, in this study, we explore a semi-automatic approach to generate a code smell dataset with high quality data samples. Specifically, we first applied a set of automatic generation rules to produce candidate smelly samples. We then employed multiple metrics to group the data samples into an automatically accepted group and a manually reviewed group, enabling reviewers to concentrate their efforts on ambiguous samples. Furthermore, we established structured review guidelines and developed a annotation tool to support the manual validation process. Based on the proposed semi-automatic generation approach, we created an open-source code smell dataset, SACS, covering three widely studied code smells: Long Method, Large Class, and Feature Envy. Each code smell category includes over 10,000 labeled samples. This dataset could provide a large-scale and publicly available benchmark to facilitate future studies on code smell detection and automated refactoring.", "AI": {"tldr": "\u63d0\u51fa\u534a\u81ea\u52a8\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u5f02\u5473\u6570\u636e\u96c6SACS\uff0c\u5305\u542b\u4e09\u79cd\u5e38\u89c1\u4ee3\u7801\u5f02\u5473\uff0c\u6bcf\u79cd\u8d85\u8fc710,000\u4e2a\u6807\u6ce8\u6837\u672c", "motivation": "\u4ee3\u7801\u5f02\u5473\u7814\u7a76\u9762\u4e34\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u6311\u6218\uff0c\u624b\u52a8\u6784\u5efa\u6210\u672c\u9ad8\uff0c\u81ea\u52a8\u751f\u6210\u8d28\u91cf\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u5e73\u8861\u6548\u7387\u4e0e\u8d28\u91cf", "method": "\u91c7\u7528\u534a\u81ea\u52a8\u65b9\u6cd5\uff1a\u5148\u5e94\u7528\u81ea\u52a8\u751f\u6210\u89c4\u5219\u4ea7\u751f\u5019\u9009\u6837\u672c\uff0c\u7136\u540e\u7528\u591a\u6307\u6807\u5206\u7ec4\u4e3a\u81ea\u52a8\u63a5\u53d7\u7ec4\u548c\u4eba\u5de5\u5ba1\u6838\u7ec4\uff0c\u5efa\u7acb\u7ed3\u6784\u5316\u5ba1\u6838\u6307\u5357\u548c\u6807\u6ce8\u5de5\u5177\u652f\u6301\u4eba\u5de5\u9a8c\u8bc1", "result": "\u521b\u5efa\u4e86\u5f00\u6e90\u4ee3\u7801\u5f02\u5473\u6570\u636e\u96c6SACS\uff0c\u6db5\u76d6Long Method\u3001Large Class\u3001Feature Envy\u4e09\u79cd\u4ee3\u7801\u5f02\u5473\uff0c\u6bcf\u7c7b\u8d85\u8fc710,000\u4e2a\u6807\u6ce8\u6837\u672c", "conclusion": "SACS\u6570\u636e\u96c6\u4e3a\u4ee3\u7801\u5f02\u5473\u68c0\u6d4b\u548c\u81ea\u52a8\u91cd\u6784\u7814\u7a76\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u516c\u5f00\u57fa\u51c6\uff0c\u534a\u81ea\u52a8\u65b9\u6cd5\u5e73\u8861\u4e86\u6570\u636e\u8d28\u91cf\u4e0e\u751f\u6210\u6548\u7387"}}
{"id": "2602.15761", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.15761", "abs": "https://arxiv.org/abs/2602.15761", "authors": ["Simantika Bhattacharjee Dristi", "Matthew B. Dwyer"], "title": "A Differential Fuzzing-Based Evaluation of Functional Equivalence in LLM-Generated Code Refactorings", "comment": null, "summary": "With the rapid adoption of large language models (LLMs) in automated code refactoring, assessing and ensuring functional equivalence between LLM-generated refactoring and the original implementation becomes critical. While prior work typically relies on predefined test cases to evaluate correctness, in this work, we leverage differential fuzzing to check functional equivalence in LLM-generated code refactorings. Unlike test-based evaluation, a differential fuzzing-based equivalence checker needs no predefined test cases and can explore a much larger input space by executing and comparing thousands of automatically generated test inputs. In a large-scale evaluation of six LLMs (CodeLlama, Codestral, StarChat2, Qwen-2.5, Olmo-3, and GPT-4o) across three datasets and two refactoring types, we find that LLMs show a non-trivial tendency to alter program semantics, producing 19-35% functionally non-equivalent refactorings. Our experiments further demonstrate that about 21% of these non-equivalent refactorings remain undetected by the existing test suites of the three evaluated datasets. Collectively, the findings of this study imply that reliance on existing tests might overestimate functional equivalence in LLM-generated code refactorings, which remain prone to semantic divergence.", "AI": {"tldr": "\u4f7f\u7528\u5dee\u5206\u6a21\u7cca\u6d4b\u8bd5\u8bc4\u4f30LLM\u4ee3\u7801\u91cd\u6784\u7684\u529f\u80fd\u7b49\u4ef7\u6027\uff0c\u53d1\u73b019-35%\u7684\u91cd\u6784\u5b58\u5728\u8bed\u4e49\u6539\u53d8\uff0c21%\u7684\u9519\u8bef\u91cd\u6784\u672a\u88ab\u73b0\u6709\u6d4b\u8bd5\u96c6\u53d1\u73b0", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u4ee3\u7801\u91cd\u6784\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8bc4\u4f30LLM\u751f\u6210\u91cd\u6784\u4e0e\u539f\u59cb\u5b9e\u73b0\u4e4b\u95f4\u7684\u529f\u80fd\u7b49\u4ef7\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u6d4b\u8bd5\u7528\u4f8b\u8bc4\u4f30\u6b63\u786e\u6027\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5dee\u5206\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u68c0\u67e5LLM\u751f\u6210\u4ee3\u7801\u91cd\u6784\u7684\u529f\u80fd\u7b49\u4ef7\u6027\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u9884\u5b9a\u4e49\u6d4b\u8bd5\u7528\u4f8b\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u6570\u5343\u4e2a\u6d4b\u8bd5\u8f93\u5165\u5e76\u6267\u884c\u6bd4\u8f83\uff0c\u63a2\u7d22\u66f4\u5927\u7684\u8f93\u5165\u7a7a\u95f4\u3002\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u4e24\u79cd\u91cd\u6784\u7c7b\u578b\u4e0a\u5bf9\u516d\u4e2aLLM\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "LLM\u663e\u793a\u51fa\u663e\u8457\u6539\u53d8\u7a0b\u5e8f\u8bed\u4e49\u7684\u8d8b\u52bf\uff0c\u4ea7\u751f19-35%\u529f\u80fd\u4e0d\u7b49\u4ef7\u7684\u91cd\u6784\u3002\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8868\u660e\uff0c\u7ea621%\u7684\u8fd9\u4e9b\u4e0d\u7b49\u4ef7\u91cd\u6784\u5728\u4e09\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u73b0\u6709\u6d4b\u8bd5\u5957\u4ef6\u4e2d\u672a\u88ab\u68c0\u6d4b\u5230\u3002", "conclusion": "\u4f9d\u8d56\u73b0\u6709\u6d4b\u8bd5\u53ef\u80fd\u4f1a\u9ad8\u4f30LLM\u751f\u6210\u4ee3\u7801\u91cd\u6784\u7684\u529f\u80fd\u7b49\u4ef7\u6027\uff0c\u8fd9\u4e9b\u91cd\u6784\u4ecd\u7136\u5bb9\u6613\u51fa\u73b0\u8bed\u4e49\u504f\u5dee\u3002\u5dee\u5206\u6a21\u7cca\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
