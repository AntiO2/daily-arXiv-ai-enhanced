{"id": "2509.20563", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20563", "abs": "https://arxiv.org/abs/2509.20563", "authors": ["Skyler Ruiter", "Jiannan Tian", "Fengguang Song"], "title": "FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines", "comment": null, "summary": "Modern scientific simulations and instruments generate data volumes that\noverwhelm memory and storage, throttling scalability. Lossy compression\nmitigates this by trading controlled error for reduced footprint and throughput\ngains, yet optimal pipelines are highly data and objective specific, demanding\ncompression expertise. GPU compressors supply raw throughput but often\nhard-code fused kernels that hinder rapid experimentation, and underperform in\nrate-distortion. We present FZModules, a heterogeneous framework for assembling\nerror-bounded custom compression pipelines from high-performance modules\nthrough a concise extensible interface. We further utilize an asynchronous\ntask-backed execution library that infers data dependencies, manages memory\nmovement, and exposes branch and stage level concurrency for powerful\nasynchronous compression pipelines. Evaluating three pipelines built with\nFZModules on four representative scientific datasets, we show they can compare\nend-to-end speedup of fused-kernel GPU compressors while achieving similar\nrate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,\ndomain-tailored design.", "AI": {"tldr": "FZModules\u662f\u4e00\u4e2a\u5f02\u6784\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u9ad8\u6027\u80fd\u6a21\u5757\u7ec4\u88c5\u8bef\u5dee\u6709\u754c\u7684\u81ea\u5b9a\u4e49\u538b\u7f29\u6d41\u6c34\u7ebf\uff0c\u652f\u6301\u5feb\u901f\u5b9e\u9a8c\u5e76\u5b9e\u73b0\u4e0e\u878d\u5408\u5185\u6838GPU\u538b\u7f29\u5668\u76f8\u5f53\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9ad8\u4fdd\u771fCPU\u6216\u6df7\u5408\u538b\u7f29\u5668\u76f8\u4f3c\u7684\u7387\u5931\u771f\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u79d1\u5b66\u6a21\u62df\u548c\u4eea\u5668\u751f\u6210\u7684\u6570\u636e\u91cf\u8d85\u8fc7\u5185\u5b58\u548c\u5b58\u50a8\u5bb9\u91cf\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u867d\u7136\u6709\u635f\u538b\u7f29\u901a\u8fc7\u63a7\u5236\u8bef\u5dee\u6765\u51cf\u5c0f\u6570\u636e\u5360\u7528\u7a7a\u95f4\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u6700\u4f73\u6d41\u6c34\u7ebf\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6570\u636e\u548c\u76ee\u6807\uff0c\u9700\u8981\u538b\u7f29\u4e13\u4e1a\u77e5\u8bc6\u3002GPU\u538b\u7f29\u5668\u63d0\u4f9b\u539f\u59cb\u541e\u5410\u91cf\uff0c\u4f46\u901a\u5e38\u786c\u7f16\u7801\u878d\u5408\u5185\u6838\uff0c\u963b\u788d\u5feb\u901f\u5b9e\u9a8c\uff0c\u5e76\u4e14\u5728\u7387\u5931\u771f\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFZModules\u5f02\u6784\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u6d01\u53ef\u6269\u5c55\u7684\u63a5\u53e3\u4ece\u9ad8\u6027\u80fd\u6a21\u5757\u7ec4\u88c5\u8bef\u5dee\u6709\u754c\u7684\u81ea\u5b9a\u4e49\u538b\u7f29\u6d41\u6c34\u7ebf\u3002\u5229\u7528\u5f02\u6b65\u4efb\u52a1\u652f\u6301\u7684\u6267\u884c\u5e93\u63a8\u65ad\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\u3001\u7ba1\u7406\u5185\u5b58\u79fb\u52a8\uff0c\u5e76\u4e3a\u5f3a\u5927\u7684\u5f02\u6b65\u538b\u7f29\u6d41\u6c34\u7ebf\u66b4\u9732\u5206\u652f\u548c\u9636\u6bb5\u7ea7\u5e76\u53d1\u3002", "result": "\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e09\u4e2a\u4f7f\u7528FZModules\u6784\u5efa\u7684\u6d41\u6c34\u7ebf\uff0c\u7ed3\u679c\u663e\u793a\u5b83\u4eec\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u878d\u5408\u5185\u6838GPU\u538b\u7f29\u5668\u76f8\u5f53\u7684\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u66f4\u9ad8\u4fdd\u771f\u5ea6\u7684CPU\u6216\u6df7\u5408\u538b\u7f29\u5668\u76f8\u4f3c\u7684\u7387\u5931\u771f\u6027\u80fd\u3002", "conclusion": "FZModules\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u3001\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u5b9a\u5236\u7684\u538b\u7f29\u6d41\u6c34\u7ebf\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u6570\u636e\u538b\u7f29\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.20603", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.20603", "abs": "https://arxiv.org/abs/2509.20603", "authors": ["Angel M. Beltre", "Jeff Ogden", "Kevin Pedretti"], "title": "Experience Deploying Containerized GenAI Services at an HPC Center", "comment": "10 pages, 12 figures", "summary": "Generative Artificial Intelligence (GenAI) applications are built from\nspecialized components -- inference servers, object storage, vector and graph\ndatabases, and user interfaces -- interconnected via web-based APIs. While\nthese components are often containerized and deployed in cloud environments,\nsuch capabilities are still emerging at High-Performance Computing (HPC)\ncenters. In this paper, we share our experience deploying GenAI workloads\nwithin an established HPC center, discussing the integration of HPC and cloud\ncomputing environments. We describe our converged computing architecture that\nintegrates HPC and Kubernetes platforms running containerized GenAI workloads,\nhelping with reproducibility. A case study illustrates the deployment of the\nLlama Large Language Model (LLM) using a containerized inference server (vLLM)\nacross both Kubernetes and HPC platforms using multiple container runtimes. Our\nexperience highlights practical considerations and opportunities for the HPC\ncontainer community, guiding future research and tool development.", "AI": {"tldr": "\u672c\u6587\u5206\u4eab\u4e86\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u5fc3\u90e8\u7f72\u751f\u6210\u5f0fAI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7ecf\u9a8c\uff0c\u8ba8\u8bba\u4e86HPC\u4e0e\u4e91\u8ba1\u7b97\u73af\u5883\u7684\u96c6\u6210\uff0c\u63d0\u51fa\u4e86\u878d\u5408\u8ba1\u7b97\u67b6\u6784\uff0c\u5e76\u901a\u8fc7Llama\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8de8\u5e73\u53f0\u90e8\u7f72\u3002", "motivation": "\u867d\u7136\u751f\u6210\u5f0fAI\u5e94\u7528\u901a\u5e38\u90e8\u7f72\u5728\u4e91\u73af\u5883\u4e2d\uff0c\u4f46\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2d\u5fc3\u7684\u90e8\u7f72\u80fd\u529b\u4ecd\u5728\u53d1\u5c55\u4e2d\uff0c\u9700\u8981\u63a2\u7d22HPC\u4e0e\u4e91\u8ba1\u7b97\u7684\u96c6\u6210\u65b9\u6848\u3002", "method": "\u91c7\u7528\u878d\u5408\u8ba1\u7b97\u67b6\u6784\uff0c\u96c6\u6210HPC\u548cKubernetes\u5e73\u53f0\u8fd0\u884c\u5bb9\u5668\u5316\u7684\u751f\u6210\u5f0fAI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f7f\u7528\u591a\u79cd\u5bb9\u5668\u8fd0\u884c\u65f6\u5728Kubernetes\u548cHPC\u5e73\u53f0\u4e0a\u90e8\u7f72vLLM\u63a8\u7406\u670d\u52a1\u5668\u3002", "result": "\u6210\u529f\u90e8\u7f72\u4e86Llama\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u8de8\u5e73\u53f0\u5bb9\u5668\u5316\u90e8\u7f72\u7684\u53ef\u884c\u6027\uff0c\u4e3aHPC\u5bb9\u5668\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u8df5\u7ecf\u9a8c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aHPC\u5bb9\u5668\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8003\u8651\u56e0\u7d20\u548c\u53d1\u5c55\u673a\u4f1a\uff0c\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\u548c\u5de5\u5177\u5f00\u53d1\u3002"}}
{"id": "2509.20776", "categories": ["cs.DC", "cs.MS", "G.4"], "pdf": "https://arxiv.org/pdf/2509.20776", "abs": "https://arxiv.org/abs/2509.20776", "authors": ["Elaheh Hassani", "Md Taufique Hussain", "Ariful Azad"], "title": "Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment", "comment": "32 pages", "summary": "We present scalable distributed-memory algorithms for sparse matrix\npermutation, extraction, and assignment. Our methods follow an\nIdentify-Exchange-Build (IEB) strategy where each process identifies the local\nnonzeros to be sent, exchanges the required data, and then builds its local\nsubmatrix from the received elements. This approach reduces communication\ncompared to SpGEMM-based methods in distributed memory. By employing\nsynchronization-free multithreaded algorithms, we further accelerate local\ncomputations, achieving substantially better performance than existing\nlibraries such as CombBLAS and PETSc. We design efficient software for these\noperations and evaluate their performance on two university clusters and the\nPerlmutter supercomputer. Our experiments span a variety of application\nscenarios, including matrix permutation for load balancing, matrix reordering,\nsubgraph extraction, and streaming graph applications. In all cases, we compare\nour algorithms against CombBLAS, the most comprehensive distributed library for\nthese operations, and, in some scenarios, against PETSc. Overall, this work\nprovides a comprehensive study of algorithms, software implementations,\nexperimental evaluations, and applications for sparse matrix permutation,\nextraction, and assignment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u7b97\u6cd5\u7528\u4e8e\u7a00\u758f\u77e9\u9635\u7684\u7f6e\u6362\u3001\u63d0\u53d6\u548c\u8d4b\u503c\u64cd\u4f5c\uff0c\u91c7\u7528Identify-Exchange-Build\u7b56\u7565\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u76f8\u6bd4\u73b0\u6709\u5e93\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u7a00\u758f\u77e9\u9635\u64cd\u4f5c\u5e93\u5982CombBLAS\u548cPETSc\u5728\u6027\u80fd\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u901a\u4fe1\u5f00\u9500\u65b9\u9762\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u63d0\u5347\u5927\u89c4\u6a21\u7a00\u758f\u77e9\u9635\u5904\u7406\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528Identify-Exchange-Build(IEB)\u7b56\u7565\uff1a\u8bc6\u522b\u672c\u5730\u975e\u96f6\u5143\u7d20\u3001\u4ea4\u6362\u6240\u9700\u6570\u636e\u3001\u4ece\u63a5\u6536\u5143\u7d20\u6784\u5efa\u672c\u5730\u5b50\u77e9\u9635\uff1b\u4f7f\u7528\u65e0\u540c\u6b65\u591a\u7ebf\u7a0b\u7b97\u6cd5\u52a0\u901f\u672c\u5730\u8ba1\u7b97\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u5b66\u96c6\u7fa4\u548cPerlmutter\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4CombBLAS\u548cPETSc\uff0c\u65b0\u7b97\u6cd5\u5728\u77e9\u9635\u7f6e\u6362\u3001\u5b50\u56fe\u63d0\u53d6\u3001\u6d41\u56fe\u5e94\u7528\u7b49\u573a\u666f\u4e0b\u83b7\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7a00\u758f\u77e9\u9635\u7f6e\u6362\u3001\u63d0\u53d6\u548c\u8d4b\u503c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u7b97\u6cd5\u7814\u7a76\u3001\u8f6f\u4ef6\u5b9e\u73b0\u3001\u5b9e\u9a8c\u8bc4\u4f30\u548c\u5e94\u7528\u6848\u4f8b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.20819", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.20819", "abs": "https://arxiv.org/abs/2509.20819", "authors": ["Andre Merzky", "Mikhail Titov", "Matteo Turilli", "Shantenu Jha"], "title": "Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads", "comment": "12 pages, 1 table, 8 figures", "summary": "Scientific workflows increasingly involve both HPC and machine-learning\ntasks, combining MPI-based simulations, training, and inference in a single\nexecution. Launchers such as Slurm's srun constrain concurrency and throughput,\nmaking them unsuitable for dynamic and heterogeneous workloads. We present a\nperformance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two\ncomplementary runtime systems that enable hierarchical resource management and\nhigh-throughput function execution. Using synthetic and production-scale\nworkloads on Frontier, we characterize the task execution properties of RP\nacross runtime configurations. RP+Flux sustains up to 930 tasks/s, and\nRP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,\nsrun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.\nFor IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%\nrelative to srun/Slurm and increases throughput more than four times on up to\n1,024. These results demonstrate hybrid runtime integration in RP as a scalable\napproach for hybrid AI-HPC workloads.", "AI": {"tldr": "RADICAL-Pilot\u4e0eFlux\u548cDragon\u8fd0\u884c\u65f6\u7cfb\u7edf\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408AI-HPC\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\uff0c\u4efb\u52a1\u6267\u884c\u901f\u7387\u53ef\u8fbe1500+\u4efb\u52a1/\u79d2\uff0c\u76f8\u6bd4\u4f20\u7edfsrun\u67094\u500d\u4ee5\u4e0a\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u79d1\u5b66\u5de5\u4f5c\u6d41\u65e5\u76ca\u7ed3\u5408HPC\u548c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff0c\u4f46\u4f20\u7edf\u542f\u52a8\u5668\u5982srun\u5728\u5e76\u53d1\u6027\u548c\u541e\u5410\u91cf\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u4e0d\u9002\u5408\u52a8\u6001\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u5c06RADICAL-Pilot\u4e0eFlux\u548cDragon\u4e24\u4e2a\u4e92\u8865\u7684\u8fd0\u884c\u65f6\u7cfb\u7edf\u96c6\u6210\uff0c\u5b9e\u73b0\u5206\u5c42\u8d44\u6e90\u7ba1\u7406\u548c\u9ad8\u541e\u5410\u91cf\u51fd\u6570\u6267\u884c\uff0c\u5728Frontier\u7cfb\u7edf\u4e0a\u4f7f\u7528\u5408\u6210\u548c\u751f\u4ea7\u89c4\u6a21\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u6027\u80fd\u7814\u7a76\u3002", "result": "RP+Flux\u53ef\u6301\u7eed\u6267\u884c930\u4efb\u52a1/\u79d2\uff0cRP+Flux+Dragon\u8d85\u8fc71500\u4efb\u52a1/\u79d2\uff0c\u5229\u7528\u7387\u8d85\u8fc799.6%\uff1b\u800csrun\u5cf0\u503c\u4ec5\u4e3a152\u4efb\u52a1/\u79d2\uff0c\u5229\u7528\u7387\u4f4e\u4e8e50%\u3002\u5728IMPECCABLE.v2\u836f\u7269\u53d1\u73b0\u5e94\u7528\u4e2d\uff0cRP+Flux\u76f8\u6bd4srun/Slurm\u7f29\u77ed30-60%\u5b8c\u6210\u65f6\u95f4\u3002", "conclusion": "RP\u4e0e\u8fd0\u884c\u65f6\u7cfb\u7edf\u7684\u6df7\u5408\u96c6\u6210\u4e3a\u6df7\u5408AI-HPC\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.21009", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.21009", "abs": "https://arxiv.org/abs/2509.21009", "authors": ["Wei Gao", "Yuheng Zhao", "Dakai An", "Tianyuan Wu", "Lunxi Cao", "Shaopan Xiong", "Ju Huang", "Weixun Wang", "Siran Yang", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng", "Wei Wang"], "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training", "comment": "16pages,14 figures", "summary": "Reinforcement Learning (RL) is a pivotal post-training technique for\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nsynchronous RL post-training often suffers from significant GPU\nunderutilization, referred to as bubbles, caused by imbalanced response lengths\nwithin rollout steps. Many RL systems attempt to alleviate this problem by\nrelaxing synchronization, but this can compromise training accuracy. In this\npaper, we introduce tail batching, a novel rollout scheduling strategy for\nsynchronous RL that systematically consolidates prompts leading to long-tail\nresponses into a small subset of rollout steps (long rounds), while ensuring\nthat the majority of steps (short rounds) involve only balanced, short\nrollouts. By excluding long responses from short rounds and rescheduling them\ninto a few designated long rounds, tail batching effectively reduces GPU idle\ntime during rollouts and significantly accelerates RL training without\nsacrificing accuracy. We present RollPacker, a system that fully harnesses the\nbenefits of tail batching through holistic optimizations across all three RL\nstages: elastic parallelism adaptation for rollout, dynamic resource allocation\nand scheduling for reward, and stream-based training. Empirical results show\nthat RollPacker achieves a 2.03x-2.56x end-to-end training time reduction\ncompared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5\nfamily of LLMs on up to 128 H800 GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3atail batching\u7684\u65b0\u578b\u540c\u6b65\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u957f\u5c3e\u54cd\u5e94\u6574\u5408\u5230\u5c11\u6570\u4e13\u7528\u6b65\u9aa4\u4e2d\uff0c\u663e\u8457\u51cf\u5c11GPU\u7a7a\u95f2\u65f6\u95f4\uff0c\u5e76\u5f00\u53d1\u4e86RollPacker\u7cfb\u7edf\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f42.03x-2.56x\u7684\u52a0\u901f\u3002", "motivation": "\u540c\u6b65RL\u540e\u8bad\u7ec3\u5b58\u5728GPU\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u7531\u54cd\u5e94\u957f\u5ea6\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684bubbles\u9020\u6210\u3002\u73b0\u6709\u7cfb\u7edf\u901a\u8fc7\u653e\u5bbd\u540c\u6b65\u6027\u6765\u7f13\u89e3\uff0c\u4f46\u4f1a\u727a\u7272\u8bad\u7ec3\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fatail batching\u7b56\u7565\uff0c\u5c06\u5bfc\u81f4\u957f\u5c3e\u54cd\u5e94\u7684\u63d0\u793a\u6574\u5408\u5230\u5c11\u91cf\u957f\u8f6e\u6b21\u4e2d\uff0c\u786e\u4fdd\u5927\u591a\u6570\u6b65\u9aa4\u53ea\u6d89\u53ca\u5e73\u8861\u7684\u77edrollout\u3002\u5f00\u53d1RollPacker\u7cfb\u7edf\uff0c\u5728\u4e09\u4e2aRL\u9636\u6bb5\u8fdb\u884c\u5168\u9762\u4f18\u5316\uff1a\u5f39\u6027\u5e76\u884c\u9002\u914d\u3001\u52a8\u6001\u8d44\u6e90\u5206\u914d\u8c03\u5ea6\u548c\u57fa\u4e8e\u6d41\u7684\u8bad\u7ec3\u3002", "result": "\u5728128\u4e2aH800 GPU\u4e0a\u5bf9Qwen2.5\u7cfb\u5217LLMs\u8fdb\u884c\u6d4b\u8bd5\uff0c\u76f8\u6bd4veRL\u5b9e\u73b02.03x-2.56x\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\uff0c\u76f8\u6bd4RLHFuse\u6700\u9ad8\u5b9e\u73b02.24x\u52a0\u901f\u3002", "conclusion": "tail batching\u7b56\u7565\u6709\u6548\u51cf\u5c11GPU\u7a7a\u95f2\u65f6\u95f4\uff0c\u663e\u8457\u52a0\u901fRL\u8bad\u7ec3\u800c\u4e0d\u727a\u7272\u7cbe\u5ea6\uff0cRollPacker\u7cfb\u7edf\u5145\u5206\u5229\u7528\u4e86\u8fd9\u4e00\u4f18\u52bf\u3002"}}
{"id": "2509.21037", "categories": ["cs.DC", "cs.MS", "D.1.3; G.1.3; G.4"], "pdf": "https://arxiv.org/pdf/2509.21037", "abs": "https://arxiv.org/abs/2509.21037", "authors": ["Jakub Homola", "Ond\u0159ej Meca", "Lubom\u00edr \u0158\u00edha", "Tom\u00e1\u0161 Brzobohat\u00fd"], "title": "Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods", "comment": "12 pages (originally 10 pages without references), 10 figures,\n  submitted to SC25 conference", "summary": "Schur complement matrices emerge in many domain decomposition methods that\ncan solve complex engineering problems using supercomputers. Today, as most of\nthe high-performance clusters' performance lies in GPUs, these methods should\nalso be accelerated.\n  Typically, the offloaded components are the explicitly assembled dense Schur\ncomplement matrices used later in the iterative solver for multiplication with\na vector. As the explicit assembly is expensive, it represents a significant\noverhead associated with this approach to acceleration. It has already been\nshown that the overhead can be minimized by assembling the Schur complements\ndirectly on the GPU.\n  This paper shows that the GPU assembly can be further improved by wisely\nutilizing the sparsity of the input matrices. In the context of FETI methods,\nwe achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the\nwhole assembly, making the acceleration beneficial from as few as 10\niterations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u8f93\u5165\u77e9\u9635\u7a00\u758f\u6027\u6765\u6539\u8fdbGPU\u4e0aSchur\u8865\u77e9\u9635\u7ec4\u88c5\u7684\u65b9\u6cd5\uff0c\u5728FETI\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e865.1\u500d\u7684GPU\u90e8\u5206\u52a0\u901f\u548c3.3\u500d\u7684\u6574\u4f53\u7ec4\u88c5\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u4e3b\u8981\u4f9d\u8d56GPU\uff0c\u9700\u8981\u52a0\u901f\u57df\u5206\u89e3\u65b9\u6cd5\u4e2d\u7684Schur\u8865\u77e9\u9635\u8ba1\u7b97\u3002\u663e\u5f0f\u7ec4\u88c5Schur\u8865\u77e9\u9635\u6210\u672c\u9ad8\u6602\uff0c\u662f\u52a0\u901f\u65b9\u6cd5\u7684\u4e3b\u8981\u5f00\u9500\u3002", "method": "\u901a\u8fc7\u660e\u667a\u5229\u7528\u8f93\u5165\u77e9\u9635\u7684\u7a00\u758f\u6027\u6765\u6539\u8fdbGPU\u4e0a\u7684Schur\u8865\u77e9\u9635\u7ec4\u88c5\u8fc7\u7a0b\uff0c\u51cf\u5c11\u663e\u5f0f\u7ec4\u88c5\u7684\u5f00\u9500\u3002", "result": "\u5728FETI\u65b9\u6cd5\u4e2d\uff0cGPU\u90e8\u5206\u4ee3\u7801\u5b9e\u73b0\u4e865.1\u500d\u52a0\u901f\uff0c\u6574\u4f53\u7ec4\u88c5\u5b9e\u73b0\u4e863.3\u500d\u52a0\u901f\uff0c\u4f7f\u5f97\u4ece\u4ec510\u6b21\u8fed\u4ee3\u5f00\u59cb\u5c31\u80fd\u83b7\u5f97\u52a0\u901f\u6548\u76ca\u3002", "conclusion": "\u5229\u7528\u8f93\u5165\u77e9\u9635\u7a00\u758f\u6027\u53ef\u4ee5\u663e\u8457\u6539\u8fdbGPU\u4e0a\u7684Schur\u8865\u77e9\u9635\u7ec4\u88c5\u6027\u80fd\uff0c\u4f7fGPU\u52a0\u901f\u5728\u8f83\u5c11\u8fed\u4ee3\u6b21\u6570\u4e0b\u5373\u5177\u6709\u5b9e\u9645\u6548\u76ca\u3002"}}
{"id": "2509.21039", "categories": ["cs.DC", "cs.CE", "cs.ET", "cs.PL"], "pdf": "https://arxiv.org/pdf/2509.21039", "abs": "https://arxiv.org/abs/2509.21039", "authors": ["William F. Godoy", "Tatiana Melnichenko", "Pedro Valero-Lara", "Wael Elwasif", "Philip Fackler", "Rafael Ferreira Da Silva", "Keita Teranishi", "Jeffrey S. Vetter"], "title": "Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem", "comment": "Accepted at the IEEE/ACM SC25 Conference WACCPD Workshop. The\n  International Conference for High Performance Computing, Networking, Storage,\n  and Analysis, St. Louis, MO, Nov 16-21, 2025. 15 pages, 7 figures. WFG and TM\n  contributed equally", "summary": "We explore the performance and portability of the novel Mojo language for\nscientific computing workloads on GPUs. As the first language based on the\nLLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,\nMojo aims to close performance and productivity gaps by combining Python's\ninteroperability and CUDA-like syntax for compile-time portable GPU\nprogramming. We target four scientific workloads: a seven-point stencil\n(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and\nHartree-Fock (compute-bound with atomic operations); and compare their\nperformance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We\nshow that Mojo's performance is competitive with CUDA and HIP for memory-bound\nkernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math\ncompute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve\nand programming requirements are still fairly low-level, Mojo can close\nsignificant gaps in the fragmented Python ecosystem in the convergence of\nscientific computing and AI.", "AI": {"tldr": "\u8bc4\u4f30Mojo\u8bed\u8a00\u5728GPU\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u79fb\u690d\u6027\uff0c\u4e0eCUDA\u548cHIP\u5bf9\u6bd4\uff0c\u53d1\u73b0Mojo\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u539f\u5b50\u64cd\u4f5c\u548c\u5feb\u901f\u6570\u5b66\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8eMLIR\u7684\u65b0\u8bed\u8a00Mojo\u5728\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u8868\u73b0\uff0c\u65e8\u5728\u5f25\u5408Python\u751f\u6001\u7cfb\u7edf\u4e2d\u6027\u80fd\u548c\u751f\u4ea7\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u9488\u5bf9\u56db\u79cd\u79d1\u5b66\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\uff1a\u4e03\u70b9\u6a21\u677f\uff08\u5185\u5b58\u5bc6\u96c6\u578b\uff09\u3001BabelStream\uff08\u5185\u5b58\u5bc6\u96c6\u578b\uff09\u3001miniBUDE\uff08\u8ba1\u7b97\u5bc6\u96c6\u578b\uff09\u548cHartree-Fock\uff08\u5e26\u539f\u5b50\u64cd\u4f5c\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\uff09\uff0c\u5728NVIDIA H100\u548cAMD MI300A GPU\u4e0a\u4e0e\u4f9b\u5e94\u5546\u57fa\u7ebf\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\u3002", "result": "Mojo\u5728\u5185\u5b58\u5bc6\u96c6\u578b\u5185\u6838\u4e0a\u7684\u6027\u80fd\u4e0eCUDA\u548cHIP\u76f8\u5f53\uff0c\u4f46\u5728AMD GPU\u4e0a\u7684\u539f\u5b50\u64cd\u4f5c\u4ee5\u53caAMD\u548cNVIDIA GPU\u4e0a\u7684\u5feb\u901f\u6570\u5b66\u8ba1\u7b97\u5bc6\u96c6\u578b\u5185\u6838\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u867d\u7136\u5b66\u4e60\u66f2\u7ebf\u548c\u7f16\u7a0b\u8981\u6c42\u4ecd\u7136\u76f8\u5bf9\u5e95\u5c42\uff0c\u4f46Mojo\u53ef\u4ee5\u5728\u79d1\u5b66\u8ba1\u7b97\u548cAI\u878d\u5408\u7684\u788e\u7247\u5316Python\u751f\u6001\u7cfb\u7edf\u4e2d\u5f25\u5408\u91cd\u8981\u5dee\u8ddd\u3002"}}
{"id": "2509.21137", "categories": ["cs.DC", "cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.21137", "abs": "https://arxiv.org/abs/2509.21137", "authors": ["Huynh Q. N. Vo", "Md Tawsif Rahman Chowdhury", "Paritosh Ramanan", "Gozde Tutuncuoglu", "Junchi Yang", "Feng Qiu", "Murat Yildirim"], "title": "From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem", "comment": "Main Article (12 Pages, 3 Figures), Appendix (4 Pages)", "summary": "The exponential growth of computational workloads is surpassing the\ncapabilities of conventional architectures, which are constrained by\nfundamental limits. In-memory computing (IMC) with RRAM provides a promising\nalternative by providing analog computations with significant gains in latency\nand energy use. However, existing algorithms developed for conventional\narchitectures do not translate to IMC, particularly for constrained\noptimization problems where frequent matrix reprogramming remains\ncost-prohibitive for IMC applications. Here we present a distributed in-memory\nprimal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays\nof RRAM devices. Our approach minimizes costly write cycles, incorporates\nrobustness against device non-idealities, and leverages a symmetric\nblock-matrix formulation to unify operations across distributed crossbars. We\nintegrate a physics-based simulation framework called MELISO+ to evaluate\nperformance under realistic device conditions. Benchmarking against\nGPU-accelerated solvers on large-scale linear programs demonstrates that our\nRRAM-based solver achieves comparable accuracy with up to three orders of\nmagnitude reductions in energy consumption and latency. These results\ndemonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the\ntransformative potential of algorithm-hardware co-design for solving\nlarge-scale optimization through distributed in-memory computing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9RRAM\u9635\u5217\u7684\u5206\u5e03\u5f0f\u5185\u5b58\u539f\u59cb-\u5bf9\u5076\u6df7\u5408\u68af\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\uff0c\u76f8\u6bd4GPU\u52a0\u901f\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u4f20\u7edf\u67b6\u6784\u53d7\u9650\u4e8e\u57fa\u672c\u7269\u7406\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6307\u6570\u589e\u957f\u9700\u6c42\u3002\u5185\u5b58\u8ba1\u7b97(RRAM)\u867d\u7136\u63d0\u4f9b\u4e86\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u80fd\u8017\u7684\u6a21\u62df\u8ba1\u7b97\uff0c\u4f46\u73b0\u6709\u7b97\u6cd5\u4e0d\u9002\u7528\u4e8eIMC\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u9891\u7e41\u77e9\u9635\u91cd\u7f16\u7a0b\u7684\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u5185\u5b58\u539f\u59cb-\u5bf9\u5076\u6df7\u5408\u68af\u5ea6\u65b9\u6cd5\uff0c\u6700\u5c0f\u5316\u6602\u8d35\u7684\u5199\u5165\u5468\u671f\uff0c\u5305\u542b\u5bf9\u8bbe\u5907\u975e\u7406\u60f3\u6027\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5229\u7528\u5bf9\u79f0\u5757\u77e9\u9635\u516c\u5f0f\u5728\u5206\u5e03\u5f0f\u4ea4\u53c9\u9635\u5217\u4e2d\u7edf\u4e00\u64cd\u4f5c\u3002\u4f7f\u7528MELISO+\u7269\u7406\u6a21\u62df\u6846\u67b6\u8bc4\u4f30\u5b9e\u9645\u8bbe\u5907\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u5927\u89c4\u6a21\u7ebf\u6027\u7a0b\u5e8f\u4e0a\u4e0eGPU\u52a0\u901f\u6c42\u89e3\u5668\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u793a\u57fa\u4e8eRRAM\u7684\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u80fd\u8017\u548c\u5ef6\u8fdf\u964d\u4f4e\u4e86\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728RRAM\u4e0a\u5b9e\u73b0\u7684PDHG\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\u5668\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u5206\u5e03\u5f0f\u5185\u5b58\u8ba1\u7b97\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u5927\u89c4\u6a21\u4f18\u5316\u95ee\u9898\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2509.21275", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.21275", "abs": "https://arxiv.org/abs/2509.21275", "authors": ["Shiju Wang", "Yujie Wang", "Ao Sun", "Fangcheng Fu", "Zijian Zhu", "Bin Cui", "Xu Han", "Kaisheng Ma"], "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training", "comment": null, "summary": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5f39\u6027\u6d41\u6c34\u7ebf\u5e76\u884c\uff08EPP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u8c03\u4ee4\u724c\u7ea7\u548c\u6279\u6b21\u7ea7\u6d41\u6c34\u7ebf\u5e76\u884c\u6765\u9002\u5e94\u8d44\u6e90\u548c\u8d1f\u8f7d\u5f02\u8d28\u6027\uff0c\u5e76\u6784\u5efa\u4e86InfiniPipe\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e861.69\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6848\u5728\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u5927\u3001\u5185\u5b58\u6d88\u8017\u9ad8\u6216\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e14\u9759\u6001\u8c03\u5ea6\u65b9\u6cd5\u5ffd\u7565\u4e86\u5e8f\u5217\u957f\u5ea6\u5206\u5e03\u7684\u504f\u659c\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faEPP\u65b9\u6cd5\u534f\u8c03\u4e24\u79cd\u6d41\u6c34\u7ebf\u5e76\u884c\u7c92\u5ea6\uff0c\u6784\u5efaInfiniPipe\u7cfb\u7edf\uff0c\u5305\u542b\u8d44\u6e90\u611f\u77e5\u548c\u8d1f\u8f7d\u5747\u8861\u7684\u5e8f\u5217\u5904\u7406\u5668\uff0c\u4ee5\u53ca\u8054\u5408\u4f18\u5316\u6d41\u6c34\u7ebf\u8c03\u5ea6\u548c\u68af\u5ea6\u68c0\u67e5\u70b9\u7684\u534f\u540c\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cInfiniPipe\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u4e861.69\u500d\u7684\u52a0\u901f\u3002", "conclusion": "EPP\u548cInfiniPipe\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u6311\u6218\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7c92\u5ea6\u9009\u62e9\u548c\u534f\u540c\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
