<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: 本文总结了EuroHPC JU CEEC项目在欧洲主要HPC系统上测量、分析和优化能源消耗的集体经验，通过CFD应用案例研究评估了不同架构的能耗表现。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算系统面临能源效率挑战，计算需求增长和架构复杂性导致显著的能源足迹。

Method: 使用代表性CFD应用（waLBerla、FLEXI/GALÆXI、Neko、NekRS）在LUMI、MareNostrum5、MeluXina和JUWELS Booster等系统的CPU和GPU分区上进行案例研究，评估能源到解和时间到解指标。

Result: 结果显示加速器和混合精度技术在保持计算精度的同时具有降低能耗的优势。

Conclusion: 需要在HPC系统上促进能源测量，以提高意识、教育社区并采取行动实现更可持续的百亿亿次计算。

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [2] [Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots](https://arxiv.org/abs/2511.03286)
*Ehud Shapiro*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Global digital platforms are software systems designed to serve entire
populations, with some already serving billions of people. We propose atomic
transactions-based multiagent transition systems and protocols as a formal
framework to study them; introduce essential agents -- minimal sets of agents
the removal of which makes communication impossible; and show that the
cardinality of essential agents partitions all global platforms into four
classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite $>1$ (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we
provide centralised, decentralised, federated, and grassroots specifications
via multiagent atomic transactions, and prove they satisfy basic correctness
properties. We discuss informally additional global platforms -- currencies,
``sharing economy'' apps, AI, and more. While this may be the first
characterisation of centralised, decentralised, and federated global platforms,
grassroots platforms have been formally defined previously, but using different
notions. Here, we prove that their original definition implies that all agents
are essential, placing grassroots platforms in a distinct class within the
broader formal context that includes all global platforms. This work provides
the first mathematical framework for classifying any global platform --
existing or imagined -- by providing a multiagent atomic-transactions
specification of it and determining the cardinality of the minimal set of
essential agents in the ensuing multiagent protocol. It thus

</details>


### [3] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: UMDAM是一种针对NPU-PIM协同执行优化的统一内存亲和性数据布局和DRAM地址映射方案，通过列主序、基于tile的布局和可配置的DRAM映射策略，在不引入额外内存开销或带宽损失的情况下，显著提升边缘设备上LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: LLM在边缘设备NPU上部署时，解码阶段仍存在内存密集型问题，限制了性能。NPU-PIM协同执行面临数据布局不匹配、带宽损失和冗余存储等挑战。

Method: 提出UMDAM方案，采用列主序、基于tile的数据布局和可配置的DRAM地址映射策略，确保与NPU计算兼容的同时最大化PIM效率。

Result: 在OPT模型上的综合评估显示，UMDAM将首token时间(TTFT)最多减少3.0倍，末token时间(TTLT)减少2.18倍。

Conclusion: UMDAM显著提升了边缘设备上端到端LLM推理效率，解决了NPU-PIM协同执行的关键挑战。

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [4] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: 评估三种隔离策略（cgroups和CPU固定、Docker容器、Firecracker MicroVMs）在云环境duet基准测试中的效果，发现进程隔离通常能降低误报，但Docker容器对噪声影响更敏感。


<details>
  <summary>Details</summary>
Motivation: 云环境中的基准测试面临多租户资源争用导致的性能变异性问题，duet基准测试通过在同一VM上并发运行两个工作负载版本来缓解此问题，但需要额外的隔离机制来处理同步工作负载之间的VM内部争用。

Method: 通过运行duet设置与噪声生成器并行的基准测试，比较三种隔离策略（cgroups和CPU固定、Docker容器、Firecracker MicroVMs）与无隔离基线实验的效果。

Result: 所有实验在噪声生成影响下显示出不同的延迟分布，但进程隔离通常降低了误报，除了Docker容器实验。尽管Docker容器内部依赖cgroups和CPU固定，但对噪声导致的性能退化更敏感。

Conclusion: 建议对同步工作负载使用进程隔离，但排除Docker容器。

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [5] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: 提出了一种基于谱空间的拓扑编码方法，用于分析分布式计算中无色任务的可行性，通过Stone对偶性建立了通用分布式可计算性定理。


<details>
  <summary>Details</summary>
Motivation: 统一分布式计算中的拓扑方法，为消息对手模型下的无色任务可解性提供通用理论框架。

Method: 将全局状态视为谱空间而非抽象单纯复形，使用Alexandrov拓扑在面偏序集上，通过谱空间的投影极限定义极限对象。

Result: 得到了通用分布式可计算性定理：存在算法解决无色任务当且仅当存在与任务规范兼容的谱映射。

Conclusion: 该框架统一了许多已知的无色可计算性定理，并解释了着色和非着色模型具有相同计算能力的原因。

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [6] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文扩展了分布式计算中任务可解性的几何特征，将无色任务可解性定理推广到输入相关对手模型，并给出了k-集合协议在条件相关对手下的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定对手模型，但实际分布式系统中对手行为可能依赖于输入配置。本文旨在将可解性特征扩展到输入相关对手，并研究条件相关对手下的k-集合协议问题。

Method: 使用拓扑几何方法，将无色可计算性定理推广到输入相关对手模型，分析核心弹性对手的计算能力等价性，并通过几何构造简化证明过程。

Result: 证明了输入相关对手的可解性特征，核心弹性对手在崩溃时机不同的情况下具有相同计算能力，并给出了k-集合协议在条件相关对手下的充要条件。

Conclusion: 本文建立了输入相关对手和条件相关对手的统一理论框架，为分布式任务的可解性分析提供了更通用的几何特征方法。

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [7] [Formalizing ETLT and ELTL Design Patterns and Proposing Enhanced Variants: A Systematic Framework for Modern Data Engineering](https://arxiv.org/abs/2511.03393)
*Chiara Rucco,Motaz Saad,Antonella Longo*

Main category: cs.DB

TL;DR: 本文正式化ETLT和ELTL作为可重用设计模式，并引入增强变体ETLT++和ELTL++来解决治理、质量保证和可观测性方面的持续差距。


<details>
  <summary>Details</summary>
Motivation: 传统ETL和ELT设计模式难以满足现代可扩展性、治理和实时数据处理需求。混合方法如ETLT和ELTL已在实践中使用，但文献缺乏最佳实践和这些方法作为设计模式的正式认可。

Method: 在模式框架内系统定义ETLT和ELTL模式，概述其结构、权衡和用例。通过嵌入显式契约、版本控制、语义管理和持续监控作为强制性设计义务，将其扩展为ETLT++和ELTL++。

Result: 提出的框架为从业者提供结构化路线图，以构建可审计、可扩展且成本效益高的管道，在多云和实时环境中统一质量执行、血统和可用性。

Conclusion: 通过正式化ETLT和ELTL，并通过ETLT++和ELTL++增强它们，这项工作弥合了临时实践与系统设计之间的差距，为现代可信数据工程提供了可重用基础。

Abstract: Traditional ETL and ELT design patterns struggle to meet modern requirements
of scalability, governance, and real-time data processing. Hybrid approaches
such as ETLT (Extract-Transform-Load-Transform) and ELTL
(Extract-Load-Transform-Load) are already used in practice, but the literature
lacks best practices and formal recognition of these approaches as design
patterns. This paper formalizes ETLT and ELTL as reusable design patterns by
codifying implicit best practices and introduces enhanced variants, ETLT++ and
ELTL++, to address persistent gaps in governance, quality assurance, and
observability. We define ETLT and ELTL patterns systematically within a design
pattern framework, outlining their structure, trade-offs, and use cases.
Building on this foundation, we extend them into ETLT++ and ELTL++ by embedding
explicit contracts, versioning, semantic curation, and continuous monitoring as
mandatory design obligations. The proposed framework offers practitioners a
structured roadmap to build auditable, scalable, and cost-efficient pipelines,
unifying quality enforcement, lineage, and usability across multi-cloud and
real-time contexts. By formalizing ETLT and ELTL, and enhancing them through
ETLT++ and ELTL++, this work bridges the gap between ad hoc practice and
systematic design, providing a reusable foundation for modern, trustworthy data
engineering.

</details>


### [8] [HERP: Hardware for Energy Efficient and Realtime DB Search and Cluster Expansion in Proteomics](https://arxiv.org/abs/2511.03437)
*Md Mizanur Rahaman Nayan,Zheyu Li,Flavio Ponzina,Sumukh Pinge,Tajana Rosing,Azad J. Naeemi*

Main category: cs.DB

TL;DR: 提出了一种轻量级增量聚类和高度并行化的数据库搜索平台，针对资源受限环境，在不牺牲性能的前提下实现低能耗和低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统全聚类和搜索方法需要大量资源且延迟高，无法满足资源受限环境的需求。

Method: 利用质谱分析洞察，采用分桶并行化和查询调度；一次性硬件初始化预聚类蛋白质组学数据，支持连续数据库搜索和本地重新聚类；使用预聚类数据的启发式方法指导增量聚类。

Result: 增量聚类速度提升20倍，聚类误差仅增加0.3%；数据库搜索结果与最先进工具重叠率达96%；人类基因组数据集（131GB）设置消耗1.19mJ，1000次查询搜索消耗1.1μJ；分桶并行化实现100倍加速。

Conclusion: 该方法为资源受限环境提供了一种实用高效的替代方案，显著降低了能耗和延迟，同时保持了搜索质量。

Abstract: Database (DB) search and clustering are fundamental in proteomics but
conventional full clustering and search approaches demand high resources and
incur long latency. We propose a lightweight incremental clustering and highly
parallelizable DB search platform tailored for resource-constrained
environments, delivering low energy and latency without compromising
performance. By leveraging mass-spectrometry insights, we employ bucket-wise
parallelization and query scheduling to reduce latency. A one-time hardware
initialization with pre-clustered proteomics data enables continuous DB search
and local re-clustering, offering a more practical and efficient alternative to
clustering from scratch. Heuristics from pre-clustered data guide incremental
clustering, accelerating the process by 20x with only a 0.3% increase in
clustering error. DB search results overlap by 96% with state-of-the-art tools,
validating search quality. The hardware leverages a 3T 2M T J SOT-CAM at the
7nm node with a compute-in-memory design. For the human genome draft dataset
(131GB), setup requires 1.19mJ for 2M spectra, while a 1000 query search
consumes 1.1{\mu}J. Bucket-wise parallelization further achieves 100x speedup.

</details>


### [9] [In-Memory Indexing and Querying of Provenance in Data Preparation Pipelines](https://arxiv.org/abs/2511.03480)
*Khalid Belhajjame,Haroun Mezrioui,Yuyan Zhao*

Main category: cs.DB

TL;DR: 提出了一种基于张量的索引机制，用于高效捕获和查询数据准备流水线的细粒度数据溯源信息，结合追溯性溯源和前瞻性溯源来支持各种溯源查询。


<details>
  <summary>Details</summary>
Motivation: 数据溯源在数据准备流水线中有多种应用，如调试故障流水线、解释结果、验证公平性和识别数据质量问题，但需要高效的捕获和查询机制。

Method: 利用张量以最小内存捕获数据处理操作的细粒度溯源，在记录级溯源基础上提供属性级粒度，通过将追溯性溯源与前瞻性溯源信息相结合。

Result: 通过真实和合成数据的评估验证了该方法能够高效回答广泛的溯源查询问题。

Conclusion: 结合追溯性和前瞻性溯源的张量索引机制能够有效支持数据流水线的各种溯源应用需求。

Abstract: Data provenance has numerous applications in the context of data preparation
pipelines. It can be used for debugging faulty pipelines, interpreting results,
verifying fairness, and identifying data quality issues, which may affect the
sources feeding the pipeline execution. In this paper, we present an indexing
mechanism to efficiently capture and query pipeline provenance. Our solution
leverages tensors to capture fine-grained provenance of data processing
operations, using minimal memory. In addition to record-level lineage
relationships, we provide finer granularity at the attribute level. This is
achieved by augmenting tensors, which capture retrospective provenance, with
prospective provenance information, drawing connections between input and
output schemas of data processing operations. We demonstrate how these two
types of provenance (retrospective and prospective) can be combined to answer a
broad range of provenance queries efficiently, and show effectiveness through
evaluation exercises using both real and synthetic data.

</details>


### [10] [Analytical Queries for Unstructured Data](https://arxiv.org/abs/2511.03489)
*Daniel Kang*

Main category: cs.DB

TL;DR: 本文讨论了在非结构化数据（特别是视频分析）中使用机器学习的数据管理系统面临的挑战和最新进展，包括查询表达、计算成本和错误处理等问题。


<details>
  <summary>Details</summary>
Motivation: 随着非结构化数据的爆炸式增长和机器学习能力的提升，如何有效部署这些技术来分析现实世界数据成为重要挑战，特别是在查询效率、数据表达形式和错误处理方面。

Method: 数据管理社区通过用户定义函数、结构化模式和不透明查询等方式改进查询表达；通过近似计算和优化技术降低计算成本；通过异常检测和漂移检测处理ML模型错误。

Result: 近期研究在查询表达、优化方法和错误处理方面取得了进展，使得在非结构化数据上部署机器学习分析变得更加可行。

Conclusion: 虽然机器学习在非结构化数据分析中面临诸多挑战，但通过数据管理系统的创新方法，这些问题正在得到有效解决，为现实世界数据分析提供了新的可能性。

Abstract: Unstructured data, in the form of text, images, video, and audio, is produced
at exponentially higher rates. In tandem, machine learning (ML) methods have
become increasingly powerful at analyzing unstructured data. Modern ML methods
can now detect objects in images, understand actions in videos, and even
classify complex legal texts based on legal intent. Combined, these trends make
it increasingly feasible for analysts and researchers to automatically
understand the "real world." However, there are major challenges in deploying
these techniques: 1) executing queries efficiently given the expense of ML
methods, 2) expressing queries over bespoke forms of data, and 3) handling
errors in ML methods.
  In this monograph, we discuss challenges and advances in data management
systems for unstructured data using ML, with a particular focus on video
analytics. Using ML to answer queries introduces new challenges.First, even
turning user intent into queries can be challenging: it is not obvious how to
express a query of the form "select instances of cars turning left." Second, ML
models can be orders of magnitude more expensive compared to processing
traditional structured data. Third, ML models and the methods to accelerate
analytics with ML models can be error-prone.
  Recent work in the data management community has aimed to address all of
these challenges. Users can now express queries via user-defined functions,
opaquely through standard structured schemas, and even by providing examples.
Given a query, recent work focuses on optimizing queries by approximating
expensive "gold" methods with varying levels of guarantees. Finally, to handle
errors in ML models, recent work has focused on applying outlier and drift
detection to data analytics with ML.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [SELF-REDRAFT: Eliciting Intrinsic Exploration-Exploitation Balance in Test-Time Scaling for Code Generation](https://arxiv.org/abs/2511.02854)
*Yixiang Chen,Tianshi Zheng,Shijue Huang,Zhitao He,Yi R. Fung*

Main category: cs.SE

TL;DR: SELF-REDRAFT框架在无需测试用例的情况下，通过平衡利用和探索来提高代码生成性能，相比Self-Refine有更好表现，但反馈生成和判别能力仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 在真实代码生成场景中，测试用例通常不可用，现有方法在利用（迭代优化）和探索（采样投票）之间的平衡研究不足。

Method: 基于Self-Refine框架构建SELF-REDRAFT，鼓励模型对根本性错误的解决方案提出新草案，平衡利用和探索。

Result: 在相同最大迭代次数下，SELF-REDRAFT持续优于Self-Refine，但不同LLM的平衡策略差异显著，反馈生成和判别能力仍是主要瓶颈。

Conclusion: 本研究为测试时扩展中的内在探索-利用平衡建立了基线，并指出反馈和判别能力是未来改进的关键方向。

Abstract: Test-time scaling without interpreter feedback is essential for real-world
code generation scenarios where test cases are not readily available. While
existing paradigms often rely on either greedy exploitation (i.e., iterative
refinement) or stochastic exploration (i.e., relying on sample-based voting or
reranking mechanisms), the balance between these two dimensions remains
underexplored. To investigate the LLM's intrinsic ability to balance
exploitation and exploration, we introduce SELF-REDRAFT, a framework built upon
Self-Refine that encourages the model to propose new drafts for solutions that
are fundamentally flawed. Our results show that SELF-REDRAFT consistently
achieves better performance than Self-Refine when converged under the same
maximum number of iterations. Still, we observe that significant room for
improvement remains, largely due to two core aspects of current self-redraft
capabilities: constrained capacity for generating instructive feedback and
fragile discriminative judgment. We also find that balancing strategies vary
notably across different LLMs, reflecting distinct, model-specific behaviors.
Overall, our study establishes a baseline for intrinsic
exploration-exploitation balancing in test-time scaling and identifies feedback
and discrimination as key areas with potential for future advances.

</details>


### [12] [The Evolution of Agile and Hybrid Project Management Methodologies: A Systematic Literature Review](https://arxiv.org/abs/2511.02859)
*Bianca Leech,Ridewaan Hanslo*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了敏捷方法论向混合框架的演变，识别了实施挑战和成功因素，强调混合方法结合了迭代灵活性和结构化治理。


<details>
  <summary>Details</summary>
Motivation: IT项目的快速发展推动了项目管理方法论的转变，从传统瀑布式到敏捷框架，再到混合模型。需要研究敏捷方法论向混合框架的演变过程及其在大型和受监管环境中的适应性。

Method: 采用PRISMA指导的系统文献综述方法，分析了过去8年的同行评审研究，识别关键趋势。

Result: 混合方法论源于敏捷在大型和受监管环境中的局限性，成功实施依赖于领导支持、定制化流程整合和持续改进机制。

Conclusion: 研究强调需要基于具体情境进行适应性调整而非采用僵化框架，为组织进行混合转型提供了实用见解。

Abstract: The rapid evolution of IT projects has driven the transformation of project
management methodologies, from traditional waterfall approaches to agile
frameworks and, more recently, hybrid models. This systematic literature review
investigates the evolution of agile methodologies into hybrid frameworks,
analysing their implementation challenges and success factors. We identify key
trends through PRISMA-guided analysis of peer-reviewed studies from the last 8
years. Hybrid methodologies emerge from agile limitations in large-scale and
regulated environments, combining iterative flexibility with structured
governance. Agile has several implementation challenges, leading to hybrid
methods, and the success hinges on leadership support, tailored process
integration, and continuous improvement mechanisms. The study explores the need
for contextual adaptation over rigid frameworks, offering practical insights
for organisations navigating hybrid transitions.

</details>


### [13] [LM-Fix: Lightweight Bit-Flip Detection and Rapid Recovery Framework for Language Models](https://arxiv.org/abs/2511.02866)
*Ahmad Tahmasivand,Noureldin Zahran,Saba Al-Sayouri,Mohammed Fouda,Khaled N. Khasawneh*

Main category: cs.SE

TL;DR: LM-Fix是一个轻量级的大语言模型故障检测与快速恢复框架，通过短测试向量和哈希引导检测位翻转故障，实现局部修复而无需完全重载。


<details>
  <summary>Details</summary>
Motivation: 现有完整性方法对现代大语言模型来说通常过于笨重或缓慢，需要一种实用的低开销解决方案来保持LLM在生产环境中的可靠性。

Method: 运行短测试向量并通过哈希引导检查检测位翻转故障，然后进行局部修复而不需要完全重载模型。

Result: 在多个模型中，TVL=200时检测到超过94%的单比特翻转和近100%的多比特翻转，运行时开销约为1%到7.7%；恢复速度比重载快100倍以上。

Conclusion: LM-Fix为保持大语言模型在生产环境中的可靠性提供了一个实用且低开销的解决方案。

Abstract: This paper presents LM-Fix, a lightweight detection and rapid recovery
framework for faults in large language models (LLMs). Existing integrity
approaches are often heavy or slow for modern LLMs. LM-Fix runs a short
test-vector pass and uses hash-guided checks to detect bit-flip faults, then
repairs them locally without a full reload. Across multiple models, it detects
over 94% of single-bit flips at TVL=200 and nearly 100% of multi-bit flips with
approximately 1% to 7.7% runtime overhead; recovery is more than 100x faster
than reloading. These results show a practical, low-overhead solution to keep
LLMs reliable in production

</details>


### [14] [Analysis of AdvFusion: Adapter-based Multilingual Learning for Code Large Language Models](https://arxiv.org/abs/2511.02869)
*Amirreza Esmaeili,Fahd Seddik,Yongyi Ji,Fatemeh Fard,Fuxiang Chen*

Main category: cs.SE

TL;DR: AdvFusion是一种基于PEFT的方法，在多语言代码模型上测试了代码生成、代码翻译和提交消息生成三个新任务，发现不同模型/任务表现出不同特性。


<details>
  <summary>Details</summary>
Motivation: 探索AdvFusion在Code-LLMs上的扩展应用，验证其在更多软件工程任务中的有效性，并与现有PEFT方法进行比较。

Method: 使用AdvFusion方法，在预训练的Code-LLMs上对三个新任务（代码生成、代码翻译、提交消息生成）进行实验，并与AdapterFusion、LoRA、Compacter、TaskAdapter等PEFT方法进行对比。

Result: 不同任务表现各异：代码生成中AdvFusion优于AdapterFusion但不如其他PEFT方法；提交消息生成中AdapterFusion更好；代码翻译中AdvFusion整体表现较差，且随着模型增大差距扩大。

Conclusion: AdvFusion在不同Code-LLMs和任务中表现不一致，需要根据具体任务选择合适的PEFT方法，没有一种方法在所有情况下都最优。

Abstract: Programming languages can benefit from one another by utilizing a language
model for software engineering tasks. Full fine-tuning and Parameter Efficient
Fine-Tuning (PEFT) of Code Language Models (Code-LMs) has been explored for
multilingual knowledge transfer. AdapterFusion is a PEFT architecture that aims
to enhance task performance by leveraging information from multiple programming
languages, but primarily focuses on the target programming language.
  In our previous work, we proposed AdvFusion, a novel PEFT-based approach that
effectively learns from other programming languages before adapting to the
target task. Though previous experiments showed that AdvFusion outperformed
AdapterFusion and LoRA, it was applied on pre-trained Code-LMs and was limited
to only two tasks, code summarization and method name prediction. In this
study, we expanded our work and investigated AdvFusion on Code Large Language
Models (Code-LLMs), considering three new tasks: code generation, code
translation, and commit message generation. We observed that different
Code-LLMs/tasks exhibit different characteristics. In code generation,
AdvFusion outperformed AdapterFusion but not other PEFT methods (LoRA,
Compacter, and TaskAdapter). In commit message generation, AdapterFusion
performed better than AdvFusion, and contrary to code generation, we found that
the other PEFT methods do not have better performance. In code translation,
AdvFusion performed worse than AdapterFusion overall, with the performance gap
marginally widening as the model size increases. However, consistent with code
generation, other PEFT methods showed better performance.

</details>


### [15] [An Analysis of Early-Stage Functional Safety Analysis Methods and Their Integration into Model-Based Systems Engineering](https://arxiv.org/abs/2511.02874)
*Jannatul Shefa,Taylan G. Topcu*

Main category: cs.SE

TL;DR: 本文对比了FMEA、FHA和FFIP三种安全分析方法，发现FFIP更适合现代互联系统的安全需求，并分析了这些方法与MBSE集成的现状，指出目前缺乏统一的集成框架。


<details>
  <summary>Details</summary>
Motivation: 随着系统日益复杂，在系统生命周期早期进行有效的安全分析至关重要，以在风险升级前识别和缓解风险。

Method: 采用两阶段方法：第一阶段对比FMEA、FHA和FFIP技术，分析其程序、优势和局限性；第二阶段回顾将这些方法集成到MBSE中的现有研究。

Result: 分析显示FFIP在识别系统涌现行为、二阶效应和故障传播方面能力更强；MBSE集成主要关注FMEA，FHA和FFIP集成尚处于起步阶段；FMEA-MBSE集成可分为四类方法。

Conclusion: 虽然存在多种MBSE集成方法，但缺乏普遍建立的框架或标准，这为支持数字工程转型的集成方法留下了发展空间。

Abstract: As systems become increasingly complex, conducting effective safety analysis
in the earlier phases of a system's lifecycle is essential to identify and
mitigate risks before they escalate. To that end, this paper investigates the
capabilities of key safety analysis techniques, namely: Failure Mode and
Effects Analysis (FMEA), Functional Hazard Analysis (FHA), and Functional
Failure Identification and Propagation (FFIP), along with the current state of
the literature in terms of their integration into Model-Based Systems
Engineering (MBSE). A two-phase approach is adopted. The first phase is focused
on contrasting FMEA, FHA, and FFIP techniques, examining their procedures,
along with a documentation of their relative strengths and limitations. Our
analysis highlights FFIP's capability in identifying emergent system behaviors,
second-order effects, and fault propagation; thus, suggesting it is better
suited for the safety needs of modern interconnected systems. Second, we review
the existing research on the efforts to integrate each of these methods into
MBSE. We find that MBSE integration efforts primarily focus on FMEA, and
integration of FHA and FFIP is nascent. Additionally, FMEA-MBSE integration
efforts could be organized into four categories: model-to-model transformation,
use of external customized algorithms, built-in MBSE packages, and manual use
of standard MBSE diagrams. While our findings indicate a variety of MBSE
integration approaches, there is no universally established framework or
standard. This leaves room for an integration approach that could support the
ongoing Digital Engineering transformation efforts by enabling a more
synergistic lifecycle safety management methods and tools.

</details>


### [16] [CS Educator challenges and their solutions : A systematic mapping study](https://arxiv.org/abs/2511.02876)
*Anjali Chouhan,Sruti Srinivasa Ragavan,Amey Karkare*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了计算机科学教育中面临的挑战及应对措施，识别了十个主题领域的重复性问题，并指出研究不足的领域。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育快速发展，但缺乏对教育者面临的具体挑战和应对措施的系统分类与综合，不清楚哪些领域已得到充分研究，哪些仍需关注。

Method: 对过去五年同行评审研究论文进行结构化文献综述，重点关注十个分类主题（教学法、情感、技术、制度等维度）中的挑战和补救措施。

Result: 分析揭示了评估实践、教师培训、课堂管理和情感健康等领域的重复性问题，以及专业发展计划和政策干预等应对策略，同时发现若干研究不足的领域。

Conclusion: 本综述提供了对计算机科学教育现状的整合理解，为研究人员、课程设计者和政策制定者改善教学效果和教育者支持提供了有价值的见解。

Abstract: Computer Science (CS) education is expanding rapidly, but educators continue
to face persistent challenges in teaching and learning environments.Despite
growing interest, limited systematic work exists to categorize and synthesize
the specific challenges faced by CS educators and the remedies adopted in
response.This is problematic because it remains unclear which areas have been
thoroughly addressed and which still lack sufficient scholarly attention. In
this study, we conducted a structured literature review of peer-reviewed
research papers published over the last five years, focusing on challenges and
remedies across ten categorized themes, including pedagogical, emotional,
technological, and institutional dimensions.Our analysis revealed recurring
issues in areas such as assessment practices, teacher training, classroom
management, and emotional well-being, along with various strategies such as
professional development programs and policy interventions adopted to mitigate
them while also revealing several areas that have received insufficient
attention.This review offers a consolidated understanding of the CS education
landscape, providing valuable insights for researchers, curriculum designers,
and policymakers aiming to improve teaching effectiveness and educator support.

</details>


### [17] [AgentSLA : Towards a Service Level Agreement for AI Agents](https://arxiv.org/abs/2511.02885)
*Gwendal Jouneaux,Jordi Cabot*

Main category: cs.SE

TL;DR: 本文提出了基于ISO/IEC 25010标准的AI智能体质量模型和领域特定语言，用于支持AI智能体服务的SLA定义。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体成为软件系统的关键组件，从模型即服务转向智能体即服务带来了新的挑战，特别是在QoS和SLA规范方面缺乏共识和方法。

Method: 开发基于ISO/IEC 25010标准的AI智能体质量模型，并设计领域特定语言来支持AI智能体服务的SLA定义。

Result: 提出了系统的质量模型和SLA定义语言，为AI智能体的质量保证提供了标准化框架。

Conclusion: 该研究为解决AI智能体质量保证和SLA定义的挑战提供了实用解决方案，有助于提升智能软件系统的质量。

Abstract: AI components are increasingly becoming a key element of all types of
software systems to enhance their functionality. These AI components are often
implemented as AI Agents, offering more autonomy than a plain integration of
Large Language Models (LLMs), moving from a Model-as-a-Service paradigm to an
Agent-as-a-Service one, bringing new challenges to the development of smart
software systems. Indeed, while support for the design, implementation, and
deployment of those agents exist, the specification of Quality of Service (QoS)
and definition of Service Level Agreements (SLAs) aspects for those agents,
important to ensure the quality of the resulting systems, remains an open
challenge. Part of this is due to the difficulty to clearly define quality in
the context of AI components, resulting in a lack of consensus on how to best
approach Quality Assurance (QA) for these types of systems. To address this
challenge, this paper proposes both a quality model for AI agents based on the
ISO/IEC 25010 standard, and a domain specific language to support the
definition of SLAs for the services provided by these AI agents.

</details>


### [18] [Comprehension-Performance Gap in GenAI-Assisted Brownfield Programming: A Replication and Extension](https://arxiv.org/abs/2511.02922)
*Yunhan Qiao,Christopher Hundhausen,Summit Haque,Md Istiak Hossain Shihab*

Main category: cs.SE

TL;DR: GitHub Copilot显著提高了遗留代码库编程任务的生产力（减少任务时间、增加测试用例通过率），但并未改善对代码的理解能力，显示出理解与性能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI编程助手在遗留代码库（brownfield）编程任务中对开发人员代码理解能力的影响，复制并扩展先前研究。

Method: 采用组内实验设计，18名计算机科学研究生在有和没有Copilot的情况下完成功能实现任务，比较任务时间、测试用例通过率和理解得分。

Result: Copilot显著减少了任务时间并增加了测试用例通过率，但理解得分在不同条件下没有差异，且理解与任务性能之间没有相关性。

Conclusion: 生成式AI工具可以加速遗留代码库的编程进度，但这种进步可能不会带来对代码库理解的提升，这对编程教育和GenAI工具设计具有重要启示。

Abstract: Code comprehension is essential for brownfield programming tasks, in which
developers maintain and enhance legacy code bases. Generative AI (GenAI) coding
assistants such as GitHub Copilot have been shown to improve developer
productivity, but their impact on code understanding is less clear. We
replicate and extend a previous study by exploring both performance and
comprehension in GenAI-assisted brownfield programming tasks. In a
within-subjects experimental study, 18 computer science graduate students
completed feature implementation tasks with and without Copilot. Results show
that Copilot significantly reduced task time and increased the number of test
cases passed. However, comprehension scores did not differ across conditions,
revealing a comprehension-performance gap: participants passed more test cases
with Copilot, but did not demonstrate greater understanding of the legacy
codebase. Moreover, we failed to find a correlation between comprehension and
task performance. These findings suggest that while GenAI tools can accelerate
programming progress in a legacy codebase, such progress may come without an
improved understanding of that codebase. We consider the implications of these
findings for programming education and GenAI tool design.

</details>


### [19] [Risk Estimation in Differential Fuzzing via Extreme Value Theory](https://arxiv.org/abs/2511.02927)
*Rafael Baez,Alejandro Olivas,Nathan K. Diamond,Marcelo Frias,Yannic Noller,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 本文应用极值理论(EVT)来评估差分模糊测试中遗漏或低估bug的风险，通过统计方法分析测试结果的尾部风险，在真实Java库中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 差分模糊测试作为动态分析无法保证bug的完全发现，需要一种方法来评估测试活动中遗漏bug的风险。

Method: 应用极值理论(EVT)分析差分模糊测试结果的极值分布，并与马尔可夫不等式、切比雪夫不等式和贝叶斯因子等基线方法进行比较。

Result: EVT方法在14.3%的情况下优于基线方法，64.2%的情况下与基线持平，在真实Java库中平均节省了数千万字节码执行。

Conclusion: 极值理论为差分模糊测试提供了有效的风险评估框架，能够显著提高测试效率并准确评估遗漏bug的风险。

Abstract: Differential testing is a highly effective technique for automatically
detecting software bugs and vulnerabilities when the specifications involve an
analysis over multiple executions simultaneously. Differential fuzzing, in
particular, operates as a guided randomized search, aiming to find (similar)
inputs that lead to a maximum difference in software outputs or their
behaviors. However, fuzzing, as a dynamic analysis, lacks any guarantees on the
absence of bugs: from a differential fuzzing campaign that has observed no bugs
(or a minimal difference), what is the risk of observing a bug (or a larger
difference) if we run the fuzzer for one or more steps?
  This paper investigates the application of Extreme Value Theory (EVT) to
address the risk of missing or underestimating bugs in differential fuzzing.
The key observation is that differential fuzzing as a random process resembles
the maximum distribution of observed differences. Hence, EVT, a branch of
statistics dealing with extreme values, is an ideal framework to analyze the
tail of the differential fuzzing campaign to contain the risk. We perform
experiments on a set of real-world Java libraries and use differential fuzzing
to find information leaks via side channels in these libraries. We first
explore the feasibility of EVT for this task and the optimal hyperparameters
for EVT distributions. We then compare EVT-based extrapolation against baseline
statistical methods like Markov's as well as Chebyshev's inequalities, and the
Bayes factor. EVT-based extrapolations outperform the baseline techniques in
14.3% of cases and tie with the baseline in 64.2% of cases. Finally, we
evaluate the accuracy and performance gains of EVT-enabled differential fuzzing
in real-world Java libraries, where we reported an average saving of tens of
millions of bytecode executions by an early stop.

</details>


### [20] [Assurance Case Development for Evolving Software Product Lines: A Formal Approach](https://arxiv.org/abs/2511.03026)
*Logan Murphy,Torin Viger,Alessio Di Sandro,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: 提出了一种用于软件产品线(SPL)的形式化提升保证案例(AC)开发和回归分析方法，解决了为大量产品单独开发AC不可行的问题。


<details>
  <summary>Details</summary>
Motivation: 在软件产品线中，为每个产品单独开发严格的保证案例是不可行的，且当产品线演进时难以评估变更影响。需要将AC开发和维护提升到整个产品线层面。

Method: 形式化定义了面向SPL的可变性感知AC语言，研究了基于模板的AC开发提升方法，定义了SPL演进对可变性感知AC影响的回归分析。

Result: 开发了基于模型的保证管理工具，并通过医疗设备产品线的AC开发案例验证了方法的有效性。

Conclusion: 该方法能够为整个软件产品线同时开发单一AC，并以可变性感知的方式进行回归分析，解决了大规模产品线中AC开发的可行性问题。

Abstract: In critical software engineering, structured assurance cases (ACs) are used
to demonstrate how key system properties are supported by evidence (e.g., test
results, proofs). Creating rigorous ACs is particularly challenging in the
context of software product lines (SPLs), i.e, sets of software products with
overlapping but distinct features and behaviours. Since SPLs can encompass very
large numbers of products, developing a rigorous AC for each product
individually is infeasible. Moreover, if the SPL evolves, e.g., by the
modification or introduction of features, it can be infeasible to assess the
impact of this change. Instead, the development and maintenance of ACs ought to
be lifted such that a single AC can be developed for the entire SPL
simultaneously, and be analyzed for regression in a variability-aware fashion.
In this article, we describe a formal approach to lifted AC development and
regression analysis. We formalize a language of variability-aware ACs for SPLs
and study the lifting of template-based AC development. We also define a
regression analysis to determine the effects of SPL evolutions on
variability-aware ACs. We describe a model-based assurance management tool
which implements these techniques, and illustrate our contributions by
developing an AC for a product line of medical devices.

</details>


### [21] [Adaptive Detection of Software Aging under Workload Shift](https://arxiv.org/abs/2511.03103)
*Rafael José Moura,Maria Gizele Nascimento,Fumio Machida,Ermeson Andrade*

Main category: cs.SE

TL;DR: 提出基于机器学习的自适应软件老化检测方法，在动态工作负载条件下保持高检测精度


<details>
  <summary>Details</summary>
Motivation: 软件老化影响长期运行系统，导致性能逐渐下降和故障风险增加，需要解决动态工作负载条件下的检测问题

Method: 比较静态模型与包含DDM和ADWIN自适应检测器的自适应模型，将概念漂移检测方法应用于处理工作负载变化

Result: 静态模型在新工作负载下性能显著下降，而带ADWIN的自适应模型在所有场景中F1-Score均超过0.93

Conclusion: 自适应模型特别是ADWIN方法能有效处理工作负载变化，在软件老化检测中保持高精度

Abstract: Software aging is a phenomenon that affects long-running systems, leading to
progressive performance degradation and increasing the risk of failures. To
mitigate this problem, this work proposes an adaptive approach based on machine
learning for software aging detection in environments subject to dynamic
workload conditions. We evaluate and compare a static model with adaptive
models that incorporate adaptive detectors, specifically the Drift Detection
Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept
drift scenarios and applied in this work to handle workload shifts. Experiments
with simulated sudden, gradual, and recurring workload transitions show that
static models suffer a notable performance drop when applied to unseen workload
profiles, whereas the adaptive model with ADWIN maintains high accuracy,
achieving an F1-Score above 0.93 in all analyzed scenarios.

</details>


### [22] [Automated Prompt Generation for Code Intelligence: An Empirical study and Experience in WeChat](https://arxiv.org/abs/2511.03136)
*Kexing Ji,Shiyun Fu,Cuiyun Gao,Yujia Chen,Zezhou Yang,Chaozheng Wang,Yuetang Deng*

Main category: cs.SE

TL;DR: 本文研究了大型代码模型的自动提示生成方法，通过指令生成和多步推理显著提升了代码智能任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型代码模型的效果受提示质量影响很大，但当前提示设计多为手动，耗时且依赖特定模型和任务。NLP领域的自动提示生成在代码智能中研究不足，存在自动化需求。

Method: 实证研究指令生成和多步推理两种自动提示生成方法，并在四种开源LCMs和三个代码智能任务上评估。基于结果提出结合最佳方法的新APG方法。

Result: IG和MSR相比基础提示显著提升性能。提出的方法在代码翻译、代码总结和API推荐任务上分别平均提升28.38%、58.11%和84.53%。在工业数据集上API推荐的MRR提升148.89%。

Conclusion: 自动提示生成能有效提升大型代码模型在代码智能任务中的性能，提出的方法在多个任务和数据集上表现出显著改进。

Abstract: Large Code Models (LCMs) show potential in code intelligence, but their
effectiveness is greatly influenced by prompt quality. Current prompt design is
mostly manual, which is time-consuming and highly dependent on specific LCMs
and tasks. While automated prompt generation (APG) exists in NLP, it is
underexplored for code intelligence. This creates a gap, as automating the
prompt process is essential for developers facing diverse tasks and black-box
LCMs.
  To mitigate this, we empirically investigate two important parts of APG:
Instruction Generation (IG) and Multi-Step Reasoning (MSR). IG provides a
task-related description to instruct LCMs, while MSR guides them to produce
logical steps before the final answer. We evaluate widely-used APG methods for
each part on four open-source LCMs and three code intelligence tasks: code
translation (PL-PL), code summarization (PL-NL), and API recommendation
(NL-PL).Experimental results indicate that both IG and MSR dramatically enhance
performance compared to basic prompts. Based on these results, we propose a
novel APG approach combining the best methods of the two parts. Experiments
show our approach achieves average improvements of 28.38% in CodeBLEU (code
translation), 58.11% in ROUGE-L (code summarization), and 84.53% in
SuccessRate@1 (API recommendation) over basic prompts. To validate its
effectiveness in an industrial scenario, we evaluate our approach on
WeChat-Bench, a proprietary dataset, achieving an average MRR improvement of
148.89% for API recommendation.

</details>


### [23] [RefAgent: A Multi-agent LLM-based Framework for Automatic Software Refactoring](https://arxiv.org/abs/2511.03153)
*Khouloud Oueslati,Maxime Lamothe,Foutse Khomh*

Main category: cs.SE

TL;DR: RefAgent是一个基于LLM的多智能体框架，用于端到端软件重构，通过专门的规划、执行、测试和迭代优化智能体，显著提升重构效果。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在软件重构中依赖静态指令，而基于LLM的智能体能够动态适应上下文并自主决策，探索多智能体架构在重构活动中的潜力。

Method: 提出RefAgent多智能体框架，包含规划、执行、测试和迭代优化智能体，利用自反思和工具调用能力进行端到端重构。

Result: 在8个开源Java项目上评估，RefAgent达到90%的中位单元测试通过率，减少52.5%的代码异味，提升8.6%的关键质量属性，相比单智能体方法提升64.7%的测试通过率和40.1%的编译成功率。

Conclusion: 多智能体架构在自动化软件重构方面具有显著优势，能够有效提升重构质量和效率。

Abstract: Large Language Models (LLMs) have substantially influenced various software
engineering tasks. Indeed, in the case of software refactoring, traditional
LLMs have shown the ability to reduce development time and enhance code
quality. However, these LLMs often rely on static, detailed instructions for
specific tasks. In contrast, LLM-based agents can dynamically adapt to evolving
contexts and autonomously make decisions by interacting with software tools and
executing workflows. In this paper, we explore the potential of LLM-based
agents in supporting refactoring activities. Specifically, we introduce
RefAgent, a multi-agent LLM-based framework for end-to-end software
refactoring. RefAgent consists of specialized agents responsible for planning,
executing, testing, and iteratively refining refactorings using self-reflection
and tool-calling capabilities. We evaluate RefAgent on eight open-source Java
projects, comparing its effectiveness against a single-agent approach, a
search-based refactoring tool, and historical developer refactorings. Our
assessment focuses on: (1) the impact of generated refactorings on software
quality, (2) the ability to identify refactoring opportunities, and (3) the
contribution of each LLM agent through an ablation study. Our results show that
RefAgent achieves a median unit test pass rate of 90%, reduces code smells by a
median of 52.5%, and improves key quality attributes (e.g., reusability) by a
median of 8.6%. Additionally, it closely aligns with developer refactorings and
the search-based tool in identifying refactoring opportunities, attaining a
median F1-score of 79.15% and 72.7%, respectively. Compared to single-agent
approaches, RefAgent improves the median unit test pass rate by 64.7% and the
median compilation success rate by 40.1%. These findings highlight the promise
of multi-agent architectures in advancing automated software refactoring.

</details>


### [24] [Understanding Robustness of Model Editing in Code LLMs: An Empirical Study](https://arxiv.org/abs/2511.03182)
*Vinaik Chhetri,A. B Siddique,Umar Farooq*

Main category: cs.SE

TL;DR: 本研究系统评估了5种模型编辑方法在代码LLMs中的应用，发现在API弃用场景下，即时编辑会显著降低模型性能，语法有效性下降高达86个百分点，功能正确性下降45点。顺序编辑进一步加剧性能退化，正确采用率仅约6%。


<details>
  <summary>Details</summary>
Motivation: LLMs在训练后保持静态，而编程语言和API持续演化，导致生成的代码可能已过时或不兼容。重新训练LLMs成本高昂，模型编辑作为轻量级替代方案具有潜力，但需要验证其是否真正实现语法和语义适应。

Method: 对5种最先进的模型编辑方法（Constrained FT、GRACE、MEMIT、PMET、ROME）在3个开源代码LLMs（CodeLlama、CodeQwen1.5、DeepSeek-Coder）上进行系统研究，在受控API弃用场景下评估即时和顺序编辑设置。

Result: 即时编辑持续降低模型性能，语法有效性下降高达86个百分点，功能正确性下降45点。顺序编辑进一步加剧退化，某些情况下模型性能完全崩溃。正确采用率仅约6%，大多数通过生成依赖变通方法而非正确采用预期更改。

Conclusion: 当前模型编辑方法在代码LLMs中效果有限，主要产生表面修复而非真正的语法语义适应，正确采用率极低，需要更有效的编辑技术来应对API演化挑战。

Abstract: Large language models (LLMs) are increasingly used in software development.
However, while LLMs remain static after pretraining, programming languages and
APIs continue to evolve, leading to the generation of deprecated or
incompatible code that undermines reliability. Retraining LLMs from scratch to
reflect such changes is computationally expensive, making model editing a
promising lightweight alternative that updates only a small subset of
parameters. Despite its potential, it remains unclear whether model editing
yields genuine syntactic and semantic adaptations or merely superficial fixes.
In this work, we present a systematic study of five state-of-the-art model
editing methods: Constrained Fine-Tuning (FT), GRACE, MEMIT, PMET, and ROME. We
apply these methods to three leading open-source code LLMs, CodeLlama,
CodeQwen1.5, and DeepSeek-Coder, under controlled API deprecation scenarios.
Our evaluation covers both instant and sequential editing settings, using three
disjoint evaluation sets designed to assess reliability, generalization, and
specificity. We measure model correctness at three levels: successful
compilation, partial test case pass, and full test pass. Our findings show that
instant edits consistently degrade model performance, with syntactic validity
dropping by up to 86 percentage points and functional correctness declining by
45 points even in the best-performing setting. Sequential edits further amplify
this degradation, and in some cases, model performance collapses entirely.
Across all models, most passing generations relied on workarounds rather than
correctly adopting the intended changes, while faulty adoptions that result in
test failures or compilation errors were significantly more frequent. Correct
adoptions, where the model correctly integrates the intended change, occurred
in only about 6% of cases.

</details>


### [25] [Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling](https://arxiv.org/abs/2511.03404)
*Qianhui Zhao,Li Zhang,Fang Liu,Junhang Cheng,Chengru Wu,Junchen Ai,Qiaoyuanhe Meng,Lichen Zhang,Xiaoli Lian,Shubin Song,Yuanping Guo*

Main category: cs.SE

TL;DR: 提出了ProjectGen多智能体框架和CodeProjectEval数据集，用于解决项目级代码生成中的语义鸿沟、层次依赖管理和质量维护问题，在基准测试中表现显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有项目级代码生成研究存在数据集不现实、评估指标不可靠、用户需求与机器可解释结构之间的语义鸿沟、层次依赖管理困难以及生成过程质量维护等关键限制。

Method: 提出ProjectGen多智能体框架，将项目分解为架构设计、骨架生成和代码填充三个阶段，采用迭代优化和基于记忆的上下文管理，并引入语义软件架构树(SSAT)来桥接用户需求和源代码实现。

Result: 在DevBench小型项目级代码生成数据集上通过52/124个测试用例，比基线方法提升57%；在CodeProjectEval数据集上通过310个测试用例，比基线方法提升约10倍。

Conclusion: ProjectGen框架在项目级代码生成任务中实现了最先进的性能，有效解决了现有方法的局限性，为实际软件开发中的快速迭代和持续交付需求提供了有力支持。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
progress in automated code generation. In real-world software engineering, the
growing demand for rapid iteration and continuous delivery underscores the
importance of project-level code generation, where LLMs are expected to
generate complete software projects directly from complex user requirements.
Although existing studies have made initial explorations, they still face key
limitations, including unrealistic datasets and unreliable evaluation metrics
that fail to reflect real-world complexity, the semantic gap between
human-written requirements and machine-interpretable structures, and
difficulties in managing hierarchical dependencies and maintaining quality
throughout the generation process. To address these limitations, we first
introduce CodeProjectEval, a project-level code generation dataset built from
18 real-world repositories with 12.7 files and 2,388.6 lines of code per task
on average, supplemented with documentation and executable test cases for
automatic evaluation. We further propose ProjectGen, a multi-agent framework
that decomposes projects into architecture design, skeleton generation, and
code filling stages with iterative refinement and memory-based context
management. Within this framework, we introduce the Semantic Software
Architecture Tree (SSAT), a structured and semantically rich representation
that effectively bridges user requirements and source code implementation.
Experiments show that ProjectGen achieves state-of-the-art performance, passing
52/124 test cases on the small-scale project-level code generation dataset
DevBench, a 57% improvement over the baseline approaches, and 310 test cases on
CodeProjectEval, representing an improvement of roughly tenfold compared to the
baselines.

</details>


### [26] [Light over Heavy: Automated Performance Requirements Quantification with Linguistic Inducement](https://arxiv.org/abs/2511.03421)
*Shihai Wang,Tao Chen*

Main category: cs.SE

TL;DR: LQPR是一种高效的自动化性能需求量化方法，通过将量化问题转化为分类问题，使用轻量级语言诱导匹配机制，在成本降低两个数量级的同时，在75%以上的案例中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的性能需求量化主要依赖人工方法，成本高且容易出错。需要一种自动化的量化方法来提高效率和准确性。

Method: LQPR将性能需求量化转化为分类问题，利用性能需求具有强模式和简洁性的特点，设计轻量级语言诱导匹配机制，而非依赖大型语言模型。

Result: 在多样化数据集上与9种最先进的学习方法比较，LQPR在75%以上的案例中排名最佳，且成本降低了两个数量级。

Conclusion: 对于性能需求量化任务，专门化方法比通用的LLM驱动方法更合适，LQPR证明了这一点。

Abstract: Elicited performance requirements need to be quantified for compliance in
different engineering tasks, e.g., configuration tuning and performance
testing. Much existing work has relied on manual quantification, which is
expensive and error-prone due to the imprecision. In this paper, we present
LQPR, a highly efficient automatic approach for performance requirements
quantification.LQPR relies on a new theoretical framework that converts
quantification as a classification problem. Despite the prevalent applications
of Large Language Models (LLMs) for requirement analytics, LQPR takes a
different perspective to address the classification: we observed that
performance requirements can exhibit strong patterns and are often
short/concise, therefore we design a lightweight linguistically induced
matching mechanism. We compare LQPR against nine state-of-the-art
learning-based approaches over diverse datasets, demonstrating that it is
ranked as the sole best for 75% or more cases with two orders less cost. Our
work proves that, at least for performance requirement quantification,
specialized methods can be more suitable than the general LLM-driven
approaches.

</details>


### [27] [U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility](https://arxiv.org/abs/2511.03517)
*Wencheng Ye,Yan Liu*

Main category: cs.SE

TL;DR: U2F是一个认知启发的多智能体框架，通过拥抱不确定性来发现软件工程中的创新解决方案，显著提升了解决方案的新颖性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的软件工程智能体主要解决明确定义的问题，往往忽视预定义框架之外的替代或创新解决方案，这在开放世界软件环境中存在局限性。

Method: U2F包含两个关键组件：(1) 发现-探索-集成智能体系统，用于发现和综合潜在解决方案；(2) 跨三个维度的认知增强机制：跨领域类比推理、逆向思维和外部验证。

Result: 在218个真实世界软件赋能故事上，U2F实现了显著改进：整体新颖性提升14%，语义新颖性提升51%，可行性稳定在4.02/5.0。

Conclusion: 结果表明，在软件工程中拥抱不确定性可以作为创新的催化剂，具有重要潜力。

Abstract: Large language models (LLMs) have shown strong capabilities in software
engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle
well-defined problems using conventional methods, often overlooking alternative
or innovative solutions beyond their predefined frameworks. This limitation is
evident in open-world software environments, where emerging challenges
transcend established paradigms.
  We propose U2F (Unknown Unknowns to Functional solutions), a
cognitive-inspired, uncertainty-embracing multi-agent framework that
systematically surfaces "Unknown Unknowns" - novel solution pathways absent
from initial formulations but holding innovative potential. U2F consists of two
key components: (1) a Discovery-Exploration-Integration agent system for
uncovering and synthesizing potential solutions, and (2) cognitive enhancement
mechanisms across three dimensions: cross-domain analogical reasoning, reverse
thinking, and external validation, which strategically reframe and extend
conventional solution boundaries.
  Applied to 218 real-world software enabler stories curated from authentic
engineering tasks, U2F achieved notable improvements: human experts reported a
14 percent increase in overall novelty, 51 percent improvement in semantic
novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based
evaluator. These results highlight the potential of embracing uncertainty as a
catalyst for innovation in software engineering.

</details>


### [28] [Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code Understanding](https://arxiv.org/abs/2511.03549)
*Ziv Nevo,Orna Raz,Karen Yorav*

Main category: cs.SE

TL;DR: 提出了一种利用GitHub自然语言工件增强LLM代码理解的方法，通过提取GitHub上下文、生成代码目的解释和验证解释三个组件，提升代码解释的质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的代码解释缺乏软件工程上下文基础，需要利用GitHub中的自然语言工件来增强代码理解能力。

Method: 系统包含三个组件：提取结构化GitHub上下文、基于上下文生成代码高级解释、验证解释准确性。实现了独立工具和MCP服务器两种形式。

Result: 用户研究表明，生成的代码洞察通常有帮助且非平凡，且没有出现幻觉问题。

Conclusion: 利用GitHub自然语言工件可以有效增强LLM的代码理解能力，生成的解释质量更高且更可靠。

Abstract: Understanding the purpose of source code is a critical task in software
maintenance, onboarding, and modernization. While large language models (LLMs)
have shown promise in generating code explanations, they often lack grounding
in the broader software engineering context. We propose a novel approach that
leverages natural language artifacts from GitHub -- such as pull request
descriptions, issue descriptions and discussions, and commit messages -- to
enhance LLM-based code understanding. Our system consists of three components:
one that extracts and structures relevant GitHub context, another that uses
this context to generate high-level explanations of the code's purpose, and a
third that validates the explanation. We implemented this as a standalone tool,
as well as a server within the Model Context Protocol (MCP), enabling
integration with other AI-assisted development tools. Our main use case is that
of enhancing a standard LLM-based code explanation with code insights that our
system generates. To evaluate explanations' quality, we conducted a small scale
user study, with developers of several open projects, as well as developers of
proprietary projects. Our user study indicates that when insights are generated
they often are helpful and non trivial, and are free from hallucinations.

</details>


### [29] [The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents](https://arxiv.org/abs/2511.03690)
*Xingyao Wang,Simon Rosenberg,Juan Michelini,Calvin Smith,Hoang Tran,Engel Nyst,Rohit Malhotra,Xuhui Zhou,Valerie Chen,Robert Brennan,Graham Neubig*

Main category: cs.SE

TL;DR: OpenHands Software Agent SDK是一个用于构建软件工程代理的完整工具包，提供灵活性、安全性和用户交互能力，支持从简单到复杂的代理实现。


<details>
  <summary>Details</summary>
Motivation: 构建生产级软件工程代理面临复杂性挑战，需要灵活实现、可靠安全执行和用户交互接口。

Method: 重新设计OpenHands框架的代理组件，提供简单接口实现代理，支持自定义工具、内存管理等功能，具备本地到远程执行可移植性、REST/WebSocket服务和多种接口连接能力。

Result: 在SWE-Bench Verified和GAIA基准测试中表现出色，提供原生沙箱执行、生命周期控制、模型无关的多LLM路由和内置安全分析。

Conclusion: OpenHands Software Agent SDK为原型设计、开发新型自定义应用和大规模可靠部署代理提供了实用基础。

Abstract: Agents are now used widely in the process of software development, but
building production-ready software engineering agents is a complex task.
Deploying software agents effectively requires flexibility in implementation
and experimentation, reliable and secure execution, and interfaces for users to
interact with agents. In this paper, we present the OpenHands Software Agent
SDK, a toolkit for implementing software development agents that satisfy these
desiderata. This toolkit is a complete architectural redesign of the agent
components of the popular OpenHands framework for software development agents,
which has 64k+ GitHub stars. To achieve flexibility, we design a simple
interface for implementing agents that requires only a few lines of code in the
default case, but is easily extensible to more complex, full-featured agents
with features such as custom tools, memory management, and more. For security
and reliability, it delivers seamless local-to-remote execution portability,
integrated REST/WebSocket services. For interaction with human users, it can
connect directly to a variety of interfaces, such as visual workspaces (VS
Code, VNC, browser), command-line interfaces, and APIs. Compared with existing
SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native
sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and
built-in security analysis. Empirical results on SWE-Bench Verified and GAIA
benchmarks demonstrate strong performance. Put together, these elements allow
the OpenHands Software Agent SDK to provide a practical foundation for
prototyping, unlocking new classes of custom applications, and reliably
deploying agents at scale.

</details>
