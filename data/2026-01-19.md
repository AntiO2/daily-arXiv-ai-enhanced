<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [AFLL: Real-time Load Stabilization for MMO Game Servers Based on Circular Causality Learning](https://arxiv.org/abs/2601.10998)
*Shinsuk Kang,Youngjae Kim*

Main category: cs.DC

TL;DR: AFLL系统通过实时学习服务器消息与客户端请求的因果关系，实现预测性限流，在MMO游戏中显著降低CPU时间和线程竞争，同时保证关键消息传递。


<details>
  <summary>Details</summary>
Motivation: 传统MMO服务器负载管理方法存在两个问题：要么无差别限流所有消息类型（损害游戏体验），要么使用固定的启发式规则（无法适应动态工作负载）。需要一种能够自适应动态负载、区分消息优先级的方法。

Method: AFLL（自适应反馈循环学习）系统实时学习服务器输出消息与后续客户端输入请求之间的因果关系。使用反向传播算法持续调整消息类型权重，实现预测性限流，在过载发生前阻止低优先级消息，同时保证关键消息的传递。

Result: 在1000名并发玩家的控制实验中，AFLL将平均CPU时间降低48.3%（13.2ms到6.8ms），峰值CPU时间降低51.7%（54.0ms到26.1ms），线程竞争降低64.4%（19.6%到7.0%）。通过后台计算和缓存优化实现零学习开销，所有指标的可重复性变异系数小于2%。

Conclusion: AFLL证明了循环因果关系学习能够为延迟关键系统提供实用的实时自适应能力，识别了消息阻塞与负载减少之间的三阶段因果链，为MMO服务器负载管理提供了有效的解决方案。

Abstract: Massively Multiplayer Online (MMO) game servers must handle thousands of simultaneous players while maintaining sub-100ms response times. When server load exceeds capacity, traditional approaches either uniformly throttle all message types regardless of importance (damaging gameplay) or apply fixed heuristic rules that fail to adapt to dynamic workloads. This paper presents AFLL (Adaptive Feedback Loop Learning), a real-time load stabilization system that learns the causal relationship between outgoing server messages and subsequent incoming client requests. AFLL employs backpropagation to continuously adjust message type weights, enabling predictive throttling that blocks low-priority messages before overload occurs while guaranteeing critical message delivery. Through controlled experiments with 1,000 concurrent players, AFLL reduced average CPU time by 48.3% (13.2ms to 6.8ms), peak CPU time by 51.7% (54.0ms to 26.1ms), and thread contention by 64.4% (19.6% to 7.0%), while maintaining zero learning overhead through background computation and caching optimizations. The system achieved remarkable reproducibility (CV < 2% across all metrics) and identified a three-stage causal chain linking message blocking to load reduction. AFLL demonstrates that circular causality learning enables practical real-time adaptation for latency-critical systems.

</details>


### [2] [Konflux: Optimized Function Fusion for Serverless Applications](https://arxiv.org/abs/2601.11156)
*Niklas Kowallik,Trever Schirmer,David Bermbach*

Main category: cs.DC

TL;DR: 提出一个系统，通过模拟FaaS平台来分析所有可能的函数融合配置，避免在生产环境中进行昂贵的暴力测试，发现只有少数融合配置是最优解且受定价模型影响


<details>
  <summary>Details</summary>
Motivation: FaaS已成为无服务器云计算的核心范式，但优化FaaS部署仍然具有挑战性。函数融合可以将多个函数组合成单个部署单元，从而降低复杂服务器无应用程序的成本和延迟。然而，即使在小规模应用中，可能的融合配置数量也非常庞大，使得在生产环境中进行暴力基准测试既昂贵又耗时。

Method: 提出一个系统，通过模拟FaaS平台来分析所有可能的函数融合配置。该系统支持本地实验，无需重新配置实时平台，显著降低了相关成本和时间。在多个示例FaaS应用和资源限制下评估所有融合配置。

Result: 研究结果表明，在分析成本和延迟权衡时，只有有限的融合配置代表最优解决方案，这些最优配置受到特定定价模型的强烈影响。

Conclusion: 通过模拟FaaS平台进行本地实验的系统能够有效分析所有可能的函数融合配置，避免了昂贵的生产环境测试，并揭示了定价模型对最优融合配置选择的重要影响。

Abstract: Function-as-a-Service (FaaS) has become a central paradigm in serverless cloud computing, yet optimizing FaaS deployments remains challenging. Using function fusion, multiple functions can be combined into a single deployment unit, which can be used to reduce cost and latency of complex serverless applications comprising multiple functions. Even in small-scale applications, the number of possible fusion configurations is vast, making brute-force benchmarking in production both cost- and time-prohibitive.
  In this paper, we present a system that can analyze every possible fusion setup of complex applications. By emulating the FaaS platform, our system enables local experimentation, eliminating the need to reconfigure the live platform and significantly reducing associated cost and time. We evaluate all fusion configurations across a number of example FaaS applications and resource limits. Our results reveal that, when analyzing cost and latency trade-offs, only a limited set of fusion configurations represent optimal solutions, which are strongly influenced by the specific pricing model in use.

</details>


### [3] [Space-Optimal, Computation-Optimal, Topology-Agnostic, Throughput-Scalable Causal Delivery through Hybrid Buffering](https://arxiv.org/abs/2601.11487)
*Paulo Sérgio Almeida*

Main category: cs.DC

TL;DR: 提出一种新的混合发送者-接收者缓冲算法，在保持恒定元数据开销的同时实现因果有序消息传递


<details>
  <summary>Details</summary>
Motivation: 现有因果有序消息传递方法存在元数据开销过大或吞吐量可扩展性问题。传统发送者缓冲方法有太多缺点，而接收者缓冲方法在大规模系统中元数据开销过高。

Method: 提出SPS+FIFO策略，结合发送者缓冲（执行SPS）和接收者缓冲（执行FIFO）的混合方法。采用精心设计的数据结构实现计算最优和恒定元数据开销。

Result: 新算法克服了纯发送者缓冲的限制，实现每消息恒定元数据大小，计算开销摊销后为常数，且无拓扑限制。

Conclusion: 该算法是首个拓扑无关的因果传递算法，兼具恒定元数据开销和计算最优特性，解决了现有方法的可扩展性问题。

Abstract: Message delivery respecting causal ordering (causal delivery) is one of the most classic and widely useful abstraction for inter-process communication in a distributed system. Most approaches tag messages with causality information and buffer them at the receiver until they can be safely delivered. Except for specific approaches that exploit communication topology, therefore not generally applicable, they incur a metadata overhead which is prohibitive for a large number of processes. Much less used are the approaches that enforce causal order by buffering messages at the sender, until it is safe to release them to the network, as the classic algorithm has too many drawbacks. In this paper, first we discuss the limitations of sender-only buffering approaches and introduce the Sender Permission to Send (SPS) enforcement strategy, showing that SPS + FIFO implies Causal. We analyze a recent sender-buffering algorithm, Cykas, which follows SPS + FIFO, albeit very conservatively, pointing out throughput scalability and liveness issues. Then, we introduce a novel SPS + FIFO based algorithm, which adopts a new hybrid approach: enforcing causality by combining sender-buffering to enforce SPS and receiver-buffering to enforce FIFO. The algorithm overcomes limitations of sender-only buffering, and achieves effectively constant metadata size per message. By a careful choice of data-structures, the algorithm is also computationally-optimal, with amortized effectively constant processing overhead. As far as we know, there is no other topology-agnostic causal delivery algorithm with these properties.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [LogicLens: Leveraging Semantic Code Graph to explore Multi Repository large systems](https://arxiv.org/abs/2601.10773)
*Niko Usai,Dario Montagnini,Kristian Ilianov Iliev,Raffaele Camanzo*

Main category: cs.SE

TL;DR: LogicLens是一个基于语义多仓库图的对话式智能体，帮助开发者通过自然语言探索复杂软件系统，结合代码分析和LLM语义增强。


<details>
  <summary>Details</summary>
Motivation: 理解大型软件系统具有挑战性，特别是当代码分布在多个仓库和微服务中时。开发者不仅需要理解代码结构，还需要理解隐含且分散的领域逻辑和运行时行为。

Method: 通过预处理构建语义多仓库图，结合AST解析和仓库遍历的语法代码分析，以及使用大型语言模型进行语义增强。该图捕获结构元素和功能抽象，然后通过自然语言交互动态检索相关子图。

Result: 在真实世界的多仓库场景中评估了系统有效性，展示了从语义图结构中自然产生的涌现能力，包括影响分析和基于症状的调试。

Conclusion: LogicLens通过语义图增强的对话式交互，为理解复杂软件系统提供了有效方法，能够支持技术或功能查询，并展现出有价值的涌现能力。

Abstract: Understanding large software systems is a challenging task, especially when code is distributed across multiple repositories and microservices. Developers often need to reason not only about the structure of the code, but also about its domain logic and runtime behaviors, which are typically implicit and scattered. We introduce LogicLens, a reactive conversational agent that assists developers in exploring complex software systems through a semantic multi-repository graph. This graph is built in a preprocessing step by combining syntactic code analysis, via AST parsing and repository traversal, with semantic enrichment using Large Language Models (LLMs). The resulting graph captures both structural elements, such as files, classes, and functions, as well as functional abstractions like domain entities, operations, and workflows. Once the graph is constructed, LogicLens enables developers to interact with it via natural language, dynamically retrieving relevant subgraphs and answering technical or functional queries. We present the architecture of the system, discuss emergent behaviors, and evaluate its effectiveness on real-world multi-repository scenarios. We demonstrate emergent capabilities including impact analysis and symptom-based debugging that arise naturally from the semantic graph structure.

</details>


### [5] [Multi-Artifact Analysis of Self-Admitted Technical Debt in Scientific Software](https://arxiv.org/abs/2601.10850)
*Eric L. Melin,Nasir U. Eisty,Gregory Watson,Addi Malviya-Thakur*

Main category: cs.SE

TL;DR: 该研究首次系统性地识别和分析了科学软件中的"科学债务"——一种特殊形式的自认技术债务，并开发了多源SATD分类器来检测这种领域特定的技术债务。


<details>
  <summary>Details</summary>
Motivation: 科学软件中的自认技术债务对研究结果的有效性和可重复性构成独特风险，传统SATD分类可能无法充分捕捉这些领域特定问题，因此需要专门研究科学债务。

Method: 对23个开源科学软件项目进行多工件分析，涵盖代码注释、提交信息、拉取请求和问题跟踪器；构建并验证科学债务数据集，开发多源SATD分类器，并进行从业者验证评估实用性。

Result: 分类器在23个项目900,358个工件上表现优异；SATD在拉取请求和问题跟踪器中最为普遍；基于传统SATD训练的模型常遗漏科学债务；从业者验证确认科学债务具有实际识别价值。

Conclusion: 科学债务是科学软件中独特的SATD形式，传统分类无法充分捕捉，需要专门的识别和管理方法。研究提供了首个正式的多工件视角，强调在科学软件中需要定制化的SATD检测方法。

Abstract: Context: Self-admitted technical debt (SATD) occurs when developers acknowledge shortcuts in code. In scientific software (SSW), such debt poses unique risks to the validity and reproducibility of results. Objective: This study aims to identify, categorize, and evaluate scientific debt, a specialized form of SATD in SSW, and assess the extent to which traditional SATD categories capture these domain-specific issues. Method: We conduct a multi-artifact analysis across code comments, commit messages, pull requests, and issue trackers from 23 open-source SSW projects. We construct and validate a curated dataset of scientific debt, develop a multi-source SATD classifier, and conduct a practitioner validation to assess the practical relevance of scientific debt. Results: Our classifier performs strongly across 900,358 artifacts from 23 SSW projects. SATD is most prevalent in pull requests and issue trackers, underscoring the value of multi-artifact analysis. Models trained on traditional SATD often miss scientific debt, emphasizing the need for its explicit detection in SSW. Practitioner validation confirmed that scientific debt is both recognizable and useful in practice. Conclusions: Scientific debt represents a unique form of SATD in SSW that that is not adequately captured by traditional categories and requires specialized identification and management. Our dataset, classification analysis, and practitioner validation results provide the first formal multi-artifact perspective on scientific debt, highlighting the need for tailored SATD detection approaches in SSW.

</details>


### [6] [Struggling to Connect: A Researchers' Reflection on Networking in Software Engineering](https://arxiv.org/abs/2601.10907)
*Shalini Chakraborty*

Main category: cs.SE

TL;DR: 该论文探讨了软件工程研究中网络建设的不平等问题，分析了国家、移民身份、语言、性别等因素如何影响研究者建立专业联系，并提出了社区驱动的"专家声音"倡议来解决这些不平等。


<details>
  <summary>Details</summary>
Motivation: 网络建设对软件工程研究的发展和研究者知名度至关重要，但建立网络的机会和能力分布不均，且常被视为个人技能，而忽视了工作场所、文化和环境对研究者动机和网络形成的显著影响。

Method: 基于现有文献和个人经验的反思性报告，探讨了国家居住地、移民身份、语言、性别和周围环境等因素如何影响研究者在全球研究生态系统中建立专业联系和取得成功的能力。

Result: 识别了网络建设中常被忽视的障碍，包括结构性不平等和系统性偏见，这些障碍限制了某些群体参与全球研究网络的机会。

Conclusion: 倡导社区驱动的"专家声音"倡议，以承认和解决网络建设中的不平等问题，促进更包容和公平的研究生态系统。

Abstract: Networking is central to the growth and visibility of software engineering research and researchers. However, opportunities and capacities to build such networks are not easily identified and often are unevenly distributed. While networking is often viewed as an individual skill, a researchers workplace, culture and environment significantly influence their motivation and, consequently, the networks they form. This paper explores how factors such as country of residence, immigration status, language, gender, and surrounding context affect researchers' ability to establish professional connections and succeed within the global research ecosystem. Drawing on existing literature and personal experience, this reflective report examines the often-invisible barriers to networking and advocates for a community-driven "expert voice" initiative to acknowledge and address these inequities.

</details>


### [7] [Change And Cover: Last-Mile, Pull Request-Based Regression Test Augmentation](https://arxiv.org/abs/2601.10942)
*Zitong Zhou,Matteo Paltenghi,Miryung Kim,Michael Pradel*

Main category: cs.SE

TL;DR: ChaCo：基于LLM的测试增强技术，专门针对PR中未覆盖的代码行生成测试，填补"最后一英里"回归测试空白


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，即使有完善的测试套件，PR修改的代码行仍可能存在未覆盖的情况，形成"最后一英里"回归测试空白。现有测试生成器通常关注整体覆盖率，而非专门针对PR中的未覆盖行。

Method: ChaCo采用基于LLM的测试增强方法：1) 考虑PR特定的补丁覆盖率；2) 提取相关测试上下文（现有测试函数、夹具、数据生成器）；3) 将生成的测试与现有测试套件集成，保持结构和风格一致，并生成测试添加摘要供开发者审查。

Result: 在三个复杂开源项目（SciPy、Qiskit、Pandas）的145个PR上评估：30%的PR实现了完整补丁覆盖率，成本仅0.11美元。人工评审评分：测试值得添加（4.53/5.0）、集成良好（4.2/5.0）、与PR相关（4.7/5.0）。提交的12个测试中8个已被合并，发现并修复了2个未知bug。

Conclusion: ChaCo能有效填补PR中的测试覆盖空白，具有实用性和成本效益。测试上下文对上下文感知测试生成至关重要，能带来2倍覆盖率提升。该方法可集成到CI工作流中，自动化最后一英里回归测试增强。

Abstract: Software is in constant evolution, with developers frequently submitting pull requests (PRs) to introduce new features or fix bugs. Testing PRs is critical to maintaining software quality. Yet, even in projects with extensive test suites, some PR-modified lines remain untested, leaving a "last-mile" regression test gap. Existing test generators typically aim to improve overall coverage, but do not specifically target the uncovered lines in PRs. We present Change And Cover (ChaCo), an LLM-based test augmentation technique that addresses this gap. It makes three contributions: (i) ChaCo considers the PR-specific patch coverage, offering developers augmented tests for code just when it is on the developers' mind. (ii) We identify providing suitable test context as a crucial challenge for an LLM to generate useful tests, and present two techniques to extract relevant test content, such as existing test functions, fixtures, and data generators. (iii) To make augmented tests acceptable for developers, ChaCo carefully integrates them into the existing test suite, e.g., by matching the test's structure and style with the existing tests, and generates a summary of the test addition for developer review. We evaluate ChaCo on 145 PRs from three popular and complex open-source projects - SciPy, Qiskit, and Pandas. The approach successfully helps 30% of PRs achieve full patch coverage, at the cost of $0.11, showing its effectiveness and practicality. Human reviewers find the tests to be worth adding (4.53/5.0), well integrated (4.2/5.0), and relevant to the PR (4.7/5.0). Ablations show test context is crucial for context-aware test generation, leading to 2x coverage. We submitted 12 tests, of which 8 have already been merged, and two previously unknown bugs were exposed and fixed. We envision our approach to be integrated into CI workflows, automating the last mile of regression test augmentation.

</details>


### [8] [ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development](https://arxiv.org/abs/2601.11077)
*Jie Yang,Honglin Guo,Li Ji,Jiazheng Zhou,Rui Zheng,Zhikai Lei,Shuo Zhang,Zhiheng Xi,Shichun Liu,Yuxin Wang,Bo Wang,Yining Zheng,Tao Gui,Xipeng Qiu*

Main category: cs.SE

TL;DR: ABC-Bench是一个专门评估智能体后端编码能力的基准测试，要求AI代理在真实可执行的工作流中完成从仓库探索到容器化服务部署的完整开发生命周期。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试主要评估静态上下文中的代码逻辑，忽略了现实世界工程（特别是后端开发）所需的动态、全过程要求，包括环境配置和服务部署等复杂任务。

Method: 通过可扩展的自动化流水线，从开源仓库中收集了224个实际任务，涵盖8种编程语言和19个框架，要求智能体管理从仓库探索到实例化容器化服务的完整开发生命周期，并通过端到端API测试。

Result: 评估显示，即使是当前最先进的模型在这些整体性任务上也难以提供可靠的性能，表明当前模型能力与实际后端工程需求之间存在显著差距。

Conclusion: ABC-Bench填补了现有基准测试的空白，为评估智能体后端编码能力提供了更真实、全面的测试框架，揭示了AI代理在实际工程应用中的局限性。

Abstract: The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.

</details>


### [9] [Patterns of Bot Participation and Emotional Influence in Open-Source Development](https://arxiv.org/abs/2601.11138)
*Matteo Vaccargiu,Riccardo Lai,Maria Ilaria Lunesu,Andrea Pinna,Giuseppe Destefanis*

Main category: cs.SE

TL;DR: 研究以太坊生态系统中机器人如何参与开源讨论及其对开发者情感语调的影响。发现少量机器人（0.28%）能改变开发者沟通的时间和情感动态。


<details>
  <summary>Details</summary>
Motivation: 研究开源社区中机器人的角色，特别是它们如何影响开发者讨论的情感动态。了解自动化工具在技术协作中的社会影响。

Method: 分析10个代码库的36,875个账户（含105个验证机器人）。使用27个情感类别的模型分析情感变化，比较机器人与人类在拉取请求和问题讨论中的参与模式和时间差异。

Result: 人类参与呈U型模式，机器人参与模式不同（拉取请求中均匀参与，问题讨论中后期参与）。机器人响应更快但维护角色较慢。机器人更中立，但它们的介入导致人类评论中立性降低，情感转向感激、钦佩、乐观，远离困惑。

Conclusion: 即使少量机器人也能显著改变开发者沟通的时间和情感动态，表明自动化工具在开源协作中具有重要的社会影响。

Abstract: We study how bots contribute to open-source discussions in the Ethereum ecosystem and whether they influence developers' emotional tone. Our dataset covers 36,875 accounts across ten repositories with 105 validated bots (0.28%). Human participation follows a U-shaped pattern, while bots engage in uniform (pull requests) or late-stage (issues) activity. Bots respond faster than humans in pull requests but play slower maintenance roles in issues. Using a model trained on 27 emotion categories, we find bots are more neutral, yet their interventions are followed by reduced neutrality in human comments, with shifts toward gratitude, admiration, and optimism and away from confusion. These findings indicate that even a small number of bots are associated with changes in both timing and emotional dynamics of developer communication.

</details>


### [10] [Automation and Reuse Practices in GitHub Actions Workflows: A Practitioner's Perspective](https://arxiv.org/abs/2601.11299)
*Hassan Onsori Delicheh,Guillaume Cardoen,Alexandre Decan,Tom Mens*

Main category: cs.SE

TL;DR: 调查419名GitHub Actions实践者，发现自动化主要聚焦CI/CD核心任务，安全分析和性能监控不足；重用Actions普遍但可重用工作流采用较少；存在版本维护挑战，复制粘贴仍常见以获得更多控制权。


<details>
  <summary>Details</summary>
Motivation: GitHub Actions原生支持工作流自动化，但工作流维护对开发者来说是负担，缺乏关于工作流实践者自动化与重用偏好的知识。需要了解实践者的开发实践和挑战，以识别支持工作流维护的机会。

Method: 对419名GitHub Actions实践者进行调查，研究他们自动化哪些任务、偏好的工作流创建机制、优先考虑的非功能性特征，以及工作流重用机制的实践和挑战。

Result: 自动化主要集中于核心CI/CD任务，安全分析和性能监控等关键领域关注不足；实践者强烈依赖可重用Actions，但可重用工作流采用频率较低；存在Action版本管理和维护挑战；复制粘贴仍是常见做法以获得更多控制权并避免依赖可重用组件的复杂性。

Conclusion: 需要改进工具支持、增强广泛自动化任务的支持，以及更好的可重用工作流组件发现、管理和信任机制。工作流维护需要更全面的自动化覆盖和更好的重用实践支持。

Abstract: GitHub natively supports workflow automation through GitHub Actions. Yet, workflow maintenance is often considered a burden for software developers, who frequently face difficulties in writing, testing, debugging, and maintaining workflows. Little knowledge exists concerning the automation and reuse practices favoured by workflow practitioners. We therefore surveyed 419 practitioners to elucidate good and bad workflow development practices and to identify opportunities for supporting workflow maintenance. Specifically, we investigate the tasks that practitioners tend to automate using GitHub Actions, their preferred workflow creation mechanisms, and the non-functional characteristics they prioritise. We also examine the practices and challenges associated with GitHub's workflow reuse mechanisms. We observe a tendency to focus automation efforts on core CI/CD tasks, with less emphasis on crucial areas like security analysis and performance monitoring. Practitioners strongly rely on reusable Actions, but reusable workflows see less frequent adoption. Furthermore, we observed challenges with Action versioning and maintenance. Copy-pasting remains a common practice to have more control and avoid the complexity of depending on reusable components. These insights suggest the need for improved tooling, enhanced support for a wide range of automation tasks, and better mechanisms for discovering, managing, and trusting reusable workflow components.

</details>


### [11] [RITA: A Tool for Automated Requirements Classification and Specification from Online User Feedback](https://arxiv.org/abs/2601.11362)
*Manjeshwar Aniruddh Mallya,Alessio Ferrari,Mohammad Amin Zadenoori,Jacek Dąbrowski*

Main category: cs.SE

TL;DR: RITA是一个集成轻量级开源大语言模型的工具，通过统一工作流将在线用户反馈转化为需求制品，支持自动请求分类、非功能性需求识别和自然语言需求规范生成，并与Jira集成。


<details>
  <summary>Details</summary>
Motivation: 在线用户反馈是需求工程的重要资源，但存在量大、噪声多的问题。现有工具仅支持单个反馈分析任务，缺乏端到端集成，限制了实际应用和真实效果评估。

Method: 开发RITA工具，集成轻量级开源大语言模型，构建统一工作流。支持自动请求分类、非功能性需求识别、自然语言需求规范生成，提供用户友好界面，并与Jira集成实现需求规范到开发工具的无缝转移。

Result: RITA利用经过评估的LLM技术，能够高效地将原始用户反馈转化为需求制品，有助于弥合研究与实践之间的差距。

Conclusion: RITA通过集成LLM技术提供端到端的反馈驱动需求工程支持，解决了现有工具集成不足的问题，促进了研究向实践的转化。

Abstract: Context and motivation. Online user feedback is a valuable resource for requirements engineering, but its volume and noise make analysis difficult. Existing tools support individual feedback analysis tasks, but their capabilities are rarely integrated into end-to-end support. Problem. The lack of end-to-end integration limits the practical adoption of existing RE tools and makes it difficult to assess their real-world usefulness. Solution. To address this challenge, we present RITA, a tool that integrates lightweight open-source large language models into a unified workflow for feedback-driven RE. RITA supports automated request classification, non-functional requirement identification, and natural-language requirements specification generation from online feedback via a user-friendly interface, and integrates with Jira for seamless transfer of requirements specifications to development tools. Results and conclusions. RITA exploits previously evaluated LLM-based RE techniques to efficiently transform raw user feedback into requirements artefacts, helping bridge the gap between research and practice. A demonstration is available at: https://youtu.be/8meCLpwQWV8.

</details>


### [12] [A Practical Guide to Establishing Technical Debt Management](https://arxiv.org/abs/2601.11430)
*Marion Wiese*

Main category: cs.SE

TL;DR: 该白皮书基于博士研究成果，为团队提供技术债务管理的实用指南，区分"最佳实践"和"锦上添花"两类方法，强调团队自主决策而非僵化框架。


<details>
  <summary>Details</summary>
Motivation: 将学术研究成果转化为实际可用的指导，帮助团队建立适合自身需求的技术债务管理系统，填补理论与实践之间的鸿沟。

Method: 通过与研究人员合作，支持三家不同公司的团队建立定制化的技术债务管理系统，收集实践经验，区分"最佳实践"（所有团队都采用的方法）和"锦上添花"（至少一个团队使用的方法）。

Result: 开发出团队技术债务管理指南，提供方向性指导而非僵化框架，强调团队共同决策，并包含向全公司范围扩展的建议。

Conclusion: 技术债务管理需要根据团队具体情况定制，指南应作为参考而非规定，团队自主决策是成功实施的关键，同时为更大范围的组织级实施提供了基础。

Abstract: This white paper provides an overview of the topic of "technical debt" and presents an approach for managing technical debt in teams. The white paper is based on the results of my dissertation, which aimed to translate scientific findings into practical guidance. To this end, I collaborated with other researchers to support three teams from different companies in adapting and establishing a technical debt management system tailored to their specific needs. Research findings were supplemented with details or additional approaches. Research results that were less practical were discarded. The result is a guide on establishing technical debt management within a team. The guide is intended to provide orientation and not be a rigid framework. We distinguish between "best practices" and "nice-to-haves." "Best practices" are understood to be all approaches that were adopted by all three teams. "Nice-to-haves" were used by at least one team. In many places, it is explicitly mentioned that the team should decide together how to design the process. This also applies, of course, to all areas where this was not explicitly mentioned. This white paper explicitly does not cover the establishment of technical debt management across the entire company, but provides suggestions for this at the end.

</details>
