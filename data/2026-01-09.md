<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Sphinx: Benchmarking and Modeling for LLM-Driven Pull Request Review](https://arxiv.org/abs/2601.04252)
*Daoan Zhang,Shuo Zhang,Zijian Jin,Jiebo Luo,Shengyu Fu,Elsie Nallipogu*

Main category: cs.SE

TL;DR: SPHINX是一个基于LLM的PR代码审查统一框架，通过结构化数据生成、清单式评估基准和清单奖励策略优化，解决了现有自动化代码审查的三大挑战，在审查完整性和精确性上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 自动化PR代码审查面临三大挑战：噪声监督、有限的上下文理解、不充分的评估指标。现有方法无法有效处理这些限制，需要更系统化的解决方案。

Method: SPHINX包含三个核心组件：1) 结构化数据生成管道，通过比较伪修改代码和合并代码生成上下文丰富的审查评论；2) 基于清单的评估基准，通过结构化可操作验证点评估审查质量；3) 清单奖励策略优化(CRPO)，使用基于规则的、可解释的奖励来对齐模型行为与实际审查实践。

Result: 实验表明，使用SPHINX训练的模型在审查完整性和精确性上达到最先进性能，在清单覆盖率上比专有和开源基线高出40%。模型不仅流畅，而且具有上下文感知能力、技术精确性，并能实际部署到开发工作流中。

Conclusion: SPHINX框架能够开发出既流畅又具有上下文感知能力、技术精确性且可实际部署的PR审查模型，解决了自动化代码审查的关键挑战，数据将在审查后发布。

Abstract: Pull request (PR) review is essential for ensuring software quality, yet automating this task remains challenging due to noisy supervision, limited contextual understanding, and inadequate evaluation metrics. We present Sphinx, a unified framework for LLM-based PR review that addresses these limitations through three key components: (1) a structured data generation pipeline that produces context-rich, semantically grounded review comments by comparing pseudo-modified and merged code; (2) a checklist-based evaluation benchmark that assesses review quality based on structured coverage of actionable verification points, moving beyond surface-level metrics like BLEU; and (3) Checklist Reward Policy Optimization (CRPO), a novel training paradigm that uses rule-based, interpretable rewards to align model behavior with real-world review practices. Extensive experiments show that models trained with Sphinx achieve state-of-the-art performance on review completeness and precision, outperforming both proprietary and open-source baselines by up to 40\% in checklist coverage. Together, Sphinx enables the development of PR review models that are not only fluent but also context-aware, technically precise, and practically deployable in real-world development workflows. The data will be released after review.

</details>


### [2] [A Systematic Mapping Study on the Debugging of Autonomous Driving Systems](https://arxiv.org/abs/2601.04293)
*Nathan Shaw,Sanjeetha Pennada,Robert M Hierons,Donghwan Shin*

Main category: cs.SE

TL;DR: 这篇系统映射研究综述了自动驾驶系统调试的现状，发现该领域研究分散但前景广阔，提出了未来研究方向与术语标准化建议。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶系统向商业化部署发展，确保其安全可靠性日益重要。虽然已有大量测试方法研究用于检测故障，但调试过程（定位和修复故障）却很少受到关注，而调试对于维持系统安全可靠性至关重要。

Method: 采用系统映射研究方法，对自动驾驶系统调试领域的现有研究进行全面梳理和分析。

Result: 研究发现自动驾驶系统调试存在多种方法，当前研究格局呈现分散但前景广阔的特点。

Conclusion: 研究揭示了自动驾驶系统调试的现状，提出了未来研究方向，并建议建立问题定义和术语标准化框架。

Abstract: As Autonomous Driving Systems (ADS) progress towards commercial deployment, there is an increasing focus on ensuring their safety and reliability. While considerable research has been conducted on testing methods for detecting faults in ADS, very little attention has been paid to debugging in ADS. Debugging is an essential process that follows test failures to localise and repair the faults in the systems to maintain their safety and reliability. This Systematic Mapping Study (SMS) aims to provide a detailed overview of the current landscape of ADS debugging, highlighting existing approaches and identifying gaps in research. The study also proposes directions for future work and standards for problem definition and terminology in the field. Our findings reveal various methods for ADS debugging and highlight the current fragmented yet promising landscape.

</details>


### [3] [Advancing Language Models for Code-related Tasks](https://arxiv.org/abs/2601.04526)
*Zhao Tian*

Main category: cs.SE

TL;DR: 该研究通过三个互补方向系统解决语言模型在复杂编程场景中的限制：改进代码数据质量、增强模型架构、提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在复杂编程场景中仍存在困难，主要受限于数据质量、模型架构和推理能力。需要系统性地解决这些挑战，以促进语言模型在软件开发中的实际应用。

Method: 采用三个互补方向：1) 通过代码差异引导的对抗增强技术(CODA)和代码去噪技术(CodeDenoise)改进代码数据质量；2) 通过语法引导的代码语言模型(LEAM和LEAM++)增强模型架构；3) 通过提示技术(muFiX)和基于代理的技术(Specine)提升模型推理能力。

Result: 研究提出了一系列创新技术，包括数据增强、模型架构优化和推理方法改进，旨在提升语言模型在软件工程任务中的性能。

Conclusion: 该研究通过系统性的多方向方法，有望促进语言模型在软件开发中的实际应用，并进一步推动智能软件工程的发展。

Abstract: Recent advances in language models (LMs) have driven significant progress in various software engineering tasks. However, existing LMs still struggle with complex programming scenarios due to limitations in data quality, model architecture, and reasoning capability. This research systematically addresses these challenges through three complementary directions: (1) improving code data quality with a code difference-guided adversarial augmentation technique (CODA) and a code denoising technique (CodeDenoise); (2) enhancing model architecture via syntax-guided code LMs (LEAM and LEAM++); and (3) advancing model reasoning with a prompting technique (muFiX) and an agent-based technique (Specine). These techniques aim to promote the practical adoption of LMs in software development and further advance intelligent software engineering.

</details>


### [4] [AdaptEval: A Benchmark for Evaluating Large Language Models on Code Snippet Adaptation](https://arxiv.org/abs/2601.04540)
*Tanghaoran Zhang,Xinjun Mao,Shangwen Wang,Yuxin Zhao,Yao Lu,Jin Zhang,Zhang Zhang,Kang Yang,Yue Yu*

Main category: cs.SE

TL;DR: 提出了AdaptEval基准测试，用于评估大语言模型在代码片段适应任务上的能力，填补了该领域评估空白。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在软件工程任务中应用广泛，但在代码重用中的关键活动——代码适应方面，缺乏专门的评估基准，导致其实际效用不明确。

Method: AdaptEval基准具有三个特点：1) 基于Stack Overflow和GitHub开发者实践的实际上下文；2) 任务级和适应级的多粒度标注；3) 包含适应级和函数级测试的两层评估框架。

Result: 评估了6个指令调优LLM和3个推理LLM，发现AdaptEval能从多角度评估模型适应能力，并揭示其当前局限性，特别是难以遵循明确指令的问题。

Conclusion: AdaptEval填补了代码适应评估的空白，为LLM在该领域的能力研究和实际应用提供了重要支持。

Abstract: Recent advancements in large language models (LLMs) have automated various software engineering tasks, with benchmarks emerging to evaluate their capabilities. However, for adaptation, a critical activity during code reuse, there is no benchmark to assess LLMs' performance, leaving their practical utility in this area unclear. To fill this gap, we propose AdaptEval, a benchmark designed to evaluate LLMs on code snippet adaptation. Unlike existing benchmarks, AdaptEval incorporates the following three distinctive features: First, Practical Context. Tasks in AdaptEval are derived from developers' practices, preserving rich contextual information from Stack Overflow and GitHub communities. Second, Multi-granularity Annotation. Each task is annotated with requirements at both task and adaptation levels, supporting the evaluation of LLMs across diverse adaptation scenarios. Third, Fine-grained Evaluation. AdaptEval includes a two-tier testing framework combining adaptation-level and function-level tests, which enables evaluating LLMs' performance across various individual adaptations. Based on AdaptEval, we conduct the first empirical study to evaluate six instruction-tuned LLMs and especially three reasoning LLMs on code snippet adaptation. Experimental results demonstrate that AdaptEval enables the assessment of LLMs' adaptation capabilities from various perspectives. It also provides critical insights into their current limitations, particularly their struggle to follow explicit instructions. We hope AdaptEval can facilitate further investigation and enhancement of LLMs' capabilities in code snippet adaptation, supporting their real-world applications.

</details>


### [5] [4D-ARE: Bridging the Attribution Gap in LLM Agent Requirements Engineering](https://arxiv.org/abs/2601.04556)
*Bo Yu,Lei Zhao*

Main category: cs.SE

TL;DR: 论文提出4D-ARE方法，用于在AI代理设计阶段系统化地指定需要推理的内容，弥补现有运行时推理框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理运行时推理框架（如ReAct、Chain-of-Thought）关注"如何推理"，但缺乏对"推理什么"的设计时规范。作者发现即使代理能完美执行任务，当被问及"为什么完成率是80%"时，它只返回指标而非因果解释，这反映了代理需要明确的领域知识规范。

Method: 提出4D-ARE（四维归因驱动代理需求工程）方法，核心洞察是决策者需要归因而非简单答案。归因问题组织成四个维度（结果→过程→支持→长期），基于Pearl因果层次理论。框架通过五层操作化，产生可直接编译为系统提示的工件。

Result: 在金融服务行业进行了试点部署，展示了该方法的可行性。4D-ARE解决了代理应该推理什么的问题，补充了现有运行时框架对如何推理的关注。

Conclusion: 系统化的规范设计能放大基础技术进步的力量。本文提出了方法论建议和初步工业验证，严格的实证评估计划在未来工作中进行。

Abstract: We deployed an LLM agent with ReAct reasoning and full data access. It executed flawlessly, yet when asked "Why is completion rate 80%?", it returned metrics instead of causal explanation. The agent knew how to reason but we had not specified what to reason about. This reflects a gap: runtime reasoning frameworks (ReAct, Chain-of-Thought) have transformed LLM agents, but design-time specification--determining what domain knowledge agents need--remains under-explored. We propose 4D-ARE (4-Dimensional Attribution-Driven Agent Requirements Engineering), a preliminary methodology for specifying attribution-driven agents. The core insight: decision-makers seek attribution, not answers. Attribution concerns organize into four dimensions (Results -> Process -> Support -> Long-term), motivated by Pearl's causal hierarchy. The framework operationalizes through five layers producing artifacts that compile directly to system prompts. We demonstrate the methodology through an industrial pilot deployment in financial services. 4D-ARE addresses what agents should reason about, complementing runtime frameworks that address how. We hypothesize systematic specification amplifies the power of these foundational advances. This paper presents a methodological proposal with preliminary industrial validation; rigorous empirical evaluation is planned for future work.

</details>


### [6] [Extending Delta Debugging Minimization for Spectrum-Based Fault Localization](https://arxiv.org/abs/2601.04689)
*Charaka Geethal Kapugama*

Main category: cs.SE

TL;DR: DDMIN-LOC结合Delta Debugging Minimization（DDMIN）和基于频谱的故障定位（SBFL），仅需单个失败输入即可定位字符串输入程序中的故障语句。


<details>
  <summary>Details</summary>
Motivation: DDMIN算法能找出最小失败诱导输入，但无法定位具体故障语句。需要一种方法在仅有单个失败输入的情况下，既能最小化输入又能定位故障。

Method: 结合DDMIN和SBFL：1）使用DDMIN生成通过和失败测试用例；2）收集这些测试用例的执行频谱；3）应用SBFL算法（Tarantula、Ochiai等）计算语句可疑度分数；4）结合分数对语句进行排名。

Result: 在QuixBugs和Codeflaws基准的136个程序上评估，使用Jaccard算法效果最佳：大多数情况下只需检查不到20%的可执行行即可定位故障，且故障语句在排名中通常位于前3位。

Conclusion: DDMIN-LOC有效解决了DDMIN无法定位故障语句的问题，仅需单个失败输入即可在字符串输入程序中实现高效故障定位，为调试提供了实用工具。

Abstract: This paper introduces DDMIN-LOC, a technique that combines Delta Debugging Minimization (DDMIN) with Spectrum-Based Fault Localization (SBFL). It can be applied to programs taking string inputs, even when only a single failure-inducing input is available. DDMIN is an algorithm that systematically explores the minimal failure-inducing input that exposes a bug, given an initial failing input. However, it does not provide information about the faulty statements responsible for the failure. DDMIN-LOC addresses this limitation by collecting the passing and failing inputs generated during the DDMIN process and computing suspiciousness scores for program statements and predicates using SBFL algorithms. These scores are then combined to rank statements according to their likelihood of being faulty. DDMIN-LOC requires only one failing input of the buggy program, although it can be applied only to programs that take string inputs. DDMIN-LOC was evaluated on 136 programs selected from the QuixBugs and Codeflaws benchmarks using the SBFL algorithms Tarantula, Ochiai, GenProg, Jaccard and DStar. Experimental results show that DDMIN-LOC performs best with Jaccard: in most subjects, fewer than 20% executable lines need to be examined to locate the faulty statements. Moreover, in most subjects, faulty statements are ranked within the top 3 positions in all the generated test suites derived from different failing inputs.

</details>


### [7] [A Longitudinal Analysis of Gamification in Untappd: Ethical Reflections on a Social Drinking Application](https://arxiv.org/abs/2601.04841)
*Jefferson Seide Molléri,Sami Hyrynsalmi,Antti Hakkala,Kai K. Kimppa,Jouni Smed*

Main category: cs.SE

TL;DR: 对社交饮酒应用Untappd的纵向伦理分析，发现尽管平台有所调整，但原有的伦理问题依然存在，呼吁将伦理反思嵌入软件生命周期


<details>
  <summary>Details</summary>
Motivation: 研究Untappd这类通过徽章、连续记录和社交分享来游戏化饮酒行为的应用，分析其设计特征如何影响用户自主性和福祉，以及平台伦理框架的演变

Method: 采用纵向研究方法，基于2020年的探索性研究，在2025年重新审视平台；结合传统伦理理论和软件工程实践框架，分析五类徽章及其对用户的影响

Result: 研究发现，尽管平台进行了小幅调整和表面免责声明，但原有的许多伦理问题仍然存在；游戏化设计可能使危险行为正常化

Conclusion: 需要在软件生命周期中嵌入持续的伦理反思，以防止通过设计使危险行为正常化；软件开发者应承担更多伦理责任

Abstract: This paper presents a longitudinal ethical analysis of Untappd, a social drinking application that gamifies beer consumption through badges, streaks, and social sharing. Building on an exploratory study conducted in 2020, we revisit the platform in 2025 to examine how its gamification features and ethical framings have evolved. Drawing on traditional ethical theory and practical frameworks for Software Engineering, we analyze five categories of badges and their implications for user autonomy and well-being. Our findings show that, despite small adjustments and superficial disclaimers, many of the original ethical issues remain. We argue for continuous ethical reflection built embedded into software lifecycles to prevent the normalization of risky behaviors through design.

</details>


### [8] [Analyzing Message-Code Inconsistency in AI Coding Agent-Authored Pull Requests](https://arxiv.org/abs/2601.04886)
*Jingzhi Gong,Giovanni Pinna,Yixin Bian,Jie M. Zhang*

Main category: cs.SE

TL;DR: 研究分析了AI代码助手生成的PR描述与实际代码变更之间的不一致性，发现1.7%的PR存在严重不一致问题，导致接受率降低51.7%，合并时间延长3.5倍。


<details>
  <summary>Details</summary>
Motivation: AI编码代理生成的PR描述是向人类评审者传达代码变更的主要渠道，但这些描述与实际变更之间的对齐程度尚未被探索，引发了人们对AI代理可信度的担忧。

Method: 使用PR消息-代码不一致性（PR-MCI）方法分析了五个AI代理的23,247个PR，贡献了974个手动标注的PR，识别了八种PR-MCI类型。

Result: 发现406个PR（1.7%）表现出高PR-MCI，其中声称未实现变更的描述是最常见问题（45.4%）。高MCI的PR接受率降低51.7%（28.3% vs 80.0%），合并时间延长3.5倍（55.8 vs 16.0小时）。

Conclusion: 不可靠的PR描述会削弱对AI代理的信任，需要建立PR-MCI验证机制和改进PR生成方法，以实现可信赖的人机协作。

Abstract: Pull request (PR) descriptions generated by AI coding agents are the primary channel for communicating code changes to human reviewers. However, the alignment between these messages and the actual changes remains unexplored, raising concerns about the trustworthiness of AI agents. To fill this gap, we analyzed 23,247 agentic PRs across five agents using PR message-code inconsistency (PR-MCI). We contributed 974 manually annotated PRs, found 406 PRs (1.7%) exhibited high PR-MCI, and identified eight PR-MCI types, revealing that descriptions claiming unimplemented changes was the most common issue (45.4%). Statistical tests confirmed that high-MCI PRs had 51.7% lower acceptance rates (28.3% vs. 80.0%) and took 3.5x longer to merge (55.8 vs. 16.0 hours). Our findings suggest that unreliable PR descriptions undermine trust in AI agents, highlighting the need for PR-MCI verification mechanisms and improved PR generation to enable trustworthy human-AI collaboration.

</details>


### [9] [AVX / NEON Intrinsic Functions: When Should They Be Used?](https://arxiv.org/abs/2601.04922)
*Théo Boivin,Joeffrey Legaux*

Main category: cs.SE

TL;DR: 提出跨配置基准测试，评估AVX/NEON内联函数在通用开发项目中的能力与限制，指导开发者根据OS、架构和编译器选择是否使用内联函数进行向量化优化。


<details>
  <summary>Details</summary>
Motivation: 在需要向量化策略优化代码的通用开发项目中，探索AVX/NEON内联函数的能力与限制，为开发者提供何时使用内联函数的指导，帮助其根据操作系统、架构和可用编译器做出明智选择。

Method: 提出跨配置基准测试方法，在不同操作系统、架构和编译器环境下，对比分析使用内联函数与普通代码的性能表现，特别关注条件分支等关键场景。

Result: 内联函数在条件分支处理中表现出色，执行时间仅为普通代码的5%左右；但在许多情况下，编译器已能很好地自动向量化代码，使得内联函数变得不必要。

Conclusion: 内联函数在特定场景（如条件分支）中非常高效，但开发者应谨慎评估使用场景，因为现代编译器已具备良好的自动向量化能力，盲目使用内联函数可能不必要。

Abstract: A cross-configuration benchmark is proposed to explore the capacities and limitations of AVX / NEON intrinsic functions in a generic context of development project, when a vectorisation strategy is required to optimise the code. The main aim is to guide developers to choose when using intrinsic functions, depending on the OS, architecture and/or available compiler. Intrinsic functions were observed highly efficient in conditional branching, with intrinsic version execution time reaching around 5% of plain code execution time. However, intrinsic functions were observed as unnecessary in many cases, as the compilers already well auto-vectorise the code.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [AHA: Scalable Alternative History Analysis for Operational Timeseries Applications](https://arxiv.org/abs/2601.04432)
*Harshavardhan Kamarthi,Harshil Shah,Henry Milner,Sayan Sinha,Yan Li,B. Aditya Prakash,Vyas Sekar*

Main category: cs.DB

TL;DR: AHA系统为高维时序数据的替代历史分析提供成本效益和保真度保证，相比传统方法降低85倍总拥有成本


<details>
  <summary>Details</summary>
Motivation: 运营商和分析师需要对高维时序数据进行回顾性分析（如异常检测、算法评估等），但传统数据处理方案要么运营成本高，要么无法保证准确重放

Method: 设计AHA系统，基于三个关键洞察：1)底层统计的可分解性；2)属性值组合中子群体活跃度的稀疏性；3)现代分析数据库中聚合操作的高效结构

Result: 在多个真实数据集和生产管道案例研究中，AHA为广泛的下游任务提供100%准确度，相比传统方法总拥有成本（计算+存储）降低高达85倍

Conclusion: AHA系统成功解决了高维时序数据替代历史分析中的成本效率和保真度挑战，为运营商提供经济高效的解决方案

Abstract: Many operational systems collect high-dimensional timeseries data about users/systems on key performance metrics. For instance, ISPs, content distribution networks, and video delivery services collect quality of experience metrics for user sessions associated with metadata (e.g., location, device, ISP). Over such historical data, operators and data analysts often need to run retrospective analysis; e.g., analyze anomaly detection algorithms, experiment with different configurations for alerts, evaluate new algorithms, and so on. We refer to this class of workloads as alternative history analysis for operational datasets. We show that in such settings, traditional data processing solutions (e.g., data warehouses, sampling, sketching, big-data systems) either pose high operational costs or do not guarantee accurate replay. We design and implement a system, called AHA (Alternative History Analytics), that overcomes both challenges to provide cost efficiency and fidelity for high-dimensional data. The design of AHA is based on analytical and empirical insights about such workloads: 1) the decomposability of underlying statistics; 2) sparsity in terms of active number of subpopulations over attribute-value combinations; and 3) efficiency structure of aggregation operations in modern analytics databases. Using multiple real-world datasets and as well as case-studies on production pipelines at a large video analytics company, we show that AHA provides 100% accuracy for a broad range of downstream tasks and up to 85x lower total cost of ownership (i.e., compute + storage) compared to conventional methods.

</details>


### [11] [Does Provenance Interact?](https://arxiv.org/abs/2601.04722)
*Chrysanthi Kosyfaki,Ruiyuan Zhang,Nikos Mamoulis,Xiaofang Zhou*

Main category: cs.DB

TL;DR: 本文提出使用时间交互网络(TINs)高效表示时间溯源信息，解决流式系统中溯源图随数据量超线性增长的可扩展性问题，为大规模数据流提供实用工具。


<details>
  <summary>Details</summary>
Motivation: 数据溯源在数据库查询解释和科学工作流审计等多个领域有重要应用，但传统溯源方法面临计算成本和存储开销的挑战。在流式系统中，溯源图随数据量超线性增长，存在显著的可扩展性问题。现有时间溯源方法主要关注系统级调试，在数据管理应用方面存在空白。

Method: 提出使用时间交互网络(TINs)表示时间溯源信息，将数据分为离散型和流动型两类，定义五种时间溯源查询类型，并提出基于状态的索引方法。该方法适用于流式系统、交通网络和金融网络等多种场景。

Result: 展示了TINs在多个领域的适用性，提出了系统化的时间溯源框架，包括数据分类、查询类型定义和索引方法，为大规模数据流的时间溯源提供了理论基础。

Conclusion: 本文提出了一个研究议程，旨在通过时间交互网络使时间溯源成为大规模数据流的实用工具，并规划了未来的研究方向，为解决溯源可扩展性问题提供了新思路。

Abstract: Data provenance (the process of determining the origin and derivation of data outputs) has applications across multiple domains including explaining database query results and auditing scientific workflows. Despite decades of research, provenance tracing remains challenging due to computational costs and storage overhead. In streaming systems such as Apache Flink, provenance graphs can grow super-linearly with data volume, posing significant scalability challenges. Temporal provenance is a promising direction, attaching timestamps to provenance information, enabling time-focused queries without maintaining complete historical records. However, existing temporal provenance methods primarily focus on system-level debugging, leaving a gap in data management applications. This paper proposes an agenda that uses Temporal Interaction Networks (TINs) to represent temporal provenance efficiently. We demonstrate TINs' applicability across streaming systems, transportation networks, and financial networks. We classify data into discrete and liquid types, define five temporal provenance query types, and propose a state-based indexing approach. Our vision outlines research directions toward making temporal provenance a practical tool for large-scale dataflows.

</details>


### [12] [Structural Indexing of Relational Databases for the Evaluation of Free-Connex Acyclic Conjunctive Queries](https://arxiv.org/abs/2601.04757)
*Cristian Riveros,Benjamin Scheidt,Nicole Schweikardt*

Main category: cs.DB

TL;DR: 提出基于数据库结构对称性的索引方法，用于高效评估自由连接无环连接查询，预处理时间与辅助数据库大小线性相关，在某些情况下辅助数据库大小可远小于原数据库。


<details>
  <summary>Details</summary>
Motivation: 现有索引方法主要基于值或顺序（如B+树），缺乏利用数据库内部结构对称性的索引技术。需要一种能够利用数据库结构对称性来高效评估自由连接无环连接查询的方法。

Method: 构建基于结构对称性的索引结构，核心是创建辅助数据库D_col，该数据库的大小与Scheidt和Schweikardt的"关系颜色细化"算法分配的颜色数量相关。利用数据库元组间的结构对称性来加速查询处理。

Result: 对于任何自由连接无环连接查询，可以在与D_col大小线性相关的预处理时间后，以常数延迟计数或枚举查询结果。D_col大小在某些情况下可远小于原数据库（如二叉树中对数大小，正则图中常数大小）。

Conclusion: 这是首个基于数据库内部结构对称性的索引基础性成果，能够在某些情况下实现比数据库大小更优的查询性能，为利用结构对称性优化查询评估提供了新方向。

Abstract: We present an index structure to boost the evaluation of free-connex acyclic conjunctive queries (fc-ACQs) over relational databases. The main ingredient of the index associated with a given database $D$ is an auxiliary database $D_{col}$. Our main result states that for any fc-ACQ $Q$ over $D$, we can count the number of answers of $Q$ or enumerate them with constant delay after a preprocessing phase that takes time linear in the size of $D_{col}$.
  Unlike previous indexing methods based on values or order (e.g., B+ trees), our index is based on structural symmetries among tuples in a database, and the size of $D_{col}$ is related to the number of colors assigned to $D$ by Scheidt and Schweikardt's "relational color refinement" (2025). In the particular case of graphs, this coincides with the minimal size of an equitable partition of the graph. For example, the size of $D_{col}$ is logarithmic in the case of binary trees and constant for regular graphs. Even in the worst-case that $D$ has no structural symmetries among tuples at all, the size of $D_{col}$ is still linear in the size of $D$.
  Given that the size of $D_{col}$ is bounded by the size of $D$ and can be much smaller (even constant for some families of databases), our index is the first foundational result on indexing internal structural symmetries of a database to evaluate all fc-ACQs with performance potentially strictly smaller than the database size.

</details>


### [13] [LGTD: Local-Global Trend Decomposition for Season-Length-Free Time Series Analysis](https://arxiv.org/abs/2601.04820)
*Chotanansub Sophaken,Thanadej Rattanakornphan,Piyanon Charoenpoonpanich,Thanapol Phungtua-eng,Chainarong Amornbunchornvej*

Main category: cs.DB

TL;DR: LGTD是一种无需指定季节长度的时序分解方法，通过全局趋势和自适应局部趋势来隐式捕捉季节性结构，适用于不规则、漂移或弱周期性的时间序列。


<details>
  <summary>Details</summary>
Motivation: 现有季节趋势分解方法需要用户指定或估计季节长度，并假设稳定的周期结构，这限制了在大型异构集合中的鲁棒性和可部署性，因为实际数据中的重复模式可能存在漂移、间歇出现或多时间尺度。

Method: LGTD将时间序列分解为平滑全局趋势、自适应局部趋势（其重复出现产生隐式季节性结构）和残差分量。首先估计捕获长期演化的全局趋势，然后使用AutoTrend（自适应误差驱动的局部线性趋势推断模块）将去趋势信号分割为短期分段线性区间。

Result: LGTD在固定、过渡和可变季节长度设置的合成基准测试中表现出稳健和平衡的分解性能，特别是在基于周期的方法性能下降的情况下。该方法计算复杂度与序列长度呈线性关系。

Conclusion: LGTD通过消除手动指定季节长度的需求，支持在具有不规则、漂移或弱周期性结构的时间序列上实现自动化、低接触部署，为下游分析提供了更鲁棒的分解基础。

Abstract: Time series decomposition into trend, seasonal structure, and residual components is a core primitive for downstream analytics such as anomaly detection, change-point detection, and forecasting. However, most existing seasonal-trend decomposition methods rely on user-specified or estimated season lengths and implicitly assume stable periodic structure. These assumptions limit robustness and deployability in large, heterogeneous collections where recurring patterns may drift, appear intermittently, or exist at multiple time scales.
  We propose LGTD (Local-Global Trend Decomposition), a season-length-free decomposition framework that represents a time series as the sum of a smooth global trend, adaptive local trends whose recurrence induces implicit (emergent) seasonal structure, and a residual component. Rather than explicitly modeling seasonality through a fixed or estimated period, LGTD treats seasonal structure as an emergent property arising from repeated local trend regimes. Concretely, LGTD first estimates a global trend capturing long-term evolution, then applies AutoTrend, an adaptive error-driven local linear trend inference module, to segment the detrended signal into short-lived piecewise-linear regimes. Residuals are obtained after removing both global and local trends.
  By eliminating manual season-length specification, LGTD supports automated, low-touch deployment across time series with irregular, drifting, or weakly periodic structure. We analyze computational complexity and show that LGTD scales linearly with series length under mild conditions. Experiments on synthetic benchmarks demonstrate robust and balanced decomposition performance across fixed, transitive, and variable season-length settings, especially where period-based methods degrade.

</details>


### [14] [Responsibility Measures for Conjunctive Queries with Negation](https://arxiv.org/abs/2601.04868)
*Meghyn Bienvenu,Diego Figueira,Pierre Lafourcade*

Main category: cs.DB

TL;DR: 该论文为具有否定原子的联合合取查询(UCQ¬)提出了两种责任度量方法，将单调查询的责任度量扩展到非单调查询，并分析了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有责任度量研究几乎完全集中在单调查询上，但实际应用中需要处理包含否定原子的查询。需要为UCQ¬定义合理的责任度量来解释查询结果中数据库事实的贡献。

Method: 提出两种方法：1) 仅给正数据库事实分配分数；2) 同时考虑正事实和否定事实。这些方法将单调查询的drastic Shapley和WSMS度量扩展到UCQ¬，并研究其数据复杂度和组合复杂度。

Result: 证明了WSMS度量对所有UCQ¬查询在数据复杂度上是可处理的，并为合适的带否定合取查询类在组合复杂度上建立了可处理性。

Conclusion: 成功将责任度量从单调查询扩展到包含否定原子的查询，提供了两种合理的解释方法，并证明了某些度量在计算上的可行性，为非单调查询的解释性分析提供了理论基础。

Abstract: We contribute to the recent line of work on responsibility measures that quantify the contributions of database facts to obtaining a query result. In contrast to existing work which has almost exclusively focused on monotone queries, here we explore how to define responsibility measures for unions of conjunctive queries with negated atoms (UCQ${}^\lnot$). Starting from the question of what constitutes a reasonable notion of explanation or relevance for queries with negated atoms, we propose two approaches, one assigning scores to (positive) database facts and the other also considering negated facts. Our approaches, which are orthogonal to the previously studied score of Reshef et al., can be used to lift previously studied scores for monotone queries, known as drastic Shapley and weighted sums of minimal supports (WSMS), to UCQ$^\lnot$. We investigate the data and combined complexity of the resulting measures, notably showing that the WSMS measures are tractable in data complexity for all UCQ${}^\lnot$ queries and further establishing tractability in combined complexity for suitable classes of conjunctive queries with negation.

</details>


### [15] [Rule Rewriting Revisited: A Fresh Look at Static Filtering for Datalog and ASP](https://arxiv.org/abs/2601.05108)
*Philipp Hanisch,Markus Krötzsch*

Main category: cs.DB

TL;DR: 静态过滤是Datalog的数据无关优化方法，推广了关系数据库的代数查询重写技术。本文回顾并更新了该方法，扩展到ASP，提出可处理的近似算法以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Kifer和Lozinskii在1986年就提出了静态过滤方法，但该方法在近期研究和系统开发中被忽视，其特例被独立重新发现。因此需要回顾原始方法，使用现代术语和更通用的过滤器谓词，并将其扩展到答案集编程（ASP）。

Method: 使用更新术语和更通用的过滤器谓词来重新表述原始静态过滤方法，将其扩展到ASP。提出可处理的近似算法，包括一般情况下的双指数复杂度和有界元数谓词的单指数复杂度近似。

Result: 扩展后的方法比经典方法更通用但也更复杂。提出的可处理近似算法在典型情况下仍能显著改进逻辑程序，例如在真实世界数据上可将规则系统性能提升一个数量级。

Conclusion: 静态过滤作为Datalog的数据无关优化方法值得重新关注，通过现代术语更新和扩展到ASP，配合可处理近似算法，能在实际应用中显著提升性能。

Abstract: Static filtering is a data-independent optimisation method for Datalog, which generalises algebraic query rewriting techniques from relational databases. In spite of its early discovery by Kifer and Lozinskii in 1986, the method has been overlooked in recent research and system development, and special cases are being rediscovered independently. We therefore recall the original approach, using updated terminology and more general filter predicates that capture features of modern systems, and we show how to extend its applicability to answer set programming (ASP). The outcome is strictly more general but also more complex than the classical approach: double exponential in general and single exponential even for predicates of bounded arity. As a solution, we propose tractable approximations of the algorithm that can still yield much improved logic programs in typical cases, e.g., it can improve the performance of rule systems over real-world data in the order of magnitude.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [ParaCodex: A Profiling-Guided Autonomous Coding Agent for Reliable Parallel Code Generation and Translation](https://arxiv.org/abs/2601.04327)
*Erel Kaplan,Tomer Bitan,Lian Ghrayeb,Le Chen,Tom Yotam,Niranjan Hasabnis,Gal Oren*

Main category: cs.DC

TL;DR: ParaCodex是一个基于Codex的自主编码系统，用于将串行CPU内核转换为OpenMP GPU卸载内核，通过热点分析、数据规划、正确性检查和性能优化，在多个基准测试中实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 并行编程对HPC和AI至关重要，但编写正确且高效的代码（特别是OpenMP GPU卸载）仍然具有挑战性。现有自主编码代理输出脆弱，缺乏领域特定的支撑框架。

Method: 提出ParaCodex工作流，将基于Codex的代理转变为自主OpenMP GPU卸载系统，采用分阶段热点分析、显式数据规划、正确性门控和性能分析引导的优化方法。

Result: 在HeCBench、Rodinia和NAS基准测试中，ParaCodex成功处理31个有效内核中的31个，在25/31情况下优于参考OpenMP实现，几何平均加速比在HeCBench上达到3倍，Rodinia上达到5倍，在所有测试套件上都优于零样本Codex基线。

Conclusion: ParaCodex展示了自主编码代理在HPC领域的潜力，通过结构化工作流和领域特定优化，能够有效生成高性能的OpenMP GPU卸载代码，显著提升开发效率。

Abstract: Parallel programming is central to HPC and AI, but producing code that is correct and fast remains challenging, especially for OpenMP GPU offload, where data movement and tuning dominate. Autonomous coding agents can compile, test, and profile on target hardware, but outputs are brittle without domain scaffolding.
  We present ParaCodex, an HPC-engineer workflow that turns a Codex-based agent into an autonomous OpenMP GPU offload system using staged hotspot analysis, explicit data planning, correctness gating, and profiling-guided refinement. We evaluate translation from serial CPU kernels to OpenMP GPU offload kernels on HeCBench, Rodinia, and NAS. After excluding five kernels, ParaCodex succeeded on all 31 valid kernels. The generated kernels improved GPU time over reference OpenMP implementations in 25/31 cases, achieving geometric-mean speedups of 3x on HeCBench and 5x on Rodinia, and outperforming a zero-shot Codex baseline on all suites. We also evaluate CUDA to OpenMP offload translation on ParEval, where ParaCodex maintains high compilation and validation rates in code-only and end-to-end settings.

</details>


### [17] [Hybrid Cloud Architectures for Research Computing: Applications and Use Cases](https://arxiv.org/abs/2601.04349)
*Xaver Stiensmeier,Alexander Kanitz,Jan Krüger,Santiago Insua,Adrián Rošinec,Viktória Spišáková,Lukáš Hejtmánek,David Yuan,Gavin Farrell,Jonathan Tedds,Juha Törnroos,Harald Wagener,Alex Sczyrba,Nils Hoffmann,Matej Antol*

Main category: cs.DC

TL;DR: 本文全面综述了混合云部署模型，重点关注网格与云平台（OpenPBS、SLURM、OpenStack、Kubernetes）和工作流管理工具（Nextflow、Snakemake、CWL），探讨了联邦计算、多云编排和负载调度策略，并基于生命科学领域的实施案例提出了加速研究计算中混合云采用的路线图。


<details>
  <summary>Details</summary>
Motivation: 科学研究日益依赖稳健且可扩展的IT基础设施来支持复杂计算工作流。随着研究基础设施、NREN和商业云提供商提供的服务激增，研究人员必须在一个碎片化的计算环境生态系统中导航，平衡性能、成本、可扩展性和可访问性。混合云架构通过集成多个计算环境来增强灵活性、资源效率和专用硬件访问，提供了一个有吸引力的解决方案。

Method: 本文采用综述分析方法，全面概述混合云部署模型，重点关注网格和云平台（包括OpenPBS、SLURM、OpenStack、Kubernetes）以及工作流管理工具（Nextflow、Snakemake、CWL）。探索联邦计算、多云编排和负载调度策略，解决互操作性、数据安全、可重复性和网络性能等关键挑战。基于ELIXIR计算平台协调的生命科学领域实施案例及其在更广泛EOSC背景下的集成。

Result: 通过分析现有混合云部署模型和技术工具，识别了研究计算中的关键挑战和解决方案。基于ELIXIR计算平台的实际实施经验，提出了一个加速混合云在研究计算中采用的路线图，强调治理框架和技术解决方案，以推动可持续和可扩展的基础设施发展。

Conclusion: 混合云架构为研究计算提供了灵活、高效和可扩展的解决方案。通过适当的治理框架和技术集成，可以克服互操作性、数据安全和性能等挑战。ELIXIR计算平台在生命科学领域的成功实施为其他学科提供了可借鉴的模式，加速混合云在研究计算中的广泛采用，促进科学研究的创新和发展。

Abstract: Scientific research increasingly depends on robust and scalable IT infrastructures to support complex computational workflows. With the proliferation of services provided by research infrastructures, NRENs, and commercial cloud providers, researchers must navigate a fragmented ecosystem of computing environments, balancing performance, cost, scalability, and accessibility. Hybrid cloud architectures offer a compelling solution by integrating multiple computing environments to enhance flexibility, resource efficiency, and access to specialised hardware.
  This paper provides a comprehensive overview of hybrid cloud deployment models, focusing on grid and cloud platforms (OpenPBS, SLURM, OpenStack, Kubernetes) and workflow management tools (Nextflow, Snakemake, CWL). We explore strategies for federated computing, multi-cloud orchestration, and workload scheduling, addressing key challenges such as interoperability, data security, reproducibility, and network performance. Drawing on implementations from life sciences, as coordinated by the ELIXIR Compute Platform and their integration into a wider EOSC context, we propose a roadmap for accelerating hybrid cloud adoption in research computing, emphasising governance frameworks and technical solutions that can drive sustainable and scalable infrastructure development.

</details>


### [18] [Sharded Elimination and Combining for Highly-Efficient Concurrent Stacks](https://arxiv.org/abs/2601.04523)
*Ajay Singh,Nikos Metaxakis,Panagiota Fatourou*

Main category: cs.DC

TL;DR: 提出一种基于分片和fetch&increment的新型阻塞线性化栈实现，性能比现有并发栈提升高达2倍


<details>
  <summary>Details</summary>
Motivation: 现有并发栈在高并发场景下性能不足，需要设计一种能够减少争用、提高并行性的新实现

Method: 结合分片技术、fetch&increment操作、新颖的消除机制和组合方法，有效混合这些技术以实现高性能

Result: 在大多数工作负载下性能比现有并发栈提升高达2倍，特别在大规模线程系统和高争用场景下表现优异

Conclusion: 提出的栈实现通过创新的消除机制和组合方法，显著提高了并发栈的性能和可扩展性

Abstract: We present a new blocking linearizable stack implementation which utilizes sharding and fetch&increment to achieve significantly better performance than all existing concurrent stacks. The proposed implementation is based on a novel elimination mechanism and a new combining approach that are efficiently blended to gain high performance. Our implementation results in enhanced parallelism and low contention when accessing the shared stack. Experiments show that the proposed stack implementation outperforms all existing concurrent stacks by up to 2X in most workloads. It is particularly efficient in systems supporting a large number of threads and in high contention scenarios.

</details>


### [19] [Quantifying Autoscaler Vulnerabilities: An Empirical Study of Resource Misallocation Induced by Cloud Infrastructure Faults](https://arxiv.org/abs/2601.04659)
*Gijun Park*

Main category: cs.DC

TL;DR: 研究通过模拟实验分析四种常见基础设施故障对云资源自动扩缩容的影响，发现存储故障导致最高成本开销，路由故障导致资源分配不足，水平扩缩对瞬时故障更敏感。


<details>
  <summary>Details</summary>
Motivation: 云环境中资源自动扩缩机制依赖准确的性能指标做出最优资源配置决策。当硬件故障、网络中断、软件异常等基础设施故障破坏这些指标时，自动扩缩器可能系统性地过度或不足配置资源，导致运营成本增加或服务可靠性下降。

Method: 通过受控模拟实验，测量四种常见故障类别对垂直和水平自动扩缩行为的影响，涵盖多种实例配置和服务水平目标(SLO)阈值。

Result: 存储相关故障产生最大的成本开销，在水平扩缩策略下每月增加高达258美元；路由异常持续导致自动扩缩器偏向资源分配不足；水平扩缩对故障引起的指标失真更敏感，特别是在阈值边界附近。

Conclusion: 基于实证的洞察为设计容错自动扩缩策略提供了可操作建议，帮助区分真实工作负载波动与故障伪影，提高云资源管理的鲁棒性。

Abstract: Resource autoscaling mechanisms in cloud environments depend on accurate performance metrics to make optimal provisioning decisions. When infrastructure faults including hardware malfunctions, network disruptions, and software anomalies corrupt these metrics, autoscalers may systematically over- or under-provision resources, resulting in elevated operational expenses or degraded service reliability. This paper conducts controlled simulation experiments to measure how four prevalent fault categories affect both vertical and horizontal autoscaling behaviors across multiple instance configurations and service level objective (SLO) thresholds. Experimental findings demonstrate that storage-related faults generate the largest cost overhead, adding up to $258 monthly under horizontal scaling policies, whereas routing anomalies consistently bias autoscalers toward insufficient resource allocation. The sensitivity to fault-induced metric distortions differs markedly between scaling strategies: horizontal autoscaling exhibits greater susceptibility to transient anomalies, particularly near threshold boundaries. These empirically-grounded insights offer actionable recommendations for designing fault-tolerant autoscaling policies that distinguish genuine workload fluctuations from failure artifacts.

</details>


### [20] [Cognitive Infrastructure: A Unified DCIM Framework for AI Data Centers](https://arxiv.org/abs/2601.04750)
*Krishna Chaitanya Sunkara*

Main category: cs.DC

TL;DR: DCIM 3.0是一个统一框架，集成了语义推理、预测分析、自主编排和统一连接，用于下一代AI数据中心管理


<details>
  <summary>Details</summary>
Motivation: 解决基础设施自动化、可持续性和数字孪生设计中的关键挑战，特别是在AI数据中心环境中

Method: 通过知识图谱智能、热建模和统一设备连接协议(UDCP)构建统一管理框架

Result: 提出了DCIM 3.0框架，集成了语义推理、预测分析、自主编排和统一连接功能

Conclusion: DCIM 3.0为下一代AI数据中心管理提供了全面的解决方案，解决了基础设施自动化、可持续性和数字孪生设计的关键问题

Abstract: This work presents DCIM 3.0, a unified framework integrating semantic reasoning, predictive analytics, autonomous orchestration, and unified connectivity for next-generation AI data center management. The framework addresses critical challenges in infrastructure automation, sustainability, and digital-twin design through knowledge graph-based intelligence, thermal modeling, and the Unified Device Connectivity Protocol (UDCP).Keywords-Data Center Infrastructure Management, DCIM, AI Data Centers, Knowledge Graphs, Digital Twin, Thermal Management, Infrastructure Automation, Sustainability, GPU Computing, Data Center

</details>


### [21] [Proof of Commitment: A Human-Centric Resource for Permissionless Consensus](https://arxiv.org/abs/2601.04813)
*Homayoun Maleki,Nekane Sainz,Jon Legarda*

Main category: cs.DC

TL;DR: PoCmt是一种基于人类实时参与的非并行化资源的新型共识协议，通过人类挑战机制确保Sybil攻击成本线性增长，相比传统PoW/PoS具有更好的抗Sybil攻击能力。


<details>
  <summary>Details</summary>
Motivation: 现有共识协议（如PoW和PoS）使用可并行化资源（计算或资本），这些资源一旦获得就可以以近乎零边际成本分配给多个身份，无法实现真正的线性Sybil成本。需要一种基于非并行化资源的共识机制来从根本上解决Sybil攻击问题。

Method: 提出Proof of Commitment (PoCmt)协议，基于人类实时参与这一非并行化资源。通过人类挑战预言机（Human Challenge Oracle）发布身份绑定、时间敏感的挑战，限制每个时间窗口内可解决的挑战数量。验证者维护承诺状态，记录累计人类努力、协议参与和在线可用性。

Result: 理论分析表明，基于可并行化资源的协议允许零边际Sybil成本，而PoCmt强制执行严格线性成本曲线。通过加权骨干分析证明PoCmt在部分同步条件下实现安全性、活性和承诺比例公平性。模拟验证了人类时间容量作为唯一对抗瓶颈的有效性。

Conclusion: PoCmt为共识设计空间提供了一个新方向，将无许可安全性建立在持续的人类努力而非计算或资本上，从根本上解决了Sybil攻击的边际成本问题。

Abstract: Permissionless consensus protocols require a scarce resource to regulate leader election and provide Sybil resistance. Existing paradigms such as Proof of Work and Proof of Stake instantiate this scarcity through parallelizable resources like computation or capital. Once acquired, these resources can be subdivided across many identities at negligible marginal cost, making linear Sybil cost fundamentally unattainable.
  We introduce Proof of Commitment (PoCmt), a consensus primitive grounded in a non-parallelizable resource: real-time human engagement. Validators maintain a commitment state capturing cumulative human effort, protocol participation, and online availability. Engagement is enforced through a Human Challenge Oracle that issues identity-bound, time-sensitive challenges, limiting the number of challenges solvable within each human window.
  Under this model, sustaining multiple active identities requires proportional human-time effort. We establish a cost-theoretic separation showing that protocols based on parallelizable resources admit zero marginal Sybil cost, whereas PoCmt enforces a strictly linear cost profile. Using a weighted-backbone analysis, we show that PoCmt achieves safety, liveness, and commitment-proportional fairness under partial synchrony.
  Simulations complement the analysis by isolating human-time capacity as the sole adversarial bottleneck and validating the predicted commitment drift and fairness properties. These results position PoCmt as a new point in the consensus design space, grounding permissionless security in sustained human effort rather than computation or capital.

</details>


### [22] [Parallel Quadratic Selected Inversion in Quantum Transport Simulation](https://arxiv.org/abs/2601.04904)
*Vincent Maillou,Matthias Bollhofer,Olaf Schenk,Alexandros Nikolaos Ziogas,Mathieu Luisier*

Main category: cs.DC

TL;DR: 提出分布式递归格林函数方法，用于加速纳米器件量子输运模拟中的选择逆矩阵和选择二次矩阵方程求解，支持多终端晶体管结构，在16个GPU上比PARDISO快5.2倍。


<details>
  <summary>Details</summary>
Motivation: 随着晶体管尺寸缩小到纳米尺度，需要精确的量子输运模拟。非平衡格林函数(NEGF)方法是理想选择，但计算量大，涉及选择逆矩阵和选择二次矩阵方程求解。现有算法如递归格林函数(RGF)适合GPU加速，但通常是顺序执行、需要块三对角矩阵输入，且仅限于共享内存并行，限制了可模拟的器件尺寸。

Method: 提出分布式方法，基于RGF实现并行选择逆矩阵和选择二次矩阵方程求解。扩展方法处理带箭头结构的块三对角矩阵，支持多终端晶体管结构。在真实纳米带晶体管量子输运模拟数据集上评估性能。

Result: 在16个GPU上，融合的选择逆矩阵和选择二次矩阵方程求解器比PARDISO的选择逆矩阵模块快5.2倍（应用于16倍短的器件）。展示了方法在加速NEGF基纳米器件模拟方面的潜力。

Conclusion: 提出的分布式方法解决了现有RGF技术的局限性，支持大规模并行计算和多终端结构，显著加速了纳米器件量子输运模拟，为更大尺寸器件的精确模拟提供了有效工具。

Abstract: Driven by Moore's Law, the dimensions of transistors have been pushed down to the nanometer scale. Advanced quantum transport (QT) solvers are required to accurately simulate such nano-devices. The non-equilibrium Green's function (NEGF) formalism lends itself optimally to these tasks, but it is computationally very intensive, involving the selected inversion (SI) of matrices and the selected solution of quadratic matrix (SQ) equations. Existing algorithms to tackle these numerical problems are ideally suited to GPU acceleration, e.g., the so-called recursive Green's function (RGF) technique, but they are typically sequential, require block-tridiagonal (BT) matrices as inputs, and their implementation has been so far restricted to shared memory parallelism, thus limiting the achievable device sizes. To address these shortcomings, we introduce distributed methods that build on RGF and enable parallel selected inversion and selected solution of the quadratic matrix equation. We further extend them to handle BT matrices with arrowhead, which allows for the investigation of multi-terminal transistor structures. We evaluate the performance of our approach on a real dataset from the QT simulation of a nano-ribbon transistor and compare it with the sparse direct package PARDISO. When scaling to 16 GPUs, our fused SI and SQ solver is 5.2x faster than the SI module of PARDISO applied to a device 16x shorter. These results highlight the potential of our method to accelerate NEGF-based nano-device simulations.

</details>


### [23] [Asynchronous Secure Federated Learning with Byzantine aggregators](https://arxiv.org/abs/2601.04930)
*Antonella Del Pozzo,Achille Desreumaux,Mathieu Gestin,Alexandre Rapetti,Sara Tucci-Piergiovanni*

Main category: cs.DC

TL;DR: 提出异步通信环境下针对恶意聚合器的联邦平均隐私保护方案，结合安全聚合与差分隐私，通过复制聚合器容忍拜占庭故障，无需共识协议


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习隐私保护方案主要针对同步通信和诚实聚合器，缺乏对异步通信环境下恶意聚合器的有效防护，且异步通信会带来客户端参与不均和隐私预算失衡问题

Method: 1) 复制聚合器容忍拜占庭故障；2) 客户端使用掩码和高斯噪声保护模型更新；3) 引入包含机制确保客户端均匀参与；4) 利用复制服务器解掩码模型，无需聚合器间共识

Result: 方案在所有指标上达到与现有最佳方案相同的性能，能够容忍恶意聚合器操纵，在异步环境下保持训练活性，同时确保客户端隐私保护和参与均衡

Conclusion: 该工作首次在异步通信模型下解决了恶意聚合器的联邦学习隐私保护问题，通过创新的架构设计绕过了异步环境下的共识不可能性，提高了系统可用性

Abstract: Privacy-preserving federated averaging is a central approach for protecting client privacy in federated learning. In this paper, we study this problem in an asynchronous communications setting with malicious aggregators. We propose a new solution to provide federated averaging in this model while protecting the client's data privacy through secure aggregation and differential privacy. Our solution maintains the same performance as the state of the art across all metrics. The main contributions of this paper are threefold. First, unlike existing single- or multi-server solutions, we consider malicious aggregation servers that may manipulate the model to leak clients' data or halt computation. To tolerate this threat, we replicate the aggregators, allowing a fraction of them to be corrupted. Second, we propose a new privacy preservation protocol for protocols in asynchronous communication models with Byzantine aggregators. In this protocol, clients mask their values and add Gaussian noise to their models. In contrast with previous works, we use the replicated servers to unmask the models, while ensuring the liveness of training even if aggregators misbehave. Third, the asynchronous communication model introduces new challenges not present in existing approaches. In such a setting, faster clients may contribute more frequently, potentially reducing their privacy and biasing the training. To address this, we introduce an inclusion mechanism that ensures uniform client participation and balanced privacy budgets. Interestingly, the solution presented in this paper does not rely on agreement between aggregators. Thus, we circumvent the known impossibility of consensus in asynchronous settings where processes might crash. Additionally, this feature increases availability, as a consensus-based algorithm only progresses in periods of low latency.

</details>


### [24] [Nalar: An agent serving framework](https://arxiv.org/abs/2601.05109)
*Marco Laju,Donghyun Son,Saurabh Agarwal,Nitin Kedia,Myungjin Lee,Jayanth Srinivasa,Aditya Akella*

Main category: cs.DC

TL;DR: Nalar是一个专门为LLM驱动的智能体应用设计的服务框架，通过分离工作流规范与执行、提供运行时可见性和控制，实现了高效、可扩展的智能体应用服务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的智能体应用面临多组件异构性、动态控制流、长运行状态和不可预测延迟等挑战，需要专门的服务框架来高效处理这些复杂任务。

Method: 1. 使用轻量级自动生成的存根将智能体和工具调用转换为携带依赖和上下文元数据的futures；2. 管理状态层解耦逻辑状态与物理放置；3. 两级控制架构结合全局策略计算与本地事件驱动执行。

Result: 在三个智能体工作负载上，Nalar将尾部延迟降低了34-74%，实现了最高2.9倍的加速，在基线失败时仍能维持80 RPS，并能扩展到13万个futures且控制开销低于500毫秒。

Conclusion: Nalar通过其创新的架构设计，能够在不增加开发者编排逻辑负担的情况下，为异构智能体应用提供可扩展、高效且策略驱动的服务。

Abstract: LLM-driven agentic applications increasingly automate complex, multi-step tasks, but serving them efficiently remains challenging due to heterogeneous components, dynamic and model-driven control flow, long-running state, and unpredictable latencies. Nalar is a ground-up agent-serving framework that cleanly separates workflow specification from execution while providing the runtime visibility and control needed for robust performance. Nalar preserves full Python expressiveness, using lightweight auto-generated stubs that turn agent and tool invocations into futures carrying dependency and context metadata. A managed state layer decouples logical state from physical placement, enabling safe reuse, migration, and consistent retry behavior. A two-level control architecture combines global policy computation with local event-driven enforcement to support adaptive routing, scheduling, and resource management across evolving workflows. Together, these mechanisms allow Nalar to deliver scalable, efficient, and policy-driven serving of heterogeneous agentic applications without burdening developers with orchestration logic. Across three agentic workloads, Nalar cuts tail latency by 34--74\%, achieves up to $2.9\times$ speedups, sustains 80 RPS where baselines fail, and scales to 130K futures with sub-500 ms control overhead.

</details>
