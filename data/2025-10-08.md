<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 17]
- [cs.SE](#cs.SE) [Total: 15]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices](https://arxiv.org/abs/2510.05109)
*Yilong Li,Shuai Zhang,Yijing Zeng,Hao Zhang,Xinmiao Xiong,Jingyu Liu,Pan Hu,Suman Banerjee*

Main category: cs.DC

TL;DR: NANOMIND是一个硬件-软件协同设计的大型多模态模型推理框架，通过将模型分解为模块化组件并在异构加速器上动态调度，显著提升了能效和性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型通常以整体方式执行，无法充分利用现代SoC中的异构加速器，导致高延迟和低能效。

Method: 将大型模型分解为模块化"砖块"，通过模块级动态卸载技术将各组件映射到最合适的加速器上，结合定制硬件设计、系统级调度和优化的低比特计算内核。

Result: 系统在资源效率上优于现有实现，能耗降低42.3%，GPU内存使用减少11.2%，电池供电设备可运行LLaVA-OneVision近半天，LLaMA-3-8B语音交互达20.8小时。

Conclusion: NANOMIND框架通过硬件-软件协同设计和模块化调度，实现了在资源受限设备上高效运行大型多模态模型，为边缘智能设备提供了可行的解决方案。

Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision
and audio encoders, projectors, and large language models. Yet, they are almost
always executed monolithically, which underutilizes the heterogeneous
accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end
latency. In this paper, we present NANOMIND, a hardware--software co-design
inference framework for Large Multimodal Models (LMMs) that breaks large models
into modular ``bricks'' (vision, language, audio, etc.) and maps each to its
ideal accelerator. The key insight is that large models can be broken into
modular components and scheduled to run on the most appropriate compute units.
It performs module-level dynamic offloading across accelerators on
unified-memory SoCs. By combining customized hardware design, system-level
scheduling, and optimized low-bit computation kernels, we demonstrate our
framework with a compact, battery-powered device capable of running LMMs
entirely on device. This prototype functions as a self-contained intelligent
assistant that requires no network connectivity, while achieving higher
throughput and superior power efficiency under strict resource constraints. The
design further bypasses CPU bottlenecks and reduces redundant memory usage
through token-aware buffer management and module-level coordination. Our system
outperforms existing implementations in resource efficiency, cutting energy
consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a
battery-powered device to run LLaVA-OneVision with a camera for nearly half a
day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.

</details>


### [2] [Agora: Bridging the GPU Cloud Resource-Price Disconnect](https://arxiv.org/abs/2510.05111)
*Ian McDougall,Noah Scott,Joon Huh,Kirthevasan Kandasamy,Karthikeyan Sankaralingam*

Main category: cs.DC

TL;DR: 本文提出了一种基于特征的GPU云服务定价框架，以解决FLOPs与内存带宽性能增长不匹配导致的定价效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的浮点运算能力持续按摩尔定律增长，但内存带宽增长滞后，导致基于时间的传统定价模型对带宽密集型工作负载不经济，造成市场扭曲和硬件分配低效。

Method: 提出了基于特征的定价框架，将成本直接与资源消耗（包括内存带宽）挂钩，并设计了Agora系统架构来实现这一框架，支持50微秒和10微秒的采样频率。

Result: 实验表明，50微秒采样可实现近乎理想的定价效果，仅损失5%收入；10微秒采样效果更好，仅损失2.4%收入。现代遥测系统已能支持这种测量频率。

Conclusion: 该基于特征的定价方法能够为云GPU资源创建更透明高效的市场，在不同GPU应用和硬件代际上都得到了实证验证。

Abstract: The historic trend of Moore's Law, which predicted exponential growth in
computational performance per dollar, has diverged for modern Graphics
Processing Units (GPUs). While Floating Point Operations per Second (FLOPs)
capabilities have continued to scale economically, memory bandwidth has not,
creating a significant price-performance disconnect. This paper argues that the
prevailing time-based pricing models for cloud GPUs are economically
inefficient for bandwidth-bound workloads. These models fail to account for the
rising marginal cost of memory bandwidth, leading to market distortions and
suboptimal hardware allocation. To address this, we propose a novel
feature-based pricing framework that directly links cost to resource
consumption, including but not limited to memory bandwidth. We provide a robust
economic and algorithmic definition of this framework and introduce Agora, a
practical and secure system architecture for its implementation. Our
implementation of Agora shows that a 50us sampling provides nearly perfect
pricing as what ideal sampling would provide - losing only 5\% of revenue. 10us
sampling is even better result in 2.4\% loss. Modern telemetry systems can
already provide this rate of measurement, and our prototype implementation
shows the system design for feature-based pricing is buildable. Our evaluation
across diverse GPU applications and hardware generations empirically validates
the effectiveness of our approach in creating a more transparent and efficient
market for cloud GPU resources.

</details>


### [3] [A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training](https://arxiv.org/abs/2510.05112)
*Lijuan Jiang,Xingjian Qian,Zhenxiang Ma,Zan Zong,Hengjie Li,Chao Yang,Jidong Zhai*

Main category: cs.DC

TL;DR: FlexPipe是一个可编程的流水线并行框架，通过领域特定语言和自动调度器，实现了高效的流水线调度自动探索和灵活定制，相比现有框架获得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法依赖预定义调度策略，无法自动适应新兴模型架构，且手动实现调度代码负担重、灵活性差，需要更智能的自动化解决方案。

Method: 提出FlexPipe框架，包含简洁的领域特定语言(DSL)和自动调度器，支持广泛的调度类型自动探索，并提供灵活的微批次计算顺序控制机制。

Result: 相比主流大规模并行框架Megtron-LM获得最高2.28倍性能加速，相比最先进的自动流水线并行框架获得最高1.49倍性能加速。

Conclusion: FlexPipe通过自动化调度探索和灵活的程序化控制，显著提升了流水线并行的生产力和性能，为复杂DNN模型提供了高效的并行解决方案。

Abstract: Pipeline parallelism is an essential distributed parallelism method.
Increasingly complex and diverse DNN models necessitate meticulously customized
pipeline schedules for performance. However, existing practices typically rely
on predefined schedules, each with strengths, but fail to adapt automatically
to the emerging model architectures. Exploring novel high-efficiency schedules
is daunting due to the enormous and varying schedule space. Besides, manually
implementing schedules can be challenging due to the onerous coding burdens and
constantly changing needs. Unfortunately, existing frameworks have limitations
in automated schedule exploration and lack flexibility and controllability.
  This paper presents FlexPipe, a programmable pipeline parallelism framework
with enhanced productivity, programmability, debuggability, and ease of tuning.
FlexPipe has two main components: a succinct domain-specific language (DSL) and
an automated scheduler. FlexPipe enables automated schedule exploration for
various parallel scenarios within a broad spectrum of schedule types at a small
search cost. Besides, users can swiftly develop and customize schedules using
the FlexPipe DSL, which embodies flexible controllability in the pipeline order
of micro-batch computations over stages. It also provides convenient mechanisms
to include new operations in schedules to meet changing demands. Our evaluation
results demonstrate that FlexPipe achieves up to 2.28X performance speedup
compared to the popular large-scale parallel framework Megtron-LM, and gains up
to 1.49X performance speedup compared to the state-of-the-art automated
pipeline parallelism framework.

</details>


### [4] [Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum](https://arxiv.org/abs/2510.05118)
*Cynthia Marcelino,Noah Krennmair,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Lumos是一个用于分析无服务器运行时性能的模型和基准测试工具，重点研究WebAssembly在边缘云连续体中的性能表现。


<details>
  <summary>Details</summary>
Motivation: WebAssembly作为轻量级可移植运行时在无服务器函数执行中日益重要，特别是在边缘云连续体等异构和资源受限环境中，但其性能收益与权衡尚未得到充分理解。

Method: 开发Lumos性能模型和基准测试工具，识别工作负载、系统和环境层面的性能驱动因素，对最先进的容器和Wasm运行时（解释模式与预编译模式）进行基准测试。

Result: 预编译Wasm镜像比容器小30倍，冷启动延迟降低16%；但解释模式Wasm的暖延迟高出55倍，I/O序列化开销高出10倍。

Conclusion: Wasm在边缘云连续体中具有显著优势，特别是预编译模式在资源效率和冷启动方面表现优异，但解释模式存在性能瓶颈。

Abstract: WebAssembly has emerged as a lightweight and portable runtime to execute
serverless functions, particularly in heterogeneous and resource-constrained
environments such as the Edge Cloud Continuum. However, the performance
benefits versus trade-offs remain insufficiently understood. This paper
presents Lumos, a performance model and benchmarking tool for characterizing
serverless runtimes. Lumos identifies workload, system, and environment-level
performance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art
containers and the Wasm runtime in interpreted mode and with ahead-of-time
compilation. Our performance characterization shows that AoT-compiled Wasm
images are up to 30x smaller and decrease cold-start latency by up to 16%
compared to containers, while interpreted Wasm suffers up to 55x higher warm
latency and up to 10x I/O-serialization overhead.

</details>


### [5] [Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines](https://arxiv.org/abs/2510.05127)
*Harshit Goyal*

Main category: cs.DC

TL;DR: 使用随机森林回归预测大数据管道资源利用率的AI方法，在Google Borg集群数据上实现高精度预测(R²=0.99)，支持云环境成本感知的自动扩缩容。


<details>
  <summary>Details</summary>
Motivation: 现代云计算中资源分配面临挑战：过度配置导致不必要成本，配置不足则带来性能下降和SLA违规风险。

Method: 预处理Google Borg集群跟踪数据，清洗、转换并提取相关特征(CPU、内存、使用分布)，使用随机森林回归模型预测资源利用率。

Result: 模型达到高预测精度(R²=0.99，MAE=0.0048，RMSE=0.137)，能捕捉工作负载特征与资源利用率之间的非线性关系。对小到中型作业表现优异，大规模作业方差较高。

Conclusion: AI驱动的预测在云环境中具有成本感知自动扩缩容的潜力，既能减少不必要的资源配置，又能保障服务质量。

Abstract: Efficient resource allocation is a key challenge in modern cloud computing.
Over-provisioning leads to unnecessary costs, while under-provisioning risks
performance degradation and SLA violations. This work presents an artificial
intelligence approach to predict resource utilization in big data pipelines
using Random Forest regression. We preprocess the Google Borg cluster traces to
clean, transform, and extract relevant features (CPU, memory, usage
distributions). The model achieves high predictive accuracy (R Square = 0.99,
MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between
workload characteristics and resource utilization. Error analysis reveals
impressive performance on small-to-medium jobs, with higher variance in rare
large-scale jobs. These results demonstrate the potential of AI-driven
prediction for cost-aware autoscaling in cloud environments, reducing
unnecessary provisioning while safeguarding service quality.

</details>


### [6] [FlashResearch: Real-time Agent Orchestration for Efficient Deep Research](https://arxiv.org/abs/2510.05145)
*Lunyiu Nie,Nedim Lipka,Ryan A. Rossi,Swarat Chaudhuri*

Main category: cs.DC

TL;DR: FlashResearch是一个高效深度研究框架，通过将顺序推理转换为并行运行时编排，动态分解复杂查询为树状子任务，实现5倍加速同时保持质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理受限于顺序推理过程，导致高延迟、运行时适应性差和资源分配效率低，无法满足交互式应用需求。

Method: 提出三个核心贡献：自适应规划器动态分配计算资源；实时编排层监控研究进度并剪枝冗余路径；多维并行化框架支持研究广度和深度的并发。

Result: 实验表明FlashResearch在固定时间预算内持续提升最终报告质量，在保持可比质量的同时实现高达5倍的加速。

Conclusion: FlashResearch通过并行运行时编排有效解决了深度研究代理的顺序推理瓶颈，为交互式应用提供了实用的解决方案。

Abstract: Deep research agents, which synthesize information across diverse sources,
are significantly constrained by their sequential reasoning processes. This
architectural bottleneck results in high latency, poor runtime adaptability,
and inefficient resource allocation, making them impractical for interactive
applications. To overcome this, we introduce FlashResearch, a novel framework
for efficient deep research that transforms sequential processing into
parallel, runtime orchestration by dynamically decomposing complex queries into
tree-structured sub-tasks. Our core contributions are threefold: (1) an
adaptive planner that dynamically allocates computational resources by
determining research breadth and depth based on query complexity; (2) a
real-time orchestration layer that monitors research progress and prunes
redundant paths to reallocate resources and optimize efficiency; and (3) a
multi-dimensional parallelization framework that enables concurrency across
both research breadth and depth. Experiments show that FlashResearch
consistently improves final report quality within fixed time budgets, and can
deliver up to a 5x speedup while maintaining comparable quality.

</details>


### [7] [Percepta: High Performance Stream Processing at the Edge](https://arxiv.org/abs/2510.05149)
*Clarisse Sousa,Tiago Fonseca,Luis Lino Ferreira,Ricardo Venâncio,Ricardo Severino*

Main category: cs.DC

TL;DR: Percepta是一个轻量级数据流处理系统，专为边缘AI工作负载设计，特别支持强化学习，具备奖励函数计算、模型重训练数据存储和实时数据准备等功能。


<details>
  <summary>Details</summary>
Motivation: 实时数据和物联网设备的增长暴露了云中心解决方案在延迟、带宽和隐私方面的局限性，推动了边缘计算的发展。物联网带来的数据速率协调、协议转换、数据丢失处理和AI模型集成等问题需要解决。

Method: 开发了Percepta系统，提供数据标准化、异构协议和采样率协调、缺失数据处理等专门功能，支持奖励函数计算和模型重训练数据存储。

Result: Percepta系统能够有效处理边缘AI部署中的挑战，支持连续决策制定，特别适合强化学习等AI工作负载。

Conclusion: Percepta作为一个轻量级DSP系统，成功解决了边缘计算中AI部署面临的数据处理挑战，为边缘AI应用提供了有效的解决方案。

Abstract: The rise of real-time data and the proliferation of Internet of Things (IoT)
devices have highlighted the limitations of cloud-centric solutions,
particularly regarding latency, bandwidth, and privacy. These challenges have
driven the growth of Edge Computing. Associated with IoT appears a set of other
problems, like: data rate harmonization between multiple sources, protocol
conversion, handling the loss of data and the integration with Artificial
Intelligence (AI) models. This paper presents Percepta, a lightweight Data
Stream Processing (DSP) system tailored to support AI workloads at the edge,
with a particular focus on such as Reinforcement Learning (RL). It introduces
specialized features such as reward function computation, data storage for
model retraining, and real-time data preparation to support continuous
decision-making. Additional functionalities include data normalization,
harmonization across heterogeneous protocols and sampling rates, and robust
handling of missing or incomplete data, making it well suited for the
challenges of edge-based AI deployment.

</details>


### [8] [SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading](https://arxiv.org/abs/2510.05164)
*Yuanzhe Shen,Yide Liu,Zisu Huang,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.DC

TL;DR: SATER是一个双模式兼容的方法，通过最短响应偏好优化和置信度感知拒绝机制来微调模型，显著减少冗余输出和响应时间，同时提高预生成路由的性能和级联路由的效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)虽然性能出色但成本高昂，而小型语言模型(SLMs)成本低但能力有限。现有路由策略各有优缺点，需要一种能平衡性能与成本的新方法。

Method: 提出SATER方法，通过最短响应偏好优化和置信度感知拒绝机制来微调模型，兼容预生成路由和级联路由两种模式。

Result: 在三个SLMs和六个不同复杂度的数据集上的实验表明，SATER在保持可比性能的同时，计算成本降低超过50%，级联延迟降低超过80%。

Conclusion: SATER方法有效解决了LLM部署中的性能与成本权衡问题，显著提升了路由策略的效率。

Abstract: Large language models (LLMs) demonstrate remarkable performance across
diverse tasks, yet their effectiveness frequently depends on costly commercial
APIs or cloud services. Model selection thus entails a critical trade-off
between performance and cost: high-performing LLMs typically incur substantial
expenses, whereas budget-friendly small language models (SLMs) are constrained
by limited capabilities. Current research primarily proposes two routing
strategies: pre-generation routing and cascade routing. Both approaches have
distinct characteristics, with cascade routing typically offering superior
cost-effectiveness and accuracy despite its higher latency. To further address
the limitations of both approaches, we introduce SATER, a dual-mode compatible
approach that fine-tunes models through shortest-response preference
optimization and a confidence-aware rejection mechanism. SATER significantly
reduces redundant outputs and response times, while improving both the
performance of pre-generation routing and the efficiency of cascade routing.
Experiments across three SLMs and six datasets, varying in type and complexity,
demonstrate that SATER achieves comparable performance while consistently
reducing computational costs by over 50\% and cascade latency by over 80\%.

</details>


### [9] [OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training](https://arxiv.org/abs/2510.05186)
*Hongpei Li,Han Zhang,Huikang Liu,Dongdong Ge,Yinyu Ye*

Main category: cs.DC

TL;DR: 提出了一种基于优化视角的流水线调度方法，通过联合考虑内存容量、激活重用和流水线气泡最小化，动态优化内存与时间的权衡，相比现有方法显著提升了吞吐量和内存利用率。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方法主要依赖启发式规则和粗粒度的激活卸载策略，未能充分考虑内存约束与调度效率之间的精细权衡，存在优化空间。

Method: 将调度问题建模为约束优化问题，联合考虑内存容量、激活重用和流水线气泡最小化，根据模型结构和硬件配置动态优化内存与时间的权衡。

Result: 实验结果表明，该方法在相同设备内存限制下可将空闲流水线时间减少高达50%，在某些情况下还能在有限内存预算内训练更大的模型。

Conclusion: 提出的优化方法能够显著提升流水线并行训练的效率，通过精细调度实现更好的内存利用和吞吐量表现。

Abstract: Pipeline parallelism (PP) has become a standard technique for scaling large
language model (LLM) training across multiple devices. However, despite recent
progress in reducing memory consumption through activation offloading, existing
approaches remain largely heuristic and coarse-grained, often overlooking the
fine-grained trade-offs between memory, computation, and scheduling latency. In
this work, we revisit the pipeline scheduling problem from a principled
optimization perspective. We observe that prevailing strategies either rely on
static rules or aggressively offload activations without fully leveraging the
interaction between memory constraints and scheduling efficiency. To address
this, we formulate scheduling as a constrained optimization problem that
jointly accounts for memory capacity, activation reuse, and pipeline bubble
minimization. Solving this model yields fine-grained schedules that reduce
pipeline bubbles while adhering to strict memory budgets. Our approach
complements existing offloading techniques: whereas prior approaches trade
memory for time in a fixed pattern, we dynamically optimize the tradeoff with
respect to model structure and hardware configuration. Experimental results
demonstrate that our method consistently improves both throughput and memory
utilization. In particular, we reduce idle pipeline time by up to 50% under the
same per-device memory limit, and in some cases, enable the training of larger
models within limited memory budgets.

</details>


### [10] [Performance of a high-order MPI-Kokkos accelerated fluid solver](https://arxiv.org/abs/2510.05254)
*Filipp Sporykhin,Holger Homann*

Main category: cs.DC

TL;DR: 该论文研究了现代高性能计算架构上流体动力学问题的数值方案性能，发现高阶（八阶）模拟比低阶（三、四阶）模拟在相同全局误差下计算时间更少，GPU在大规模网格模拟中性能优于CPU但能耗更高


<details>
  <summary>Details</summary>
Motivation: 研究现代高性能计算架构（多核CPU和GPU）上流体动力学数值方案的性能表现，特别关注高阶数值方法在不同硬件平台上的计算效率和能耗特性

Method: 使用Kokkos库和MPI实现空间节点间断Galerkin方案（最高八阶）与Runge-Kutta时间积分方法（最高六阶）的耦合，在CPU和GPU系统上测试线性对流方程和等温Euler方程

Result: 高阶模拟计算效率更高，八阶方案比三、四阶方案获得相同全局误差所需计算时间更少；GPU在大规模网格（超过10^7自由度）模拟中性能显著优于CPU，但小网格模拟CPU更快；现代GPU需要更大网格才能高效利用，存在反弹效应

Conclusion: 高阶数值方法在现代硬件上具有显著性能优势，GPU适合大规模模拟但能耗较高，硬件选择需根据问题规模权衡性能与能耗

Abstract: This work discusses the performance of a modern numerical scheme for fluid
dynamical problems on modern high-performance computing architectures. Our code
implements a spatial nodal discontinuous Galerkin scheme that we test up to an
order of convergence of eight. It is temporally coupled to a set of Runge-Kutta
methods of orders up to six. The code integrates the linear advection equations
as well as the isothermal Euler equations in one, two, and three dimensions. In
order to target modern hardware involving many-core Central Processing Units
and accelerators such as Graphic Processing Units we use the Kokkos library in
conjunction with the Message Passing Interface to run our single source code on
various GPU systems. We find that the higher the order the faster is the code.
Eighth-order simulations attain a given global error with much less computing
time than third- or fourth-order simulations. The RK scheme has a smaller
impact on the code performance and a classical fourth-order scheme seems to
generally be a good choice. The code performs very well on all considered GPUs.
The many-CPU performance is also very good and perfect weak scaling is observed
up to many hundreds of CPU cores using MPI. We note that small grid-size
simulations are faster on CPUs than on GPUs while GPUs win significantly over
CPUs for simulations involving more than $10^7$ degrees of freedom ($\approx
3100^2$ grid points). When it comes to the environmental impact of numerical
simulations we estimate that GPUs consume less energy than CPUs for large
grid-size simulations but more energy on small grids. We observe a tendency
that the more modern is the GPU the larger needs to be the grid in order to use
it efficiently. This yields a rebound effect because larger simulations need
longer computing times and in turn more energy that is not compensated by the
energy efficiency gain of the newer GPUs.

</details>


### [11] [cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications](https://arxiv.org/abs/2510.05476)
*Xi Wang,Bin Ma,Jongryool Kim,Byungil Koh,Hoshik Kim,Dong Li*

Main category: cs.DC

TL;DR: cMPI是首个在真实CXL平台上使用CXL内存共享优化MPI点对点通信的工作，将跨节点通信转换为CXL内存内的内存事务和数据拷贝，绕过传统网络协议。


<details>
  <summary>Details</summary>
Motivation: 传统MPI库使用复杂的网络互连和协议栈进行跨节点通信，存在性能瓶颈。CXL内存共享技术提供了新的通信优化机会。

Method: 利用CXL内存共享技术，将MPI通信转换为CXL内存内的内存事务和数据拷贝，解决数据对象管理、缓存一致性和原子操作等挑战。

Result: CXL内存共享比TCP互连延迟降低7.2-8.1倍，在小型消息传输中，相比标准以太网NIC和高端SmartNIC，延迟和带宽分别提升49倍和72倍。

Conclusion: CXL内存共享为MPI通信提供了显著的性能提升，是高性能计算通信优化的有效方法。

Abstract: Message Passing Interface (MPI) is a foundational programming model for
high-performance computing. MPI libraries traditionally employ network
interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP
and RoCE) with complex software stacks for cross-node communication. We present
cMPI, the first work to optimize MPI point-to-point communication (both
one-sided and two-sided) using CXL memory sharing on a real CXL platform,
transforming cross-node communication into memory transactions and data copies
within CXL memory, bypassing traditional network protocols. We analyze
performance across various interconnects and find that CXL memory sharing
achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in
small- and medium-scale clusters. We address challenges of CXL memory sharing
for MPI communication, including data object management over the dax
representation [50], cache coherence, and atomic operations. Overall, cMPI
outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x
and 72x in latency and bandwidth, respectively, for small messages.

</details>


### [12] [Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting](https://arxiv.org/abs/2510.05497)
*Zhongkai Yu,Yue Guan,Zihao Yu,Chenyang Zhou,Shuyi Pei,Yangwook Kang,Yufei Ding,Po-An Tsai*

Main category: cs.DC

TL;DR: 该论文对MoE架构LLM的数据移动模式进行了全面分析，提出了6个关键洞察，并在晶圆级GPU上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: MoE架构LLM的随机专家选择机制在多单元服务系统中引入了显著的数据移动开销，成为主要性能瓶颈，需要深入分析其模式以优化系统设计。

Method: 对三个最先进的大规模MoE模型（200B-671B）进行数据移动为中心的分析，使用超过24,000个请求，涵盖多样化工作负载，生成150GB+的跟踪文件，从时间和空间角度进行系统分析。

Result: 基于分析洞察对晶圆级GPU进行微小架构修改，在DeepSeek V3和Qwen3上分别实现了6.3倍和4.0倍的平均加速。

Conclusion: 这是首个大规模MoE模型的全面数据为中心分析，提供了指导未来服务系统设计的关键洞察，相关分析数据和模拟框架将公开以促进该领域研究。

Abstract: Large Language Models (LLMs) with Mixture of Experts (MoE) architectures
achieve remarkable performance improvements, but their random expert selection
mechanism introduces significant data movement overhead that becomes the
dominant bottleneck in multi-unit serving systems. To forecast the patterns
underlying this data movement, we conduct comprehensive data-movement-centric
profiling across three state-of-the-art large-scale MoE models (200B- 671B)
using over 24,000 requests spanning diverse workloads. With the resulting
150GB+ trace files, we perform systematic analysis from both temporal and
spatial perspectives and distill six key insights to guide the design of
diverse future serving systems. Taking wafer-scale GPUs as a case study, we
demonstrate that minor architectural modifications leveraging our insights
achieve substantial performance gains, delivering 6.3X and 4.0X average
speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first
comprehensive data-centric analysis of MoE models at scale. Our profiling
traces and analysis results are publicly available at
{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will
also release our simulation framework shortly to facilitate future research in
this area.

</details>


### [13] [Toward Systems Foundations for Agentic Exploration](https://arxiv.org/abs/2510.05556)
*Jiakai Xu,Tianle Zhou,Eugene Wu,Kostis Kaffes*

Main category: cs.DC

TL;DR: 该论文分析了LLM智能体探索中快照/恢复机制的不足，指出了三个核心挑战：分支语义、外部副作用和原生分叉。


<details>
  <summary>Details</summary>
Motivation: 当前通用的快照/恢复工具（如CRIU或容器提交）在智能体探索场景中性能不足，特别是在真实部署环境中，当智能体与其他智能体和用户共享文件、套接字和云API时，这些机制完全失效。

Method: 通过基准测试六种快照/恢复机制，评估它们在隔离测试环境和真实部署环境中的表现。

Result: 研究发现通用工具即使在隔离测试环境中也不够快，在真实部署环境中完全失效。

Conclusion: 论文识别了三个开放的基础挑战：分支语义问题、外部副作用处理和原生分叉需求，这些挑战需要在微秒级别克隆数据库和运行时而不进行批量复制。

Abstract: Agentic exploration, letting LLM-powered agents branch, backtrack, and search
across many execution paths, demands systems support well beyond today's
pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that
generic tools such as CRIU or container commits are not fast enough even in
isolated testbeds, and they crumble entirely in real deployments where agents
share files, sockets, and cloud APIs with other agents and human users. In this
talk, we pinpoint three open fundamental challenges: fork semantics, which
concerns how branches reveal or hide tentative updates; external side-effects,
where fork awareness must be added to services or their calls intercepted; and
native forking, which requires cloning databases and runtimes in microseconds
without bulk copying.

</details>


### [14] [Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems](https://arxiv.org/abs/2510.05621)
*Zhiyuan Ren,Tao Zhang,Wenchi Chen*

Main category: cs.DC

TL;DR: 提出了确定性因果结构（DCS）作为分布式多智能体系统的形式化基础，将正确性与操作策略解耦，确保策略演进不会破坏完整性保证。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中正确性常与调度、批处理等操作策略纠缠，导致系统脆弱，性能驱动的策略演进可能破坏完整性保证。

Method: 开发了最小公理化理论，证明了四个结果：存在性与唯一性、策略无关不变性、观测等价性和公理最小性。

Result: DCS解决了CRDT等值中心收敛模型无法处理的因果模糊性，移除任何公理都会使确定性崩溃为模糊性。

Conclusion: DCS确立了正确性作为固定、策略无关的基础，形成"正确性即底盘"范式，可在其上模块化、安全、可演进地构建分布式智能系统。

Abstract: In distributed multi-agent systems, correctness is often entangled with
operational policies such as scheduling, batching, or routing, which makes
systems brittle since performance-driven policy evolution may break integrity
guarantees. This paper introduces the Deterministic Causal Structure (DCS), a
formal foundation that decouples correctness from policy. We develop a minimal
axiomatic theory and prove four results: existence and uniqueness,
policy-agnostic invariance, observational equivalence, and axiom minimality.
These results show that DCS resolves causal ambiguities that value-centric
convergence models such as CRDTs cannot address, and that removing any axiom
collapses determinism into ambiguity. DCS thus emerges as a boundary principle
of asynchronous computation, analogous to CAP and FLP: correctness is preserved
only within the expressive power of a join-semilattice. All guarantees are
established by axioms and proofs, with only minimal illustrative constructions
included to aid intuition. This work establishes correctness as a fixed,
policy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which
distributed intelligent systems can be built modularly, safely, and evolvably.

</details>


### [15] [Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium](https://arxiv.org/abs/2510.05711)
*Ailiya Borjigin,Cong He*

Main category: cs.DC

TL;DR: 本文介绍了时间流动性溢价(TLP)概念，为时间绑定稳定币建立无套利定价模型和动态风险控制机制，通过调整LTV比率来管理跨市场流动性。


<details>
  <summary>Details</summary>
Motivation: 解决传统证券在市场闭市期间的流动性问题，通过时间绑定稳定币实现跨市场连续流动性，减少时间维度的市场低效率。

Method: 结合金融工程（无套利条件、期权式定价）和实证金融（交叉上市股票和期货的事件研究），构建定价模型和动态LTV调整机制。

Result: TLP随闭市时间和波动率增长，但可通过自适应LTV控制；提供了回测结果和多种可视化分析工具。

Conclusion: 时间绑定稳定币是减少时间维度市场低效率的有效工具，为未来研究和部署提供了理论框架和实践指导。

Abstract: Time-bound stablecoins are DeFi assets that temporarily tokenize traditional
securities during market off-hours, enabling continuous cross-market liquidity.
We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of
providing liquidity when the primary market is closed. We build a no-arbitrage
pricing model that yields a band for fair values over different expiries, and a
dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real
time to keep TLP within a target range. Our analysis blends financial
engineering (no-arbitrage conditions, option-style pricing) with empirical
finance (event studies on cross-listed stocks and futures) to measure TLP under
time-zone frictions. We define TLP formally, derive closed-form expressions for
its term structure under idealized assumptions, and simulate scenarios that
vary volatility and collateralization. We then propose an LTV policy that
raises or lowers collateral to expand or curtail time-bound stablecoin supply,
analogous to a central bank adjusting rates to defend a peg. We outline
empirical proxies for TLP, including ADR premiums, overseas index futures
versus cash index divergence, and pre-market versus official close gaps.
Results show that TLP grows with closure length and volatility, yet can be
contained by adaptive LTV. We provide backtests and figures (term-structure
curves, capital-efficiency versus tail-risk trade-offs, time-liquidity
heatmaps) and discuss protocol design (vault structure, closing-price oracles,
on-chain auction liquidations). The findings position time-bound stablecoins as
a tool to reduce temporal market inefficiencies and inform future research and
deployment.

</details>


### [16] [A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications](https://arxiv.org/abs/2510.05738)
*Ritesh Chandra,Sonali Agarwal,Navjot Singh,Sadhana Tiwari*

Main category: cs.DC

TL;DR: 本文系统综述了本体驱动的语义数据管理在医疗大数据分析中的应用，将相关研究分为六大类别，探讨了本体技术与大数据框架的结合潜力。


<details>
  <summary>Details</summary>
Motivation: 医疗数据的爆炸式增长（来自电子健康记录、医学影像、可穿戴设备等）推动了数据湖和集中式架构的采用，但缺乏有效治理会导致数据沼泽问题。本体驱动的语义数据管理能够通过将元数据链接到医疗知识图谱来增强语义互操作性。

Method: 采用系统性研究策略，制定关键研究问题，在主要学术数据库中进行结构化文献检索，将选定研究分析并分类为六大类本体驱动的医疗分析类别。

Result: 识别了六类本体驱动的医疗分析：本体驱动集成框架、语义建模元数据丰富、基于本体的数据访问、基本语义数据管理、基于本体的推理决策支持、非结构化数据语义标注。

Conclusion: 本体技术与大数据框架（如Hadoop、Spark、Kafka等）的结合具有提供可扩展智能医疗分析的潜力，需要关注人工智能、机器学习、物联网和实时分析等新兴趋势，以指导可持续、互操作和高性能医疗数据生态系统的发展。

Abstract: Exponential growth in heterogeneous healthcare data arising from electronic
health records (EHRs), medical imaging, wearable sensors, and biomedical
research has accelerated the adoption of data lakes and centralized
architectures capable of handling the Volume, Variety, and Velocity of Big Data
for advanced analytics. However, without effective governance, these
repositories risk devolving into disorganized data swamps. Ontology-driven
semantic data management offers a robust solution by linking metadata to
healthcare knowledge graphs, thereby enhancing semantic interoperability,
improving data discoverability, and enabling expressive, domain-aware access.
This review adopts a systematic research strategy, formulating key research
questions and conducting a structured literature search across major academic
databases, with selected studies analyzed and classified into six categories of
ontology-driven healthcare analytics: (i) ontology-driven integration
frameworks, (ii) semantic modeling for metadata enrichment, (iii)
ontology-based data access (OBDA), (iv) basic semantic data management, (v)
ontology-based reasoning for decision support, and (vi) semantic annotation for
unstructured data. We further examine the integration of ontology technologies
with Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting
their combined potential to deliver scalable and intelligent healthcare
analytics. For each category, recent techniques, representative case studies,
technical and organizational challenges, and emerging trends such as artificial
intelligence, machine learning, the Internet of Things (IoT), and real-time
analytics are reviewed to guide the development of sustainable, interoperable,
and high-performance healthcare data ecosystems.

</details>


### [17] [EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models](https://arxiv.org/abs/2510.05943)
*Zheyue Tan,Mustapha Abdullahi,Tuo Shi,Huining Yuan,Zelai Xu,Chao Yu,Boxun Li,Bo Zhao*

Main category: cs.DC

TL;DR: EARL是一个用于高效代理强化学习的可扩展系统，通过动态并行选择器和数据分发器来解决长上下文训练中的内存和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决代理强化学习训练中上下文长度快速增长导致的内存使用膨胀、延迟增加和内存不足故障，以及中间张量积累造成的跨设备数据移动瓶颈。

Method: 设计并行选择器根据序列长度和系统负载动态调整模型和训练并行度，以及数据分发器执行布局感知的去中心化中间数据批次交换。

Result: 提高了吞吐量，减少了长上下文故障，实现了代理大语言模型的稳定大规模训练，无需依赖上下文长度的硬限制或惩罚。

Conclusion: EARL系统通过动态并行化和高效数据管理，成功解决了代理强化学习训练中的可扩展性问题。

Abstract: Reinforcement learning (RL) has become a pivotal component of large language
model (LLM) post-training, and agentic RL extends this paradigm to operate as
agents through multi-turn interaction and tool use. Scaling such systems
exposes two practical bottlenecks: (1) context length grows rapidly during
training, inflating memory usage and latency, and triggering out-of-memory
(OOM) failures; and (2) intermediate tensors accumulate with context length,
making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a
parallelism selector that dynamically adapts model and training parallelism
across RL stages based on sequence length and system load, and a data
dispatcher that performs layout-aware, decentralized exchange of intermediate
data batches. Together, these components increase throughput, reduce
long-context failures, and enable stable large-scale training of agentic LLMs
without relying on hard limits or penalties of context length.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing](https://arxiv.org/abs/2510.05147)
*Yu Zhu*

Main category: cs.SE

TL;DR: 提出了一种基于强化学习的自适应配置分配框架，将测试资源配置问题建模为序列决策问题，结合Q学习和混合奖励设计，在动态环境中优于传统静态方法。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统需要在异构且不断变化的环境中进行严格测试，但传统组合优化方法是静态的，无法适应故障概率随时间漂移的非平稳环境。

Method: 使用强化学习框架，将配置分配重构为序列决策问题，结合Q学习和融合模拟结果与实时反馈的混合奖励设计，采用自适应在线-离线训练方案。

Result: 广泛的仿真研究表明，该方法持续优于静态和基于优化的基线方法，接近oracle性能。

Conclusion: 这项工作确立了强化学习作为自适应配置分配的新范式，超越了传统方法，在动态测试和资源调度领域具有广泛适用性。

Abstract: Ensuring reliability in modern software systems requires rigorous
pre-production testing across highly heterogeneous and evolving environments.
Because exhaustive evaluation is infeasible, practitioners must decide how to
allocate limited testing resources across configurations where failure
probabilities may drift over time. Existing combinatorial optimization
approaches are static, ad hoc, and poorly suited to such non-stationary
settings. We introduce a novel reinforcement learning (RL) framework that
recasts configuration allocation as a sequential decision-making problem. Our
method is the first to integrate Q-learning with a hybrid reward design that
fuses simulated outcomes and real-time feedback, enabling both sample
efficiency and robustness. In addition, we develop an adaptive online-offline
training scheme that allows the agent to quickly track abrupt probability
shifts while maintaining long-run stability. Extensive simulation studies
demonstrate that our approach consistently outperforms static and
optimization-based baselines, approaching oracle performance. This work
establishes RL as a powerful new paradigm for adaptive configuration
allocation, advancing beyond traditional methods and offering broad
applicability to dynamic testing and resource scheduling domains.

</details>


### [19] [VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation](https://arxiv.org/abs/2510.05156)
*Lesly Miculicich,Mihir Parmar,Hamid Palangi,Krishnamurthy Dj Dvijotham,Mirko Montanari,Tomas Pfister,Long T. Le*

Main category: cs.SE

TL;DR: VeriGuard是一个为LLM智能体提供形式化安全保障的双阶段框架，通过离线验证和在线监控确保智能体行为符合安全约束。


<details>
  <summary>Details</summary>
Motivation: 在医疗等敏感领域部署自主AI智能体存在严重的安全、隐私和对抗攻击风险，现有系统无法完全保证智能体行为符合预设安全约束。

Method: 采用双阶段架构：离线阶段通过明确用户意图、合成行为策略并进行形式化验证来确保策略正确性；在线阶段作为运行时监控器验证每个拟执行动作。

Result: 该框架能够实际应用形式化保证，显著提高LLM智能体的可信度。

Conclusion: VeriGuard通过分离离线验证和在线监控，为LLM智能体提供了强大且可验证的安全保障机制。

Abstract: The deployment of autonomous AI agents in sensitive domains, such as
healthcare, introduces critical risks to safety, security, and privacy. These
agents may deviate from user objectives, violate data handling policies, or be
compromised by adversarial attacks. Mitigating these dangers necessitates a
mechanism to formally guarantee that an agent's actions adhere to predefined
safety constraints, a challenge that existing systems do not fully address. We
introduce VeriGuard, a novel framework that provides formal safety guarantees
for LLM-based agents through a dual-stage architecture designed for robust and
verifiable correctness. The initial offline stage involves a comprehensive
validation process. It begins by clarifying user intent to establish precise
safety specifications. VeriGuard then synthesizes a behavioral policy and
subjects it to both testing and formal verification to prove its compliance
with these specifications. This iterative process refines the policy until it
is deemed correct. Subsequently, the second stage provides online action
monitoring, where VeriGuard operates as a runtime monitor to validate each
proposed agent action against the pre-verified policy before execution. This
separation of the exhaustive offline validation from the lightweight online
monitoring allows formal guarantees to be practically applied, providing a
robust safeguard that substantially improves the trustworthiness of LLM agents.

</details>


### [20] [Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework](https://arxiv.org/abs/2510.05365)
*Irtaza Sajid Qureshi,Zhen Ming,Jiang*

Main category: cs.SE

TL;DR: 本文系统评估了LLM在测试用例生成中的推理能力，基于Bloom认知分类学框架，发现LLM在记忆层面表现良好，但在应用层面面临严重挑战，特别是在标识符突变时性能下降超过60%。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在软件测试中的泛化能力，探究其是否能超越记忆模式并真正理解自然语言错误报告。

Method: 基于LIBRO框架，使用Bloom认知分类学的六个层次（记忆、理解、应用、分析、评估、创造），在Defects4J、GHRB数据集及其变体上评估StarCoder和GPT-4o模型。

Result: LLM能够复现先前结果（记忆层），对语言重述和翻译具有部分鲁棒性（理解层），但在标识符突变时性能下降超过60%（应用层）。开放书本设置下的少样本学习可将成功率提高三倍，结构化技术元素比叙述性描述对测试生成更重要。

Conclusion: 研究揭示了LLM生成测试的认知过程，为改进性能提供了具体方向，并为此任务建立了稳健现实的评估范式。

Abstract: Large Language Models (LLMs) are increasingly applied to automated software
testing, yet their ability to generalize beyond memorized patterns and reason
about natural language bug reports remains unclear. We present a systematic
evaluation of LLM reasoning in test case generation, structured around the
cognitive layers of Bloom's taxonomy: \textit{Remember}, \textit{Understand},
\textit{Apply}, \textit{Analyze}, \textit{Evaluate}, and \textit{Create}, which
progressively assess higher levels of cognitive and reasoning capabilities.
Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,
GHRB, and mutated variants that introduce linguistic and semantic challenges.
Our findings show that both models largely reproduce prior results with minor
deviations (\textit{Remember}), exhibit partial robustness to linguistic
rephrasings and translations while uncovering unique reproducible bugs
(\textit{Understand}), but suffer severe performance drops exceeding 60\% under
identifier mutations (\textit{Apply}). Conversely, providing near-identical
few-shot examples in an open-book setting improves success rates by up to three
times, and component-level analysis reveals that structured technical elements,
such as test code and method names, are far more impactful than narrative
descriptions for successful test generation (\textit{Analyze}). These insights
illuminate the cognitive processes underlying LLM-generated tests, suggest
concrete directions for improving performance, and establish a robust and
realistic evaluation paradigm for this task.

</details>


### [21] [Who Do You Think You Are? Creating RSE Personas from GitHub Interactions](https://arxiv.org/abs/2510.05390)
*Felicity Anderson,Julien Sindt,Neil Chue Hong*

Main category: cs.SE

TL;DR: 本文提出了一种结合软件仓库挖掘和数据驱动角色的方法，用于识别研究软件工程中的常见和罕见开发模式，通过对GitHub上115,174名贡献者的分析识别出7种不同交互程度的角色类型。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程缺乏系统化的角色描述方法，难以理解贡献者的行为模式和项目动态，这阻碍了RSE的改进和发展。

Method: 结合软件仓库挖掘和数据驱动角色方法，分析GitHub上1,284个研究软件仓库的贡献者行为模式，从42,284个候选仓库中抽样研究。

Result: 成功识别并命名了7种从低到高交互程度的角色：临时贡献者、偶尔贡献者、项目组织者、适度贡献者、低流程完成者、低编码完成者和活跃贡献者。

Conclusion: 该方法证明了大数据集分析在比较具有不同项目管理因素、研究领域和贡献者背景的软件项目方面的可行性，为研究软件工程提供了重要基础。

Abstract: We describe data-driven RSE personas: an approach combining software
repository mining and data-driven personas applied to research software (RS),
an attempt to describe and identify common and rare patterns of Research
Software Engineering (RSE) development. This allows individuals and RS project
teams to understand their contributions, impact and repository dynamics - an
important foundation for improving RSE. We evaluate the method on different
patterns of collaborative interaction behaviours by contributors to mid-sized
public RS repositories (those with 10-300 committers) on GitHub. We demonstrate
how the RSE personas method successfully characterises a sample of 115,174
repository contributors across 1,284 RS repositories on GitHub, sampled from
42,284 candidate software repository records queried from Zenodo. We identify,
name and summarise seven distinct personas from low to high interactivity:
Ephemeral Contributor; Occasional Contributor; Project Organiser; Moderate
Contributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.
This demonstrates that large datasets can be analysed despite difficulties of
comparing software projects with different project management factors, research
domains and contributor backgrounds.

</details>


### [22] [UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification](https://arxiv.org/abs/2510.05441)
*Yiannis Charalambous,Claudionor N. Coelho Jr,Luis Lamb,Lucas C. Cordeiro*

Main category: cs.SE

TL;DR: UnitTenX是一个开源AI多智能体系统，用于为遗留代码生成单元测试，提高测试覆盖率和关键值测试。


<details>
  <summary>Details</summary>
Motivation: 解决复杂遗留代码库的测试生成挑战，提高软件可靠性和可维护性。

Method: 结合AI智能体、形式化方法和大型语言模型(LLMs)来自动化测试生成。

Result: 能够生成高质量测试并识别潜在问题，同时提高遗留代码的可读性和文档化。

Conclusion: 尽管LLMs在错误检测方面存在局限性，但UnitTenX为改善软件可靠性提供了稳健框架。

Abstract: This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent
system designed to generate unit tests for legacy code, enhancing test coverage
and critical value testing. UnitTenX leverages a combination of AI agents,
formal methods, and Large Language Models (LLMs) to automate test generation,
addressing the challenges posed by complex and legacy codebases. Despite the
limitations of LLMs in bug detection, UnitTenX offers a robust framework for
improving software reliability and maintainability. Our results demonstrate the
effectiveness of this approach in generating high-quality tests and identifying
potential issues. Additionally, our approach enhances the readability and
documentation of legacy code.

</details>


### [23] [What Types of Code Review Comments Do Developers Most Frequently Resolve?](https://arxiv.org/abs/2510.05450)
*Saul Goldman,Hong Yi Lin,Jirat Pasuksmit,Patanamon Thongtanunam,Kla Tantithamthavorn,Zhe Wang,Ray Zhang,Ali Behnaz,Fan Jiang,Michael Siers,Ryan Jiang,Mike Buller,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: 研究LLM生成的代码审查评论中哪些类型最可能触发代码变更，发现可读性、bug和维护性相关的评论比代码设计相关的评论更易被开发者采纳。


<details>
  <summary>Details</summary>
Motivation: 理解LLM生成的代码审查评论中哪些类型最可能被开发者采纳并触发代码变更，这对于识别可操作的评论至关重要。

Method: 开发了LLM-as-a-Judge方法，基于五类分类法自动分类审查评论，并进行实证研究分析人类和LLM评论者的表现差异。

Result: LLM和人类审查者在不同项目背景下各有优势；可读性、bug和维护性相关的评论解决率高于代码设计相关的评论。

Conclusion: 大部分LLM生成的评论是可操作的，LLM和人类审查者具有互补性，研究为提高LLM驱动的代码审查工具的实际效果提供了建议。

Abstract: Large language model (LLM)-powered code review automation tools have been
introduced to generate code review comments. However, not all generated
comments will drive code changes. Understanding what types of generated review
comments are likely to trigger code changes is crucial for identifying those
that are actionable. In this paper, we set out to investigate (1) the types of
review comments written by humans and LLMs, and (2) the types of generated
comments that are most frequently resolved by developers. To do so, we
developed an LLM-as-a-Judge to automatically classify review comments based on
our own taxonomy of five categories. Our empirical study confirms that (1) the
LLM reviewer and human reviewers exhibit distinct strengths and weaknesses
depending on the project context, and (2) readability, bugs, and
maintainability-related comments had higher resolution rates than those focused
on code design. These results suggest that a substantial proportion of
LLM-generated comments are actionable and can be resolved by developers. Our
work highlights the complementarity between LLM and human reviewers and offers
suggestions to improve the practical effectiveness of LLM-powered code review
tools.

</details>


### [24] [An Empirical Study of Security-Policy Related Issues in Open Source Projects](https://arxiv.org/abs/2510.05604)
*Rintaro Kanaji,Brittany Reid,Yutaro Kashiwa,Raula Gaikovina Kula,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究分析了GitHub项目中SECURITY.md文件在漏洞报告过程中的有效性和操作挑战，通过对711个相关issue的分类分析发现79.5%是添加文件的请求，包含链接的报告关闭时间中位数缩短2天。


<details>
  <summary>Details</summary>
Motivation: 理解SECURITY.md文件在开源社区漏洞报告过程中的实际效果和操作挑战，为改进安全报告政策和社区管理提供依据。

Method: 对711个随机抽样的SECURITY.md相关issue进行分类分析，并对包括SECURITY.md在内的6个社区健康文件的关闭时间和响应次数进行定量比较分析。

Result: 79.5%的SECURITY.md相关issue是添加文件的请求；包含链接的报告关闭时间中位数缩短2天。

Conclusion: 研究结果为改进安全报告政策和社区管理提供了实用见解，有助于构建更安全的开源生态系统。

Abstract: GitHub recommends that projects adopt a SECURITY.md file that outlines
vulnerability reporting procedures. However, the effectiveness and operational
challenges of such files are not yet fully understood. This study aims to
clarify the challenges that SECURITY.md files face in the vulnerability
reporting process within open-source communities. Specifically, we classified
and analyzed the content of 711 randomly sampled issues related to SECURITY.md.
We also conducted a quantitative comparative analysis of the close time and
number of responses for issues concerning six community health files, including
SECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues
were requests to add the file, and reports that included links were closed,
with a median time that was 2 days shorter. These findings offer practical
insights for improving security reporting policies and community management,
ultimately contributing to a more secure open-source ecosystem.

</details>


### [25] [The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment](https://arxiv.org/abs/2510.05705)
*Eva Martín del Pico,Josep Lluís Gelpí,Salvador Capella-Gutiérrez*

Main category: cs.SE

TL;DR: Software Observatory是一个整合软件元数据的网络平台，用于分析生命科学研究软件生态系统的趋势，并基于FAIR原则评估软件质量。


<details>
  <summary>Details</summary>
Motivation: 在快速变化的研究软件开发环境中，需要了解当前趋势和识别可能阻碍科学进展的差距，FAIR原则可以作为理解这些趋势的代理机制。

Method: 通过OpenEBench的Software Observatory平台，整合来自多个来源的软件元数据，提供不同粒度级别的可视化分析，并使用FAIRsoft Evaluator组件评估软件的FAIR性。

Result: 该平台能够分析研究软件生态系统的趋势、识别模式和进展，并为用户提供软件FAIR性评估分数和改进指导。

Conclusion: Software Observatory是研究人员、软件开发者和利益相关者的宝贵资源，有助于促进更好的软件开发实践和对FAIR原则的遵守。

Abstract: In the ever-changing realm of research software development, it is crucial
for the scientific community to grasp current trends to identify gaps that can
potentially hinder scientific progress. The adherence to the FAIR (Findable,
Accessible, Interoperable, Reusable) principles can serve as a proxy to
understand those trends and provide a mechanism to propose specific actions.
  The Software Observatory at OpenEBench
(https://openebench.bsc.es/observatory) is a novel web portal that consolidates
software metadata from various sources, offering comprehensive insights into
critical research software aspects. Our platform enables users to analyse
trends, identify patterns and advancements within the Life Sciences research
software ecosystem, and understand its evolution over time. It also evaluates
research software according to FAIR principles for research software, providing
scores for different indicators.
  Users have the ability to visualise this metadata at different levels of
granularity, ranging from the entire software landscape to specific communities
to individual software entries through the FAIRsoft Evaluator. Indeed, the
FAIRsoft Evaluator component streamlines the assessment process, helping
developers efficiently evaluate and obtain guidance to improve their software's
FAIRness.
  The Software Observatory represents a valuable resource for researchers and
software developers, as well as stakeholders, promoting better software
development practices and adherence to FAIR principles for research software.

</details>


### [26] [Digital Twins for Software Engineering Processes](https://arxiv.org/abs/2510.05768)
*Robin Kimmel,Judith Michael,Andreas Wortmann,Jingxi Zhang*

Main category: cs.SE

TL;DR: 本文提出利用数字孪生技术来改进软件工程过程，帮助软件专家更高效利用时间，支持领域专家开发高质量软件。


<details>
  <summary>Details</summary>
Motivation: 面对熟练软件工程师短缺的挑战，需要更好地表示、理解和优化软件工程过程，让软件专家能最佳利用时间，同时支持领域专家开发高质量软件。

Method: 提出构建软件工程数字孪生的概念框架，用于在运行时表示软件系统并与之交互以控制其过程。

Result: 概述了软件工程数字孪生的潜在益处、可能形态，以及实现和部署所需的条件。

Conclusion: 数字孪生技术有潜力显著改善软件工程过程，但目前还需要解决实现和部署方面的挑战。

Abstract: Digital twins promise a better understanding and use of complex systems. To
this end, they represent these systems at their runtime and may interact with
them to control their processes. Software engineering is a wicked challenge in
which stakeholders from many domains collaborate to produce software artifacts
together. In the presence of skilled software engineer shortage, our vision is
to leverage DTs as means for better rep- resenting, understanding, and
optimizing software engineering processes to (i) enable software experts making
the best use of their time and (ii) support domain experts in producing
high-quality software. This paper outlines why this would be beneficial, what
such a digital twin could look like, and what is missing for realizing and
deploying software engineering digital twins.

</details>


### [27] [Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding](https://arxiv.org/abs/2510.05788)
*Nikita Pavlichenko,Iurii Nazarov,Ivan Dolgov,Ekaterina Garanina,Dmitry Ustalov,Ivan Bondyrev,Kseniia Lysaniuk,Evgeniia Vu,Kirill Chekmenev,Joseph Shtok,Yaroslav Golubev,Anton Semenkin,Uladzislau Sazanovich*

Main category: cs.SE

TL;DR: Mellum模型家族是专为JetBrains IDE交互使用设计的4B参数开源代码补全模型，采用Llama架构，在4T多语言代码上预训练，通过精心数据管理和多阶段训练实现高质量代码补全。


<details>
  <summary>Details</summary>
Motivation: 开发一个满足交互式代码补全成本、延迟要求的高质量、紧凑型模型，为IDE用户提供实用的代码建议功能。

Method: 采用端到端工业级流程：严格数据治理、多阶段训练（包括中间填充和项目上下文监督微调）、基于真实场景反馈的直接偏好优化对齐。

Result: 模型在离线基准测试和生产部署中表现优异，证明了精心数据管理和上下文打包能力对高质量建议的重要性。

Conclusion: Mellum模型提供了一个从研究原型到大规模生产的实用蓝图，证明了专注、开源模型能够满足数十万用户的生产需求。

Abstract: We present the Mellum models family, open-weight code completion models
designed for interactive use in JetBrains IDEs. Mellums have 4B parameters,
adopt a Llama-style architecture, and are pre-trained on ~4T tokens of
permissively licensed, multi-language code. Our studies show that (i) careful
data curation and staged training significantly improve the model's quality,
(ii) editor-critical capabilities such as context packing are necessary for
high-quality suggestions, and (iii) a compact, task-focused model can meet the
cost and latency constraints of interactive completion.
  In the paper, we describe an end-to-end industrial pipeline for producing
contextualized in-editor completion: disciplined data governance, multi-stage
training that includes fill-in-the-middle and project context via supervised
fine-tuning, and alignment via direct preference optimization using feedback
from real-world scenarios. Our quality evaluations include both large-scale
offline benchmarks and online telemetry from production deployments in
JetBrains IDEs. Mellums are released under the Apache-2.0 license on
HuggingFace, with a public model card providing a reproducible reference for
practitioners. Our experience offers a pragmatic blueprint for taking a
focused, open model from a research prototype to at scale production for
hundreds of thousands of users.

</details>


### [28] [A Wave of Resignations in the Aftermath of Remote Onboarding](https://arxiv.org/abs/2510.05878)
*Darja Smite,Franz Zieris,Lars-Ola Damm*

Main category: cs.SE

TL;DR: 该研究分析了爱立信公司在疫情期间不同工作模式（现场、远程、混合）对员工离职率的影响，发现疫情期间远程入职的员工在头三年离职率显著更高，即使返回办公室后也是如此。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情永久改变了工作结构，但完全远程工作对软件团队存在挑战。研究旨在了解不同工作模式如何影响员工保留率。

Method: 使用爱立信瑞典公司2016-2025年的人力资源数据，分析疫情前、期间和之后不同工作模式下的员工离职模式。

Result: 2021年夏季至2023年夏季离职率显著增加，特别是工龄不足5年的员工。疫情期间远程入职的员工在前三年离职可能性显著更高。

Conclusion: 精心设计的混合工作模式，结合组织归属感和导师制度，可以维持知识密集型公司的员工保留率。新员工现场工作要求应配合团队成员和资深员工的现场存在，以促进企业环境整合。

Abstract: The COVID-19 pandemic has permanently altered workplace structures,
normalizing remote work. However, critical evidence highlights challenges with
fully remote arrangements, particularly for software teams. This study
investigates employee resignation patterns at Ericsson, a global developer of
software-intensive systems, before, during, and after the pandemic. Using HR
data from 2016-2025 in Ericsson Sweden, we analyze how different work
modalities (onsite, remote, and hybrid) influence employee retention. Our
findings show a marked increase in resignations from summer 2021 to summer
2023, especially among employees with less than five years of tenure. Employees
onboarded remotely during the pandemic were significantly more likely to resign
within their first three years, even after returning to the office. Exit
surveys suggest that remote onboarding may fail to establish the necessary
organizational attachment, the feeling of belonging and long-term retention. By
contrast, the company's eventual successful return to pre-pandemic retention
rates illustrates the value of differentiated work policies and supports
reconsidering selective return-to-office (RTO) mandates. Our study demonstrates
the importance of employee integration practices in hybrid environments where
the requirement for in-office presence for recent hires shall be accompanied by
in-office presence from their team members and more senior staff whose
mentoring and social interactions contribute to integration into the corporate
work environment. We hope these actionable insights will inform HR leaders and
policymakers in shaping post-pandemic work practices, demonstrating that
carefully crafted hybrid models anchored in organizational attachment and
mentorship can sustain retention in knowledge-intensive companies.

</details>


### [29] [Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications](https://arxiv.org/abs/2510.05968)
*Scott Frees*

Main category: cs.SE

TL;DR: 提出了构建LLM驱动的报告系统的模式，通过解耦查询生成与数据检索来解决上下文窗口限制问题，引入了双响应模式来支持迭代查询优化和带外数据访问。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在报告系统中面临上下文窗口限制，无法直接处理完整数据集。虽然Model Context Protocol定义了ResourceLink用于引用外部资源，但缺乏可扩展报告架构的实现模式。

Method: 引入双响应模式扩展ResourceLink，支持迭代查询优化和带外数据访问，同时提供多租户安全和资源生命周期管理模式。

Result: 开发了解决LLM驱动报告应用基本挑战的实用模式，为开发者提供构建此类系统的实践指导。

Conclusion: 这些模式解决了LLM驱动报告系统中的核心问题，为开发可扩展的报告架构提供了实用的实现方案。

Abstract: Large language models translate natural language into database queries, yet
context window limitations prevent direct deployment in reporting systems where
complete datasets exhaust available tokens. The Model Context Protocol
specification defines ResourceLink for referencing external resources, but
practical patterns for implementing scalable reporting architectures remain
undocumented. This paper presents patterns for building LLM-powered reporting
systems that decouple query generation from data retrieval. We introduce a
dual-response pattern extending ResourceLink to support both iterative query
refinement and out-of-band data access, accompanied by patterns for
multi-tenant security and resource lifecycle management. These patterns address
fundamental challenges in LLM-driven reporting applications and provide
practical guidance for developers building them.

</details>


### [30] [Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools](https://arxiv.org/abs/2510.06000)
*Daniel Otten,Trevor Stalnaker,Nathan Wintersgill,Oscar Chaparro,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该研究通过调查91名软件工程师，系统分析了GenAI工具在软件开发中的实际应用模式，发现代码生成普遍但熟练用户更倾向将其用于调试和代码审查等复杂任务，开发者偏好多轮对话而非单次提示。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注个别提示工程技术，而非软件开发者将GenAI工具整合到专业实践中的整体工作流程，需要系统调查GenAI在软件开发中的实际应用情况。

Method: 通过大规模调查91名软件工程师（其中72名活跃GenAI用户），分析提示策略、对话模式和可靠性评估在不同软件工程任务中的应用。

Result: 14个关键发现显示：代码生成几乎普及，但熟练度与使用AI进行调试和代码审查等复杂任务强相关；开发者偏好迭代式多轮对话；文档任务被认为最可靠，而复杂代码生成和调试面临重大挑战。

Conclusion: 研究为当前开发者实践提供了经验基准，从简单代码生成到更深层次的工作流程整合，为未来改进提供了可行见解。

Abstract: The integration of generative artificial intelligence (GenAI) tools has
fundamentally transformed software development. Although prompt engineering has
emerged as a critical skill, existing research focuses primarily on individual
techniques rather than software developers' broader workflows. This study
presents a systematic investigation of how software engineers integrate GenAI
tools into their professional practice through a large-scale survey examining
prompting strategies, conversation patterns, and reliability assessments across
various software engineering tasks.
  We surveyed 91 software engineers, including 72 active GenAI users, to
understand AI usage patterns throughout the development process. Our 14 key
findings show that while code generation is nearly universal, proficiency
strongly correlates with using AI for more nuanced tasks such as debugging and
code review, and that developers prefer iterative multi-turn conversations to
single-shot prompting. Documentation tasks are perceived as most reliable,
while complex code generation and debugging present sizable challenges. Our
insights provide an empirical baseline of current developer practices, from
simple code generation to deeper workflow integration, with actionable insights
for future improvements.

</details>


### [31] [Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations](https://arxiv.org/abs/2510.06104)
*Elijah Kayode Adejumo,Brittany Johnson*

Main category: cs.SE

TL;DR: 该研究探讨使用大语言模型将静态分析指标转化为可理解的风险解释，帮助开源软件贡献者更好地理解和修改代码


<details>
  <summary>Details</summary>
Motivation: 开源软件贡献者在修改代码时难以理解静态分析工具产生的复杂指标，需要更直观的风险解释和指导

Method: 提出使用大语言模型将故障预测指标转化为描述性、上下文性和可操作性解释，计划通过任务研究评估效果

Result: 目前处于研究阶段，提出了LLM生成解释的方法框架，尚未有实证结果

Conclusion: LLM有潜力改善开源软件贡献者对代码风险的理解，需要进一步研究验证其实际效果

Abstract: Open Source Software (OSS) has become a very important and crucial
infrastructure worldwide because of the value it provides. OSS typically
depends on contributions from developers across diverse backgrounds and levels
of experience. Making safe changes, such as fixing a bug or implementing a new
feature, can be challenging, especially in object-oriented systems where
components are interdependent. Static analysis and defect-prediction tools
produce metrics (e.g., complexity,coupling) that flag potentially fault-prone
components, but these signals are often hard for contributors new or unfamiliar
with the codebase to interpret. Large Language Models (LLMs) have shown strong
performance on software engineering tasks such as code summarization and
documentation generation. Building on this progress, we investigate whether
LLMs can translate fault-prediction metrics into clear, human-readable risk
explanations and actionable guidance to help OSS contributors plan and review
code modifications. We outline explanation types that an LLM-generated
assistant could provide (descriptive, contextual, and actionable explanations).
We also outline our next steps to assess usefulness through a task-based study
with OSS contributors, comparing metric-only baselines to LLM-generated
explanations on decision quality, time-to-completion, and error rates

</details>


### [32] [Automated Program Repair of Uncompilable Student Code](https://arxiv.org/abs/2510.06187)
*Griffin Pitts,Aum Pandya,Darsh Rank,Tirth Bhatt,Muntasir Hoq,Bita Akram*

Main category: cs.SE

TL;DR: 使用大型语言模型自动修复CS1课程中无法编译的学生代码，以保留学生结构意图并用于学生建模


<details>
  <summary>Details</summary>
Motivation: CS1学习环境中大量学生编程提交无法编译，限制了在学生建模和知识追踪中的使用，传统建模流程往往排除这些情况

Method: 评估GPT-5、Claude 3.5 Haiku和Gemini 2.5 Flash等LLM在高/低上下文提示条件下作为修复代理的能力，评估编译性、编辑距离、结构保留和逻辑保留

Result: 所有三个LLM都能产生可编译修复，但在保留学生控制流和代码结构方面表现不同，影响其教学效用

Conclusion: 通过恢复无法编译的提交，这项工作能够对学习者的编码过程和发展进行更丰富全面的分析

Abstract: A significant portion of student programming submissions in CS1 learning
environments are uncompilable, limiting their use in student modeling and
downstream knowledge tracing. Traditional modeling pipelines often exclude
these cases, discarding observations of student learning. This study
investigates automated program repair as a strategy to recover uncompilable
code while preserving students' structural intent for use in student modeling.
Within this framework, we assess large language models (LLMs) as repair agents,
including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash
(Google), under high- and low-context prompting conditions. Repairs were
evaluated for compilability, edit distance, and preservation of students'
original structure and logic. We find that while all three LLMs are capable of
producing compilable repairs, their behavior diverges in how well they preserve
students' control flow and code structure, which affects their pedagogical
utility. By recovering uncompilable submissions, this work enables richer and
more comprehensive analyses of learners' coding processes and development over
time.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [33] [Redefining Cost Estimation in Database Systems: The Role of Execution Plan Features and Machine Learning](https://arxiv.org/abs/2510.05612)
*Utsav Pathak,Amit Mankodi*

Main category: cs.DB

TL;DR: 提出基于机器学习的SQL查询运行时预测框架，使用PostgreSQL执行计划特征和查询语义表示，XGBoost模型表现最佳，在65%情况下预测误差在10%以内。


<details>
  <summary>Details</summary>
Motivation: 传统数据库系统的静态启发式成本模型在复杂工作负载下无法准确反映实际查询性能，需要机器学习技术来改进查询优化。

Method: 构建自动化数据收集和特征提取流水线，整合执行计划的标量和结构特征以及SQL查询的语义表示，比较基线回归器、改进的XGBoost模型和基于LSTM的序列模型。

Result: XGBoost模型显著优于其他方法，均方误差为0.3002，在65%以上的情况下预测准确率在真实运行时的10%以内。

Conclusion: 基于树的机器学习与执行计划特征结合在改进查询优化器成本估计方面具有巨大潜力。

Abstract: Accurate query runtime prediction is a critical component of effective query
optimization in modern database systems. Traditional cost models, such as those
used in PostgreSQL, rely on static heuristics that often fail to reflect actual
query performance under complex and evolving workloads. This remains an active
area of research, with recent work exploring machine learning techniques to
replace or augment traditional cost estimators. In this paper, we present a
machine learning-based framework for predicting SQL query runtimes using
execution plan features extracted from PostgreSQL. Our approach integrates
scalar and structural features from execution plans and semantic
representations of SQL queries to train predictive models. We construct an
automated pipeline for data collection and feature extraction using
parameterized TPC-H queries, enabling systematic evaluation of multiple
modeling techniques. Unlike prior efforts that focus either on cardinality
estimation or on synthetic cost metrics, we model the actual runtimes using
fine-grained plan statistics and query embeddings derived from execution
traces, to improve the model accuracy. We compare baseline regressors, a
refined XGBoost model, and a sequential LSTM-based model to assess their
effectiveness in runtime prediction. Our dataset includes over 1000 queries
generated from TPC-H query templates executed in PostgreSQL with EXPLAIN
ANALYZE. Experimental results show that the XGBoost model significantly
outperforms others, achieving a mean squared error of 0.3002 and prediction
accuracy within 10% of the true runtime in over 65% of cases. The findings
highlight the potential of tree-based learning combined with execution plan
features for improving cost estimation in query optimizers.

</details>


### [34] [Speeding up SQL subqueries via decoupling of non-correlated predicate (extended version)](https://arxiv.org/abs/2510.05907)
*Dmitrii Radivonchik,Yakov Kuzin,Anton Chizhov,Dmitriy Shcheka,Mikhail Firsov,Kirill Smirnov,George Chernishev*

Main category: cs.DB

TL;DR: 提出了一种处理SQL相关子查询的新技术，通过分离非相关谓词部分来减少相关部分的评估次数，在合适条件下可实现5倍性能提升


<details>
  <summary>Details</summary>
Motivation: 提高相关子查询的处理效率，通过优化查询重写来减少不必要的计算

Method: 1) 识别可受益的查询类型并提出重写方案；2) 在基于块的Volcano查询处理模型中适配该方法；3) 在支持位置感知和延迟物化的列存储中实现

Result: 在PosDB和PostgreSQL上的实验表明，在合适条件下性能可提升5倍

Conclusion: 该技术能有效优化相关子查询处理，特别是在非相关谓词选择性较高的情况下效果显著

Abstract: In this paper, we discuss a novel technique for processing correlated
subqueries in SQL. The core idea is to isolate the non-correlated part of the
predicate and use it to reduce the number of evaluations of the correlated
part. We begin by providing an overview of several classes of queries that may
benefit from this technique. For each class, we propose a potential rewrite and
discuss the conditions under which it is advantageous. Next, we address the
evaluation aspects of the proposed rewrites: 1) we describe our approach to
adapting the block-based Volcano query processing model, and 2) we discuss the
benefits of implementing that technique within a position-enabled column-store
with late materialization support. Finally, we present a simple cost model that
allows estimation of the benefits of said rewrites.
  Our evaluation has a quantitative part and a qualitative part. The former
focuses on studying the impact of non-correlated predicate selectivity on our
technique. The latter identifies the limitations of our approach by comparing
it with alternative approaches available in existing systems. Overall,
experiments conducted using PosDB (a position-enabled column-store) and
PostgreSQL demonstrated that, under suitable conditions, our technique can
achieve a 5x improvement.

</details>
