<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 3]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [NeurIDA: Dynamic Modeling for Effective In-Database Analytics](https://arxiv.org/abs/2512.08483)
*Lingze Zeng,Naili Xing,Shaofeng Cai,Peng Lu,Gang Chen,Jian Pei,Beng Chin Ooi*

Main category: cs.DB

TL;DR: NeurIDA是一个自主的端到端数据库内分析系统，通过动态调整最佳可用基础模型来服务特定分析任务，支持自然语言查询，在多个真实数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型是静态且任务特定的，而RDBMS环境是动态的且需要支持多样化的分析查询。每个分析任务都需要从头构建定制化流水线，导致开发开销大，限制了ML在分析中的广泛应用。

Method: 提出动态数据库内建模范式，在关系数据上预训练可组合的基础模型架构。接收任务时，NeurIDA根据任务和数据配置文件动态选择和配置相关组件。支持自然语言查询，通过LLM代理解释用户意图并生成分析报告。

Result: 在五个真实世界数据集的十个任务上，NeurIDA持续提供高达12%的AUC-ROC改进和25%的相对MAE减少。

Conclusion: NeurIDA设计实现了易用性、高效性和有效性的数据库内AI分析，解决了传统ML模型与动态RDBMS环境之间的不匹配问题。

Abstract: Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.
  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically "tweaks" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improve- ment in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA

</details>


### [2] [Analyzing Deviations from Monotonic Trends through Database Repair](https://arxiv.org/abs/2512.08526)
*Shunit Agmon,Jonathan Gal,Amir Gilad,Ester Livshits,Or Mutay,Brit Youngmann,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 提出Aggregate Order Dependencies (AODs)概念，用于量化数据集违反单调趋势的程度，并研究通过最小化删除元组来修复AOD违反的问题。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中常出现违反预期单调趋势的现象（如教育水平越高平均工资越高），需要量化这种偏离程度并修复数据。

Method: 引入AODs作为顺序依赖的聚合扩展，形式化AOD修复问题，分析计算复杂度，提出通用算法模板并针对常见聚合函数实例化，开发优化技术和启发式方法。

Result: 在真实和合成数据集上的实验验证了算法效率，启发式方法表现良好，案例研究展示了框架如何发现和解释AOD违反。

Conclusion: AODs为量化数据集单调趋势违反提供了有效框架，提出的算法和启发式方法在实际应用中具有高效性和实用性。

Abstract: Datasets often exhibit violations of expected monotonic trends - for example, higher education level correlating with higher average salary, newer homes being more expensive, or diabetes prevalence increasing with age. We address the problem of quantifying how far a dataset deviates from such trends. To this end, we introduce Aggregate Order Dependencies (AODs), an aggregation-centric extension of the previously studied order dependencies. An AOD specifies that the aggregated value of a target attribute (e.g., mean salary) should monotonically increase or decrease with the grouping attribute (e.g., education level).
  We formulate the AOD repair problem as finding the smallest set of tuples to delete from a table so that the given AOD is satisfied. We analyze the computational complexity of this problem and propose a general algorithmic template for solving it. We instantiate the template for common aggregation functions, introduce optimization techniques that substantially improve the runtime of the template instances, and develop efficient heuristic alternatives. Our experimental study, carried out on both real-world and synthetic datasets, demonstrates the practical efficiency of the algorithms and provides insight into the performance of the heuristics. We also present case studies that uncover and explain unexpected AOD violations using our framework.

</details>


### [3] [Causal Explanations for Disparate Trends: Where and Why?](https://arxiv.org/abs/2512.08679)
*Tal Blau,Brit Youngmann,Anna Fariha,Yuval Moskovitch*

Main category: cs.DB

TL;DR: ExDis框架用于自动发现两组数据间差异的因果解释，识别差异最显著的数据子区域及其因果因素


<details>
  <summary>Details</summary>
Motivation: 数据分析中常遇到两组数据间的差异难以理解，需要能识别差异最显著的数据区域及其因果因素的解释，传统方法难以处理大规模高维数据

Method: 提出ExDis框架，形式化定义优化问题，分析计算复杂度，开发高效算法识别差异最显著的子群体及其因果贡献因素

Result: 在三个真实数据集上的实验表明，ExDis能生成有意义的因果解释，优于现有方法，并能有效扩展到大规模高维数据集

Conclusion: ExDis为自动发现数据差异的因果解释提供了有效框架，具有可解释性和可操作性，能帮助用户做出数据驱动的决策

Abstract: During data analysis, we are often perplexed by certain disparities observed between two groups of interest within a dataset. To better understand an observed disparity, we need explanations that can pinpoint the data regions where the disparity is most pronounced, along with its causes, i.e., factors that alleviate or exacerbate the disparity. This task is complex and tedious, particularly for large and high-dimensional datasets, demanding an automatic system for discovering explanations (data regions and causes) of an observed disparity. It is critical that explanations for disparities are not only interpretable but also actionable-enabling users to make informed, data-driven decisions. This requires explanations to go beyond surface-level correlations and instead capture causal relationships. We introduce ExDis, a framework for discovering causal Explanations for Disparities between two groups of interest. ExDis identifies data regions (subpopulations) where disparities are most pronounced (or reversed), and associates specific factors that causally contribute to the disparity within each identified data region. We formally define the ExDis framework and the associated optimization problem, analyze its complexity, and develop an efficient algorithm to solve the problem. Through extensive experiments over three real-world datasets, we demonstrate that ExDis generates meaningful causal explanations, outperforms prior methods, and scales effectively to handle large, high-dimensional datasets.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation](https://arxiv.org/abs/2512.07917)
*Zhehao Dong,Shanghai Du,Zhen Lu,Yue Yang*

Main category: cs.SE

TL;DR: CFD-copilot：一个基于大语言模型的领域专用框架，通过自然语言驱动完整的CFD仿真工作流，从设置到后处理，降低非专家用户的使用门槛。


<details>
  <summary>Details</summary>
Motivation: CFD仿真配置需要深厚的物理建模和数值方法专业知识，这对非专家用户构成了障碍。虽然大语言模型在自动化科学任务方面受到关注，但由于CFD工作流严格的领域特定要求，实现端到端的自动化仍然具有挑战性。

Method: 开发了CFD-copilot框架，包含：1）微调的大语言模型直接将用户描述翻译为可执行的CFD设置；2）多智能体系统集成LLM与仿真执行、自动错误修正和结果分析；3）后处理采用模型上下文协议（MCP），将LLM推理与外部工具执行解耦，通过统一接口访问多种专用后处理功能。

Result: 在NACA 0012翼型和30P-30N三元素翼型等基准测试中，结果表明领域特定适应和MCP的结合共同提高了LLM驱动工程工作流的可靠性和效率。

Conclusion: CFD-copilot框架通过领域专用的大语言模型和模块化设计，成功实现了自然语言驱动的端到端CFD仿真自动化，为工程工作流提供了更可靠和高效的解决方案。

Abstract: Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.

</details>


### [5] [DeepCode: Open Agentic Coding](https://arxiv.org/abs/2512.07921)
*Zongwei Li,Zhonghang Li,Zirui Guo,Xubin Ren,Chao Huang*

Main category: cs.SE

TL;DR: DeepCode是一个完全自主的框架，通过信息流管理解决文档到代码库合成中的信息过载与上下文瓶颈冲突，在PaperBench基准测试中超越商业代理和人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实现高保真文档到代码库合成（如科学论文到代码）时面临重大挑战，主要是信息过载与LLMs上下文瓶颈之间的根本冲突。

Method: 将仓库合成视为信道优化问题，通过四个信息操作最大化有限上下文预算下的任务相关信号：蓝图蒸馏进行源压缩、使用状态化代码内存进行结构化索引、通过检索增强生成进行条件知识注入、闭环错误校正。

Result: 在PaperBench基准测试中达到最先进性能，显著优于Cursor和Claude Code等领先商业代理，并在关键复现指标上超越顶尖机构的博士级人类专家。

Conclusion: 通过系统地将论文规范转化为生产级实现，达到人类专家质量，为自主科学复现建立新基础，加速研究评估和发现。

Abstract: Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.

</details>


### [6] [An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face](https://arxiv.org/abs/2512.07983)
*Nan Jia,Anita Raja,Raffi Khatchadourian*

Main category: cs.SE

TL;DR: 该论文提出了一个评估学习型软件系统语义保持性的经验框架，通过挖掘HuggingFace上的模型演化数据，检测语义漂移并分析重构模式。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习成为高自治系统的核心组成部分，确保学习型软件系统的可信性变得至关重要。然而，ML的非确定性和运行时定义语义使得传统软件重构变得复杂，需要定义和评估语义保持性。

Method: 从HuggingFace挖掘模型演化数据，包括提交历史、模型卡片和性能指标。建立了一个可复现的管道，从170万个HuggingFace条目中提取数据，对536个模型和4000多个指标进行评估。通过案例研究追踪版本间的性能变化，分析提交消息以识别常见重构模式。

Result: 创建了大规模ML模型演化数据集，开发了评估语义保持性的实用管道，通过案例研究展示了语义漂移的实际检测。虽然API限制影响了完整阈值估计，但该框架为定义社区接受的语义保持边界奠定了基础。

Conclusion: 该研究为更可维护和可信的ML系统奠定了基础，通过实证方法评估语义保持性，有助于提高学习型软件系统的可信度。

Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.

</details>


### [7] [A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering](https://arxiv.org/abs/2512.07990)
*Thanh Nguyen,Chaima Boufaied,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 该论文综述了AI系统中公平性要求的现状，分析了公平性定义、SDLC中的管理实践、违反原因及后果，强调需要将公平性置于与有效性同等重要的地位。


<details>
  <summary>Details</summary>
Motivation: 当前AI/ML应用过度关注模型有效性（如F1分数），而公平性关注不足。需要系统研究AI系统中公平性要求的定义、管理和违反后果，以促进公平性在软件开发中的整合。

Method: 采用灰色文献综述方法，审查现有文献，分析AI环境中公平性要求的定义方式、在软件开发生命周期（SDLC）中的管理实践、违反原因及相应后果。

Result: 研究发现：1）AI系统中公平性要求有多种定义，通常强调不同人口和社会属性间的非歧视和平等对待；2）公平性管理实践在SDLC各阶段差异显著，特别是在模型训练、偏见缓解、公平性监控评估和数据处理方面；3）公平性违反主要与数据表示偏见、算法设计偏见、人为判断、评估和透明度差距相关；4）后果包括广义伤害、刻板印象强化、数据隐私风险、AI决策信任和合法性丧失。

Conclusion: 需要建立一致的框架和实践，将公平性整合到AI软件中，给予公平性与有效性同等的关注。研究强调了系统性解决AI公平性问题的重要性。

Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.

</details>


### [8] [What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions](https://arxiv.org/abs/2512.08032)
*Arghavan Sanei,Chaima Amiri,Atefeh Shokrizadeh,Jinghui Cheng*

Main category: cs.SE

TL;DR: 分析开源软件可用性讨论中的论证特征与质量，发现讨论以论证为主但质量参差不齐，评论质量低于帖子，论证特征影响参与者后续行为。


<details>
  <summary>Details</summary>
Motivation: 开源软件的可用性常被技术复杂性所忽视，论证在可用性讨论中是关键工具，但目前对这类讨论中论证话语的特征了解不足，导致难以为参与者提供有效支持。

Method: 对五个开源软件项目的可用性讨论进行综合分析，研究论证话语和论证质量的特征。

Result: 可用性讨论以论证为主但质量不一；问题评论的论证质量低于问题帖子，表明开源社区在可用性方面缺乏集体智慧；论证话语和质量对参与者后续行为有不同影响。

Conclusion: 研究为开源软件利益相关者构建更有效论证提供见解，有助于最终改善开源软件可用性，这些见解也可为其他分布式协作社区的研究提供参考。

Abstract: The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.

</details>


### [9] [Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs](https://arxiv.org/abs/2512.08213)
*Md Nazmul Haque,Elizabeth Lin,Lawrence Arkoh,Biruk Tadesse,Bowen Xu*

Main category: cs.SE

TL;DR: 量化会显著增加LLM生成Go包时的幻觉率和漏洞风险，4-bit量化模型表现最差


<details>
  <summary>Details</summary>
Motivation: 量化技术被广泛用于降低LLM推理成本，但量化如何影响LLM生成的软件依赖的正确性和安全性尚不清楚，特别是量化是否会增加包幻觉和漏洞风险

Method: 对五种Qwen模型尺寸在完整精度、8-bit和4-bit量化下进行系统实证研究，使用三个数据集（SO、MBPP、paraphrase）评估包幻觉率和漏洞存在率

Result: 量化显著增加包幻觉率，4-bit模型退化最严重；即使在正确生成的包中，漏洞存在率也随精度降低而上升；幻觉的包大多呈现类似真实URL的Go模块路径模式

Conclusion: 量化部署的LLM在代码生成和依赖推荐中存在可靠性和安全隐患，需要谨慎评估量化对软件安全的影响

Abstract: Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.
  In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.

</details>


### [10] [Migrating QAOA from Qiskit 1.x to 2.x: An experience report](https://arxiv.org/abs/2512.08245)
*Julien Cardinal,Imen Benzarti,Ghizlane El boussaidi,Christophe Pere*

Main category: cs.SE

TL;DR: 量子算法迁移时，框架升级导致QAOA结果差异巨大，根本原因是采样预算（shots数）的隐式变化影响概率分布准确性。


<details>
  <summary>Details</summary>
Motivation: 量子算法在不同框架间迁移时，即使电路、优化器和哈密顿量相同，也可能产生截然不同的结果，这影响了算法的准确性和可重复性。需要探究量子-经典交互层面的隐藏参数如何主导混合算法性能。

Method: 将量子近似优化算法（QAOA）从Qiskit 1.x（v1原语）迁移到Qiskit 2.x（v2原语）的自定义实现，系统分析结果差异的原因。通过对比不同采样预算（shots数）下的概率分布来定位问题根源。

Result: 研究发现：Qiskit 1.x隐式使用无限shots产生密集概率分布，而Qiskit 2.x默认10,000 shots仅捕获23%的状态空间。将shots增加到250,000后恢复了库级精度，表明采样预算是影响结果的关键隐藏参数。

Conclusion: 量子-经典交互层面的隐藏参数（如采样预算）对混合量子算法性能有主导性影响。研究为开发者和框架设计者提供了确保量子软件迁移可重复性的实用建议，强调了明确配置这些参数的重要性。

Abstract: Migrating quantum algorithms across evolving frameworks introduces subtle behavioral changes that affect accuracy and reproducibility. This paper reports our experience converting the Quantum Approximate Optimization Algorithm (QAOA) from Qiskit Algorithms with Qiskit 1.x (v1 primitives) to a custom implementation using Qiskit 2.x (v2 primitives). Despite identical circuits, optimizers, and Hamiltonians, the new version produced drastically different results. A systematic analysis revealed the root cause: the sampling budget -- the number of circuit executions (shots) per iteration. The library's implicit use of unlimited shots yielded dense probability distributions, whereas the v2 default of 10 000 shots captured only 23% of the state space. Increasing shots to 250 000 restored library-level accuracy. This study highlights how hidden parameters at the quantum--classical interaction level can dominate hybrid algorithm performance and provides actionable recommendations for developers and framework designers to ensure reproducible results in quantum software migration.

</details>


### [11] [Formally and Empirically Verified Methodologies for Scalable Hierarchical Full-Stack Systems](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: 论文提出了PBFD和PDFD两种工业级全栈软件开发方法，通过图论建模和形式化验证确保结构行为正确性，结合TLE三层封装技术实现高性能，在8年企业部署中实现零关键故障，性能远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决工业级全栈软件开发中的可扩展性、结构正确性和性能问题，将形式化方法与实际工程实践相结合，提供可验证的高性能开发方法论。

Method: 1. 提出PBFD（主广度优先开发）和PDFD（主深度优先开发）两种方法，将软件开发建模为带统一状态机的分层有向图；2. 使用CSP和LTL进行形式化验证；3. 提出TLE三层封装技术，基于位掩码编码实现高效数据管理；4. 通过CSP失败-发散精化验证TLE操作。

Result: 1. 8年企业部署零关键故障；2. 开发速度比Salesforce OmniScript快约20倍；3. 查询性能快7-8倍；4. 存储比传统关系模型减少11.7倍；5. 实现有界精化终止、无死锁和结构完整性；6. 开源MVP实现验证了关键行为属性。

Conclusion: PBFD和PDFD是经过形式化和实证验证的工业级全栈软件开发方法，通过图论建模和形式化验证确保正确性，结合TLE技术实现高性能，在实际部署中表现出卓越的可靠性和效率优势。

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary Depth-First Development (PDFD)-formally and empirically verified methodologies for scalable, industrial-grade full-stack software engineering. Both approaches enforce structural and behavioral correctness through graph-theoretic modeling, bridging formal methods and real-world practice. PBFD and PDFD model software development as layered directed graphs with unified state machines, verified using Communicating Sequential Processes (CSP) and Linear Temporal Logic (LTL). This guarantees bounded-refinement termination, deadlock freedom, and structural completeness. To manage hierarchical data at scale, we present the Three-Level Encapsulation (TLE)-a novel bitmask-based encoding scheme. TLE operations are verified via CSP failures-divergences refinement, ensuring constant-time updates and compact storage that underpin PBFD's robust performance. PBFD demonstrates exceptional industrial viability through eight years of enterprise deployment with zero critical failures, achieving approximately 20x faster develop-ment than Salesforce OmniScript, 7-8x faster query performance, and 11.7x storage reduction compared to conventional relational models. These results are established through longitudinal observational studies, quasi-experimental runtime comparisons, and controlled schema-level experiments. Open-source Minimum Viable Product implementations validate key behavioral properties, including bounded refinement and constant-time bitmask operations, un-der reproducible conditions. All implementations, formal specifications, and non-proprietary datasets are publicly available.

</details>


### [12] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: Token Sugar：通过将高频冗长代码模式替换为可逆的简洁表示，减少LLM代码生成中的token数量，降低计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码任务中表现出色，但编程语言的冗长性（如格式元素和样板代码）导致token数量膨胀，增加了推理成本和生成时间。现有方法局限于语法层面的简化，未能充分利用语义层面的token减少机会。

Method: 提出Token Sugar概念：将高频、冗长的代码模式替换为可逆的、token高效的简写形式。设计系统化解决方案：从代码语料库中挖掘高频、token密集的模式，为每个模式映射唯一简写，通过代码转换将简写集成到LLM预训练中。

Result: 获得了799个（代码模式，简写）对，可在源代码中减少高达15.1%的token数量，且与现有语法方法互补。在Token Sugar增强数据上训练的三个广泛使用的LLM，在生成过程中实现了显著的token节省（高达11.2%减少），同时保持与未经处理代码训练的基线模型几乎相同的Pass@1分数。

Conclusion: Token Sugar通过语义层面的代码简化，有效减少了LLM代码生成中的token数量，降低了计算成本，同时保持了代码生成质量，为高效代码生成提供了新途径。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [13] [FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection](https://arxiv.org/abs/2512.08277)
*Yihan Liao,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Jialong Li*

Main category: cs.SE

TL;DR: FedLAD是一个用于联邦学习环境下日志异常检测的统一平台，支持多种模型、数据集和聚合策略，提供自监控、自配置和自适应控制功能。


<details>
  <summary>Details</summary>
Motivation: 现有的日志异常检测方法大多假设集中式训练，这在隐私约束和日志分散性的实际场景中不切实际。联邦学习提供了有前景的替代方案，但缺乏专门针对联邦环境下日志异常检测需求的测试平台。

Method: 开发了FedLAD平台，支持即插即用集成多种日志异常检测模型、基准数据集和聚合策略，同时提供运行时验证日志记录（自监控）、参数调优（自配置）和自适应策略控制（自适应）支持。

Result: FedLAD填补了联邦学习框架与日志异常检测需求之间的空白，为未来研究提供了可重复和可扩展的实验基础。项目代码已公开在GitHub上。

Conclusion: FedLAD为联邦学习环境下的日志异常检测提供了一个统一的训练和评估平台，通过支持可重复和可扩展的实验，为这一领域的研究奠定了坚实基础。

Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.

</details>


### [14] [Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework](https://arxiv.org/abs/2512.08286)
*Liao Hu,Qiteng Wu,Ruoyu Qi*

Main category: cs.SE

TL;DR: SolidGPT是一个开源边缘-云混合开发者助手，通过语义代码搜索、自动化项目工作流和隐私优先设计，解决LLM在开发工作流中语义理解、生产力和数据隐私的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决将大型语言模型集成到移动和软件开发工作流中的三个核心矛盾：语义理解、开发者生产力和数据隐私。云端工具存在数据暴露和延迟风险，而本地解决方案缺乏对代码库和开发工具的全上下文理解。

Method: 构建基于GitHub的开源边缘-云混合开发者助手SolidGPT，支持：1）与代码库对话的交互式查询；2）自动化软件项目工作流（生成PRD、任务分解、看板等）；3）配置私有可扩展代理，支持本地代码文件夹、Notion集成和AI角色定制。

Result: SolidGPT通过语义丰富的代码导航、集成的文档和任务管理、隐私优先设计，提升了开发者生产力。支持本地运行（Docker或VSCode），同时可选连接LLM API，为智能移动和软件工程提供实用的隐私保护边缘助手。

Conclusion: SolidGPT通过结合交互式代码查询、自动化项目脚手架和人机协作，提供了一个实用且尊重隐私的边缘助手，加速了真实世界的开发工作流，特别适合智能移动和软件工程场景。

Abstract: The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.

</details>


### [15] [Measuring Agile Agreement: Development and Validation of the Manifesto and Principle Scales](https://arxiv.org/abs/2512.08461)
*Nicolas Matton,Anthony Simonofski,Marie-Ange Remiche,Benoît Vanderose*

Main category: cs.SE

TL;DR: 本文开发并验证了两个独立量表：宣言同意量表（MAS）和原则同意量表（PAS），用于区分对敏捷宣言抽象价值观的同意与对具体实践原则的同意，填补了敏捷协议测量的方法论空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能区分对敏捷宣言抽象高层价值观的同意与对12条具体日常实践原则的同意，导致个人"敏捷协议"的测量定义不清且具有挑战性。

Method: 设计了两个独立量表：新颖的宣言同意量表（MAS）和系统改编自先前工具的原则同意量表（PAS）。通过系统化的项目创建与选择、调查设计和验证过程，使用比例优势逻辑回归、Bland-Altman图和组内相关系数等方法进行收敛与发散分析。

Result: 两个量表均显示出良好的内部一致性和结构效度。虽然两者中度相关，但分析表明它们不可互换，捕捉了敏捷协议的不同维度。量表在比利时IT专业人员群体中得到验证。

Conclusion: 本研究提供了一对公开可用的测量工具，为更细致地测量敏捷协议迈出了关键一步，能够区分不同感知层次的敏捷协议，有助于更精确地解释个人与敏捷方法的匹配度。

Abstract: While the importance of human factors in agile software development is widely acknowledged, the measurement of an individual's "agile agreement" remains an ill-defined and challenging area. A key limitation in existing research is the failure to distinguish between agreement with the abstract, high-level values of the Agile Manifesto and agreement with the concrete, day-to-day practices derived from the 12 Principles. This paper addresses this methodological gap by presenting the design and validation of two distinct instruments: the novel Manifesto Agreement Scale (MAS), and the Principle Agreement Scale (PAS), which is a systematic adaptation and refinement of a prior instrument.
  We detail the systematic process of item creation and selection, survey design, and validation. The results demonstrate that both scales possess important internal consistency and construct validity. A convergence and divergence analysis, including Proportional Odds Logistic Regression, a Bland-Altman plot, and an Intraclass Correlation Coefficient (ICC), reveals that while the two scales are moderately correlated, they are not interchangeable and capture distinct dimensions of agile agreement. The primary contribution of this work is a pair of publicly available instruments, validated within a specific demographic of Belgian IT professionals. These scales represent a critical initial step toward facilitating a more nuanced measurement of agile agreement, distinguishing agile agreement across various levels of perception and aiding in a more refined interpretation of person-agile fit.

</details>


### [16] [Measuring Computer Science Enthusiasm: A Questionnaire-Based Analysis of Age and Gender Effects on Students' Interest](https://arxiv.org/abs/2512.08472)
*Kai Marquardt,Robert Hanak,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 研究发现年龄比性别对计算机科学兴趣发展影响更大，识别出青春期早期兴趣显著下降，特别是女孩，并揭示精心设计的短期活动能在较晚年龄有效重新激活兴趣。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为早期接触是维持计算机科学兴趣的主要途径，但缺乏对年龄和性别影响的细致分析。本研究旨在通过解构年龄和性别对CS兴趣的不同影响，为CS教育提供更动态、年龄敏感的理论框架。

Method: 基于兴趣的人-对象理论(POI)，将热情概念化为短期激活的兴趣表达。开发了理论基础的问卷，用于评估CS干预活动的热情潜力。收集了400多名参与在线CS课程学生的前后测数据，分析年龄和性别相关的热情模式。

Result: 发现青春期早期热情显著下降，特别是女孩。年龄比性别对兴趣发展影响更大，识别出关键发展断点。尽管基线态度较低，年长学生在干预后表现出最大的积极变化，表明精心设计的短期活动能在较晚年龄有效重新激活兴趣。

Conclusion: 挑战了早期接触是维持CS兴趣主要途径的传统观点。强调需要动态、年龄敏感的CS教育框架，使教学策略与个体发展轨迹相匹配。短期干预活动即使对较年长学生也能有效重新激活兴趣。

Abstract: This study offers new insights into students' interest in computer science (CS) education by disentangling the distinct effects of age and gender across a diverse adolescent sample. Grounded in the person-object theory of interest (POI), we conceptualize enthusiasm as a short-term, activating expression of interest that combines positive affect, perceived relevance, and intention to re-engage. Experiencing such enthusiasm can temporarily shift CS attitudes and strengthen future engagement intentions, making it a valuable lens for evaluating brief outreach activities. To capture these dynamics, we developed a theoretically grounded questionnaire for pre-post assessment of the enthusiasm potential of CS interventions. Using data from more than 400 students participating in online CS courses, we examined age- and gender-related patterns in enthusiasm. The findings challenge the prevailing belief that early exposure is the primary pathway to sustained interest in CS. Instead, we identify a marked decline in enthusiasm during early adolescence, particularly among girls, alongside substantial variability in interest trajectories across age groups. Crucially, our analyses reveal that age is a more decisive factor than gender in shaping interest development and uncover key developmental breakpoints. Despite starting with lower baseline attitudes, older students showed the largest positive changes following the intervention, suggesting that well-designed short activities can effectively re-activate interest even at later ages. Overall, the study highlights the need for a dynamic, age-sensitive framework for CS education in which instructional strategies are aligned with developmental trajectories.

</details>


### [17] [Gamification with Purpose: What Learners Prefer to Motivate Their Learning](https://arxiv.org/abs/2512.08551)
*Kai Marquardt,Mona Schulz,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 研究通过系统文献综述识别10种常用游戏设计元素，采用最佳-最差量表调查125名参与者偏好，发现学习者更倾向直接支持学习过程的元素（进度条、概念图、即时反馈、成就），并揭示六个动机主题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在了解学习者在教育情境中对游戏设计元素的偏好，以指导开发目标驱动的游戏化策略。强调以学习者为中心的方法，使游戏化设计与教学目标保持一致，同时减轻内在动机被侵蚀等风险。

Method: 1. 进行系统文献综述，识别10个广泛讨论的游戏设计元素；2. 为每个元素开发视觉原型；3. 对125名参与者实施最佳-最差量表调查，获取偏好排序；4. 收集定性反馈以揭示动机驱动因素。

Result: 学习者一致偏好直接支持学习过程的游戏设计元素，特别是进度条、概念图、即时反馈和成就。定性分析揭示六个重复出现的动机主题：可见进展、内容相关性、建设性反馈、自主性、能力感和社交联系。

Conclusion: 学习者重视与教育内容有意义整合并能支持内在动机的游戏化元素。目标对齐的游戏化应优先考虑可视化学习进展和提供可操作反馈的工具，而非仅仅依赖外在激励。

Abstract: This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.

</details>


### [18] [Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain](https://arxiv.org/abs/2512.08657)
*Renato Cordeiro Ferreira,Aditya Dhinavahi,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 该论文介绍了在构建海洋异常检测系统Ocean Guard时，应用六边形架构（Ports and Adapters模式）实现软件架构复用的经验，展示了如何从单一代码库构建多个微服务。


<details>
  <summary>Details</summary>
Motivation: ML赋能系统（MLES）由于需要多个组件协同工作而具有内在复杂性。作者希望通过分享Ocean Guard系统的构建经验，展示如何通过架构模式复用解决MLES开发中的挑战，为软件工程师、机器学习工程师和数据科学家提供实践参考。

Method: 采用六边形架构（Ports and Adapters模式）作为核心架构模式，通过端口和适配器实现业务逻辑与外部依赖的解耦。特别关注如何从单一代码库构建多个微服务，实现架构组件的复用。

Result: 成功构建了Ocean Guard系统，这是一个用于海事领域异常检测的MLES。通过应用六边形架构模式，实现了架构组件的有效复用，能够从单一代码库构建多个微服务，提高了开发效率和系统可维护性。

Conclusion: 六边形架构模式适用于构建ML赋能系统，能够有效支持架构复用和微服务开发。该经验报告为软件工程师、机器学习工程师和数据科学家提供了实用的架构指导，鼓励他们在MLES开发中应用这种模式。

Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.

</details>


### [19] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: RESTifAI是一个基于LLM的自动化工具，用于生成可复用、CI/CD就绪的REST API测试，专注于快乐路径和负向测试场景。


<details>
  <summary>Details</summary>
Motivation: 现有API测试工具主要关注内部服务器错误，缺乏对有效测试场景（快乐路径）的系统性构建，以及针对无效输入或业务规则违规的负向测试验证。

Method: 采用LLM驱动的方法，系统性地构建有效测试场景（快乐路径），并推导负向测试用例，以验证预期功能（2xx响应）和针对无效输入或业务规则违规的鲁棒性（4xx响应）。

Result: RESTifAI在性能上与最新的LLM工具（AutoRestTest和LogiAgent）相当，同时解决了可复用性、oracle复杂性和集成方面的限制。提供了比较结果并展示了在工业服务中的适用性。

Conclusion: RESTifAI是一个有效的自动化REST API测试生成工具，能够生成可复用、CI/CD就绪的测试，填补了现有工具在快乐路径和负向测试方面的不足，已在工业环境中验证其适用性。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [20] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 研究代码LLM的多重校准方法，通过考虑代码复杂度、长度、编程语言等因素，显著提升校准效果，在三个函数合成基准测试中优于未校准和基线校准方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成技术的普及，需要确保代码LLM的置信度分数能真实反映代码正确性的概率。当前校准方法可能未充分利用编码问题的额外因素。

Method: 研究了四种多重校准方法，在三个函数合成基准测试上使用最新代码LLM（Qwen3 Coder、GPT-OSS、DeepSeek-R1-Distill），考虑了代码复杂度、长度、编程语言等因素。

Result: 多重校准相比未校准的token似然提升了1.03技能分数，相比基线校准提升了0.37技能分数。消融研究分析了各因素的影响，并公开了包含代码生成、似然值和正确性标签的数据集。

Conclusion: 多重校准能显著提升代码LLM的校准效果，考虑编码问题的额外因素有助于建立更可靠的置信度估计，公开数据集将促进未来代码LLM校准研究。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [21] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 论文分析了软件开发知识问答任务的重要性，发现现有基准测试存在局限性，提出了从真实对话构建SimpleDevQA基准的方法，并验证了代码LLM在该任务上的优势。


<details>
  <summary>Details</summary>
Motivation: 研究发现软件开发知识问答在真实用户-LLM对话中占比最高（39.6%），但现有基准测试主要关注代码理解，忽略了更广泛的开发知识需求，且缺乏基于真实用户查询的评估。

Method: 设计了三阶段管道，将真实世界对话转换为简单的开发知识寻求问答对，构建了多语言基准SimpleDevQA（包含2,740个问答对，涵盖英语、中文和俄语）。

Result: 实验表明：代码LLM普遍优于同等规模的一般LLM；RAG策略平均提升LLM准确率11.3%；LLM在开发知识问答中存在系统性过度自信，且回答准确性与陈述的置信度呈正相关；代码生成能力强的LLM在开发知识问答中也表现更好。

Conclusion: 软件开发知识问答是重要的实际需求，现有评估存在不足，提出的SimpleDevQA基准能更准确地评估LLM在该任务上的能力，代码LLM和RAG策略能有效提升性能。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


### [22] [Exploring the Garden of Forking Paths in Empirical Software Engineering Research: A Multiverse Analysis](https://arxiv.org/abs/2512.08910)
*Nathan Cassee,Robert Feldt*

Main category: cs.SE

TL;DR: 对软件工程实证研究中"分岔路径花园"问题的多宇宙分析，揭示分析方法选择对研究结果的巨大影响


<details>
  <summary>Details</summary>
Motivation: 软件工程实证研究中，研究人员在数据处理、操作化和统计模型选择上有很大自由度，这种"分岔路径花园"现象虽然提供了灵活性，但也威胁到研究的稳健性和可重复性

Method: 采用多宇宙分析方法，选取一篇已发表的软件仓库挖掘研究，识别出9个关键分析决策点，每个点都有至少一个合理的替代方案，系统性地运行所有3072种分析流程

Result: 只有不到0.2%的分析宇宙（6个）重现了已发表的结果，绝大多数产生了定性上不同甚至相反的发现，表明方法选择对结果的影响比通常承认的要深远得多

Conclusion: 建议软件工程研究人员在标准报告基础上增加对合理分析变体的稳健性检查，或至少明确论证每个分析决策的合理性，并提出结构化分类模型来帮助改进方法选择的论证

Abstract: In empirical software engineering (SE) research, researchers have considerable freedom to decide how to process data, what operationalizations to use, and which statistical model to fit. Gelman and Loken refer to this freedom as leading to a "garden of forking paths". Although this freedom is often seen as an advantage, it also poses a threat to robustness and replicability: variations in analytical decisions, even when justifiable, can lead to divergent conclusions.
  To better understand this risk, we conducted a so-called multiverse analysis on a published empirical SE paper. The paper we picked is a Mining Software Repositories study, as MSR studies commonly use non-trivial statistical models to analyze post-hoc, observational data. In the study, we identified nine pivotal analytical decisions-each with at least one equally defensible alternative and systematically reran all the 3,072 resulting analysis pipelines on the original dataset. Interestingly, only 6 of these universes (<0.2%) reproduced the published results; the overwhelming majority produced qualitatively different, and sometimes even opposite, findings.
  This case study of a data analytical method commonly applied to empirical software engineering data reveals how methodological choices can exert a more profound influence on outcomes than is often acknowledged. We therefore advocate that SE researchers complement standard reporting with robustness checks across plausible analysis variants or, at least, explicitly justify each analytical decision. We propose a structured classification model to help classify and improve justification for methodological choices. Secondly, we show how the multiverse analysis is a practical tool in the methodological arsenal of SE researchers, one that can help produce more reliable, reproducible science.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [23] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: 提出结合CXL.mem技术的MPI通信性能评估工具链与扩展性能模型，用于预测CXL.mem在HPC系统中数据交换的性能优势


<details>
  <summary>Details</summary>
Motivation: CXL.mem技术可实现多节点共享内存池，为高效节点间通信提供新可能，但需要评估工具来预测其性能效益

Method: 扩展Mitos内存跟踪采样工具分析MPI应用数据访问模式，建立细粒度性能模型预测CXL.mem相比传统MPI消息的传输优势

Result: 在2D热传导小型应用和HPCG基准测试上验证模型有效性，支持针对性的CXL.mem集成优化

Conclusion: 提出的工具链和性能模型能够识别最可能从CXL.mem获益的MPI调用，为HPC系统中异构内存技术的优化部署提供指导

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [24] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS (CFS) 是首个在POSIX兼容框架内集成多凭证功能的文件系统，基于边缘计算中的全局数据平面构建，使用DataCapsule作为存储提供者。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算环境中，需要一种能够支持多凭证访问控制的文件系统，以提供安全的数据共享和管理能力，同时保持POSIX兼容性。

Method: CFS采用三层架构：1) DataCapsule服务器负责边缘存储和复制；2) 可信执行环境中的中间件管理写入权限；3) POSIX兼容的客户端文件系统。

Result: 实验评估显示CFS读写性能相对适中，但保持了高度的功能正确性，使其成为实际软件开发场景中的可行选择。

Conclusion: CFS成功实现了多凭证通用访问API的目标，为边缘计算环境提供了创新的文件系统解决方案，并指出了未来增强实用性的方向。

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [25] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: Chopper是一个用于分析和可视化多GPU LLM训练性能的框架，首次在AMD MI300X GPU上对Llama 3 8B训练进行了全面的端到端性能分析，发现频率管理（DVFS效应）是影响性能的主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型训练性能的理解主要局限于单GPU微基准测试或内核级性能分析，缺乏对多GPU训练中通信、计算、内存行为和电源管理等复杂交互的系统性研究，特别是在现代GPU架构上的实际表现。

Method: 开发了Chopper框架，该框架能够收集、对齐和可视化跨多个粒度（从单个内核到操作、层、阶段、迭代和GPU）的GPU内核跟踪和硬件性能计数器。使用该框架在8个AMD MI300X GPU节点上对Llama 3 8B模型在FSDP配置下的训练进行了全面分析。

Result: 分析揭示了多个先前未被充分探索的瓶颈和行为：1）内存确定性能够实现更高、更稳定的GPU和内存频率；2）频率开销（DVFS效应）是理论和实际性能差距的最大贡献者，超过了MFMA利用率损失、通信/计算重叠和内核启动开销的影响。

Conclusion: Chopper提供了首个在AMD MI300X GPU上对LLM训练进行多粒度全面分析的工具，为优化训练框架、改进电源管理策略以及指导未来GPU架构和系统设计提供了可操作的见解。

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [26] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: HERMES是一个面向异构计算连续体的新框架，通过资源货币化、编排和语义互操作性，解决云到边缘应用的扩展性、互操作性和信任问题。


<details>
  <summary>Details</summary>
Motivation: 工业对从云到边缘的超分布式应用需求增长，但现有解决方案在可扩展性、互操作性和信任方面存在固有局限性，无法满足这些需求。

Method: 引入HERMES框架，建立开放、无缝、安全的环境，实现从云服务器到边缘设备的智能编排，在分布式市场中货币化数据和服务，通过语义互操作性共享知识。

Result: HERMES通过整合这些关键方面，为新一代分布式应用奠定了基础，使其更高效、可信和自主。

Conclusion: HERMES框架能够转变计算连续体中的连接性和数据利用，解决当前超分布式应用面临的核心挑战。

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [27] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: 提出基于Ozaki-II方案的高性能复数矩阵乘法仿真方法，在INT8矩阵引擎上实现单双精度复数矩阵乘法，相比cuBLAS原生实现获得4-6.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构中低精度矩阵乘法单元比高精度单元具有显著更高的吞吐量，因此利用低精度硬件仿真高精度矩阵乘法成为高性能计算领域的重要研究方向。

Method: 基于Ozaki-II方案，开发在INT8矩阵引擎上仿真单精度和双精度复数矩阵乘法的高性能方法。

Result: 在NVIDIA B200 GPU上，相比cuBLAS原生单双精度复数矩阵乘法，分别获得4.0-5.6倍和4.4-6.5倍加速；可在精度与速度之间灵活权衡。

Conclusion: 提出的方法具有成为广泛应用默认算法的潜力，能够在精度和性能之间提供灵活的选择。

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [28] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: Magneton是一个差分能量调试工具，通过比较相似ML系统的能量消耗，自动识别导致能量浪费的代码区域和配置选择，在9个流行ML系统中发现了16个已知和8个新的能量低效问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的训练和部署能耗极高，现有优化主要关注硬件能效，但软件设计不良导致的能量浪费被忽视。ML框架和应用中存在冗余或设计不良的操作，消耗更多能量却不提升性能，而开发者缺乏检测和诊断这些问题的工具。

Method: 提出差分能量调试方法，基于相似ML系统实现相同功能但能量消耗差异巨大的观察。设计实现Magneton能量分析器，在算子级别比较相似ML系统的能量消耗，自动定位导致过度能量使用的代码区域和配置选择。

Result: 在9个流行ML系统（涵盖LLM推理、通用ML框架和图像生成）中应用Magneton，检测并诊断了16个已知的软件能量低效案例，并进一步发现了8个先前未知的案例，其中7个已得到开发者确认。

Conclusion: 软件能量浪费是ML系统能效的重要问题，差分能量调试方法有效，Magneton工具能够帮助开发者识别和修复ML软件中的能量低效问题，提高整体系统能效。

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [29] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 该论文研究了轻量级线程（协程）环境下的互斥锁实现，发现传统OS线程锁在轻量级线程中会导致死锁，提出了适用于不同轻量级线程库的TTAS和MCS锁改进版本，并推荐使用cohort锁作为通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统多线程数据结构是为操作系统线程设计的，而轻量级线程（协程）的锁实现研究不足。轻量级线程虽然启动和上下文切换开销低，但需要手动调用上下文切换才能实现并行，这导致传统锁实现在新环境下可能失效。

Method: 针对轻量级线程环境，修改了TTAS和MCS锁的实现，特别关注yielding和sleeping两种上下文切换机制。分析了不同轻量级线程库的特性，提出了适用于各种库的cohort锁方案。

Result: 发现传统OS线程锁在轻量级线程中会导致死锁问题，TTAS和MCS锁的性能在不同设置下差异显著。cohort锁通过结合多个MCS队列和公共TTAS，在各种轻量级线程库中都能取得良好性能。

Conclusion: 轻量级线程需要专门的锁实现，不能直接使用传统OS线程锁。cohort锁是适用于不同轻量级线程库的平衡解决方案，结合了MCS和TTAS的优点。

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [30] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 提出一种无需修改代码或干扰分布式执行环境，即可为基于参与者模型的分布式系统生成穷尽测试套件的方法


<details>
  <summary>Details</summary>
Motivation: 分布式系统设计与实现具有挑战性，虽然通常有形式化规范并通过模型检查验证，但实现与规范之间仍存在差距，无法保证实现无缺陷

Method: 使用基于模型的测试方法，将系统模型解释为有限状态自动机，为参与者模型编写的分布式系统高效生成覆盖所有可能状态和转换的穷尽测试套件

Result: 成功验证了基于Viewstamped Replication复制算法的实现，该算法在实际系统中使用

Conclusion: 该方法能有效弥合分布式系统实现与形式化规范之间的差距，无需修改代码或干扰执行环境即可保证实现正确性

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [31] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 通过时空云工作负载转移策略，可显著降低云计算的碳、水和土地利用足迹，空间转移效果更显著，时空结合效果最佳


<details>
  <summary>Details</summary>
Motivation: 研究云计算工作负载的时空转移潜力，以降低云服务对环境的多重影响（碳、水、土地利用足迹）

Method: 使用AWS和Azure的真实云提供商数据及不同应用（大数据分析和FaaS）的工作负载跟踪进行模拟研究

Result: 空间转移可大幅降低20%-85%的环境足迹；时间转移也有降低但效果较小；两者结合效果最佳，主要由空间转移驱动，时间调整提供额外增量效益

Conclusion: 时空云工作负载转移是有效的环境足迹降低策略，对电网混合数据预测误差和季节变化具有鲁棒性

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>
