<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [MS-Index: Fast Top-k Subsequence Search for Multivariate Time Series under Euclidean Distance](https://arxiv.org/abs/2512.14723)
*Jens E. d'Hondt,Teun Kortekaas,Odysseas Papapetrou,Themis Palpanas*

Main category: cs.DB

TL;DR: MS-Index是一种用于多元时间序列子序列搜索的新算法，支持查询时动态选择相关通道，在34个数据集上比现有方法快1-2个数量级。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列分析中，子序列搜索是常见任务，但实际应用中并非所有通道都与特定查询相关。现有方法无法有效支持查询时动态选择相关通道，导致搜索效率低下。

Method: 提出MS-Index算法，这是一种精确的最近邻多元时间序列子序列搜索方法，支持查询时动态选择查询通道，查询性能随查询通道数量呈亚线性扩展。

Result: 在34个数据集上的实验评估显示，MS-Index在原始和归一化子序列搜索中，性能比现有最优方法快1-2个数量级。

Conclusion: MS-Index算法有效解决了多元时间序列子序列搜索中动态通道选择的问题，显著提升了搜索效率，具有实际应用价值。

Abstract: Modern applications frequently collect and analyze temporal data in the form of multivariate time series (MTS) -- time series that contain multiple channels. A common task in this context is subsequence search, which involves identifying all MTS that contain subsequences highly similar to a query time series. In practical scenarios, not all channels of an MTS are relevant to every query. For instance, airplane sensors may gather data on a plethora of components and subsystems, but only a few of these are relevant to a specific query, such as identifying the cause of a malfunctioning landing gear, or a specific flight maneuver. Consequently, the relevant query channels are often specified at query time. In this work, we introduce the Multivariate Subsequence Index (MS-Index), a novel algorithm for nearest neighbor MTS subsequence search under Euclidean distance that supports ad-hoc selection of query channels. The algorithm is exact and demonstrates query performance that scales sublinearly to the number of query channels. We examine the properties of \name with a thorough experimental evaluation over 34 datasets, and show that it outperforms the state-of-the-art one to two orders of magnitude for both raw and normalized subsequences.

</details>


### [2] [Extracting node comparison insights for the interactive exploration of property graphs](https://arxiv.org/abs/2512.15157)
*Cristina Aguiar,Jacques Chabin,Alexandre Chanson,Mirian Halfeld-Ferrari,Nicolas Hiot,Nicolas Labroche,Patrick Marcel,Verónika Peralta,Felipe Vasconcelos*

Main category: cs.DB

TL;DR: 提出一种从属性图中自动提取节点比较的方法，支持交互式探索分析


<details>
  <summary>Details</summary>
Motivation: 虽然图节点评分（如中心性）已研究数十年，但基于属性比较属性图中的节点尚未得到充分研究

Method: 1) 利用节点上下文设计比较指标；2) 定义基于这些指标对节点分组的问题，确保提取的比较既显著又不直接；3) 提出多种启发式算法解决该问题

Result: 在真实属性图数据库上的测试表明，简单启发式算法可在几分钟内获得洞察，而较慢的启发式算法能获得更高质量的洞察

Conclusion: 该方法能有效支持属性图的交互式探索分析，为基于节点属性的比较提供了新思路

Abstract: While scoring nodes in graphs to understand their importance (e.g., in terms of centrality) has been investigated for decades, comparing nodes in property graphs based on their properties has not, to our knowledge, yet been addressed. In this paper, we propose an approach to automatically extract comparison of nodes in property graphs, to support the interactive exploratory analysis of said graphs. We first present a way of devising comparison indicators using the context of nodes to be compared. Then, we formally define the problem of using these indicators to group the nodes so that the comparisons extracted are both significant and not straightforward. We propose various heuristics for solving this problem. Our tests on real property graph databases show that simple heuristics can be used to obtain insights within minutes while slower heuristics are needed to obtain insights of higher quality.

</details>


### [3] [Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting](https://arxiv.org/abs/2512.15308)
*Basil Ell*

Main category: cs.DB

TL;DR: 提出用于有向标记多重图（如RDF图）的图模式关联规则（GPARs），支持生成性任务（扩展图）和评估性任务（评估图的可信度），超越现有形式化方法，在无重复语义下评估图模式，并在概率空间中定义置信度、提升度、杠杆度和确信度等度量。


<details>
  <summary>Details</summary>
Motivation: 现有关联规则形式化方法（如图函数依赖、图实体依赖、关系关联规则、多关系和路径关联规则等）在处理有向标记多重图时存在局限性，需要一种更强大的框架来同时支持生成性和评估性任务，并能更有效地考虑图的拓扑结构。

Method: 提出图模式关联规则（GPARs）框架，在无重复任何语义下评估图模式，定义概率空间，并推导置信度、提升度、杠杆度和确信度等概率度量。分析这些度量与经典项集对应物的关系，并确定其特性保持的条件。

Result: GPARs框架超越了现有相关形式化方法，能够更有效地考虑图拓扑结构，在概率设置中定义了完整的关联规则度量体系，并建立了与经典项集度量的理论关系。

Conclusion: GPARs为有向标记多重图提供了一种强大的关联规则框架，既支持生成性任务也支持评估性任务，通过无重复语义更好地考虑图拓扑，并在概率基础上建立了完整的度量体系，为图数据分析提供了新的理论工具。

Abstract: We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.

</details>


### [4] [Revisiting Task-Oriented Dataset Search in the Era of Large Language Models: Challenges, Benchmark, and Solution](https://arxiv.org/abs/2512.15363)
*Zixin Wei,Yucan Guo,Jinyang Li,Xiaolin Han,Xiaolong Jin,Chenhao Ma*

Main category: cs.DB

TL;DR: KATS是一个面向任务的端到端数据集搜索系统，通过构建任务-数据集知识图谱和混合查询引擎，解决了数据集搜索中的用户意图模糊、任务-数据集映射差距和实体歧义等挑战。


<details>
  <summary>Details</summary>
Motivation: 数据驱动研究中寻找合适数据集是关键的"第一步"，但现有搜索系统难以处理基于高级任务描述的数据集搜索，主要面临三大挑战：用户意图模糊、任务-数据集映射差距和实体歧义。

Method: KATS包含离线知识库构建和在线查询处理两个核心组件。离线部分使用协作多智能体框架构建动态可更新的任务-数据集知识图谱，采用基于语义的机制进行任务实体链接和数据集实体消歧。在线部分使用结合向量搜索和图排名的混合查询引擎。

Result: 在专门设计的CS-TDS基准测试套件上，KATS在效果和效率方面显著优于最先进的检索增强生成框架，为下一代数据集发现系统提供了稳健蓝图。

Conclusion: KATS通过创新的知识图谱构建和混合检索方法，有效解决了任务导向数据集搜索的关键挑战，并提供了标准化评估基准，推动了数据集发现系统的发展。

Abstract: The search for suitable datasets is the critical "first step" in data-driven research, but it remains a great challenge. Researchers often need to search for datasets based on high-level task descriptions. However, existing search systems struggle with this task due to ambiguous user intent, task-to-dataset mapping and benchmark gaps, and entity ambiguity. To address these challenges, we introduce KATS, a novel end-to-end system for task-oriented dataset search from unstructured scientific literature. KATS consists of two key components, i.e., offline knowledge base construction and online query processing. The sophisticated offline pipeline automatically constructs a high-quality, dynamically updatable task-dataset knowledge graph by employing a collaborative multi-agent framework for information extraction, thereby filling the task-to-dataset mapping gap. To further address the challenge of entity ambiguity, a unique semantic-based mechanism is used for task entity linking and dataset entity resolution. For online retrieval, KATS utilizes a specialized hybrid query engine that combines vector search with graph-based ranking to generate highly relevant results. Additionally, we introduce CS-TDS, a tailored benchmark suite for evaluating task-oriented dataset search systems, addressing the critical gap in standardized evaluation. Experiments on our benchmark suite show that KATS significantly outperforms state-of-the-art retrieval-augmented generation frameworks in both effectiveness and efficiency, providing a robust blueprint for the next generation of dataset discovery systems.

</details>


### [5] [ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata](https://arxiv.org/abs/2512.15365)
*Gajendra Doniparthi,Shashank Balu Pandhare,Stefan Deßloch,Timo Mühlhaus*

Main category: cs.DB

TL;DR: ArcBERT是一个基于大语言模型的系统，用于研究数据管理中的元数据探索，能够理解自然语言查询和元数据结构层次


<details>
  <summary>Details</summary>
Motivation: 传统研究数据管理搜索应用需要关键词查询而非自然语言，而大语言模型在领域特定NLP任务中的应用越来越普遍，需要更智能的元数据探索工具

Method: 开发ArcBERT系统，这是一个基于大语言模型的系统，能够理解自然语言查询并依赖语义匹配，同时理解元数据的结构和层次关系

Result: ArcBERT能够有效处理多样化的用户查询模式，提供比传统关键词搜索更智能的元数据探索能力

Conclusion: ArcBERT展示了基于大语言模型的系统在研究数据管理元数据探索中的潜力，能够理解自然语言和元数据结构，提供更有效的搜索体验

Abstract: Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Reexamining Paradigms of End-to-End Data Movement](https://arxiv.org/abs/2512.15028)
*Chin Fang,Timothy Stitt,Michael J. McManus,Toshio Moriya*

Main category: cs.DC

TL;DR: 论文挑战了单纯以网络带宽为中心的数据传输优化观点，指出实际性能瓶颈往往在网络核心之外，需要硬件-软件协同设计的整体方法。


<details>
  <summary>Details</summary>
Motivation: 当前高性能数据传输研究过度关注网络带宽，特别是100Gbps以上的国际链路，但仅关注链路速度无法保证实际边缘到核心端到端的数据移动能力，存在基准测试与实际生产环境的性能差距。

Method: 研究六个常见范式：网络延迟、TCP拥塞控制算法等网络因素，以及CPU性能、虚拟化等主机端因素；使用支持延迟仿真的高速广域网性能预测测试床，并在从资源受限边缘环境到瑞士-加州100Gbps运营链路的多种生产环境中进行广泛测量。

Result: 主要瓶颈通常位于网络核心之外；硬件-软件协同设计能确保从1Gbps到100Gbps及以上速度的数据传输性能一致性；该方法有效缩小了基准测试结果与复杂生产环境之间的性能差距。

Conclusion: 高性能数据传输需要超越单纯网络带宽的视角，采用包含主机端因素的硬件-软件协同设计整体方法，才能在实际生产环境中实现可持续的数据移动能力。

Abstract: The pursuit of high-performance data transfer often focuses on raw network bandwidth, and international links of 100 Gbps or higher are frequently considered the primary enabler. While necessary, this network-centric view is incomplete, equating provisioned link speeds with practical, sustainable data movement capabilities across the entire edge-to-core spectrum. This paper investigates six common paradigms, from the often-cited constraints of network latency and TCP congestion control algorithms to host-side factors such as CPU performance and virtualization that critically impact data movement workflows. We validated our findings using a latency-emulation-capable testbed for high-speed WAN performance prediction and through extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link connecting Switzerland and California, U.S. These results show that the principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design ensures consistent performance, whether moving data at 1 Gbps or 100 Gbps and faster. This approach effectively closes the fidelity gap between benchmark results and diverse and complex production environments.

</details>


### [7] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ是一个用于中等规模语言模型（3B-32B参数）训练的端到端CUDA/C++实现，可在消费级GPU上高效运行，支持单张16GB显卡训练7B模型，4张RTX 4090训练32B模型。


<details>
  <summary>Details</summary>
Motivation: 解决在消费级GPU（内存有限、通信较慢）上训练中等规模语言模型的挑战，使研究人员和个人开发者能够在有限硬件资源下进行模型训练。

Method: 采用端到端CUDA/C++实现，针对内存和通信瓶颈进行优化：激活检查点、卸载技术、基于复制引擎的集合通信，同时保持标准8位训练流程，无需额外算法近似。

Result: 在单张16GB中端游戏显卡上可训练或微调7B模型，在4张RTX 4090的工作站上可训练32B模型，FLOP利用率约50%，效率可与云级GPU上的生产级系统相媲美。

Conclusion: LLMQ证明了在消费级硬件上高效训练中等规模语言模型的可行性，为资源有限的研究者提供了实用的训练解决方案，降低了语言模型训练的门槛。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [8] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 该论文探索GPU上Bloom过滤器的优化设计，通过向量化、线程协作和计算延迟三个维度优化，在保持高精度的同时实现高吞吐量，性能超越现有方法11.35-15.4倍。


<details>
  <summary>Details</summary>
Motivation: GPU具有大规模线程级并行性和高带宽内存，适合加速Bloom过滤器变体至每秒数十亿次操作。虽然CPU优化实现已有深入研究，但GPU设计仍未被充分探索，需要填补这一空白。

Method: 在GPU上从三个维度探索设计空间：向量化、线程协作和计算延迟。提出模块化的CUDA/C++实现，当过滤器适合GPU缓存域时能获得最大性能提升。

Result: 优化设计克服了传统速度与精度之间的权衡，在保持高精度配置优越准确性的同时，实现了通常仅限于高错误率变体的吞吐量。在相同错误率下，批量过滤器查找性能提升11.35倍，构建性能提升15.4倍，在B200 GPU上达到实际速度极限的92%以上。

Conclusion: 该研究填补了GPU上Bloom过滤器设计的空白，通过系统优化实现了突破性的性能提升，同时保持了高精度。提出的模块化实现将开源，为大规模并行架构上的近似成员查询提供了高效解决方案。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


### [9] [LeaseGuard: Raft Leases Done Right](https://arxiv.org/abs/2512.15659)
*A. Jesse Jiryu Davis,Murat Demirbas,Lingzhi Deng*

Main category: cs.DC

TL;DR: LeaseGuard是一种基于Raft的新型租约算法，通过选举保证实现零网络往返的一致读取，显著提升读写性能


<details>
  <summary>Details</summary>
Motivation: Raft系统需要保证读取一致性，现有方法要么通信开销大（安全检查），要么租约协议模糊且损害可用性，导致大多数Raft系统实现不正确或根本不实现租约

Method: 提出LeaseGuard算法，利用Raft选举的特定保证，采用TLA+严格规范，包含两个新颖优化：快速恢复写入吞吐量和提高读取可用性

Result: 将一致读取的网络开销从1次往返降至0次，写入吞吐量从约1000次/秒提升至约10000次/秒，新领导者可立即允许99%的读取成功

Conclusion: LeaseGuard提供了简单、严格规范的租约算法，显著提升分布式数据库的读写性能和可用性，解决了传统租约协议的问题

Abstract: Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.

</details>


### [10] [Dynamic Rebatching for Efficient Early-Exit Inference with DREX](https://arxiv.org/abs/2512.15705)
*Xuting Liu,Daniel Alexander,Siva Kesava Reddy Kakarla,Behnaz Arzani,Vincent Liu*

Main category: cs.DC

TL;DR: DREX系统通过动态重批处理技术解决早期退出LLM的批处理效率问题，在保持输出质量的同时提升吞吐量2-12%，完全消除非自愿退出。


<details>
  <summary>Details</summary>
Motivation: 传统批处理框架不适合早期退出LLM架构，因为批处理中不同请求的退出时机不同。现有解决方案要么强制统一决策错过退出机会，要么强制提前退出降低输出质量。

Method: 提出动态重批处理技术：在早期退出点动态重组批次，满足退出条件的请求立即处理，继续的请求暂存缓冲并重新分组为新批次转发到更深层。实现DREX系统，包含无复制重批处理缓冲和EE/SLA感知调度器两个关键优化。

Result: DREX相比基线方法提升吞吐量2-12%，同时保持输出质量。关键的是完全消除了非自愿退出，为保持EE模型预期的输出质量提供了关键保证。

Conclusion: 动态重批处理是解决早期退出LLM批处理效率的有效方案，DREX系统通过智能调度和内存优化实现了吞吐量提升和质量保证的平衡。

Abstract: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [How Deep Does Your Dependency Tree Go? An Empirical Study of Dependency Amplification Across 10 Package Ecosystems](https://arxiv.org/abs/2512.14739)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 对10个主流包生态系统的依赖放大效应进行实证研究，发现Maven的依赖放大最严重（平均24.7倍），而CocoaPods最低（0.32倍），挑战了npm依赖放大最高的假设。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发依赖包生态系统，单个依赖可能引入大量传递依赖，这种依赖放大对软件供应链安全有重大影响，但不同生态系统间的放大模式尚未进行大规模比较研究。

Method: 对10个主流生态系统的500个项目进行实证研究，包括Maven Central、npm Registry、crates.io、PyPI、NuGet Gallery、RubyGems、Go Modules、Packagist、CocoaPods和Pub，分析依赖放大模式。

Result: Maven平均放大24.70倍最高，Go Modules 4.48倍，npm 4.32倍，CocoaPods 0.32倍最低；28%的Maven项目超过10倍放大，而Cargo、PyPI等生态系统为零；45对比较中有22对存在显著差异。

Conclusion: 依赖放大差异源于生态系统设计选择，如依赖解析行为、标准库完整性和平台约束；建议采用针对特定生态系统的安全策略，包括对Maven进行系统审计，对npm和RubyGems进行异常检测。

Abstract: Modern software development relies on package ecosystems where a single declared dependency can pull in many additional transitive packages. This dependency amplification, defined as the ratio of transitive to direct dependencies, has major implications for software supply chain security, yet amplification patterns across ecosystems have not been compared at scale. We present an empirical study of 500 projects across ten major ecosystems, including Maven Central for Java, npm Registry for JavaScript, crates io for Rust, PyPI for Python, NuGet Gallery for dot NET, RubyGems for Ruby, Go Modules for Go, Packagist for PHP, CocoaPods for Swift and Objective C, and Pub for Dart. Our analysis shows that Maven exhibits mean amplification of 24.70 times, compared to 4.48 times for Go Modules, 4.32 times for npm, and 0.32 times for CocoaPods. We find significant differences with large effect sizes in 22 of 45 pairwise comparisons, challenging the assumption that npm has the highest amplification due to its many small purpose packages. We observe that 28 percent of Maven projects exceed 10 times amplification, indicating a systematic pattern rather than isolated outliers, compared to 14 percent for RubyGems, 12 percent for npm, and zero percent for Cargo, PyPI, Packagist, CocoaPods, and Pub. We attribute these differences to ecosystem design choices such as dependency resolution behavior, standard library completeness, and platform constraints. Our findings suggest adopting ecosystem specific security strategies, including systematic auditing for Maven environments, targeted outlier detection for npm and RubyGems, and continuation of current practices for ecosystems with controlled amplification. We provide a full replication package with data and analysis scripts.

</details>


### [12] [VDMN: A Graphical Notation for Modelling Value Driver Trees](https://arxiv.org/abs/2512.14740)
*Benjamin Matthies*

Main category: cs.SE

TL;DR: 本研究提出了价值驱动建模符号（VDMN），一种用于系统化价值驱动树建模的图形化表示法，填补了该领域缺乏系统建模指南的空白。


<details>
  <summary>Details</summary>
Motivation: 价值驱动树（VDTs）作为概念模型，用于说明和分析关键绩效指标与业务结果之间的因果关系，支持管理决策和价值管理。然而，尽管应用日益广泛，目前仍缺乏系统化的建模指南。

Method: 开发了价值驱动建模符号（VDMN），包括一套全面的语义构造和直观的图形语法。通过两个案例研究应用该符号，并通过专家访谈评估其实用性。

Result: 研究结果表明，VDMN支持一致且易于理解的价值驱动树建模。专家评估证实了该符号的实用价值。

Conclusion: VDMN代表了价值驱动树建模系统化和标准化的重要一步，为管理决策提供了更规范的工具支持。

Abstract: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.

</details>


### [13] [Revisiting the Reliability of Language Models in Instruction-Following](https://arxiv.org/abs/2512.14754)
*Jianshuo Dong,Yutong Zhang,Yan Liu,Zhenyu Zhong,Tao Wei,Chao Zhang,Han Qiu*

Main category: cs.SE

TL;DR: 本文提出nuance-oriented reliability概念，发现当前LLMs在细微提示修改下性能下降显著（最高61.8%），并开发了IFEval++基准和reliable@k指标进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 虽然先进LLMs在IFEval等基准上取得了接近天花板的指令跟随准确率，但这些高分并不一定能转化为现实世界中的可靠服务，因为用户经常变化措辞、上下文框架和任务表述。需要研究模型在表达相似意图但带有细微差别的"cousin prompts"上是否表现一致。

Method: 提出reliable@k新指标，开发自动化流水线通过数据增强生成高质量的cousin prompts，构建IFEval++进行系统评估，并在20个专有和26个开源LLMs上进行测试。

Result: 当前模型在nuance-oriented reliability方面存在严重不足，在细微提示修改下性能下降最高可达61.8%。同时对该现象进行了特征分析，并探索了三种潜在的改进方法。

Conclusion: nuance-oriented reliability是迈向更可靠、更可信赖LLM行为的关键但尚未充分探索的下一步。研究强调了这一维度的重要性，并提供了评估工具和基准。

Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.

</details>


### [14] [Examining Software Developers' Needs for Privacy Enforcing Techniques: A survey](https://arxiv.org/abs/2512.14756)
*Ioanna Theophilou,Georgia M. Kapitsaki*

Main category: cs.SE

TL;DR: 调查显示开发者需要更多自动化工具来满足数据隐私法规合规要求，隐私经验丰富的开发者对隐私工具需求更迫切


<details>
  <summary>Details</summary>
Motivation: GDPR、CCPA/CPRA等数据隐私法规要求软件系统必须合规，但开发者缺乏法律知识，难以在系统中集成合规功能。现有研究关注开发者对隐私原则的理解，但尚未研究开发者在隐私法规合规方面的具体需求，这些需求对开发自动化工具（如生成式AI）至关重要。

Method: 通过问卷调查68名开发者，研究他们在隐私法规合规方面的需求，并分析影响这些需求的因素。

Result: 大多数开发者表示需要更多自动化工具来协助隐私合规，隐私经验越丰富的开发者对隐私工具的需求越迫切。

Conclusion: 研究结果有助于开发者更好地定位隐私法规合规工作，并指出迫切需要开发隐私促进工具来简化合规流程。

Abstract: Data privacy legislation, such as GDPR and CCPA/CPRA, has rendered data privacy law compliance a requirement of all software systems. Developers need to implement various kinds of functionalities to cover law needs, including user rights and law principles. As data compliance is tightly coupled with legal knowledge, it is not always easy to perform such integrations in software systems. Prior studies have focused on developers' understanding of privacy principles, such as Privacy by Design, and have examined privacy techniques used in the software industry. Nevertheless, emerging developer needs that can assist in privacy law compliance have not been examined but are useful in understanding what development automation tools, such as Generative AI, need to cover to make the compliance process more straightforward and seamless within the development process. In this work, we present a survey that examines the above needs with the participation of 68 developers, while we have examined which factors affect practitioners' needs. Most developers express a need for more automated tools, while privacy experience increases practitioners' concerns for privacy tools. Our results can assist practitioners in better positioning their development activities within privacy law compliance and point to an urgent need for privacy facilitators.

</details>


### [15] [CAPE: Capability Achievement via Policy Execution](https://arxiv.org/abs/2512.14761)
*David Ball*

Main category: cs.SE

TL;DR: CAPE协议通过将需求转化为可执行规范并训练模型来满足这些规范，解决了AI系统缺乏表达和执行需求能力的问题，将违规率降低了81%，成本降低了5-20倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统缺乏表达和执行需求的能力。预训练产生智能，后训练优化偏好，但都不能保证模型可靠地满足明确的、上下文相关的约束。这解释了为什么高智能模型在部署中经常失败，尽管在基准测试中表现良好。

Method: 引入能力工程（Capability Engineering）的系统实践，通过CAPE协议实现Specify -> Verify -> Correct -> Train循环。该方法基于两个实证发现：上下文客观性（固定上下文后主观属性变得客观）和验证保真度缩放（验证准确性随模型规模提升）。

Result: 在六个领域的109,500个示例中，CAPE相对于DPO将违规率降低了81%（标准差小于0.3%）。通过用可重用规范替代逐示例标注，CAPE将成本降低了5-20倍，时间线从数月缩短到数周。

Conclusion: CAPE协议、PredicateGraph模式、CPL规范语言和政策包已发布在Apache 2.0下。同时推出了CapabilityBench，这是一个针对社区贡献政策的模型评估公共注册表，将评估从智能基准转向能力测量。

Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.

</details>


### [16] [Workflows vs Agents for Code Translation](https://arxiv.org/abs/2512.14762)
*Henry Gray,Tom Yotam,Octavian Udrea*

Main category: cs.SE

TL;DR: 比较两种基于大语言模型的MATLAB到HDL语法修复方法：结构化专家流程与自主代理方法，后者使用MCP动态选择工具，在中小规模模型上效果更好。


<details>
  <summary>Details</summary>
Motivation: MATLAB到HDL的转换是FPGA/ASIC部署的关键但资源密集型步骤。虽然大语言模型提供了自动化路径，但由于HDL代码训练有限，端到端转换容易产生语法错误且脆弱。

Method: 比较两种LLM驱动的语法修复方法：1）结构化专家设计的固定操作流程；2）使用模型上下文协议（MCP）动态选择工具的自主代理方法。在42个MATLAB信号处理函数上测试，专注于语法修复阶段。

Result: 自主代理方法在解决初始语法错误方面更有效，能够解锁更多候选代码进入流水线。上游改进带来可测量的下游改进，特别是在中型模型上，将仿真成功率提高了20多个百分点。条件检索在8B和30B模型中有效，在235B模型中最终成功增益较小。

Conclusion: 当设计适当时，这些代理框架在补偿中小规模模型的能力限制方面最为有效。条件检索对中小模型有帮助，而大规模模型则可以从简单的RAG变体获得最高最终成功率。

Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.

</details>


### [17] [Let the Barbarians In: How AI Can Accelerate Systems Performance Research](https://arxiv.org/abs/2512.14806)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Shubham Agarwal,Mert Cemri,Bowen Wang,Alexander Krentsel,Tian Xia,Jongseok Park,Shuo Yang,Jeff Chen,Lakshya Agrawal,Ashwin Naren,Shulu Li,Ruiying Ma,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.SE

TL;DR: 论文提出AI驱动的系统研究（ADRS）范式，通过AI生成、评估、优化的迭代循环来自动化系统性能问题的解决方案发现，并在多个案例中证明ADRS生成的方案能达到或超越人类设计的SOTA水平。


<details>
  <summary>Details</summary>
Motivation: AI正在变革研究过程，但需要可靠的验证器来验证AI生成的候选方案。系统性能研究特别适合这种范式，因为系统性能问题天然具备验证条件——可以在真实系统或模拟器中实现候选方案并用预定义工作负载进行评估。

Method: 提出AI驱动的系统研究（ADRS）范式，包含生成、评估、优化的迭代循环。使用多个开源ADRS实例（OpenEvolve、GEPA、ShinkaEvolve），在十个案例研究（如多云区域调度、MoE负载均衡、LLM-based SQL、事务调度等）中验证方法有效性。

Result: ADRS生成的解决方案能够匹配甚至超越人类设计的最先进方案。基于这些发现，论文总结了有效使用ADRS的最佳实践（如提示规范程度、反馈量、鲁棒评估等）。

Conclusion: 虽然还没有适用于所有系统研究的通用ADRS配方，但初步发现和识别的挑战为未来研究提供了有意义的指导。随着研究人员精力逐渐转向问题制定和战略监督，ADRS范式有望推动系统研究的发展。

Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight. Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.

</details>


### [18] [Industry Expectations and Skill Demands in Quantum Software Testing](https://arxiv.org/abs/2512.14861)
*Ronnie de Souza Santos,Teresa Baldassarre,Cesar França*

Main category: cs.SE

TL;DR: 量子软件测试结合传统软件质量保证与实验验证，需要专业人员具备编程自动化与量子技术知识及跨学科协作能力。


<details>
  <summary>Details</summary>
Motivation: 量子软件测试面临与经典软件工程根本不同的新挑战，本研究旨在了解量子软件行业如何定义测试角色以及对这些职位专业人员的技能期望。

Method: 分析了110个来自量子软件和硬件开发组织的招聘职位，识别与测试相关的活动、能力和技能要求。

Result: 量子环境下的测试结合了传统软件质量保证与实验验证，强调校准、控制和混合量子-经典验证。雇主寻求具备编程自动化专业知识、量子特定技术知识和跨学科协作技能的专业人员。

Conclusion: 量子软件测试仍处于早期但快速发展的阶段，连接了软件工程和实验物理，突显了需要使测试实践与工业现实相一致的教育和研究努力。

Abstract: Quantum software testing introduces new challenges that differ fundamentally from those in classical software engineering. Aims: This study investigates how the quantum software industry defines testing roles and what skills are expected from professionals in these positions. Method: We analyzed 110 job postings from organizations involved in quantum software and hardware development, identifying activities, competencies, and skill requirements related to testing. Results: The findings show that testing in quantum contexts combines traditional software quality assurance with experimental validation, emphasizing calibration, control, and hybrid quantum-classical verification. Employers seek professionals who integrate programming and automation expertise with quantum-specific technical knowledge and interdisciplinary collaboration skills. Conclusions: Quantum software testing remains at an early but rapidly evolving stage that bridges software engineering and experimental physics, highlighting the need for educational and research efforts that align testing practices with industrial realities.

</details>


### [19] [Evaluating Code Reasoning Abilities of Large Language Models Under Real-World Settings](https://arxiv.org/abs/2512.14917)
*Changshu Liu,Alireza Ghazanfari,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: RE2-Bench是一个包含1,101个代码推理问题的基准测试，其中195个来自成熟的实际项目，通过静态和动态程序分析处理复杂类型，将问题分为Easy和Hard两类，揭示了LLMs在实际复杂代码推理中的性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准测试使用简单程序，无法反映真实世界的复杂性（如过程间依赖、API调用、嵌套结构、复杂类型等），导致对LLMs泛化能力的评估存在偏差，需要更真实的评估基准。

Method: 提出RE2-Bench基准，包含1,101个推理问题，其中195个来自真实项目；利用静态和动态程序分析自动序列化和反序列化复合、复杂和自定义类型；通过9个可解释的代码复杂度指标，采用多数投票机制将问题分类为Easy或Hard两个难度等级。

Result: 对6个通用和推理导向的LLMs在两个代码推理任务（输入预测和输出预测）上的评估显示，从Easy到Hard问题的性能显著下降（输入预测下降51.50%，输出预测下降42.15%），证实先前评估高估了LLMs的推理能力。

Conclusion: RE2-Bench提供了一个更真实的代码推理评估基准，揭示了LLMs在面对真实世界代码复杂性时的局限性，表明现有评估方法严重高估了LLMs的实际推理能力。

Abstract: Code reasoning tasks are becoming prevalent in large language model (LLM) assessments. Existing benchmarks involve simple programs, failing to represent real-world complexities such as inter- or intra-procedural dependencies, core or third-party API calls, highly nested constructs, and non-primitive complex types. Evaluating LLMs under such a simplistic setting poses a significant threat to assumptions about their generalizability in practice. To enable a more realistic evaluation of code reasoning, this paper proposes RE2-Bench, a benchmark of 1,101 reasoning problems, including 195 drawn from mature real-world projects. RE2-Bench leverages static and dynamic program analysis to automatically serialize and deserialize compound, complex, and custom types in real-world code, going far beyond the primitive-only settings used in prior work.
  A key feature of RE2-Bench is categorizing each reasoning problem as Easy or Hard via a principled majority-vote mechanism over nine interpretable code complexity metrics, resulting in two well-separated and semantically meaningful difficulty categories suitable for precise calibration of LLM reasoning ability. A comprehensive evaluation of six general-purpose and reasoning-oriented LLMs on two widely used code reasoning tasks -- input prediction and output prediction -- using RE2-Bench reveals a significant performance drop from Easy to Hard problems (51.50\% for input prediction and 42.15\% for output prediction), confirming that prior evaluations substantially overestimate the reasoning capabilities of LLMs.

</details>


### [20] [Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent](https://arxiv.org/abs/2512.14990)
*Mehil B Shah,Mohammad Masudur Rahman,Foutse Khomh*

Main category: cs.SE

TL;DR: RepGen：一种基于LLM的自动化深度学习bug复现方法，相比现有技术将复现率从60.38%提升到80.19%


<details>
  <summary>Details</summary>
Motivation: 深度学习应用存在大量bug，但由于DL模型的非确定性和软硬件环境耦合，手动复现率极低（仅约3%），需要自动化解决方案

Method: 构建学习增强的项目上下文，制定全面的bug复现计划，采用迭代的生成-验证-精炼机制，使用LLM生成能够复现bug的代码

Result: 在106个真实DL bug上实现80.19%的复现率，比最先进技术提升19.81%；开发者研究中，成功复现率提升23.35%，时间减少56.8%，认知负荷降低

Conclusion: RepGen是首个自动化、智能化的DL bug复现方法，显著提高了复现效率和成功率，为DL系统调试提供了有效工具

Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.

</details>


### [21] [Toxicity Ahead: Forecasting Conversational Derailment on GitHub](https://arxiv.org/abs/2512.15031)
*Mia Mohammad Imran,Robert Zita,Rahat Rizvi Rahman,Preetha Chatterjee,Kostadin Damevski*

Main category: cs.SE

TL;DR: 本文提出一个基于LLM的两步提示框架，用于预测GitHub开源社区对话的毒性偏离，通过生成对话动态摘要来早期检测有害对话，性能优于传统NLP方法。


<details>
  <summary>Details</summary>
Motivation: 开源软件社区的毒性互动会减少贡献者参与度并威胁项目可持续性。目前大多数主动审核策略需要人工操作，耗费维护者大量时间和精力，需要更可扩展的自动化方法来预防毒性对话的发生。

Method: 提出基于LLM的两步提示框架：1) 使用Least-to-Most提示策略生成对话动态摘要(SCDs)；2) 利用这些摘要估计对话偏离的可能性。在Qwen和Llama模型上评估，并构建了包含159个毒性线程和207个非毒性线程的数据集。

Result: LtM策略在决策阈值为0.3时，Qwen和Llama模型分别达到0.901和0.852的F1分数，优于传统NLP基线。在包含308个GitHub问题线程的外部验证集上，F1分数最高达0.797。分析发现紧张触发因素、情感转变和特定对话模式可以预测毒性。

Conclusion: 结构化LLM提示框架能有效早期检测开源软件社区的对话偏离，支持主动且可解释的审核，有助于预防毒性互动并维持健康的社区环境。

Abstract: Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.
  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.

</details>


### [22] [An Exploratory Study of Bayesian Prompt Optimization for Test-Driven Code Generation with Large Language Models](https://arxiv.org/abs/2512.15076)
*Shlok Tomar,Aryan Deshwal,Ethan Villalovoz,Mattia Fazzini,Haipeng Cai,Janardhan Rao Doppa*

Main category: cs.SE

TL;DR: BODE-GEN使用贝叶斯优化在连续嵌入空间中搜索最佳提示，以提高LLM生成代码的功能正确性


<details>
  <summary>Details</summary>
Motivation: LLM生成代码的正确性受提示影响很大，手动提示工程耗时且不一定能找到最优提示，需要系统化的方法自动搜索最佳提示

Method: 提出BODE-GEN方法：1) 使用辅助LLM将离散提示空间映射到连续嵌入空间；2) 采用随机投影和维度缩放先验构建高斯过程代理模型；3) 在嵌入空间中进行贝叶斯优化搜索最佳提示

Result: 在HumanEval+基准测试中，BODE-GEN相比固定提示和手动提示工程能提高代码生成准确率，且样本效率高，只需较少优化迭代就能改进代码准确性

Conclusion: BODE-GEN提供了一种数据驱动、自适应的提示搜索方法，能有效提升LLM代码生成的功能正确性，为自动提示优化提供了新思路

Abstract: We consider the task of generating functionally correct code using large language models (LLMs). The correctness of generated code is influenced by the prompt used to query the given base LLM. We formulate the problem of finding the appropriate prompt as combinatorial search process and propose a Bayesian optimization (BO) approach referred to as {\em BO for Code GENeration (BODE-GEN)}. BODE-GEN performs an adaptive data-driven search over prompts guided by training data in the form of prompts tried and the functional accuracy of the generated code over a set of given test cases. The key insight is to perform BO in continuous embedding space by using an auxiliary LLM to bridge the gap between discrete prompt space and continuous embedding space. We leverage two synergistic ideas, namely, random projections and dimensionality scaled priors, to build effective Gaussian process based surrogate models over the high-dimensional embedding space. Our experiments on the HumanEval+ benchmark using multiple base LLMs show that BODE-GEN can improve performance in terms of code generation accuracy compared to fixed prompts and manual prompt engineering. Additionally, we demonstrate that BODE-GEN is sample-efficient, requiring relatively few iterations of BO to demonstrate improvements in code accuracy.

</details>


### [23] [Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering](https://arxiv.org/abs/2512.15148)
*Hang Yu,Yuzhou Lai,Li Zhang,Xiaoli Lian,Fang Liu,Yanrui Dong,Ting Zhang,Zhi Jin,David Lo*

Main category: cs.SE

TL;DR: 该论文通过系统分析1367篇学术论文和282份工业界调查，揭示LLM驱动的软件工程研究中学术进展与工业需求之间的差距，并提出七个关键启示以指导未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型正在重塑软件工程领域，学术研究在自动化测试、程序修复等方面持续取得进展，但这些学术进展与真实工业需求的对齐程度仍不明确。需要弥合学术研究与工业实践之间的鸿沟。

Method: 1. 系统分析2022-2025年间FSE、ASE、ICSE三个顶级SE会议的1367篇论文，识别研究主题、常用基准、工业相关性和开源可用性
2. 对17个组织进行实证调查，通过结构化问卷收集282份关于六个关键主题的反馈：程序分析、自动化测试、代码生成/补全、问题解决、预训练代码模型和依赖管理

Result: 通过对比学术能力与工业反馈，得出七个关键启示：1) 软件需求和架构方面的挑战未得到充分解决；2) 智能SE方法的可靠性和可解释性问题；3) 学术研究中的输入假设问题；4) 实际评估中的紧张关系；5) 伦理考量；6) 学术基准与工业需求的脱节；7) 开源可用性不足

Conclusion: 该研究旨在重新聚焦学术界对重要但未充分探索问题的关注，并指导未来软件工程研究朝着更大的工业影响力方向发展，弥合学术进展与工业实践之间的差距。

Abstract: The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.

</details>


### [24] [Automating Execution and Verification of BPMN+DMN Business Processes](https://arxiv.org/abs/2512.15214)
*Giuseppe Della Penna,Igor Melatti*

Main category: cs.SE

TL;DR: BDTransTest工具将BPMN+DMN业务流程转换为Java程序，并生成测试计划来检测语义错误，提高流程验证的自动化程度


<details>
  <summary>Details</summary>
Motivation: 现有BPMN+DMN建模框架只能检测语法错误，无法发现语义（行为）故障，设计师需要手动运行流程来检测问题，且专有工具的转换过程不透明

Method: 开发BDTransTest工具，提供：1) BPMN+DMN流程到Java程序的转换；2) 测试计划的合成与执行，可能需要设计师澄清输入域；3) 测试覆盖率分析（节点和边）

Result: 在文献中的BPMN+DMN流程上进行了实验评估，展示了工具的有效性

Conclusion: BDTransTest通过自动化翻译和测试计划生成，改进了BPMN+DMN流程的语义验证能力，解决了现有工具只能检测语法错误的局限性

Abstract: The increasing and widespread use of BPMN business processes, also embodying DMN tables, requires tools and methodologies to verify their correctness. However, most commonly used frameworks to build BPMN+DMN models only allow designers to detect syntactical errors, thus ignoring semantic (behavioural) faults. This forces business processes designers to manually run single executions of their BPMN+DMN processes using proprietary tools in order to detect failures. Furthermore, how proprietary tools translate a BPMN+DMN process to a computer simulation is left unspecified. In this paper, we advance this state of the art by designing a tool, named BDTransTest providing: i) a translation from a BPMN + DMN process B to a Java program P ; ii) the synthesis and execution of a testing plan for B, that may require the business designer to disambiguate some input domain; iii) the analysis of the coverage achieved by the testing plan in terms of nodes and edges of B. Finally, we provide an experimental evaluation of our methodology on BPMN+DMN processes from the literature.

</details>


### [25] [Heterogeneous Model Alignment in Digital Twin](https://arxiv.org/abs/2512.15281)
*Faima Abbasi,Jean-Sébastien Sottet,Cedric Pruski*

Main category: cs.SE

TL;DR: 提出一种用于多层模型驱动数字孪生的异构模型对齐框架，通过自适应一致性机制和LLM验证的对齐过程，自动发现语义对应关系，减少人工映射，提高跨模型类型的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术整合异构数据和模型，但多层模型驱动数字孪生中异构模型跨抽象层对齐存在挑战，导致语义不匹配、不一致和同步问题。现有方法依赖静态映射和手动更新，通常不灵活、易出错且可能损害数据完整性。

Method: 提出异构模型对齐框架，包含灵活性机制使元模型能够自适应互连，同时保持跨抽象层的语义一致性。整合：(i) 自适应一致性机制，将元模型与演化模型链接；(ii) LLM验证的对齐过程，将元模型基于领域知识，确保整个数字孪生生命周期中的结构保真度和概念一致性。

Result: 该方法自动发现语义对应关系，最小化人工映射，增强跨不同模型类型的可扩展性。通过空气质量用例说明该方法，并使用OAEI测试用例验证其性能。

Conclusion: 提出的异构模型对齐框架解决了多层模型驱动数字孪生中的模型对齐挑战，通过自适应机制和LLM验证确保语义一致性，提高了数字孪生系统的灵活性和可靠性。

Abstract: Digital twin (DT) technology integrates heterogeneous data and models, along with semantic technologies to create multi-layered digital representation of physical systems. DTs enable monitoring, simulation, prediction, and optimization to enhance decision making and operational efficiency. A key challenge in multi-layered, model-driven DTs is aligning heterogeneous models across abstraction layers, which can lead to semantic mismatches, inconsistencies, and synchronization issues. Existing methods, relying on static mappings and manual updates, are often inflexible, error-prone, and risk compromising data integrity. To address these limitations, we present a heterogeneous model alignment approach for multi-layered, model-driven DTs. The framework incorporates a flexibility mechanism that allows metamodels to adapt and interconnect seamlessly while maintaining semantic coherence across abstraction layers. It integrates: (i) adaptive conformance mechanisms that link metamodels with evolving models and (ii) a large language model (LLM) validated alignment process that grounds metamodels in domain knowledge, ensuring structural fidelity and conceptual consistency throughout the DT lifecycle. This approach automates semantic correspondences discovery, minimizes manual mapping, and enhances scalability across diverse model types. We illustrate the approach using air quality use case and validate its performance using different test cases from Ontology Alignment Evaluation Initiative (OAEI) tracks.

</details>


### [26] [Can AI Generate more Comprehensive Test Scenarios? Review on Automated Driving Systems Test Scenario Generation Methods](https://arxiv.org/abs/2512.15422)
*Ji Zhou,Yongqi Zhao,Yixian Hu,Hexuan Li,Zhengguo Gu,Nan Xu,Arno Eichberger*

Main category: cs.SE

TL;DR: 该论文系统综述了2015-2025年自动驾驶系统场景测试方法，重点关注2023-2025年AI辅助和多模态方法的发展，提出了分类法、伦理检查表和ODD覆盖图等贡献。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶系统验证方法（如大规模道路测试）成本高、耗时长，而现有场景测试综述未能全面覆盖最新方法和技术进展，需要系统性分析以支持安全可靠的自动驾驶部署。

Method: 通过全面检索2015-2025年的文献，系统分析了31篇主要研究和10篇综述，重点关注2023-2025年的最新框架，对传统方法和新兴AI生成方法进行对比评估。

Result: 识别出三个持续存在的差距：缺乏标准化评估指标、伦理和人为因素整合不足、多模态和特定ODD场景覆盖不够。传统方法依赖专家知识和事故数据，而新方法利用生成模型（LLM、GAN、扩散模型、强化学习）合成多样化安全关键场景。

Conclusion: 提出了包含多模态扩展的细化分类法、负责任场景设计的伦理安全检查表，以及带有场景难度模式的ODD覆盖图，为研究人员提供方法学清晰度，为行业提供实践指导，支持可重复评估并加速高级别自动驾驶的安全部署。

Abstract: Ensuring the safety and reliability of Automated Driving Systems (ADS) remains a critical challenge, as traditional verification methods such as large-scale on-road testing are prohibitively costly and time-consuming.To address this,scenario-based testing has emerged as a scalable and efficient alternative,yet existing surveys provide only partial coverage of recent methodological and technological advances.This review systematically analyzes 31 primary studies,and 10 surveys identified through a comprehensive search spanning 2015~2025;however,the in-depth methodological synthesis and comparative evaluation focus primarily on recent frameworks(2023~2025),reflecting the surge of Artificial Intelligent(AI)-assisted and multimodal approaches in this period.Traditional approaches rely on expert knowledge,ontologies,and naturalistic driving or accident data,while recent developments leverage generative models,including large language models,generative adversarial networks,diffusion models,and reinforcement learning frameworks,to synthesize diverse and safety-critical scenarios.Our synthesis identifies three persistent gaps:the absence of standardized evaluation metrics,limited integration of ethical and human factors,and insufficient coverage of multimodal and Operational Design Domain (ODD)-specific scenarios.To address these challenges,this review contributes a refined taxonomy that incorporates multimodal extensions,an ethical and safety checklist for responsible scenario design,and an ODD coverage map with a scenario-difficulty schema to enable transparent benchmarking.Collectively,these contributions provide methodological clarity for researchers and practical guidance for industry,supporting reproducible evaluation and accelerating the safe deployment of higher-level ADS.

</details>


### [27] [Insecure Ingredients? Exploring Dependency Update Patterns of Bundled JavaScript Packages on the Web](https://arxiv.org/abs/2512.15447)
*Ben Swierzy,Marc Ohm,Michael Meier*

Main category: cs.SE

TL;DR: Aletheia：一种通过抄袭检测算法从JavaScript包中识别软件包版本的方法，显著优于现有方法。研究发现5-20%的网站在16周内更新依赖，捆绑包更新速度比CDN包快10倍，但少数大型供应商主导了及时更新。


<details>
  <summary>Details</summary>
Motivation: JavaScript生态系统中存在大量易受攻击的软件包版本，但现有检测方法无法大规模分析现代Web应用的依赖更新行为。需要一种包无关的方法来准确识别生产网站中使用的软件包版本。

Method: 提出Aletheia方法，利用抄袭检测领域的算法来剖析JavaScript包，识别其中使用的软件包版本。该方法不针对特定软件包，能够处理打包后的代码。

Result: Aletheia在实践环境中明显优于现有方法。对Tranco前10万个域名的爬取显示：5-20%的域名在16周内更新依赖；捆绑包更新速度显著快于CDN包，易受攻击版本减少10倍；少数大型供应商是及时更新的主要推动力。

Conclusion: 虽然捆绑包依赖更新更快、安全性更好，但定量指标不能完全反映实际情况，因为少数大型供应商主导了更新趋势。需要更全面的视角来理解依赖更新行为。

Abstract: Reusable software components, typically distributed as packages, are a central paradigm of modern software development. The JavaScript ecosystem serves as a prime example, offering millions of packages with their use being promoted as idiomatic. However, download statistics on npm raise security concerns as they indicate a high popularity of vulnerable package versions while their real prevalence on production websites remains unknown. Package version detection mechanisms fill this gap by extracting utilized packages and versions from observed artifacts on the web. Prior research focuses on mechanisms for either hand-selected popular packages in bundles or for single-file resources utilizing the global namespace. This does not allow for a thorough analysis of modern web applications' dependency update behavior at scale. In this work, we improve upon this by presenting Aletheia, a package-agnostic method which dissects JavaScript bundles to identify package versions through algorithms originating from the field of plagiarism detection. We show that this method clearly outperforms the existing approaches in practical settings. Furthermore, we crawl the Tranco top 100,000 domains to reveal that 5% - 20% of domains update their dependencies within 16 weeks. Surprisingly, from a longitudinal perspective, bundled packages are updated significantly faster than their CDN-included counterparts, with consequently up to 10 times fewer known vulnerable package versions included. Still, we observe indicators that few widespread vendors seem to be a major driving force behind timely updates, implying that quantitative measures are not painting a complete picture.

</details>


### [28] [A Container-based Approach For Proactive Asset Administration Shell Digital Twins](https://arxiv.org/abs/2512.15452)
*Carsten Ellwein,Jingxi Zhang,Andreas Wortmann,Antony Ayman Alfy Meckhael*

Main category: cs.SE

TL;DR: 提出基于子模型的AAS架构，通过容器化服务集成使数字孪生从被动数据存储转变为主动执行增值服务的动态系统


<details>
  <summary>Details</summary>
Motivation: 现有AAS作为静态信息模型，缺乏动态服务集成和系统适应能力，无法支持可执行行为和主动功能

Method: 提出子模型架构，在AAS中引入结构化服务概念，通过扩展子模型包含行为定义，构建模块化事件驱动架构，基于触发条件部署容器化服务

Result: 通过3轴铣床案例研究验证，使AAS不仅能作为被动数字表示，还能作为执行增值服务的主动接口

Conclusion: 该架构为数字孪生环境中未来AI驱动适应和系统级智能奠定了基础，实现了从静态信息模型到动态服务执行平台的转变

Abstract: In manufacturing, digital twins, realized as Asset Administration Shells (AAS), have emerged as a prevalent practice. These digital replicas, often utilized as structured repositories of asset-related data, facilitate interoperability across diverse systems. However, extant approaches treat the AAS as a static information model, lacking support for dynamic service integration and system adaptation. The existing body of literature has not yet thoroughly explored the potential for integrating executable behavior, particularly in the form of containerized services, into or from the AAS. This integration could serve to enable proactive functionality. In this paper, we propose a submodel-based architecture that introduces a structured service notion to the AAS, enabling services to dynamically interact with and adapt AAS instances at runtime. This concept is implemented through the extension of a submodel with behavioral definitions, resulting in a modular event-driven architecture capable of deploying containerized services based on embedded trigger conditions. The approach is illustrated through a case study on a 3-axis milling machine. Our contribution enables the AAS to serve not only as a passive digital representation but also as an active interface for executing added-value services.%, thereby laying the foundation for future AI-driven adaptation and system-level intelligence in digital twin environments.

</details>


### [29] [On Assessing the Relevance of Code Reviews Authored by Generative Models](https://arxiv.org/abs/2512.15466)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 提出基于多主观排序的代码审查评估方法，发现ChatGPT生成的评论质量显著优于人类评论，甚至超过StackExchange的采纳答案


<details>
  <summary>Details</summary>
Motivation: 现有代码审查生成评估方法存在缺陷：要么依赖与单一标准答案的自动比较（无法捕捉人类观点的多样性），要么依赖主观的"有用性"评估（概念模糊）。需要更有效的评估方法来衡量大语言模型在代码审查中的表现。

Method: 提出多主观排序评估方法，使用CodeReview StackExchange的280个独立代码审查请求和对应评论数据集，让多位人类评委对ChatGPT生成的评论与平台上最佳人类回答进行质量排序比较。

Result: ChatGPT生成的评论质量排名显著优于人类评论，甚至超过了StackExchange的采纳答案。这表明大语言模型在代码审查任务中具有优异表现。

Conclusion: 提出的多主观排序方法为生成式AI在代码审查中的性能评估提供了更有意义的评估框架，同时也警示了将AI不加检查地集成到审查流程中的潜在风险。

Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.

</details>


### [30] [How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?](https://arxiv.org/abs/2512.15468)
*Hua Yang,Alejandro Velasco,Thanh Le-Cong,Md Nazmul Haque,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 研究发现语义等价代码转换技术可有效规避成员推断检测，变量重命名规则能降低10.19%的检测成功率，暴露了代码大模型许可合规执行中的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 代码大模型训练依赖大量公开和私有代码数据，引发知识产权合规问题。现有成员推断技术用于检测未经授权使用，但可能被语义等价代码转换技术规避，需要系统研究这种规避风险。

Method: 系统研究语义等价代码转换规则对成员推断检测的影响，分析不同转换规则（包括变量重命名等）的效果，进行因果分析验证变量重命名的因果效应，并测试多种转换组合的效果。

Result: 最坏情况下每个转换规则仅使模型准确率下降1.5%，转换后数据集可有效替代微调。变量重命名规则能降低成员推断成功率10.19%，因果分析确认其破坏检测的因果效应最强。多种转换组合不会进一步降低检测效果。

Conclusion: 语义等价代码转换技术可显著削弱成员推断检测效果，暴露了代码大模型许可合规执行的关键漏洞，表明基于转换的混淆技术能有效规避知识产权合规检测。

Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.

</details>


### [31] [WuppieFuzz: Coverage-Guided, Stateful REST API Fuzzing](https://arxiv.org/abs/2512.15554)
*Thomas Rooijakkers,Anne Nijsten,Cristian Daniele,Erieke Weitenberg,Ringo Groenewegen,Arthur Melissen*

Main category: cs.SE

TL;DR: WuppieFuzz是一个基于LibAFL的开源REST API模糊测试工具，支持白盒、灰盒和黑盒测试，通过OpenAPI规范生成初始测试用例，使用REST特定变异器探索不同代码路径，并自动化测试框架创建。


<details>
  <summary>Details</summary>
Motivation: REST API广泛用于业务系统，暴露的端点存在安全风险。由于端点数量庞大，需要自动化测试技术（如模糊测试）来降低安全风险。

Method: 基于LibAFL构建REST API模糊测试工具，使用OpenAPI规范生成初始请求序列，结合REST特定变异器和LibAFL提供的变异器进行测试，通过覆盖率指导选择测试序列，自动化创建测试框架。

Result: 在Petstore API上评估了白盒方法的鲁棒性和不同功率调度策略的有效性，监测了端点和代码覆盖率随时间的变化以衡量方法效果。

Conclusion: WuppieFuzz是一个有效的REST API模糊测试工具，能够自动化测试过程，减少手动工作量，并提供多种报告帮助修复漏洞。

Abstract: Many business processes currently depend on web services, often using REST APIs for communication. REST APIs expose web service functionality through endpoints, allowing easy client interaction over the Internet. To reduce the security risk resulting from exposed endpoints, thorough testing is desired. Due to the generally vast number of endpoints, automated testing techniques, like fuzzing, are of interest.
  This paper introduces WuppieFuzz, an open-source REST API fuzzer built on LibAFL, supporting white-box, grey-box and black-box fuzzing. Using an OpenAPI specification, it can generate an initial input corpus consisting of sequences of requests. These are mutated with REST-specific and LibAFL-provided mutators to explore different code paths in the software under test. Guided by the measured coverage, WuppieFuzz then selects which request sequences to send next to reach complex states in the software under test. In this process, it automates harness creation to reduce manual efforts often required in fuzzing. Different kinds of reporting are provided by the fuzzer to help fixing bugs.
  We evaluated our tool on the Petstore API to assess the robustness of the white-box approach and the effectiveness of different power schedules. We further monitored endpoint and code coverage over time to measure the efficacy of the approach.

</details>


### [32] [A High-level Synthesis Toolchain for the Julia Language](https://arxiv.org/abs/2512.15679)
*Benedict Short,Ian McInerney,John Wickerson*

Main category: cs.SE

TL;DR: 提出基于MLIR的编译器工具链，可将Julia语言编写的计算内核自动编译为SystemVerilog，无需额外指令或语言定制，解决FPGA特定加速器开发的"双语言问题"。


<details>
  <summary>Details</summary>
Motivation: 随着Exascale计算和数据驱动方法的发展，问题规模急剧增加，计算需求增长，推动了向GPU/TPU等通用硬件加速器的卸载计算，以及对使用FPGA设计问题特定加速器的重新关注。然而，这些特定加速器的开发过程存在"双语言问题"：算法通常用高级语言开发，但内核需要在完全不同的抽象级别用另一种语言实现，需要完全不同的专业知识。

Method: 提出基于MLIR的编译器工具链，能够自动将Julia编程语言编写的内核编译为SystemVerilog，无需额外指令或语言定制。工具链支持动态和静态调度，直接集成AXI4-Stream协议与子系统接口，生成供应商无关的RTL。

Result: 原型工具链能够合成一组信号处理/数学基准测试，在真实FPGA设备上以100MHz运行，达到仅从C/C++等低级语言编译的最先进工具链生成设计吞吐量的59.71%到82.6%。

Conclusion: 该工具链允许领域专家像往常一样用Julia编写计算内核，然后无需额外编译指示或修改即可将其重定向到FPGA，解决了FPGA特定加速器开发中的双语言问题。

Abstract: With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the "two-language problem": algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.

</details>
