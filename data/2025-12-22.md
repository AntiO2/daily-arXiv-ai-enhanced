<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SpIDER: Spatially Informed Dense Embedding Retrieval for Software Issue Localization](https://arxiv.org/abs/2512.16956)
*Shravan Chaudhari,Rahul Thomas Jacob,Mononito Goswami,Jiajun Cao,Shihab Rashid,Christian Bock*

Main category: cs.SE

TL;DR: SpIDER是一种增强的密集检索方法，通过图探索和LLM推理来改进代码单元的语义检索，在多种编程语言上表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 现有代码检索方法（如BM25或密集嵌入）在探索代码库和利用其图结构方面存在不足，需要更有效的语义检索方法支持LLM编码代理

Method: 提出SpIDER方法，结合图探索获取辅助上下文，并使用LLM进行推理，增强密集嵌入检索的能力

Result: SpIDER在多种编程语言上持续改进密集检索性能，表现优于传统方法

Conclusion: 通过结合图探索和LLM推理，SpIDER能够更有效地检索语义相关的代码单元，为LLM编码代理提供更好的支持

Abstract: Retrieving code units (e.g., files, classes, functions) that are semantically relevant to a given user query, bug report, or feature request from large codebases is a fundamental challenge for LLM-based coding agents. Agentic approaches typically employ sparse retrieval methods like BM25 or dense embedding strategies to identify relevant units. While embedding-based approaches can outperform BM25 by large margins, they often lack exploration of the codebase and underutilize its underlying graph structure. To address this, we propose SpIDER (Spatially Informed Dense Embedding Retrieval), an enhanced dense retrieval approach that incorporates LLM-based reasoning over auxiliary context obtained through graph-based exploration of the codebase. Empirical results show that SpIDER consistently improves dense retrieval performance across several programming languages.

</details>


### [2] [Resilient Microservices: A Systematic Review of Recovery Patterns, Strategies, and Evaluation Frameworks](https://arxiv.org/abs/2512.16959)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 对2014-2025年间微服务恢复策略的PRISMA系统文献综述，识别9种弹性主题，提出恢复模式分类法、弹性评估评分清单和约束感知决策矩阵


<details>
  <summary>Details</summary>
Motivation: 微服务系统在现代分布式计算环境中至关重要，但仍易受部分故障、级联超时和不一致恢复行为的影响。现有关于弹性和恢复模式的研究调查多为描述性，缺乏系统证据综合或定量严谨性

Method: 采用PRISMA标准的系统文献综述方法，从IEEE Xplore、ACM Digital Library和Scopus数据库中筛选2014-2025年间的实证研究。从412条初始记录中，通过透明的纳入、排除和质量评估标准，最终选择26项高质量研究

Result: 识别出9个重复出现的弹性主题：断路器、带抖动和预算的重试、带补偿的Saga、幂等性、舱壁隔离、自适应背压、可观测性、混沌验证。提出了恢复模式分类法、弹性评估评分清单（用于标准化基准测试）和约束感知决策矩阵（映射延迟、一致性和成本权衡到适当的恢复机制）

Conclusion: 将碎片化的弹性研究整合为结构化、可分析的证据基础，支持可重复评估和知情设计容错且性能感知的微服务系统

Abstract: Microservice based systems underpin modern distributed computing environments but remain vulnerable to partial failures, cascading timeouts, and inconsistent recovery behavior. Although numerous resilience and recovery patterns have been proposed, existing surveys are largely descriptive and lack systematic evidence synthesis or quantitative rigor. This paper presents a PRISMA aligned systematic literature review of empirical studies on microservice recovery strategies published between 2014 and 2025 across IEEE Xplore, ACM Digital Library, and Scopus. From an initial corpus of 412 records, 26 high quality studies were selected using transparent inclusion, exclusion, and quality assessment criteria. The review identifies nine recurring resilience themes encompassing circuit breakers, retries with jitter and budgets, sagas with compensation, idempotency, bulkheads, adaptive backpressure, observability, and chaos validation. As a data oriented contribution, the paper introduces a Recovery Pattern Taxonomy, a Resilience Evaluation Score checklist for standardized benchmarking, and a constraint aware decision matrix mapping latency, consistency, and cost trade offs to appropriate recovery mechanisms. The results consolidate fragmented resilience research into a structured and analyzable evidence base that supports reproducible evaluation and informed design of fault tolerant and performance aware microservice systems.

</details>


### [3] [Sensor Management System (SMS): Open-source software for FAIR sensor metadata management in Earth system sciences](https://arxiv.org/abs/2512.17280)
*Christof Lorenza,Nils Brinckmann,Jan Bumberger,Marc Hanisch,Tobias Kuhnert,Ulrich Loup,Rubankumar Moorthy,Florian Obsersteiner,David Schäfer,Thomas Schnicke*

Main category: cs.SE

TL;DR: 开发传感器管理系统(SMS)用于管理复杂传感器系统的全生命周期元数据，提供一致的FAIR数据支持


<details>
  <summary>Details</summary>
Motivation: 环境观测数据需要一致全面的元数据支持，包括时间分辨的上下文信息（如部署变化、配置调整、维护操作），以确保可靠结论和洞察

Method: 开发用户友好的传感器管理系统(SMS)，通过明确定义的术语（设备、平台、配置、站点）建模复杂传感器系统，记录系统相关操作的连续历史，并链接到后续系统和服务

Result: SMS成为数字生态系统的核心元素，通过连接PID注册、受控词汇表等服务，建立终端用户社区，促进传感器相关元数据的一致、可持续和FAIR提供

Conclusion: SMS通过全面管理传感器生命周期信息，解决了环境观测数据元数据一致性问题，为可靠数据分析和洞察提供了关键基础设施支持

Abstract: Deriving reliable conclusions and insights from environmental observational data urgently requires the enrichment with consistent and comprehensive metadata, including time-resolved context such as changing deployments, configurations, and maintenance actions. We have therefore developed the Sensor Management System (SMS), which provides a user-friendly and feature-rich platform for modeling even the most complex sensor systems and managing all sensor-related information across their life cycle. Each entity is described via well-defined terms like Devices, Platforms and Configurations, as well as Sites that are further enhanced with attributes for, e.g., instrument manufacturers, contact information or measured quantities and complemented by a continuous history of system-related actions. By further linking the SMS to sub-sequent systems and services like PID-registration or controlled vocabularies and establishing a community of end-users, the SMS provides the central element of a digital ecosystem, that fosters a more consistent, sustainable and FAIR provision of sensor-related metadata.

</details>


### [4] [Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs](https://arxiv.org/abs/2512.17334)
*Zhi Ma,Cheng Wen,Zhexin Su,Xiao Liang,Cong Tian,Shengchao Qin,Mengfei Yang*

Main category: cs.SE

TL;DR: Req2LTL框架通过分层中间表示OnionL，结合LLM语义分解和确定性规则合成，将自然语言软件需求自动转换为线性时序逻辑规范，在航空航天需求上达到88.4%语义准确率和100%语法正确率。


<details>
  <summary>Details</summary>
Motivation: 将自然语言软件需求自动转换为形式化规范是扩展形式化验证到工业环境的关键挑战，特别是在安全关键领域。现有基于规则和学习的方法都有显著局限，LLM虽然擅长语义提取，但仍难以处理真实工业需求的复杂性、模糊性和逻辑深度。

Method: 提出Req2LTL模块化框架，通过分层中间表示OnionL桥接自然语言和线性时序逻辑。利用LLM进行语义分解，结合确定性规则合成，确保语法有效性和语义保真度。

Result: 在真实世界航空航天需求上的综合评估显示，Req2LTL达到88.4%语义准确率和100%语法正确率，显著优于现有方法。

Conclusion: Req2LTL框架成功解决了自然语言需求到形式化规范的转换问题，通过结合LLM语义能力和确定性规则合成，在工业环境中实现了高准确性和可靠性的自动化转换。

Abstract: Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.

</details>


### [5] [What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted Execution Environments in Practice](https://arxiv.org/abs/2512.17363)
*Yuqing Niu,Jieke Shi,Ruidong Han,Ye Liu,Chengyan Ma,Yunbo Lyu,David Lo*

Main category: cs.SE

TL;DR: 首次大规模实证研究真实世界可信执行环境(TEE)应用，分析241个开源项目，发现IoT安全是主要用例(30%)，32.4%项目重新实现加密功能，25.3%项目存在不安全编码行为。


<details>
  <summary>Details</summary>
Motivation: 可信执行环境(TEE)如Intel SGX和ARM TrustZone被广泛用于保护敏感数据和代码，但开发者实际如何使用TEE在实践中缺乏了解。本研究旨在填补这一空白，通过实证分析了解真实世界TEE应用的使用模式、开发实践和安全问题。

Method: 收集并分析GitHub上241个使用Intel SGX和ARM TrustZone的开源项目。结合人工检查和定制化静态分析脚本，从三个维度进行研究：1) 应用领域分类和时间趋势分析；2) TEE集成方式分析；3) 安全实践检查。

Result: 主要发现：1) IoT设备安全是主导用例(30%)，与学术界关注的区块链和密码系统(7%)形成鲜明对比，AI模型保护(12%)是新兴领域；2) 32.4%项目重新实现加密功能而非使用官方SDK API，表明当前SDK可用性和可移植性有限；3) 25.3%项目(61/241)存在不安全编码行为，如硬编码密钥和缺少输入验证，削弱了安全保证。

Conclusion: 研究揭示了真实世界TEE应用的实际使用模式和安全问题，为改进TEE SDK的可用性和支持开发者进行可信软件开发提供了重要启示。当前SDK需要更好的可用性和可移植性，开发者需要更多安全编码支持。

Abstract: Trusted Execution Environments (TEEs), such as Intel SGX and ARM TrustZone, provide isolated regions of CPU and memory for secure computation and are increasingly used to protect sensitive data and code across diverse application domains. However, little is known about how developers actually use TEEs in practice. This paper presents the first large-scale empirical study of real-world TEE applications. We collected and analyzed 241 open-source projects from GitHub that utilize the two most widely-adopted TEEs, Intel SGX and ARM TrustZone. By combining manual inspection with customized static analysis scripts, we examined their adoption contexts, usage patterns, and development practices across three phases. First, we categorized the projects into 8 application domains and identified trends in TEE adoption over time. We found that the dominant use case is IoT device security (30%), which contrasts sharply with prior academic focus on blockchain and cryptographic systems (7%), while AI model protection (12%) is rapidly emerging as a growing domain. Second, we analyzed how TEEs are integrated into software and observed that 32.4% of the projects reimplement cryptographic functionalities instead of using official SDK APIs, suggesting that current SDKs may have limited usability and portability to meet developers' practical needs. Third, we examined security practices through manual inspection and found that 25.3% (61 of 241) of the projects exhibit insecure coding behaviors when using TEEs, such as hardcoded secrets and missing input validation, which undermine their intended security guarantees. Our findings have important implications for improving the usability of TEE SDKs and supporting developers in trusted software development.

</details>


### [6] [GraphCue for SDN Configuration Code Synthesis](https://arxiv.org/abs/2512.17371)
*Haomin Qi,Fengfei Yu,Chengbo Huang*

Main category: cs.SE

TL;DR: GraphCue：基于拓扑感知检索和代理循环的SDN自动化配置框架，通过图神经网络嵌入和结构化提示实现88.2%的配置成功率


<details>
  <summary>Details</summary>
Motivation: SDN（软件定义网络）配置自动化面临挑战，需要处理复杂的网络拓扑和配置约束。传统方法难以有效结合拓扑结构和配置验证，导致配置错误和效率低下。

Method: 1. 将每个配置案例抽象为JSON图结构
2. 使用三层轻量级图卷积网络（GCN）进行图嵌入，采用对比学习训练
3. 基于拓扑相似性检索最接近的已验证参考配置
4. 将检索结果注入结构化提示中约束代码生成
5. 通过验证器执行候选配置，将失败反馈给代理形成闭环

Result: 1. 在628个验证案例上，GraphCue在20次迭代内达到88.2%的通过率
2. 95%的验证循环在9秒内完成
3. 消融实验显示：没有检索或结构化提示时性能显著下降，证明拓扑感知检索和约束条件调节是关键性能驱动因素

Conclusion: GraphCue通过结合拓扑感知检索、结构化提示约束和代理循环验证，有效解决了SDN自动化配置问题。该方法证明了图神经网络在配置检索中的有效性，以及结构化约束在代码生成中的重要性，为网络自动化提供了实用框架。

Abstract: We present GraphCue, a topology-grounded retrieval and agent-in-the-loop framework for automated SDN configuration. Each case is abstracted into a JSON graph and embedded using a lightweight three-layer GCN trained with contrastive learning. The nearest validated reference is injected into a structured prompt that constrains code generation, while a verifier closes the loop by executing the candidate configuration and feeding failures back to the agent. On 628 validation cases, GraphCue achieves an 88.2 percent pass rate within 20 iterations and completes 95 percent of verification loops within 9 seconds. Ablation studies without retrieval or structured prompting perform substantially worse, indicating that topology-aware retrieval and constraint-based conditioning are key drivers of performance.

</details>


### [7] [CIFE: Code Instruction-Following Evaluation](https://arxiv.org/abs/2512.17387)
*Sravani Gunnu,Shanmukha Guttula,Hima Patel*

Main category: cs.SE

TL;DR: 论文提出了一个包含1000个Python任务的基准测试，每个任务平均有7个开发者指定的约束条件，用于评估LLM在代码生成中不仅关注功能正确性，还要满足鲁棒性、格式、安全等约束的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在真实世界代码生成中，仅功能正确性不足以可靠部署，开发者还期望模型遵守鲁棒性、格式、安全等显式约束。现有基准主要通过测试用例执行评估正确性，对模型如何可靠遵循这些约束提供有限洞察。

Method: 通过四阶段人-LLM协作流程构建包含1000个Python任务的基准，每个任务平均有7个约束条件，涵盖13个类别。约束经过精心设计确保原子性、相关性和客观性。评估14个开源和闭源模型，使用互补的遵守度指标，并提出C2A分数来联合捕捉正确性和约束合规性。

Result: 结果显示部分遵守和严格遵守之间存在显著差距：强大模型的部分遵守率超过90%，但严格遵守率仅为39-66%。这表明仅功能正确性不足，需要一致遵守开发者意图。

Conclusion: 可信的代码生成不仅需要正确性，还需要对开发者意图的一致遵守。提出的基准和C2A分数为评估LLM在真实世界代码生成中的可靠性提供了更全面的框架。

Abstract: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

</details>


### [8] [SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories](https://arxiv.org/abs/2512.17419)
*Lilin Wang,Lucas Ramalho,Alan Celestino,Phuc Anthony Pham,Yu Liu,Umang Kumar Sinha,Andres Portillo,Onassis Osunwa,Gabriel Maduekwe*

Main category: cs.SE

TL;DR: SWE-Bench++是一个自动化的多语言代码仓库级任务基准框架，通过GitHub PR生成可执行的编程任务，覆盖11种语言，包含11,133个实例，显著扩展了现有基准的覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有基准如SWE-bench存在手动构建、静态数据集、仅关注Python错误修复等局限性，需要更自动化、多语言、覆盖更广任务类型的评估框架。

Method: 通过四阶段自动化流程：程序化采集、环境合成、测试预言提取、质量保证，从GitHub PR生成可执行任务。对于强模型失败的任务，通过提示引导的轨迹合成转换为训练数据。

Result: 构建了包含11,133个实例、3,971个仓库、11种语言的基准。在1,782个实例子集上，Claude Sonnet 4.5达到36.20% pass@10，GPT-5 34.57%，Gemini 2.5 Pro 24.92%，GPT-4o 16.89%。微调SWE-Bench++数据在SWE-bench多语言基准上带来可测量的改进。

Conclusion: SWE-Bench++提供了一个可扩展、多语言的基准，用于评估和改进仓库级代码生成能力，解决了现有基准的局限性。

Abstract: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

</details>


### [9] [An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys](https://arxiv.org/abs/2512.17455)
*Ronnie de Souza Santos,Italo Santos,Maria Teresa Baldassarre,Cleyton Magalhaes,Mairieli Wessel*

Main category: cs.SE

TL;DR: 该研究探讨了大型语言模型在软件工程调查中的滥用问题，分析了AI生成回答对数据真实性和研究有效性的威胁，并提出了检测和预防方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的发展，调查参与者可能使用生成工具伪造或操纵回答，这威胁到软件工程调查的数据真实性、有效性和研究完整性，需要系统研究这一问题。

Method: 通过Prolific平台收集2025年两次调查部署的数据，分析参与者回答内容以识别异常或伪造回答。对疑似AI生成的回答子集进行定性模式检查、叙事特征描述，并使用Scribbr AI检测器进行自动检测。

Result: 分析发现49份调查回答中存在重复序列、统一措辞和表面个性化等结构模式，表明是合成创作。这些虚假叙事模仿连贯推理但隐藏了伪造内容，破坏了构念、内部和外部有效性。

Conclusion: 研究将数据真实性确定为软件工程调查中有效性的新维度。强调可靠证据需要结合自动和解释性验证程序、透明报告和社区标准来检测和预防AI生成回答，从而保护软件工程调查的可信度。

Abstract: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

</details>


### [10] [When Data Quality Issues Collide: A Large-Scale Empirical Study of Co-Occurring Data Quality Issues in Software Defect Prediction](https://arxiv.org/abs/2512.17460)
*Emmanuel Charleson Dapaah,Jens Grabowski*

Main category: cs.SE

TL;DR: 首次大规模实证分析软件缺陷预测中五种数据质量问题（类别不平衡、类别重叠、无关特征、属性噪声、离群值）的共现效应，发现共现普遍存在，识别了各问题的性能临界点，揭示了模型性能与鲁棒性的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有SDP研究通常孤立地分析单一数据质量问题，但现实世界中这些问题经常共现并相互作用。本研究旨在填补这一研究空白，全面理解多种数据质量问题如何共同影响模型性能。

Method: 在374个数据集和5个分类器上进行大规模实证分析，使用可解释提升机（Explainable Boosting Machines）和分层交互分析，在默认超参数设置下量化直接效应和条件效应。

Result: 数据质量问题共现普遍存在（最不常见的属性噪声问题也在93%的数据集中与其他问题共现）。类别重叠是最具一致性的有害问题，识别了各问题的性能临界点：类别重叠0.20、类别不平衡0.65-0.70、无关特征0.94。发现了反直觉模式（如低无关特征时离群值能提升性能），并揭示了性能与鲁棒性的权衡。

Conclusion: 本研究通过联合分析数据质量问题的普遍性、共现性、临界点和条件效应，填补了SDP研究的空白，强调需要超越孤立分析，提供对现实环境中数据质量问题如何影响模型性能的全面、数据感知的理解。

Abstract: Software Defect Prediction (SDP) models are central to proactive software quality assurance, yet their effectiveness is often constrained by the quality of available datasets. Prior research has typically examined single issues such as class imbalance or feature irrelevance in isolation, overlooking that real-world data problems frequently co-occur and interact. This study presents, to our knowledge, the first large-scale empirical analysis in SDP that simultaneously examines five co-occurring data quality issues (class imbalance, class overlap, irrelevant features, attribute noise, and outliers) across 374 datasets and five classifiers. We employ Explainable Boosting Machines together with stratified interaction analysis to quantify both direct and conditional effects under default hyperparameter settings, reflecting practical baseline usage.
  Our results show that co-occurrence is nearly universal: even the least frequent issue (attribute noise) appears alongside others in more than 93% of datasets. Irrelevant features and imbalance are nearly ubiquitous, while class overlap is the most consistently harmful issue. We identify stable tipping points around 0.20 for class overlap, 0.65-0.70 for imbalance, and 0.94 for irrelevance, beyond which most models begin to degrade. We also uncover counterintuitive patterns, such as outliers improving performance when irrelevant features are low, underscoring the importance of context-aware evaluation. Finally, we expose a performance-robustness trade-off: no single learner dominates under all conditions.
  By jointly analyzing prevalence, co-occurrence, thresholds, and conditional effects, our study directly addresses a persistent gap in SDP research. Hence, moving beyond isolated analyses to provide a holistic, data-aware understanding of how quality issues shape model performance in real-world settings.

</details>


### [11] [Why Is My Transaction Risky? Understanding Smart Contract Semantics and Interactions in the NFT Ecosystem](https://arxiv.org/abs/2512.17500)
*Yujing Chen,Xuanming Liu,Zhiyuan Wan,Zuobin Wang,David Lo,Difan Xie,Xiaohu Yang*

Main category: cs.SE

TL;DR: 该研究对NFT生态系统中的智能合约语义和交互进行了大规模实证分析，发现智能合约语义多样性有限，诈骗代币存在字节码趋同现象，并识别出与风险交易相关的特定交互模式。


<details>
  <summary>Details</summary>
Motivation: NFT生态系统存在诈骗代币风险，但先前研究缺乏对智能合约语义和交互在交易过程中如何体现这些风险的理解。需要填补这一空白，以更好地理解诈骗代币风险如何通过合约语义和交互表现出来。

Method: 使用以太坊上近1亿笔交易、跨越2000万个区块的精选数据集，对NFT生态系统中的智能合约语义和交互进行大规模实证研究。分析合约类型、交互模式、字节码多样性等特征。

Result: 发现NFT生态系统中智能合约语义多样性有限，主要由代理合约、代币合约和DeFi合约主导。市场合约和代理注册表合约在交易中最常参与交互。代币合约在字节码层面具有多样性，而诈骗代币则表现出字节码趋同。某些智能合约交互模式在风险和低风险交易中都常见，而其他模式则主要与风险交易相关。

Conclusion: 基于研究发现，提出了缓解区块链生态系统风险的建议，并规划了未来研究方向。研究为理解NFT生态系统中智能合约语义和交互如何影响诈骗代币风险提供了重要见解。

Abstract: The NFT ecosystem represents an interconnected, decentralized environment that encompasses the creation, distribution, and trading of Non-Fungible Tokens (NFTs), where key actors, such as marketplaces, sellers, and buyers, utilize smart contracts to facilitate secure, transparent, and trustless transactions. Scam tokens are deliberately created to mislead users and facilitate financial exploitation, posing significant risks in the NFT ecosystem. Prior work has explored the NFT ecosystem from various perspectives, including security challenges, actor behaviors, and risks from scams and wash trading, leaving a gap in understanding the semantics and interactions of smart contracts during transactions, and how the risks associated with scam tokens manifest in relation to the semantics and interactions of contracts. To bridge this gap, we conducted a large-scale empirical study on smart contract semantics and interactions in the NFT ecosystem, using a curated dataset of nearly 100 million transactions across 20 million blocks on Ethereum. We observe a limited semantic diversity among smart contracts in the NFT ecosystem, dominated by proxy, token, and DeFi contracts. Marketplace and proxy registry contracts are the most frequently involved in smart contract interactions during transactions, engaging with a broad spectrum of contracts in the ecosystem. Token contracts exhibit bytecode-level diversity, whereas scam tokens exhibit bytecode convergence. Certain interaction patterns between smart contracts are common to both risky and non-risky transactions, while others are predominantly associated with risky transactions. Based on our findings, we provide recommendations to mitigate risks in the blockchain ecosystem, and outline future research directions.

</details>


### [12] [SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review](https://arxiv.org/abs/2512.17540)
*Kai Wang,Bingcheng Mao,Shuai Jia,Yujie Ding,Dongming Han,Tianyi Ma,Bin Cao*

Main category: cs.SE

TL;DR: SGCR框架通过将LLM基于人工编写的规范进行代码审查，采用显式和隐式双路径架构，在工业环境中实现了42%的开发者采纳率，相比基线LLM提升90.9%。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动化代码审查存在可靠性不足、缺乏上下文感知和控制能力的问题，阻碍了实际应用。需要一种方法将LLM的生成能力与软件工程对可靠性的严格要求相结合。

Method: 提出Specification-Grounded Code Review (SGCR)框架，采用双路径架构：显式路径确保确定性遵守从规范派生的预定义规则；隐式路径启发式地发现和验证超出这些规则的问题。

Result: 在HiThink Research的工业环境中部署SGCR，其建议获得了42%的开发者采纳率，相比基线LLM的22%实现了90.9%的相对改进。

Conclusion: 规范基础化是连接LLM生成能力与软件工程严格可靠性需求的有力范式，能够产生可信赖且相关的代码审查反馈。

Abstract: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

</details>


### [13] [A Practical Solution to Systematically Monitor Inconsistencies in SBOM-based Vulnerability Scanners](https://arxiv.org/abs/2512.17710)
*Martin Rosso,Muhammad Asad Jahangir Jaffar,Alessandro Brighente,Mauro Conti*

Main category: cs.SE

TL;DR: 本文介绍了SVS-TEST，一种用于评估软件物料清单（SBOM）漏洞扫描工具能力、成熟度和故障条件的测试方法，揭示了现有工具在可靠性方面的显著差异和静默失败问题。


<details>
  <summary>Details</summary>
Motivation: SBOM为软件产品漏洞识别提供了新机会，但行业采用SBOM漏洞扫描（SVS）时，观察到不一致和意外行为导致假阴性和静默失败，需要系统评估工具可靠性。

Method: 提出SVS-TEST方法和工具，通过16个精心设计的SBOM及其真实基准，在真实场景中评估7个实际SVS工具的能力、成熟度和故障条件。

Result: 发现SVS工具在可靠性和错误处理方面存在显著差异，多个工具在有效输入SBOM上静默失败，造成虚假的安全感。

Conclusion: SVS-TEST可帮助组织和工具开发者监控SVS能力与成熟度，所有研究结果已公开并提前披露给工具开发者。

Abstract: Software Bill of Materials (SBOM) provides new opportunities for automated vulnerability identification in software products. While the industry is adopting SBOM-based Vulnerability Scanning (SVS) to identify vulnerabilities, we increasingly observe inconsistencies and unexpected behavior, that result in false negatives and silent failures. In this work, we present the background necessary to understand the underlying complexity of SVS and introduce SVS-TEST, a method and tool to analyze the capability, maturity, and failure conditions of SVS-tools in real-world scenarios. We showcase the utility of SVS-TEST in a case study evaluating seven real-world SVS-tools using 16 precisely crafted SBOMs and their respective ground truth. Our results unveil significant differences in the reliability and error handling of SVS-tools; multiple SVS-tools silently fail on valid input SBOMs, creating a false sense of security. We conclude our work by highlighting implications for researchers and practitioners, including how organizations and developers of SVS-tools can utilize SVS-TEST to monitor SVS capability and maturity. All results and research artifacts are made publicly available and all findings were disclosed to the SVS-tool developers ahead of time.

</details>


### [14] [LLM-based Behaviour Driven Development for Hardware Design](https://arxiv.org/abs/2512.17814)
*Rolf Drechsler,Qian Liu*

Main category: cs.SE

TL;DR: 本文探讨了在硬件设计中利用大语言模型（LLM）支持行为驱动开发（BDD），以自动化从文本规范中提取精确行为场景的过程。


<details>
  <summary>Details</summary>
Motivation: 随着系统规模增大，硬件和系统设计的测试验证复杂性显著增加。虽然BDD在软件工程中已被证明有效，但在硬件设计中尚未广泛应用，其中一个主要原因是需要从文本规范中手动提取精确行为场景的繁重工作。

Method: 研究使用基于大语言模型（LLM）的技术来支持硬件设计中的行为驱动开发（BDD），利用LLM的先进能力自动化从文本规范中提取行为场景的过程。

Result: 未在摘要中明确说明具体结果，但表明LLM为自动化硬件设计中的BDD提供了新的机会。

Conclusion: 大语言模型的发展为自动化硬件设计中行为驱动开发的场景提取提供了有前景的解决方案，有望减少手动工作量并提高效率。

Abstract: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.
  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor](https://arxiv.org/abs/2512.16926)
*Oren Bell,Harun Teper,Mario Günzel,Chris Gill,Jian-Jia Chen*

Main category: cs.DC

TL;DR: 提出使用events executor实现固定作业级优先级调度器，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型，缩小实时系统理论与ROS2调度分析之间的差距


<details>
  <summary>Details</summary>
Motivation: 当前ROS2调度方法主要关注简单链式任务，存在局限性，需要支持任意有向无环图的分析和调度

Method: 使用events executor实现固定作业级优先级调度器，将ROS2应用抽象为树森林并映射到传统实时DAG任务模型，需要特殊的事件队列实现和支持LIFO顺序消息传递的通信中间件

Result: 实现能够生成与传统固定优先级DAG任务调度器相同的调度结果，尽管缺乏通常所需的优先关系信息

Conclusion: 该方法进一步缩小了成熟实时系统理论与ROS2调度分析之间的差距，为ROS2中任意DAG的调度提供了新途径

Abstract: This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.

</details>


### [16] [LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation](https://arxiv.org/abs/2512.17023)
*Patrick Diehl,Noujoud Nader,Deepti Gupta*

Main category: cs.DC

TL;DR: 系统评估大型语言模型在生成高性能计算代码方面的能力，重点关注ChatGPT-4/5、Claude和LLaMA在实现Mandelbrot集并行计算时的表现。


<details>
  <summary>Details</summary>
Motivation: 并行编程是高性能计算中最具挑战性的领域之一，需要深厚的同步、通信和内存模型知识。尽管现代C++标准和OpenMP、MPI等框架简化了并行化，但掌握这些范式仍然复杂。大型语言模型在代码生成方面显示出潜力，但它们在生成正确高效的高性能计算代码方面的效果尚不明确。

Method: 系统评估领先的大型语言模型（包括ChatGPT-4和5、Claude、LLaMA）在生成Mandelbrot集的C++实现任务上的表现。测试涵盖共享内存、基于指令和分布式内存三种并行范式。每个生成的程序都使用GCC 11.5.0编译和执行，以评估其正确性、鲁棒性和可扩展性。

Result: 结果显示ChatGPT-4和ChatGPT-5在语法精度和可扩展性能方面表现强劲，能够生成正确且高效的高性能计算代码。

Conclusion: 大型语言模型特别是ChatGPT-4和5在生成高性能计算并行代码方面具有显著潜力，能够有效简化复杂的并行编程任务，为高性能计算开发提供新的自动化工具。

Abstract: Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.

</details>


### [17] [Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving](https://arxiv.org/abs/2512.17077)
*Jiakun Fan,Yanglin Zhang,Xiangchen Li,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: dLLM-Serve是一个针对扩散大语言模型的高效服务系统，通过内存优化、计算调度和生成质量协同优化，解决了扩散模型特有的内存危机和资源振荡问题，在多种硬件上实现了显著的吞吐量提升和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型研究主要关注内核级优化，缺乏针对生产环境中扩散过程独特内存动态的整体服务框架。作者识别出dLLM特有的"内存占用危机"，由单一logit张量和计算密集型"刷新"阶段与带宽密集型"重用"阶段之间的严重资源振荡引起。

Method: dLLM-Serve采用三种核心技术：1) Logit-Aware Activation Budgeting分解瞬态张量峰值；2) Phase-Multiplexed Scheduler交错处理异构请求阶段；3) Head-Centric Sparse Attention将逻辑稀疏性与物理存储解耦。

Result: 在多样化工作负载(LiveBench、Burst、OSC)和GPU(RTX 4090、L40S)上评估，相比最先进基线，dLLM-Serve在消费级RTX 4090上提升吞吐量1.61-1.81倍，在服务器级L40S上提升1.60-1.74倍，在重负载下将尾部延迟降低近4倍。

Conclusion: dLLM-Serve建立了首个可扩展dLLM推理的蓝图，将理论算法稀疏性转化为跨异构硬件的实际时钟加速，为扩散大语言模型的高效服务提供了系统级解决方案。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical "memory footprint crisis" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound "Refresh" phases and bandwidth-bound "Reuse" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\times$-1.81$\times$ on the consumer-grade RTX 4090 and 1.60$\times$-1.74$\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.

</details>


### [18] [Scalable Distributed Vector Search via Accuracy Preserving Index Construction](https://arxiv.org/abs/2512.17264)
*Yuming Xu,Qianxi Zhang,Qi Chen,Baotong Lu,Menghao Li,Philip Adams,Mingqin Li,Zengzhong Li,Jing Liu,Cheng Li,Fan Yang*

Main category: cs.DC

TL;DR: SPIRE是一个可扩展的向量索引系统，通过平衡分区粒度和递归构建多级索引，在数十亿向量规模下实现了高精度、低延迟和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有分布式索引设计在扩展到数十亿向量时难以平衡准确性、延迟和吞吐量之间的权衡，需要新的索引设计来解决这一挑战。

Method: 1) 识别平衡的分区粒度以避免读取成本爆炸；2) 引入保持精度的递归构建方法，构建具有可预测搜索成本和稳定准确性的多级索引。

Result: 在46个节点上对最多80亿向量的实验中，SPIRE实现了高可扩展性，吞吐量比最先进系统提高了最高9.64倍。

Conclusion: SPIRE通过创新的分区粒度和递归构建方法，成功解决了大规模向量索引中准确性、延迟和吞吐量的平衡问题，为数十亿级向量搜索提供了高效解决方案。

Abstract: Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.

</details>


### [19] [The HEAL Data Platform](https://arxiv.org/abs/2512.17506)
*Brienna M. Larrick,L. Philip Schumm,Mingfei Shao,Craig Barnes,Anthony Juehne,Hara Prasad Juvvla,Michael B. Kranz,Michael Lukowski,Clint Malson,Jessica N. Mazerik,Christopher G. Meyer,Jawad Qureshi,Erin Spaniol,Andrea Tentner,Alexander VanTol,Peter Vassilatos,Sara Volk de Garcia,Robert L. Grossman*

Main category: cs.DC

TL;DR: 开发基于云的联邦系统，作为NIH HEAL计划数据的一站式搜索、发现和分析平台


<details>
  <summary>Details</summary>
Motivation: HEAL计划产生多种数据类型，分散在多个NIH和第三方数据存储库中，需要一个统一的发现平台来促进数据二次利用

Method: 基于开源Gen3平台构建，使用框架服务（认证授权、持久标识符、元数据管理）和API，与19个数据存储库互操作

Result: 平台覆盖1000多项HEAL研究，每月数百用户，提供丰富元数据和云计算环境，促进数据发现和二次分析

Conclusion: HEAL数据平台实现了连接存储库中数据的搜索、发现和分析，通过确保FAIR原则最大化HEAL数据的价值

Abstract: Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.
  Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.
  Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.
  Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.
  Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.

</details>


### [20] [Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing](https://arxiv.org/abs/2512.17574)
*Lingxiao Zhao,Haoran Zhou,Yuezhi Che,Dazhao Cheng*

Main category: cs.DC

TL;DR: FlashCodec和UnifiedServe联合优化多模态大语言模型推理系统，通过GPU协同解码消除预处理瓶颈，并通过逻辑解耦物理共享资源提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的三阶段流水线（多模态预处理、视觉编码、LLM推理）存在显著系统瓶颈：CPU解码主导首token延迟，视觉编码与LLM推理异构导致资源利用率低。

Method: 提出FlashCodec（多GPU协同视频解码加速预处理）和UnifiedServe（逻辑解耦但物理共享GPU资源的视觉编码与推理优化框架）两个互补设计。

Result: 相比最先进系统，可服务3.0倍更多请求或满足1.5倍更严格的SLO要求，同时实现高达4.4倍的吞吐量提升。

Conclusion: 通过端到端优化多模态大语言模型推理流水线，显著提升系统性能、资源利用率和响应延迟，为高效多模态服务提供解决方案。

Abstract: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.
  To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [21] [Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows](https://arxiv.org/abs/2512.17429)
*Kyriakos Psarakis*

Main category: cs.DB

TL;DR: 该论文提出通过数据流模型简化云应用开发，引入Stateflow编程模型和Styx分布式引擎，实现高性能、可序列化的事务处理，并支持弹性伸缩。


<details>
  <summary>Details</summary>
Motivation: 当前构建可扩展、一致的云应用需要跨云计算、分布式系统、数据库和软件工程的专业知识，限制了开发人员范围。论文旨在通过解决可编程性、高性能容错事务和无服务器语义三大挑战，实现云应用开发的民主化。

Method: 1. 首先探索云应用与流式数据流执行模型的相似性，通过T-Statefun（Apache Flink Statefun的事务扩展）验证可行性；2. 引入Stateflow高级面向对象编程模型，将应用编译为状态化数据流图；3. 开发Styx分布式流式数据流引擎，提供确定性、多分区、可序列化事务；4. 扩展Styx支持事务化状态迁移以实现弹性伸缩。

Result: T-Statefun验证了数据流系统可通过状态化函数即服务API支持事务性云应用，但存在可编程性和性能限制。Stateflow编程模型显著减少了样板代码。Styx引擎消除了显式事务失败处理，性能显著优于现有系统，并支持弹性工作负载下的状态迁移。

Conclusion: 通过将云应用映射到流式数据流模型，结合Stateflow编程抽象和Styx事务引擎，成功解决了云应用开发的三大挑战，实现了高性能、容错的事务处理，并支持动态工作负载下的弹性伸缩，为更广泛的开发者群体提供了构建云应用的可行方案。

Abstract: Web applications underpin much of modern digital life, yet building scalable and consistent cloud applications remains difficult, requiring expertise across cloud computing, distributed systems, databases, and software engineering. These demands restrict development to a small number of highly specialized engineers. This thesis aims to democratize cloud application development by addressing three challenges: programmability, high-performance fault-tolerant serializable transactions, and serverless semantics.
  The thesis identifies strong parallels between cloud applications and the streaming dataflow execution model. It first explores this connection through T-Statefun, a transactional extension of Apache Flink Statefun, demonstrating that dataflow systems can support transactional cloud applications via a stateful functions-as-a-service API. However, this approach revealed significant limitations in programmability and performance.
  To overcome these issues, the thesis introduces Stateflow, a high-level object-oriented programming model that compiles applications into stateful dataflow graphs with minimal boilerplate. Building on this model, the thesis presents Styx, a distributed streaming dataflow engine that provides deterministic, multi-partition, serializable transactions with strong fault tolerance guarantees. Styx eliminates explicit transaction failure handling and significantly outperforms state-of-the-art systems.
  Finally, the thesis extends Styx with transactional state migration to support elasticity under dynamic workloads.

</details>
