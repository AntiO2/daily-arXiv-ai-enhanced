<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Securing LLM-as-a-Service for Small Businesses: An Industry Case Study of a Distributed Chatbot Deployment Platform](https://arxiv.org/abs/2601.15528)
*Jiazhu Xie,Bowen Li,Heyu Fu,Chong Gao,Ziqi Xu,Fengling Han*

Main category: cs.DC

TL;DR: 开源多租户平台让小企业通过无代码工作流部署定制化LLM客服机器人，基于分布式k3s集群实现成本效益，并集成针对RAG场景的提示注入防御机制。


<details>
  <summary>Details</summary>
Motivation: LLM问答系统在小企业中具有自动化客服和知识访问的潜力，但实际部署面临基础设施成本高、工程复杂度大、安全风险（特别是RAG场景）等挑战。

Method: 构建基于分布式轻量级k3s集群的开源多租户平台，通过加密覆盖网络连接异构低成本机器，实现容器隔离和租户数据访问控制，并集成平台级提示注入防御机制。

Result: 通过真实电商部署验证，平台能在小企业面临的成本、运营和安全约束下，实现安全高效的LLM客服机器人服务。

Conclusion: 该平台证明小企业可以在现实约束条件下部署安全、高效的定制化LLM客服系统，无需模型重训练或企业级基础设施。

Abstract: Large Language Model (LLM)-based question-answering systems offer significant potential for automating customer support and internal knowledge access in small businesses, yet their practical deployment remains challenging due to infrastructure costs, engineering complexity, and security risks, particularly in retrieval-augmented generation (RAG)-based settings. This paper presents an industry case study of an open-source, multi-tenant platform that enables small businesses to deploy customised LLM-based support chatbots via a no-code workflow. The platform is built on distributed, lightweight k3s clusters spanning heterogeneous, low-cost machines and interconnected through an encrypted overlay network, enabling cost-efficient resource pooling while enforcing container-based isolation and per-tenant data access controls. In addition, the platform integrates practical, platform-level defences against prompt injection attacks in RAG-based chatbots, translating insights from recent prompt injection research into deployable security mechanisms without requiring model retraining or enterprise-scale infrastructure. We evaluate the proposed platform through a real-world e-commerce deployment, demonstrating that secure and efficient LLM-based chatbot services can be achieved under realistic cost, operational, and security constraints faced by small businesses.

</details>


### [2] [Advancing RT Core-Accelerated Fixed-Radius Nearest Neighbor Search](https://arxiv.org/abs/2601.15633)
*Enzo Meneses,Hugo Bec,Cristóbal A. Navarroa,Benoît Crespin,Felipe A. Quezada,Nancy Hitschfeld,Heinich Porro,Maxime Maria*

Main category: cs.DC

TL;DR: 提出三种改进RT Core上粒子FRNN物理模拟的方法：BVH更新/重建优化器、无需邻居列表的RT Core新用法、支持周期性边界条件的RT Core技术，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 现有的RT Core粒子模拟方法存在性能瓶颈，特别是在处理动态变化的粒子系统、内存消耗大的邻居列表以及周期性边界条件时效率不高，需要更优化的方法。

Method: 1) 实时BVH更新/重建比例优化器，根据模拟动态自适应调整；2) 两种无需邻居列表的RT Core新用法；3) 支持周期性边界条件的RT Core技术。

Result: BVH优化器使RT Core流水线比其他方法快约3.4倍；新方法在模拟步长性能上提升1.3-2.0倍；支持周期性边界条件无显著性能损失；方法在不同GPU代际上均能良好扩展。

Conclusion: 提出的三种方法显著提升了RT Core上FRNN物理模拟的性能和能效，同时明确了RT Core与传统GPU计算的适用场景，为硬件加速粒子模拟提供了实用指导。

Abstract: In this work we introduce three ideas that can further improve particle FRNN physics simulations running on RT Cores; i) a real-time update/rebuild ratio optimizer for the bounding volume hierarchy (BVH) structure, ii) a new RT core use, with two variants, that eliminates the need of a neighbor list and iii) a technique that enables RT cores for FRNN with periodic boundary conditions (BC). Experimental evaluation using the Lennard-Jones FRNN interaction model as a case study shows that the proposed update/rebuild ratio optimizer is capable of adapting to the different dynamics that emerge during a simulation, leading to a RT core pipeline up to $\sim 3.4\times$ faster than with other known approaches to manage the BVH. In terms of simulation step performance, the proposed variants can significantly improve the speedup and EE of the base RT core idea; from $\sim1.3\times$ at small radius to $\sim2.0\times$ for log normal radius distributions. Furthermore, the proposed variants manage to simulate cases that would otherwise not fit in memory because of the use of neighbor lists, such as clusters of particles with log normal radius distribution. The proposed RT Core technique to support periodic BC is indeed effective as it does not introduce any significant penalty in performance. In terms of scaling, the proposed methods scale both their performance and EE across GPU generations. Throughout the experimental evaluation, we also identify the simulation cases were regular GPU computation should still be preferred, contributing to the understanding of the strengths and limitations of RT cores.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [ToolCaching: Towards Efficient Caching for LLM Tool-calling](https://arxiv.org/abs/2601.15335)
*Yi Zhai,Dian Shen,Junzhou Luo,Bin Yang*

Main category: cs.SE

TL;DR: ToolCaching：一个面向LLM工具调用的高效缓存框架，通过语义和系统特征评估请求可缓存性，使用VAAC算法实现自适应缓存管理，显著提升缓存命中率和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具调用系统存在冗余或重复请求问题，传统缓存策略因请求语义异构、动态工作负载和不同新鲜度要求而失效，需要专门针对LLM工具调用的缓存解决方案。

Method: 提出ToolCaching框架，集成语义和系统级特征评估请求可缓存性和缓存价值；核心VAAC算法结合基于bandit的准入机制和基于价值的多因素淘汰策略，综合考虑请求频率、最近性和缓存价值。

Result: 在合成和公开工具调用工作负载上的实验表明，ToolCaching与VAAC相比标准策略实现高达11%的缓存命中率提升和34%的延迟降低。

Conclusion: ToolCaching能有效加速LLM工具调用在实际应用中的性能，解决了传统缓存策略在LLM工具调用场景下的局限性。

Abstract: Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.

</details>


### [4] [Lost in Transcription: How Speech-to-Text Errors Derail Code Understanding](https://arxiv.org/abs/2601.15339)
*Jayant Havare,Ashish Mittal,Srikanth Tamilselvam,Ganesh Ramakrishnan*

Main category: cs.SE

TL;DR: 开发了一个支持多语言语音驱动的代码理解框架，通过ASR转录和LLM优化处理非英语用户的语音查询，显著提升了代码理解和检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码理解系统主要面向英语键盘用户，限制了多语言和语音优先场景下的可访问性，特别是在印度等地区。语音界面虽然更具包容性，但涉及代码的语音查询面临非标准英语用法、领域特定词汇和自定义标识符等独特挑战。

Method: 开发了一个多语言语音驱动框架：1) 接受用户母语的语音查询；2) 使用自动语音识别(ASR)进行转录；3) 利用大型语言模型(LLM)进行代码感知的ASR输出优化；4) 与代码模型接口执行代码问答和检索任务。在四种印度语言和英语上进行评估。

Result: 系统分析了转录错误对下游任务性能的影响，识别了ASR在代码处理中的关键失败模式，并证明LLM引导的优化显著提升了转录和代码理解阶段的性能。在CodeSearchNet、CoRNStack和CodeQA等基准测试中表现出色。

Conclusion: 研究强调了语音界面需要代码敏感的适应性调整，并为构建稳健的多语言语音驱动编程工具提供了实用解决方案，促进了编程工具的可访问性和包容性。

Abstract: Code understanding is a foundational capability in software engineering tools and developer workflows. However, most existing systems are designed for English-speaking users interacting via keyboards, which limits accessibility in multilingual and voice-first settings, particularly in regions like India. Voice-based interfaces offer a more inclusive modality, but spoken queries involving code present unique challenges due to the presence of non-standard English usage, domain-specific vocabulary, and custom identifiers such as variable and function names, often combined with code-mixed expressions. In this work, we develop a multilingual speech-driven framework for code understanding that accepts spoken queries in a user native language, transcribes them using Automatic Speech Recognition (ASR), applies code-aware ASR output refinement using Large Language Models (LLMs), and interfaces with code models to perform tasks such as code question answering and code retrieval through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on four widely spoken Indic languages and English, we systematically characterize how transcription errors impact downstream task performance. We also identified key failure modes in ASR for code and demonstrated that LLM-guided refinement significantly improves performance across both transcription and code understanding stages. Our findings underscore the need for code-sensitive adaptations in speech interfaces and offer a practical solution for building robust, multilingual voice-driven programming tools.

</details>


### [5] [A Prompt-Based Framework for Loop Vulnerability Detection Using Local LLMs](https://arxiv.org/abs/2601.15352)
*Adeyemi Adeseye,Aisvarya Adeseye*

Main category: cs.SE

TL;DR: 提出基于本地大语言模型的提示框架，用于检测Python代码中的循环漏洞，包括控制逻辑错误、安全风险和资源管理问题。


<details>
  <summary>Details</summary>
Motivation: 循环漏洞是软件开发中的高风险构造，传统静态分析工具依赖语法模式难以检测语义缺陷，而本地LLM能解决隐私、延迟和依赖问题，提供上下文理解能力。

Method: 设计结构化提示框架，包含语言特定意识、代码感知基础、版本敏感性和幻觉预防等保护特征，使用本地部署的LLaMA 3.2和Phi 3.5模型进行迭代提示测试。

Result: Phi模型在精确率、召回率和F1分数上优于LLaMA模型，验证了本地LLM在循环漏洞检测中的有效性。

Conclusion: 设计有效的提示对于本地LLM执行安全准确的代码漏洞分析至关重要，本地LLM为解决传统静态分析工具的局限性提供了新途径。

Abstract: Loop vulnerabilities are one major risky construct in software development. They can easily lead to infinite loops or executions, exhaust resources, or introduce logical errors that degrade performance and compromise security. The problem are often undetected by traditional static analyzers because such tools rely on syntactic patterns, which makes them struggle to detect semantic flaws. Consequently, Large Language Models (LLMs) offer new potential for vulnerability detection because of their ability to understand code contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or Gemini addresses issues such as privacy, latency, and dependency concerns by facilitating efficient offline analysis. Consequently, this study proposes a prompt-based framework that utilize local LLMs for the detection of loop vulnerabilities within Python 3.7+ code. The framework targets three categories of loop-related issues, such as control and logic errors, security risks inside loops, and resource management inefficiencies. A generalized and structured prompt-based framework was designed and tested with two locally deployed LLMs (LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative prompting. The designed prompt-based framework included key safeguarding features such as language-specific awareness, code-aware grounding, version sensitivity, and hallucination prevention. The LLM results were validated against a manually established baseline truth, and the results indicate that Phi outperforms LLaMA in precision, recall, and F1-score. The findings emphasize the importance of designing effective prompts for local LLMs to perform secure and accurate code vulnerability analysis.

</details>


### [6] [Testing Deep Learning Libraries via Neurosymbolic Constraint Learning](https://arxiv.org/abs/2601.15493)
*M M Abid Naziri,Shinhae Kim,Feiran Qin,Marcelo d'Amorim,Saikat Dutta*

Main category: cs.SE

TL;DR: Centaur：首个使用动态学习输入约束的神经符号技术，用于测试深度学习库API，通过LLM生成候选规则并验证，结合SMT求解器生成有效多样的测试输入，显著提升分支覆盖并发现新bug。


<details>
  <summary>Details</summary>
Motivation: 深度学习库（如PyTorch）复杂且包含bug，现有测试方法因缺乏API规范而难以准确建模输入约束，导致错过有效输入或产生误报。

Method: 开发神经符号技术Centaur：1）设计表示API参数一阶逻辑公式的语法，表达张量属性（形状、数据类型）和参数间关系；2）使用LLM生成候选规则并用种子输入验证；3）定制细化策略消除虚假/冗余规则；4）结合SMT求解和随机采样生成有效多样的测试输入。

Result: 约束学习的平均召回率和精确率均为94.0%；相比TitanFuzz、ACETest和Pathfinder，分别多覆盖203、150和9,608个分支；在PyTorch和TensorFlow中发现26个新bug，其中18个已确认。

Conclusion: Centaur通过动态学习API输入约束，有效解决了深度学习库测试中缺乏规范的问题，显著提升了测试覆盖率和bug发现能力，证明了神经符号方法的有效性。

Abstract: Deep Learning (DL) libraries (e.g., PyTorch) are popular in AI development. These libraries are complex and contain bugs. Researchers have proposed various bug-finding techniques for such libraries. Yet, there is much room for improvement. A key challenge in testing DL libraries is the lack of API specifications. Prior testing approaches often inaccurately model the input specifications of DL APIs, resulting in missed valid inputs that could reveal bugs or false alarms due to invalid inputs.
  To address this challenge, we develop Centaur -- the first neurosymbolic technique to test DL library APIs using dynamically learned input constraints. Centaur leverages the key idea that formal API constraints can be learned from a small number of automatically generated seed inputs, and that the learned constraints can be solved using SMT solvers to generate valid and diverse test inputs.
  We develop a novel grammar that represents first-order logic formulae over API parameters and expresses tensor-related properties (e.g., shape, data types) as well as relational properties between parameters. We use the grammar to guide a Large Language Model (LLM) to enumerate syntactically correct candidate rules, validated using seed inputs. Further, we develop a custom refinement strategy to prune the set of learned rules to eliminate spurious or redundant rules. We use the learned constraints to systematically generate valid and diverse inputs by integrating SMT-based solving with randomized sampling.
  We evaluate Centaur for testing PyTorch and TensorFlow. Our results show that Centaur's constraints have a recall of 94.0% and a precision of 94.0% on average. In terms of coverage, Centaur covers 203, 150, and 9,608 more branches than TitanFuzz, ACETest and Pathfinder, respectively. Using Centaur, we also detect 26 new bugs in PyTorch and TensorFlow, 18 of which are confirmed.

</details>


### [7] [FARM: Field-Aware Resolution Model for Intelligent Trigger-Action Automation](https://arxiv.org/abs/2601.15687)
*Khusrav Badalov,Young Yoon*

Main category: cs.SE

TL;DR: FARM是一个两阶段架构，用于自动化生成完整的TAP小程序，包括正确的成分到字段绑定配置，相比现有方法显著提升了准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的TAP平台自动化研究主要关注服务级别的自然语言预测，但生成的程序往往不可执行，仍需要手动配置。本文研究函数级别的配置问题，旨在生成具有完整配置的可执行小程序。

Method: 提出FARM两阶段架构：第一阶段使用对比双编码器与选择性层冻结训练，从1724个触发函数和1287个动作函数中检索候选；第二阶段采用基于LLM的多智能体管道进行选择和配置，包括意图分析、触发选择、跨模式评分的动作选择以及配置验证。

Result: 在函数级别上，FARM在Gold数据集上达到81%的联合准确率（Noisy 62%，One-shot 70%）；在服务级别上达到81%准确率，比TARGE提升23个百分点。能够生成成分到字段绑定，产生可执行的自动化配置。

Conclusion: FARM通过两阶段架构有效解决了TAP小程序生成中的函数级别配置问题，显著提高了自动化生成的准确率和可执行性，为Web of Things自动化提供了更实用的解决方案。

Abstract: Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable Web of Things (WoT) automation by composing event-driven rules across heterogeneous services. A TAP applet links a trigger to an action and must bind trigger outputs (ingredients) to action inputs (fields) to be executable. Prior work largely treats TAP as service-level prediction from natural language, which often yields non-executable applets that still require manual configuration. We study the function-level configuration problem: generating complete applets with correct ingredient-to-field bindings. We propose FARM (Field-Aware Resolution Model), a two-stage architecture for automated applet generation with full configuration. Stage 1 trains contrastive dual encoders with selective layer freezing over schema-enriched representations, retrieving candidates from 1,724 trigger functions and 1,287 action functions (2.2M possible trigger-action pairs). Stage 2 performs selection and configuration using an LLM-based multi-agent pipeline. It includes intent analysis, trigger selection, action selection via cross-schema scoring, and configuration verification. Agents coordinate through shared state and agreement-based selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot) at the function level, where both trigger and action functions must match the ground truth. For comparison with service-level baselines, we map functions to their parent services and evaluate at the service level. FARM reaches 81% joint accuracy and improves over TARGE by 23 percentage points. FARM also generates ingredient-to-field bindings, producing executable automation configurations.

</details>


### [8] [Evaluating and Achieving Controllable Code Completion in Code LLM](https://arxiv.org/abs/2601.15879)
*Jiajun Zhang,Zeyu Cui,Lei Zhang,Jian Yang,Jiaxi Yang,Qiang Liu,Zilei Wang,Binyuan Hui,Liang Wang,Junyang Lin*

Main category: cs.SE

TL;DR: 提出了首个指令引导的代码补全基准C3-Bench，包含2,195个任务，评估了40多个主流LLM，发现开源与专有模型在指令跟随能力上存在显著差距，并通过数据合成管道开发了SOTA模型Qwen2.5-Coder-C3。


<details>
  <summary>Details</summary>
Motivation: 当前代码补全基准主要关注基于上下文的代码功能正确性，忽视了LLM在代码补全过程中遵循用户指令的能力，而这正是LLM辅助编程中的常见场景。现有评估方法未能跟上LLM代码补全能力的发展。

Method: 1) 构建首个指令引导的代码补全基准C3-Bench，包含2,195个精心设计的补全任务；2) 对40多个主流LLM进行综合评估；3) 开发基于Qwen2.5-Coder的数据合成管道，生成高质量的指令-补全对用于监督微调。

Result: 1) 揭示了开源与先进专有模型在代码补全任务中指令跟随能力的显著差距；2) 通过微调得到的Qwen2.5-Coder-C3模型在C3-Bench上达到最先进性能；3) 所有代码、数据集和模型均已开源。

Conclusion: C3-Bench为增强LLM的代码补全和指令跟随能力提供了有价值的见解，为代码LLM的未来研究确立了新方向。开源资源将促进代码LLM领域的可复现性和进一步研究。

Abstract: Code completion has become a central task, gaining significant attention with the rise of large language model (LLM)-based tools in software engineering. Although recent advances have greatly improved LLMs' code completion abilities, evaluation methods have not advanced equally. Most current benchmarks focus solely on functional correctness of code completions based on given context, overlooking models' ability to follow user instructions during completion-a common scenario in LLM-assisted programming. To address this limitation, we present the first instruction-guided code completion benchmark, Controllable Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs across C3-Bench and conventional benchmarks, we reveal substantial gaps in instruction-following capabilities between open-source and advanced proprietary models during code completion tasks. Moreover, we develop a straightforward data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality instruction-completion pairs for supervised fine-tuning (SFT). The resulting model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our findings provide valuable insights for enhancing LLMs' code completion and instruction-following capabilities, establishing new directions for future research in code LLMs. To facilitate reproducibility and foster further research in code LLMs, we open-source all code, datasets, and models.

</details>


### [9] [The Role of Cognitive Abilities in Requirements Inspection: Comparing UML and Textual Representations](https://arxiv.org/abs/2601.16009)
*Giovanna Broccia,Sira Vegas,Alessio Ferrari*

Main category: cs.SE

TL;DR: UML序列图与文本需求结合使用对需求检查准确性的影响取决于个体的认知能力（工作记忆和心智旋转能力），存在复杂的三向交互作用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估UML序列图与文本需求结合是否比纯文本需求更能提高需求检查准确性，并探索认知能力（工作记忆和心智旋转技能）是否影响两种处理方式下的表现差异。

Method: 采用交叉实验设计，38名参与者分别在两种处理方式（纯文本 vs 文本+UML支持）下进行需求检查。使用线性混合效应模型和广义线性模型分析处理方式、周期、序列和认知能力的影响。

Result: 发现表示类型、工作记忆容量和心智旋转能力之间存在显著的三向交互作用。具有高认知能力的参与者在UML支持下的违规检测表现下降，但在UML辅助检查中的论证准确性提高。

Conclusion: UML支持的效果并非对所有个体一致，认知能力影响多模态信息（图表和文本）处理方式。高认知能力可能支持更深层次的推理过程，但可能损害简单的违规检测任务。

Abstract: The representation of requirements plays a critical role in the accuracy of requirements inspection. While visual representations, such as UML diagrams, are widely used alongside text-based requirements, their effectiveness in supporting inspection is still debated. Cognitive abilities, such as working memory and mental rotation skills, may also influence inspection accuracy. This study aims to evaluate whether the use of UML sequence diagrams alongside text-based requirements improves the accuracy of requirements inspection compared to text-based requirements alone and to explore whether cognitive abilities are associated with differences in performance across the two treatments (text vs text with UML support). We conducted a crossover experiment with 38 participants to assess the accuracy of requirements inspection under the two treatments in terms of issues found and justifications provided. Linear mixed-effects and generalized linear models were used to analyse the effects of treatment, period, sequence, and cognitive abilities. The results indicate a significant three-way interaction between representation type, working memory capacity, and mental rotation ability. This finding suggests that the effectiveness of UML support is not uniform across individuals: participants with high scores in both cognitive abilities experienced reduced performance when using UML for violation detection. Conversely, the same cognitive profile was associated with improved justification accuracy under UML-aided inspection, indicating that higher cognitive abilities may support deeper reasoning processes when dealing with multi-modal information, i.e., diagrams and text.

</details>


### [10] [Towards a Goal-Centric Assessment of Requirements Engineering Methods for Privacy by Design](https://arxiv.org/abs/2601.16080)
*Oleksandr Kosenkov,Ehsan Zabardast,Jannik Fischbach,Tony Gorschek,Daniel Mendez*

Main category: cs.SE

TL;DR: 研究人员提出了一种以目标为中心的评估框架，用于评估隐私设计（PbD）的需求工程方法，强调应根据组织目标而非仅流程特性来选择方法。


<details>
  <summary>Details</summary>
Motivation: 尽管GDPR隐私设计原则有众多需求工程方法，但组织难以确定哪种方法最适合其目标，需要系统化的评估框架来指导方法选择。

Method: 通过文献综述、访谈和实践者验证，综合构建了以目标为中心的PbD方法评估方法，强调基于组织目标而非流程特性进行评估。

Result: 研究发现实践者通常没有系统化地实施隐私设计，建议评估PbD方法时应以组织目标为导向，而非仅关注流程特性。

Conclusion: 以目标为中心的评估方法有望支持隐私设计需求工程实践的开发、选择和定制，帮助组织更有效地实施GDPR隐私设计要求。

Abstract: Implementing privacy by design (PbD) according to the General Data Protection Regulation (GDPR) is met with a growing number of requirements engineering (RE) approaches. However, the question of which RE method for PbD fits best the goals of organisations remains a challenge. We report our endeavor to close this gap by synthesizing a goal-centric approach for PbD methods assessment. We used literature review, interviews, and validation with practitioners to achieve the goal of our study. As practitioners do not approach PbD systematically, we suggest that RE methods for PbD should be assessed against organisational goals, rather than process characteristics only. We hope that, when further developed, the goal-centric approach could support the development, selection, and tailoring of RE practices for PbD.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [11] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: NL4ST是一个交互式工具，允许用户用自然语言查询时空数据库，通过三层架构将自然语言转换为物理查询计划。


<details>
  <summary>Details</summary>
Motivation: 随着移动计算设备和定位技术的发展，时空数据爆炸式增长，但非专业用户难以使用专业查询语言（如SQL）来查询这些数据，需要自然语言查询的支持。

Method: 采用三层架构：1) 知识库和语料库进行知识准备；2) 自然语言理解进行实体链接；3) 生成物理查询计划。工具支持范围查询、最近邻查询和连接查询等时空查询。

Result: 在四个真实和合成数据集上验证了NL4ST能够生成有效的时空物理查询计划，工具已在线提供并配有演示视频。

Conclusion: NL4ST成功弥合了非专业用户与数据库查询计划之间的鸿沟，使非专业用户能够通过自然语言轻松查询时空数据库。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [12] [Efficient Cloud-edge Collaborative Approaches to SPARQL Queries over Large RDF graphs](https://arxiv.org/abs/2601.15992)
*Shidan Ma,Peng Peng,Xu Zhou,M. Tamer Özsu,Lei Zou,Guo Chen*

Main category: cs.DB

TL;DR: 本文首次探索将边缘计算集成到RDF图数据存储与查询中，通过模式诱导子图解决数据定位问题，并构建系统模型优化查询分配与资源分配，在真实云平台上验证了性能优势。


<details>
  <summary>Details</summary>
Motivation: 随着RDF图的广泛应用，基于云的SPARQL查询解决方案在带宽受限或高负载环境下存在性能瓶颈。本文首次探索将边缘计算集成到图数据存储与处理中，以提升查询性能。

Method: 1) 引入模式诱导子图概念解决数据定位问题；2) 构建联合考虑数据分布、查询特征、网络通信和计算资源的系统模型；3) 将查询分配与计算资源分配联合建模为MINLP问题；4) 使用改进的分支定界算法求解。

Result: 在真实云平台上的实验结果表明，所提方法在效率方面优于现有最先进的基准方法。代码已在GitHub上开源。

Conclusion: 本文成功将边缘计算集成到RDF图数据管理中，通过创新的数据定位方法和系统优化模型，有效解决了云环境下SPARQL查询的性能瓶颈问题。

Abstract: With the increasing use of RDF graphs, storing and querying such data using SPARQL remains a critical problem. Current mainstream solutions rely on cloud-based data management architectures, but often suffer from performance bottle- necks in environments with limited bandwidth or high system load. To address this issue, this paper explores for the first time the integration of edge computing to move graph data storage and processing to edge environments, thereby improving query performance. This approach requires offloading query processing to edge servers, which involves addressing two challenges: data localization and network scheduling. First, the data localization challenge lies in computing the subgraphs maintained on edge servers to quickly identify the servers that can handle specific queries. To address this challenge, we introduce a new concept of pattern-induced subgraphs. Second, the network scheduling challenge involves efficiently assigning queries to edge and cloud servers to optimize overall system performance. We tackle this by constructing a overall system model that jointly captures data distribution, query characteristics, network communication, and computational resources. Accordingly, we further propose a joint formulation of query assignment and computational resource allocation, modeling it as a Mixed Integer Nonlinear Programming (MINLP) problem and solve this problem using a modified branch-and-bound algorithm. Experimental results on real datasets under a real cloud platform demonstrate that our proposed method outperforms the state-of-the-art baseline methods in terms of efficiency. The codes are available on GitHub

</details>


### [13] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: EAIFD：一种基于超图最小命中集枚举的高效增量函数依赖发现算法，通过多属性哈希表和两步验证策略显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统静态FD发现算法在增量更新时需要完全重新执行，效率低下；现有增量算法存在严重的性能和内存瓶颈，需要更高效的增量FD发现解决方案

Method: 1. 维护差异集的偏序超图，将增量FD发现问题转化为超图最小命中集枚举；2. 设计多属性哈希表(MHT)高效存储有效FD的键值映射，内存消耗与数据集大小无关；3. 开发两步验证策略：先利用MHT减少验证空间，再选择性加载数据块进行批量验证，避免重复I/O操作

Result: 在真实数据集上的实验表明，EAIFD相比现有算法：运行时间提升一个数量级，内存使用减少两个数量级以上

Conclusion: EAIFD是一种高效且可扩展的增量FD发现解决方案，通过创新的超图方法和内存优化技术，显著克服了现有算法的性能瓶颈

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>
