<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.DC](#cs.DC) [Total: 15]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Machine Learning Practitioners' Views on Data Quality in Light of EU Regulatory Requirements: A European Online Survey](https://arxiv.org/abs/2602.06594)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 提出一个将数据质量维度与欧盟监管要求对齐的框架，并通过调查揭示当前实践与监管期望之间的差距


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在欧盟监管环境下，数据质量如何符合监管要求是实践者面临的关键挑战，需要解决这一理论与实践脱节的问题

Method: 1) 提出将数据质量维度与欧盟监管要求对齐的实用框架；2) 对180多名欧盟数据从业者进行在线调查，了解他们在确保ML系统数据质量符合监管要求时的做法、挑战和需求

Result: 发现当前实践与监管期望之间存在重要差距，从业者需要更集成的数据质量工具以及技术与法律从业者之间更好的协作

Conclusion: 研究结果为弥合技术专长与监管合规之间的差距提供了建议，最终促进负责任和可信的ML部署

Abstract: Understanding how data quality aligns with regulatory requirements in machine learning (ML) systems presents a critical challenge for practitioners navigating the evolving EU regulatory landscape. To address this, we first propose a practical framework aligning established data quality dimensions with specific EU regulatory requirements. Second, we conducted a comprehensive online survey with over 180 EU-based data practitioners, investigating their approaches, key challenges, and unmet needs when ensuring data quality in ML systems that align with regulatory requirements. Our findings highlight crucial gaps between current practices and regulatory expectations, underscoring practitioners' need for more integrated data quality tools and better collaboration between technical and legal practitioners. These insights inform recommendations for bridging technical expertise and regulatory compliance, ultimately fostering responsible and trustworthy ML deployments.

</details>


### [2] [Filtered Approximate Nearest Neighbor Search Cost Estimation](https://arxiv.org/abs/2602.06721)
*Wenxuan Xia,Mingyu Yang,Wentao Li,Wei Wang*

Main category: cs.DB

TL;DR: 提出E2E成本估计框架用于过滤近似k近邻搜索，通过捕捉查询向量分布与属性选择性的相关性，实现更准确估计并优化搜索性能


<details>
  <summary>Details</summary>
Motivation: 混合查询结合高维向量相似性和结构化属性过滤在学术界和工业界受到广泛关注，但优化这类查询面临挑战，主要因为组合过滤器导致的搜索成本高度可变

Method: 提出新颖的E2E成本估计框架，显式捕捉查询向量分布与属性值选择性之间的相关性，利用这些估计来优化搜索终止条件

Result: 在真实世界数据集上的实验表明，该方法相比最先进基线将检索效率提高了2-3倍，同时保持高搜索精度

Conclusion: E2E框架通过准确估计过滤AKNN搜索成本，显著提升了混合查询的优化性能，为下游任务如早期终止提供了实用工具

Abstract: Hybrid queries combining high-dimensional vector similarity with structured attribute filtering have garnered significant attention across both academia and industry. A critical instance of this paradigm is filtered Approximate k Nearest Neighbor (AKNN) search, where embeddings (e.g., image or text) are queried alongside constraints such as labels or numerical range. While essential for rich retrieval, optimizing these queries remains challenging due to the highly variable search cost induced by combined filters. In this paper, we propose a novel cost estimation framework, E2E, for filtered AKNN search and demonstrate its utility in downstream optimization tasks, specifically early termination. Unlike existing approaches, our model explicitly captures the correlation between the query vector distribution and attribute-value selectivity, yielding significantly higher estimation accuracy. By leveraging these estimates to refine search termination conditions, we achieve substantial performance gains. Experimental results on real-world datasets demonstrate that our approach improves retrieval efficiency by 2x-3x over state-of-the-art baselines while maintaining high search accuracy.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [3] [SVRepair: Structured Visual Reasoning for Automated Program Repair](https://arxiv.org/abs/2602.06090)
*Xiaoxuan Tang,Jincheng Wang,Liwei Luo,Jingxuan Xu,Sheng Zhou,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: SVRepair是一个多模态程序修复框架，通过结构化视觉表示将视觉工件转换为语义场景图，帮助大语言模型更好地理解和修复包含视觉信息的bug。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型程序修复方法大多是单模态的，无法有效利用bug报告中包含的丰富视觉信息（如截图、控制流图等）。直接使用密集的视觉输入会导致上下文丢失和噪声，使得多模态大语言模型难以将视觉观察转化为精确的故障定位和可执行补丁。

Method: SVRepair首先微调一个视觉语言模型（SVR），将异构的视觉工件统一转换为语义场景图，捕捉GUI元素及其结构关系。基于该图，驱动编码代理进行故障定位和补丁合成，并引入迭代的视觉工件分割策略，逐步将输入缩小到bug相关区域，抑制无关上下文并减少幻觉。

Result: 在多个基准测试中表现出最先进的性能：在SWE-Bench M上达到36.47%准确率，MMCode上38.02%，CodeVision上95.12%，验证了SVRepair在多模态程序修复中的有效性。

Conclusion: SVRepair通过结构化视觉表示成功解决了多模态程序修复中的语义鸿沟问题，将视觉信息有效转化为代码相关的上下文，显著提升了程序修复的准确性和实用性。

Abstract: Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \textbf{36.47\%} accuracy on SWE-Bench M, \textbf{38.02\%} on MMCode, and \textbf{95.12\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.

</details>


### [4] [Coding Agents with Environment Interaction: A Theoretical Perspective](https://arxiv.org/abs/2602.06098)
*Nicolas Menet,Michael Hersche,Andreas Krause,Abbas Rahimi*

Main category: cs.SE

TL;DR: 本文为编码代理在测试驱动开发中的环境交互策略提供了概率框架，分析了代码选择与反馈生成两种范式，证明了模糊功能相似性估计器优于功能等价估计器，并将反向提示框架为Thompson采样的近似。


<details>
  <summary>Details</summary>
Motivation: 编码代理在测试驱动软件开发中应用日益广泛，但其环境交互策略的理论机制尚未得到充分探索。需要为两种主导范式（代码选择与反馈生成）提供理论框架，以理解其内在机制并指导改进。

Method: 1. 将代码选择启发式形式化为环境感知的正确性估计器；2. 证明基于模糊功能相似性的估计器在信噪比上严格优于功能等价估计器；3. 将反向提示框架为Thompson采样的上下文近似；4. 推导具有不可观测组件的奖励函数的遗憾界；5. 使用三个先进开源模型在三个基准数据集上进行实验验证。

Result: 理论分析表明：模糊功能相似性估计器通过引入归纳偏置，在信噪比上严格优于功能等价估计器；反向提示的效果受非正式任务描述模糊性的限制（不可约遗憾）。实验在BigCodeBenchHard、LeetCodeDataset和QiskitHumanEvalSim上验证了这些发现，并基于理论洞察创建了新基准QiskitHumanEvalSimX。

Conclusion: 本文为编码代理的环境交互策略提供了首个概率理论框架，揭示了模糊功能相似性估计器的优越性，解释了反向提示的局限性，并提出了改进任务描述的方法。这些理论洞察有助于设计更有效的编码代理系统。

Abstract: Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.

</details>


### [5] [Scaling Mobile Chaos Testing with AI-Driven Test Execution](https://arxiv.org/abs/2602.06223)
*Juan Marcano,Ashish Samant,Kai Song,Lingchao Chen,Kaelan Mikowicz,Tim Smyth,Mengdie Zhang,Ali Zamani,Arturo Bravo Rovirosa,Sowjanya Puligadda,Srikanth Prodduturi,Mayank Bansal*

Main category: cs.SE

TL;DR: 论文提出了一种自动化移动混沌测试系统，将LLM驱动的移动测试平台DragonCrawl与服务级故障注入系统uHavoc结合，解决了传统混沌工程在移动测试中因用户流、地理位置和故障场景组合爆炸而无法扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式系统中的移动应用容易受到后端服务故障的影响，但传统的混沌工程方法无法扩展移动测试，因为需要验证的用户流、地理位置和故障场景的组合呈爆炸式增长，手动编写测试用例不切实际。

Method: 系统整合了DragonCrawl（基于LLM的移动测试平台）和uHavoc（服务级故障注入系统）。核心洞察是自适应AI驱动的测试执行可以在后端降级条件下导航移动应用，无需为每个用户流、城市和故障类型的组合手动编写测试用例。

Result: 自2024年第一季度以来，系统在Uber的Rider、Driver和Eats应用的47个关键流程中执行了超过18万次自动化混沌测试，相当于约3.9万小时的手动测试工作量。识别了23个弹性风险，其中70%是架构依赖违规（非关键服务故障影响核心用户流）。12个问题严重到阻止行程请求或食品订单，2个导致仅通过移动混沌测试才能检测到的应用崩溃。自动化根因分析将调试时间从数小时缩短到数分钟，在将移动故障归因于特定后端服务方面达到88%的precision@5。

Conclusion: 该系统设计在故障注入下保持99%的测试可靠性，操作经验表明，在生产规模下实现持续的移动弹性验证是可行的。自动化移动混沌测试系统能够大规模识别传统后端测试无法发现的移动端特定问题，显著提高了移动应用的弹性。

Abstract: Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.

</details>


### [6] [Trustworthy AI Software Engineers](https://arxiv.org/abs/2602.06310)
*Aldeida Aleti,Baishakhi Ray,Rashina Hoda,Simin Chen*

Main category: cs.SE

TL;DR: 该论文探讨AI编码代理作为软件工程师的可信度问题，提出在人类-AI团队中评估AI软件工程师信任度的关键维度和方法。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理的快速发展，软件工程师的本质定义受到挑战。论文旨在重新审视AI代理作为软件工程师的意义，并深入思考如何建立对AI软件工程师的信任。

Method: 基于软件工程的传统定义和最新AI代理系统研究，将AI软件工程师概念化为人类-AI软件工程团队的参与者。通过历史视角和新兴愿景，识别影响AI软件工程师可信度的关键维度。

Result: 识别出AI软件工程师可信度的四个关键维度：技术质量、透明度和问责制、认知谦逊、社会与伦理对齐。同时指出信任评估存在根本性测量缺口——并非所有影响信任的因素都能轻易量化。

Conclusion: 倡导采用"设计即伦理"的方法来设计和治理AI软件工程系统，以在未来人类-AI团队中建立适当的信任关系，并强调需要解决信任测量与评估的挑战。

Abstract: With the rapid rise of AI coding agents, the fundamental premise of what it means to be a software engineer is in question. In this vision paper, we re-examine what it means for an AI agent to be considered a software engineer and then critically think about what makes such an agent trustworthy. \textit{Grounded} in established definitions of software engineering (SE) and informed by recent research on agentic AI systems, we conceptualise AI software engineers as participants in human-AI SE teams composed of human software engineers and AI models and tools, and we distinguish trustworthiness as a key property of these systems and actors rather than a subjective human attitude. Based on historical perspectives and emerging visions, we identify key dimensions that contribute to the trustworthiness of AI software engineers, spanning technical quality, transparency and accountability, epistemic humility, and societal and ethical alignment. We further discuss how trustworthiness can be evaluated and demonstrated, highlighting a fundamental trust measurement gap: not everything that matters for trust can be easily measured. Finally, we outline implications for the design, evaluation, and governance of AI SE systems, advocating for an ethics-by-design approach to enable appropriate trust in future human-AI SE teams.

</details>


### [7] [AgentStepper: Interactive Debugging of Software Development Agents](https://arxiv.org/abs/2602.06593)
*Robert Hutter,Michael Pradel*

Main category: cs.SE

TL;DR: AgentStepper：首个基于LLM的软件工程代理交互式调试器，通过结构化对话表示、断点、单步执行等功能提升代理轨迹的可理解性和调试效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的软件开发代理难以调试，因为其复杂的动态特性使得开发者难以理解LLM查询、工具调用和代码修改的中间过程，需要更高层次的抽象来提升调试效率。

Method: 提出AgentStepper调试器，将代理轨迹表示为LLM、代理程序和工具之间的结构化对话，支持断点、单步执行、实时编辑提示和工具调用，同时捕获并显示仓库级别的代码变更。

Result: 集成到三个先进代理（ExecutionAgent、SWE-Agent、RepairAgent）仅需少量代码修改（39-42行）。用户研究表明，相比传统工具，AgentStepper提升轨迹解释能力（64% vs 67%），提高bug识别成功率（17% vs 60%），并显著降低工作负荷（挫败感从5.4/7.0降至2.4/7.0）。

Conclusion: AgentStepper通过提供类似传统软件调试的交互式体验，有效解决了LLM基软件开发代理的调试难题，显著提升了开发者的理解和调试效率。

Abstract: Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.

</details>


### [8] [Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study](https://arxiv.org/abs/2602.06671)
*Shijia Dong,Haoruo Zhao,Paul Harvey*

Main category: cs.SE

TL;DR: 提出AST(NIT)方法，将AST序列化以增强LLM的代码摘要能力，相比原始代码减少输入长度和训练时间，同时保持摘要质量


<details>
  <summary>Details</summary>
Motivation: 传统编码器-解码器模型已证明AST能提升代码摘要质量，但现有LLM方法主要依赖原始代码或仅使用部分AST信号，完整的AST表示潜力尚未在LLM中得到充分探索

Method: 提出AST(NIT)方法，通过AST增强和序列化技术，保留词汇细节并将结构信息编码为LLM兼容的序列，使用LLaMA-3.1-8B模型在CodeXGLUE Python数据集上进行实验

Result: 序列化后的AST能减少LLM输入长度，缩短训练时间，同时达到与现有方法相当的摘要质量

Conclusion: AST(NIT)方法成功将完整的AST表示整合到LLM中，为代码摘要任务提供了更高效的解决方案

Abstract: Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.

</details>


### [9] [Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA](https://arxiv.org/abs/2602.06709)
*Duong Bui,Stefan Grintz,Alexander Berndt,Thomas Bach*

Main category: cs.SE

TL;DR: LLM结合历史故障数据可自动化CI/CD管道故障管理，在工业项目中达到92.1%的精确解决方案生成准确率。


<details>
  <summary>Details</summary>
Motivation: CI/CD管道故障管理手动操作耗时，传统程序无法处理非结构化信息，而LLM在处理非结构化数据方面表现出潜力，需要验证其在工业项目中的实际效果。

Method: 在SAP HANA大型工业项目中评估LLM系统，提供管道信息、故障管理指令和历史故障数据等不同领域知识，通过消融研究确定各类知识对解决方案准确性的贡献。

Result: 历史故障数据对系统准确性贡献最大，使系统在92.1%的情况下生成精确解决方案；结合领域知识时错误定位准确率达97.4%，无领域知识时为84.2%。

Conclusion: LLM结合历史故障数据是自动化CI/CD管道故障管理的有效方法，在工业环境中表现出高准确性和实用性。

Abstract: CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.

</details>


### [10] [Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience](https://arxiv.org/abs/2602.06831)
*Marco De Luca,Domenico Amalfitano,Anna Rita Fasolino,Porfirio Tramontana*

Main category: cs.SE

TL;DR: 提出一种为嵌入式固件定义软件度量阈值的结构化方法，用于可解释的故障预测，替代黑盒AI模型


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如ISO 26262标准），需要可靠的软件质量保证。虽然机器学习故障预测模型准确率高，但缺乏可解释性，限制了工业应用。开发者需要可直接用于SQA流程和缺陷缓解策略的可操作见解。

Method: 通过结构化流程定义特定上下文的软件度量阈值：1）分析三个真实C嵌入式固件项目；2）使用Coverity和Understand静态分析工具提取软件度量；3）通过统计分析和假设检验识别区分性度量；4）推导能区分故障与非故障函数的经验阈值；5）支持跨项目故障预测，从一个项目集推导阈值并应用于独立开发的固件。

Result: 实验验证显示推导的阈值能有效识别故障倾向函数，具有高精度。这些阈值可作为可解释的故障预测解决方案，符合行业标准和SQA实践。

Conclusion: 该方法为黑盒AI模型提供了实用替代方案，允许开发者系统评估软件质量、采取预防措施，并将基于度量的故障预测集成到工业开发工作流中，以缓解软件故障。

Abstract: Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.

</details>


### [11] [TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code](https://arxiv.org/abs/2602.06875)
*Jiangping Huang,Wenguang Ye,Weisong Sun,Jian Zhang,Mingyue Zhang,Yang Liu*

Main category: cs.SE

TL;DR: TraceCoder：一个多智能体协作框架，通过运行时追踪和因果分析来修复LLM生成的代码错误，结合历史教训学习和回滚机制，显著提升修复准确率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码修复方法依赖简单的通过/失败信号，缺乏对程序行为的深入洞察，难以精确定位错误，且修复过程容易陷入低效循环，需要更智能的修复框架。

Method: TraceCoder采用多智能体协作框架，模拟人类专家的观察-分析-修复过程：1) 在代码中插入诊断探针捕获细粒度运行时追踪；2) 对追踪进行因果分析定位根本原因；3) 引入历史教训学习机制从失败修复中学习；4) 使用回滚机制确保每次迭代都是严格改进。

Result: 在多个基准测试中，TraceCoder相比现有先进基线在Pass@1准确率上取得最高34.43%的相对提升；消融实验显示迭代修复过程单独贡献65.61%的相对增益；在准确率和成本效率方面均显著优于领先的迭代方法。

Conclusion: TraceCoder通过深度运行时洞察、因果分析和历史学习，有效解决了LLM代码修复中的关键挑战，为自动化代码修复提供了更智能、高效的解决方案。

Abstract: Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [12] [Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing](https://arxiv.org/abs/2602.06057)
*Satyam Kumar,Saurabh Jha*

Main category: cs.DC

TL;DR: QEIL框架通过推理时间缩放定律和异构编排，在资源受限的边缘设备上实现高效本地LLM推理，显著提升覆盖率和能效。


<details>
  <summary>Details</summary>
Motivation: 资源受限边缘设备上的大语言模型推理仍然是一个主要挑战，现有解决方案严重依赖云或数据中心基础设施，需要低延迟智能系统的本地推理方案。

Method: 提出QEIL统一框架，包含：1）推理时间缩放定律，显示异构工作负载分配实现超线性效率增益；2）硬件感知路由，通过分析成本模型考虑计算吞吐量、内存带宽、功耗和热限制；3）性能-能耗权衡量化，使用智能每瓦特、能量覆盖效率等新指标。通过渐进样本复用统一编排器组合这些组件。

Result: 在125M到2.6B参数的五个模型系列上评估，结果显示：7-10.5个百分点的pass@k覆盖率提升，35.6-78.2%的能量减少，68%的平均功耗降低（满足边缘热预算），15.8%的延迟改善，零准确率损失。

Conclusion: 推理时间缩放定律是通用且架构无关的，异构边缘编排是能量受限智能系统的最优策略。

Abstract: Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.

</details>


### [13] [Mapping Gemma3 onto an Edge Dataflow Architecture](https://arxiv.org/abs/2602.06063)
*Shouyu Du,Miaoxiang Yu,Zhiheng Ni,Jillian Cai,Qing Yang,Tao Wei,Zhenyu Xu*

Main category: cs.DC

TL;DR: 本文首次在AMD Ryzen AI NPU上端到端部署Gemma3系列大语言和视觉模型，通过硬件感知优化技术实现显著性能提升，证明现代NPU可在边缘设备上实现实用、低功耗的LLM和VLM推理。


<details>
  <summary>Details</summary>
Motivation: 探索如何在边缘设备的tiled数据流架构（AMD Ryzen AI NPU）上高效部署大型语言和视觉模型，解决传统CPU和iGPU在边缘推理中的性能瓶颈和功耗问题。

Method: 提出一系列硬件感知优化技术：预填充阶段采用高效反量化引擎、优化tiled矩阵乘法内核、提出FlowQKV分块流水线注意力机制；解码阶段引入FusedDQP（融合反量化和投影）和FlowKV（重构注意力以维持高内存带宽利用率），并结合紧凑的Q4NX 4位量化格式。

Result: 相比iGPU，预填充速度提升5.2倍，解码速度提升4.8倍；相比CPU，分别提升33.5倍和2.2倍。能效比iGPU提升67.2倍，比CPU提升222.9倍。

Conclusion: 现代NPU能够在边缘设备上实现实用、低功耗的LLM和VLM推理，为将基于Transformer的模型映射到tiled数据流加速器提供了可推广的蓝图。

Abstract: We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\times$ faster prefill and $4.8\times$ faster decoding versus the iGPU, and $33.5\times$ and $2.2\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\times$ and $222.9\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.

</details>


### [14] [iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems](https://arxiv.org/abs/2602.06064)
*Yi-Xiang Hu,Yuke Wang,Feng Wu,Zirui Huang,Shuli Zeng,Xiang-Yang Li*

Main category: cs.DC

TL;DR: iScheduler：基于强化学习的迭代调度框架，用于解决资源投资问题，通过分解子问题和序列化进程选择，在工业规模任务上实现43倍加速


<details>
  <summary>Details</summary>
Motivation: 传统混合整数规划和约束规划方法在大规模资源投资问题实例上计算缓慢，且动态更新需要低延迟的调度修订，需要更高效的解决方案

Method: 将资源投资问题建模为马尔可夫决策过程，分解为子问题，通过强化学习驱动的迭代调度框架进行序列化进程选择，支持重用未更改的进程调度

Result: 在包含1000个实例、2500-10000个任务的工业规模基准测试中，iScheduler在保持竞争力的资源成本的同时，将可行性时间减少了高达43倍

Conclusion: iScheduler通过强化学习和迭代调度方法有效解决了大规模资源投资问题，显著加速优化过程并支持动态重新配置

Abstract: Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\times$ against strong commercial baselines.

</details>


### [15] [HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference](https://arxiv.org/abs/2602.06069)
*Dinesh Gopalan,Ratul Ali*

Main category: cs.DC

TL;DR: HQP框架通过结合剪枝和量化，在边缘设备上实现3.12倍推理加速和55%模型压缩，同时保证精度下降小于1.5%


<details>
  <summary>Details</summary>
Motivation: 边缘云环境中对高保真实时推理的需求日益增长，但面临严重的延迟和能耗限制，需要激进的模型优化方法

Method: 提出混合量化与剪枝框架，采用基于Fisher信息矩阵近似的敏感度感知结构剪枝算法，在满足精度约束后进行8位后训练量化

Result: 在NVIDIA Jetson边缘平台上，MobileNetV3和ResNet-18模型达到3.12倍推理加速和55%模型压缩，精度下降严格控制在1.5%以内

Conclusion: HQP框架是硬件无关的优越解决方案，适用于资源受限边缘基础设施的超低延迟AI部署

Abstract: The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.

</details>


### [16] [Computationally Efficient Laplacian CL-colME](https://arxiv.org/abs/2602.06070)
*Nikola Stankovic*

Main category: cs.DC

TL;DR: 本文提出CL-colME，一种基于拉普拉斯共识的分散式协作均值估计新变体，相比C-colME避免了计算昂贵的归一化过程，在保持收敛性和准确性的同时提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 分散式协作均值估计（colME）是异构网络中的基本任务。现有的基于图的变体B-colME和C-colME虽然实现了高可扩展性，但C-colME依赖于双随机平均矩阵，需要进行计算昂贵的归一化过程。需要一种既能保持收敛性能又能提高计算效率的新方法。

Method: 提出CL-colME（基于拉普拉斯共识的协作均值估计），采用拉普拉斯共识机制替代双随机平均矩阵，避免了复杂的归一化计算过程。该方法利用图拉普拉斯算子实现节点间的信息交换和共识达成。

Result: 仿真结果表明，CL-colME在保持与C-colME相同的收敛行为和估计准确性的同时，显著提高了计算效率。新方法避免了归一化过程的计算开销，在分布式环境中具有更好的性能表现。

Conclusion: CL-colME是一种有效的分散式协作均值估计方法，通过采用拉普拉斯共识机制，在保持算法收敛性和准确性的前提下，显著降低了计算复杂度，为大规模异构网络中的分布式估计问题提供了更高效的解决方案。

Abstract: Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.

</details>


### [17] [FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs](https://arxiv.org/abs/2602.06071)
*Rajat Vadiraj Dwaraknath,Sungyoon Kim,Mert Pilanci*

Main category: cs.DC

TL;DR: 提出BlockPerm-SJLT稀疏草图与FlashSketch CUDA内核协同设计，解决GPU上稀疏草图内存访问效率问题，在保持理论保证的同时实现1.7倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏草图（如稀疏Johnson-Lindenstrauss变换）虽然能降低计算成本，但其随机稀疏性导致GPU内存访问模式不规则，严重影响内存带宽利用率，限制了在GPU上的高效实现。

Method: 采用草图-内核协同设计：1) 设计新的稀疏草图家族BlockPerm-SJLT，其稀疏结构专门优化GPU内存访问；2) 开发对应的CUDA内核FlashSketch高效实现这些草图；3) 引入可调参数平衡GPU效率与草图鲁棒性。

Result: 1) 理论证明BlockPerm-SJLT满足无意识子空间嵌入（OSE）保证；2) 在RandNLA基准测试和GraSS数据归因流水线中，FlashSketch在质量与速度的帕累托前沿上优于现有方法；3) 相比之前最佳GPU草图实现，获得约1.7倍的几何平均加速。

Conclusion: 通过草图-内核协同设计方法，成功解决了稀疏草图在GPU上的效率瓶颈，BlockPerm-SJLT和FlashSketch在保持理论保证的同时显著提升了GPU实现效率，推动了随机数值线性代数在GPU上的实际应用。

Abstract: Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.

</details>


### [18] [PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference](https://arxiv.org/abs/2602.06072)
*Rui Ning,Wei Zhang,Fan Lai*

Main category: cs.DC

TL;DR: PackInfer是一个内核级注意力框架，通过将异质批处理请求打包成负载均衡的执行组，优化LLM推理中的注意力计算效率和I/O平衡，显著提升GPU利用率和推理性能。


<details>
  <summary>Details</summary>
Motivation: 生产环境中LLM服务需要批处理具有高度异质序列长度的请求以实现高吞吐量，但现有注意力优化技术（如FlashAttention）主要针对单个请求，导致计算和I/O不平衡、拖尾效应严重以及GPU资源利用不足的问题。

Method: 1. 将批处理请求编排成负载均衡的执行组，通过将多个请求打包到统一的内核启动中来饱和GPU利用率；2. 直接在打包的查询-键区域上构建注意力内核，消除冗余计算并平衡线程块执行；3. 采用I/O感知分组，将共享前缀的请求共置，并将KV缓存重组为组连续布局，减少内存碎片和冗余数据移动。

Result: 在实际工作负载评估中，PackInfer相比最先进的FlashAttention，推理延迟降低13.0-20.1%，吞吐量提升20%。

Conclusion: PackInfer通过计算和I/O感知的异质批处理推理执行，有效解决了生产LLM服务中的注意力效率问题，显著提升了GPU资源利用率和整体推理性能。

Abstract: Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.

</details>


### [19] [Experimental Analysis of Server-Side Caching for Web Performance](https://arxiv.org/abs/2602.06074)
*Mohammad Umar,Bharat Tripathi*

Main category: cs.DC

TL;DR: 实验研究表明，在小型Web应用中，简单的内存缓存能显著降低响应时间，适合教育和简单应用场景。


<details>
  <summary>Details</summary>
Motivation: 虽然缓存技术已被广泛研究，但缺乏针对小型Web应用中简单内存缓存效果的实验研究。本文旨在填补这一研究空白，探索简单服务器端缓存在小型应用中的性能影响。

Method: 采用实验方法比较两种服务器配置：无缓存配置 vs 带内存缓存和固定TTL的配置。使用轻量级Web服务器框架，在相同环境条件下通过重复HTTP请求测量响应时间。

Result: 结果显示缓存请求的响应时间显著降低，证明了简单服务器端缓存在提升Web应用性能方面的有效性。

Conclusion: 简单内存缓存能有效改善小型Web应用性能，特别适合教育环境和简单应用场景，其中简单性和可复现性至关重要。

Abstract: Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.

</details>


### [20] [MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments](https://arxiv.org/abs/2602.06075)
*Guangyi Liu,Pengxiang Zhao,Yaozhen Liang,Qinyi Luo,Shunye Tang,Yuxiang Chai,Weifeng Lin,Han Xiao,WenHao Wang,Siheng Chen,Zhengxi Lu,Gao Wu,Hao Wang,Liang Liu,Yong Liu*

Main category: cs.DC

TL;DR: MemGUI-Bench是一个专门评估移动GUI代理记忆能力的基准测试，填补了现有基准在记忆相关任务评估上的空白，包含128个跨时空记忆任务和自动化评估流程。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI代理基准测试在记忆能力评估方面存在系统性缺陷，只有5.2-11.8%的任务涉及记忆，且缺乏跨会话学习评估。需要专门的记忆中心化基准来全面评估GUI代理的记忆能力。

Method: 1) 提出系统化记忆分类法，分析11个代理的5种架构；2) 创建128个跨26个应用的任务，其中89.8%通过跨时空记忆保持来挑战记忆能力；3) 开发MemGUI-Eval自动化评估管道，包含渐进式审查和7个分层指标；4) 对11个最先进代理进行RQ驱动评估。

Result: 实验显示所有评估系统都存在显著记忆缺陷，识别出5种不同的失败模式，并综合出5个可操作的设计启示。基准测试资源将完全开源并持续维护。

Conclusion: MemGUI-Bench填补了移动GUI代理记忆评估的空白，揭示了现有系统的记忆局限性，为未来GUI代理设计提供了重要指导，所有资源将开源以促进社区发展。

Abstract: Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \textbf{\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/.

</details>


### [21] [Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)
*Liangyu Wang,Siqi Zhang,Junjie Wang,Yiming Dong,Bo Zheng,Zihan Qiu,Shengkun Tang,Di Wang,Rui Men,Dayiheng Liu*

Main category: cs.DC

TL;DR: Canzona是一个统一的异步负载均衡框架，用于解决矩阵优化器在分布式训练中的效率问题，通过解耦逻辑优化器分配与物理参数分布，在256 GPU上实现1.57倍端到端迭代加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型扩展推动了对矩阵优化器的兴趣，但其整体更新需求与分布式框架中的张量碎片化存在冲突。现有方案存在计算冗余或违反高效通信原语几何约束的问题。

Method: 提出Canzona框架：1) 数据并行中采用α平衡静态分区策略，保持原子性同时平衡负载；2) 张量并行中设计异步计算流水线，利用微组调度批量处理碎片化更新并隐藏重构开销。

Result: 在Qwen3模型家族（最多320亿参数）和256个GPU上的评估显示，该方法保持了现有并行架构的效率，端到端迭代时间加速1.57倍，优化器步骤延迟降低5.8倍。

Conclusion: Canzona成功解决了矩阵优化器在分布式训练中的效率冲突，通过统一的异步负载均衡框架实现了显著的性能提升，为大规模模型训练提供了有效的优化器集成方案。

Abstract: The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.

</details>


### [22] [LAAFD: LLM-based Agents for Accelerated FPGA Design](https://arxiv.org/abs/2602.06085)
*Maxim Moraru,Kamalavasan Kamalakkannan,Jered Dominguez-Trujillo,Patrick Diehl,Atanu Barai,Julien Loiseau,Zachary Kent Baker,Howard Pritchard,Galen M Shipman*

Main category: cs.DC

TL;DR: LAAFD是一个基于大语言模型的自动化工作流，能够将通用C++代码转换为优化的Vitis HLS内核，实现接近手工调优的性能（99.9%几何平均性能），显著降低FPGA加速的专业门槛。


<details>
  <summary>Details</summary>
Motivation: FPGA在科学计算和边缘计算中具有高性能、低延迟和能效优势，但需要专门的硬件专业知识，限制了其广泛采用。虽然高级综合（HLS）提高了生产力，但竞争性设计仍需要硬件感知优化和精心设计的数据流。

Method: LAAFD采用基于大语言模型的智能工作流，将通用C++代码转换为优化的Vitis HLS内核。自动化关键转换包括深度流水线、向量化和数据流分区，并通过HLS协同仿真和综合反馈形成闭环，验证正确性并迭代改进执行周期。

Result: 在代表HPC常见计算模式的15个内核测试套件中，LAAFD相比手工调优的Vitis HLS基线实现了99.9%的几何平均性能。对于模板计算工作负载，LAAFD性能与最先进的基于DSL的HLS代码生成器SODA相当，同时生成更易读的内核。

Conclusion: LAAFD显著降低了FPGA加速的专业知识门槛，同时不牺牲效率，为FPGA在科学和边缘计算中的更广泛采用提供了可行路径。

Abstract: FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.

</details>


### [23] [BouquetFL: Emulating diverse participant hardware in Federated Learning](https://arxiv.org/abs/2602.06498)
*Arno Geimer*

Main category: cs.DC

TL;DR: BouquetFL是一个联邦学习框架，通过资源限制在单机上模拟异构客户端硬件，帮助研究者在不依赖多物理设备的情况下研究系统异构性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数联邦学习研究在单机上进行模拟，忽略了实际部署中客户端硬件异构性的重要影响，这导致实验条件与实际情况存在差距。

Method: 通过程序化资源限制来模拟不同硬件配置，提供从常见消费设备和小型实验室设备中提取的多种硬件配置文件，以及基于真实硬件流行度的自定义硬件采样器。

Result: 开发了BouquetFL框架，能够在单物理机器上模拟异构客户端硬件，使联邦学习实验更贴近实际部署条件。

Conclusion: BouquetFL填补了联邦学习研究中的方法学空白，为研究高度异构联邦的研究者提供了便捷工具，使实验实践更接近实际部署条件。

Abstract: In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.

</details>


### [24] [FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training](https://arxiv.org/abs/2602.06499)
*Gyeongseo Park,Eungyeong Lee,Song-woo Sok,Myung-Hoon Cha,Kwangwon Koh,Baik-Song An,Hongyeon Kim,Ki-Dong Kang*

Main category: cs.DC

TL;DR: FCDP 是一种针对带宽受限集群的分布式训练优化方法，通过将前向传播参数缓存在主机内存中，并在反向传播时通过快速节点内all-gather复用，减少50%的节点间通信，在PEFT场景下可减少99%以上的节点间流量。


<details>
  <summary>Details</summary>
Motivation: 现有分布式训练方法在带宽受限的硬件环境下存在瓶颈：ZeRO-3在节点间通信开销大；GPU内存缓存方法（如MiCS、ZeRO++）会导致内存不足；主机内存卸载方法（如ZeRO-Offload、ZeRO-Infinity）因PCIe开销降低吞吐量。需要一种能在带宽受限集群上高效利用主机内存的方法。

Method: FCDP将主机内存作为快速缓存层而非溢出层，缓存前向传播参数并在反向传播时通过节点内all-gather复用，消除冗余的节点间通信。对于参数高效微调（PEFT），选择性通信仅训练参数以最大化缓存效果。

Result: 在商品硬件集群上，FCDP相比ZeRO-3实现高达100倍的吞吐量提升，相比ZeRO++实现51倍提升，同时保持ZeRO-3的最大批处理大小。

Conclusion: FCDP通过创新性地将主机内存作为快速缓存层，有效解决了带宽受限集群上的分布式训练瓶颈，在保持内存效率的同时大幅提升训练吞吐量，特别适用于参数高效微调场景。

Abstract: Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.

</details>


### [25] [DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving](https://arxiv.org/abs/2602.06502)
*Ying Yuan,Pengfei Zuo,Bo Wang,Zhangyu Chen,Zhipeng Tan,Zhou Yu*

Main category: cs.DC

TL;DR: DualMap提出了一种双映射调度策略，通过两个独立的哈希函数将请求映射到候选实例，智能选择更好的候选实例，在分布式LLM服务中同时实现缓存亲和性和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 在LLM服务中，跨请求重用KV缓存对降低TTFT和服务成本至关重要。现有的调度器无法协调缓存亲和性调度（将相同前缀的请求放在一起）和负载均衡调度（将请求均匀分布）之间的冲突，因为它们都在单一映射空间内操作。

Method: DualMap采用双映射调度策略：1）通过两个独立的哈希函数基于请求提示将每个请求映射到两个候选实例；2）基于当前系统状态智能选择更好的候选实例；3）结合SLO感知请求路由、热点感知重新平衡和轻量级双哈希环扩展三种技术来应对动态和倾斜的工作负载。

Result: 在真实工作负载上的实验表明，在相同的TTFT SLO约束下，DualMap相比最先进的工作将有效请求容量提高了最多2.25倍。

Conclusion: DualMap通过创新的双映射调度策略成功解决了缓存亲和性和负载均衡之间的权衡问题，在分布式LLM服务中实现了更好的性能表现。

Abstract: In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\times$ under the same TTFT SLO constraints compared with SOTA work.

</details>


### [26] [Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms](https://arxiv.org/abs/2602.06555)
*Lanpei Li,Massimo Coppola,Malio Li,Valerio Besozzi,Jack Bell,Vincenzo Lomonaco*

Main category: cs.DC

TL;DR: 提出一个在无服务器平台上动态管理结构化并行处理骨架的框架，专注于Farm模式，结合AI驱动的动态扩展来提升性能和资源效率


<details>
  <summary>Details</summary>
Motivation: 将HPC级别的性能和弹性引入无服务器和连续计算环境，同时保持骨架编程的可编程性优势

Method: 基于OpenFaaS平台实现Farm模式，将工作池自动扩展视为QoS感知的资源管理问题，框架包含可重用farm模板和基于Gymnasium的监控控制层，支持反应式和基于学习的控制器

Result: AI驱动的管理比纯基于模型的性能引导更能适应平台特定限制，在保持高效资源使用和稳定扩展行为的同时改善QoS

Conclusion: AI驱动的动态扩展在无服务器平台上管理并行度方面是有效的，能够更好地平衡性能和资源效率

Abstract: We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.

</details>
