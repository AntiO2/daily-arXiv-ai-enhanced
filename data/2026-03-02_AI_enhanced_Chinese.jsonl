{"id": "2602.23598", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.23598", "abs": "https://arxiv.org/abs/2602.23598", "authors": ["Md Hasanur Rashid", "Jesun Firoz", "Nathan R. Tallent", "Luanzheng Guo", "Meng Tang", "Dong Dai"], "title": "QoSFlow: Ensuring Service Quality of Distributed Workflows Using Interpretable Sensitivity Models", "comment": "to be published in 40th IEEE International Parallel & Distributed Processing Symposium (IPDPS), 2026", "summary": "With the increasing importance of distributed scientific workflows, there is a critical need to ensure Quality of Service (QoS) constraints, such as minimizing time or limiting execution to resource subsets. However, the unpredictable nature of workflow behavior, even with similar configurations, makes it difficult to provide QoS guarantees. For effective reasoning about QoS scheduling, we introduce QoSFlow, a performance modeling method that partitions a workflow's execution configuration space into regions with similar behavior. Each region groups configurations with comparable execution times according to a given statistical sensitivity, enabling efficient QoS-driven scheduling through analytical reasoning rather than exhaustive testing. Evaluation on three diverse workflows shows that QoSFlow's execution recommendations outperform the best-performing standard heuristic by 27.38%. Empirical validation confirms that QoSFlow's recommended configurations consistently match measured execution outcomes across different QoS constraints.", "AI": {"tldr": "QoSFlow\u662f\u4e00\u79cd\u6027\u80fd\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5de5\u4f5c\u6d41\u6267\u884c\u914d\u7f6e\u7a7a\u95f4\u5212\u5206\u4e3a\u5177\u6709\u76f8\u4f3c\u884c\u4e3a\u7684\u533a\u57df\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684QoS\u9a71\u52a8\u8c03\u5ea6\uff0c\u76f8\u6bd4\u6700\u4f73\u6807\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\u6027\u80fd\u63d0\u534727.38%\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u9700\u8981\u786e\u4fdd\u670d\u52a1\u8d28\u91cf\u7ea6\u675f\uff0c\u4f46\u5de5\u4f5c\u6d41\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u4f7f\u5f97\u63d0\u4f9bQoS\u4fdd\u8bc1\u53d8\u5f97\u56f0\u96be\u3002", "method": "QoSFlow\u5c06\u5de5\u4f5c\u6d41\u7684\u6267\u884c\u914d\u7f6e\u7a7a\u95f4\u5212\u5206\u4e3a\u5177\u6709\u76f8\u4f3c\u884c\u4e3a\u7684\u533a\u57df\uff0c\u6bcf\u4e2a\u533a\u57df\u6839\u636e\u7ed9\u5b9a\u7684\u7edf\u8ba1\u654f\u611f\u6027\u5c06\u5177\u6709\u53ef\u6bd4\u6267\u884c\u65f6\u95f4\u7684\u914d\u7f6e\u5206\u7ec4\uff0c\u901a\u8fc7\u5206\u6790\u63a8\u7406\u800c\u975e\u7a77\u4e3e\u6d4b\u8bd5\u5b9e\u73b0\u9ad8\u6548\u8c03\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u5de5\u4f5c\u6d41\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cQoSFlow\u7684\u6267\u884c\u5efa\u8bae\u6bd4\u6700\u4f73\u6807\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\u6027\u80fd\u63d0\u534727.38%\uff0c\u7ecf\u9a8c\u9a8c\u8bc1\u786e\u8ba4\u5176\u63a8\u8350\u914d\u7f6e\u5728\u4e0d\u540cQoS\u7ea6\u675f\u4e0b\u4e0e\u5b9e\u6d4b\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "QoSFlow\u901a\u8fc7\u914d\u7f6e\u7a7a\u95f4\u5206\u533a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u79d1\u5b66\u5de5\u4f5c\u6d41\u7684QoS\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u7684\u5206\u6790\u63a8\u7406\u6846\u67b6\u6765\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\u7ea6\u675f\u3002"}}
{"id": "2602.23758", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23758", "abs": "https://arxiv.org/abs/2602.23758", "authors": ["Dingyu Yang", "Fanyong Kong", "Jie Dai", "Shiyou Qian", "Shuangwei Li", "Jian Cao", "Guangtao Xue", "Gang Chen"], "title": "Hestia: Hyperthread-Level Scheduling for Cloud Microservices with Interference-Aware Attention", "comment": "This paper has been accepted for publication in Design Automation Conference(DAC 2026)", "summary": "Modern cloud servers routinely co-locate multiple latency-sensitive microservice instances to improve resource efficiency. However, the diversity of microservice behaviors, coupled with mutual performance interference under simultaneous multithreading (SMT), makes large-scale placement increasingly complex. Existing interference aware schedulers and isolation techniques rely on coarse core-level profiling or static resource partitioning, leaving asymmetric hyperthread-level heterogeneity and SMT contention dynamics largely unmodeled. We present Hestia, a hyperthread-level, interference-aware scheduling framework powered by self-attention. Through an extensive analysis of production traces encompassing 32,408 instances across 3,132 servers, we identify two dominant contention patterns -- sharing-core (SC) and sharing-socket (SS) -- and reveal strong asymmetry in their impact. Guided by these insights, Hestia incorporates (1) a self-attention-based CPU usage predictor that models SC/SS contention and hardware heterogeneity, and (2) an interference scoring model that estimates pairwise contention risks to guide scheduling decisions. We evaluate Hestia through large-scale simulation and a real production deployment. Hestia reduces the 95th-percentile service latency by up to 80\\%, lowers overall CPU consumption by 2.3\\% under the same workload, and surpasses five state-of-the-art schedulers by up to 30.65\\% across diverse contention scenarios.", "AI": {"tldr": "Hestia\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u7684\u8d85\u7ebf\u7a0b\u7ea7\u5e72\u6270\u611f\u77e5\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u5171\u4eab\u6838\u5fc3\u548c\u5171\u4eab\u63d2\u69fd\u4e24\u79cd\u4e3b\u8981\u4e89\u7528\u6a21\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u5fae\u670d\u52a1\u5ef6\u8fdf\u5e76\u63d0\u5347CPU\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u4e91\u670d\u52a1\u5668\u901a\u5e38\u5171\u7f6e\u591a\u4e2a\u5ef6\u8fdf\u654f\u611f\u7684\u5fae\u670d\u52a1\u5b9e\u4f8b\u4ee5\u63d0\u9ad8\u8d44\u6e90\u6548\u7387\uff0c\u4f46\u5fae\u670d\u52a1\u884c\u4e3a\u7684\u591a\u6837\u6027\u52a0\u4e0aSMT\u4e0b\u7684\u76f8\u4e92\u6027\u80fd\u5e72\u6270\uff0c\u4f7f\u5f97\u5927\u89c4\u6a21\u8c03\u5ea6\u53d8\u5f97\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u6838\u5fc3\u7ea7\u5206\u6790\u6216\u9759\u6001\u8d44\u6e90\u5206\u533a\uff0c\u672a\u80fd\u5145\u5206\u5efa\u6a21\u8d85\u7ebf\u7a0b\u7ea7\u5f02\u6784\u6027\u548cSMT\u4e89\u7528\u52a8\u6001\u3002", "method": "Hestia\u5305\u542b\uff1a(1) \u57fa\u4e8e\u81ea\u6ce8\u610f\u529b\u7684CPU\u4f7f\u7528\u7387\u9884\u6d4b\u5668\uff0c\u5efa\u6a21SC/SS\u4e89\u7528\u548c\u786c\u4ef6\u5f02\u6784\u6027\uff1b(2) \u5e72\u6270\u8bc4\u5206\u6a21\u578b\uff0c\u4f30\u8ba1\u6210\u5bf9\u4e89\u7528\u98ce\u9669\u4ee5\u6307\u5bfc\u8c03\u5ea6\u51b3\u7b56\u3002\u6846\u67b6\u57fa\u4e8e\u5bf932,408\u4e2a\u5b9e\u4f8b\u548c3,132\u53f0\u670d\u52a1\u5668\u7684\u751f\u4ea7\u8ddf\u8e2a\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u4e24\u79cd\u4e3b\u8981\u4e89\u7528\u6a21\u5f0f\u3002", "result": "Hestia\u5c06\u7b2c95\u767e\u5206\u4f4d\u670d\u52a1\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe80%\uff0c\u76f8\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u603b\u4f53CPU\u6d88\u8017\u964d\u4f4e2.3%\uff0c\u5728\u591a\u6837\u5316\u4e89\u7528\u573a\u666f\u4e2d\u8d85\u8d8a\u4e94\u79cd\u6700\u5148\u8fdb\u8c03\u5ea6\u5668\u8fbe30.65%\u3002", "conclusion": "Hestia\u901a\u8fc7\u8d85\u7ebf\u7a0b\u7ea7\u5e72\u6270\u611f\u77e5\u8c03\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u670d\u52a1\u5171\u7f6e\u4e2d\u7684SMT\u4e89\u7528\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e91\u670d\u52a1\u5668\u7684\u8d44\u6e90\u6548\u7387\u548c\u5fae\u670d\u52a1\u6027\u80fd\u3002"}}
{"id": "2602.23927", "categories": ["cs.DC", "cs.FL", "cs.MA", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23927", "abs": "https://arxiv.org/abs/2602.23927", "authors": ["Laura Bocchi", "Raymond Hu", "Adriana Laura Voinea", "Simon Thompson"], "title": "Mixed Choice in Asynchronous Multiparty Session Types", "comment": null, "summary": "We present a multiparty session type (MST) framework with asynchronous mixed choice (MC). We propose a core construct for MC that allows transient inconsistencies in protocol state between distributed participants, but ensures all participants can always eventually reach a mutually consistent state. We prove the correctness of our system by establishing a progress property and an operational correspondence between global types and distributed local type projections. Based on our theory, we implement a practical toolchain for specifying and validating asynchronous MST protocols featuring MC, and programming compliant gen_statem processes in Erlang/OTP. We test our framework by using our toolchain to specify and reimplement part of the amqp_client of the RabbitMQ broker for Erlang.", "AI": {"tldr": "\u63d0\u51fa\u652f\u6301\u5f02\u6b65\u6df7\u5408\u9009\u62e9\u7684\u591a\u65b9\u4f1a\u8bdd\u7c7b\u578b\u6846\u67b6\uff0c\u786e\u4fdd\u5206\u5e03\u5f0f\u53c2\u4e0e\u8005\u6700\u7ec8\u8fbe\u6210\u4e00\u81f4\u72b6\u6001\uff0c\u5e76\u5b9e\u73b0Erlang\u5de5\u5177\u94fe\u9a8c\u8bc1", "motivation": "\u73b0\u6709\u591a\u65b9\u4f1a\u8bdd\u7c7b\u578b\u6846\u67b6\u5728\u5904\u7406\u5f02\u6b65\u6df7\u5408\u9009\u62e9\u65f6\u5b58\u5728\u9650\u5236\uff0c\u9700\u8981\u652f\u6301\u5206\u5e03\u5f0f\u53c2\u4e0e\u8005\u95f4\u7684\u6682\u65f6\u4e0d\u4e00\u81f4\u4f46\u6700\u7ec8\u4e00\u81f4\u7684\u72b6\u6001\u7ba1\u7406", "method": "\u8bbe\u8ba1\u6838\u5fc3\u6784\u9020\u652f\u6301\u5f02\u6b65\u6df7\u5408\u9009\u62e9\uff0c\u5141\u8bb8\u534f\u8bae\u72b6\u6001\u7684\u6682\u65f6\u4e0d\u4e00\u81f4\u4f46\u786e\u4fdd\u6700\u7ec8\u4e00\u81f4\u6027\uff1b\u5efa\u7acb\u5168\u5c40\u7c7b\u578b\u4e0e\u5206\u5e03\u5f0f\u672c\u5730\u7c7b\u578b\u6295\u5f71\u7684\u64cd\u4f5c\u5bf9\u5e94\u5173\u7cfb\uff1b\u5b9e\u73b0Erlang/OTP\u5de5\u5177\u94fe\u7528\u4e8e\u534f\u8bae\u89c4\u8303\u548c\u9a8c\u8bc1", "result": "\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u6b63\u786e\u6027\uff08\u8fdb\u5c55\u5c5e\u6027\u548c\u64cd\u4f5c\u5bf9\u5e94\u6027\uff09\uff1b\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u94fe\uff1b\u6210\u529f\u5e94\u7528\u4e8eRabbitMQ\u7684amqp_client\u90e8\u5206\u91cd\u65b0\u5b9e\u73b0", "conclusion": "\u63d0\u51fa\u7684\u5f02\u6b65\u6df7\u5408\u9009\u62e9\u591a\u65b9\u4f1a\u8bdd\u7c7b\u578b\u6846\u67b6\u6709\u6548\u652f\u6301\u5206\u5e03\u5f0f\u534f\u8bae\u89c4\u8303\u4e0e\u9a8c\u8bc1\uff0c\u4e3aErlang/OTP\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4f1a\u8bdd\u7c7b\u578b\u5de5\u5177\u94fe"}}
{"id": "2602.23935", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.23935", "abs": "https://arxiv.org/abs/2602.23935", "authors": ["Bowen Sun", "Christos D. Antonopoulos", "Evgenia Smirni", "Bin Ren", "Nikolaos Bellas", "Spyros Lalis"], "title": "Green or Fast? Learning to Balance Cold Starts and Idle Carbon in Serverless Computing", "comment": null, "summary": "Serverless computing simplifies cloud deployment but introduces new challenges in managing service latency and carbon emissions. Reducing cold-start latency requires retaining warm function instances, while minimizing carbon emissions favors reclaiming idle resources. This balance is further complicated by time-varying grid carbon intensity and varying workload patterns, under which static keep-alive policies are inefficient. We present LACE-RL, a latency-aware and carbon-efficient management framework that formulates serverless pod retention as a sequential decision problem. LACE-RL uses deep reinforcement learning to dynamically tune keep-alive durations, jointly modeling cold-start probability, function-specific latency costs, and real-time carbon intensity. Using the Huawei Public Cloud Trace, we show that LACE-RL reduces cold starts by 51.69% and idle keep-alive carbon emissions by 77.08% compared to Huawei's static policy, while achieving better latency-carbon trade-offs than state-of-the-art heuristic and single-objective baselines, approaching Oracle performance.", "AI": {"tldr": "LACE-RL\uff1a\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u670d\u52a1\u5668\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7ba1\u7406\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u51fd\u6570\u5b9e\u4f8b\u4fdd\u6d3b\u65f6\u95f4\uff0c\u5728\u964d\u4f4e\u51b7\u542f\u52a8\u5ef6\u8fdf\u548c\u51cf\u5c11\u78b3\u6392\u653e\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u670d\u52a1\u5668\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7b80\u5316\u4e86\u4e91\u90e8\u7f72\uff0c\u4f46\u5e26\u6765\u4e86\u670d\u52a1\u5ef6\u8fdf\u548c\u78b3\u6392\u653e\u7ba1\u7406\u7684\u65b0\u6311\u6218\u3002\u51cf\u5c11\u51b7\u542f\u52a8\u5ef6\u8fdf\u9700\u8981\u4fdd\u7559\u70ed\u51fd\u6570\u5b9e\u4f8b\uff0c\u800c\u6700\u5c0f\u5316\u78b3\u6392\u653e\u5219\u503e\u5411\u4e8e\u56de\u6536\u7a7a\u95f2\u8d44\u6e90\u3002\u8fd9\u79cd\u5e73\u8861\u5728\u65f6\u53d8\u7684\u7535\u7f51\u78b3\u5f3a\u5ea6\u548c\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\u4e0b\u53d8\u5f97\u66f4\u52a0\u590d\u6742\uff0c\u9759\u6001\u4fdd\u6d3b\u7b56\u7565\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faLACE-RL\u6846\u67b6\uff0c\u5c06\u670d\u52a1\u5668\u65e0\u670d\u52a1\u5668Pod\u4fdd\u7559\u95ee\u9898\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u3002\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u4fdd\u6d3b\u6301\u7eed\u65f6\u95f4\uff0c\u8054\u5408\u5efa\u6a21\u51b7\u542f\u52a8\u6982\u7387\u3001\u51fd\u6570\u7279\u5b9a\u5ef6\u8fdf\u6210\u672c\u548c\u5b9e\u65f6\u78b3\u5f3a\u5ea6\u3002", "result": "\u57fa\u4e8e\u534e\u4e3a\u516c\u5171\u4e91\u8ffd\u8e2a\u6570\u636e\uff0cLACE-RL\u76f8\u6bd4\u534e\u4e3a\u9759\u6001\u7b56\u7565\u51cf\u5c1151.69%\u7684\u51b7\u542f\u52a8\u548c77.08%\u7684\u7a7a\u95f2\u4fdd\u6d3b\u78b3\u6392\u653e\uff0c\u5728\u5ef6\u8fdf-\u78b3\u6392\u653e\u6743\u8861\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u542f\u53d1\u5f0f\u548c\u5355\u76ee\u6807\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1Oracle\u6027\u80fd\u3002", "conclusion": "LACE-RL\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u670d\u52a1\u5668\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\u5ef6\u8fdf\u548c\u78b3\u6392\u653e\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u3001\u667a\u80fd\u7684\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\u3002"}}
{"id": "2602.23469", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.23469", "abs": "https://arxiv.org/abs/2602.23469", "authors": ["Lixi Zhou", "Kanchan Chowdhury", "Lulu Xie", "Jaykumar Tandel", "Hong Guan", "Zhiwei Fan", "Xinwei Fu", "Jia Zou"], "title": "CACTUSDB: Unlock Co-Optimization Opportunities for SQL and AI/ML Inferences", "comment": "Accepted to ICDE 2026 as a full research paper", "summary": "There is a growing demand for supporting inference queries that combine Structured Query Language (SQL) and Artificial Intelligence / Machine Learning (AI/ML) model inferences in database systems, to avoid data denormalization and transfer, facilitate management, and alleviate privacy concerns. Co-optimization techniques for executing inference queries in database systems without accuracy loss fall into four categories: (O1) Relational algebra optimization treating AI/ML models as black-box user-defined functions (UDFs); (O2) Factorized AI/ML inferences; (O3) Tensor-relational transformation; and (O4) General cross-optimization techniques. However, we found none of the existing database systems support all these techniques simultaneously, resulting in suboptimal performance. In this work, we identify two key challenges to address the above problem: (1) the difficulty of unifying all co-optimization techniques that involve disparate data and computation abstractions in one system; and (2) the lack of an optimizer that can effectively explore the exponential search space. To address these challenges, we present CactusDB, a novel system built atop Velox - a high-performance, UDF-centric database engine, open-sourced by Meta. CactusDB features a three-level Intermediate Representations (IR) that supports relational operators, expression operators, and ML functions to enable flexible optimization of arbitrary sub-computations. Additionally, we propose a novel Monte-Carlo Tree Search (MCTS)-based optimizer with query embedding, co-designed with our unique three-level IR, enabling shared and reusable optimization knowledge across different queries. Evaluation of 12 representative inference workloads and 2,000 randomly generated inference queries on well-known datasets, such as MovieLens and TPCx-AI, shows that CactusDB achieves up to 441 times speedup compared to alternative systems.", "AI": {"tldr": "CactusDB\u662f\u4e00\u4e2a\u65b0\u578b\u6570\u636e\u5e93\u7cfb\u7edf\uff0c\u652f\u6301SQL\u4e0eAI/ML\u6a21\u578b\u63a8\u7406\u7684\u8054\u5408\u67e5\u8be2\u4f18\u5316\uff0c\u901a\u8fc7\u4e09\u7ea7\u4e2d\u95f4\u8868\u793a\u548cMCTS\u4f18\u5316\u5668\u5b9e\u73b0\u9ad8\u8fbe441\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u5e93\u7cfb\u7edf\u9700\u8981\u652f\u6301SQL\u4e0eAI/ML\u6a21\u578b\u63a8\u7406\u7684\u8054\u5408\u67e5\u8be2\uff0c\u4ee5\u907f\u514d\u6570\u636e\u53bb\u89c4\u8303\u5316\u3001\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u3001\u4fbf\u4e8e\u7ba1\u7406\u548c\u7f13\u89e3\u9690\u79c1\u95ee\u9898\u3002\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u540c\u65f6\u652f\u6301\u6240\u6709\u56db\u79cd\u534f\u540c\u4f18\u5316\u6280\u672f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7406\u60f3\u3002", "method": "\u57fa\u4e8eMeta\u5f00\u6e90\u7684Velox\u9ad8\u6027\u80fdUDF\u4e2d\u5fc3\u5316\u6570\u636e\u5e93\u5f15\u64ce\uff0c\u6784\u5efaCactusDB\u7cfb\u7edf\uff0c\u91c7\u7528\u4e09\u7ea7\u4e2d\u95f4\u8868\u793a\uff08\u5173\u7cfb\u8fd0\u7b97\u7b26\u3001\u8868\u8fbe\u5f0f\u8fd0\u7b97\u7b26\u3001ML\u51fd\u6570\uff09\u652f\u6301\u7075\u6d3b\u4f18\u5316\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u67e5\u8be2\u5d4c\u5165\u6280\u672f\u5b9e\u73b0\u8de8\u67e5\u8be2\u7684\u4f18\u5316\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5728MovieLens\u548cTPCx-AI\u7b49\u77e5\u540d\u6570\u636e\u96c6\u4e0a\uff0c\u5bf912\u4e2a\u4ee3\u8868\u6027\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u548c2000\u4e2a\u968f\u673a\u751f\u6210\u7684\u63a8\u7406\u67e5\u8be2\u8fdb\u884c\u8bc4\u4f30\uff0cCactusDB\u76f8\u6bd4\u66ff\u4ee3\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u8fbe441\u500d\u7684\u52a0\u901f\u3002", "conclusion": "CactusDB\u6210\u529f\u89e3\u51b3\u4e86\u7edf\u4e00\u591a\u79cd\u534f\u540c\u4f18\u5316\u6280\u672f\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e09\u7ea7\u4e2d\u95f4\u8868\u793a\u548cMCTS\u4f18\u5316\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86SQL\u4e0eAI/ML\u6a21\u578b\u63a8\u7406\u8054\u5408\u67e5\u8be2\u7684\u6027\u80fd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.23647", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23647", "abs": "https://arxiv.org/abs/2602.23647", "authors": ["Quanjun Zhang", "Chengyu Gao", "Yu Han", "Ye Shang", "Chunrong Fang", "Zhenyu Chen", "Liang Xiao"], "title": "SGAgent: Suggestion-Guided LLM-Based Multi-Agent Framework for Repository-Level Software Repair", "comment": "23 pages, 3 figures", "summary": "The rapid advancement of Large Language Models (LLMs) has led to the emergence of intelligent agents capable of autonomously interacting with environments and invoking external tools. Recently, agent-based software repair approaches have received widespread attention, as repair agents can automatically analyze and localize bugs, generate patches, and achieve state-of-the-art performance on repository-level benchmarks. However, existing approaches usually adopt a localize-then-fix paradigm, jumping directly from \"where the bug is\" to \"how to fix it\", leaving a fundamental reasoning gap. To this end, we propose SGAgent, a Suggestion-Guided multi-Agent framework for repository-level software repair, which follows a localize-suggest-fix paradigm. SGAgent introduces a suggestion phase to strengthen the transition from localization to repair. The suggester starts from the buggy locations and incrementally retrieves relevant context until it fully understands the bug, and then provides actionable repair suggestions. Moreover, we construct a Knowledge Graph from the target repository and develop a KG-based toolkit to enhance SGAgent's global contextual awareness and repository-level reasoning. Three specialized sub-agents (i.e., localizer, suggester, and fixer) collaborate to achieve automated end-to-end software repair. Experimental results on SWE-Bench show that SGAgent with Claude-3.5 achieves 51.3% repair accuracy, 81.2% file-level and 52.4% function-level localization accuracy with an average cost of $1.48 per instance, outperforming all baselines using the same base model. Furthermore, SGAgent attains 48% accuracy on VUL4J and VJBench for vulnerability repair, demonstrating strong generalization across tasks and programming languages.", "AI": {"tldr": "SGAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5efa\u8bae\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u8f6f\u4ef6\u4fee\u590d\u6846\u67b6\uff0c\u91c7\u7528\u5b9a\u4f4d-\u5efa\u8bae-\u4fee\u590d\u8303\u5f0f\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u5728SWE-Bench\u4e0a\u8fbe\u523051.3%\u7684\u4fee\u590d\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8f6f\u4ef6\u4fee\u590d\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5b9a\u4f4d-\u4fee\u590d\u8303\u5f0f\uff0c\u76f4\u63a5\u4ece\"bug\u5728\u54ea\u91cc\"\u8df3\u5230\"\u5982\u4f55\u4fee\u590d\"\uff0c\u5b58\u5728\u6839\u672c\u6027\u7684\u63a8\u7406\u9e3f\u6c9f\u3002\u9700\u8981\u52a0\u5f3a\u4ece\u5b9a\u4f4d\u5230\u4fee\u590d\u7684\u8fc7\u6e21\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faSGAgent\u6846\u67b6\uff0c\u91c7\u7528\u5b9a\u4f4d-\u5efa\u8bae-\u4fee\u590d\u4e09\u9636\u6bb5\u8303\u5f0f\uff1a1) \u5b9a\u4f4d\u5668\u8bc6\u522bbug\u4f4d\u7f6e\uff1b2) \u5efa\u8bae\u5668\u4ecebug\u4f4d\u7f6e\u9010\u6b65\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u7406\u89e3bug\u672c\u8d28\u540e\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u4fee\u590d\u5efa\u8bae\uff1b3) \u4fee\u590d\u5668\u57fa\u4e8e\u5efa\u8bae\u751f\u6210\u8865\u4e01\u3002\u540c\u65f6\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u611f\u77e5\u3002", "result": "\u5728SWE-Bench\u4e0a\uff0cSGAgent\u4f7f\u7528Claude-3.5\u8fbe\u523051.3%\u7684\u4fee\u590d\u51c6\u786e\u7387\uff0c81.2%\u7684\u6587\u4ef6\u7ea7\u548c52.4%\u7684\u51fd\u6570\u7ea7\u5b9a\u4f4d\u51c6\u786e\u7387\uff0c\u5e73\u5747\u6bcf\u4e2a\u5b9e\u4f8b\u6210\u672c1.48\u7f8e\u5143\u3002\u5728VUL4J\u548cVJBench\u6f0f\u6d1e\u4fee\u590d\u4efb\u52a1\u4e0a\u8fbe\u523048%\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u548c\u7f16\u7a0b\u8bed\u8a00\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SGAgent\u901a\u8fc7\u5f15\u5165\u5efa\u8bae\u9636\u6bb5\u548c\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\uff0c\u6709\u6548\u5f25\u5408\u4e86\u5b9a\u4f4d\u4e0e\u4fee\u590d\u4e4b\u95f4\u7684\u63a8\u7406\u9e3f\u6c9f\uff0c\u5728\u8f6f\u4ef6\u4fee\u590d\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.24044", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24044", "abs": "https://arxiv.org/abs/2602.24044", "authors": ["Ferran Agullo", "Joan Oliveras", "Chen Wang", "Alberto Gutierrez-Torre", "Olivier Tardieu", "Alaa Youssef", "Jordi Torres", "Josep Ll. Berral"], "title": "Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving", "comment": "journal extension of the workshop paper titled as \"A data-driven ml approach for maximizing performance in llm-adapter serving\"", "summary": "Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u7cbe\u51c6\u6027\u80fd\u9884\u6d4b\u548c\u8d2a\u5fc3\u653e\u7f6e\u7b97\u6cd5\uff0c\u5728\u5206\u5e03\u5f0fLLM\u9002\u914d\u5668\u670d\u52a1\u4e2d\u6700\u5927\u5316GPU\u6548\u7387\uff0c\u51cf\u5c11\u6240\u9700GPU\u6570\u91cf", "motivation": "LLM\u9002\u914d\u5668\u867d\u7136\u80fd\u4f4e\u6210\u672c\u5b9e\u73b0\u6a21\u578b\u4e13\u4e1a\u5316\uff0c\u4f46\u5728\u5206\u5e03\u5f0f\u670d\u52a1\u7cfb\u7edf\u4e2d\u5f15\u5165\u590d\u6742\u7684\u7f13\u5b58\u548c\u8c03\u5ea6\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5ef6\u8fdf\u6700\u5c0f\u5316\uff0c\u800c\u901a\u8fc7\u541e\u5410\u91cf\u6700\u5927\u5316\u5b9e\u73b0\u8d44\u6e90\u6548\u7387\u7684\u7814\u7a76\u4e0d\u8db3", "method": "\u63d0\u51fa\u4e09\u7ec4\u4ef6\u6d41\u6c34\u7ebf\uff1a1) \u9488\u5bf9LLM\u9002\u914d\u5668\u670d\u52a1\u7684\u6570\u5b57\u5b6a\u751f(DT)\uff1b2) \u57fa\u4e8eDT\u751f\u6210\u6570\u636e\u8bad\u7ec3\u7684\u84b8\u998f\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1b3) \u5229\u7528ML\u6027\u80fd\u4f30\u8ba1\u7684\u8d2a\u5fc3\u653e\u7f6e\u7b97\u6cd5\u3002DT\u9ad8\u4fdd\u771f\u6a21\u62df\u771f\u5b9e\u7cfb\u7edf\u52a8\u6001\uff0cML\u6a21\u578b\u52a0\u901f\u6027\u80fd\u4f30\u8ba1", "result": "DT\u5b9e\u73b0\u4f4e\u4e8e5%\u7684\u541e\u5410\u91cf\u4f30\u8ba1\u8bef\u5dee\uff0c\u6267\u884c\u901f\u5ea6\u6bd4\u5b8c\u6574LLM\u57fa\u51c6\u6d4b\u8bd5\u5feb90\u500d\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u6d41\u6c34\u7ebf\u663e\u8457\u63d0\u9ad8GPU\u6548\u7387\uff0c\u51cf\u5c11\u7ef4\u6301\u76ee\u6807\u5de5\u4f5c\u8d1f\u8f7d\u6240\u9700\u7684GPU\u6570\u91cf", "conclusion": "\u8be5\u6d41\u6c34\u7ebf\u80fd\u6709\u6548\u4f18\u5316LLM\u9002\u914d\u5668\u670d\u52a1\u7684GPU\u6548\u7387\uff0c\u5e76\u53ef\u9002\u5e94\u5176\u4ed6\u76ee\u6807\u5982\u5ef6\u8fdf\u6700\u5c0f\u5316\uff0c\u5c55\u793a\u4e86\u672a\u6765\u5927\u89c4\u6a21LLM\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u7684\u901a\u7528\u6027"}}
{"id": "2602.23571", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.23571", "abs": "https://arxiv.org/abs/2602.23571", "authors": ["Quanqing Xu", "Mingqiang Zhuang", "Chuanhui Yang", "Quanwei Wan", "Fusheng Han", "Fanyu Kong", "Hao Liu", "Hu Xu", "Junyu Ye"], "title": "OceanBase Bacchus: a High-Performance and Scalable Cloud-Native Shared Storage Architecture for Multi-Cloud", "comment": null, "summary": "Although an increasing number of databases now embrace shared-storage architectures, current storage-disaggregated systems have yet to strike an optimal balance between cost and performance. In high-concurrency read/write scenarios, B+-tree-based shared storage struggles to efficiently absorb frequent in-place updates. Existing LSM-tree-backed disaggregated storage designs are hindered by the intricate implementation of cross-node shared-log mechanisms, where no satisfactory solution yet exists.\n  This paper presents OceanBase Bacchus, an LSM-tree architecture tailored for object storage provided by cloud vendors. The system sustains high-performance reads and writes while rendering compute nodes stateless through shared service-oriented PALF (Paxos-backed Append-only Log File system) logging and asynchronous background services. We employ a Shared Block Cache Service to flexibly utilize cache resources. Our design places log synchronization into a shared service, providing a novel solution for log sharing in storage-compute-separated databases. The architecture decouples functionality across modules, enabling elastic scaling where compute, cache, and storage resources can be resized rapidly and independently. Through experimental evaluation using multiple benchmark tests, including SysBench and TPC-H, we confirm that OceanBase Bacchus achieves performance comparable to or superior to that of HBase in OLTP scenarios and significantly outperforms StarRocks in OLAP workloads. Leveraging Bacchus's support for multi-cloud deployment and consistent performance, we not only retain high availability and competitive performance but also achieve substantial reductions in storage costs by 59% in OLTP scenarios and 89% in OLAP scenarios.", "AI": {"tldr": "OceanBase Bacchus\uff1a\u9488\u5bf9\u4e91\u5bf9\u8c61\u5b58\u50a8\u4f18\u5316\u7684LSM-tree\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u65e5\u5fd7\u670d\u52a1\u5b9e\u73b0\u5b58\u50a8\u8ba1\u7b97\u5206\u79bb\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u5b58\u50a8\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u5171\u4eab\u5b58\u50a8\u67b6\u6784\u5728\u6210\u672c\u4e0e\u6027\u80fd\u4e4b\u95f4\u5c1a\u672a\u8fbe\u5230\u6700\u4f18\u5e73\u8861\u3002B+-tree\u67b6\u6784\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u9ad8\u5e76\u53d1\u8bfb\u5199\u573a\u666f\u4e2d\u7684\u9891\u7e41\u539f\u5730\u66f4\u65b0\uff0c\u800c\u73b0\u6709LSM-tree\u5b58\u50a8\u5206\u79bb\u8bbe\u8ba1\u53c8\u53d7\u9650\u4e8e\u590d\u6742\u7684\u8de8\u8282\u70b9\u5171\u4eab\u65e5\u5fd7\u673a\u5236\u5b9e\u73b0\u3002", "method": "\u91c7\u7528LSM-tree\u67b6\u6784\u9002\u914d\u4e91\u5bf9\u8c61\u5b58\u50a8\uff0c\u901a\u8fc7\u5171\u4eab\u7684PALF\uff08\u57fa\u4e8ePaxos\u7684\u4ec5\u8ffd\u52a0\u65e5\u5fd7\u6587\u4ef6\u7cfb\u7edf\uff09\u65e5\u5fd7\u670d\u52a1\u548c\u5f02\u6b65\u540e\u53f0\u670d\u52a1\u4f7f\u8ba1\u7b97\u8282\u70b9\u65e0\u72b6\u6001\u5316\uff0c\u4f7f\u7528\u5171\u4eab\u5757\u7f13\u5b58\u670d\u52a1\u7075\u6d3b\u5229\u7528\u7f13\u5b58\u8d44\u6e90\uff0c\u5c06\u65e5\u5fd7\u540c\u6b65\u529f\u80fd\u653e\u5165\u5171\u4eab\u670d\u52a1\u4e2d\u3002", "result": "\u5728SysBench\u548cTPC-H\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOceanBase Bacchus\u5728OLTP\u573a\u666f\u4e0b\u6027\u80fd\u4e0eHBase\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5728OLAP\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8eStarRocks\u3002\u652f\u6301\u591a\u4e91\u90e8\u7f72\uff0c\u5728\u4fdd\u6301\u9ad8\u53ef\u7528\u6027\u548c\u7ade\u4e89\u529b\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0cOLTP\u573a\u666f\u5b58\u50a8\u6210\u672c\u964d\u4f4e59%\uff0cOLAP\u573a\u666f\u964d\u4f4e89%\u3002", "conclusion": "OceanBase Bacchus\u4e3a\u5b58\u50a8\u8ba1\u7b97\u5206\u79bb\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u65e5\u5fd7\u5171\u4eab\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u529f\u80fd\u89e3\u8026\u5b9e\u73b0\u5f39\u6027\u6269\u5c55\uff0c\u5728\u4e91\u5bf9\u8c61\u5b58\u50a8\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u6210\u672c\u4e0e\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.23736", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23736", "abs": "https://arxiv.org/abs/2602.23736", "authors": ["Ruixiang Qian", "Chunrong Fang", "Zengxu Chen", "Youxin Fu", "Zhenyu Chen"], "title": "Peeling Off the Cocoon: Unveiling Suppressed Golden Seeds for Mutational Greybox Fuzzing", "comment": "Accepted by OOPSLA 2026", "summary": "PoCo is a technique that aims to enhance modern coverage-based seed selection (CSS) techniques (such as afl-cmin) by gradually removing obstacle conditional statements and conducting deeper seed selection.", "AI": {"tldr": "PoCo\u901a\u8fc7\u9010\u6b65\u79fb\u9664\u969c\u788d\u6761\u4ef6\u8bed\u53e5\u5e76\u8fdb\u884c\u66f4\u6df1\u5165\u7684\u79cd\u5b50\u9009\u62e9\u6765\u589e\u5f3a\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u79cd\u5b50\u9009\u62e9\u6280\u672f", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u79cd\u5b50\u9009\u62e9\u6280\u672f\uff08\u5982afl-cmin\uff09\u5728\u5904\u7406\u590d\u6742\u6761\u4ef6\u8bed\u53e5\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u63a2\u7d22\u6df1\u5c42\u4ee3\u7801\u8def\u5f84", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff1a1) \u8bc6\u522b\u5e76\u79fb\u9664\u969c\u788d\u6761\u4ef6\u8bed\u53e5\uff1b2) \u8fdb\u884c\u66f4\u6df1\u5165\u7684\u79cd\u5b50\u9009\u62e9\uff1b3) \u9010\u6b65\u4f18\u5316\u79cd\u5b50\u96c6\u5408", "result": "PoCo\u80fd\u591f\u66f4\u6709\u6548\u5730\u9009\u62e9\u79cd\u5b50\uff0c\u63d0\u9ad8\u4ee3\u7801\u8986\u76d6\u7387\uff0c\u53d1\u73b0\u66f4\u591a\u6df1\u5c42\u6f0f\u6d1e", "conclusion": "PoCo\u6280\u672f\u663e\u8457\u6539\u8fdb\u4e86\u4f20\u7edfCSS\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u6761\u4ef6\u5206\u652f\u65f6\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.24237", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.24237", "abs": "https://arxiv.org/abs/2602.24237", "authors": ["Harinder Singh"], "title": "nvidia-pcm: A D-Bus-Driven Platform Configuration Manager for OpenBMC Environments", "comment": "7 pages, 1 figure, 10 references", "summary": "GPU-accelerated server platforms that share most of their hardware architecture often require separate firmware images due to minor hardware differences--different component identifiers, thermal profiles, or interconnect topologies. I built nvidia-pcm to eliminate that overhead. nvidia-pcm is a platform configuration manager for NVBMC, NVIDIA's OpenBMC-based firmware distribution, that enables a single firmware image to serve multiple platform variants. At boot, nvidia-pcm queries hardware identity data over D-Bus and exports the correct platform-specific configuration as environment variables. Downstream services read those variables without knowing or caring which hardware variant they are running on. The result is that platform differences are captured entirely in declarative JSON files, not in separate build artifacts. This paper describes the architecture, implementation, and deployment impact of nvidia-pcm, and shares lessons learned from solving the platform-identity problem at a deliberately minimal level of abstraction--prioritizing adoption simplicity over comprehensive hardware modeling.", "AI": {"tldr": "nvidia-pcm\u662f\u4e00\u4e2a\u5e73\u53f0\u914d\u7f6e\u7ba1\u7406\u5668\uff0c\u901a\u8fc7\u67e5\u8be2\u786c\u4ef6\u8eab\u4efd\u6570\u636e\u5e76\u5bfc\u51fa\u73af\u5883\u53d8\u91cf\uff0c\u4f7f\u5355\u4e2a\u56fa\u4ef6\u955c\u50cf\u80fd\u591f\u670d\u52a1\u591a\u4e2aGPU\u670d\u52a1\u5668\u5e73\u53f0\u53d8\u4f53\uff0c\u6d88\u9664\u4e86\u4e3a\u5fae\u5c0f\u786c\u4ef6\u5dee\u5f02\u7ef4\u62a4\u5355\u72ec\u56fa\u4ef6\u955c\u50cf\u7684\u5f00\u9500\u3002", "motivation": "GPU\u52a0\u901f\u670d\u52a1\u5668\u5e73\u53f0\u786c\u4ef6\u67b6\u6784\u76f8\u4f3c\u4f46\u5b58\u5728\u5fae\u5c0f\u5dee\u5f02\uff08\u5982\u7ec4\u4ef6\u6807\u8bc6\u7b26\u3001\u70ed\u914d\u7f6e\u6587\u4ef6\u3001\u4e92\u8fde\u62d3\u6251\uff09\uff0c\u8fd9\u5bfc\u81f4\u9700\u8981\u7ef4\u62a4\u591a\u4e2a\u5355\u72ec\u7684\u56fa\u4ef6\u955c\u50cf\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u548c\u7ef4\u62a4\u5f00\u9500\u3002", "method": "nvidia-pcm\u5728\u542f\u52a8\u65f6\u901a\u8fc7D-Bus\u67e5\u8be2\u786c\u4ef6\u8eab\u4efd\u6570\u636e\uff0c\u5c06\u6b63\u786e\u7684\u5e73\u53f0\u7279\u5b9a\u914d\u7f6e\u5bfc\u51fa\u4e3a\u73af\u5883\u53d8\u91cf\u3002\u4e0b\u6e38\u670d\u52a1\u8bfb\u53d6\u8fd9\u4e9b\u53d8\u91cf\u800c\u65e0\u9700\u5173\u5fc3\u5177\u4f53\u786c\u4ef6\u53d8\u4f53\u3002\u5e73\u53f0\u5dee\u5f02\u5b8c\u5168\u901a\u8fc7\u58f0\u660e\u5f0fJSON\u6587\u4ef6\u6355\u83b7\uff0c\u800c\u975e\u5355\u72ec\u7684\u6784\u5efa\u5de5\u4ef6\u3002", "result": "\u5b9e\u73b0\u4e86\u5355\u4e2a\u56fa\u4ef6\u955c\u50cf\u670d\u52a1\u591a\u4e2a\u5e73\u53f0\u53d8\u4f53\u7684\u80fd\u529b\uff0c\u5e73\u53f0\u5dee\u5f02\u5b8c\u5168\u901a\u8fc7JSON\u6587\u4ef6\u7ba1\u7406\uff0c\u7b80\u5316\u4e86\u56fa\u4ef6\u5206\u53d1\u548c\u7ef4\u62a4\uff0c\u63d0\u9ad8\u4e86\u5f00\u53d1\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u4ee5\u6700\u5c0f\u62bd\u8c61\u7ea7\u522b\u89e3\u51b3\u5e73\u53f0\u8eab\u4efd\u95ee\u9898\uff0c\u4f18\u5148\u8003\u8651\u91c7\u7528\u7b80\u5355\u6027\u800c\u975e\u5168\u9762\u7684\u786c\u4ef6\u5efa\u6a21\uff0cnvidia-pcm\u6210\u529f\u7b80\u5316\u4e86\u591a\u5e73\u53f0\u56fa\u4ef6\u7ba1\u7406\uff0c\u4e3a\u7c7b\u4f3c\u786c\u4ef6\u53d8\u4f53\u7ba1\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23999", "categories": ["cs.DB", "cs.DS", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23999", "abs": "https://arxiv.org/abs/2602.23999", "authors": ["Jifan Shi", "Jianyang Gao", "James Xia", "Tam\u00e1s B\u00e9la Feh\u00e9r", "Cheng Long"], "title": "GPU-Native Approximate Nearest Neighbor Search with IVF-RaBitQ: Fast Index Build and Search", "comment": null, "summary": "Approximate nearest neighbor search (ANNS) on GPUs is gaining increasing popularity for modern retrieval and recommendation workloads that operate over massive high-dimensional vectors. Graph-based indexes deliver high recall and throughput but incur heavy build-time and storage costs. In contrast, cluster-based methods build and scale efficiently yet often need many probes for high recall, straining memory bandwidth and compute. Aiming to simultaneously achieve fast index build, high-throughput search, high recall, and low storage requirement for GPUs, we present IVF-RaBitQ (GPU), a GPU-native ANNS solution that integrates the cluster-based method IVF with RaBitQ quantization into an efficient GPU index build/search pipeline. Specifically, for index build, we develop a scalable GPU-native RaBitQ quantization method that enables fast and accurate low-bit encoding at scale. For search, we develop GPU-native distance computation schemes for RaBitQ codes and a fused search kernel to achieve high throughput with high recall. With IVF-RaBitQ implemented and integrated into the NVIDIA cuVS Library, experiments on cuVS Bench across multiple datasets show that IVF-RaBitQ offers a strong performance frontier in recall, throughput, index build time, and storage footprint. For Recall approximately equal to 0.95, IVF-RaBitQ achieves 2.2x higher QPS than the state-of-the-art graph-based method CAGRA, while also constructing indices 7.7x faster on average. Compared to the cluster-based method IVF-PQ, IVF-RaBitQ delivers on average over 2.7x higher throughput while avoiding accessing the raw vectors for reranking.", "AI": {"tldr": "IVF-RaBitQ\uff1a\u4e00\u79cdGPU\u539f\u751f\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u65b9\u6848\uff0c\u7ed3\u5408IVF\u805a\u7c7b\u548cRaBitQ\u91cf\u5316\uff0c\u5728GPU\u4e0a\u5b9e\u73b0\u5feb\u901f\u7d22\u5f15\u6784\u5efa\u3001\u9ad8\u541e\u5410\u91cf\u641c\u7d22\u3001\u9ad8\u53ec\u56de\u7387\u548c\u4f4e\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u73b0\u6709GPU\u4e0a\u7684ANNS\u65b9\u6cd5\u5b58\u5728\u6743\u8861\uff1a\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u53ec\u56de\u7387\u9ad8\u4f46\u6784\u5efa\u65f6\u95f4\u548c\u5b58\u50a8\u6210\u672c\u9ad8\uff1b\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5\u6784\u5efa\u6548\u7387\u9ad8\u4f46\u9700\u8981\u591a\u6b21\u63a2\u6d4b\u624d\u80fd\u8fbe\u5230\u9ad8\u53ec\u56de\uff0c\u6d88\u8017\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u9700\u8981\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u7d22\u5f15\u6784\u5efa\u3001\u9ad8\u541e\u5410\u91cf\u641c\u7d22\u3001\u9ad8\u53ec\u56de\u7387\u548c\u4f4e\u5b58\u50a8\u9700\u6c42\u3002", "method": "1. \u5c06\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5IVF\u4e0eRaBitQ\u91cf\u5316\u96c6\u6210\u5230GPU\u539f\u751f\u7684\u7d22\u5f15\u6784\u5efa/\u641c\u7d22\u6d41\u6c34\u7ebf\u4e2d\uff1b2. \u5f00\u53d1\u53ef\u6269\u5c55\u7684GPU\u539f\u751fRaBitQ\u91cf\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u7684\u4f4e\u6bd4\u7279\u7f16\u7801\uff1b3. \u5f00\u53d1GPU\u539f\u751f\u7684RaBitQ\u7801\u8ddd\u79bb\u8ba1\u7b97\u65b9\u6848\u548c\u878d\u5408\u641c\u7d22\u5185\u6838\uff0c\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u548c\u9ad8\u53ec\u56de\u7387\u3002", "result": "\u5728cuVS Bench\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1aIVF-RaBitQ\u5728\u53ec\u56de\u7387\u3001\u541e\u5410\u91cf\u3001\u7d22\u5f15\u6784\u5efa\u65f6\u95f4\u548c\u5b58\u50a8\u5360\u7528\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u5728\u53ec\u56de\u7387\u7ea60.95\u65f6\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5CAGRA\u7684QPS\u9ad82.2\u500d\uff0c\u7d22\u5f15\u6784\u5efa\u901f\u5ea6\u5feb7.7\u500d\uff1b\u6bd4\u57fa\u4e8e\u805a\u7c7b\u7684\u65b9\u6cd5IVF-PQ\u541e\u5410\u91cf\u9ad82.7\u500d\u4ee5\u4e0a\uff0c\u4e14\u65e0\u9700\u8bbf\u95ee\u539f\u59cb\u5411\u91cf\u8fdb\u884c\u91cd\u6392\u5e8f\u3002", "conclusion": "IVF-RaBitQ\u4e3aGPU\u4e0a\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6027\u80fd\u524d\u6cbf\uff0c\u6210\u529f\u5e73\u8861\u4e86\u7d22\u5f15\u6784\u5efa\u901f\u5ea6\u3001\u641c\u7d22\u541e\u5410\u91cf\u3001\u53ec\u56de\u7387\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u5df2\u96c6\u6210\u5230NVIDIA cuVS\u5e93\u4e2d\u3002"}}
{"id": "2602.23866", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23866", "abs": "https://arxiv.org/abs/2602.23866", "authors": ["Ibragim Badertdinov", "Maksim Nekrashevich", "Anton Shevtsov", "Alexander Golubev"], "title": "SWE-rebench V2: Language-Agnostic SWE Task Collection at Scale", "comment": null, "summary": "Software engineering agents (SWE) are improving rapidly, with recent gains largely driven by reinforcement learning (RL). However, RL training is constrained by the scarcity of large-scale task collections with reproducible execution environments and reliable test suites. Although a growing number of benchmarks have emerged, datasets suitable for training remain limited in scale and diversity or often target a limited set of high-resource language ecosystems. We introduce SWE-rebench V2, a language-agnostic automated pipeline for harvesting executable real-world SWE tasks and constructing RL training environments at scale. The pipeline synthesizes repository-specific installation and test procedures via an interactive setup agent, and filters unsound instances using an ensemble of LLM judges, validated against human-verified SWE-bench annotations. Using this pipeline, we construct a dataset of 32,000+ tasks spanning 20 languages and 3,600+ repositories, with pre-built images for reproducible execution. To further scale training data, we additionally release 120,000+ tasks with installation instructions, fail-to-pass tests and rich metadata, where the problem statement is generated based on the original pull request description. We validate the collected instances through a diagnostic study that covers a subset of tasks in five programming languages across seven popular models, and provide instance-level metadata that flags common confounders such as overly restrictive tests and underspecified descriptions. We release the datasets, the collection and execution code, and associated artifacts to enable large-scale training of SWE agents across diverse languages and repositories.", "AI": {"tldr": "SWE-rebench V2\uff1a\u4e00\u4e2a\u8bed\u8a00\u65e0\u5173\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6536\u96c6\u53ef\u6267\u884c\u7684\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u5e76\u6784\u5efaRL\u8bad\u7ec3\u73af\u5883\uff0c\u5305\u542b32,000+\u4e2a\u4efb\u52a1\u548c120,000+\u4e2a\u6269\u5c55\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53d7\u9650\u4e8e\u5927\u89c4\u6a21\u4efb\u52a1\u96c6\u7684\u7a00\u7f3a\u6027\uff0c\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u3001\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u4e14\u4e3b\u8981\u9488\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u751f\u6001\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8bed\u8a00\u65e0\u5173\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u8bbe\u7f6e\u4ee3\u7406\u5408\u6210\u4ed3\u5e93\u7279\u5b9a\u7684\u5b89\u88c5\u548c\u6d4b\u8bd5\u7a0b\u5e8f\uff0c\u4f7f\u7528LLM\u8bc4\u59d4\u96c6\u5408\u8fc7\u6ee4\u65e0\u6548\u5b9e\u4f8b\uff0c\u5e76\u57fa\u4e8e\u4eba\u7c7b\u9a8c\u8bc1\u7684SWE-bench\u6807\u6ce8\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b32,000+\u4e2a\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d620\u79cd\u8bed\u8a00\u548c3,600+\u4e2a\u4ed3\u5e93\uff0c\u5e76\u63d0\u4f9b\u9884\u6784\u5efa\u955c\u50cf\u7528\u4e8e\u53ef\u91cd\u590d\u6267\u884c\uff1b\u989d\u5916\u53d1\u5e03\u4e86120,000+\u4e2a\u5e26\u6709\u5b89\u88c5\u8bf4\u660e\u3001\u6d4b\u8bd5\u548c\u5143\u6570\u636e\u7684\u4efb\u52a1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u6570\u636e\u96c6\u548c\u5de5\u5177\uff0c\u80fd\u591f\u652f\u6301\u8de8\u591a\u79cd\u8bed\u8a00\u548c\u4ed3\u5e93\u7684\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u3002"}}
{"id": "2602.24271", "categories": ["cs.DB", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.24271", "abs": "https://arxiv.org/abs/2602.24271", "authors": ["Boram Jung", "Yuliang Li", "Hung-Wei Tseng"], "title": "NSHEDB: Noise-Sensitive Homomorphic Encrypted Database Query Engine", "comment": null, "summary": "Homomorphic encryption (HE) enables computations directly on encrypted data, offering strong cryptographic guarantees for secure and privacy-preserving data storage and query execution. However, despite its theoretical power, practical adoption of HE in database systems remains limited due to extreme cipher-text expansion, memory overhead, and the computational cost of bootstrapping, which resets noise levels for correctness.\n  This paper presents NSHEDB, a secure query processing engine designed to address these challenges at the system architecture level. NSHEDB uses word-level leveled HE (LHE) based on the BFV scheme to minimize ciphertext expansion and avoid costly bootstrapping. It introduces novel techniques for executing equality, range, and aggregation operations using purely homomorphic computation, without transciphering between different HE schemes (e.g., CKKS/BFV/TFHE) or relying on trusted hardware. Additionally, it incorporates a noise-aware query planner to extend computation depth while preserving security guarantees.\n  We implement and evaluate NSHEDB on real-world database workloads (TPC-H) and show that it achieves 20x-V1370x speedup and a 73x storage reduction compared to state-of-the-art HE-based systems, while upholding 128-bit security in a semi-honest model with no key release or trusted components.", "AI": {"tldr": "NSHEDB\u662f\u4e00\u4e2a\u57fa\u4e8e\u540c\u6001\u52a0\u5bc6\u7684\u5b89\u5168\u67e5\u8be2\u5904\u7406\u5f15\u64ce\uff0c\u91c7\u7528BFV\u65b9\u6848\u7684\u8bcd\u7ea7\u5206\u5c42\u540c\u6001\u52a0\u5bc6\uff0c\u901a\u8fc7\u7eaf\u540c\u6001\u8ba1\u7b97\u652f\u6301\u7b49\u5f0f\u3001\u8303\u56f4\u548c\u805a\u5408\u64cd\u4f5c\uff0c\u65e0\u9700\u65b9\u6848\u8f6c\u6362\u6216\u53ef\u4fe1\u786c\u4ef6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u7cfb\u7edf20-1370\u500d\u52a0\u901f\u548c73\u500d\u5b58\u50a8\u51cf\u5c11\u3002", "motivation": "\u540c\u6001\u52a0\u5bc6\uff08HE\uff09\u867d\u7136\u7406\u8bba\u4e0a\u5f3a\u5927\uff0c\u4f46\u5728\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u5bc6\u6587\u6269\u5c55\u4e25\u91cd\u3001\u5185\u5b58\u5f00\u9500\u5927\u4ee5\u53ca\u91cd\u7f6e\u566a\u58f0\u7684\u81ea\u4e3e\u64cd\u4f5c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eBFV\u65b9\u6848\u7684\u8bcd\u7ea7\u5206\u5c42\u540c\u6001\u52a0\u5bc6\uff08LHE\uff09\u6700\u5c0f\u5316\u5bc6\u6587\u6269\u5c55\u5e76\u907f\u514d\u6602\u8d35\u7684\u81ea\u4e3e\u64cd\u4f5c\uff1b\u5f15\u5165\u7eaf\u540c\u6001\u8ba1\u7b97\u6280\u672f\u6267\u884c\u7b49\u5f0f\u3001\u8303\u56f4\u548c\u805a\u5408\u64cd\u4f5c\uff0c\u65e0\u9700\u5728\u4e0d\u540cHE\u65b9\u6848\u95f4\u8f6c\u6362\u6216\u4f9d\u8d56\u53ef\u4fe1\u786c\u4ef6\uff1b\u91c7\u7528\u566a\u58f0\u611f\u77e5\u67e5\u8be2\u89c4\u5212\u5668\u6269\u5c55\u8ba1\u7b97\u6df1\u5ea6\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u5e93\u5de5\u4f5c\u8d1f\u8f7d\uff08TPC-H\uff09\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684HE\u7cfb\u7edf\uff0cNSHEDB\u5b9e\u73b0\u4e8620-1370\u500d\u52a0\u901f\u548c73\u500d\u5b58\u50a8\u51cf\u5c11\uff0c\u540c\u65f6\u5728\u534a\u8bda\u5b9e\u6a21\u578b\u4e2d\u4fdd\u6301128\u4f4d\u5b89\u5168\u6027\uff0c\u65e0\u9700\u5bc6\u94a5\u91ca\u653e\u6216\u53ef\u4fe1\u7ec4\u4ef6\u3002", "conclusion": "NSHEDB\u901a\u8fc7\u7cfb\u7edf\u67b6\u6784\u5c42\u9762\u7684\u521b\u65b0\uff0c\u89e3\u51b3\u4e86\u540c\u6001\u52a0\u5bc6\u5728\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u969c\u788d\uff0c\u4e3a\u5b89\u5168\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u5b58\u50a8\u548c\u67e5\u8be2\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23905", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23905", "abs": "https://arxiv.org/abs/2602.23905", "authors": ["Syed Ammar Asdaque", "Imran Haider", "Muhammad Umar Malik", "Maryam Abdul Ghafoor", "Abdul Ali Bangash"], "title": "Novice Developers Produce Larger Review Overhead for Project Maintainers while Vibe Coding", "comment": "Accepted to MSR 2026 Mining Challenge", "summary": "AI coding agents allow software developers to generate code quickly, which raises a practical question for project managers and open source maintainers: can vibe coders with less development experience substitute for expert developers? To explore whether developer experience still matters in AI-assisted development, we study $22,953$ Pull Requests (PRs) from $1,719$ vibe coders in the GitHub repositories of the AIDev dataset. We split vibe coders into lower experience vibe coders ($\\mathit{Exp}_{Low}$) and higher experience vibe coders ($\\mathit{Exp}_{High}$) and compare contribution magnitude and PR acceptance rates across PR categories. We find that $\\mathit{Exp}_{Low}$ submits PRs with larger volume ($2.15\\times$ more commits and $1.47\\times$ more files changed) than $\\mathit{Exp}_{High}$. Moreover, $\\mathit{Exp}_{Low}$ PRs, when compared to $\\mathit{Exp}_{High}$, receive $4.52\\times$ more review comments, and have $31\\%$ lower acceptance rates, and remain open $5.16\\times$ longer before resolution. Our results indicate that low-experienced vibe coders focus on generating more code while shifting verification burden onto reviewers. For practice, project managers may not be able to safely replace experienced developers with low-experience vibe coders without increasing review capacity. Development teams should therefore combine targeted training for novices with adaptive PR review cycles.", "AI": {"tldr": "\u7814\u7a76AI\u8f85\u52a9\u5f00\u53d1\u4e2d\u5f00\u53d1\u8005\u7ecf\u9a8c\u662f\u5426\u4ecd\u91cd\u8981\uff0c\u53d1\u73b0\u4f4e\u7ecf\u9a8c\u5f00\u53d1\u8005\u63d0\u4ea4\u66f4\u5927\u4ee3\u7801\u91cf\u4f46\u63a5\u53d7\u7387\u66f4\u4f4e\u3001\u5ba1\u67e5\u8d1f\u62c5\u66f4\u91cd\uff0c\u7ecf\u9a8c\u5f00\u53d1\u8005\u65e0\u6cd5\u88ab\u4f4e\u7ecf\u9a8cAI\u5f00\u53d1\u8005\u5b89\u5168\u66ff\u4ee3", "motivation": "\u63a2\u7a76\u5728AI\u7f16\u7801\u4ee3\u7406\u5feb\u901f\u751f\u6210\u4ee3\u7801\u7684\u65f6\u4ee3\uff0c\u9879\u76ee\u7ba1\u7406\u8005\u80fd\u5426\u7528\u7ecf\u9a8c\u8f83\u5c11\u7684AI\u5f00\u53d1\u8005\u66ff\u4ee3\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u53d1\u8005\uff0c\u9a8c\u8bc1\u5f00\u53d1\u8005\u7ecf\u9a8c\u5728AI\u8f85\u52a9\u5f00\u53d1\u4e2d\u662f\u5426\u4ecd\u7136\u91cd\u8981", "method": "\u5206\u6790AIDev\u6570\u636e\u96c6\u4e2d22,953\u4e2aPR\uff0c\u5c061,719\u540dAI\u5f00\u53d1\u8005\u5206\u4e3a\u4f4e\u7ecf\u9a8c\u548c\u9ad8\u7ecf\u9a8c\u4e24\u7ec4\uff0c\u6bd4\u8f83\u8d21\u732e\u91cf\u3001PR\u63a5\u53d7\u7387\u3001\u5ba1\u67e5\u8bc4\u8bba\u6570\u3001\u89e3\u51b3\u65f6\u95f4\u7b49\u6307\u6807", "result": "\u4f4e\u7ecf\u9a8c\u5f00\u53d1\u8005\u63d0\u4ea4PR\u5305\u542b2.15\u500d\u66f4\u591a\u63d0\u4ea4\u548c1.47\u500d\u66f4\u591a\u6587\u4ef6\u66f4\u6539\uff0c\u6536\u52304.52\u500d\u66f4\u591a\u5ba1\u67e5\u8bc4\u8bba\uff0c\u63a5\u53d7\u7387\u4f4e31%\uff0c\u89e3\u51b3\u65f6\u95f4\u957f5.16\u500d", "conclusion": "\u4f4e\u7ecf\u9a8cAI\u5f00\u53d1\u8005\u4e13\u6ce8\u4e8e\u751f\u6210\u66f4\u591a\u4ee3\u7801\u4f46\u5c06\u9a8c\u8bc1\u8d1f\u62c5\u8f6c\u79fb\u7ed9\u5ba1\u67e5\u8005\uff0c\u65e0\u6cd5\u5b89\u5168\u66ff\u4ee3\u7ecf\u9a8c\u5f00\u53d1\u8005\uff0c\u9700\u7ed3\u5408\u65b0\u624b\u57f9\u8bad\u548c\u81ea\u9002\u5e94PR\u5ba1\u67e5\u5468\u671f"}}
{"id": "2602.23922", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23922", "abs": "https://arxiv.org/abs/2602.23922", "authors": ["Ana Catarina Ribeiro"], "title": "Invariant-Driven Automated Testing", "comment": null, "summary": "Microservice architectures are an emergent technology that builds business logic into a suite of small services. Each microservice runs in its process and the communication is made through lightweight mechanisms, usually HTTP resource API. These architectures are built upon independently deployable and, supposedly, reliable pieces of software that may, or may not, have been developed by the team using it. Nowadays, industries are dangerously migrating into microservice architectures without an effective and automatic process for testing the software being used. Furthermore, current API specification languages are not expressive enough to be used for testing purposes. To solve this problem it is necessary to extend currently broadly used API specification languages. APOSTL is a specification language to annotate APIs specifications based on first-order logic, with some restrictions. It has the purpose of extending the currently used API description languages with properties that can be useful for testing purposes, transforming these description documents into useful testing artefacts. Besides providing information needed for testing an application, APOSTL also provides an API with semantic. This additional information is then leveraged to automate microservice testing. The work developed in this thesis aims to fully automate the microservice testing process. It is achieved by the implementation of PETIT a tool able to test microservices when provided with an OpenAPI Specification document, written in JSON and properly annotated with the previously proposed specification language, APOSTL. The tool is able to analyze microservices independently from the source code availability.", "AI": {"tldr": "APOSTL\u662f\u4e00\u79cd\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u7684API\u89c4\u8303\u8bed\u8a00\uff0c\u7528\u4e8e\u6269\u5c55\u73b0\u6709API\u63cf\u8ff0\u8bed\u8a00\u4ee5\u652f\u6301\u6d4b\u8bd5\u76ee\u7684\uff0c\u7ed3\u5408PETIT\u5de5\u5177\u5b9e\u73b0\u5fae\u670d\u52a1\u6d4b\u8bd5\u81ea\u52a8\u5316", "motivation": "\u5f53\u524d\u5fae\u670d\u52a1\u67b6\u6784\u8fc1\u79fb\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u73b0\u6709API\u89c4\u8303\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u4ee5\u652f\u6301\u6d4b\u8bd5\u9700\u6c42\uff0c\u9700\u8981\u6269\u5c55API\u89c4\u8303\u8bed\u8a00\u6765\u521b\u5efa\u6709\u7528\u7684\u6d4b\u8bd5\u5de5\u4ef6", "method": "\u63d0\u51faAPOSTL\u89c4\u8303\u8bed\u8a00\uff0c\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u6269\u5c55\u73b0\u6709API\u63cf\u8ff0\u8bed\u8a00\uff0c\u6dfb\u52a0\u53ef\u7528\u4e8e\u6d4b\u8bd5\u7684\u5c5e\u6027\uff1b\u5f00\u53d1PETIT\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790OpenAPI\u89c4\u8303\u6587\u6863\uff08\u7528APOSTL\u6ce8\u89e3\uff09\u6765\u81ea\u52a8\u5316\u6d4b\u8bd5\u5fae\u670d\u52a1", "result": "APOSTL\u80fd\u591f\u4e3aAPI\u89c4\u8303\u6dfb\u52a0\u8bed\u4e49\u4fe1\u606f\uff0cPETIT\u5de5\u5177\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u6e90\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u5206\u6790\u5fae\u670d\u52a1\uff0c\u5b9e\u73b0\u5fae\u670d\u52a1\u6d4b\u8bd5\u8fc7\u7a0b\u7684\u5b8c\u5168\u81ea\u52a8\u5316", "conclusion": "\u901a\u8fc7APOSTL\u6269\u5c55API\u89c4\u8303\u8bed\u8a00\u5e76\u7ed3\u5408PETIT\u5de5\u5177\uff0c\u53ef\u4ee5\u89e3\u51b3\u5fae\u670d\u52a1\u6d4b\u8bd5\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u4e3a\u884c\u4e1a\u63d0\u4f9b\u6709\u6548\u7684\u5fae\u670d\u52a1\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23957", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23957", "abs": "https://arxiv.org/abs/2602.23957", "authors": ["Alexander Berndt", "Zolt\u00e1n Nochta", "Thomas Bach"], "title": "The Vocabulary of Flaky Tests in the Context of SAP HANA", "comment": "Accepted to ESEM IGC 2023", "summary": "Background. Automated test execution is an important activity to gather information about the quality of a software project. So-called flaky tests, however, negatively affect this process. Such tests fail seemingly at random without changes to the code and thus do not provide a clear signal. Previous work proposed to identify flaky tests based on the source code identifiers in the test code. So far, these approaches have not been evaluated in a large-scale industrial setting. Aims. We evaluate approaches to identify flaky tests and their root causes based on source code identifiers in the test code in a large-scale industrial project. Method. First, we replicate previous work by Pinto et al. in the context of SAP HANA. Second, we assess different feature extraction techniques, namely TF-IDF and TF-IDFC-RF. Third, we evaluate CodeBERT and XGBoost as classification models. For a sound comparison, we utilize both the data set from previous work and two data sets from SAP HANA. Results. Our replication shows similar results on the original data set and on one of the SAP HANA data sets. While the original approach yielded an F1-Score of 0.94 on the original data set and 0.92 on the SAP HANA data set, our extensions achieve F1-Scores of 0.96 and 0.99, respectively. The reliance on external data sources is a common root cause for test flakiness in the context of SAP HANA. Conclusions. The vocabulary of a large industrial project seems to be slightly different with respect to the exact terms, but the categories for the terms, such as remote dependencies, are similar to previous empirical findings. However, even with rather large F1-Scores, both finding source code identifiers for flakiness and a black box prediction have limited use in practice as the results are not actionable for developers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5728\u5927\u578b\u5de5\u4e1a\u9879\u76eeSAP HANA\u4e2d\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6d4b\u8bd5\u4ee3\u7801\u4e2d\u6e90\u4ee3\u7801\u6807\u8bc6\u7b26\u8bc6\u522b\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u7684\u65b9\u6cd5\uff0c\u590d\u5236\u4e86\u5148\u524d\u7814\u7a76\u5e76\u6269\u5c55\u4e86\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684F1\u5206\u6570\uff0c\u4f46\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u4e2d\u5bf9\u5f00\u53d1\u8005\u7684\u53ef\u64cd\u4f5c\u6027\u6709\u9650\u3002", "motivation": "\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u4f1a\u968f\u673a\u5931\u8d25\uff0c\u5f71\u54cd\u8f6f\u4ef6\u8d28\u91cf\u8bc4\u4f30\u3002\u5148\u524d\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u6d4b\u8bd5\u4ee3\u7801\u4e2d\u7684\u6e90\u4ee3\u7801\u6807\u8bc6\u7b26\u8bc6\u522b\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5c1a\u672a\u5728\u5927\u578b\u5de5\u4e1a\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u5728SAP HANA\u8fd9\u4e00\u5927\u578b\u5de5\u4e1a\u9879\u76ee\u4e2d\u8bc4\u4f30\u8fd9\u4e9b\u65b9\u6cd5\u53ca\u5176\u8bc6\u522b\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u6839\u672c\u539f\u56e0\u7684\u80fd\u529b\u3002", "method": "\u9996\u5148\u5728SAP HANA\u73af\u5883\u4e2d\u590d\u5236Pinto\u7b49\u4eba\u7684\u5148\u524d\u7814\u7a76\uff1b\u5176\u6b21\u8bc4\u4f30\u4e0d\u540c\u7684\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff08TF-IDF\u548cTF-IDFC-RF\uff09\uff1b\u7b2c\u4e09\u8bc4\u4f30CodeBERT\u548cXGBoost\u4f5c\u4e3a\u5206\u7c7b\u6a21\u578b\uff1b\u4f7f\u7528\u5148\u524d\u7814\u7a76\u7684\u6570\u636e\u96c6\u548c\u4e24\u4e2aSAP HANA\u6570\u636e\u96c6\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u590d\u5236\u7814\u7a76\u5728\u539f\u59cb\u6570\u636e\u96c6\u548c\u4e00\u4e2aSAP HANA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u76f8\u4f3c\u7ed3\u679c\uff08\u539f\u59cb\u65b9\u6cd5F1\u5206\u6570\u5206\u522b\u4e3a0.94\u548c0.92\uff09\u3002\u6269\u5c55\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\uff08F1\u5206\u6570\u5206\u522b\u4e3a0.96\u548c0.99\uff09\u3002\u53d1\u73b0\u5916\u90e8\u6570\u636e\u6e90\u4f9d\u8d56\u662fSAP HANA\u4e2d\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u7684\u5e38\u89c1\u6839\u672c\u539f\u56e0\u3002", "conclusion": "\u5927\u578b\u5de5\u4e1a\u9879\u76ee\u7684\u8bcd\u6c47\u5728\u5177\u4f53\u672f\u8bed\u4e0a\u7565\u6709\u4e0d\u540c\uff0c\u4f46\u672f\u8bed\u7c7b\u522b\uff08\u5982\u8fdc\u7a0b\u4f9d\u8d56\uff09\u4e0e\u5148\u524d\u5b9e\u8bc1\u53d1\u73b0\u76f8\u4f3c\u3002\u5c3d\u7ba1\u83b7\u5f97\u4e86\u8f83\u9ad8\u7684F1\u5206\u6570\uff0c\u4f46\u57fa\u4e8e\u6e90\u4ee3\u7801\u6807\u8bc6\u7b26\u8bc6\u522b\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u548c\u9ed1\u76d2\u9884\u6d4b\u5728\u5b9e\u9645\u4e2d\u5e94\u7528\u6709\u9650\uff0c\u56e0\u4e3a\u7ed3\u679c\u5bf9\u5f00\u53d1\u8005\u4e0d\u591f\u53ef\u64cd\u4f5c\u3002"}}
{"id": "2602.24108", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.24108", "abs": "https://arxiv.org/abs/2602.24108", "authors": ["Yakun Zhang", "Zihan Wang", "Xinzhi Peng", "Zihao Xie", "Xiaodong Wang", "Xutao Li", "Dan Hao", "Lu Zhang", "Yunming Ye"], "title": "Context-Aware Functional Test Generation via Business Logic Extraction and Adaptation", "comment": null, "summary": "Functional testing is essential for verifying that the business logic of mobile applications aligns with user requirements, serving as the primary methodology for quality assurance in software development. Despite its importance, functional testing remains heavily dependent on manual effort due to two core challenges. First, acquiring and reusing complex business logic from unstructured requirements remains difficult, which hinders the understanding of specific functionalities. Second, a significant semantic gap exists when adapting business logic to the diverse GUI environments, which hinders the generation of test cases for specific mobile applications. To address the preceding challenges, we propose LogiDroid, a two-stage approach that generates individual functional test cases by extracting business logic and adapting it to target applications. First, in the Knowledge Retrieval and Fusion stage, we construct a dataset to retrieve relevant cases and extract business logic for the target functionality. Second, in the Context-Aware Test Generation stage, LogiDroid jointly analyzes the extracted business logic and the real-time GUI environment to generate functional test cases. This design allows LogiDroid to accurately understand application semantics and use domain expertise to generate complete test cases with verification assertions. We assess the effectiveness of LogiDroid using two widely-used datasets that cover 28 real-world applications and 190 functional requirements. Experimental results show that LogiDroid successfully tested 40% of functional requirements on the FrUITeR dataset (an improvement of over 48% compared to the state-of-the-art approaches) and 65% on the Lin dataset (an improvement of over 55% compared to the state-of-the-art approaches). These results demonstrate the significant effectiveness of LogiDroid in functional test generation.", "AI": {"tldr": "LogiDroid\uff1a\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u4e1a\u52a1\u903b\u8f91\u5e76\u5c06\u5176\u9002\u914d\u5230\u76ee\u6807\u5e94\u7528\u6765\u751f\u6210\u79fb\u52a8\u5e94\u7528\u529f\u80fd\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u79fb\u52a8\u5e94\u7528\u529f\u80fd\u6d4b\u8bd5\u5bf9\u9a8c\u8bc1\u4e1a\u52a1\u903b\u8f91\u7b26\u5408\u7528\u6237\u9700\u6c42\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4ecd\u4e25\u91cd\u4f9d\u8d56\u4eba\u5de5\u3002\u4e3b\u8981\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1) \u4ece\u975e\u7ed3\u6784\u5316\u9700\u6c42\u4e2d\u83b7\u53d6\u548c\u590d\u7528\u590d\u6742\u4e1a\u52a1\u903b\u8f91\u56f0\u96be\uff1b2) \u5c06\u4e1a\u52a1\u903b\u8f91\u9002\u914d\u5230\u591a\u6837\u5316GUI\u73af\u5883\u65f6\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faLogiDroid\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u77e5\u8bc6\u68c0\u7d22\u4e0e\u878d\u5408\u9636\u6bb5\uff1a\u6784\u5efa\u6570\u636e\u96c6\u68c0\u7d22\u76f8\u5173\u6848\u4f8b\u5e76\u63d0\u53d6\u76ee\u6807\u529f\u80fd\u7684\u4e1a\u52a1\u903b\u8f91\uff1b2) \u4e0a\u4e0b\u6587\u611f\u77e5\u6d4b\u8bd5\u751f\u6210\u9636\u6bb5\uff1a\u8054\u5408\u5206\u6790\u63d0\u53d6\u7684\u4e1a\u52a1\u903b\u8f91\u548c\u5b9e\u65f6GUI\u73af\u5883\u751f\u6210\u529f\u80fd\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5305\u542b\u9a8c\u8bc1\u65ad\u8a00\u3002", "result": "\u5728\u8986\u76d628\u4e2a\u771f\u5b9e\u5e94\u7528\u548c190\u4e2a\u529f\u80fd\u9700\u6c42\u7684\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLogiDroid\u5728FrUITeR\u6570\u636e\u96c6\u4e0a\u6210\u529f\u6d4b\u8bd5\u4e8640%\u7684\u529f\u80fd\u9700\u6c42\uff08\u6bd4SOTA\u63d0\u534748%\u4ee5\u4e0a\uff09\uff0c\u5728Lin\u6570\u636e\u96c6\u4e0a\u6210\u529f\u6d4b\u8bd5\u4e8665%\uff08\u6bd4SOTA\u63d0\u534755%\u4ee5\u4e0a\uff09\u3002", "conclusion": "LogiDroid\u901a\u8fc7\u51c6\u786e\u7406\u89e3\u5e94\u7528\u8bed\u4e49\u5e76\u5229\u7528\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u751f\u6210\u5b8c\u6574\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5728\u529f\u80fd\u6d4b\u8bd5\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6709\u6548\u6027\uff0c\u89e3\u51b3\u4e86\u4e1a\u52a1\u903b\u8f91\u63d0\u53d6\u548cGUI\u9002\u914d\u7684\u6311\u6218\u3002"}}
