{"id": "2602.17734", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17734", "abs": "https://arxiv.org/abs/2602.17734", "authors": ["Raja Soundaramourty", "Ozkan Kilic", "Ramu Chenchaiah"], "title": "Five Fatal Assumptions: Why T-Shirt Sizing Systematically Fails for AI Projects", "comment": null, "summary": "Agile estimation techniques, particularly T-shirt sizing, are widely used in software development for their simplicity and utility in scoping work. However, when we apply these methods to artificial intelligence initiatives -- especially those involving large language models (LLMs) and multi-agent systems -- the results can be systematically misleading. This paper shares an evidence-backed analysis of five foundational assumptions we often make during T-shirt sizing. While these assumptions usually hold true for traditional software, they tend to fail in AI contexts: (1) linear effort scaling, (2) repeatability from prior experience, (3) effort-duration fungibility, (4) task decomposability, and (5) deterministic completion criteria. Drawing on recent research into multi-agent system failures, scaling principles, and the inherent unreliability of multi-turn conversations, we show how AI development breaks these rules. We see this through non-linear performance jumps, complex interaction surfaces, and \"tight coupling\" where a small change in data cascades through the entire stack. To help teams navigate this, we propose Checkpoint Sizing: a more human-centric, iterative approach that uses explicit decision gates where scope and feasibility are reassessed based on what we learn during development, rather than what we assumed at the start. This paper is intended for engineering managers, technical leads, and product owners responsible for planning and delivering AI initiatives.", "AI": {"tldr": "T-shirt sizing\u7b49\u654f\u6377\u4f30\u7b97\u65b9\u6cd5\u5728\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u6709\u6548\uff0c\u4f46\u5728AI\u9879\u76ee\uff08\u7279\u522b\u662fLLM\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u4e2d\u4f1a\u7cfb\u7edf\u6027\u5730\u8bef\u5bfc\uff0c\u56e0\u4e3aAI\u5f00\u53d1\u6253\u7834\u4e86\u4f20\u7edf\u4f30\u7b97\u7684\u4e94\u4e2a\u57fa\u672c\u5047\u8bbe\u3002", "motivation": "AI\u9879\u76ee\uff08\u7279\u522b\u662fLLM\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u4f7f\u7528\u4f20\u7edf\u7684T-shirt sizing\u7b49\u654f\u6377\u4f30\u7b97\u65b9\u6cd5\u65f6\uff0c\u7ed3\u679c\u4f1a\u7cfb\u7edf\u6027\u5730\u8bef\u5bfc\uff0c\u56e0\u4e3aAI\u5f00\u53d1\u7684\u7279\u70b9\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u4e0d\u540c\uff0c\u9700\u8981\u66f4\u5408\u9002\u7684\u4f30\u7b97\u65b9\u6cd5\u3002", "method": "\u5206\u6790T-shirt sizing\u7684\u4e94\u4e2a\u57fa\u672c\u5047\u8bbe\u5728AI\u5f00\u53d1\u4e2d\u7684\u5931\u6548\u60c5\u51b5\uff0c\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5931\u8d25\u3001\u6269\u5c55\u539f\u5219\u548c\u591a\u8f6e\u5bf9\u8bdd\u56fa\u6709\u4e0d\u53ef\u9760\u6027\u7684\u7814\u7a76\uff0c\u63d0\u51faCheckpoint Sizing\u65b9\u6cd5\u2014\u2014\u4e00\u79cd\u66f4\u4ee5\u4eba\u4e3a\u672c\u3001\u8fed\u4ee3\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u660e\u786e\u7684\u51b3\u7b56\u95e8\u6765\u91cd\u65b0\u8bc4\u4f30\u8303\u56f4\u548c\u53ef\u884c\u6027\u3002", "result": "AI\u5f00\u53d1\u6253\u7834\u4e86\u4f20\u7edf\u4f30\u7b97\u7684\u4e94\u4e2a\u5047\u8bbe\uff1a(1)\u7ebf\u6027\u52aa\u529b\u6269\u5c55\uff0c(2)\u5148\u524d\u7ecf\u9a8c\u7684\u53ef\u91cd\u590d\u6027\uff0c(3)\u52aa\u529b-\u6301\u7eed\u65f6\u95f4\u53ef\u4e92\u6362\u6027\uff0c(4)\u4efb\u52a1\u53ef\u5206\u89e3\u6027\uff0c(5)\u786e\u5b9a\u6027\u5b8c\u6210\u6807\u51c6\u3002\u8fd9\u8868\u73b0\u4e3a\u975e\u7ebf\u6027\u6027\u80fd\u8df3\u8dc3\u3001\u590d\u6742\u4ea4\u4e92\u8868\u9762\u548c\"\u7d27\u8026\u5408\"\u73b0\u8c61\u3002", "conclusion": "\u63d0\u51faCheckpoint Sizing\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u662f\u4e00\u79cd\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5728\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u57fa\u4e8e\u5b9e\u9645\u5b66\u4e60\u800c\u975e\u521d\u59cb\u5047\u8bbe\u91cd\u65b0\u8bc4\u4f30\u8303\u56f4\u548c\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u8d1f\u8d23AI\u9879\u76ee\u89c4\u5212\u548c\u4ea4\u4ed8\u7684\u5de5\u7a0b\u7ecf\u7406\u3001\u6280\u672f\u8d1f\u8d23\u4eba\u548c\u4ea7\u54c1\u8d1f\u8d23\u4eba\u3002"}}
{"id": "2602.17838", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17838", "abs": "https://arxiv.org/abs/2602.17838", "authors": ["Lara Khatib", "Micheal Pu", "Bogdan Vasilescu", "Meiyappan Nagappan"], "title": "Examining LLMs Ability to Summarize Code Through Mutation-Analysis", "comment": null, "summary": "As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as \"bugs\", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5f02\u6d4b\u8bd5\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLM\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u662f\u5426\u51c6\u786e\u53cd\u6620\u7a0b\u5e8f\u5b9e\u9645\u884c\u4e3a\uff0c\u800c\u975e\u8868\u9762\u610f\u56fe\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u5171624\u6b21\u8bc4\u4f30\u53d1\u73b0\uff0c\u968f\u7740\u4ee3\u7801\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u6458\u8981\u51c6\u786e\u6027\u663e\u8457\u4e0b\u964d\uff0c\u4e14LLM\u503e\u5411\u4e8e\u63cf\u8ff0\u7b97\u6cd5\u610f\u56fe\u800c\u975e\u5b9e\u9645\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u5f00\u53d1\u8005\u8d8a\u6765\u8d8a\u4f9d\u8d56LLM\u751f\u6210\u7684\u4ee3\u7801\u6458\u8981\u8fdb\u884c\u6587\u6863\u3001\u6d4b\u8bd5\u548c\u5ba1\u67e5\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e9b\u6458\u8981\u662f\u5426\u51c6\u786e\u53cd\u6620\u7a0b\u5e8f\u7684\u5b9e\u9645\u884c\u4e3a\u3002LLM\u7ecf\u5e38\u81ea\u4fe1\u5730\u63cf\u8ff0\u4ee3\u7801\u770b\u8d77\u6765\u5e94\u8be5\u505a\u4ec0\u4e48\uff08\u610f\u56fe\uff09\uff0c\u800c\u5ffd\u7565\u5b9a\u4e49\u5b9e\u9645\u884c\u4e3a\u7684\u7ec6\u5fae\u8fb9\u754c\u60c5\u51b5\u6216\u903b\u8f91\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u53d8\u5f02\u7684\u8bc4\u4f30\u65b9\u6cd5\uff1a\u751f\u6210\u4ee3\u7801\u6458\u8981\uff0c\u5728\u4ee3\u7801\u4e2d\u6ce8\u5165\u9488\u5bf9\u6027\u53d8\u5f02\uff0c\u68c0\u67e5LLM\u662f\u5426\u66f4\u65b0\u5176\u6458\u8981\u4ee5\u53cd\u6620\u65b0\u884c\u4e3a\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\uff1a1\uff0912\u4e2a\u53d7\u63a7\u5408\u6210\u7a0b\u5e8f\u7684324\u4e2a\u53d8\u5f02\uff1b2\uff0950\u4e2a\u4eba\u5de5\u7f16\u5199\u7a0b\u5e8f\u7684150\u4e2a\u53d8\u5f02\u6837\u672c\uff1b3\uff09\u6bd4\u8f83GPT-4\u548cGPT-5.2\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u6458\u8981\u51c6\u786e\u6027\u968f\u590d\u6742\u5ea6\u6025\u5267\u4e0b\u964d\uff1a\u5355\u51fd\u657076.5% vs \u591a\u7ebf\u7a0b\u7cfb\u7edf17.3%\u3002\u5728\u4eba\u5de5\u7f16\u5199\u7a0b\u5e8f\u4e2d\uff0c\u6458\u8981\u51c6\u786e\u7387\u4e3a49.3%\u3002GPT-5.2\u76f8\u6bd4GPT-4\u6709\u663e\u8457\u63d0\u5347\uff0849.3%\u523085.3%\uff09\uff0c\u4f46\u4e24\u8005\u4ecd\u96be\u4ee5\u533a\u5206\u5b9e\u73b0\u7ec6\u8282\u4e0e\u6807\u51c6\u7b97\u6cd5\u6a21\u5f0f\u3002\u53d8\u5f02\u7c7b\u578b\u548c\u4f4d\u7f6e\u5f71\u54cd\u8f83\u5f31\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u786e\u7acb\u4e86\u53d8\u5f02\u5206\u6790\u4f5c\u4e3a\u8bc4\u4f30LLM\u751f\u6210\u6458\u8981\u662f\u5426\u53cd\u6620\u7a0b\u5e8f\u884c\u4e3a\u800c\u975e\u8868\u9762\u6587\u672c\u6a21\u5f0f\u7684\u7cfb\u7edf\u65b9\u6cd5\u3002\u5373\u4f7f\u6700\u65b0\u6a21\u578b\u5728\u8bc6\u522b\u53d8\u5f02\u4e3a\"bug\"\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u4ecd\u9700\u63d0\u5347\u5bf9\u5b9e\u73b0\u7ec6\u8282\u4e0e\u7b97\u6cd5\u6a21\u5f0f\u7684\u533a\u5206\u80fd\u529b\u3002"}}
{"id": "2602.17887", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17887", "abs": "https://arxiv.org/abs/2602.17887", "authors": ["Carla Fern\u00e1ndez-Navarro", "Francisco Chicano"], "title": "Automated LLM-Based Accessibility Remediation: From Conventional Websites to Angular Single-Page Applications", "comment": null, "summary": "Web accessibility remains an unresolved issue for a large part of the web content. There are many tools to detect errors automatically, but fixing those issues is still mostly a manual, slow, and costly process in which it is easy for developers to overlook specific details. The situation becomes even more complex with modern Single-Page Applications (SPAs), whose dynamic nature makes traditional static analysis approaches inadequate. This work proposes a system that aims to address this challenge by using Large Language Models (LLMs) to automate accessibility fixes. The proposal presents a modular workflow applicable to both static websites and complex Angular projects. The framework actively implements corrections within the DOM of static web pages or the source code of SPAs. The system was tested on 12 static websites and 6 open-source Angular projects, fixing 80% of the accessibility issues on public websites and 86% of the issues on Angular applications. Our proposal also generates meaningful visual descriptions for images while preserving the application's design and stability. This work contributes to ensuring that accessibility stops being a technical debt deferred to the future and becomes a natural part of everyday development workflows.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4fee\u590d\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u652f\u6301\u9759\u6001\u7f51\u7ad9\u548cAngular\u5355\u9875\u5e94\u7528\uff0c\u80fd\u4fee\u590d80-86%\u7684\u95ee\u9898\u5e76\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\u3002", "motivation": "\u7f51\u9875\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u73b0\u6709\u5de5\u5177\u53ea\u80fd\u68c0\u6d4b\u4f46\u4fee\u590d\u4ecd\u9700\u4eba\u5de5\uff0c\u8fc7\u7a0b\u7f13\u6162\u4e14\u6613\u51fa\u9519\u3002\u7279\u522b\u662f\u5355\u9875\u5e94\u7528(SPA)\u7684\u52a8\u6001\u7279\u6027\u4f7f\u4f20\u7edf\u9759\u6001\u5206\u6790\u65b9\u6cd5\u5931\u6548\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u6a21\u5757\u5316\u5de5\u4f5c\u6d41\uff0c\u53ef\u5e94\u7528\u4e8e\u9759\u6001\u7f51\u7ad9\u548cAngular\u9879\u76ee\u3002\u7cfb\u7edf\u76f4\u63a5\u5728\u9759\u6001\u7f51\u9875\u7684DOM\u6216SPA\u7684\u6e90\u4ee3\u7801\u4e2d\u5b9e\u65bd\u4fee\u6b63\uff0c\u540c\u65f6\u751f\u6210\u6709\u610f\u4e49\u7684\u56fe\u50cf\u63cf\u8ff0\u5e76\u4fdd\u6301\u5e94\u7528\u8bbe\u8ba1\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u572812\u4e2a\u9759\u6001\u7f51\u7ad9\u548c6\u4e2a\u5f00\u6e90Angular\u9879\u76ee\u4e0a\u6d4b\u8bd5\uff0c\u4fee\u590d\u4e8680%\u7684\u516c\u5171\u7f51\u7ad9\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\u548c86%\u7684Angular\u5e94\u7528\u95ee\u9898\u3002\u7cfb\u7edf\u80fd\u6709\u6548\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\uff0c\u540c\u65f6\u4fdd\u6301\u5e94\u7528\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u52a9\u4e8e\u5c06\u53ef\u8bbf\u95ee\u6027\u4ece\u6280\u672f\u503a\u52a1\u8f6c\u53d8\u4e3a\u65e5\u5e38\u5f00\u53d1\u6d41\u7a0b\u7684\u81ea\u7136\u7ec4\u6210\u90e8\u5206\uff0c\u4e3a\u9759\u6001\u7f51\u7ad9\u548c\u590d\u6742SPA\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u4fee\u590d\u65b9\u6848\u3002"}}
{"id": "2602.17955", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17955", "abs": "https://arxiv.org/abs/2602.17955", "authors": ["Imgyeong Lee", "Tayyib Ul Hassan", "Abram Hindle"], "title": "Mining Type Constructs Using Patterns in AI-Generated Code", "comment": null, "summary": "Artificial Intelligence (AI) increasingly automates various parts of the software development tasks. Although AI has enhanced the productivity of development tasks, it remains unstudied whether AI essentially outperforms humans in type-related programming tasks, such as employing type constructs properly for type safety, during its tasks. Moreover, there is no systematic study that evaluates whether AI agents overuse or misuse the type constructs under the complicated type systems to the same extent as humans. In this study, we present the first empirical analysis to answer these questions in the domain of TypeScript projects. Our findings show that, in contrast to humans, AI agents are 9x more prone to use the 'any' keyword. In addition, we observed that AI agents use advanced type constructs, including those that ignore type checks, more often compared to humans. Surprisingly, even with all these issues, Agentic pull requests (PRs) have 1.8x higher acceptance rates compared to humans for TypeScript. We encourage software developers to carefully confirm the type safety of their codebases whenever they coordinate with AI agents in the development process.", "AI": {"tldr": "AI\u4ee3\u7406\u5728TypeScript\u9879\u76ee\u4e2d\u6bd4\u4eba\u7c7b\u66f4\u9891\u7e41\u4f7f\u7528'any'\u5173\u952e\u5b57\u548c\u5ffd\u7565\u7c7b\u578b\u68c0\u67e5\u7684\u9ad8\u7ea7\u7c7b\u578b\u6784\u9020\uff0c\u4f46AI\u63d0\u4ea4\u7684PR\u63a5\u53d7\u7387\u5374\u662f\u4eba\u7c7b\u76841.8\u500d", "motivation": "\u5c3d\u7ba1AI\u63d0\u5347\u4e86\u8f6f\u4ef6\u5f00\u53d1\u6548\u7387\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695aAI\u5728\u7c7b\u578b\u76f8\u5173\u7f16\u7a0b\u4efb\u52a1\uff08\u5982\u6b63\u786e\u4f7f\u7528\u7c7b\u578b\u6784\u9020\u786e\u4fdd\u7c7b\u578b\u5b89\u5168\uff09\u4e0a\u662f\u5426\u4f18\u4e8e\u4eba\u7c7b\uff0c\u4e5f\u6ca1\u6709\u7cfb\u7edf\u7814\u7a76\u8bc4\u4f30AI\u4ee3\u7406\u5728\u590d\u6742\u7c7b\u578b\u7cfb\u7edf\u4e2d\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u8fc7\u5ea6\u4f7f\u7528\u6216\u8bef\u7528\u7c7b\u578b\u6784\u9020", "method": "\u5728TypeScript\u9879\u76ee\u9886\u57df\u8fdb\u884c\u9996\u6b21\u5b9e\u8bc1\u5206\u6790\uff0c\u6bd4\u8f83AI\u4ee3\u7406\u548c\u4eba\u7c7b\u5728\u7c7b\u578b\u6784\u9020\u4f7f\u7528\u4e0a\u7684\u5dee\u5f02", "result": "AI\u4ee3\u7406\u4f7f\u7528'any'\u5173\u952e\u5b57\u7684\u503e\u5411\u662f\u4eba\u7c7b\u76849\u500d\uff1bAI\u4ee3\u7406\u66f4\u9891\u7e41\u4f7f\u7528\u5ffd\u7565\u7c7b\u578b\u68c0\u67e5\u7684\u9ad8\u7ea7\u7c7b\u578b\u6784\u9020\uff1b\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5c3d\u7ba1\u5b58\u5728\u8fd9\u4e9b\u95ee\u9898\uff0cAI\u4ee3\u7406\u63d0\u4ea4\u7684PR\u63a5\u53d7\u7387\u6bd4\u4eba\u7c7b\u9ad81.8\u500d", "conclusion": "\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728\u4e0eAI\u4ee3\u7406\u534f\u4f5c\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u5e94\u4ed4\u7ec6\u786e\u8ba4\u4ee3\u7801\u5e93\u7684\u7c7b\u578b\u5b89\u5168\u6027"}}
{"id": "2602.17675", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17675", "abs": "https://arxiv.org/abs/2602.17675", "authors": ["Takao Morita"], "title": "Mind the Boundary: Stabilizing Gemini Enterprise A2A via a Cloud Run Hub Across Projects and Accounts", "comment": "7 pages. Implementation and evaluation study of cross-boundary agent orchestration for Gemini Enterprise UI", "summary": "Enterprise conversational UIs increasingly need to orchestrate heterogeneous backend agents and tools across project and account boundaries in a secure and reproducible way. Starting from Gemini Enterprise Agent-to-Agent (A2A) invocation, we implement an A2A Hub orchestrator on Cloud Run that routes queries to four paths: a public A2A agent deployed in a different project, an IAM-protected Cloud Run A2A agent in a different account, a retrieval-augmented generation path combining Discovery Engine and Vertex AI Search with direct retrieval of source text from Google Cloud Storage, and a general question answering path via Vertex AI. We show that practical interoperability is governed not only by protocol compliance but also by Gemini Enterprise UI constraints and boundary-dependent authentication. Real UI requests arrive as text-only inputs and include empty accepted output mode lists, so mixing structured data into JSON-RPC responses can trigger UI errors. To address this, we enforce a text-only compatibility mode on the JSON-RPC endpoint while separating structured outputs and debugging signals into a REST tool API. On a four-query benchmark spanning expense policy, project management assistance, general knowledge, and incident response deadline extraction, we confirm deterministic routing and stable UI responses. For the retrieval path, granting storage object read permissions enables evidence-backed extraction of the fifteen minute deadline. All experiments are reproducible using the repository snapshot tagged a2a-hub-gemini-ui-stable-paper.", "AI": {"tldr": "\u8bba\u6587\u5b9e\u73b0\u4e86\u4e00\u4e2aA2A Hub\u7f16\u6392\u5668\uff0c\u7528\u4e8e\u5728\u8de8\u9879\u76ee\u548c\u8d26\u6237\u8fb9\u754c\u7684\u5b89\u5168\u53ef\u590d\u73b0\u73af\u5883\u4e2d\u534f\u8c03\u5f02\u6784\u540e\u7aef\u4ee3\u7406\u548c\u5de5\u5177\uff0c\u89e3\u51b3\u4e86Gemini Enterprise UI\u7ea6\u675f\u548c\u8eab\u4efd\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u4f01\u4e1a\u5bf9\u8bddUI\u9700\u8981\u5728\u8de8\u9879\u76ee\u548c\u8d26\u6237\u8fb9\u754c\u7684\u5b89\u5168\u53ef\u590d\u73b0\u65b9\u5f0f\u4e0b\u534f\u8c03\u5f02\u6784\u540e\u7aef\u4ee3\u7406\u548c\u5de5\u5177\uff0c\u4f46\u5b9e\u9645\u4e92\u64cd\u4f5c\u6027\u53d7\u5230Gemini Enterprise UI\u7ea6\u675f\u548c\u8fb9\u754c\u76f8\u5173\u8eab\u4efd\u9a8c\u8bc1\u7684\u9650\u5236\u3002", "method": "\u5728Cloud Run\u4e0a\u5b9e\u73b0A2A Hub\u7f16\u6392\u5668\uff0c\u5c06\u67e5\u8be2\u8def\u7531\u5230\u56db\u4e2a\u8def\u5f84\uff1a\u8de8\u9879\u76ee\u7684\u516c\u5171A2A\u4ee3\u7406\u3001\u8de8\u8d26\u6237\u7684IAM\u4fdd\u62a4Cloud Run A2A\u4ee3\u7406\u3001\u7ed3\u5408Discovery Engine\u548cVertex AI Search\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8def\u5f84\uff0c\u4ee5\u53ca\u901a\u8fc7Vertex AI\u7684\u901a\u7528\u95ee\u7b54\u8def\u5f84\u3002\u4e3a\u5904\u7406UI\u7ea6\u675f\uff0c\u5728JSON-RPC\u7aef\u70b9\u5f3a\u5236\u6267\u884c\u7eaf\u6587\u672c\u517c\u5bb9\u6a21\u5f0f\uff0c\u540c\u65f6\u5c06\u7ed3\u6784\u5316\u8f93\u51fa\u548c\u8c03\u8bd5\u4fe1\u53f7\u5206\u79bb\u5230REST\u5de5\u5177API\u3002", "result": "\u5728\u6db5\u76d6\u8d39\u7528\u653f\u7b56\u3001\u9879\u76ee\u7ba1\u7406\u534f\u52a9\u3001\u901a\u7528\u77e5\u8bc6\u548c\u4e8b\u4ef6\u54cd\u5e94\u622a\u6b62\u65f6\u95f4\u63d0\u53d6\u7684\u56db\u67e5\u8be2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u786e\u8ba4\u4e86\u786e\u5b9a\u6027\u8def\u7531\u548c\u7a33\u5b9a\u7684UI\u54cd\u5e94\u3002\u5bf9\u4e8e\u68c0\u7d22\u8def\u5f84\uff0c\u6388\u4e88\u5b58\u50a8\u5bf9\u8c61\u8bfb\u53d6\u6743\u9650\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8e\u8bc1\u636e\u768415\u5206\u949f\u622a\u6b62\u65f6\u95f4\u63d0\u53d6\u3002\u6240\u6709\u5b9e\u9a8c\u5747\u53ef\u901a\u8fc7\u5b58\u50a8\u5e93\u5feb\u7167\u590d\u73b0\u3002", "conclusion": "\u5b9e\u9645\u7684\u4f01\u4e1a\u4ee3\u7406\u95f4\u4e92\u64cd\u4f5c\u6027\u4e0d\u4ec5\u9700\u8981\u534f\u8bae\u5408\u89c4\u6027\uff0c\u8fd8\u5fc5\u987b\u8003\u8651UI\u7ea6\u675f\u548c\u8fb9\u754c\u76f8\u5173\u8eab\u4efd\u9a8c\u8bc1\u3002\u901a\u8fc7\u5206\u79bb\u7eaf\u6587\u672cUI\u54cd\u5e94\u548c\u7ed3\u6784\u5316\u6570\u636e\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301UI\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u590d\u6742\u7f16\u6392\u529f\u80fd\u3002"}}
{"id": "2602.17858", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.17858", "abs": "https://arxiv.org/abs/2602.17858", "authors": ["Thinh On", "Senjuti Basu Roy", "Baruch Schieber"], "title": "Multi-Attribute Group Fairness in $k$-NN Queries on Vector Databases", "comment": null, "summary": "We initiate the study of multi-attribute group fairness in $k$-nearest neighbor ($k$-NN) search over vector databases. Unlike prior work that optimizes efficiency or query filtering, fairness imposes count constraints to ensure proportional representation across groups defined by protected attributes. When fairness spans multiple attributes, these constraints must be satisfied simultaneously, making the problem computationally hard. To address this, we propose a computational framework that produces high-quality approximate nearest neighbors with good trade-offs between search time, memory/indexing cost, and recall. We adapt locality-sensitive hashing (LSH) to accelerate candidate generation and build a lightweight index over the Cartesian product of protected attribute values. Our framework retrieves candidates satisfying joint count constraints and then applies a post-processing stage to construct fair $k$-NN results across all attributes. For 2 attributes, we present an exact polynomial-time flow-based algorithm; for 3 or more, we formulate ILP-based exact solutions with higher computational cost. We provide theoretical guarantees, identify efficiency--fairness trade-offs, and empirically show that existing vector search methods cannot be directly adapted for fairness. Experimental evaluations demonstrate the generality of the proposed framework and scalability.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u591a\u5c5e\u6027\u7fa4\u4f53\u516c\u5e73\u6027k\u8fd1\u90bb\u641c\u7d22\u6846\u67b6\uff0c\u89e3\u51b3\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u8de8\u591a\u4e2a\u53d7\u4fdd\u62a4\u5c5e\u6027\u7684\u6bd4\u4f8b\u8868\u793a\u95ee\u9898\uff0c\u901a\u8fc7LSH\u52a0\u901f\u548c\u8f7b\u91cf\u7ea7\u7d22\u5f15\u5b9e\u73b0\u6548\u7387\u4e0e\u516c\u5e73\u6027\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u5411\u91cf\u641c\u7d22\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6548\u7387\u6216\u67e5\u8be2\u8fc7\u6ee4\uff0c\u7f3a\u4e4f\u5bf9\u591a\u5c5e\u6027\u7fa4\u4f53\u516c\u5e73\u6027\u7684\u8003\u8651\u3002\u5f53\u516c\u5e73\u6027\u8981\u6c42\u8de8\u591a\u4e2a\u53d7\u4fdd\u62a4\u5c5e\u6027\u65f6\uff0c\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u6bd4\u4f8b\u8868\u793a\u7ea6\u675f\uff0c\u8fd9\u5e26\u6765\u4e86\u8ba1\u7b97\u4e0a\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u8ba1\u7b97\u6846\u67b6\uff1a1) \u4f7f\u7528\u5c40\u90e8\u654f\u611f\u54c8\u5e0c(LSH)\u52a0\u901f\u5019\u9009\u751f\u6210\uff1b2) \u5728\u53d7\u4fdd\u62a4\u5c5e\u6027\u503c\u7684\u7b1b\u5361\u5c14\u79ef\u4e0a\u6784\u5efa\u8f7b\u91cf\u7ea7\u7d22\u5f15\uff1b3) \u68c0\u7d22\u6ee1\u8db3\u8054\u5408\u8ba1\u6570\u7ea6\u675f\u7684\u5019\u9009\uff1b4) \u540e\u5904\u7406\u9636\u6bb5\u6784\u5efa\u516c\u5e73\u7684k-NN\u7ed3\u679c\u3002\u9488\u5bf92\u5c5e\u6027\u63d0\u51fa\u57fa\u4e8e\u6d41\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7cbe\u786e\u7b97\u6cd5\uff0c\u9488\u5bf93+\u5c5e\u6027\u63d0\u51fa\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212(ILP)\u7684\u7cbe\u786e\u89e3\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff1a1) \u73b0\u6709\u5411\u91cf\u641c\u7d22\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u9002\u5e94\u516c\u5e73\u6027\u8981\u6c42\uff1b2) \u63d0\u51fa\u7684\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff1b3) \u5728\u641c\u7d22\u65f6\u95f4\u3001\u5185\u5b58/\u7d22\u5f15\u6210\u672c\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u6743\u8861\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u7814\u7a76\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u591a\u5c5e\u6027\u7fa4\u4f53\u516c\u5e73\u6027k-NN\u641c\u7d22\u7684\u5de5\u4f5c\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6548\u7387\u4e0e\u516c\u5e73\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u516c\u5e73\u6027\u611f\u77e5\u7684\u5411\u91cf\u641c\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6848\u3002"}}
{"id": "2602.18012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18012", "abs": "https://arxiv.org/abs/2602.18012", "authors": ["Pragati Kumari", "Novarun Deb"], "title": "DeCEAT: Decoding Carbon Emissions for AI-driven Software Testing", "comment": null, "summary": "The increasing use of language models in automated software testing raises concerns about their environmental impact, yet existing sustainability analyses focus almost exclusively on large language models. As a result, the energy and carbon characteristics of small language models (SLMs) during test generation remain largely unexplored. To address this gap, this work introduces the DeCEAT framework, which systematically evaluates the environmental and performance trade-offs of SLMs using the HumanEval benchmark and adaptive prompt variants (based on the Anthropic template). The framework quantifies emission and time-aware behavior under controlled conditions, with CodeCarbon measuring energy consumption and carbon emissions, and unit test coverage assessing the quality of generated tests. Our results show that different SLMs exhibit distinct sustainability strengths: some prioritize lower energy use and faster execution, while others maintain higher stability or accuracy under carbon constraints. These findings demonstrate that sustainability in the generation of SLM-driven tests is multidimensional and strongly shaped by prompt design. This work provides a focused sustainability evaluation framework specifically tailored to automated SLM-based test generation, clarifying how prompt structure and model choice jointly influence environmental and performance outcomes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDeCEAT\u6846\u67b6\uff0c\u4e13\u95e8\u8bc4\u4f30\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u4e2d\u7684\u73af\u5883\u53ef\u6301\u7eed\u6027\u548c\u6027\u80fd\u6743\u8861\uff0c\u53d1\u73b0\u4e0d\u540cSLM\u5728\u80fd\u8017\u3001\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5404\u6709\u4f18\u52bf\uff0c\u4e14\u63d0\u793a\u8bbe\u8ba1\u5bf9\u7ed3\u679c\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u7684\u53ef\u6301\u7eed\u6027\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\u7279\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faDeCEAT\u6846\u67b6\uff0c\u4f7f\u7528HumanEval\u57fa\u51c6\u548c\u57fa\u4e8eAnthropic\u6a21\u677f\u7684\u81ea\u9002\u5e94\u63d0\u793a\u53d8\u4f53\uff0c\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u7cfb\u7edf\u8bc4\u4f30SLM\u7684\u73af\u5883\u548c\u6027\u80fd\u6743\u8861\u3002\u4f7f\u7528CodeCarbon\u6d4b\u91cf\u80fd\u8017\u548c\u78b3\u6392\u653e\uff0c\u4f7f\u7528\u5355\u5143\u6d4b\u8bd5\u8986\u76d6\u7387\u8bc4\u4f30\u751f\u6210\u6d4b\u8bd5\u7684\u8d28\u91cf\u3002", "result": "\u4e0d\u540cSLM\u5c55\u73b0\u51fa\u4e0d\u540c\u7684\u53ef\u6301\u7eed\u6027\u4f18\u52bf\uff1a\u4e00\u4e9b\u6a21\u578b\u4f18\u5148\u8003\u8651\u4f4e\u80fd\u8017\u548c\u5feb\u901f\u6267\u884c\uff0c\u800c\u53e6\u4e00\u4e9b\u5728\u78b3\u6392\u653e\u7ea6\u675f\u4e0b\u4fdd\u6301\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u6216\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660eSLM\u9a71\u52a8\u7684\u6d4b\u8bd5\u751f\u6210\u53ef\u6301\u7eed\u6027\u662f\u591a\u7ef4\u5ea6\u7684\uff0c\u4e14\u53d7\u63d0\u793a\u8bbe\u8ba1\u5f3a\u70c8\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u81ea\u52a8\u5316SLM\u6d4b\u8bd5\u751f\u6210\u7684\u53ef\u6301\u7eed\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u9610\u660e\u4e86\u63d0\u793a\u7ed3\u6784\u548c\u6a21\u578b\u9009\u62e9\u5982\u4f55\u5171\u540c\u5f71\u54cd\u73af\u5883\u548c\u6027\u80fd\u7ed3\u679c\uff0c\u586b\u8865\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u751f\u6210\u53ef\u6301\u7eed\u6027\u7814\u7a76\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.17678", "categories": ["cs.DC", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17678", "abs": "https://arxiv.org/abs/2602.17678", "authors": ["Oreofe Solarin"], "title": "It's Not Just Timestamps: A Study on Docker Reproducibility", "comment": null, "summary": "Reproducible container builds promise a simple integrity check for software supply chains: rebuild an image from its Dockerfile and compare hashes. We build a Docker measurement pipeline and apply it to a stratified sample of 2,000 GitHub repositories that contained a Dockerfile. We found that only 56% produce any buildable image, and just 2.7% of those are bitwise reproducible without any infrastructure configurations. After modifying infrastructure configurations, we raise bitwise reproducibility by 18.6%, but 78.7% of buildable Dockerfiles remain non-reproducible. We analyze the root causes of the remaining differences, and find that beyond timestamps and metadata, developer-controlled choices such as uncleaned caches, logs, documentation, and floating versions are dominant causes of non-reproducibility. We derive concrete Dockerfile guidelines from these patterns and discuss how they can inform future linters and Continuous Integration (CI) checks for reproducible containers.", "AI": {"tldr": "Docker\u5bb9\u5668\u6784\u5efa\u7684\u53ef\u91cd\u73b0\u6027\u7814\u7a76\uff1a\u5bf92000\u4e2aGitHub\u4ed3\u5e93\u7684\u5206\u6790\u663e\u793a\uff0c\u4ec5\u670956%\u7684Dockerfile\u80fd\u6210\u529f\u6784\u5efa\uff0c\u5176\u4e2d\u4ec52.7%\u80fd\u5728\u4e0d\u4fee\u6539\u57fa\u7840\u8bbe\u65bd\u914d\u7f6e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6bd4\u7279\u7ea7\u53ef\u91cd\u73b0\u3002\u901a\u8fc7\u8c03\u6574\u914d\u7f6e\u53ef\u5c06\u53ef\u91cd\u73b0\u6027\u63d0\u534718.6%\uff0c\u4f46\u4ecd\u670978.7%\u7684\u6784\u5efa\u4e0d\u53ef\u91cd\u73b0\uff0c\u4e3b\u8981\u539f\u56e0\u4e3a\u65f6\u95f4\u6233\u3001\u7f13\u5b58\u3001\u65e5\u5fd7\u3001\u6587\u6863\u548c\u6d6e\u52a8\u7248\u672c\u7b49\u5f00\u53d1\u8005\u53ef\u63a7\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76Docker\u5bb9\u5668\u6784\u5efa\u7684\u53ef\u91cd\u73b0\u6027\uff0c\u9a8c\u8bc1\"\u4eceDockerfile\u91cd\u5efa\u955c\u50cf\u5e76\u6bd4\u8f83\u54c8\u5e0c\u503c\"\u8fd9\u4e00\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b8c\u6574\u6027\u68c0\u67e5\u65b9\u6cd5\u7684\u5b9e\u9645\u53ef\u884c\u6027\uff0c\u8bc6\u522b\u5f71\u54cd\u53ef\u91cd\u73b0\u6027\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "method": "\u6784\u5efaDocker\u6d4b\u91cf\u6d41\u6c34\u7ebf\uff0c\u5bf92000\u4e2a\u5305\u542bDockerfile\u7684GitHub\u4ed3\u5e93\u8fdb\u884c\u5206\u5c42\u62bd\u6837\u5206\u6790\u3002\u9996\u5148\u6d4b\u8bd5\u539f\u59cb\u914d\u7f6e\u4e0b\u7684\u53ef\u91cd\u73b0\u6027\uff0c\u7136\u540e\u4fee\u6539\u57fa\u7840\u8bbe\u65bd\u914d\u7f6e\u91cd\u65b0\u6d4b\u8bd5\uff0c\u5206\u6790\u5269\u4f59\u5dee\u5f02\u7684\u6839\u672c\u539f\u56e0\u3002", "result": "\u4ec556%\u7684Dockerfile\u80fd\u6210\u529f\u6784\u5efa\u955c\u50cf\uff0c\u5176\u4e2d\u4ec52.7%\u80fd\u5728\u539f\u59cb\u914d\u7f6e\u4e0b\u5b9e\u73b0\u6bd4\u7279\u7ea7\u53ef\u91cd\u73b0\u3002\u4fee\u6539\u57fa\u7840\u8bbe\u65bd\u914d\u7f6e\u540e\uff0c\u53ef\u91cd\u73b0\u6027\u63d0\u534718.6%\uff0c\u4f46\u4ecd\u670978.7%\u7684\u6784\u5efa\u4e0d\u53ef\u91cd\u73b0\u3002\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u65f6\u95f4\u6233\u3001\u5143\u6570\u636e\u3001\u672a\u6e05\u7406\u7684\u7f13\u5b58\u3001\u65e5\u5fd7\u3001\u6587\u6863\u548c\u6d6e\u52a8\u7248\u672c\u7b49\u5f00\u53d1\u8005\u53ef\u63a7\u56e0\u7d20\u3002", "conclusion": "Docker\u5bb9\u5668\u6784\u5efa\u7684\u53ef\u91cd\u73b0\u6027\u73b0\u72b6\u4e0d\u5bb9\u4e50\u89c2\uff0c\u5927\u90e8\u5206\u6784\u5efa\u65e0\u6cd5\u5b9e\u73b0\u6bd4\u7279\u7ea7\u91cd\u73b0\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u5177\u4f53\u7684Dockerfile\u6307\u5bfc\u539f\u5219\uff0c\u8fd9\u4e9b\u53d1\u73b0\u53ef\u4e3a\u672a\u6765\u5f00\u53d1\u53ef\u91cd\u73b0\u5bb9\u5668\u76f8\u5173\u7684\u4ee3\u7801\u68c0\u67e5\u5de5\u5177\u548c\u6301\u7eed\u96c6\u6210\u68c0\u67e5\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2602.17913", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17913", "abs": "https://arxiv.org/abs/2602.17913", "authors": ["Qiming Zhu", "Shunian Chen", "Rui Yu", "Zhehao Wu", "Benyou Wang"], "title": "From Lossy to Verified: A Provenance-Aware Tiered Memory for Agents", "comment": null, "summary": "Long-horizon agents often compress interaction histories into write-time summaries. This creates a fundamental write-before-query barrier: compression decisions are made before the system knows what a future query will hinge on. As a result, summaries can cause unverifiable omissions -- decisive constraints (e.g., allergies) may be dropped, leaving the agent unable to justify an answer with traceable evidence. Retaining raw logs restores an authoritative source of truth, but grounding on raw logs by default is expensive: many queries are answerable from summaries, yet raw grounding still requires processing far longer contexts, inflating token consumption and latency.\n  We propose TierMem, a provenance-linked framework that casts retrieval as an inference-time evidence allocation problem. TierMem uses a two-tier memory hierarchy to answer with the cheapest sufficient evidence: it queries a fast summary index by default, and a runtime sufficiency router Escalates to an immutable raw-log store only when summary evidence is insufficient. TierMem then writes back verified findings as new summary units linked to their raw sources. On LoCoMo, TierMem achieves 0.851 accuracy (vs.0.873 raw-only) while reducing input tokens by 54.1\\% and latency by 60.7%.", "AI": {"tldr": "TierMem\u662f\u4e00\u4e2a\u4e24\u5c42\u7ea7\u8bb0\u5fc6\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u65f6\u8bc1\u636e\u5206\u914d\u89e3\u51b3\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u4e2d\u7684\u6458\u8981\u538b\u7f29\u4e0e\u539f\u59cb\u65e5\u5fd7\u67e5\u8be2\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u901a\u5e38\u5c06\u4ea4\u4e92\u5386\u53f2\u538b\u7f29\u4e3a\u5199\u5165\u65f6\u6458\u8981\uff0c\u4f46\u8fd9\u9020\u6210\u4e86\"\u5148\u5199\u540e\u67e5\"\u7684\u969c\u788d\uff1a\u538b\u7f29\u51b3\u7b56\u5728\u4e0d\u77e5\u9053\u672a\u6765\u67e5\u8be2\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\u505a\u51fa\uff0c\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u88ab\u9057\u6f0f\u3002\u867d\u7136\u4fdd\u7559\u539f\u59cb\u65e5\u5fd7\u80fd\u63d0\u4f9b\u6743\u5a01\u771f\u76f8\u6765\u6e90\uff0c\u4f46\u9ed8\u8ba4\u57fa\u4e8e\u539f\u59cb\u65e5\u5fd7\u67e5\u8be2\u6210\u672c\u9ad8\u6602\uff0c\u8bb8\u591a\u67e5\u8be2\u672c\u53ef\u4ece\u6458\u8981\u4e2d\u56de\u7b54\u3002", "method": "\u63d0\u51faTierMem\u6846\u67b6\uff0c\u91c7\u7528\u4e24\u5c42\u7ea7\u8bb0\u5fc6\u67b6\u6784\uff1a\u9ed8\u8ba4\u67e5\u8be2\u5feb\u901f\u6458\u8981\u7d22\u5f15\uff0c\u5f53\u6458\u8981\u8bc1\u636e\u4e0d\u8db3\u65f6\uff0c\u8fd0\u884c\u65f6\u5145\u5206\u6027\u8def\u7531\u5668\u5c06\u5347\u7ea7\u67e5\u8be2\u5230\u4e0d\u53ef\u53d8\u7684\u539f\u59cb\u65e5\u5fd7\u5b58\u50a8\u3002\u7cfb\u7edf\u5c06\u9a8c\u8bc1\u540e\u7684\u53d1\u73b0\u4f5c\u4e3a\u65b0\u7684\u6458\u8981\u5355\u5143\u5199\u56de\uff0c\u5e76\u4e0e\u539f\u59cb\u6765\u6e90\u94fe\u63a5\u3002", "result": "\u5728LoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTierMem\u8fbe\u52300.851\u51c6\u786e\u7387\uff08\u539f\u59cb\u65e5\u5fd7\u65b9\u6cd5\u4e3a0.873\uff09\uff0c\u540c\u65f6\u51cf\u5c1154.1%\u7684\u8f93\u5165token\u548c60.7%\u7684\u5ef6\u8fdf\u3002", "conclusion": "TierMem\u901a\u8fc7\u63a8\u7406\u65f6\u8bc1\u636e\u5206\u914d\u548c\u4e24\u5c42\u7ea7\u8bb0\u5fc6\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u65e5\u5fd7\u65b9\u6cd5\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u957f\u65f6\u7a0b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bb0\u5fc6\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18142", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18142", "abs": "https://arxiv.org/abs/2602.18142", "authors": ["Sebastian Dingler", "Frederik Boenke"], "title": "Toward Automated Virtual Electronic Control Unit (ECU) Twins for Shift-Left Automotive Software Testing", "comment": null, "summary": "Automotive software increasingly outpaces hardware availability, forcing late integration and expensive hardware-in-the-loop (HiL) bottlenecks. The InnoRegioChallenge project investigated whether a virtual test and integration environment can reproduce electronic control unit (ECU) behavior early enough to run real software binaries before physical hardware exists. We report a prototype that generates instruction-accurate processor models in SystemC/TLM~2.0 using an agentic, feedback-driven workflow coupled to a reference simulator via the GNU Debugger (GDB). The results indicate that the most critical technical risk -- CPU behavioral fidelity -- can be reduced through automated differential testing and iterative model correction. We summarize the architecture, the agentic modeling loop, and project outcomes, and we extrapolate plausible technical details consistent with the reported qualitative findings. While cloud-scale deployment and full toolchain integration remain future work, the prototype demonstrates a viable shift-left path for virtual ECU twins, enabling reproducible tests, non-intrusive tracing, and fault-injection campaigns aligned with safety standards.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u865a\u62dfECU\u6d4b\u8bd5\u73af\u5883\u539f\u578b\uff0c\u901a\u8fc7\u6307\u4ee4\u7ea7\u7cbe\u786e\u7684\u5904\u7406\u5668\u6a21\u578b\u548c\u81ea\u52a8\u5316\u5efa\u6a21\u6d41\u7a0b\uff0c\u80fd\u591f\u5728\u786c\u4ef6\u53ef\u7528\u524d\u8fd0\u884c\u771f\u5b9e\u8f6f\u4ef6\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u5b9e\u73b0\u6d4b\u8bd5\u5de6\u79fb\u3002", "motivation": "\u6c7d\u8f66\u8f6f\u4ef6\u5f00\u53d1\u901f\u5ea6\u8d85\u8fc7\u786c\u4ef6\u53ef\u7528\u6027\uff0c\u5bfc\u81f4\u540e\u671f\u96c6\u6210\u548c\u6602\u8d35\u7684\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u74f6\u9888\u3002\u9700\u8981\u80fd\u591f\u5728\u7269\u7406\u786c\u4ef6\u53ef\u7528\u524d\u8fd0\u884c\u771f\u5b9e\u8f6f\u4ef6\u4e8c\u8fdb\u5236\u7684\u865a\u62df\u6d4b\u8bd5\u73af\u5883\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSystemC/TLM 2.0\u7684\u6307\u4ee4\u7ea7\u7cbe\u786e\u5904\u7406\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u4ee3\u7406\u5f0f\u53cd\u9988\u9a71\u52a8\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ed3\u5408GNU\u8c03\u8bd5\u5668\u4e0e\u53c2\u8003\u6a21\u62df\u5668\u8fde\u63a5\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u5dee\u5f02\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6a21\u578b\u6821\u6b63\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u8868\u660e\uff0c\u6700\u5173\u952e\u7684\u6280\u672f\u98ce\u9669\u2014\u2014CPU\u884c\u4e3a\u4fdd\u771f\u5ea6\u2014\u2014\u53ef\u4ee5\u901a\u8fc7\u81ea\u52a8\u5316\u5dee\u5f02\u6d4b\u8bd5\u548c\u8fed\u4ee3\u6a21\u578b\u6821\u6b63\u6765\u964d\u4f4e\u3002\u5b9e\u73b0\u4e86\u865a\u62dfECU\u5b6a\u751f\u4f53\u7684\u53ef\u884c\u8def\u5f84\uff0c\u652f\u6301\u53ef\u91cd\u590d\u6d4b\u8bd5\u3001\u975e\u4fb5\u5165\u5f0f\u8ffd\u8e2a\u548c\u7b26\u5408\u5b89\u5168\u6807\u51c6\u7684\u6545\u969c\u6ce8\u5165\u3002", "conclusion": "\u867d\u7136\u4e91\u89c4\u6a21\u90e8\u7f72\u548c\u5b8c\u6574\u5de5\u5177\u94fe\u96c6\u6210\u4ecd\u9700\u672a\u6765\u5de5\u4f5c\uff0c\u4f46\u539f\u578b\u5c55\u793a\u4e86\u865a\u62dfECU\u5b6a\u751f\u4f53\u7684\u53ef\u884c\u5de6\u79fb\u8def\u5f84\uff0c\u80fd\u591f\u5728\u786c\u4ef6\u53ef\u7528\u524d\u8fdb\u884c\u65e9\u671f\u8f6f\u4ef6\u6d4b\u8bd5\u548c\u96c6\u6210\uff0c\u51cf\u5c11\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u74f6\u9888\u3002"}}
{"id": "2602.17774", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17774", "abs": "https://arxiv.org/abs/2602.17774", "authors": ["Wael Al-Manasrah", "Zuhair AlSader", "Tim Brecht", "Ahmed Alquraan", "Samer Al-Kiswany"], "title": "Message-Oriented Middleware Systems: Technology Overview", "comment": null, "summary": "We present a comprehensive characterization study of open-source message-oriented middleware (MOM) systems. We followed a rigorous methodology to select and study ten popular and diverse MOM systems. For each system, we examine 42 features with a total of 134 different options. We found that MOM systems have evolved to provide a framework for modern cloud applications through high flexibility and configurability and by offering core building blocks for complex applications including transaction support, active messaging, resource management, flow control, and native support for multi-tenancy. We also identify that there is an opportunity for the community to consolidate its efforts on fewer open-source projects.\n  We have also created an annotated data set that makes it easy to verify our findings, which can also be used to help practitioners and developers understand and compare the features of different systems. For a wider impact, we make our data set publicly available.", "AI": {"tldr": "\u5bf910\u4e2a\u5f00\u6e90\u6d88\u606f\u4e2d\u95f4\u4ef6\u7cfb\u7edf\u8fdb\u884c\u5168\u9762\u7684\u7279\u5f81\u5206\u6790\uff0c\u6db5\u76d642\u4e2a\u7279\u5f81\u5171134\u4e2a\u9009\u9879\uff0c\u53d1\u73b0MOM\u7cfb\u7edf\u5df2\u6f14\u53d8\u4e3a\u73b0\u4ee3\u4e91\u5e94\u7528\u63d0\u4f9b\u9ad8\u7075\u6d3b\u6027\u548c\u53ef\u914d\u7f6e\u6027\u7684\u6846\u67b6\uff0c\u5e76\u521b\u5efa\u4e86\u53ef\u516c\u5f00\u8bbf\u95ee\u7684\u6807\u6ce8\u6570\u636e\u96c6\u3002", "motivation": "\u5bf9\u5f00\u6e90\u6d88\u606f\u4e2d\u95f4\u4ef6\u7cfb\u7edf\u8fdb\u884c\u5168\u9762\u7279\u5f81\u5206\u6790\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u5b9e\u8df5\u8005\u7406\u89e3\u4e0d\u540c\u7cfb\u7edf\u7684\u7279\u6027\uff0c\u4fc3\u8fdb\u793e\u533a\u5728\u66f4\u5c11\u7684\u5f00\u6e90\u9879\u76ee\u4e0a\u96c6\u4e2d\u52aa\u529b\u3002", "method": "\u91c7\u7528\u4e25\u8c28\u65b9\u6cd5\u5b66\u9009\u62e9\u5e76\u7814\u7a7610\u4e2a\u6d41\u884c\u4e14\u591a\u6837\u5316\u7684MOM\u7cfb\u7edf\uff0c\u4e3a\u6bcf\u4e2a\u7cfb\u7edf\u68c0\u67e542\u4e2a\u7279\u5f81\u5171134\u4e2a\u4e0d\u540c\u9009\u9879\uff0c\u521b\u5efa\u6807\u6ce8\u6570\u636e\u96c6\u3002", "result": "\u53d1\u73b0MOM\u7cfb\u7edf\u5df2\u6f14\u53d8\u4e3a\u73b0\u4ee3\u4e91\u5e94\u7528\u63d0\u4f9b\u9ad8\u7075\u6d3b\u6027\u548c\u53ef\u914d\u7f6e\u6027\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u6838\u5fc3\u6784\u5efa\u5757\u5982\u4e8b\u52a1\u652f\u6301\u3001\u4e3b\u52a8\u6d88\u606f\u3001\u8d44\u6e90\u7ba1\u7406\u3001\u6d41\u91cf\u63a7\u5236\u548c\u5bf9\u591a\u79df\u6237\u7684\u539f\u751f\u652f\u6301\u3002\u8bc6\u522b\u51fa\u793e\u533a\u6709\u673a\u4f1a\u5728\u66f4\u5c11\u7684\u5f00\u6e90\u9879\u76ee\u4e0a\u96c6\u4e2d\u52aa\u529b\u3002", "conclusion": "\u901a\u8fc7\u5168\u9762\u7684\u7279\u5f81\u5206\u6790\u63ed\u793a\u4e86MOM\u7cfb\u7edf\u7684\u53d1\u5c55\u8d8b\u52bf\uff0c\u521b\u5efa\u4e86\u53ef\u9a8c\u8bc1\u7814\u7a76\u7ed3\u679c\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6bd4\u8f83\u4e0d\u540c\u7cfb\u7edf\u7279\u6027\u7684\u5de5\u5177\uff0c\u5e76\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u6269\u5927\u5f71\u54cd\u529b\u3002"}}
{"id": "2602.17914", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17914", "abs": "https://arxiv.org/abs/2602.17914", "authors": ["Zhuocheng Gan", "Yifan Wang"], "title": "Efficient Filtered-ANN via Learning-based Query Planning", "comment": null, "summary": "Filtered ANN search is an increasingly important problem in vector retrieval, yet systems face a difficult trade-off due to the execution order: Pre-filtering (filtering first, then ANN over the passing subset) requires expensive per-predicate index construction, while post-filtering (ANN first, then filtering candidates) may waste computation and lose recall under low selectivity due to insufficient candidates after filtering. We introduce a learning-based query planning framework that dynamically selects the most effective execution plan for each query, using lightweight predictions derived from dataset and query statistics (e.g., dimensionality, corpus size, distribution features, and predicate statistics). The framework supports diverse filter types, including categorical/keyword and range predicates, and is generic to use any backend ANN index. Experiments show that our method achieves up to 4x acceleration with >= 90% recall comparing to the strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u67e5\u8be2\u89c4\u5212\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u6700\u6709\u6548\u7684\u6267\u884c\u8ba1\u5212\uff08\u9884\u8fc7\u6ee4\u6216\u540e\u8fc7\u6ee4\uff09\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u9884\u6d4b\u63d0\u5347\u8fc7\u6ee4ANN\u641c\u7d22\u6027\u80fd", "motivation": "\u8fc7\u6ee4ANN\u641c\u7d22\u9762\u4e34\u6267\u884c\u987a\u5e8f\u7684\u6743\u8861\uff1a\u9884\u8fc7\u6ee4\u9700\u8981\u6602\u8d35\u7684\u6309\u8c13\u8bcd\u7d22\u5f15\u6784\u5efa\uff0c\u540e\u8fc7\u6ee4\u53ef\u80fd\u5728\u4f4e\u9009\u62e9\u6027\u4e0b\u6d6a\u8d39\u8ba1\u7b97\u5e76\u635f\u5931\u53ec\u56de\u7387", "method": "\u57fa\u4e8e\u5b66\u4e60\u7684\u67e5\u8be2\u89c4\u5212\u6846\u67b6\uff0c\u5229\u7528\u6570\u636e\u96c6\u548c\u67e5\u8be2\u7edf\u8ba1\uff08\u7ef4\u5ea6\u3001\u8bed\u6599\u5927\u5c0f\u3001\u5206\u5e03\u7279\u5f81\u3001\u8c13\u8bcd\u7edf\u8ba1\uff09\u8fdb\u884c\u8f7b\u91cf\u7ea7\u9884\u6d4b\uff0c\u52a8\u6001\u9009\u62e9\u6700\u6709\u6548\u7684\u6267\u884c\u8ba1\u5212\uff0c\u652f\u6301\u591a\u79cd\u8fc7\u6ee4\u7c7b\u578b\uff0c\u53ef\u4e0e\u4efb\u4f55\u540e\u7aefANN\u7d22\u5f15\u901a\u7528", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u9ad8\u8fbe4\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u226590%\u7684\u53ec\u56de\u7387", "conclusion": "\u63d0\u51fa\u7684\u5b66\u4e60\u578b\u67e5\u8be2\u89c4\u5212\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u6ee4ANN\u641c\u7d22\u4e2d\u7684\u6267\u884c\u987a\u5e8f\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd"}}
{"id": "2602.18190", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18190", "abs": "https://arxiv.org/abs/2602.18190", "authors": ["Jorge Melegati"], "title": "Role and Identity Work of Software Engineering Professionals in the Generative AI Era", "comment": "Accepted to the 19th International Conference on Cooperative and Human Aspects of Software Engineering (CHASE 2026)", "summary": "The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u8eab\u4efd\u8ba4\u540c\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e0d\u540c\u89d2\u8272\uff08\u5f00\u53d1\u8005\u4e0e\u6d4b\u8bd5\u8005\uff09\u5728\u8eab\u4efd\u5de5\u4f5c\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u7814\u7a76\u8bae\u7a0b\u4ee5\u652f\u6301GenAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91c7\u7528\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u751f\u6210\u5f0fAI\u7684\u91c7\u7528\u4f1a\u89e6\u53d1\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u7684\u8eab\u4efd\u5de5\u4f5c\uff08\u8eab\u4efd\u5f62\u6210\u3001\u9002\u5e94\u548c\u62d2\u7edd\uff09\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u672a\u8003\u8651\u4e0d\u540c\u89d2\u8272\uff08\u5982\u5f00\u53d1\u8005\u548c\u6d4b\u8bd5\u8005\uff09\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u672c\u6587\u8ba4\u4e3a\u89d2\u8272\u662f\u5b9a\u4e49\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8eab\u4efd\u5de5\u4f5c\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u56de\u987e\u5173\u4e8e\u4e0d\u540c\u89d2\u8272\u7684\u73b0\u6709\u7814\u7a76\u4ee5\u53ca\u8fd1\u671f\u5173\u4e8e\u5982\u4f55\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u91c7\u7528\u751f\u6210\u5f0fAI\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u7814\u7a76\u8bae\u7a0b\u3002\u8be5\u8bae\u7a0b\u65e8\u5728\u66f4\u597d\u5730\u7406\u89e3\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u7531\u751f\u6210\u5f0fAI\u91c7\u7528\u89e6\u53d1\u7684\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8eab\u4efd\u5de5\u4f5c\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u652f\u6301\u8fd9\u79cd\u91c7\u7528\u7684\u65b0\u5de5\u5177\u3002", "result": "\u672c\u6587\u672a\u62a5\u544a\u5177\u4f53\u5b9e\u8bc1\u7ed3\u679c\uff0c\u800c\u662f\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u8bae\u7a0b\u3002\u8be5\u8bae\u7a0b\u5305\u62ec\uff1a1\uff09\u7406\u89e3\u89d2\u8272\u5982\u4f55\u5f71\u54cd\u8eab\u4efd\u5de5\u4f5c\uff1b2\uff09\u57fa\u4e8e\u6b64\u7406\u89e3\u5f00\u53d1\u652f\u6301\u751f\u6210\u5f0fAI\u91c7\u7528\u7684\u65b0\u5de5\u5177\uff1b3\uff09\u8ba8\u8bba\u6f5c\u5728\u5b9e\u8df5\u610f\u4e49\u3002", "conclusion": "\u89d2\u8272\u662f\u7406\u89e3\u751f\u6210\u5f0fAI\u91c7\u7528\u5982\u4f55\u5f71\u54cd\u8f6f\u4ef6\u4e13\u4e1a\u4eba\u5458\u8eab\u4efd\u5de5\u4f5c\u7684\u5173\u952e\u56e0\u7d20\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u63a2\u7d22\u4e0d\u540c\u89d2\u8272\u5728\u8eab\u4efd\u5de5\u4f5c\u4e0a\u7684\u5dee\u5f02\uff0c\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u652f\u6301\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdb\u751f\u6210\u5f0fAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6709\u6548\u91c7\u7528\u3002"}}
{"id": "2602.17808", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.17808", "abs": "https://arxiv.org/abs/2602.17808", "authors": ["Nathan Ng", "Walid A. Hanafy", "Prashanthi Kadambi", "Balachandra Sunil", "Ayush Gupta", "David Irwin", "Yogesh Simmhan", "Prashant Shenoy"], "title": "Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs", "comment": null, "summary": "IoT applications are increasingly relying on on-device AI accelerators to ensure high performance, especially in limited connectivity and safety-critical scenarios. However, the limited on-chip memory of these accelerators forces inference runtimes to swap model segments between host and accelerator memory, substantially inflating latency. While collaborative processing by partitioning the model processing between CPU and accelerator resources can reduce accelerator memory pressure and latency, naive partitioning may worsen end-to-end latency by either shifting excessive computation to the CPU or failing to sufficiently curb swapping, a problem that is further amplified in multi-tenant and dynamic environments.\n  To address these issues, we present SwapLess, a system for adaptive, multi-tenant TPU-CPU collaborative inference for memory-constrained Edge TPUs. SwapLess utilizes an analytic queueing model that captures partition-dependent CPU/TPU service times as well as inter- and intra-model swapping overheads across different workload mixes and request rates. Using this model, SwapLess continuously adjusts both the partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead. An implementation on Edge TPU-equipped platforms demonstrates that SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads relative to the default Edge TPU compiler.", "AI": {"tldr": "SwapLess\u662f\u4e00\u4e2a\u7528\u4e8e\u5185\u5b58\u53d7\u9650Edge TPU\u7684\u81ea\u9002\u5e94\u591a\u79df\u6237TPU-CPU\u534f\u540c\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u6392\u961f\u6a21\u578b\u52a8\u6001\u8c03\u6574\u5206\u533a\u70b9\u548cCPU\u6838\u5fc3\u5206\u914d\uff0c\u663e\u8457\u964d\u4f4e\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u7269\u8054\u7f51\u5e94\u7528\u4f9d\u8d56\u8bbe\u5907\u7aefAI\u52a0\u901f\u5668\uff0c\u4f46\u7247\u4e0a\u5185\u5b58\u6709\u9650\u5bfc\u81f4\u63a8\u7406\u65f6\u9700\u8981\u9891\u7e41\u5728\u4e3b\u673a\u548c\u52a0\u901f\u5668\u5185\u5b58\u95f4\u4ea4\u6362\u6a21\u578b\u7247\u6bb5\uff0c\u663e\u8457\u589e\u52a0\u5ef6\u8fdf\u3002\u73b0\u6709\u534f\u540c\u5904\u7406\u65b9\u6cd5\u8981\u4e48\u5c06\u8fc7\u591a\u8ba1\u7b97\u8f6c\u79fb\u5230CPU\uff0c\u8981\u4e48\u65e0\u6cd5\u6709\u6548\u51cf\u5c11\u4ea4\u6362\uff0c\u5728\u591a\u79df\u6237\u52a8\u6001\u73af\u5883\u4e2d\u95ee\u9898\u66f4\u4e25\u91cd\u3002", "method": "SwapLess\u4f7f\u7528\u5206\u6790\u6392\u961f\u6a21\u578b\uff0c\u6355\u6349\u5206\u533a\u76f8\u5173\u7684CPU/TPU\u670d\u52a1\u65f6\u95f4\u4ee5\u53ca\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u6df7\u5408\u548c\u8bf7\u6c42\u7387\u4e0b\u7684\u6a21\u578b\u95f4\u548c\u6a21\u578b\u5185\u4ea4\u6362\u5f00\u9500\u3002\u57fa\u4e8e\u6b64\u6a21\u578b\uff0c\u7cfb\u7edf\u5728\u7ebf\u52a8\u6001\u8c03\u6574\u5206\u533a\u70b9\u548cCPU\u6838\u5fc3\u5206\u914d\uff0c\u4ee5\u6700\u5c0f\u5316\u7aef\u5230\u7aef\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u5728\u914d\u5907Edge TPU\u7684\u5e73\u53f0\u4e0a\u7684\u5b9e\u73b0\u8868\u660e\uff0c\u76f8\u6bd4\u9ed8\u8ba4Edge TPU\u7f16\u8bd1\u5668\uff0cSwapLess\u5c06\u5355\u79df\u6237\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe63.8%\uff0c\u591a\u79df\u6237\u5de5\u4f5c\u8d1f\u8f7d\u964d\u4f4e\u9ad8\u8fbe77.4%\u3002", "conclusion": "SwapLess\u901a\u8fc7\u81ea\u9002\u5e94\u534f\u540c\u63a8\u7406\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5b58\u53d7\u9650Edge TPU\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u79df\u6237\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.18274", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18274", "abs": "https://arxiv.org/abs/2602.18274", "authors": ["Viktoriia Makovska", "Ihor Michurin", "Mariia Tokhtamysh", "George Fletcher", "Julia Stoyanovich"], "title": "Seasoning Data Modeling Education with GARLIC: A Participatory Co-Design Framework", "comment": "DataEd'26: 5th International Workshop on Data Systems Education", "summary": "Entity-Relationship (ER) modeling is commonly taught as a primarily technical activity, despite its central role in shaping how data systems represent people, processes, and institutions. Prior research in participatory design demonstrates that involving diverse stakeholders in modeling can surface tacit knowledge, challenge implicit assumptions, and produce more inclusive data representations. However, database education currently lacks structured pedagogical approaches for teaching participatory ER modeling in practice.\n  We introduce the GARLIC methodology for teaching and learning participatory ER modeling. GARLIC adapts and extends the ONION participatory ER modeling framework of Makovska et al.(HILDA 2025) into a workshop-based learning format that combines role-playing, collaborative synthesis, guided critique, and iterative refinement. GARLIC is designed to develop both technical modeling skills and critical awareness of the social and ethical dimensions of data representation. GARLIC lowers the barrier to participatory ER modeling and equips students with practical skills for collaborative, inclusive data model design.", "AI": {"tldr": "GARLIC\u662f\u4e00\u79cd\u6559\u5b66\u53c2\u4e0e\u5f0fER\u5efa\u6a21\u7684\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u3001\u534f\u4f5c\u7efc\u5408\u3001\u6307\u5bfc\u6027\u6279\u5224\u548c\u8fed\u4ee3\u6539\u8fdb\u7684\u7814\u8ba8\u4f1a\u5f62\u5f0f\uff0c\u57f9\u517b\u6280\u672f\u5efa\u6a21\u6280\u80fd\u548c\u6570\u636e\u8868\u793a\u7684\u793e\u4f1a\u4f26\u7406\u610f\u8bc6\u3002", "motivation": "\u5f53\u524d\u6570\u636e\u5e93\u6559\u80b2\u7f3a\u4e4f\u6559\u6388\u53c2\u4e0e\u5f0fER\u5efa\u6a21\u7684\u7ed3\u6784\u5316\u6559\u5b66\u65b9\u6cd5\u3002\u4f20\u7edf\u7684ER\u5efa\u6a21\u6559\u5b66\u8fc7\u4e8e\u6280\u672f\u5316\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u7cfb\u7edf\u5982\u4f55\u8868\u793a\u4eba\u3001\u6d41\u7a0b\u548c\u673a\u6784\u7684\u793e\u4f1a\u5f71\u54cd\u3002\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u7814\u7a76\u8868\u660e\uff0c\u8ba9\u591a\u5143\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u5efa\u6a21\u53ef\u4ee5\u53d1\u6398\u9690\u6027\u77e5\u8bc6\u3001\u6311\u6218\u9690\u542b\u5047\u8bbe\uff0c\u5e76\u4ea7\u751f\u66f4\u5177\u5305\u5bb9\u6027\u7684\u6570\u636e\u8868\u793a\u3002", "method": "GARLIC\u65b9\u6cd5\u57fa\u4e8eMakowska\u7b49\u4eba\u7684ONION\u53c2\u4e0e\u5f0fER\u5efa\u6a21\u6846\u67b6\uff0c\u5c06\u5176\u6269\u5c55\u4e3a\u7814\u8ba8\u4f1a\u5f0f\u5b66\u4e60\u5f62\u5f0f\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u89d2\u8272\u626e\u6f14\u3001\u534f\u4f5c\u7efc\u5408\u3001\u6307\u5bfc\u6027\u6279\u5224\u548c\u8fed\u4ee3\u6539\u8fdb\u7b49\u5143\u7d20\uff0c\u65e8\u5728\u964d\u4f4e\u53c2\u4e0e\u5f0fER\u5efa\u6a21\u7684\u95e8\u69db\u3002", "result": "GARLIC\u65b9\u6cd5\u80fd\u591f\u57f9\u517b\u5b66\u751f\u6280\u672f\u5efa\u6a21\u6280\u80fd\uff0c\u540c\u65f6\u589e\u5f3a\u4ed6\u4eec\u5bf9\u6570\u636e\u8868\u793a\u7684\u793e\u4f1a\u548c\u4f26\u7406\u7ef4\u5ea6\u7684\u6279\u5224\u610f\u8bc6\u3002\u8be5\u65b9\u6cd5\u4e3a\u5b66\u751f\u63d0\u4f9b\u4e86\u534f\u4f5c\u3001\u5305\u5bb9\u6027\u6570\u636e\u6a21\u578b\u8bbe\u8ba1\u7684\u5b9e\u8df5\u6280\u80fd\u3002", "conclusion": "GARLIC\u4e3a\u6570\u636e\u5e93\u6559\u80b2\u63d0\u4f9b\u4e86\u6559\u6388\u53c2\u4e0e\u5f0fER\u5efa\u6a21\u7684\u7ed3\u6784\u5316\u6559\u5b66\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u5f53\u524d\u6559\u80b2\u4e2d\u7684\u7a7a\u767d\uff0c\u4f7f\u5b66\u751f\u80fd\u591f\u5728\u6280\u672f\u5efa\u6a21\u7684\u540c\u65f6\u8003\u8651\u793e\u4f1a\u4f26\u7406\u56e0\u7d20\uff0c\u8bbe\u8ba1\u66f4\u5305\u5bb9\u7684\u6570\u636e\u7cfb\u7edf\u3002"}}
{"id": "2602.18306", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18306", "abs": "https://arxiv.org/abs/2602.18306", "authors": ["Dongming Jin", "Zhi Jin", "Zheng Fang", "Linyu Li", "XiaoTian Yang", "Yuanpeng He", "Xiaohong Chen"], "title": "ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation", "comment": "22page, 7 figures", "summary": "With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.", "AI": {"tldr": "\u63d0\u51faReqElicitGym\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u4e2d\u7684\u8bbf\u8c08\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dLLM\u5728\u6316\u6398\u9690\u6027\u9700\u6c42\u65b9\u9762\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u968f\u7740LLM\u7f16\u7801\u80fd\u529b\u63d0\u5347\uff0c\u8f6f\u4ef6\u5f00\u53d1\u7684\u74f6\u9888\u8f6c\u5411\u9700\u6c42\u83b7\u53d6\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u5c11\u91cf\u573a\u666f\u3001\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u548c\u4e3b\u89c2\u8bc4\u5206\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u3001\u53ef\u91cf\u5316\u7684\u6bd4\u8f83\u6846\u67b6\u3002", "method": "\u63d0\u51faReqElicitGym\u8bc4\u4f30\u73af\u5883\uff0c\u5305\u542b101\u4e2a\u7f51\u7ad9\u9700\u6c42\u83b7\u53d6\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u4ea4\u4e92\u5f0f\u6a21\u62df\u7528\u6237\u548c\u4efb\u52a1\u8bc4\u4f30\u5668\uff0c\u652f\u6301\u81ea\u52a8\u5316\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u3002", "result": "\u5bf97\u4e2a\u4ee3\u8868\u6027LLM\u7684\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff1a\u5f53\u524dLLM\u4ec5\u80fd\u83b7\u53d6\u4e0d\u5230\u4e00\u534a\u7684\u9690\u6027\u9700\u6c42\uff0c\u6709\u6548\u63d0\u95ee\u591a\u51fa\u73b0\u5728\u5bf9\u8bdd\u540e\u671f\uff1b\u80fd\u83b7\u53d6\u4ea4\u4e92\u548c\u5185\u5bb9\u9700\u6c42\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u98ce\u683c\u76f8\u5173\u9700\u6c42\u3002", "conclusion": "ReqElicitGym\u4e3a\u81ea\u52a8\u5316\u5bf9\u8bdd\u5f0f\u9700\u6c42\u83b7\u53d6\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u91cf\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5f53\u524dLLM\u5728\u8bbf\u8c08\u80fd\u529b\u4e0a\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2602.17811", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.17811", "abs": "https://arxiv.org/abs/2602.17811", "authors": ["Guy Blelloch", "Andrew Brady", "Laxman Dhulipala", "Jeremy Fineman", "Kishen Gowda", "Chase Hutton"], "title": "Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation", "comment": "57 pages", "summary": "A low out-degree orientation directs each edge of an undirected graph with the goal of minimizing the maximum out-degree of a vertex. In the parallel batch-dynamic setting, one can insert or delete batches of edges, and the goal is to process the entire batch in parallel with work per edge similar to that of a single sequential update and with span (or depth) for the entire batch that is polylogarithmic. In this paper we present faster parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. All results herein achieve polylogarithmic depth, with high probability (whp); the focus of this paper is on minimizing the work, which varies across results.\n  Our first result is the first parallel batch-dynamic algorithm to maintain an asymptotically optimal orientation with asymptotically optimal expected work bounds, in an amortized sense, improving over the prior best work bounds of Liu et al.~[SPAA~'22] by a logarithmic factor.\n  Our second result is a $O(c \\log n)$ orientation algorithm with expected worst-case $O(\\sqrt{\\log n})$ work per edge update, where $c$ is a known upper-bound on the arboricity of the graph. This matches the best-known sequential worst-case $O(c \\log n)$ orientation algorithm given by Berglin and Brodal ~[Algorithmica~'18], albeit in expectation.\n  Our final result is a $O(c + \\log n)$-orientation algorithm with $O(\\log^2 n)$ expected worst-case work per edge update. This algorithm significantly improves upon the recent result of Ghaffari and Koo~[SPAA~'25], which maintains a $O(c)$-orientation with $O(\\log^9 n)$ worst-case work per edge whp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u5e76\u884c\u6279\u91cf\u52a8\u6001\u7b97\u6cd5\uff0c\u7528\u4e8e\u7ef4\u62a4\u65e0\u5411\u56fe\u7684\u4f4e\u51fa\u5ea6\u5b9a\u5411\uff0c\u5747\u5b9e\u73b0\u591a\u5bf9\u6570\u6df1\u5ea6\uff0c\u91cd\u70b9\u5728\u4e8e\u6700\u5c0f\u5316\u5de5\u4f5c\u91cf\u3002", "motivation": "\u5728\u5e76\u884c\u6279\u91cf\u52a8\u6001\u8bbe\u7f6e\u4e2d\uff0c\u9700\u8981\u5904\u7406\u8fb9\u7684\u6279\u91cf\u63d2\u5165\u6216\u5220\u9664\uff0c\u76ee\u6807\u662f\u4f7f\u6574\u4e2a\u6279\u6b21\u7684\u5e76\u884c\u5904\u7406\u5de5\u4f5c\u91cf\u63a5\u8fd1\u5355\u6b21\u987a\u5e8f\u66f4\u65b0\u7684\u6bcf\u8fb9\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u591a\u5bf9\u6570\u6df1\u5ea6\u3002\u73b0\u6709\u7b97\u6cd5\u5728\u65f6\u95f4\u590d\u6742\u5ea6\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u5e76\u884c\u6279\u91cf\u52a8\u6001\u7b97\u6cd5\uff1a1\uff09\u7b2c\u4e00\u4e2a\u5b9e\u73b0\u6e10\u8fd1\u6700\u4f18\u5b9a\u5411\u548c\u6e10\u8fd1\u6700\u4f18\u671f\u671b\u5de5\u4f5c\u91cf\u7684\u7b97\u6cd5\uff1b2\uff09\u57fa\u4e8e\u56fe\u6811\u5bbd\u5ea6\u7684O(c log n)\u5b9a\u5411\u7b97\u6cd5\uff0c\u6bcf\u8fb9\u66f4\u65b0\u671f\u671b\u6700\u574f\u60c5\u51b5\u5de5\u4f5c\u91cf\u4e3aO(\u221alog n)\uff1b3\uff09O(c + log n)\u5b9a\u5411\u7b97\u6cd5\uff0c\u6bcf\u8fb9\u66f4\u65b0\u671f\u671b\u6700\u574f\u60c5\u51b5\u5de5\u4f5c\u91cf\u4e3aO(log\u00b2 n)\u3002", "result": "1\uff09\u76f8\u6bd4Liu\u7b49\u4eba\u7684\u5de5\u4f5c\u6539\u8fdb\u4e86\u5bf9\u6570\u56e0\u5b50\uff1b2\uff09\u4e0eBerglin\u548cBrodal\u7684\u6700\u4f73\u987a\u5e8f\u6700\u574f\u60c5\u51b5\u7b97\u6cd5\u5339\u914d\uff08\u671f\u671b\u610f\u4e49\uff09\uff1b3\uff09\u76f8\u6bd4Ghaffari\u548cKoo\u7684O(log\u2079 n)\u6700\u574f\u60c5\u51b5\u5de5\u4f5c\u91cf\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u6279\u91cf\u52a8\u6001\u4f4e\u51fa\u5ea6\u5b9a\u5411\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u591a\u5bf9\u6570\u6df1\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5de5\u4f5c\u91cf\uff0c\u4e3a\u56fe\u5904\u7406\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u5e76\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18390", "categories": ["cs.DB", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.18390", "abs": "https://arxiv.org/abs/2602.18390", "authors": ["Miika Hannula", "Teymur Ismikhanov", "Jonni Virtema"], "title": "Dichotomy for Axiomatising Inclusion Dependencies on K-Databases", "comment": null, "summary": "A relation consisting of tuples annotated by an element of a monoid K is called a K-relation. A K-database is a collection of K-relations. In this paper, we study entailment of inclusion dependencies over K-databases, where K is a positive commutative monoid. We establish a dichotomy regarding the axiomatisation of the entailment of inclusion dependencies over K-databases, based on whether the monoid K is weakly absorptive or weakly cancellative. We establish that, if the monoid is weakly cancellative then the standard axioms of inclusion dependencies are sound and complete for the implication problem. If the monoid is not weakly cancellative, it is weakly absorptive and the standard axioms of inclusion dependencies together with the weak symmetry axiom are sound and complete for the implication problem. In addition, we establish that the so-called balance axiom is further required, if one stipulates that the joint weights of each K-relation of a K-database need to be the same; this generalises the notion of a K-relation being a distribution. In conjunction with the balance axiom, weak symmetry axiom boils down to symmetry.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86K-\u6570\u636e\u5e93\u4e0a\u5305\u542b\u4f9d\u8d56\u7684\u8574\u542b\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u5e7a\u534a\u7fa4K\u6027\u8d28\u7684\u4e8c\u5206\u6cd5\uff1a\u82e5K\u5f31\u53ef\u6d88\u5219\u6807\u51c6\u516c\u7406\u5b8c\u5907\uff1b\u82e5K\u5f31\u5438\u6536\u5219\u9700\u6dfb\u52a0\u5f31\u5bf9\u79f0\u516c\u7406\uff1b\u82e5\u8981\u6c42K-\u5173\u7cfb\u4e3a\u5206\u5e03\u8fd8\u9700\u6dfb\u52a0\u5e73\u8861\u516c\u7406\u3002", "motivation": "\u7814\u7a76\u5e26\u6743\u6570\u636e\u5e93\uff08K-\u6570\u636e\u5e93\uff09\u4e0a\u5305\u542b\u4f9d\u8d56\u7684\u8574\u542b\u95ee\u9898\uff0c\u5176\u4e2dK\u662f\u6b63\u4ea4\u6362\u5e7a\u534a\u7fa4\u3002\u65e8\u5728\u5efa\u7acb\u5b8c\u5907\u7684\u516c\u7406\u5316\u7cfb\u7edf\uff0c\u4e3a\u5e26\u6743\u6570\u636e\u5e93\u7684\u7ea6\u675f\u63a8\u7406\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u57fa\u4e8e\u5e7a\u534a\u7fa4K\u7684\u4ee3\u6570\u6027\u8d28\uff08\u5f31\u53ef\u6d88\u6027\u548c\u5f31\u5438\u6536\u6027\uff09\u5efa\u7acb\u4e8c\u5206\u6cd5\u3002\u5206\u6790\u4e0d\u540c\u6027\u8d28\u4e0b\u5305\u542b\u4f9d\u8d56\u8574\u542b\u95ee\u9898\u7684\u516c\u7406\u5b8c\u5907\u6027\uff0c\u5305\u62ec\u6807\u51c6\u516c\u7406\u3001\u5f31\u5bf9\u79f0\u516c\u7406\u548c\u5e73\u8861\u516c\u7406\u3002", "result": "1. \u82e5K\u5f31\u53ef\u6d88\uff0c\u6807\u51c6\u5305\u542b\u4f9d\u8d56\u516c\u7406\u5bf9\u8574\u542b\u95ee\u9898\u5b8c\u5907\uff1b2. \u82e5K\u5f31\u5438\u6536\uff0c\u9700\u6dfb\u52a0\u5f31\u5bf9\u79f0\u516c\u7406\u624d\u5b8c\u5907\uff1b3. \u82e5\u8981\u6c42K-\u5173\u7cfb\u4e3a\u5206\u5e03\uff08\u8054\u5408\u6743\u91cd\u76f8\u540c\uff09\uff0c\u8fd8\u9700\u6dfb\u52a0\u5e73\u8861\u516c\u7406\uff0c\u6b64\u65f6\u5f31\u5bf9\u79f0\u9000\u5316\u4e3a\u5bf9\u79f0\u6027\u3002", "conclusion": "K-\u6570\u636e\u5e93\u4e0a\u5305\u542b\u4f9d\u8d56\u7684\u8574\u542b\u95ee\u9898\u516c\u7406\u5316\u5b8c\u5168\u7531\u5e7a\u534a\u7fa4K\u7684\u4ee3\u6570\u6027\u8d28\u51b3\u5b9a\uff0c\u5efa\u7acb\u4e86\u6e05\u6670\u7684\u4e8c\u5206\u6cd5\uff0c\u4e3a\u5e26\u6743\u6570\u636e\u5e93\u7684\u7ea6\u675f\u7406\u8bba\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.18307", "categories": ["cs.SE", "cs.CL", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.18307", "abs": "https://arxiv.org/abs/2602.18307", "authors": ["Yutong Xin", "Qiaochu Chen", "Greg Durrett", "I\u015fil Dillig"], "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean", "comment": null, "summary": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.", "AI": {"tldr": "VeriSoftBench\uff1a\u4e00\u4e2a\u5305\u542b500\u4e2aLean 4\u8bc1\u660e\u4e49\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u8f6f\u4ef6\u9a8c\u8bc1\u800c\u975e\u6570\u5b66\u8bc1\u660e\uff0c\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u4ee3\u7801\u5e93\u73af\u5883\u4e2d\u7684\u5b9a\u7406\u8bc1\u660e\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLLM\u5728\u5b9a\u7406\u8bc1\u660e\u65b9\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u57fa\u4e8eMathlib\u6570\u5b66\u5e93\uff0c\u4f46\u8f6f\u4ef6\u9a8c\u8bc1\u4e2d\u7684\u8bc1\u660e\u901a\u5e38\u4f9d\u8d56\u4e8e\u9879\u76ee\u7279\u5b9a\u7684\u5b9a\u4e49\u4e30\u5bcc\u7684\u4ee3\u7801\u5e93\uff0c\u9700\u8981\u8bc4\u4f30LLM\u5728\u8fd9\u79cd\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u521b\u5efaVeriSoftBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u4e2a\u6765\u81ea\u5f00\u6e90\u5f62\u5f0f\u5316\u65b9\u6cd5\u9879\u76ee\u7684Lean 4\u8bc1\u660e\u4e49\u52a1\uff0c\u4fdd\u6301\u771f\u5b9e\u7684\u4ee3\u7801\u5e93\u4e0a\u4e0b\u6587\u548c\u8de8\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\uff0c\u8bc4\u4f30\u524d\u6cbfLLM\u548c\u4e13\u4e1a\u8bc1\u660e\u5668\u7684\u8868\u73b0\u3002", "result": "1) \u9488\u5bf9Mathlib\u6570\u5b66\u8bc1\u660e\u8c03\u4f18\u7684\u8bc1\u660e\u5668\u5728\u6b64\u4ee3\u7801\u5e93\u4e2d\u5fc3\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff1b2) \u6210\u529f\u7387\u4e0e\u4f20\u9012\u6027\u4ee3\u7801\u5e93\u4f9d\u8d56\u5f3a\u76f8\u5173\uff0c\u4f9d\u8d56\u94fe\u8d8a\u957f\u8d8a\u96be\u89e3\u51b3\uff1b3) \u63d0\u4f9b\u7ecf\u8fc7\u7b5b\u9009\u7684\u4f9d\u8d56\u95ed\u5305\u4e0a\u4e0b\u6587\u6bd4\u66b4\u9732\u5b8c\u6574\u4ee3\u7801\u5e93\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u8f6f\u4ef6\u9a8c\u8bc1\u4e2d\u7684\u5b9a\u7406\u8bc1\u660e\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u4ee3\u7801\u5e93\u73af\u5883\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u73b0\u6709\u57fa\u4e8e\u6570\u5b66\u8bc1\u660e\u7684LLM\u65b9\u6cd5\u4e0d\u80fd\u5f88\u597d\u5730\u8fc1\u79fb\u5230\u8f6f\u4ef6\u9a8c\u8bc1\u573a\u666f\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u590d\u6742\u7684\u9879\u76ee\u7279\u5b9a\u4f9d\u8d56\u5173\u7cfb\u3002"}}
{"id": "2602.17817", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17817", "abs": "https://arxiv.org/abs/2602.17817", "authors": ["Ehsan Yousefzadeh-Asl-Miandoab", "Reza Karimzadeh", "Danyal Yorulmaz", "Bulat Ibragimov", "P\u0131nar T\u00f6z\u00fcn"], "title": "GPU Memory and Utilization Estimation for Training-Aware Resource Management: Opportunities and Limitations", "comment": null, "summary": "Collocating deep learning training tasks improves GPU utilization but causes drastic slowdowns due to resource contention and risks Out-of-Memory (OOM) failures. Accurate memory estimation is essential for robust collocation, while GPU utilization -- a key proxy for resource contention -- enables interference-aware scheduling to reduce slowdowns and improve throughput. Existing GPU memory estimators span three paradigms -- analytical models, CPU-side libraries, and ML-based estimators -- each with distinct limitations: dependence on detailed model specifications, intrusive integration, poor generalization, and varying latency overhead. GPU heterogeneity further complicates estimation, as identical tasks can exhibit markedly different memory footprints across hardware generations. GPU utilization remains comparatively understudied, further complicated by the non-additive nature of utilization metrics and hardware sensitivity. We conduct a systematic analysis of representative estimators from each paradigm -- Horus, PyTorch FakeTensor, and our lightweight ML-based estimator -- evaluating accuracy, generalizability, and practical overhead. We construct a synthetic dataset spanning MLPs, CNNs, and Transformers with controlled architectural variations, and train MLP- and Transformer-based estimators for memory prediction. We further experiment with utilization estimation on the same dataset. Our evaluation reveals key tradeoffs and validates estimators against real-world unseen models. Significant challenges remain: analytical models are hardware-dependent, CPU-side libraries impose intrusive integration costs, and ML-based estimators struggle with cross-architecture generalization. We release all datasets, tools, and artifacts to support further research.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86GPU\u5185\u5b58\u4f30\u7b97\u7684\u4e09\u79cd\u8303\u5f0f\uff0c\u8bc4\u4f30\u4e86Horus\u3001PyTorch FakeTensor\u548c\u8f7b\u91cf\u7ea7ML\u4f30\u7b97\u5668\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u6027\u548c\u5b9e\u9645\u5f00\u9500\uff0c\u5e76\u63a2\u8ba8\u4e86GPU\u5229\u7528\u7387\u4f30\u7b97\u7684\u6311\u6218\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u5e76\u7f6e\u53ef\u63d0\u9ad8GPU\u5229\u7528\u7387\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8d44\u6e90\u7ade\u4e89\u5f15\u8d77\u7684\u663e\u8457\u51cf\u901f\u548c\u5185\u5b58\u6ea2\u51fa\u98ce\u9669\u3002\u51c6\u786e\u7684\u5185\u5b58\u4f30\u7b97\u5bf9\u4e8e\u7a33\u5065\u7684\u5e76\u7f6e\u81f3\u5173\u91cd\u8981\uff0c\u800cGPU\u5229\u7528\u7387\u4f5c\u4e3a\u8d44\u6e90\u7ade\u4e89\u7684\u5173\u952e\u4ee3\u7406\uff0c\u53ef\u5b9e\u73b0\u5e72\u6270\u611f\u77e5\u8c03\u5ea6\u4ee5\u51cf\u5c11\u51cf\u901f\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "method": "\u5bf9\u4e09\u79cd\u4ee3\u8868\u6027\u4f30\u7b97\u5668\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\uff1aHorus\uff08\u5206\u6790\u6a21\u578b\uff09\u3001PyTorch FakeTensor\uff08CPU\u7aef\u5e93\uff09\u548c\u8f7b\u91cf\u7ea7ML\u4f30\u7b97\u5668\u3002\u6784\u5efa\u5305\u542bMLP\u3001CNN\u548cTransformer\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3MLP\u548cTransformer\u57fa\u7840\u4f30\u7b97\u5668\u8fdb\u884c\u5185\u5b58\u9884\u6d4b\uff0c\u5e76\u5728\u76f8\u540c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5229\u7528\u7387\u4f30\u7b97\u5b9e\u9a8c\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u5173\u952e\u6743\u8861\uff1a\u5206\u6790\u6a21\u578b\u4f9d\u8d56\u786c\u4ef6\uff0cCPU\u7aef\u5e93\u5e26\u6765\u4fb5\u5165\u5f0f\u96c6\u6210\u6210\u672c\uff0cML\u4f30\u7b97\u5668\u5728\u8de8\u67b6\u6784\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u9a8c\u8bc1\u4e86\u4f30\u7b97\u5668\u5bf9\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4e16\u754c\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "GPU\u5185\u5b58\u4f30\u7b97\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5305\u62ec\u786c\u4ef6\u4f9d\u8d56\u6027\u3001\u96c6\u6210\u6210\u672c\u548c\u6cdb\u5316\u95ee\u9898\u3002GPU\u5229\u7528\u7387\u4f30\u7b97\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u4e14\u5b58\u5728\u975e\u52a0\u6027\u7279\u6027\u548c\u786c\u4ef6\u654f\u611f\u6027\u3002\u4f5c\u8005\u53d1\u5e03\u4e86\u6240\u6709\u6570\u636e\u96c6\u3001\u5de5\u5177\u548c\u5de5\u4ef6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2602.18357", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.18357", "abs": "https://arxiv.org/abs/2602.18357", "authors": ["Wallace Albertini", "Marina Cond\u00e9 Ara\u00fajo", "J\u00falia Cond\u00e9 Ara\u00fajo", "Antonio Pedro Santos Alves", "Marcos Kalinowski"], "title": "Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation", "comment": "Author version of the paper accepted for publication at CAIN 2026", "summary": "The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.", "AI": {"tldr": "\u63d0\u51faSCFC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u8bc4\u4f30AI\u7cfb\u7edf\u529f\u80fd\u6b63\u786e\u6027\uff0c\u8fde\u63a5\u4e1a\u52a1\u9700\u6c42\u4e0e\u7edf\u8ba1\u6307\u6807\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a\u5b9a\u4e49\u89c4\u683c\u9650\u3001\u5206\u5c42\u6982\u7387\u62bd\u6837\u3001\u81ea\u52a9\u6cd5\u4f30\u8ba1\u7f6e\u4fe1\u533a\u95f4\u3001\u8ba1\u7b97\u80fd\u529b\u6307\u6570\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u8d28\u91cf\u8bc4\u4f30\u9762\u4e34\u6311\u6218\uff0cISO/IEC 25059\u7b49\u6807\u51c6\u7f3a\u4e4f\u5b9e\u7528\u4e14\u7edf\u8ba1\u7a33\u5065\u7684\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002\u9700\u8981\u5c06\u4e1a\u52a1\u9700\u6c42\u4e0e\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u6d4b\u91cf\u8fde\u63a5\u8d77\u6765\u3002", "method": "\u63d0\u51fa\u7edf\u8ba1\u529f\u80fd\u6b63\u786e\u6027\u7f6e\u4fe1\u5ea6\uff08SCFC\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u56db\u4e2a\u6b65\u9aa4\uff1a1) \u5b9a\u4e49\u5b9a\u91cf\u89c4\u683c\u9650\uff1b2) \u8fdb\u884c\u5206\u5c42\u548c\u6982\u7387\u62bd\u6837\uff1b3) \u5e94\u7528\u81ea\u52a9\u6cd5\u4f30\u8ba1\u6027\u80fd\u6307\u6807\u7684\u7f6e\u4fe1\u533a\u95f4\uff1b4) \u8ba1\u7b97\u80fd\u529b\u6307\u6570\u4f5c\u4e3a\u6700\u7ec8\u6307\u6807\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u5de5\u4e1a\u754c\u771f\u5b9eAI\u7cfb\u7edf\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u8bc4\u4f30\uff0c\u91c7\u8bbfAI\u4e13\u5bb6\u6536\u96c6\u4e86\u5173\u4e8e\u65b9\u6cd5\u5b9e\u7528\u6027\u3001\u6613\u7528\u6027\u548c\u5b9e\u9645\u91c7\u7528\u610f\u5411\u7684\u5b9d\u8d35\u89c1\u89e3\u3002\u65b9\u6cd5\u88ab\u8bc1\u660e\u53ef\u884c\u4e14\u6709\u4ef7\u503c\u3002", "conclusion": "SCFC\u65b9\u6cd5\u662f\u64cd\u4f5c\u5316\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u4f30\u7684\u53ef\u884c\u4e14\u6709\u4ef7\u503c\u7684\u65b9\u5f0f\uff0c\u5c06\u8bc4\u4f30\u4ece\u70b9\u4f30\u8ba1\u8f6c\u53d8\u4e3a\u7edf\u8ba1\u7f6e\u4fe1\u5ea6\u9648\u8ff0\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6807\u51c6\u7684\u5b9e\u8df5\u7a7a\u767d\u3002"}}
{"id": "2602.17834", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.17834", "abs": "https://arxiv.org/abs/2602.17834", "authors": ["Duncan Adamson", "Will Rosenbaum", "Paul G. Spirakis"], "title": "Distributed Triangle Enumeration in Hypergraphs", "comment": null, "summary": "In the last decade, subgraph detection and enumeration have emerged as a central problem in distributed graph algorithms. This is largely due to the theoretical challenges and practical applications of these problems. In this paper, we initiate the systematic study of distributed sub-hypergraph enumeration in hypergraphs. To this end, we (1)~introduce several computational models for hypergraphs that generalize the CONGEST model for graphs and evaluate their relative computational power, (2)~devise algorithms for distributed triangle enumeration in our computational models and prove their optimality in two such models, (3)~introduce classes of sparse and ``everywhere sparse'' hypergraphs and describe efficient distributed algorithms for triangle enumeration in these classes, and (4)~describe general techniques that we believe to be useful for designing efficient algorithms in our hypergraph models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u5206\u5e03\u5f0f\u8d85\u56fe\u4e2d\u7684\u5b50\u8d85\u56fe\u679a\u4e3e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8d85\u56fe\u8ba1\u7b97\u6a21\u578b\u3001\u8bbe\u8ba1\u4e86\u4e09\u89d2\u5f62\u679a\u4e3e\u7b97\u6cd5\u5e76\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\uff0c\u9488\u5bf9\u7a00\u758f\u8d85\u56fe\u5f00\u53d1\u4e86\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u901a\u7528\u8bbe\u8ba1\u6280\u672f\u3002", "motivation": "\u8fc7\u53bb\u5341\u5e74\u4e2d\uff0c\u5b50\u56fe\u68c0\u6d4b\u548c\u679a\u4e3e\u5df2\u6210\u4e3a\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u65e2\u6709\u7406\u8bba\u6311\u6218\u53c8\u6709\u5b9e\u9645\u5e94\u7528\u3002\u7136\u800c\uff0c\u8d85\u56fe\u4e2d\u7684\u5206\u5e03\u5f0f\u5b50\u8d85\u56fe\u679a\u4e3e\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1) \u5f15\u5165\u591a\u4e2a\u8d85\u56fe\u8ba1\u7b97\u6a21\u578b\uff0c\u63a8\u5e7f\u4e86\u56fe\u7684CONGEST\u6a21\u578b\u5e76\u8bc4\u4f30\u5176\u8ba1\u7b97\u80fd\u529b\uff1b2) \u5728\u8fd9\u4e9b\u6a21\u578b\u4e2d\u8bbe\u8ba1\u5206\u5e03\u5f0f\u4e09\u89d2\u5f62\u679a\u4e3e\u7b97\u6cd5\uff0c\u5e76\u5728\u4e24\u4e2a\u6a21\u578b\u4e2d\u8bc1\u660e\u5176\u6700\u4f18\u6027\uff1b3) \u5b9a\u4e49\u7a00\u758f\u548c\"\u5904\u5904\u7a00\u758f\"\u8d85\u56fe\u7c7b\uff0c\u5e76\u63cf\u8ff0\u8fd9\u4e9b\u7c7b\u4e2d\u4e09\u89d2\u5f62\u679a\u4e3e\u7684\u9ad8\u6548\u5206\u5e03\u5f0f\u7b97\u6cd5\uff1b4) \u63d0\u51fa\u9002\u7528\u4e8e\u8d85\u56fe\u6a21\u578b\u7684\u901a\u7528\u7b97\u6cd5\u8bbe\u8ba1\u6280\u672f\u3002", "result": "\u5efa\u7acb\u4e86\u8d85\u56fe\u5206\u5e03\u5f0f\u8ba1\u7b97\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5f00\u53d1\u4e86\u6700\u4f18\u7684\u4e09\u89d2\u5f62\u679a\u4e3e\u7b97\u6cd5\uff0c\u4e3a\u7a00\u758f\u8d85\u56fe\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f62\u6210\u4e86\u53ef\u7528\u4e8e\u672a\u6765\u7b97\u6cd5\u8bbe\u8ba1\u7684\u901a\u7528\u6280\u672f\u5de5\u5177\u96c6\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f00\u521b\u4e86\u5206\u5e03\u5f0f\u8d85\u56fe\u5b50\u8d85\u56fe\u679a\u4e3e\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u4e3a\u8fd9\u4e00\u65b0\u5174\u9886\u57df\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u7a00\u758f\u8d85\u56fe\u7b49\u7279\u6b8a\u7ed3\u6784\u4e2d\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18007", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18007", "abs": "https://arxiv.org/abs/2602.18007", "authors": ["Jon Hu", "Thomas Jia", "Jing Zhu", "Zhendong Yu"], "title": "Joint Training on AMD and NVIDIA GPUs", "comment": null, "summary": "As large language models continue to scale, training demands on compute and system capacity grow rapidly, making single-vendor homogeneous clusters insufficient. This paper presents a technical solution for heterogeneous mixed training in AMD-NVIDIA environments. We first adopt a compatibility-oriented approach based on CPU-Forwarding Communication, with differentiated communication back-end selection across parallel groups and multi-NIC parallel data transfer. To achieve higher performance, we further propose another Device-Direct Communication approach, integrating a CPU-offloading P2P mechanism to enable direct cross-vendor GPU data transfer without host-memory staging. Experiments on LLaMA-8B and Qwen2-7B demonstrate that the proposed Device-Direct Communication approach achieves up to 98% of the throughput of an NVIDIA homogeneous system, while preserving training stability and correctness.", "AI": {"tldr": "\u63d0\u51fa\u5728AMD-NVIDIA\u5f02\u6784\u73af\u5883\u4e2d\u8fdb\u884c\u6df7\u5408\u8bad\u7ec3\u7684\u6280\u672f\u65b9\u6848\uff0c\u5305\u62ec\u517c\u5bb9\u6027\u5bfc\u5411\u7684CPU\u8f6c\u53d1\u901a\u4fe1\u548c\u6027\u80fd\u5bfc\u5411\u7684\u8bbe\u5907\u76f4\u8fde\u901a\u4fe1\u4e24\u79cd\u65b9\u6cd5\uff0c\u540e\u8005\u80fd\u8fbe\u5230NVIDIA\u540c\u6784\u7cfb\u7edf98%\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u8bad\u7ec3\u5bf9\u8ba1\u7b97\u548c\u7cfb\u7edf\u5bb9\u91cf\u7684\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u5355\u4e00\u4f9b\u5e94\u5546\u7684\u540c\u6784\u96c6\u7fa4\u5df2\u4e0d\u8db3\u591f\uff0c\u9700\u8981\u89e3\u51b3AMD-NVIDIA\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6df7\u5408\u8bad\u7ec3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u517c\u5bb9\u6027\u5bfc\u5411\u7684CPU\u8f6c\u53d1\u901a\u4fe1\uff0c\u91c7\u7528\u8de8\u5e76\u884c\u7ec4\u7684\u5dee\u5f02\u5316\u901a\u4fe1\u540e\u7aef\u9009\u62e9\u548c\u591aNIC\u5e76\u884c\u6570\u636e\u4f20\u8f93\uff1b2) \u6027\u80fd\u5bfc\u5411\u7684\u8bbe\u5907\u76f4\u8fde\u901a\u4fe1\uff0c\u96c6\u6210CPU\u5378\u8f7d\u7684P2P\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u9700\u4e3b\u673a\u5185\u5b58\u4e2d\u8f6c\u7684\u76f4\u63a5\u8de8\u4f9b\u5e94\u5546GPU\u6570\u636e\u4f20\u8f93\u3002", "result": "\u5728LLaMA-8B\u548cQwen2-7B\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u8bbe\u5907\u76f4\u8fde\u901a\u4fe1\u65b9\u6cd5\u80fd\u8fbe\u5230NVIDIA\u540c\u6784\u7cfb\u7edf98%\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6b63\u786e\u6027\u3002", "conclusion": "\u8be5\u6280\u672f\u65b9\u6848\u6210\u529f\u89e3\u51b3\u4e86AMD-NVIDIA\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6df7\u5408\u8bad\u7ec3\u95ee\u9898\uff0c\u8bbe\u5907\u76f4\u8fde\u901a\u4fe1\u65b9\u6cd5\u5728\u4fdd\u6301\u517c\u5bb9\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u63a5\u8fd1\u540c\u6784\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5f02\u6784\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18158", "categories": ["cs.DC", "cs.ET"], "pdf": "https://arxiv.org/pdf/2602.18158", "abs": "https://arxiv.org/abs/2602.18158", "authors": ["Andreas Kouloumpris", "Georgios L. Stavrinides", "Maria K. Michael", "Theocharis Theocharides"], "title": "A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum", "comment": "This version of the manuscript has been accepted for publication in Future Generation Computer Systems after peer review (Author Accepted Manuscript). It is not the final published version (Version of Record) and does not reflect any post-acceptance improvements. The Version of Record is available online at https://doi.org/10.1016/j.future.2026.108414", "summary": "A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7cbe\u786e\u7684\u591a\u76ee\u6807\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18-\u4e2d\u5fc3-\u4e91\u67b6\u6784\u4e2d\u8054\u5408\u4f18\u5316\u5de5\u4f5c\u6d41\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u5ef6\u8fdf\uff0c\u901a\u8fc7\u4e8c\u8fdb\u5236\u6574\u6570\u7ebf\u6027\u89c4\u5212\u5b9e\u73b0\u6700\u4f18\u89e3\u3002", "motivation": "\u5173\u952e\u5de5\u4f5c\u6d41\u5e94\u7528\u91c7\u7528\u7b80\u5316\u7684\u8fb9\u7f18-\u4e2d\u5fc3-\u4e91\u67b6\u6784\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5168\u9762\u89e3\u51b3\u8bbe\u5907\u9650\u5236\u548c\u591a\u6837\u5316\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u4efb\u52a1\u5206\u914d\u6311\u6218\uff0c\u7279\u522b\u662f\u53ef\u9760\u6027\u548c\u5ef6\u8fdf\u8fd9\u4e24\u4e2a\u51b2\u7a81\u76ee\u6807\u9700\u8981\u7cbe\u786e\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u7cbe\u786e\u7684\u591a\u76ee\u6807\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u91c7\u7528\u4e8c\u8fdb\u5236\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08BILP\uff09\u516c\u5f0f\uff0c\u8003\u8651\u5404\u76ee\u6807\u7684\u76f8\u5bf9\u91cd\u8981\u6027\uff0c\u6574\u5408\u65f6\u95f4\u5197\u4f59\u6280\u672f\uff0c\u5e76\u7eb3\u5165\u76f8\u5173\u7814\u7a76\u4e2d\u5e38\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u7ea6\u675f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u7b56\u7565\uff0c\u5e73\u5747\u53ef\u9760\u6027\u63d0\u534784.19%\uff0c\u5ef6\u8fdf\u6539\u558449.81%\uff1b\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6240\u6709\u5de5\u4f5c\u6d41\u7684\u5e73\u5747\u8fd0\u884c\u65f6\u95f4\u57280.03\u523050.94\u79d2\u4e4b\u95f4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7b80\u5316\u7684\u8fb9\u7f18-\u4e2d\u5fc3-\u4e91\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7cbe\u786e\u4efb\u52a1\u5206\u914d\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5e73\u8861\u5173\u952e\u5de5\u4f5c\u6d41\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u5ef6\u8fdf\u9700\u6c42\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.18188", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18188", "abs": "https://arxiv.org/abs/2602.18188", "authors": ["Antonio Cruciani", "Avinandan Das", "Alesya Raevskaya", "Jukka Suomela"], "title": "It does not matter how you define locally checkable labelings", "comment": "40 pages", "summary": "Locally checkable labeling problems (LCLs) form the foundation of the modern theory of distributed graph algorithms. First introduced in the seminal paper by Naor and Stockmeyer [STOC 1993], these are graph problems that can be described by listing a finite set of valid local neighborhoods. This seemingly simple definition strikes a careful balance between two objectives: they are a family of problems that is broad enough so that it captures numerous problems that are of interest to researchers working in this field, yet restrictive enough so that it is possible to prove strong theorems that hold for all LCL problems. In particular, the distributed complexity landscape of LCL problems is now very well understood.\n  In this work we show that the family of LCL problems is extremely robust to variations. We present a very restricted family of locally checkable problems (essentially, the \"node-edge checkable\" formalism familiar from round elimination, restricted to regular unlabeled graphs); most importantly, such problems cannot directly refer to e.g. the existence of short cycles. We show that one can translate between the two formalisms (there are local reductions in both directions that only need access to a symmetry-breaking oracle, and hence the overhead is at most an additive $O(\\log^* n)$ rounds in the LOCAL model).", "AI": {"tldr": "LCL\u95ee\u9898\u5728\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u4e2d\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5373\u4f7f\u9650\u5236\u4e3a\u66f4\u7b80\u5355\u7684\"\u8282\u70b9-\u8fb9\u53ef\u68c0\u67e5\"\u5f62\u5f0f\uff0c\u4e5f\u80fd\u901a\u8fc7\u5c40\u90e8\u7ea6\u7b80\u4e0e\u6807\u51c6LCL\u76f8\u4e92\u8f6c\u6362\uff0c\u4ec5\u9700\u5bf9\u79f0\u6027\u6253\u7834\u9884\u8a00\u673a\uff0c\u5728LOCAL\u6a21\u578b\u4e2d\u6700\u591a\u589e\u52a0O(log* n)\u8f6e\u5f00\u9500\u3002", "motivation": "\u7814\u7a76LCL\u95ee\u9898\u7684\u9c81\u68d2\u6027\uff0c\u63a2\u7d22\u5728\u66f4\u4e25\u683c\u9650\u5236\u4e0b\uff08\u5982\u8282\u70b9-\u8fb9\u53ef\u68c0\u67e5\u5f62\u5f0f\u3001\u6b63\u5219\u65e0\u6807\u7b7e\u56fe\uff09\u7684LCL\u95ee\u9898\u662f\u5426\u4ecd\u80fd\u4fdd\u6301\u4e0e\u6807\u51c6LCL\u95ee\u9898\u7684\u7b49\u4ef7\u6027\uff0c\u4ee5\u9a8c\u8bc1LCL\u6846\u67b6\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u7ea6\u7b80\u65b9\u6cd5\uff0c\u5728\u4e24\u79cd\u5f62\u5f0f\u5316\u4e4b\u95f4\u5efa\u7acb\u53cc\u5411\u8f6c\u6362\uff1a\u4ece\u6807\u51c6LCL\u5230\u53d7\u9650\u7684\u8282\u70b9-\u8fb9\u53ef\u68c0\u67e5\u5f62\u5f0f\uff0c\u4ee5\u53ca\u53cd\u5411\u8f6c\u6362\u3002\u8fd9\u4e9b\u7ea6\u7b80\u4ec5\u9700\u8bbf\u95ee\u5bf9\u79f0\u6027\u6253\u7834\u9884\u8a00\u673a\uff0c\u5728LOCAL\u6a21\u578b\u4e2d\u5b9e\u73b0\u3002", "result": "\u8bc1\u660e\u4e24\u79cd\u5f62\u5f0f\u5316\u4e4b\u95f4\u5b58\u5728\u5c40\u90e8\u7ea6\u7b80\u5173\u7cfb\uff0c\u8f6c\u6362\u5f00\u9500\u6700\u591a\u4e3aO(log* n)\u8f6e\uff0c\u8868\u660eLCL\u95ee\u9898\u5bb6\u65cf\u5bf9\u5f62\u5f0f\u5316\u53d8\u5316\u5177\u6709\u6781\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LCL\u95ee\u9898\u6846\u67b6\u975e\u5e38\u7a33\u5b9a\uff0c\u5373\u4f7f\u9650\u5236\u5230\u66f4\u7b80\u5355\u7684\u8282\u70b9-\u8fb9\u53ef\u68c0\u67e5\u5f62\u5f0f\uff0c\u4ecd\u80fd\u4fdd\u6301\u4e0e\u6807\u51c6LCL\u7684\u7b49\u4ef7\u6027\uff0c\u8fd9\u8fdb\u4e00\u6b65\u5de9\u56fa\u4e86LCL\u4f5c\u4e3a\u5206\u5e03\u5f0f\u56fe\u7b97\u6cd5\u7406\u8bba\u57fa\u7840\u7684\u5730\u4f4d\u3002"}}
{"id": "2602.18287", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.18287", "abs": "https://arxiv.org/abs/2602.18287", "authors": ["Andrea D'Iapico", "Monica Vitali"], "title": "Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum", "comment": null, "summary": "The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7eff\u8272\u7ea6\u675f\u7684\u4e91\u8fb9\u5e94\u7528\u81ea\u52a8\u90e8\u7f72\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u5206\u6790\u80fd\u8017\u6a21\u5f0f\u3001\u7ec4\u4ef6\u901a\u4fe1\u548c\u57fa\u7840\u8bbe\u65bd\u73af\u5883\u7279\u5f81\uff0c\u751f\u6210\u73af\u4fdd\u7684\u90e8\u7f72\u7b56\u7565\u3002", "motivation": "\u4fe1\u606f\u6280\u672f\u73af\u5883\u53ef\u6301\u7eed\u6027\u6210\u4e3a\u5173\u952e\u95ee\u9898\uff0c\u4e91\u8fb9\u8fde\u7eed\u4f53\u5e94\u7528\u9700\u8981\u80fd\u6548\u90e8\u7f72\u7b56\u7565\uff0c\u8003\u8651\u8ba1\u7b97\u9700\u6c42\u548c\u8282\u70b9\u73af\u5883\u5f71\u54cd\uff0c\u4f46\u52a8\u6001\u56e0\u7d20\uff08\u5e94\u7528\u884c\u4e3a\u6ce2\u52a8\u3001\u8282\u70b9\u78b3\u5f3a\u5ea6\u53d8\u5316\uff09\u4f7f\u89c4\u5212\u590d\u6742\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u751f\u6210\u7eff\u8272\u7ea6\u675f\u6307\u5bfc\u7684\u90e8\u7f72\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u5206\u6790\u80fd\u8017\u6a21\u5f0f\u3001\u7ec4\u4ef6\u95f4\u901a\u4fe1\u548c\u57fa\u7840\u8bbe\u65bd\u73af\u5883\u7279\u5f81\uff0c\u6784\u5efa\u7eff\u8272\u611f\u77e5\u7ea6\u675f\u96c6\uff0c\u5e76\u5229\u7528\u76d1\u63a7\u6570\u636e\u81ea\u52a8\u5b66\u4e60\u548c\u66f4\u65b0\u7ea6\u675f\u3002", "result": "\u5728\u4e91\u539f\u751f\u5e94\u7528\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u80fd\u6e90\u6d88\u8017\u548c\u76f8\u5173\u6392\u653e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u3001\u80fd\u6e90\u611f\u77e5\u7684\u7f16\u6392\uff0c\u80fd\u591f\u751f\u6210\u73af\u4fdd\u7684\u90e8\u7f72\u8ba1\u5212\uff0c\u4e3a\u4e91\u8fb9\u8fde\u7eed\u4f53\u5e94\u7528\u7684\u73af\u5883\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
