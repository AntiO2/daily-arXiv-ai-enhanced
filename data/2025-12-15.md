<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 6]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 自动生成的测试比人工测试有更高的分支覆盖率，但在基于频谱的故障定位（SBFL）得分上较低，特别是在深度嵌套的代码结构中


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注测试覆盖率指标，很少评估测试对故障定位的支持效果，特别是使用变异测试引入的人工故障。本研究旨在通过SBFL分数这一新指标，比较自动生成测试和人工创建测试的优劣

Method: 使用SBFL分数和代码覆盖率作为评估指标，比较自动生成测试和人工创建测试。SBFL分数表示使用频谱基于故障定位技术准确定位故障的能力

Result: 自动生成的测试比人工创建的测试获得更高的分支覆盖率，但SBFL分数较低，特别是在深度嵌套结构的代码中

Conclusion: 研究结果为如何有效结合自动生成测试和人工创建测试提供了指导，强调了在评估测试质量时需要考虑故障定位能力而不仅仅是覆盖率

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [2] [AutoFSM: A Multi-agent Framework for FSM Code Generation with IR and SystemC-Based Testing](https://arxiv.org/abs/2512.11398)
*Qiuming Luo,Yanming Lei,Kunzhong Wu,Yixuan Cao,Chengjian Liu*

Main category: cs.SE

TL;DR: AutoFSM是一个用于FSM代码生成的多智能体协作框架，通过结构化中间表示降低语法错误率，结合SystemC建模和自动测试平台生成，在SKT-FSM基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成Verilog代码用于有限状态机控制逻辑时面临语法错误频繁、调试效率低、过度依赖测试基准等问题，需要更可靠的硬件设计代码生成方案。

Method: 提出AutoFSM多智能体协作框架，引入结构清晰的中间表示来降低语法错误，提供从IR到Verilog的自动转换工具链，集成SystemC建模与自动测试平台生成。

Result: 在SKT-FSM基准测试中，AutoFSM相比开源框架MAGE在相同基础LLM下，通过率提升最高达11.94%，语法错误率降低最高达17.62%。

Conclusion: LLM结合结构化中间表示和自动化测试能显著提高RTL代码生成的可靠性和可扩展性，为硬件设计自动化提供了有效解决方案。

Abstract: With the rapid advancement of large language models (LLMs) in code generation, their applications in hardware design are receiving growing attention. However, existing LLMs face several challenges when generating Verilog code for finite state machine (FSM) control logic, including frequent syntax errors, low debugging efficiency, and heavy reliance on test benchmarks. To address these challenges, this paper proposes AutoFSM, a multi-agent collaborative framework designed for FSM code generation tasks. AutoFSM introduces a structurally clear intermediate representation (IR) to reduce syntax error rate during code generation and provides a supporting toolchain to enable automatic translation from IR to Verilog. Furthermore, AutoFSM is the first to integrate SystemC-based modeling with automatic testbench generation, thereby improving debugging efficiency and feedback quality. To systematically evaluate the framework's performance, we construct SKT-FSM, the first hierarchical FSM benchmark in the field, comprising 67 FSM samples across different complexity levels. Experimental results show that, under the same base LLM, AutoFSM consistently outperforms the open-source framework MAGE on the SKT-FSM benchmark, achieving up to an 11.94% improvement in pass rate and up to a 17.62% reduction in syntax error rate. These results demonstrate the potential of combining LLMs with structured IR and automated testing to improve the reliability and scalability of register-transfer level (RTL) code generation.

</details>


### [3] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 研究评估19个小型量化LLM在C到Java代码翻译任务中的表现，发现只有3个模型能通过50%以上的测试，大多数模型完全失败，揭示了当前量化模型在复杂C语言概念上的推理能力天花板。


<details>
  <summary>Details</summary>
Motivation: C到Java的自动翻译是一个极具挑战性的任务，涉及范式转换（过程式vs面向对象）、内存模型（手动指针vs垃圾回收）和不兼容的数据类型。研究旨在评估小型量化LLM在此任务上的实际效能。

Method: 采用新颖的混合管道方法：利用抽象语法树（AST）进行语义分解，并采用高度约束的基于规则的提示策略。评估了19个参数少于200亿的小型量化LLM。

Result: 结果呈现明显的三级性能分化：Tier 3模型（如llama3.1、gemma3、starcoder2）100%测试失败；Tier 2模型（如mistral-nemo、mistral）能生成可运行代码但存在危险的语义错误；只有Tier 1模型（phi4、deepseek-coder-v2、codeqwen）表现可行，通过超过50%的测试，但在函数指针、sizeof和枚举逻辑等复杂C概念上仍然失败。

Conclusion: 当前量化LLM在C到Java翻译任务中存在严重局限性，只有极少数模型表现可行，且在最复杂的C语言概念上存在硬性推理能力天花板，表明需要更先进的模型架构或方法来解决这一挑战性任务。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [4] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 首次系统评估差分隐私在代码大语言模型中的应用，发现DP能显著减少记忆化风险，同时保持甚至提升代码生成能力，且不影响训练效率。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型可能无意中记忆并复制训练数据中的代码片段，导致隐私泄露和知识产权侵权风险，限制了其在敏感领域的应用和训练数据来源。

Method: 应用差分隐私技术，首先分析代码大语言模型在微调过程中的记忆化行为原因，然后实证评估DP在减少记忆化同时保持代码生成能力的效果。

Result: DP显著减少了所有测试代码片段类型的记忆化，对最易记忆化的片段类型效果最好；虽然略微增加困惑度，但保持甚至提升了代码生成能力；不影响训练时间和能耗。

Conclusion: 差分隐私是保护代码大语言模型隐私的实用选择，能在不显著影响模型性能的情况下有效减少记忆化风险，为敏感领域部署提供了可行方案。

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


### [5] [Mini-SFC: A Comprehensive Simulation Framework for Orchestration and Management of Service Function Chains](https://arxiv.org/abs/2512.11527)
*Xi Wang,Shuo Shi,Chenyu Wu*

Main category: cs.SE

TL;DR: 本文介绍了Mini-SFC，一个支持数值和容器虚拟仿真的模块化SFC仿真框架，支持在线动态拓扑调整，旨在解决现有SFC仿真工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 在云计算和网络环境不断演进的背景下，服务功能链（SFC）因其灵活的部署能力在网络复杂服务实现中至关重要。现有SFC仿真工具存在局限性，需要更灵活、易用的仿真平台来支持算法验证和服务部署验证。

Method: 提出Mini-SFC模块化仿真框架，支持数值仿真和基于容器的虚拟仿真，具备在线动态拓扑调整能力。通过简化模块设计和提供标准化求解器接口，降低学习曲线。

Result: Mini-SFC作为开源平台，强调用户友好性，能够显著缩短研究者的学习曲线，并为高级SFC管理和优化提供所需的灵活性和可扩展性。

Conclusion: Mini-SFC是一个有效的SFC仿真解决方案，解决了现有工具的局限性，为研究人员提供了快速算法验证和现实服务部署验证的平台，具有实际应用价值。

Abstract: In the continuously evolving cloud computing and network environment, service function chain (SFC) plays a crucial role in implementing complex services in the network with its flexible deployment capabilities. To address the limitations of existing SFC simulation tools, this paper introduces Mini-SFC, a modular simulation framework that supports both numerical and container-based virtual simulations, while also supporting online dynamic topology adjustments. As an open-source platform emphasizing user-friendliness, Mini-SFC facilitates rapid algorithm verification and realistic service deployment validation. By simplifying module design and providing standardized solver interfaces, Mini-SFC significantly shortens the learning curve for researchers and enhances the flexibility and scalability required for advanced SFC management and optimization. For readers interested in exploring or utilizing Mini-SFC, more information is available on the official project page.

</details>


### [6] [A Study of Library Usage in Agent-Authored Pull Requests](https://arxiv.org/abs/2512.11589)
*Lukas Twist*

Main category: cs.SE

TL;DR: AI编程代理在29.5%的PR中导入库，但仅1.3%添加新依赖，且75%指定版本号，比直接使用LLM的版本控制实践更好


<details>
  <summary>Details</summary>
Motivation: 尽管AI编程代理能够完成端到端的软件开发工作流，包括提交PR，但我们对其如何使用库（真实软件开发的核心部分）知之甚少

Method: 基于AIDev数据集的26,760个代理编写的PR，研究三个问题：代理导入库的频率、引入新依赖的频率（及版本控制情况）、选择的特定库

Result: 代理经常导入库（29.5%的PR），但很少添加新依赖（1.3%）；添加依赖时遵循良好的版本控制实践（75%指定版本），优于直接使用LLM；代理使用的外部库集合非常多样化

Conclusion: 研究提供了AI编程代理如何与当前软件生态系统交互的早期实证视角，显示代理在库使用方面展现出多样性和良好的版本控制实践

Abstract: Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited "library preferences" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 提出一种基于资源利用率的任务调度算法，通过优化CPU、磁盘和I/O利用率来降低云服务能耗


<details>
  <summary>Details</summary>
Motivation: 云服务能耗巨大，传统能源有限且对环境有温室效应，需要开发节能算法。现有研究只关注平均资源利用率或最小化完成时间，未考虑物理机中不同类型的资源。

Method: 提出一种任务调度算法，通过综合考虑CPU、磁盘、I/O利用率和任务处理时间的适应度函数，显式优化多种资源利用率，从而提高活动资源利用率。

Result: 通过合成数据集进行仿真实验，与现有MaxUtil算法对比，结果显示提出的算法更节能，能耗更低。

Conclusion: 提出的算法通过显式优化多种资源利用率，实现了更好的能源效率，为云服务节能提供了有效解决方案。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [8] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX是一个AI系统，能够大规模生成功能正确的Triton PyTorch ATen内核，专注于覆盖率和正确性，而非仅优化少数高性能内核。


<details>
  <summary>Details</summary>
Motivation: 为新兴加速器平台快速生成完整的PyTorch ATen后端，解决传统方法只关注少数高性能内核而缺乏全面覆盖的问题。

Method: 集成开源大语言模型，结合自定义代码检查器、JIT编译和基于PyTorch OpInfo的测试框架，支持真实MTIA芯片和硬件模拟环境。

Result: 成功为481个独特的ATen算子生成内核和包装器，通过所有对应的PyTorch OpInfo测试（总计超过20,000个）。

Conclusion: TritorX能够在一夜之间为新加速器平台生成完整的PyTorch ATen后端，显著提升了算子覆盖率和开发效率。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [9] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文对实时迁移技术进行了全面综述，重点关注容器和虚拟机迁移，分析技术挑战、采用差异及实际应用限制。


<details>
  <summary>Details</summary>
Motivation: 现有综述常忽略实时迁移在实际应用中的关键技术细节和实际挑战。本文旨在填补这一空白，通过多维度综合分析，为实际部署提供指导。

Method: 整合现有综述内容，从迁移技术、迁移单元和基础设施特性三个维度进行综合分析，重点关注容器和虚拟机两种迁移方式。

Result: 实时迁移虽然技术成熟，但其对系统多因素的依赖导致实际部署面临挑战，在某些情况下复杂性和资源需求超过收益。容器和虚拟机迁移在采用程度上存在显著差异。

Conclusion: 本文为实时迁移技术提供了全面资源，指出了当前技术挑战，并为未来研究和开发方向提供了指导，旨在促进该技术在不同计算环境中的实际应用。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [10] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 论文系统评估GPU集群利用率低的问题，提出三种动态调度器(HPS、PBS、SBS)，在64-GPU集群模拟中显著提升利用率、吞吐量并减少饥饿问题


<details>
  <summary>Details</summary>
Motivation: GPU集群在AI训练和部署中至关重要，但实际部署的平均利用率仅约50%，主要原因是碎片化、异构工作负载和静态调度策略的限制

Method: 提出三种专用动态调度器：混合优先级(HPS)、预测性回填(PBS)和智能批处理(SBS)，在包含1000个AI作业的64-GPU、8节点集群上进行控制模拟评估

Result: 静态基线(FIFO等)GPU利用率为45-67%，吞吐量12.5-18.3作业/小时，156个作业等待超30分钟；动态调度器显著优于静态策略，HPS达到78.2%利用率、25.8作业/小时，饥饿作业减少至12个

Conclusion: 有针对性的透明调度策略能显著提高异构AI集群的GPU效率，为未来生产调度框架提供实用基础，动态多目标调度器在所有关键指标上持续优于单目标启发式方法

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [11] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 该研究比较了集中式分层联邦学习（HFL）与去中心化聚合联邦学习（AFL）和去中心化持续联邦学习（CFL）架构，发现去中心化方法在多个性能指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 集中式分层联邦学习（HFL）面临通信瓶颈和隐私问题，而去中心化方法（AFL和CFL）通过分布式计算和聚合提供了有前景的替代方案，需要系统比较这些方法的性能差异。

Method: 研究对HFL、AFL和CFL三种联邦学习架构进行了全面比较，使用Fashion MNIST和MNIST数据集进行评估，重点关注精度、召回率、F1分数和平衡准确率等性能指标。

Result: 实验结果表明，去中心化的AFL和CFL在精度、召回率、F1分数和平衡准确率等多个指标上均优于集中式的HFL架构，证明了去中心化聚合机制的有效性。

Conclusion: 去中心化联邦学习方法（AFL和CFL）在协作模型训练中表现更优，为研究者和实践者提供了性能更好的选择，推动了联邦学习向去中心化方向发展。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [12] [Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI](https://arxiv.org/abs/2512.10990)
*Jianli Jin,Ziyang Lin,Qianli Dong,Yi Chen,Jayanth Srinivasa,Myungjin Lee,Zhaowei Tan,Fan Lai*

Main category: cs.DC

TL;DR: Dora是一个面向边缘AI训练和推理的QoE感知混合并行框架，通过异构感知模型分区、竞争感知网络调度和运行时适配器，在满足用户体验质量的同时提升执行效率和降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着边缘AI应用的普及，满足用户体验质量（QoE）要求（如模型推理延迟）成为首要目标。然而，现代AI模型通常超出单个设备的资源容量，需要在异构设备上进行分布式执行，而现有规划器主要优化吞吐量或设备利用率，忽略了QoE，导致资源效率低下或QoE违规。

Method: Dora通过三个关键机制实现联合优化：1）异构感知模型分区器，确定并分配模型分区到设备，形成紧凑的QoE合规计划集；2）竞争感知网络调度器，通过最大化计算通信重叠进一步优化候选计划；3）运行时适配器，自适应组合多个计划以最大化全局效率同时尊重整体QoE。

Result: 在代表性边缘部署场景（智能家居、交通分析、小型边缘集群）中，Dora实现了1.1-6.3倍的执行加速，或者将能耗降低21-82%，同时能在运行时动态变化下保持QoE。

Conclusion: Dora框架成功解决了边缘AI分布式执行中的QoE感知优化问题，通过联合优化异构计算、竞争网络和多维QoE目标，在保证用户体验的同时显著提升了系统效率和能源效率。

Abstract: With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.
  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.

</details>


### [13] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 论文提出三种GPU原生编译方法消除CPU-GPU数据传输延迟：并行传统编译、神经编译和混合架构，理论分析显示可实现10-100倍代码迭代加速。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统在编译、执行和测试阶段存在显著的CPU-GPU数据传输延迟瓶颈，限制了代码迭代效率。

Method: 提出三种互补的GPU原生编译方法：1) 适配GPU执行的并行传统编译；2) 使用学习序列到序列翻译和概率验证的神经编译；3) 结合两者的混合架构。

Result: 理论分析显示：传统GPU编译通过消除传输可获得2-5倍改进，神经编译通过大规模并行实现10-100倍加速，混合方法提供具有正确性保证的实用部署路径。

Conclusion: GPU原生编译能显著加速代码迭代周期，形式化的概率验证框架允许在编译准确性和并行探索间权衡，对自改进AI系统和未来模拟计算基板有重要影响。

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [14] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux是一个集群调度框架，通过跨集群编排回收强化学习后训练中因严格同步导致的空闲时间，提升硬件效率1.84倍。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练采用rollout-training分离架构，将内存密集型rollout和计算密集型训练分离到专用集群以提高硬件效率。但on-policy算法的严格同步要求导致严重的依赖气泡，使得一个集群在等待另一个集群运行时处于空闲状态。

Method: 提出RollMux集群调度框架，基于"一个作业的结构性空闲可被另一个作业的活动阶段利用"的洞察。引入协同执行组抽象，将集群划分为隔离的局部域，实现双层调度架构：组间调度器使用保守随机规划优化作业放置，组内调度器编排可证明最优的轮转调度。组抽象还施加驻留约束，确保大模型状态保持在主机内存中，实现"热启动"上下文切换。

Result: 在包含328个H20和328个H800 GPU的生产级测试平台上，RollMux相比标准分离架构提升成本效率1.84倍，相比最先进的共置基线提升1.38倍，同时实现100%的服务水平目标达成率。

Conclusion: RollMux通过跨集群编排有效回收了强化学习后训练中的依赖气泡，显著提升了硬件利用率，为大规模RL训练提供了高效的调度解决方案。

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [15] [Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging](https://arxiv.org/abs/2512.11512)
*Patrick D. Manya,Eugene M. Mbuyi,Gothy T. Ngoie,Jordan F. Masakuna*

Main category: cs.DC

TL;DR: 提出一种基于多包消息的分布式剪枝增强方法，显著减少大型网络中接近中心性计算的消息交换开销


<details>
  <summary>Details</summary>
Motivation: 现有分布式近似方法（如剪枝）在大规模复杂网络中通信开销高，无法有效减少数据包交换成本

Method: 引入多包消息技术，允许节点批量传输更大的整合数据块，减少消息数量并最小化数据丢失

Result: 多包方法在消息效率（更少总消息）和计算时间上显著优于原始剪枝技术，保持近似精度

Conclusion: 该方法为去中心化接近中心性计算提供了更可扩展和高效的解决方案，特别适用于大型网络和复杂包结构

Abstract: Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.

</details>


### [16] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax是一个移动端DNN推理加速框架，通过计算图分区、分支感知内存管理和自适应调度，在不修改模型的情况下实现高达46%的延迟降低和30%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上实时DNN应用需求增长，但现有框架在处理动态控制流操作符和不支持的核函数时表现不佳，导致CPU核心闲置、高延迟和内存峰值问题。

Method: 1) 将计算DAG分区以暴露并行性；2) 采用分支感知内存管理，使用专用内存池和缓冲区重用；3) 自适应调度器根据设备内存约束执行分支；4) 细粒度子图控制实现动态模型的异构推理。

Result: 在三种不同移动设备上评估五个代表性DNN，Parallax实现：高达46%的延迟降低，平均26.5%的内存开销控制，高达30%的能耗节省，优于现有最先进框架。

Conclusion: Parallax框架无需模型重构或自定义操作符实现，就能显著提升移动DNN推理性能，满足实时移动推理的响应性需求。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [17] [FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634)
*Elia Palme,Juan Pablo Dorsch,Ali Khosravi,Giovanni Pizzi,Francesco Pagnamenta,Andrea Ceriani,Eirini Koutsaniti,Rafael Sarmiento,Ivano Bonesana,Alejandro Dabin*

Main category: cs.DC

TL;DR: FirecREST v2是新一代用于HPC资源程序化访问的RESTful API，相比前代性能提升100倍，重点改进安全性和高吞吐量


<details>
  <summary>Details</summary>
Motivation: 重新设计FirecEST以解决代理式API在密集I/O操作中的性能瓶颈，集成增强的安全性和高吞吐量作为核心需求

Method: 采用系统化的性能测试方法，识别常见瓶颈，进行全面的架构重新设计，并引入关键的设计变更

Result: 实现了100倍的性能提升，通过独立的同行验证证明了改进效果，并识别了进一步优化的机会

Conclusion: FirecREST v2的成功重新设计为HPC资源的程序化访问提供了高性能、安全的解决方案，其经验教训对其他代理式API开发具有参考价值

Abstract: Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.
  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.

</details>


### [18] [Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643)
*Manideep Reddy Chinthareddy*

Main category: cs.DC

TL;DR: 提出一种基于容器网络属性的分布式ID生成方案，无需显式worker ID，适合云原生环境


<details>
  <summary>Details</summary>
Motivation: 传统Snowflake方案在容器化环境中需要手动分配或中心协调worker ID，这与无状态微服务的运维理念冲突，增加了复杂性

Method: 使用容器的私有IPv4地址作为节点唯一性来源，采用修改后的位分配方案(1-41-16-6)，其中16位来自网络熵，保持严格单调性

Result: 在AWS、GCP、Azure环境中验证，理论单节点上限约64,000 TPS，实际3节点集群约31,000 TPS，性能与传统方案相当但提供无限水平扩展能力

Conclusion: 该方案消除了对显式worker ID的依赖，实现了云原生环境下无状态、可无限扩展的分布式ID生成

Abstract: Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.
  This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.

</details>


### [19] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO是一个视频分析框架，通过识别经历相似数据漂移的摄像头并为其训练共享模型，显著降低计算和通信成本，实现资源高效的持续学习。


<details>
  <summary>Details</summary>
Motivation: 当前视频分析中为每个摄像头单独重新训练轻量级DNN模型的方法存在高计算和通信成本，难以扩展。数据漂移在相邻摄像头间常具有时空相关性，这为共享模型训练提供了机会。

Method: ECCO包含三个核心组件：1) 轻量级分组算法动态形成和更新摄像头组；2) GPU分配器动态分配GPU资源以提高重训练准确性并确保公平性；3) 每个摄像头的传输控制器配置帧采样并基于分配的GPU资源协调带宽共享。

Result: 在三个不同数据集上的两个视觉任务评估显示，ECCO在相同计算和通信资源下将重训练准确性提高了6.7%-18.1%，或在相同准确性下支持3.3倍更多的并发摄像头。

Conclusion: ECCO通过利用摄像头间数据漂移的时空相关性，实现了资源高效的持续学习，显著降低了视频分析系统的计算和通信成本，提高了可扩展性。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>


### [20] [Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775)
*Ayush Nainwal,Atharva Kamble,Nitin Awathare*

Main category: cs.DC

TL;DR: H-MPCs是一种基于超图的多方支付通道，通过集体资助的超边替代传统双边通道，实现无领导者的高并发链下支付，解决流动性碎片化和通道耗尽问题。


<details>
  <summary>Details</summary>
Motivation: 公共区块链吞吐量低、延迟高，现有支付通道网络存在流动性碎片化（资金锁定在单一通道无法复用）和通道耗尽问题，限制了路由效率和交易成功率。现有的多方通道方案依赖领导者或协调者，存在单点故障且跨通道支付灵活性有限。

Method: 提出基于超图的多方支付通道（H-MPCs），用集体资助的超边替代传统双边通道。通过可验证、提议者排序的有向无环图更新，实现完全并发的超边内和超边间支付，提供无领导者的支付机制。

Result: 在150个节点的网络上实现，交易成功率约94%，且没有HTLC过期或路由失败，证明了H-MPCs的鲁棒性。

Conclusion: H-MPCs通过超图结构解决了现有支付通道网络的流动性碎片化和通道耗尽问题，提供了比现有设计更高的灵活性和并发性，且无需依赖领导者，避免了单点故障。

Abstract: Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.
  We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.
  Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [21] [Query Optimization Beyond Data Systems: The Case for Multi-Agent Systems](https://arxiv.org/abs/2512.11001)
*Zoi Kaoudi,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 提出面向多智能体工作流的新一代查询优化框架愿景，旨在解决当前智能体架构缺乏通用性、可扩展性和系统优化的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的智能体工作流架构大多是临时性的，缺乏通用性、可扩展性和系统优化。现有系统通常依赖固定模型和单一执行引擎，无法有效优化跨异构数据源和查询引擎的多智能体操作。

Method: 提出重新设计查询优化原则以适应多智能体工作流的新挑战：多样化智能体编排、昂贵LLM调用下的成本效率、跨异构引擎优化以及任务冗余处理。通过真实案例分析和多智能体工作流分析，构建愿景架构。

Result: 提出了一个多智能体查询优化框架的愿景架构，能够实现自动化模型选择、工作流组合和跨异构引擎执行，为新兴多智能体架构中的查询优化奠定基础。

Conclusion: 该愿景为多智能体架构中的查询优化建立了基础，并开启了一系列未来研究方向，包括智能体编排、成本优化和跨引擎执行等挑战。

Abstract: The proliferation of large language models (LLMs) has accelerated the adoption of agent-based workflows, where multiple autonomous agents reason, invoke functions, and collaborate to compose complex data pipelines. However, current approaches to building such agentic architectures remain largely ad hoc, lacking generality, scalability, and systematic optimization. Existing systems often rely on fixed models and single execution engines and are unable to efficiently optimize multiple agents operating over heterogeneous data sources and query engines. This paper presents a vision for a next-generation query optimization framework tailored to multi-agent workflows. We argue that optimizing these workflows can benefit from redesigning query optimization principles to account for new challenges: orchestration of diverse agents, cost efficiency under expensive LLM calls and across heterogeneous engines, and redundancy across tasks. Led by a real-world example and building on an analysis of multi-agent workflows, we outline our envisioned architecture and the main research challenges of building a multi-agent query optimization framework, which aims at enabling automated model selection, workflow composition, and execution across heterogeneous engines. This vision establishes the groundwork for query optimization in emerging multi-agent architectures and opens up a set of future research directions.

</details>


### [22] [KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration](https://arxiv.org/abs/2512.11067)
*Guorui Xiao,Enhao Zhang,Nicole Sullivan,Will Hansen,Magdalena Balazinska*

Main category: cs.DB

TL;DR: KathDB是一个结合关系型语义与基础模型多模态推理能力的新型数据库系统，通过人机交互通道实现可解释的多模态查询


<details>
  <summary>Details</summary>
Motivation: 传统DBMS只能处理结构化数据且复杂SQL编写困难，而现有多模态系统要么需要用户手动创建机器学习UDF，要么完全依赖黑盒LLM，牺牲了可用性或可解释性

Method: 提出KathDB系统，将关系型语义与基础模型在多模态数据上的推理能力相结合，并在查询解析、执行和结果解释阶段引入人机交互通道

Result: 用户能够迭代地获取跨数据模态的可解释答案，实现了关系型数据库的语义保证与多模态推理能力的有效融合

Conclusion: KathDB通过创新的系统设计解决了传统数据库在多模态数据处理中的局限性，在保持可解释性的同时提升了多模态查询的可用性

Abstract: Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.

</details>


### [23] [Acyclic Conjunctive Regular Path Queries are no Harder than Corresponding Conjunctive Queries](https://arxiv.org/abs/2512.11129)
*Mahmoud Abo Khamis,Alexandru-Mihai Hurjui,Ahmet Kara,Dan Olteanu,Dan Suciu*

Main category: cs.DB

TL;DR: 提出一种输出敏感的算法用于评估无环连接正则路径查询(CRPQ)，其复杂度与输入大小、输出大小和查询的"自由连接分数超树宽度"参数相关，匹配了对应连接查询(CQ)的最佳已知复杂度。


<details>
  <summary>Details</summary>
Motivation: 虽然正则路径查询(RPQs)和连接正则路径查询(CRPQs)等价于递归Datalog程序，通常从复杂性角度理解较差，但研究者希望探索无环CRPQs的输出敏感评估是否能在不改进无环CQs现有技术的情况下得到优化。

Method: 提出了一种输出敏感算法，其复杂度基于输入大小、输出大小和查询的"自由连接分数超树宽度"参数。该算法改进了最近提出的无环CRPQs输出敏感算法。

Result: 算法复杂度与对应连接查询(CQ)的最佳已知输出敏感复杂度相匹配，这意味着无环CRPQs的递归特性在输出敏感分析中不会增加额外复杂度。

Conclusion: 无环CRPQs的递归方面不会在对应(非递归)CQs的基础上增加额外复杂度，至少在输出敏感分析方面如此。这一结果令人惊讶，因为RPQs和CRPQs通常等价于递归Datalog程序，而递归Datalog程序在复杂性方面通常理解较差。

Abstract: We present an output-sensitive algorithm for evaluating an acyclic Conjunctive Regular Path Query (CRPQ). Its complexity is written in terms of the input size, the output size, and a well-known parameter of the query that is called the "free-connex fractional hypertree width". Our algorithm improves upon the complexity of the recently introduced output-sensitive algorithm for acyclic CRPQs. More notably, the complexity of our algorithm for a given acyclic CRPQ Q matches the best known output-sensitive complexity for the "corresponding" conjunctive query (CQ), that is the CQ that has the same structure as the CRPQ Q except that each RPQ is replaced with a binary atom (or a join of two binary atoms). This implies that it is not possible to improve upon our complexity for acyclic CRPQs without improving the state-of-the-art on output-sensitive evaluation for acyclic CQs. Our result is surprising because RPQs, and by extension CRPQs, are equivalent to recursive Datalog programs, which are generally poorly understood from a complexity standpoint. Yet, our result implies that the recursion aspect of acyclic CRPQs does not add any extra complexity on top of the corresponding (non-recursive) CQs, at least as far as output-sensitive analysis is concerned.

</details>


### [24] [Benchmarking RL-Enhanced Spatial Indices Against Traditional, Advanced, and Learned Counterparts](https://arxiv.org/abs/2512.11161)
*Guanli Liu,Renata Borovica-Gajic,Hai Lan,Zhifeng Bao*

Main category: cs.DB

TL;DR: 首个模块化RLESI基准测试显示，尽管RLESI通过调优可降低查询延迟，但在查询效率和构建成本上均不如学习型空间索引和先进变体，高调优成本和有限泛化能力阻碍实际应用。


<details>
  <summary>Details</summary>
Motivation: 强化学习增强空间索引(RLESI)旨在提升索引构建时的查询效率，但缺乏统一实现和全面评估，尤其在磁盘环境下的实际效益不明确，需要系统化基准测试来验证其价值。

Method: 基于现有空间索引库构建模块化可扩展基准框架，解耦索引训练与构建，支持参数调优，实现对传统、先进和学习型空间索引的一致比较。在6个数据集上评估12种代表性空间索引，涵盖点查询、范围查询、kNN、空间连接和混合读写等多种工作负载。

Result: RLESI通过调优可减少查询延迟，但在查询效率和索引构建成本上始终不如学习型空间索引和先进变体。使用延迟、I/O和索引统计作为指标，发现RLESI的高调优成本和有限泛化能力限制了其实际应用。

Conclusion: 尽管RLESI具有良好的架构兼容性前景，但其高调优成本和有限泛化能力阻碍了实际采用。学习型空间索引和先进变体在查询效率和构建成本方面表现更优。

Abstract: Reinforcement learning has recently been used to enhance index structures, giving rise to reinforcement learning-enhanced spatial indices (RLESIs) that aim to improve query efficiency during index construction. However, their practical benefits remain unclear due to the lack of unified implementations and comprehensive evaluations, especially in disk-based settings.
  We present the first modular and extensible benchmark for RLESIs. Built on top of an existing spatial index library, our framework decouples index training from building, supports parameter tuning, and enables consistent comparison with traditional, advanced, and learned spatial indices.
  We evaluate 12 representative spatial indices across six datasets and diverse workloads, including point, range, kNN, spatial join, and mixed read/write queries. Using latency, I/O, and index statistics as metrics, we find that while RLESIs can reduce query latency with tuning, they consistently underperform learned spatial indices and advanced variants in both query efficiency and index build cost. These findings highlight that although RLESIs offer promising architectural compatibility, their high tuning costs and limited generalization hinder practical adoption.

</details>


### [25] [A Cross-Chain Event-Driven Data Infrastructure for Aave Protocol Analytics and Applications](https://arxiv.org/abs/2512.11363)
*Junyi Fan,Li Sun*

Main category: cs.DB

TL;DR: 该论文构建了首个全面的Aave V3跨链事件数据集，覆盖6条主要EVM链，包含超过5000万条结构化记录，为去中心化借贷市场研究提供基础数据资源。


<details>
  <summary>Details</summary>
Motivation: 尽管Aave V3等去中心化借贷协议管理着超过100亿美元的总锁定价值，但实证研究因缺乏标准化、跨链的事件级数据集而受到严重限制。

Method: 开发开源Python数据处理管道，收集并完全解码Aave V3在6条主要EVM兼容链（以太坊、Arbitrum、Optimism、Polygon、Avalanche和Base）上的8种核心事件类型，使用动态批量处理和自动分片技术确保数据有序性和可重现性。

Result: 创建了包含超过5000万条结构化记录的公开数据集，每条记录都包含区块元数据和美元估值，支持对资本流动、利率动态、清算级联和跨链用户行为的细粒度分析。

Conclusion: 该数据集为未来去中心化借贷市场和系统性风险研究提供了基础资源，填补了该领域标准化跨链数据基础设施的空白。

Abstract: Decentralized lending protocols, exemplified by Aave V3, have transformed financial intermediation by enabling permissionless, multi-chain borrowing and lending without intermediaries. Despite managing over $10 billion in total value locked, empirical research remains severely constrained by the lack of standardized, cross-chain event-level datasets.
  This paper introduces the first comprehensive, event-driven data infrastructure for Aave V3 spanning six major EVM-compatible chains (Ethereum, Arbitrum, Optimism, Polygon, Avalanche, and Base) from respective deployment blocks through October 2025. We collect and fully decode eight core event types -- Supply, Borrow, Withdraw, Repay, LiquidationCall, FlashLoan, ReserveDataUpdated, and MintedToTreasury -- producing over 50 million structured records enriched with block metadata and USD valuations.
  Using an open-source Python pipeline with dynamic batch sizing and automatic sharding (each file less than or equal to 1 million rows), we ensure strict chronological ordering and full reproducibility. The resulting publicly available dataset enables granular analysis of capital flows, interest rate dynamics, liquidation cascades, and cross-chain user behavior, providing a foundational resource for future studies on decentralized lending markets and systemic risk.

</details>


### [26] [Bridging Textual Data and Conceptual Models: A Model-Agnostic Structuring Approach](https://arxiv.org/abs/2512.11403)
*Jacques Chabin,Mirian Halfeld Ferrari,Nicolas Hiot*

Main category: cs.DB

TL;DR: 提出一种自动化方法，将文本数据转换为模型无关的模式，能够与任何数据库模型对齐，并同时生成模式和实例。


<details>
  <summary>Details</summary>
Motivation: 需要一种自动化方法来结构化文本数据，使其能够与各种数据库模型兼容，解决文本数据到结构化模式的转换问题。

Method: 首先将文本数据表示为语义增强的语法树，然后通过迭代的树重写和语法提取进行精炼，整个过程由属性语法元模型\metaG指导。

Result: 该方法能够生成模型无关的模式及其对应的实例，并以临床医疗案例作为概念验证展示了方法的适用性。

Conclusion: 提出的自动化方法能够有效地将文本数据转换为模型无关的结构化模式，为文本数据到数据库的转换提供了可行的解决方案。

Abstract: We introduce an automated method for structuring textual data into a model-agnostic schema, enabling alignment with any database model. It generates both a schema and its instance. Initially, textual data is represented as semantically enriched syntax trees, which are then refined through iterative tree rewriting and grammar extraction, guided by the attribute grammar meta-model \metaG. The applicability of this approach is demonstrated using clinical medical cases as a proof of concept.

</details>
