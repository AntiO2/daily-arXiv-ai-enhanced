<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Query Optimization in the Wild: Realities and Trends](https://arxiv.org/abs/2510.20082)
*Yuanyuan Tian*

Main category: cs.DB

TL;DR: 论文回顾了传统查询优化器设计的局限性，并提出了三个关键趋势：优化与执行的反馈循环、从单查询扩展到工作负载优化、从单体架构转向可组合架构。


<details>
  <summary>Details</summary>
Motivation: 云计算、海量数据和统一数据平台的兴起暴露了传统单体查询优化器架构的局限性，需要新的优化方法。

Method: 从工业视角回顾查询优化的过去和现状，识别当前挑战，并分析行业中的三个关键趋势。

Result: 识别了三个正在兴起的趋势：优化与执行的紧密反馈循环、优化范围从单查询扩展到工作负载、从单体设计转向可组合架构。

Conclusion: 这些趋势为查询优化实践指明了更动态、全面和适应性强的未来发展方向。

Abstract: For nearly half a century, the core design of query optimizers in industrial
database systems has remained remarkably stable, relying on foundational
principles from System R and the Volcano/Cascades framework. However, the rise
of cloud computing, massive data volumes, and unified data platforms has
exposed the limitations of this traditional, monolithic architecture. Taking an
industrial perspective, this paper reviews the past and present of query
optimization in production systems and identifies the challenges they face
today. Then this paper highlights three key trends gaining momentum in the
industry that promise to address these challenges. First, a tighter feedback
loop between query optimization and query execution is being used to improve
the robustness of query performance. Second, the scope of optimization is
expanding from a single query to entire workloads through the convergence of
query optimization and workload optimization. Third, and perhaps most
transformatively, the industry is moving from monolithic designs to composable
architectures that foster agility and cross-engine collaboration. Together,
these trends chart a clear path toward a more dynamic, holistic, and adaptable
future for query optimization in practice.

</details>


### [2] [UREM: A High-performance Unified and Resilient Enhancement Method for Multi- and High-Dimensional Indexes](https://arxiv.org/abs/2510.20110)
*Ming Sheng,Shuliang Wang,Yong Zhang,Yi Luo,Xianbo Liu,Zeming Li*

Main category: cs.DB

TL;DR: 提出了UREM，首个用于多维和高维索引的统一弹性增强方法，能够适应不同场景，提升查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有索引增强方法存在局限性：结构导向方法在静态工作负载下表现好但缺乏通用性，布局导向方法通用性好但在静态工作负载下性能不佳。需要开发统一且弹性的增强方法。

Method: UREM方法：(1)可在不同平台上统一应用于各种索引；(2)通过布局优化在静态工作负载下提升查询性能；(3)通过部分布局重组使索引在查询变化时稳定性能。

Result: 在20个广泛使用的索引上评估UREM，结果显示：在静态工作负载下，多维和高维索引查询性能分别提升最高5.73倍和9.18倍；在动态工作负载下，平均提升5.72倍和9.47倍。

Conclusion: UREM是首个统一且弹性的索引增强方法，能够显著提升多维和高维索引在不同场景下的查询性能，部分传统索引增强后甚至能达到或超越最新先进索引的性能。

Abstract: Numerous multi- or high-dimensional indexes with distinct advantages have
been proposed on various platforms to meet application requirements. To achieve
higher-performance queries, most indexes employ enhancement methods, including
structure-oriented and layout-oriented enhancement methods. Existing
structure-oriented methods tailored to specific indexes work well under static
workloads but lack generality and degrade under dynamic workloads. The
layout-oriented methods exhibit good generality and perform well under dynamic
workloads, but exhibit suboptimal performance under static workloads.
Therefore, it is an open challenge to develop a unified and resilient
enhancement method that can improve query performance for different indexes
adaptively under different scenarios. In this paper, we propose UREM, which is
the first high-performance Unified and Resilient Enhancement Method designed
for both multi- and high-dimensional indexes, capable of adapting to different
scenarios. Specifically, UREM (1) can be uniformly applied with different
indexes on various platforms; (2) enhances the query performance of indexes by
layout optimization under static workloads; (3) enables indexes to stabilize
performance when queries shift through partial layout reorganization. We
evaluate UREM on 20 widely used indexes. Experimental results demonstrate that
UREM improves the query performance of multi- and high-dimensional indexes by
up to 5.73x and 9.18x under static workloads, and by an average of 5.72x and
9.47x under dynamic workloads. Moreover, some traditional indexes enhanced by
UREM even achieve performance comparable to or even surpassing that of recent
advanced indexes.

</details>


### [3] [RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector Database Perspective](https://arxiv.org/abs/2510.20296)
*Wenqi Jiang*

Main category: cs.DB

TL;DR: RAG-Stack是一个三支柱蓝图，用于在检索增强生成(RAG)系统中同时优化系统性能和质量，包含中间表示层、成本模型和计划探索算法。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中如何联合优化系统性能和生成质量的实际挑战，这个问题比表面看起来更复杂，涉及算法和系统层面的众多参数。

Method: 提出RAG-Stack三支柱方法：RAG-IR中间表示层解耦质量和性能，RAG-CM成本模型估计系统性能，RAG-PE计划探索算法搜索高质量高性能配置。

Result: 构建了一个系统化的质量-性能协同优化框架，为RAG系统提供了可扩展的优化方法。

Conclusion: 这个三支柱蓝图将成为未来RAG质量-性能协同优化的实际标准范式。

Abstract: Retrieval-augmented generation (RAG) has emerged as one of the most prominent
applications of vector databases. By integrating documents retrieved from a
database into the prompt of a large language model (LLM), RAG enables more
reliable and informative content generation. While there has been extensive
research on vector databases, many open research problems remain once they are
considered in the wider context of end-to-end RAG pipelines. One practical yet
challenging problem is how to jointly optimize both system performance and
generation quality in RAG, which is significantly more complex than it appears
due to the numerous knobs on both the algorithmic side (spanning models and
databases) and the systems side (from software to hardware). In this paper, we
present RAG-Stack, a three-pillar blueprint for quality-performance
co-optimization in RAG systems. RAG-Stack comprises: (1) RAG-IR, an
intermediate representation that serves as an abstraction layer to decouple
quality and performance aspects; (2) RAG-CM, a cost model for estimating system
performance given an RAG-IR; and (3) RAG-PE, a plan exploration algorithm that
searches for high-quality, high-performance RAG configurations. We believe this
three-pillar blueprint will become the de facto paradigm for RAG
quality-performance co-optimization in the years to come.

</details>


### [4] [Hybrid Mixed Integer Linear Programming for Large-Scale Join Order Optimisation](https://arxiv.org/abs/2510.20308)
*Manuel Schönberger,Immanuel Trummer,Wolfgang Mauerer*

Main category: cs.DB

TL;DR: 提出基于混合整数线性规划(MILP)的新方法来解决大规模查询的join顺序优化问题，通过创新的MILP模型优化任意bushy树结构，并嵌入混合框架实现高效扩展。


<details>
  <summary>Details</summary>
Motivation: 传统查询优化器在大型查询中面临挑战，现有启发式方法随着查询规模增大解决方案质量下降或表现次优，需要更强大的方法来生成高效执行计划。

Method: 开发新颖的MILP模型优化任意bushy树结构，将MILP方法嵌入混合框架，在MILP提供最大优势的地方应用求解器，对较不复杂的优化步骤使用更高效方法。

Result: 方法能够优雅扩展到包含多达100个关系的极大查询规模，在多种join排序方法中始终实现最稳健的计划质量。

Conclusion: 基于MILP的混合框架方法有效解决了大规模查询优化问题，克服了现有方法的局限性，提供了高质量的join排序解决方案。

Abstract: Finding optimal join orders is among the most crucial steps to be performed
by query optimisers. Though extensively studied in data management research,
the problem remains far from solved: While query optimisers rely on exhaustive
search methods to determine ideal solutions for small problems, such methods
reach their limits once queries grow in size. Yet, large queries become
increasingly common in real-world scenarios, and require suitable methods to
generate efficient execution plans. While a variety of heuristics have been
proposed for large-scale query optimisation, they suffer from degrading
solution quality as queries grow in size, or feature highly sub-optimal
worst-case behavior, as we will show.
  We propose a novel method based on the paradigm of mixed integer linear
programming (MILP): By deriving a novel MILP model capable of optimising
arbitrary bushy tree structures, we address the limitations of existing MILP
methods for join ordering, and can rely on highly optimised MILP solvers to
derive efficient tree structures that elude competing methods. To ensure
optimisation efficiency, we embed our MILP method into a hybrid framework,
which applies MILP solvers precisely where they provide the greatest advantage
over competitors, while relying on more efficient methods for less complex
optimisation steps. Thereby, our approach gracefully scales to extremely large
query sizes joining up to 100 relations, and consistently achieves the most
robust plan quality among a large variety of competing join ordering methods.

</details>


### [5] [An Empirical Study on Database Usage in Microservices](https://arxiv.org/abs/2510.20582)
*Maxime André,Marco Raglianti,Souhaila Serbout,Anthony Cleve,Michele Lanza*

Main category: cs.DB

TL;DR: 对微服务架构中数据库使用情况的实证研究，分析了约1000个GitHub项目，涵盖180种数据库技术，发现52%的微服务组合使用多种数据库类别，关系型、键值、文档和搜索数据库最常用。


<details>
  <summary>Details</summary>
Motivation: 微服务架构已成为现代软件开发的重要组成部分，但其数据管理面临重大挑战。目前关于微服务中数据库使用的研究文献稀缺，需要实证证据来理解实际使用情况。

Method: 收集并分析15年间的微服务项目数据，考察约1000个GitHub项目，涉及180种数据库技术（14个类别），进行全面的实证分析。

Result: 微服务主要使用关系型、键值、文档和搜索数据库；52%的微服务组合多种数据库类别；复杂度与数据库数量相关；老系统偏好关系型数据库，新系统更多采用键值、文档技术；小众数据库常与主流数据库结合使用。

Conclusion: 研究提供了微服务中数据库使用的实证证据，报告了18个发现和9个建议，帮助研究者和从业者更好地理解微服务架构中的数据管理实践。

Abstract: Microservices architectures are an integral part of modern software
development. Their adoption brings significant changes to database management.
Instead of relying on a single database, a microservices architecture is
typically composed of multiple, smaller, heterogeneous, and distributed DBs. In
these data-intensive systems, the variety and combination of database
categories and technologies play a crucial role in storing and managing data.
While data management in microservices is a major challenge, research
literature is scarce.
  We present an empirical study on how databases are used in microservices. On
the dataset we collected (and released as open data for future research),
considering 15 years of microservices, we examine ca. 1,000 GitHub projects
that use databases selected among 180 technologies from 14 categories. We
perform a comprehensive analysis of current practices, providing researchers
and practitioners with empirical evidence to better understand database usage
in microservices. We report 18 findings and 9 recommendations. We show that
microservices predominantly use Relational, Key-Value, Document, and Search
databases. Notably, 52% of microservices combine multiple database categories.
Complexity correlates with database count, with older systems favoring
Relational databases and newer ones increasingly adopting Key-Value and
Document technologies. Niche databases (e.g., EventStoreDB, PostGIS), while not
widespread, are often combined with a mainstream one.

</details>


### [6] [Balanced Popularity in Multi-Product Billboard Advertisement](https://arxiv.org/abs/2510.20600)
*Dildar Ali,Suman Banerjee,Yamuna Prasad*

Main category: cs.DB

TL;DR: 该论文研究多产品影响力最大化问题，旨在为多个产品选择广告位，在预算约束下最大化总影响力，同时保持各产品间影响力平衡。


<details>
  <summary>Details</summary>
Motivation: 传统广告位选择主要针对单一产品，而现实中广告商需要同时推广多个产品，且希望各产品影响力相对均衡，避免某些产品过度曝光而其他产品曝光不足。

Method: 提出线性规划松弛加随机舍入的方法，以及基于贪心算法和平衡校正的启发式算法来解决这个NP难问题。

Result: 在真实世界轨迹和广告牌数据集上的实验表明，所提出的方法相比多个基线方法能够获得更高的影响力。

Conclusion: 该研究为多产品广告投放提供了有效的解决方案，能够在预算约束下实现影响力最大化并保持产品间影响力平衡。

Abstract: The billboard advertisement has emerged as an effective out-of-home
advertisement technique where the objective is to choose a limited number of
slots to play some advertisement content (e.g., animation, video, etc.) with
the hope that the content will be visible to a large number of travelers, and
this will be helpful to earn more revenue. In this paper, we study a variant of
the influential slot selection problem where the advertiser wants to promote
multiple products. Formally, we call this problem the \textsc{Multi-Product
Influence Maximization Problem for the Balanced Popularity} Problem. The input
to our problem is a trajectory and a billboard database, as well as a budget
for each product. The goal here is to choose a subset of slots for each product
such that the aggregated influence of all the products gets maximized subject
to the following two constraints: total selection cost for each product is less
than or equal to the allocated budget for that product, and the difference
between the influence for any two products is less than or equal to a given
threshold. We show that the problem is NP-hard to solve optimally. We formulate
this problem as a linear programming problem and use linear programming
relaxation with randomized rounding. Further, we propose a greedy-based
heuristic with balance correction to solve this problem. We conduct a number of
experiments with real-world trajectory and billboard datasets, and the results
are reported. From the reported results, we observe that the proposed solution
approaches lead to more influence compared to many baseline methods.

</details>


### [7] [Downsizing Diffusion Models for Cardinality Estimation](https://arxiv.org/abs/2510.20681)
*Xinhe Mu,Zhaoqi Zhou,Zaijiu Shang,Chuan Zhou,Gang Fu,Guiying Yan,Guoliang Li,Zhiming Ma*

Main category: cs.DB

TL;DR: ADC是基于降维扩散模型的第一个联合分布基数估计器，使用得分函数积分计算点密度，结合GMM和重要性采样蒙特卡洛进行范围查询估计。ADC+通过决策树跳过高精度查询的校正阶段，提升性能。


<details>
  <summary>Details</summary>
Motivation: 受基于得分的扩散模型在估计高维复杂分布方面的优异表现启发，开发首个基于扩散模型的联合分布基数估计器。

Method: 使用轻量级得分估计器近似得分函数，通过积分计算对数似然；结合GMM预测选择性和重要性采样蒙特卡洛校正；ADC+使用决策树识别高精度查询以跳过校正阶段。

Result: 在真实数据集上，ADC+使用约66%的存储空间即可与Naru竞争，并优于其他方法；在复杂相关合成数据集上表现稳健，比Naru准确10倍。

Conclusion: ADC和ADC+是高效的联合分布基数估计器，在准确性和存储效率方面优于现有方法，特别在处理复杂相关数据时表现突出。

Abstract: Inspired by the performance of score-based diffusion models in estimating
complex text, video, and image distributions with thousands of dimensions, we
introduce Accelerated Diffusion Cardest (ADC), the first joint distribution
cardinality estimator based on a downsized diffusion model.
  To calculate the pointwise density value of data distributions, ADC's density
estimator uses a formula that evaluates log-likelihood by integrating the score
function, a gradient mapping which ADC has learned to efficiently approximate
using its lightweight score estimator. To answer ranged queries, ADC's
selectivity estimator first predicts their selectivity using a Gaussian Mixture
Model (GMM), then uses importance sampling Monte Carlo to correct its
predictions with more accurate pointwise density values calculated by the
density estimator. ADC+ further trains a decision tree to identify the
high-volume, high-selectivity queries that the GMM alone can predict very
accurately, in which case it skips the correction phase to prevent Monte Carlo
from adding more variance. Doing so lowers median Q-error and cuts per-query
latency by 25 percent, making ADC+ usually twice as fast as Naru, arguably the
state-of-the-art joint distribution cardinality estimator.
  Numerical experiments using well-established benchmarks show that on all
real-world datasets tested, ADC+ is capable of rivaling Naru and outperforming
MSCN, DeepDB, LW-Tree, and LW-NN using around 66 percent their storage space,
being at least 3 times as accurate as MSCN on 95th and 99th percentile error.
Furthermore, on a synthetic dataset where attributes exhibit complex,
multilateral correlations, ADC and ADC+ are considerably robust while almost
every other learned model suffered significant accuracy declines. In this case,
ADC+ performs better than any other tested model, being 10 times as accurate as
Naru on 95th and 99th percentile error.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [New Hardness Results for the LOCAL Model via a Simple Self-Reduction](https://arxiv.org/abs/2510.19972)
*Alkida Balliu,Filippo Casagrande,Francesco d'Amore,Dennis Olivetti*

Main category: cs.DC

TL;DR: 本文简化了Khoury和Schild的轮消除自约简技术，并应用该技术证明了最大b-匹配和边着色的随机LOCAL算法下界。


<details>
  <summary>Details</summary>
Motivation: Khoury和Schild的最大匹配下界证明虽然优美但长达25页，技术细节复杂难以消化和推广。历史上证明和技术的简化对理解图问题复杂性有重要推动作用。

Method: 提出了轮消除自约简技术的简化版本，并应用该技术分析最大b-匹配和边着色问题。

Result: 证明了最大b-匹配需要Ω(min{log₁₊ᵦΔ, log_Δ n})和Ω(√log₁₊ᵦ n)轮，边着色需要Ω(min{log Δ, log_Δ n})和Ω(√log n)轮。

Conclusion: 成功简化了轮消除自约简技术，并用更简洁的证明获得了多个图问题的下界结果，推进了对LOCAL算法复杂性的理解。

Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL
algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta,
\log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and
$\Delta$ is the maximum degree. This result is shown through a new technique,
called round elimination via self-reduction. The lower bound proof is beautiful
and presents very nice ideas. However, it spans more than 25 pages of technical
details, and hence it is hard to digest and generalize to other problems.
Historically, the simplification of proofs and techniques has marked an
important turning point in our understanding of the complexity of graph
problems. Our paper makes a step forward towards this direction, and provides
the following contributions.
  1. We present a short and simplified version of the round elimination via
self-reduction technique. The simplification of this technique enables us to
obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal
$b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$
and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching
problem is a generalization of the matching problem where each vertex can have
up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain
a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors
the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log
\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le
\Delta^{1-\varepsilon}$ and any constant $\varepsilon > 0$.

</details>


### [9] [AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training](https://arxiv.org/abs/2510.20111)
*Huawei Bai,Yifan Huang,Wenqi Shi,Ansheng You,Feifan Shao,Tengfei Han,Minghui Yu*

Main category: cs.DC

TL;DR: 提出AsyncHZP，一种异步分层零并行方法，通过自适应参数分片和多流异步调度，在保持内存效率的同时显著降低通信开销，实现大规模语言模型训练的高效扩展。


<details>
  <summary>Details</summary>
Motivation: 当前大规模集群上语言模型的训练效率和可扩展性存在瓶颈，主流ND并行方法复杂繁琐，而灵活替代方案如ZeRO则受通信开销限制。

Method: 设计异步分层零并行(AsyncHZP)，自适应地在不同副本组间重新分片参数、梯度和优化器状态，并采用多流异步调度方法在专用后台线程中执行参数全收集和梯度规约分散操作。

Result: 在密集模型和混合专家(MoE)模型上的实证评估表明，AsyncHZP在大规模下保持稳健稳定性，始终优于经典ND并行，无需复杂策略调优即可实现最先进性能。

Conclusion: AsyncHZP通过简化大规模训练路径，在保持内存效率的同时显著提升训练性能，为高效大规模语言模型训练提供了有效解决方案。

Abstract: The training efficiency and scalability of language models on massive
clusters currently remain a critical bottleneck. Mainstream approaches like ND
parallelism are often cumbersome and complex, while flexible alternatives such
as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by
communication overhead. In this paper, we propose Asynchronous Hierarchical
Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to
achieve superior performance while maintaining simplicity and memory
efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding
that can lead to inefficient communication, AsyncHZP adaptively reshards
parameters, gradients, and optimizer states across different replica groups.
This strategy optimizes device memory utilization and significantly reduces
communication overhead. In addition, we also design a multi-stream asynchronous
scheduling method that executes parameter all-gather and gradient
reduce-scatter operations in dedicated background threads, effectively
overlapping communication with computation while incurring negligible memory
fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE)
models confirm that AsyncHZP maintains robust stability at scale. It
consistently outperforms classic ND parallelism, achieving state-of-the-art
performance without complex strategic tuning, thereby simplifying the path to
efficient large-scale training.

</details>


### [10] [A Full Stack Framework for High Performance Quantum-Classical Computing](https://arxiv.org/abs/2510.20128)
*Xin Zhan,K. Grace Johnson,Aniello Esposito,Barbara Chapman,Marco Fiorentino,Kirk M. Bresniker,Raymond G. Beausoleil,Masoud Mohseni*

Main category: cs.DC

TL;DR: 提出了一个HPC-QC全栈框架，用于集成高性能计算和量子计算，支持混合工作负载开发，采用模块化硬件/设备无关的软件集成方法。


<details>
  <summary>Details</summary>
Motivation: 满足对可扩展的高性能计算和量子计算集成日益增长的需求，构建统一的量子-经典编程环境。

Method: 开发了可扩展的量子编程、调度和编译接口，在现有成熟的HPC编程环境中实现；采用自适应电路编织虚拟机将大型量子电路分区；利用Cray LLVM编译框架转换LLVM IR和量子IR。

Result: 在HPE EX超级计算机上成功演示了多个混合HPC-QC多节点多CPU和GPU工作负载，包括求解线性方程组、量子优化和模拟量子相变。

Conclusion: 该工作为基于经典HPC软件栈的统一量子-经典编程环境提供了框架。

Abstract: To address the growing needs for scalable High Performance Computing (HPC)
and Quantum Computing (QC) integration, we present our HPC-QC full stack
framework and its hybrid workload development capability with modular
hardware/device-agnostic software integration approach. The latest development
in extensible interfaces for quantum programming, dispatching, and compilation
within existing mature HPC programming environment are demonstrated. Our HPC-QC
full stack enables high-level, portable invocation of quantum kernels from
commercial quantum SDKs within HPC meta-program in compiled languages (C/C++
and Fortran) as well as Python through a quantum programming interface library
extension. An adaptive circuit knitting hypervisor is being developed to
partition large quantum circuits into sub-circuits that fit on smaller noisy
quantum devices and classical simulators. At the lower-level, we leverage Cray
LLVM-based compilation framework to transform and consume LLVM IR and Quantum
IR (QIR) from commercial quantum software frontends in a retargetable fashion
to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU
and GPU workloads (including solving linear system of equations, quantum
optimization, and simulating quantum phase transitions) have been demonstrated
on HPE EX supercomputers to illustrate functionality and execution viability
for all three components developed so far. This work provides the framework for
a unified quantum-classical programming environment built upon classical HPC
software stack (compilers, libraries, parallel runtime and process scheduling).

</details>


### [11] [Collective Communication for 100k+ GPUs](https://arxiv.org/abs/2510.20171)
*Min Si,Pavan Balaji,Yongzhou Chen,Ching-Hsiang Chu,Adi Gangidi,Saif Hasan,Subodh Iyengar,Dan Johnson,Bingzhe Liu,Jingliang Ren,Ashmitha Jeevaraj Shetty,Greg Steinbrecher,Xinfeng Xie,Yulun Wang,Bruce Wu,Jingyi Yang,Mingran Yang,Minlan Yu,Cen Zhao,Wes Bland,Denis Boyda,Suman Gumudavelli,Cristian Lumezanu,Rui Miao,Zhe Qu,Venkat Ramesh,Maxim Samoylov,Jan Seidel,Feng Tian,Qiye Tan,Shuqiang Zhang,Yimeng Zhao,Shengbao Zheng,Art Zhu,Hongyi Zeng*

Main category: cs.DC

TL;DR: 提出NCCLX通信框架，解决大规模LLM训练中传统通信方法在吞吐量和延迟方面的限制，支持超过10万GPU集群的高效数据交换。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，传统通信方法在数十万GPU规模下面临吞吐量和延迟限制，阻碍了最先进模型的开发和部署。

Method: 开发NCCLX集体通信框架，针对LLM全生命周期优化性能，支持超过10万GPU集群的复杂工作负载。

Result: 在Llama4模型上的实证评估显示通信效率显著提升。

Conclusion: 该研究为下一代LLM在空前规模上运行提供了稳健解决方案。

Abstract: The increasing scale of large language models (LLMs) necessitates highly
efficient collective communication frameworks, particularly as training
workloads extend to hundreds of thousands of GPUs. Traditional communication
methods face significant throughput and latency limitations at this scale,
hindering both the development and deployment of state-of-the-art models. This
paper presents the NCCLX collective communication framework, developed at Meta,
engineered to optimize performance across the full LLM lifecycle, from the
synchronous demands of large-scale training to the low-latency requirements of
inference. The framework is designed to support complex workloads on clusters
exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency
data exchange. Empirical evaluation on the Llama4 model demonstrates
substantial improvements in communication efficiency. This research contributes
a robust solution for enabling the next generation of LLMs to operate at
unprecedented scales.

</details>


### [12] [FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services](https://arxiv.org/abs/2510.20388)
*Víctor Rampérez,Javier Soriano,David Lizcano,Juan A. Lara*

Main category: cs.DC

TL;DR: FLAS是一种结合了主动和被动方法的自动扩展器，通过预测高指标趋势和基于资源使用指标的被动应急系统，为分布式服务提供最优扩展决策。


<details>
  <summary>Details</summary>
Motivation: 云计算因其弹性特性成为新兴技术的主要支撑，自动扩展器通过按需获取和释放资源来确保服务水平。现有方法需要在主动预测和被动响应之间权衡，FLAS旨在结合两者的优势。

Method: FLAS引入两个主要创新：(1) 预测高指标趋势的模型，可提前预测SLA参数变化；(2) 基于资源使用指标估计高指标的被动应急系统，减少侵入性并适配不同应用。

Result: 在内容发布订阅中间件(E-SilboPS)上实现FLAS，通过边界值分析测试方法验证，在包括最坏场景的多种测试案例中，确保99%以上的时间满足性能要求。

Conclusion: FLAS是首个面向内容发布订阅分布式系统的自动扩展系统，具有通用性可适配任何分布式服务，验证了其确保性能要求的有效性。

Abstract: Cloud computing has established itself as the support for the vast majority
of emerging technologies, mainly due to the characteristic of elasticity it
offers. Auto-scalers are the systems that enable this elasticity by acquiring
and releasing resources on demand to ensure an agreed service level. In this
article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for
distributed services that combines the advantages of proactive and reactive
approaches according to the situation to decide the optimal scaling actions in
every moment. The main novelties introduced by FLAS are (i) a predictive model
of the high-level metrics trend which allows to anticipate changes in the
relevant SLA parameters (e.g. performance metrics such as response time or
throughput) and (ii) a reactive contingency system based on the estimation of
high-level metrics from resource use metrics, reducing the necessary
instrumentation (less invasive) and allowing it to be adapted agnostically to
different applications. We provide a FLAS implementation for the use case of a
content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone
of an event-driven architecture. To the best of our knowledge, this is the
first auto-scaling system for content-based publish-subscribe distributed
systems (although it is generic enough to fit any distributed service). Through
an evaluation based on several test cases recreating not only the expected
contexts of use, but also the worst possible scenarios (following the
Boundary-Value Analysis or BVA test methodology), we have validated our
approach and demonstrated the effectiveness of our solution by ensuring
compliance with performance requirements over 99% of the time.

</details>


### [13] [Accurate Performance Predictors for Edge Computing Applications](https://arxiv.org/abs/2510.20495)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 提出一种自动构建和评估性能预测器的方法，在动态边缘环境中实现高达90%的预测准确率，推理时间小于往返时间的1%。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的动态边缘环境中，由于多应用共置和节点异构性，实现可预测的性能具有挑战性，但准确的性能预测对于有效的调度和资源管理至关重要。

Method: 自动构建和评估各种性能预测器，优先考虑准确性和推理时间，使用与应用程序性能最相关的监控指标历史状态进行训练，并在动态共置场景中跨多个服务器进行评估。

Result: 预测器在动态共置场景中达到90%的准确率，推理时间小于往返时间的1%，适用于具有严格实时需求和多样化资源需求的电子显微镜工作流。

Conclusion: 需要一种系统化方法，通过在动态共置场景中联合优化准确性和推理延迟来选择特定服务器的预测器，将此类预测器集成到边缘环境中可以提高资源利用率并实现可预测的性能。

Abstract: Accurate prediction of application performance is critical for enabling
effective scheduling and resource management in resource-constrained dynamic
edge environments. However, achieving predictable performance in such
environments remains challenging due to the co-location of multiple
applications and the node heterogeneity. To address this, we propose a
methodology that automatically builds and assesses various performance
predictors. This approach prioritizes both accuracy and inference time to
identify the most efficient model. Our predictors achieve up to 90% accuracy
while maintaining an inference time of less than 1% of the Round Trip Time.
These predictors are trained on the historical state of the most correlated
monitoring metrics to application performance and evaluated across multiple
servers in dynamic co-location scenarios. As usecase we consider electron
microscopy (EM) workflows, which have stringent real-time demands and diverse
resource requirements. Our findings emphasize the need for a systematic
methodology that selects server-specific predictors by jointly optimizing
accuracy and inference latency in dynamic co-location scenarios. Integrating
such predictors into edge environments can improve resource utilization and
result in predictable performance.

</details>


### [14] [Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing](https://arxiv.org/abs/2510.20506)
*Panagiotis Giannakopoulos,Bart van Knippenberg,Kishor Chandra Joshi,Nicola Calabretta,George Exarchakos*

Main category: cs.DC

TL;DR: 该论文开发了轻量级RTT预测器，通过预测应用延迟来改进负载均衡，在Kubernetes管理的GPU集群中实现高达95%的预测精度，显著降低应用延迟和资源浪费。


<details>
  <summary>Details</summary>
Motivation: 分布式应用对低端到端延迟的需求日益增长，传统负载均衡策略反应迟缓且依赖过时指标，导致次优路由决策和尾部延迟增加。

Method: 基于Kubernetes管理的GPU集群收集的时间序列监控数据，开发轻量级准确的RTT预测器，利用高度相关的监控指标子集，保持低开销同时适应不同共置场景和异构硬件。

Result: 预测器达到95%的准确率，预测延迟控制在应用RTT的10%以内。识别了最小预测精度阈值和关键系统级因素，确保在资源受限集群中的有效部署。

Conclusion: 基于仿真的评估表明，性能感知负载均衡能显著降低应用RTT并最小化资源浪费，证明了将预测性负载均衡集成到未来生产系统的可行性。

Abstract: Distributed applications increasingly demand low end-to-end latency,
especially in edge and cloud environments where co-located workloads contend
for limited resources. Traditional load-balancing strategies are typically
reactive and rely on outdated or coarse-grained metrics, often leading to
suboptimal routing decisions and increased tail latencies. This paper
investigates the use of round-trip time (RTT) predictors to enhance request
routing by anticipating application latency. We develop lightweight and
accurate RTT predictors that are trained on time-series monitoring data
collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of
highly correlated monitoring metrics, our approach maintains low overhead while
remaining adaptable to diverse co-location scenarios and heterogeneous
hardware. The predictors achieve up to 95% accuracy while keeping the
prediction delay within 10% of the application RTT. In addition, we identify
the minimum prediction accuracy threshold and key system-level factors required
to ensure effective predictor deployment in resource-constrained clusters.
Simulation-based evaluation demonstrates that performance-aware load balancing
can significantly reduce application RTT and minimize resource waste. These
results highlight the feasibility of integrating predictive load balancing into
future production systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [E-Test: E'er-Improving Test Suites](https://arxiv.org/abs/2510.19860)
*Ketai Qiu,Luca Di Grazia,Leonardo Mariani,Mauro Pezzè*

Main category: cs.SE

TL;DR: E-Test是一种利用大型语言模型从未测试的执行场景中生成新测试用例的方法，旨在缩小测试套件覆盖范围与生产环境实际执行之间的差距。


<details>
  <summary>Details</summary>
Motivation: 测试套件天然不完美，寻找超出已有测试套件范围的新测试用例极具挑战性且劳动密集，特别是在长期管理大型测试套件时。

Method: E-Test通过识别生产环境中监控到的大量场景中尚未测试的执行，并利用大型语言模型生成新测试用例来增强测试套件。

Result: 在1,975个场景的数据集上评估显示，E-Test在识别未测试执行场景方面显著优于现有方法，F1分数达到0.55，而现有方法最高为0.34，普通LLM为0.39。

Conclusion: E-Test通过有效定位未测试执行场景并减少维护测试套件所需的人工努力，显著增强了测试套件的质量。

Abstract: Test suites are inherently imperfect, and testers can always enrich a suite
with new test cases that improve its quality and, consequently, the reliability
of the target software system. However, finding test cases that explore
execution scenarios beyond the scope of an existing suite can be extremely
challenging and labor-intensive, particularly when managing large test suites
over extended periods.
  In this paper, we propose E-Test, an approach that reduces the gap between
the execution space explored with a test suite and the executions experienced
after testing by augmenting the test suite with test cases that explore
execution scenarios that emerge in production. E-Test (i) identifies executions
that have not yet been tested from large sets of scenarios, such as those
monitored during intensive production usage, and (ii) generates new test cases
that enhance the test suite. E-Test leverages Large Language Models (LLMs) to
pinpoint scenarios that the current test suite does not adequately cover, and
augments the suite with test cases that execute these scenarios.
  Our evaluation on a dataset of 1,975 scenarios, collected from highly-starred
open-source Java projects already in production and Defects4J, demonstrates
that E-Test retrieves not-yet-tested execution scenarios significantly better
than state-of-the-art approaches. While existing regression testing and field
testing approaches for this task achieve a maximum F1-score of 0.34, and
vanilla LLMs achieve a maximum F1-score of 0.39, E-Test reaches 0.55. These
results highlight the impact of E-Test in enhancing test suites by effectively
targeting not-yet-tested execution scenarios and reducing manual effort
required for maintaining test suites.

</details>


### [16] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 本文提出了电子表格操作文档化(SOD)任务，构建了包含111个代码片段及其自然语言摘要的基准，评估了5个LLM在生成电子表格文档方面的性能。


<details>
  <summary>Details</summary>
Motivation: 电子表格在商业、会计和金融领域广泛使用，但缺乏系统化文档方法，阻碍了自动化、协作和知识传递，导致关键机构知识流失的风险。

Method: 构建包含111个电子表格操作代码片段及其对应自然语言摘要的基准数据集，使用BLEU、GLEU、ROUGE-L和METEOR指标评估GPT-4o、GPT-4o-mini、LLaMA-3.3-70B、Mixtral-8x7B和Gemma2-9B五个LLM的性能。

Result: 研究发现LLM能够生成准确的电子表格文档，使SOD成为增强电子表格可重现性、可维护性和协作工作流程的可行前提步骤。

Conclusion: LLM可以有效生成电子表格操作文档，但仍有挑战需要解决。SOD是实现电子表格更好文档化的重要方向。

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


### [17] [Knowledge-Guided Multi-Agent Framework for Application-Level Software Code Generation](https://arxiv.org/abs/2510.19868)
*Qian Xiong,Bo Yang,Weisong Sun,Yiran Zhang,Tianlin Li,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: 提出KGACG框架，通过多智能体协作将软件需求规格和架构设计文档转换为可执行代码，解决复杂应用级软件代码生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成方法在复杂应用级软件代码生成中表现不足，难以保证项目代码的合理组织结构，且难以维护代码生成过程。

Method: KGACG框架包含代码组织与规划智能体(COPA)、编码智能体(CA)和测试智能体(TA)，通过反馈机制形成协作闭环，将软件需求和架构设计转换为可执行代码。

Result: 通过Java坦克大战游戏案例研究展示了KGACG框架中智能体的协作过程，同时识别了面临的挑战。

Conclusion: KGACG致力于推进应用级软件开发的自动化进程，为复杂软件代码生成提供有效解决方案。

Abstract: Automated code generation driven by Large Lan- guage Models (LLMs) has
enhanced development efficiency, yet generating complex application-level
software code remains challenging. Multi-agent frameworks show potential, but
existing methods perform inadequately in large-scale application-level software
code generation, failing to ensure reasonable orga- nizational structures of
project code and making it difficult to maintain the code generation process.
To address this, this paper envisions a Knowledge-Guided Application-Level Code
Generation framework named KGACG, which aims to trans- form software
requirements specification and architectural design document into executable
code through a collaborative closed- loop of the Code Organization & Planning
Agent (COPA), Coding Agent (CA), and Testing Agent (TA), combined with a
feedback mechanism. We demonstrate the collaborative process of the agents in
KGACG in a Java Tank Battle game case study while facing challenges. KGACG is
dedicated to advancing the automation of application-level software
development.

</details>


### [18] [BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills](https://arxiv.org/abs/2510.19898)
*Atharv Sonwane,Isadora White,Hyunji Lee,Matheus Pereira,Lucas Caccia,Minseon Kim,Zhengyan Shi,Chinmay Singh,Alessandro Sordoni,Marc-Alexandre Côté,Xingdi Yuan*

Main category: cs.SE

TL;DR: 提出了一种通过让软件工程代理在添加功能时无意中破坏测试来生成高质量、多样化软件bug的新方法，相比传统方法更接近真实开发过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过故意引入局部扰动生成bug，会产生分布外效应，不能反映真实的开发过程。需要更自然的bug生成方法来训练更好的软件工程代理。

Method: 指导软件工程代理向代码库添加功能，在此过程中可能会无意破坏测试，从而产生bug。这种方法更接近人类开发者的编辑模式。

Result: 新方法生成的bug在监督微调中表现更高效，仅用1200个bug就超越其他数据集3000个bug的效果，性能提升2%。训练出的FrogBoss模型在SWE-bench Verified上达到54.6%的pass@1，FrogMini达到45.3%。

Conclusion: 通过模拟真实开发过程生成bug的方法能产生更高质量的训练数据，显著提升软件工程代理的性能，为下一代语言模型软件工程代理的训练提供了有效解决方案。

Abstract: High quality bugs are key to training the next generation of language model
based software engineering (SWE) agents. We introduce a novel method for
synthetic generation of difficult and diverse bugs. Our method instructs SWE
Agents to introduce a feature into the codebase whereby they may
unintentionally break tests, resulting in bugs. Prior approaches often induce
an out-of-distribution effect by generating bugs intentionally (e.g. by
introducing local perturbation to existing code), which does not reflect
realistic development processes. We perform qualitative analysis to demonstrate
that our approach for generating bugs more closely reflects the patterns found
in human-authored edits. Through extensive experiments, we demonstrate that our
bugs provide more efficient training data for supervised fine-tuning,
outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k
bugs). We train on our newly generated bugs in addition to existing bug
datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench
Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on
SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over
three seeds.

</details>


### [19] [On Interaction Effects in Greybox Fuzzing](https://arxiv.org/abs/2510.19984)
*Konstantinos Kitsios,Marcel Böhme,Alberto Bacchelli*

Main category: cs.SE

TL;DR: MuoFuzz是一种灰盒模糊测试工具，通过学习最有前途的变异器序列来提高测试效率，相比AFL++和MOPT在代码覆盖率和漏洞发现方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 假设变异器应用于种子输入的顺序会影响灰盒模糊测试工具的有效性，实验证实了变异器对之间的交互效应，指向通过选择最有前途的变异器序列来提高模糊测试效率。

Method: 提出MuoFuzz，学习下一个变异器在给定先前选择的变异器条件下产生有趣输入的条件概率，然后使用随机游走从学习到的概率中采样生成变异器序列。

Result: 在FuzzBench和MAGMA基准测试中，MuoFuzz实现了最高的代码覆盖率，并发现了AFL++遗漏的四个漏洞以及AFL++和MOPT都遗漏的一个漏洞。

Conclusion: 通过学习变异器序列的条件概率，MuoFuzz能够更有效地选择变异器顺序，从而提高模糊测试的覆盖率和漏洞发现能力。

Abstract: A greybox fuzzer is an automated software testing tool that generates new
test inputs by applying randomly chosen mutators (e.g., flipping a bit or
deleting a block of bytes) to a seed input in random order and adds all
coverage-increasing inputs to the corpus of seeds. We hypothesize that the
order in which mutators are applied to a seed input has an impact on the
effectiveness of greybox fuzzers. In our experiments, we fit a linear model to
a dataset that contains the effectiveness of all possible mutator pairs and
indeed observe the conjectured interaction effect. This points us to more
efficient fuzzing by choosing the most promising mutator sequence with a higher
likelihood. We propose MuoFuzz, a greybox fuzzer that learns and chooses the
most promising mutator sequences. MuoFuzz learns the conditional probability
that the next mutator will yield an interesting input, given the previously
selected mutator. Then, it samples from the learned probability using a random
walk to generate mutator sequences. We compare the performance of MuoFuzz to
AFL++, which uses a fixed selection probability, and MOPT, which optimizes the
selection probability of each mutator in isolation. Experimental results on the
FuzzBench and MAGMA benchmarks show that MuoFuzz achieves the highest code
coverage and finds four bugs missed by AFL++ and one missed by both AFL++ and
MOPT.

</details>


### [20] [A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)](https://arxiv.org/abs/2510.19997)
*Abraham Itzhak Weinberg*

Main category: cs.SE

TL;DR: 本文提出了FAIGMOE框架，专门针对中型组织和大型企业的生成式AI采用挑战，整合了技术采用理论、组织变革管理和创新扩散视角。


<details>
  <summary>Details</summary>
Motivation: 现有技术采用框架（TAM、TOE、DOI）缺乏针对生成式AI实施的特定性，无法满足不同规模组织的独特需求，存在采用文献的关键空白。

Method: 开发了FAIGMOE概念框架，包含四个相互关联的阶段：战略评估、规划与用例开发、实施与集成、运营与优化，每个阶段都提供可扩展的指导。

Result: FAIGMOE是首个全面解决中型和大型企业生成式AI采用的概念框架，提供了可操作的实施方案、评估工具和治理模板。

Conclusion: 该框架为不同规模组织的生成式AI采用提供了专门指导，但需要通过未来研究进行实证验证。

Abstract: Generative Artificial Intelligence (GenAI) presents transformative
opportunities for organizations, yet both midsize organizations and larger
enterprises face distinctive adoption challenges. Midsize organizations
encounter resource constraints and limited AI expertise, while enterprises
struggle with organizational complexity and coordination challenges. Existing
technology adoption frameworks, including TAM (Technology Acceptance Model),
TOE (Technology Organization Environment), and DOI (Diffusion of Innovations)
theory, lack the specificity required for GenAI implementation across these
diverse contexts, creating a critical gap in adoption literature. This paper
introduces FAIGMOE (Framework for the Adoption and Integration of Generative AI
in Midsize Organizations and Enterprises), a conceptual framework addressing
the unique needs of both organizational types. FAIGMOE synthesizes technology
adoption theory, organizational change management, and innovation diffusion
perspectives into four interconnected phases: Strategic Assessment, Planning
and Use Case Development, Implementation and Integration, and
Operationalization and Optimization. Each phase provides scalable guidance on
readiness assessment, strategic alignment, risk governance, technical
architecture, and change management adaptable to organizational scale and
complexity. The framework incorporates GenAI specific considerations including
prompt engineering, model orchestration, and hallucination management that
distinguish it from generic technology adoption frameworks. As a perspective
contribution, FAIGMOE provides the first comprehensive conceptual framework
explicitly addressing GenAI adoption across midsize and enterprise
organizations, offering actionable implementation protocols, assessment
instruments, and governance templates requiring empirical validation through
future research.

</details>


### [21] [The Cost of Downgrading Build Systems: A Case Study of Kubernetes](https://arxiv.org/abs/2510.20041)
*Gareema Ranjan,Mahmoud Alfadel,Gengyi Sun,Shane McIntosh*

Main category: cs.SE

TL;DR: 本文研究了从基于工件的构建工具（如Bazel）降级到语言特定构建工具（如Go Build）的性能影响，发现虽然降级可能提高可维护性，但会显著增加构建时间和资源成本。


<details>
  <summary>Details</summary>
Motivation: 现代基于工件的构建工具可以加速构建，但团队可能因为维护困难而放弃它们。虽然之前的研究解释了降级的原因，但降级的影响尚未充分探索。

Method: 对Kubernetes项目进行案例研究，分析其从Bazel降级到Go Build期间的完整和增量构建。在四个其他也从Bazel降级的项目中复制该研究。

Result: Bazel构建比Go Build更快（完整构建快23.06-38.66分钟），但内存占用更大（81.42-351.07MB）。Bazel在高并行度下CPU负载更高。降级可能使CI资源成本增加高达76%。其他项目中也观察到类似的内存消耗模式。

Conclusion: 放弃基于工件的构建工具虽然可能提高可维护性，但对大型项目会带来显著的性能成本。这些观察有助于利益相关者在构建工具采用中平衡权衡。

Abstract: Since developers invoke the build system frequently, its performance can
impact productivity. Modern artifact-based build tools accelerate builds, yet
prior work shows that teams may abandon them for alternatives that are easier
to maintain. While prior work shows why downgrades are performed, the
implications of downgrades remain largely unexplored. In this paper, we
describe a case study of the Kubernetes project, focusing on its downgrade from
an artifact-based build tool (Bazel) to a language-specific solution (Go
Build). We reproduce and analyze the full and incremental builds of change sets
during the downgrade period. On the one hand, we find that Bazel builds are
faster than Go Build, completing full builds in 23.06-38.66 up to 75.19 impose
a larger memory footprint than Go Build of 81.42-351.07 respectively. Bazel
builds also impose a greater CPU load at parallelism settings above eight for
full builds and above one for incremental builds. We estimate that downgrading
from Bazel can increase CI resource costs by up to 76 explore whether our
observations generalize by replicating our Kubernetes study on four other
projects that also downgraded from Bazel to older build tools. We observe that
while build time penalties decrease, Bazel consistently consumes more memory.
We conclude that abandoning artifact-based build tools, despite perceived
maintainability benefits, tends to incur considerable performance costs for
large projects. Our observations may help stakeholders to balance trade-offs in
build tool adoption

</details>


### [22] [Symmetry in Software Platforms as an Architectural Principle](https://arxiv.org/abs/2510.20389)
*Bjorn Remseth*

Main category: cs.SE

TL;DR: 软件平台作为结构保持系统，通过强制执行结构规律性来产生架构鲁棒性


<details>
  <summary>Details</summary>
Motivation: 探索软件平台如何通过保持结构规律性来增强架构鲁棒性

Method: 将软件平台视为结构保持系统，分析其提供的稳定接口和行为在特定变换下的对称性

Result: 发现架构鲁棒性源于强制执行结构规律性

Conclusion: 软件平台的架构鲁棒性是通过保持结构对称性来实现的

Abstract: Software platforms often act as structure preserving systems. They provide
consistent interfaces and behaviors that remain stable under specific
transformations that we denote as symmetries. This paper explores the idea that
architectural robustness emerges from enforcing such structural regularities

</details>


### [23] [Developing a Model-Driven Reengineering Approach for Migrating PL/SQL Triggers to Java: A Practical Experience](https://arxiv.org/abs/2510.20121)
*Carlos J. Fernandez-Candel,Jesus Garcia-Molina,Francisco Javier Bermudez Ruiz,Jose Ramon Hoyos Barcelo,Diego Sevilla Ruiz,Benito Jose Cuesta Viera*

Main category: cs.SE

TL;DR: 提出了一种模型驱动的重构软件过程，用于将PL/SQL代码迁移到Java，集成了类似TDD的方法和三种代码验证机制。


<details>
  <summary>Details</summary>
Motivation: 企业需要将RAD平台（如Oracle Forms）的遗留应用迁移到现代技术，特别是将PL/SQL单体代码转换为分层的Java代码。

Method: 使用KDM模型表示遗留代码，采用模型驱动重构过程，集成类似TDD的方法增量开发模型转换，并包含三种生成的代码验证。

Result: 开发了迁移工具，成功将PL/SQL代码转换为Java代码，并详细实现了重构方法和验证过程。

Conclusion: 模型驱动工程方法不仅适用于正向工程，也能成功应用于系统演进和遗留代码迁移，证明了MDE在重构场景中的有效性。

Abstract: Model-driven software engineering (MDE) techniques are not only useful in
forward engineering scenarios, but can also be successfully applied to evolve
existing systems. RAD (Rapid Application Development) platforms emerged in the
nineties, but the success of modern software technologies motivated that a
large number of enterprises tackled the migration of their RAD applications,
such as Oracle Forms. Our research group has collaborated with a software
company in developing a solution to migrate PL/SQL monolithic code on Forms
triggers and program units to Java code separated in several tiers.
  Our research focused on the model-driven reengineering process applied to
develop the migration tool for the conversion of PL/SQL code to Java. Legacy
code is represented in form of KDM (Knowledge-Discovery Metamodel) models. In
this paper, we propose a software process to implement a model-driven
re-engineering. This process integrates a TDD-like approach to incrementally
develop model transformations with three kinds of validations for the generated
code. The implementation and validation of the re-engineering approach are
explained in detail, as well as the evaluation of some issues related with the
application of MDE.

</details>


### [24] [Automated Cloud Infrastructure-as-Code Reconciliation with AI Agents](https://arxiv.org/abs/2510.20211)
*Zhenning Yang,Hui Guan,Victor Nicolet,Brandon Paulsen,Joey Dodds,Daniel Kroening,Ang Chen*

Main category: cs.SE

TL;DR: NSync是一个自动化系统，通过分析云API调用来检测基础设施漂移，并使用LLM推断变更意图，自动更新IaC配置以实现基础设施与代码的同步。


<details>
  <summary>Details</summary>
Motivation: 当基础设施即代码(IaC)与云控制台、CLI或SDK同时使用时，IaC会失去对外部变更的可见性，导致基础设施漂移，使得配置过时，后续IaC操作可能撤销有效更新或触发错误。

Method: NSync利用云API调用追踪来检测漂移，采用基于LLM的智能架构从嘈杂的API序列中推断高层意图，使用专用工具合成目标IaC更新，并通过自演进知识库持续改进。

Result: 在5个真实Terraform项目和372个漂移场景的实验表明，NSync在准确率（从0.71提升到0.97 pass@3）和令牌效率（1.47倍提升）方面均优于基线方法。

Conclusion: NSync通过API追踪和LLM驱动的智能架构，有效解决了基础设施漂移问题，实现了IaC配置与云基础设施的自动同步。

Abstract: Cloud infrastructure is managed through a mix of interfaces -- traditionally,
cloud consoles, command-line interfaces (CLI), and SDKs are the tools of
choice. Recently, Infrastructure-as-Code/IaC frameworks (e.g., Terraform) have
quickly gained popularity. Unlike conventional tools, IaC~frameworks encode the
infrastructure in a "source-of-truth" configuration. They are capable of
automatically carrying out modifications to the cloud -- deploying, updating,
or destroying resources -- to bring the actual infrastructure into alignment
with the IaC configuration. However, when IaC is used alongside consoles, CLIs,
or SDKs, it loses visibility into external changes, causing infrastructure
drift, where the configuration becomes outdated, and later IaC operations may
undo valid updates or trigger errors.
  We present NSync, an automated system for IaC reconciliation that propagates
out-of-band changes back into the IaC program. Our key insight is that
infrastructure changes eventually all occur via cloud API invocations -- the
lowest layer for cloud management operations. NSync gleans insights from API
traces to detect drift (i.e., non-IaC changes) and reconcile it (i.e., update
the IaC configuration to capture the changes). It employs an agentic
architecture that leverages LLMs to infer high-level intents from noisy API
sequences, synthesize targeted IaC updates using specialized tools, and
continually improve through a self-evolving knowledge base of past
reconciliations. We further introduce a novel evaluation pipeline for injecting
realistic drifts into cloud infrastructure and assessing reconciliation
performance. Experiments across five real-world Terraform projects and 372
drift scenarios show that NSync outperforms the baseline both in terms of
accuracy (from 0.71 to 0.97 pass@3) and token efficiency (1.47$\times$
improvement).

</details>


### [25] [Classport: Designing Runtime Dependency Introspection for Java](https://arxiv.org/abs/2510.20340)
*Serena Cofano,Daniel Williams,Aman Sharma,Martin Monperrus*

Main category: cs.SE

TL;DR: Classport系统通过在Java类文件中嵌入依赖信息，实现了运行时依赖自省功能，解决了Java缺乏依赖观察能力的问题。


<details>
  <summary>Details</summary>
Motivation: Java缺乏运行时依赖自省能力，这对于软件供应链安全至关重要。需要一种方法能够在程序执行期间观察当前使用的依赖项。

Method: 开发了Classport系统，将依赖信息嵌入到Java类文件中，从而可以在运行时检索依赖信息。

Result: 在6个真实项目中评估Classport，证明了在运行时识别依赖项的可行性。

Conclusion: Classport的运行时依赖自省功能为运行时完整性检查开辟了重要途径。

Abstract: Runtime introspection of dependencies, i.e., the ability to observe which
dependencies are currently used during program execution, is fundamental for
Software Supply Chain security. Yet, Java has no support for it. We solve this
problem with Classport, a system that embeds dependency information into Java
class files, enabling the retrieval of dependency information at runtime. We
evaluate Classport on six real-world projects, demonstrating the feasibility in
identifying dependencies at runtime. Runtime dependency introspection with
Classport opens important avenues for runtime integrity checking.

</details>


### [26] [FMI-Based Distributed Co-Simulation with Enhanced Security and Intellectual Property Safeguards](https://arxiv.org/abs/2510.20403)
*Santiago Gil,Ecem E. Baş,Christian D. Jensen,Sebastian Engelsgaard,Giuseppe Abbiati,Cláudio Gomes*

Main category: cs.SE

TL;DR: 提出了一种基于UniFMU的分布式协同仿真方法，通过增强网络安全和知识产权保护机制，确保连接由客户端发起且模型和二进制文件位于可信平台上。


<details>
  <summary>Details</summary>
Motivation: 分布式协同仿真在促进不同利益相关者协作建模和仿真的同时保护其知识产权(IP)方面发挥关键作用。虽然协同仿真提供了隐式的IP保护，但在进行连续时间或混合系统的分布式协同仿真时，缺乏防止潜在黑客攻击的指导方针共识。

Method: 在UniFMU基础上构建分布式协同仿真方法，采用增强的网络安全和IP保护机制，确保连接由客户端发起，模型和二进制文件驻留在可信平台上。通过四种不同网络设置下的两个协同仿真演示来展示该方法的功能。

Result: 展示了该方法在四种不同网络设置下的功能实现，并分析了在这些设置中IP保护分布与性能效率之间的权衡关系。

Conclusion: 该方法成功实现了分布式协同仿真的网络安全和IP保护，同时揭示了IP保护分布与性能效率之间的权衡关系，为安全协同仿真提供了可行方案。

Abstract: Distributed co-simulation plays a key role in enabling collaborative modeling
and simulation by different stakeholders while protecting their Intellectual
Property (IP). Although IP protection is provided implicitly by co-simulation,
there is no consensus in the guidelines to conduct distributed co-simulation of
continuous-time or hybrid systems with no exposure to potential hacking
attacks. We propose an approach for distributed co-simulation on top of UniFMU
with enhanced cybersecurity and IP protection mechanisms, ensuring that the
connection is initiated by the client and the models and binaries live on
trusted platforms. We showcase the functionality of this approach using two
co-simulation demos in four different network settings and analyze the
trade-off between IP-protected distribution and performance efficiency in these
settings.

</details>


### [27] [Toward Practical Deductive Verification: Insights from a Qualitative Survey in Industry and Academia](https://arxiv.org/abs/2510.20514)
*Lea Salome Brugger,Xavier Denis,Peter Müller*

Main category: cs.SE

TL;DR: 本文通过访谈30位验证从业者，系统分析了演绎验证成功应用的因素和阻碍广泛采用的问题，并提出了具体建议。


<details>
  <summary>Details</summary>
Motivation: 演绎验证虽然在某些项目中证明有效，但尚未成为主流技术。为了推动其广泛应用，需要研究成功应用的因素和阻碍采用的根本问题。

Method: 采用半结构化访谈30位来自工业界和学术界的验证从业者，并使用主题分析方法系统分析收集的数据。

Result: 除了确认已知挑战（如进行形式化证明需要高水平专业知识），还发现了几个未充分探索的障碍，包括证明维护、对自动化的控制不足和可用性问题。

Conclusion: 基于数据分析结果，提取了演绎验证的推动因素和障碍，并为从业者、工具构建者和研究人员制定了具体建议，包括可用性、自动化和与现有工作流程集成的原则。

Abstract: Deductive verification is an effective method to ensure that a given system
exposes the intended behavior. In spite of its proven usefulness and
feasibility in selected projects, deductive verification is still not a
mainstream technique. To pave the way to widespread use, we present a study
investigating the factors enabling successful applications of deductive
verification and the underlying issues preventing broader adoption. We
conducted semi-structured interviews with 30 practitioners of verification from
both industry and academia and systematically analyzed the collected data
employing a thematic analysis approach. Beside empirically confirming familiar
challenges, e.g., the high level of expertise needed for conducting formal
proofs, our data reveal several underexplored obstacles, such as proof
maintenance, insufficient control over automation, and usability concerns. We
further use the results from our data analysis to extract enablers and barriers
for deductive verification and formulate concrete recommendations for
practitioners, tool builders, and researchers, including principles for
usability, automation, and integration with existing workflows.

</details>


### [28] [Large Language Models for Fault Localization: An Empirical Study](https://arxiv.org/abs/2510.20521)
*YingJian Xiao,RongQun Hu,WeiWei Gong,HongWei Li,AnQuan Jie*

Main category: cs.SE

TL;DR: 本文系统评估了LLMs在代码故障定位任务中的表现，比较了开源和闭源模型在不同提示策略下的性能，发现bug报告上下文能显著提升模型性能，而few-shot学习和思维链推理的效果取决于具体模型能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM在代码故障定位任务中的全面评估，而故障定位的准确性直接影响自动程序修复的效果，因此需要系统研究不同LLM模型在此任务中的表现。

Method: 使用HumanEval-Java和Defects4J数据集，评估开源模型(Qwen2.5-coder-32b-instruct, DeepSeek-V3)和闭源模型(GPT-4.1 mini, Gemini-2.5-flash)，研究标准提示、few-shot示例和思维链推理等不同提示策略对性能的影响。

Result: 实验结果显示，包含bug报告上下文能显著提升模型性能；few-shot学习虽有改进潜力但存在边际收益递减现象；思维链推理的有效性高度依赖模型本身推理能力。

Conclusion: 本研究揭示了不同LLM模型在故障定位任务中的性能特征和权衡，为理解当前LLM的优势和改进故障定位效果提供了有价值的见解。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code-related tasks, particularly in automated program repair. However, the
effectiveness of such repairs is highly dependent on the performance of
upstream fault localization, for which comprehensive evaluations are currently
lacking. This paper presents a systematic empirical study on LLMs in the
statement-level code fault localization task. We evaluate representative
open-source models (Qwen2.5-coder-32b-instruct, DeepSeek-V3) and closed-source
models (GPT-4.1 mini, Gemini-2.5-flash) to assess their fault localization
capabilities on the HumanEval-Java and Defects4J datasets. The study
investigates the impact of different prompting strategies--including standard
prompts, few-shot examples, and chain-of-reasoning--on model performance, with
a focus on analysis across accuracy, time efficiency, and economic cost
dimensions. Our experimental results show that incorporating bug report context
significantly enhances model performance. Few-shot learning shows potential for
improvement but exhibits noticeable diminishing marginal returns, while
chain-of-thought reasoning's effectiveness is highly contingent on the model's
inherent reasoning capabilities. This study not only highlights the performance
characteristics and trade-offs of different models in fault localization tasks,
but also offers valuable insights into the strengths of current LLMs and
strategies for improving fault localization effectiveness.

</details>


### [29] [A Soundness and Precision Benchmark for Java Debloating Tools](https://arxiv.org/abs/2510.20679)
*Jonas Klauke,Tom Ohlmer,Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Eric Bodden*

Main category: cs.SE

TL;DR: 开发了Deblometer微基准测试套件来评估Java去膨胀工具，发现现有工具（Deptrim、JShrink、ProGuard）都存在移除必要程序构造的问题，特别是动态类加载功能导致所有工具都不健全。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发平均依赖36个库，其中80%是传递依赖，但仅有24.9%在运行时真正需要，导致大量无用代码被包含。需要评估去膨胀工具在精确度和健全性之间的权衡。

Method: 开发Deblometer微基准测试套件，包含59个测试用例，每个用例都有手动标注的必要类和膨胀类、方法、字段的基准真值，用于精确测量去膨胀工具的健全性和精确度。

Result: 所有评估工具都会移除必要的程序构造，导致语义改变或执行崩溃。动态类加载功能导致所有工具都不健全。Deptrim保留更多膨胀构造，ProGuard移除更多必要构造，JShrink因注解支持有限导致去膨胀产物损坏。

Conclusion: 去膨胀工具存在严重的健全性问题，需要改进以确保去膨胀后软件的稳定性和可靠性。

Abstract: Modern software development reuses code by importing libraries as
dependencies. Software projects typically include an average of 36
dependencies, with 80% being transitive, meaning they are dependencies of
dependencies. Recent research indicates that only 24.9% of these dependencies
are required at runtime, and even within those, many program constructs remain
unused, adding unnecessary code to the project. This has led to the development
of debloating tools that remove unnecessary dependencies and program constructs
while balancing precision by eliminating unused constructs and soundness by
preserving all required constructs. To systematically evaluate this trade-off,
we developed Deblometer, a micro-benchmark consisting of 59 test cases designed
to assess support for various Java language features in debloating tools. Each
test case includes a manually curated ground truth specifying necessary and
bloated classes, methods, and fields, enabling precise measurement of soundness
and precision. Using Deblometer, we evaluated three popular Java debloating
tools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools
remove required program constructs, which results in changed semantics or
execution crashes. In particular, the dynamic class loading feature introduces
unsoundness in all evaluated tools. Our comparison shows that Deptrim retains
more bloated constructs, while ProGuard removes more required constructs.
JShrink's soundness is significantly affected by limited support for
annotations, which leads to corrupted debloated artifacts. These soundness
issues highlight the need to improve debloating tools to ensure stable and
reliable debloated software.

</details>


### [30] [Exploring Large Language Models for Access Control Policy Synthesis and Summarization](https://arxiv.org/abs/2510.20692)
*Adarsh Vatsa,Bethel Hall,William Eiers*

Main category: cs.SE

TL;DR: 本文研究大型语言模型在访问控制策略生成和总结中的应用效果，发现虽然LLM能生成语法正确的策略，但在权限控制方面存在问题，而结合符号方法分析现有策略时表现更好。


<details>
  <summary>Details</summary>
Motivation: 云计算的普及使得访问控制策略日益复杂，手动编写容易出错且难以精确分析。鉴于LLM在代码合成和总结方面的成功，探索其在访问控制策略自动生成和理解中的潜力。

Method: 首先研究不同LLM在访问控制策略合成中的表现，然后提出基于语义的请求总结方法，利用LLM生成策略允许请求的精确特征描述。

Result: LLM能有效生成语法正确的策略，但非推理型LLM仅45.8%能生成与规范等效的策略，推理型LLM达到93.7%。在策略分析方面，LLM与符号方法结合显示出良好前景。

Conclusion: 虽然LLM在自动策略生成方面存在显著障碍，但在与符号方法结合分析现有策略时表现出有希望的结果。

Abstract: Cloud computing is ubiquitous, with a growing number of services being hosted
on the cloud every day. Typical cloud compute systems allow administrators to
write policies implementing access control rules which specify how access to
private data is governed. These policies must be manually written, and due to
their complexity can often be error prone. Moreover, existing policies often
implement complex access control specifications and thus can be difficult to
precisely analyze in determining their behavior works exactly as intended.
Recently, Large Language Models (LLMs) have shown great success in automated
code synthesis and summarization. Given this success, they could potentially be
used for automatically generating access control policies or aid in
understanding existing policies. In this paper, we explore the effectiveness of
LLMs for access control policy synthesis and summarization. Specifically, we
first investigate diverse LLMs for access control policy synthesis, finding
that: although LLMs can effectively generate syntactically correct policies,
they have permissiveness issues, generating policies equivalent to the given
specification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time
for reasoning LLMs. We then investigate how LLMs can be used to analyze
policies by introducing a novel semantic-based request summarization approach
which leverages LLMs to generate a precise characterization of the requests
allowed by a policy. Our results show that while there are significant hurdles
in leveraging LLMs for automated policy generation, LLMs show promising results
when combined with symbolic approaches in analyzing existing policies.

</details>
