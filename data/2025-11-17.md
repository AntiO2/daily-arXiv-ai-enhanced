<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究调查了研究软件工程师对同行代码评审的看法，发现尽管同行评审对研究软件质量至关重要，但RSE面临独特挑战，需要通过结构化流程、改进工具和针对性培训来提升评审采用率。


<details>
  <summary>Details</summary>
Motivation: 研究软件对科研发现至关重要，但需求变化、复杂输入和遗留依赖阻碍了软件质量和可维护性。虽然同行代码评审能改善软件质量，但其在研究软件工程师中的采用情况尚未被探索。

Method: 通过问卷调查收集研究软件工程师对同行代码评审的看法，调查设计与先前研究保持一致以进行比较分析，同时包含针对RSE的额外问题。

Result: 收到61份有效回复，发现与先前研究结果一致，同时揭示了RSE相比更广泛开发者群体面临的独特挑战和实践差异。

Conclusion: 同行代码评审对提高研究软件质量、可维护性和可靠性至关重要。尽管RSE面临独特挑战，但通过结构化流程、改进工具和针对性培训可以提升同行评审在研究软件开发中的采用率和有效性。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [2] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 提出了一种人机协作的补丁有效性评估方法，通过LLM生成评估标准，人工审核后由LLM基于标准评估补丁，显著降低了手动标注成本。


<details>
  <summary>Details</summary>
Motivation: 当前自动程序修复(APR)评估主要依赖基于执行的测试方法，无法准确判断补丁的真实有效性，而人工标注成本高昂。

Method: 采用人机协作方法：1) LLM为每个bug生成评估标准；2) 人工一次性审核并优化标准；3) LLM基于优化后的标准评估补丁有效性。

Result: 在人类评估者一致同意的补丁上，该方法与人类共识达成高度一致(Cohen's kappa 0.75)，召回率0.94，精确率0.80。

Conclusion: 该方法能有效降低补丁有效性评估的人工成本，在人类一致同意的案例中表现优异，但在存在分歧的案例中仍有改进空间。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [3] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 提出了一种基于大语言模型和一致性检查的控制流异常检测方法，在ERTMS/ETCS铁路系统中验证了有效性


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统复杂度高，设计时验证无法完全保证运行时行为，存在控制流异常风险

Method: 利用LLM连接设计模型和实现代码，自动化源代码插桩生成事件日志，通过一致性检查进行控制流异常检测

Result: 在ERTMS/ETCS案例中，LLM插桩达到84.775%控制流覆盖率，异常检测F1分数96.610%，AUC 93.515%

Conclusion: 结合领域知识指导LLM进行源代码插桩可获得高质量软件日志，通过一致性检查实现有效的控制流异常检测

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [4] [Beyond Accuracy: Behavioral Dynamics of Agentic Multi-Hunk Repair](https://arxiv.org/abs/2511.11012)
*Noor Nashid,Daniel Ding,Keheliya Gallaba,Ahmed E. Hassan,Ali Mesbah*

Main category: cs.SE

TL;DR: 本文对LLM驱动的编程代理在多hunk缺陷修复任务上的表现进行了系统研究，发现不同代理在修复准确率、资源消耗和回归行为方面存在显著差异，并开发了Maple工具通过提供仓库级上下文来提升修复效果。


<details>
  <summary>Details</summary>
Motivation: 传统程序修复主要关注单hunk缺陷，而现实系统中普遍存在的多hunk缺陷需要跨多个不连续代码区域的协调编辑，修复难度更大，目前缺乏对此类任务的系统性研究。

Method: 在Hunk4J数据集的372个多hunk缺陷上评估四种LLM编程代理（Claude Code、Codex、Gemini-cli和Qwen Code），使用细粒度指标分析1,488个修复轨迹，涵盖定位、修复准确率、回归行为和操作动态，并开发Maple工具提供仓库级上下文。

Result: 修复准确率从25.8%（Qwen Code）到93.3%（Claude Code）不等，随着缺陷分散度和复杂度的增加而下降；高性能代理表现出更好的语义一致性，实现正向回归减少，而低性能代理常引入新测试失败；失败修复消耗更多资源（39%-343%更多token）和更长时间（43%-427%）；Maple将Gemini-cli的修复准确率提升了30%。

Conclusion: 本研究通过细粒度指标和轨迹分析，不仅关注修复准确率，还解释了编程代理在多hunk修复过程中如何定位、推理和行动，为理解LLM代理在复杂程序修复任务中的表现提供了深入见解。

Abstract: Automated program repair has traditionally focused on single-hunk defects, overlooking multi-hunk bugs that are prevalent in real-world systems. Repairing these bugs requires coordinated edits across multiple, disjoint code regions, posing substantially greater challenges. We present the first systematic study of LLM-driven coding agents (Claude Code, Codex, Gemini-cli, and Qwen Code) on this task. We evaluate these agents on 372 multi-hunk bugs from the Hunk4J dataset, analyzing 1,488 repair trajectories using fine-grained metrics that capture localization, repair accuracy, regression behavior, and operational dynamics. Results reveal substantial variation: repair accuracy ranges from 25.8% (Qwen Code) to 93.3% (Claude Code) and consistently declines with increasing bug dispersion and complexity. High-performing agents demonstrate superior semantic consistency, achieving positive regression reduction, whereas lower-performing agents often introduce new test failures. Notably, agents do not fail fast; failed repairs consume substantially more resources (39%-343% more tokens) and require longer execution time (43%-427%). Additionally, we developed Maple to provide agents with repository-level context. Empirical results show that Maple improves the repair accuracy of Gemini-cli by 30% through enhanced localization. By analyzing fine-grained metrics and trajectory-level analysis, this study moves beyond accuracy to explain how coding agents localize, reason, and act during multi-hunk repair.

</details>


### [5] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 本文研究了在工业过程自动化领域中使用LLM处理专业编程语言的方法，展示了少量样本提示技术可以解决简单问题，且能在本地部署保护敏感数据。


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究主要关注通用编程语言，而工业自动化领域使用的专业语言在专有环境中应用，其LLM效用尚未充分探索。

Method: 采用少量样本提示方法，无需投入大量资源训练针对特定领域语言的模型。

Result: 少量样本提示方法足以解决LLM原本不擅长支持的语言中的简单问题，且可在本地部署确保公司敏感数据安全。

Conclusion: 企业无需大量投入训练专用模型，通过少量样本提示即可在工业自动化专业语言中有效利用LLM，同时保障数据安全。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [6] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: SQuaD是一个多维度、时间感知的软件质量数据集，包含450个成熟开源项目的700多个质量指标，整合了9种静态分析工具，支持大规模软件质量实证研究。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集通常只关注有限维度（如代码异味、技术债务或重构活动），限制了跨时间和质量维度的综合分析。

Method: 整合9种先进的静态分析工具（SonarQube、CodeScene、PMD等），从450个成熟开源项目中提取方法、类、文件和项目级别的700多个质量指标，涵盖63,586个项目版本。

Result: 构建了包含版本控制历史、问题跟踪历史、软件漏洞数据（CVE/CWE）和过程指标的综合性数据集，支持即时缺陷预测。

Conclusion: SQuaD支持维护性、技术债务、软件演化和质量评估的大规模实证研究，并提出了自动化数据集更新和跨项目质量建模等未来研究方向。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [7] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: SCRUTINEER是一个自动化系统，用于检测智能合约可重用组件(SCRs)的逻辑级使用违规，通过复合特征提取、LLM驱动的知识构建、RAG驱动的检测器和冲突检查器实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 智能合约可重用组件(SCRs)在加速业务特定合约开发中发挥重要作用，但逻辑级使用违规风险日益严重。这种违规发生在SCR遵循使用规则但不符合当前业务逻辑时，导致重大漏洞。

Method: 1. 复合特征提取方法生成三种互补特征表示；2. LLM驱动的知识构建框架提取逻辑级使用并构建SCR知识库；3. RAG驱动的检测器结合快速检索策略进行综合分析；4. 逻辑级使用违规分析引擎集成相似性检查和快照推理冲突检查。

Result: 在3个真实数据集上的评估显示，SCRUTINEER在检测SCR逻辑级使用违规方面达到80.77%的精确率、82.35%的召回率和81.55%的F1分数。

Conclusion: SCRUTINEER是首个自动化检测SCR逻辑级使用违规的实用系统，能够有效识别SCR使用中与业务逻辑不匹配的潜在漏洞。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [8] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 提出了CertiA360工具，将敏捷方法的灵活性与航空航天安全关键系统开发的严格认证要求（如DO-178C）相结合，通过自动化变更管理和追踪来确保合规性。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等高度监管领域，敏捷方法的灵活性与DO-178C等严格认证标准存在冲突，需要找到既能保持敏捷性又能满足合规要求的解决方案。

Method: 开发CertiA360工具，自动化管理需求变更和追踪，确保在整个软件开发生命周期中保持需求成熟度和监管合规性。该工具与行业专家合作设计验证。

Result: 反馈显示CertiA360的自动化功能可以减少人工工作量，在响应需求变更的同时确保DO-178C合规性。虽然工具尚未通过DO-330认证，但证明敏捷方法可以在严格监管环境中有效应用。

Conclusion: 适当定制的敏捷方法不仅能够与安全关键系统开发和认证要求共存，还能在航空航天等高度监管领域提高效率。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [9] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: 提出FengHuang平台，一种解耦式AI基础设施，通过多级共享内存架构解决GPU中心架构在推理工作负载中的内存和通信扩展限制，实现显著的内存容量减少、GPU计算节省和通信加速。


<details>
  <summary>Details</summary>
Motivation: 传统GPU中心架构在推理工作负载中面临内存容量、带宽和互连扩展的限制，需要新的基础设施设计来解决这些扩展挑战。

Method: FengHuang平台采用多级共享内存架构，结合高速本地内存和集中式解耦远程内存，通过主动张量分页和近内存计算优化张量操作。

Result: 模拟显示FengHuang实现高达93%本地内存容量减少、50%GPU计算节省，以及16x到70x更快的GPU间通信，在GPT-3、Grok-1等模型上可减少50%GPU同时保持性能。

Conclusion: FengHuang作为机架级AI基础设施扩展解决方案，提供可扩展、灵活且经济高效的AI推理基础设施，消除供应商锁定并显著降低基础设施和电力成本。

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [10] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: HPCAgentTester是一个基于多智能体LLM的框架，用于自动化生成HPC软件的单元测试，特别针对OpenMP和MPI并行编程模型，能够生成可编译且功能正确的测试用例。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理HPC应用中的非确定性行为和同步问题，需要更有效的自动化测试解决方案来确保并行软件的可靠性。

Method: 采用多智能体LLM框架，包含专门的配方智能体和测试智能体，通过迭代式批判循环协作生成和优化测试用例，针对并行执行结构、复杂通信模式和层次化并行性。

Result: HPCAgentTester能够生成可编译且功能正确的OpenMP和MPI原语测试，有效识别传统技术常遗漏的细微错误，相比独立LLM显著提高了测试编译率和正确性。

Conclusion: 该框架为并行软件系统提供了更强大和可扩展的可靠性保障解决方案，在HPC单元测试自动化方面具有重要价值。

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


### [11] [UFO$^3$: Weaving the Digital Agent Galaxy](https://arxiv.org/abs/2511.11332)
*Chaoyun Zhang,Liqun Li,He Huang,Chiming Ni,Bo Qiao,Si Qin,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.DC

TL;DR: UFO³是一个统一异构设备编排系统，将桌面、服务器、移动设备和边缘设备整合为单一编排结构，通过TaskConstellation模型实现跨设备任务的分布式异步执行和动态优化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理框架大多局限于单一操作系统或设备，导致跨设备工作流程脆弱且需要大量手动操作，需要解决异构设备间的无缝协作问题。

Method: 采用TaskConstellation模型（分布式DAG），包含原子子任务（TaskStars）和显式控制数据依赖（TaskStarLines），通过Constellation Orchestrator进行安全异步执行，使用Agent Interaction Protocol提供持久低延迟通信。

Result: 在NebulaBench基准测试中，UFO³实现了83.3%子任务完成率、70.9%任务成功率，平均并行度为1.72，端到端延迟降低31%，在故障注入实验中表现出优雅降级和恢复能力。

Conclusion: UFO³实现了跨异构设备的准确、高效、弹性任务编排，将孤立代理统一为连贯的自适应计算结构，扩展了泛在计算的应用前景。

Abstract: Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.
  We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.

</details>


### [12] [Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster](https://arxiv.org/abs/2511.11542)
*Tomas Oppelstrup,Nicholas Giamblanco,Delyan Z. Kalchev,Ilya Sharapov,Mark Taylor,Dirk Van Essendelft,Sivasankaran Rajamanickam,Michael James*

Main category: cs.DC

TL;DR: 提出了新颖的\algorithmpropernoun{}算法，用于在分布式计算环境中高效模拟物理系统，实现了每秒160万时间步和84 PFLOP/s的性能，达到峰值性能的90%。


<details>
  <summary>Details</summary>
Motivation: 传统域分解方法在分布式计算环境中无法提供高模拟速率或高利用率，特别是Exascale系统对这些工作负载只能提供一小部分峰值性能。

Method: 使用新颖的\algorithmpropernoun{}算法，并将其应用于浅水方程来模拟小行星撞击引发的海啸，在64个Cerebras CS-3系统集群上运行460米分辨率的行星尺度模拟。

Result: 实现了超过160万时间步/秒的模拟速率和84 PFLOP/s的计算性能，在单节点和集群环境中均能达到峰值性能的90%。

Conclusion: \algorithmpropernoun{}算法能够有效克服传统域分解方法的局限性，在分布式计算环境中实现高性能的物理系统模拟。

Abstract: Simulation of physical systems is essential in many scientific and engineering domains. Commonly used domain decomposition methods are unable to deliver high simulation rate or high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel \algorithmpropernoun{} algorithm, designed to overcome these limitations. We apply this method and show simulations running in excess of 1.6 million time steps per second and simulations achieving 84 PFLOP/s. Our implementation can achieve 90\% of peak performance in both single-node and clustered environments. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale running on a cluster of 64 Cerebras CS-3 systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [13] [ResBench: A Comprehensive Framework for Evaluating Database Resilience](https://arxiv.org/abs/2511.11088)
*Puyun Hu,Wei Pan,Xun Jian,Zeqi Ma,Tianjie Li,Yang Shen,Chengzhi Han,Yudong Zhao,Zhanhuai Li*

Main category: cs.DB

TL;DR: 提出了ResBench基准测试框架，用于评估数据库在面对不利事件时的弹性能力，通过模拟不利事件并注入到正常事务处理中，从八个维度全面量化数据库的弹性表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据库基准测试主要关注理想运行环境下的性能，但实际场景中数据库会面临各种不利事件，需要从综合角度量化其应对能力。

Method: 通过清晰的层次解耦实现测试过程的自动化、标准化和可视化，模拟不利事件并在正常事务处理期间注入，使用模块收集多个指标用于评估模型。

Result: 从吞吐量、延迟、稳定性、抵抗性、恢复性、干扰期、适应能力和指标偏差八个维度评估数据库弹性，所有结果通过用户友好的图形界面呈现。

Conclusion: ResBench框架能够有效评估数据库在不利事件下的弹性表现，为数据库弹性测试提供了标准化的解决方案。

Abstract: Existing database benchmarks primarily focus on performance under ideal running environments. However, in real-world scenarios, databases probably face numerous adverse events. Quantifying the ability to cope with these events from a comprehensive perspective remains an open problem. We provide the definition of database resilience to describe its performance when facing adversity and propose ResBench, a benchmark for evaluating database resilience. This framework achieves automation, standardization, and visualization of the testing process through clear hierarchical decoupling. ResBench simulates adverse events and injects them during normal transaction processing, utilizing a module to gather multiple metrics for the evaluation model. We assess database resilience across eight dimensions: throughput, latency, stability, resistance, recovery, disturbance period, adaptation capability and metric deviation. All the results are presented to users via a user-friendly graphical interface. We demonstrate the execution process and result interpretation of ResBench using two types of adversity datasets.

</details>


### [14] [Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database](https://arxiv.org/abs/2511.11399)
*Rosario Napoli,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DB

TL;DR: 提出了一种将知识补全阶段集成到图数据库-图机器学习应用中的创新架构，通过可扩展的传递关系揭示隐藏知识，显著影响数据集行为和指标。


<details>
  <summary>Details</summary>
Motivation: 当前图数据库-图机器学习应用在分析数据时存在关键缺陷，特别是在知识图谱的知识补全方面。现有架构忽略了知识补全，导致使用不完整或碎片化的数据集，尽管这些数据包含有价值的隐藏知识，这可能导致图机器学习模型产生错误解释。

Method: 引入可扩展的传递关系，这些是通过衰减函数建模的链接，能够在网络中传播信息，允许多个节点之间进行确定性的知识流动。

Result: 实验结果表明，该方法从根本上重塑了拓扑结构和整体数据集动态，强调了这种新架构对于生成更好模型和释放基于图的数据分析全部潜力的必要性。

Conclusion: 集成知识补全阶段的图数据库-图机器学习架构能够揭示隐藏知识，显著改善数据集行为，为图机器学习模型提供更准确的数据输入，从而提升模型性能。

Abstract: Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.
  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.
  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.

</details>
