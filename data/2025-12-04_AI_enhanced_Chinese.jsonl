{"id": "2512.03278", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03278", "abs": "https://arxiv.org/abs/2512.03278", "authors": ["Michael Theologitis", "Dan Suciu"], "title": "Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases", "comment": "Accepted at AAAI 2026 Workshop on LLM-based Multi-Agent Systems (LaMAS)", "summary": "In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\\unicode{x2014}$typically a few hundred rows$\\unicode{x2014}$that conveniently fit within an LLM's context window.\n  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\\unicode{x2014}$the standard benchmark for fact verification over structured data$\\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).", "AI": {"tldr": "Thucy\uff1a\u9996\u4e2a\u8de8\u6570\u636e\u5e93\u3001\u8de8\u8868\u683c\u7684\u591a\u667a\u80fd\u4f53\u58f0\u660e\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u5728TabFact\u57fa\u51c6\u4e0a\u8d85\u8d8aSOTA 5.6\u4e2a\u767e\u5206\u70b9", "motivation": "\u5f53\u524d\u793e\u4f1a\u5b58\u5728\u5927\u91cf\u53ef\u9a8c\u8bc1\u7684\u58f0\u660e\u51b2\u7a81\uff0c\u73b0\u6709\u9a8c\u8bc1\u7cfb\u7edf\u4ec5\u80fd\u5904\u7406\u5c0f\u578b\u5355\u8868\u6570\u636e\u5e93\uff0c\u65e0\u6cd5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u590d\u6742\u591a\u6e90\u6570\u636e\u9a8c\u8bc1\u9700\u6c42", "method": "\u6784\u5efa\u5b8c\u5168\u6570\u636e\u6e90\u65e0\u5173\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u3001\u68c0\u67e5\u5e76\u63a8\u7406\u6240\u6709\u53ef\u7528\u5173\u7cfb\u6570\u636e\u5e93\uff0c\u901a\u8fc7\u751f\u6210\u652f\u6301\u6027SQL\u67e5\u8be2\u63d0\u4f9b\u900f\u660e\u8bc1\u636e", "result": "\u5728TabFact\u6570\u636e\u96c6\u4e0a\u8fbe\u523094.3%\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u7ed3\u679c88.7%\u63d0\u53475.6\u4e2a\u767e\u5206\u70b9", "conclusion": "Thucy\u9996\u6b21\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u5e93\u3001\u8de8\u8868\u683c\u7684\u81ea\u52a8\u58f0\u660e\u9a8c\u8bc1\uff0c\u901a\u8fc7SQL\u67e5\u8be2\u63d0\u4f9b\u900f\u660e\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u6784\u5316\u6570\u636e\u4e8b\u5b9e\u9a8c\u8bc1\u7684\u6027\u80fd"}}
{"id": "2512.03389", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.03389", "abs": "https://arxiv.org/abs/2512.03389", "authors": ["Shu Chen", "Deepti Raghavan", "U\u011fur \u00c7etintemel"], "title": "Continuous Prompts: LLM-Augmented Pipeline Processing over Unstructured Streams", "comment": null, "summary": "Monitoring unstructured streams increasingly requires persistent, semantics-aware computation, yet today's LLM frameworks remain stateless and one-shot, limiting their usefulness for long-running analytics. We introduce Continuous Prompts (CPs), the first framework that brings LLM reasoning into continuous stream processing. CPs extend RAG to streaming settings, define continuous semantic operators, and provide multiple implementations, primarily focusing on LLM-based approaches but also reporting one embedding-based variants. Furthermore, we study two LLM-centric optimizations, tuple batching and operator fusion, to significantly improve efficiency while managing accuracy loss.\n  Because these optimizations inherently trade accuracy for speed, we present a dynamic optimization framework that uses lightweight shadow executions and cost-aware multi-objective Bayesian optimization (MOBO) to learn throughput-accuracy frontiers and adapt plans under probing budgets.\n  We implement CPs in the VectraFlow stream processing system. Using operator-level microbenchmarks and streaming pipelines on real datasets, we show that VectraFlow can adapt to workload dynamics, navigate accuracy-efficiency trade-offs, and sustain persistent semantic queries over evolving unstructured streams.", "AI": {"tldr": "Continuous Prompts (CPs) \u6846\u67b6\u5c06 LLM \u63a8\u7406\u5f15\u5165\u8fde\u7eed\u6d41\u5904\u7406\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u6301\u7eed\u8ba1\u7b97\u89e3\u51b3\u4f20\u7edf LLM \u6846\u67b6\u5728\u6d41\u5f0f\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d LLM \u6846\u67b6\u662f\u72b6\u6001\u65e0\u5173\u7684\u5355\u6b21\u5904\u7406\u6a21\u5f0f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u975e\u7ed3\u6784\u5316\u6570\u636e\u6d41\u4e2d\u9700\u8981\u6301\u7eed\u8bed\u4e49\u611f\u77e5\u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5176\u5728\u957f\u671f\u8fd0\u884c\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "method": "1. \u63d0\u51fa Continuous Prompts (CPs) \u6846\u67b6\uff0c\u5c06 RAG \u6269\u5c55\u5230\u6d41\u5f0f\u573a\u666f\uff1b2. \u5b9a\u4e49\u8fde\u7eed\u8bed\u4e49\u7b97\u5b50\uff1b3. \u63d0\u4f9b\u591a\u79cd\u5b9e\u73b0\uff08\u4e3b\u8981\u57fa\u4e8e LLM\uff0c\u4e5f\u6709\u5d4c\u5165\u53d8\u4f53\uff09\uff1b4. \u7814\u7a76\u4e24\u79cd LLM \u4f18\u5316\uff1a\u5143\u7ec4\u6279\u5904\u7406\u548c\u7b97\u5b50\u878d\u5408\uff1b5. \u63d0\u51fa\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f71\u5b50\u6267\u884c\u548c\u6210\u672c\u611f\u77e5\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\u6765\u5b66\u4e60\u541e\u5410\u91cf-\u51c6\u786e\u7387\u8fb9\u754c\u3002", "result": "\u5728 VectraFlow \u6d41\u5904\u7406\u7cfb\u7edf\u4e2d\u5b9e\u73b0 CPs\uff0c\u901a\u8fc7\u7b97\u5b50\u7ea7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6d41\u5f0f\u7ba1\u9053\u5b9e\u9a8c\uff0c\u8bc1\u660e\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u52a8\u6001\u53d8\u5316\uff0c\u5728\u51c6\u786e\u7387\u4e0e\u6548\u7387\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u5e76\u7ef4\u6301\u5bf9\u6f14\u5316\u4e2d\u975e\u7ed3\u6784\u5316\u6d41\u7684\u6301\u7eed\u8bed\u4e49\u67e5\u8be2\u3002", "conclusion": "CPs \u6846\u67b6\u6210\u529f\u5c06 LLM \u63a8\u7406\u5f15\u5165\u8fde\u7eed\u6d41\u5904\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u51c6\u786e\u7387\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u957f\u671f\u975e\u7ed3\u6784\u5316\u6d41\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03401", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.03401", "abs": "https://arxiv.org/abs/2512.03401", "authors": ["Ryoto Miyamoto", "Akira Kasuga"], "title": "Enterprise Data Science Platform: A Unified Architecture for Federated Data Access", "comment": "10 pages, 2 figures, 3 tables, WS-D2ET @ IEEE BigData 2025", "summary": "Organizations struggle to share data across departments that have adopted different data analytics platforms. If n datasets must serve m environments, up to n*m replicas can emerge, increasing inconsistency and cost. Traditional warehouses copy data into vendor-specific stores; cross-platform access is hard. This study proposes the Enterprise Data Science Platform (EDSP), which builds on data lakehouse architecture and follows a Write-Once, Read-Anywhere principle. EDSP enables federated data access for multi-query engine environments, targeting data science workloads with periodic data updates and query response times ranging from seconds to minutes. By providing centralized data management with federated access from multiple query engines to the same data sources, EDSP eliminates data duplication and vendor lock-in inherent in traditional data warehouses. The platform employs a four-layer architecture: Data Preparation, Data Store, Access Interface, and Query Engines. This design enforces separation of concerns and reduces the need for data migration when integrating additional analytical environments. Experimental results demonstrate that major cloud data warehouses and programming environments can directly query EDSP-managed datasets. We implemented and deployed EDSP in production, confirming interoperability across multiple query engines. For data sharing across different analytical environments, EDSP achieves a 33-44% reduction in operational steps compared with conventional approaches requiring data migration. Although query latency may increase by up to a factor of 2.6 compared with native tables, end-to-end completion times remain on the order of seconds, maintaining practical performance for analytical use cases. Based on our production experience, EDSP provides practical design guidelines for addressing the data-silo problem in multi-query engine environments.", "AI": {"tldr": "EDSP\u662f\u4e00\u4e2a\u57fa\u4e8e\u6570\u636e\u6e56\u4ed3\u67b6\u6784\u7684\u4f01\u4e1a\u6570\u636e\u79d1\u5b66\u5e73\u53f0\uff0c\u91c7\u7528\"\u4e00\u6b21\u5199\u5165\uff0c\u968f\u5904\u8bfb\u53d6\"\u539f\u5219\uff0c\u5b9e\u73b0\u591a\u67e5\u8be2\u5f15\u64ce\u73af\u5883\u4e0b\u7684\u8054\u90a6\u6570\u636e\u8bbf\u95ee\uff0c\u6d88\u9664\u6570\u636e\u590d\u5236\u548c\u4f9b\u5e94\u5546\u9501\u5b9a\u3002", "motivation": "\u7ec4\u7ec7\u5728\u4e0d\u540c\u90e8\u95e8\u4f7f\u7528\u4e0d\u540c\u6570\u636e\u5206\u6790\u5e73\u53f0\u65f6\u9762\u4e34\u6570\u636e\u5171\u4eab\u96be\u9898\uff0c\u4f20\u7edf\u6570\u636e\u4ed3\u5e93\u9700\u8981\u6570\u636e\u590d\u5236\u5bfc\u81f4n*m\u526f\u672c\u95ee\u9898\uff0c\u589e\u52a0\u4e0d\u4e00\u81f4\u6027\u548c\u6210\u672c\uff0c\u4e14\u8de8\u5e73\u53f0\u8bbf\u95ee\u56f0\u96be\u3002", "method": "\u63d0\u51faEDSP\u5e73\u53f0\uff0c\u91c7\u7528\u56db\u5c42\u67b6\u6784\uff1a\u6570\u636e\u51c6\u5907\u5c42\u3001\u6570\u636e\u5b58\u50a8\u5c42\u3001\u8bbf\u95ee\u63a5\u53e3\u5c42\u548c\u67e5\u8be2\u5f15\u64ce\u5c42\uff0c\u57fa\u4e8e\u6570\u636e\u6e56\u4ed3\u67b6\u6784\u5b9e\u73b0\u8054\u90a6\u6570\u636e\u8bbf\u95ee\uff0c\u652f\u6301\u591a\u67e5\u8be2\u5f15\u64ce\u76f4\u63a5\u67e5\u8be2\u76f8\u540c\u6570\u636e\u6e90\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e3b\u8981\u4e91\u6570\u636e\u4ed3\u5e93\u548c\u7f16\u7a0b\u73af\u5883\u53ef\u76f4\u63a5\u67e5\u8be2EDSP\u7ba1\u7406\u7684\u6570\u636e\u96c6\uff0c\u751f\u4ea7\u90e8\u7f72\u786e\u8ba4\u8de8\u591a\u67e5\u8be2\u5f15\u64ce\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u6570\u636e\u8fc1\u79fb\u65b9\u6cd5\u51cf\u5c1133-44%\u64cd\u4f5c\u6b65\u9aa4\uff0c\u67e5\u8be2\u5ef6\u8fdf\u867d\u589e\u52a0\u6700\u591a2.6\u500d\u4f46\u7aef\u5230\u7aef\u5b8c\u6210\u65f6\u95f4\u4ecd\u5728\u79d2\u7ea7\u3002", "conclusion": "EDSP\u4e3a\u591a\u67e5\u8be2\u5f15\u64ce\u73af\u5883\u4e2d\u7684\u6570\u636e\u5b64\u5c9b\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u8bbe\u8ba1\u6307\u5357\uff0c\u901a\u8fc7\u96c6\u4e2d\u5f0f\u6570\u636e\u7ba1\u7406\u548c\u8054\u90a6\u8bbf\u95ee\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u64cd\u4f5c\u590d\u6742\u6027\u3002"}}
{"id": "2512.03790", "categories": ["cs.DB", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.03790", "abs": "https://arxiv.org/abs/2512.03790", "authors": ["Iris Beerepoot", "Vinicius Stein Dani", "Xixi Lu"], "title": "ExOAR: Expert-Guided Object and Activity Recognition from Textual Data", "comment": "Accepted manuscript (on August 22, 2025) to the 2nd International Workshop on Generative AI for Process Mining (GenAI4PM 2025), held in conjunction with the 7th International Conference on Process Mining (ICPM 2025)", "summary": "Object-centric process mining requires structured data, but extracting it from unstructured text remains a challenge. We introduce ExOAR (Expert-Guided Object and Activity Recognition), an interactive method that combines large language models (LLMs) with human verification to identify objects and activities from textual data. ExOAR guides users through consecutive stages in which an LLM generates candidate object types, activities, and object instances based on contextual input, such as a user's profession, and textual data. Users review and refine these suggestions before proceeding to the next stage. Implemented as a practical tool, ExOAR is initially validated through a demonstration and then evaluated with real-world Active Window Tracking data from five users. Our results show that ExOAR can effectively bridge the gap between unstructured textual data and the structured log with clear semantics needed for object-centric process analysis, while it maintains flexibility and human oversight.", "AI": {"tldr": "ExOAR\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u8bc6\u522b\u5bf9\u8c61\u548c\u6d3b\u52a8\uff0c\u4e3a\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8fc7\u7a0b\u6316\u6398\u63d0\u4f9b\u7ed3\u6784\u5316\u6570\u636e\u3002", "motivation": "\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8fc7\u7a0b\u6316\u6398\u9700\u8981\u7ed3\u6784\u5316\u6570\u636e\uff0c\u4f46\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u8fd9\u79cd\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5c06\u6587\u672c\u6570\u636e\u8f6c\u6362\u4e3a\u5177\u6709\u6e05\u6670\u8bed\u4e49\u7684\u7ed3\u6784\u5316\u65e5\u5fd7\u3002", "method": "ExOAR\u91c7\u7528\u4ea4\u4e92\u5f0f\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u9a8c\u8bc1\u3002\u7cfb\u7edf\u5f15\u5bfc\u7528\u6237\u901a\u8fc7\u8fde\u7eed\u9636\u6bb5\uff1aLLM\u57fa\u4e8e\u4e0a\u4e0b\u6587\u8f93\u5165\uff08\u5982\u7528\u6237\u804c\u4e1a\uff09\u548c\u6587\u672c\u6570\u636e\u751f\u6210\u5019\u9009\u5bf9\u8c61\u7c7b\u578b\u3001\u6d3b\u52a8\u548c\u5bf9\u8c61\u5b9e\u4f8b\uff0c\u7528\u6237\u5ba1\u67e5\u5e76\u5b8c\u5584\u8fd9\u4e9b\u5efa\u8bae\u540e\u518d\u8fdb\u5165\u4e0b\u4e00\u9636\u6bb5\u3002", "result": "ExOAR\u4f5c\u4e3a\u5b9e\u7528\u5de5\u5177\u5b9e\u73b0\uff0c\u901a\u8fc7\u6f14\u793a\u521d\u6b65\u9a8c\u8bc1\uff0c\u7136\u540e\u4f7f\u7528\u4e94\u4e2a\u7528\u6237\u7684\u771f\u5b9e\u4e16\u754c\u6d3b\u52a8\u7a97\u53e3\u8ddf\u8e2a\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660eExOAR\u80fd\u6709\u6548\u5f25\u5408\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u4e0e\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8fc7\u7a0b\u5206\u6790\u6240\u9700\u7684\u7ed3\u6784\u5316\u65e5\u5fd7\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u7075\u6d3b\u6027\u548c\u4eba\u5de5\u76d1\u7763\u3002", "conclusion": "ExOAR\u6210\u529f\u89e3\u51b3\u4e86\u4ece\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u7528\u4e8e\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u80fd\u529b\u548c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03416", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03416", "abs": "https://arxiv.org/abs/2512.03416", "authors": ["Ruiqi Lai", "Hongrui Liu", "Chengzhi Lu", "Zonghao Liu", "Siyu Cao", "Siyang Shao", "Yixin Zhang", "Luo Mai", "Dmitrii Ustiugov"], "title": "TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity", "comment": null, "summary": "The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.", "AI": {"tldr": "TokenScale\u662f\u4e00\u4e2a\u7528\u4e8e\u89e3\u8026LLM\u670d\u52a1\u7684\u81ea\u52a8\u6269\u7f29\u6846\u67b6\uff0c\u901a\u8fc7Token Velocity\u6307\u6807\u548cConvertible Decoders\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347SLO\u8fbe\u6210\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u5f53\u524dLLM\u670d\u52a1\u7684prefill/decode\u89e3\u8026\u67b6\u6784\u867d\u7136\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46\u96be\u4ee5\u5e94\u5bf9\u7a81\u53d1\u5de5\u4f5c\u8d1f\u8f7d\u3002\u73b0\u6709\u7684\u81ea\u52a8\u6269\u7f29\u7b56\u7565\u4f9d\u8d56\u6ede\u540e\u6307\u6807\uff08\u5982GPU\u5229\u7528\u7387\uff09\uff0c\u5bfc\u81f4\u5bf9\u8d1f\u8f7d\u5cf0\u503c\u53cd\u5e94\u7f13\u6162\uff0c\u9020\u6210TTFT\u548cTPOT SLO\u8fdd\u89c4\u4ee5\u53ca\u6602\u8d35\u7684\u8fc7\u5ea6\u914d\u7f6e\u3002", "method": "1. \u63d0\u51faToken Velocity\u4f5c\u4e3a\u7edf\u4e00prefill\u3001\u7f51\u7edc\u548cdecode\u9636\u6bb5\u5de5\u4f5c\u901f\u7387\u7684\u65b0\u6307\u6807\uff0c\u4f5c\u4e3a\u7cfb\u7edf\u80cc\u538b\u7684\u9886\u5148\u6307\u6807\uff0c\u5b9e\u73b0\u4e3b\u52a8\u6269\u7f29\u3002\n2. \u5f15\u5165Convertible Decoders\uff0c\u5141\u8bb8\u89e3\u7801GPU\u5728\u6d41\u91cf\u9ad8\u5cf0\u65f6\u52a8\u6001\u6267\u884cprefill\u4efb\u52a1\uff0c\u521b\u5efa\u5feb\u901f\u54cd\u5e94\u7f13\u51b2\u533a\u5438\u6536\u7a81\u53d1\u6d41\u91cf\uff0c\u6d88\u9664\u65b0prefiller\u7684\u521d\u59cb\u5316\u5ef6\u8fdf\u3002", "result": "\u5728GPU\u96c6\u7fa4\u4e0a\u7684\u751f\u4ea7\u73af\u5883\u8bc4\u4f30\u663e\u793a\uff0cTokenScale\u5c06SLO\u8fbe\u6210\u7387\u4ece50-88%\u63d0\u5347\u523080-96%\uff0c\u76f8\u6bd4DistServe\u3001BlitzScale\u548cAIBrix\u7b49\u5148\u8fdb\u7cfb\u7edf\uff0c\u6210\u672c\u964d\u4f4e4-14%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9884\u6d4b\u6027\u6307\u6807\u548c\u7075\u6d3b\u7684\u7cfb\u7edf\u8bbe\u8ba1\uff0cTokenScale\u663e\u8457\u63d0\u5347\u4e86\u89e3\u8026LLM\u670d\u52a1\u57fa\u7840\u8bbe\u65bd\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2512.03262", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.03262", "abs": "https://arxiv.org/abs/2512.03262", "authors": ["Songwen Zhao", "Danqing Wang", "Kexun Zhang", "Jiaxuan Luo", "Zhuo Li", "Lei Li"], "title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks", "comment": null, "summary": "Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSU S VI B E S\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u7f16\u7a0b\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u6027\u8868\u73b0\uff0c\u53d1\u73b0\u6240\u6709\u4ee3\u7406\u5728\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u90fd\u5f88\u5dee\uff0c\u5373\u4f7f\u529f\u80fd\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u4e5f\u53ea\u6709\u5c11\u6570\u662f\u5b89\u5168\u7684\u3002", "motivation": "\u968f\u7740vibe coding\uff08\u4eba\u7c7b\u5de5\u7a0b\u5e08\u6307\u5bfcLLM\u4ee3\u7406\u5b8c\u6210\u590d\u6742\u7f16\u7801\u4efb\u52a1\uff09\u7684\u666e\u53ca\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u8f93\u51fa\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u654f\u611f\u5e94\u7528\u4e2d\u3002", "method": "\u6784\u5efa\u5305\u542b200\u4e2a\u771f\u5b9e\u5f00\u6e90\u9879\u76ee\u4e2d\u5bfc\u81f4\u6f0f\u6d1e\u5b9e\u73b0\u7684\u7279\u5f81\u8bf7\u6c42\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5SU S VI B E S\uff0c\u8bc4\u4f30\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f16\u7801\u4ee3\u7406\uff08\u5305\u62ec\u4f7f\u7528\u524d\u6cbf\u6a21\u578b\u7684SWE-Agent\u7b49\uff09\u3002", "result": "\u6240\u6709\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5b89\u5168\u65b9\u9762\u8868\u73b0\u90fd\u5f88\u5dee\uff1a\u867d\u7136SWE-Agent with Claude 4 Sonnet\u768461%\u89e3\u51b3\u65b9\u6848\u529f\u80fd\u6b63\u786e\uff0c\u4f46\u53ea\u670910.5%\u662f\u5b89\u5168\u7684\u3002\u521d\u6b65\u5b89\u5168\u7b56\u7565\uff08\u5982\u6dfb\u52a0\u6f0f\u6d1e\u63d0\u793a\uff09\u65e0\u6cd5\u7f13\u89e3\u8fd9\u4e9b\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9vibe coding\u5728\u5b89\u5168\u654f\u611f\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\u63d0\u51fa\u4e86\u4e25\u91cd\u5173\u5207\uff0c\u8868\u660e\u5f53\u524dLLM\u7f16\u7a0b\u4ee3\u7406\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u98ce\u9669\u3002"}}
{"id": "2512.03906", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.03906", "abs": "https://arxiv.org/abs/2512.03906", "authors": ["Alberto Ronzoni", "Anina Antony", "Anjana M R", "Francesca De Leo", "Jesna Jose", "Mattia Freda", "Nandini Narayanankutty", "Rafflesia Khan", "Raji RV", "Thomas Diacci"], "title": "IBM Multilevel Process Mining vs de facto Object-Centric Process Mining approaches", "comment": null, "summary": "The academic evolution of process mining is moving toward object centric process mining, marking a significant shift in how processes are modeled and analyzed. IBM has developed its own distinctive approach called Multilevel Process Mining. This paper provides a description of the two approaches and presents a comparative analysis of their respective advantages and limitations. IBM leveraged this comparison to drive the evolution of IBM Process Mining product, creating the new Organizational Mining feature, an innovation that combines the best of the two approaches. Demonstrate the potential of this novel, innovative and distinct methodology with an example.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\u548cIBM\u591a\u7ea7\u8fc7\u7a0b\u6316\u6398\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6bd4\u8f83\u5f00\u53d1\u4e86\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u7ec4\u7ec7\u6316\u6398\u65b0\u529f\u80fd", "motivation": "\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u6b63\u5728\u5411\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\u53d1\u5c55\uff0cIBM\u5f00\u53d1\u4e86\u591a\u7ea7\u8fc7\u7a0b\u6316\u6398\u65b9\u6cd5\uff0c\u9700\u8981\u6bd4\u8f83\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u6765\u6307\u5bfc\u4ea7\u54c1\u6f14\u8fdb", "method": "\u5bf9\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\u548cIBM\u591a\u7ea7\u8fc7\u7a0b\u6316\u6398\u8fdb\u884c\u63cf\u8ff0\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u57fa\u4e8e\u6bd4\u8f83\u7ed3\u679c\u5f00\u53d1\u65b0\u7684\u7ec4\u7ec7\u6316\u6398\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u5c55\u793a\u5176\u6f5c\u529b", "result": "IBM\u57fa\u4e8e\u6bd4\u8f83\u5206\u6790\u5f00\u53d1\u4e86\u7ec4\u7ec7\u6316\u6398\u65b0\u529f\u80fd\uff0c\u8be5\u529f\u80fd\u7ed3\u5408\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5c55\u793a\u4e86\u8fd9\u79cd\u521b\u65b0\u65b9\u6cd5\u7684\u6f5c\u529b", "conclusion": "\u901a\u8fc7\u6bd4\u8f83\u4e24\u79cd\u8fc7\u7a0b\u6316\u6398\u65b9\u6cd5\uff0cIBM\u6210\u529f\u5f00\u53d1\u4e86\u521b\u65b0\u7684\u7ec4\u7ec7\u6316\u6398\u529f\u80fd\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u4e3a\u8fc7\u7a0b\u6316\u6398\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba"}}
{"id": "2512.03487", "categories": ["cs.DC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.03487", "abs": "https://arxiv.org/abs/2512.03487", "authors": ["Zhen Wang", "Bin Lin", "Qiang", "Ye"], "title": "Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks", "comment": null, "summary": "In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7a7a\u5929\u5730\u6d77\u4e00\u4f53\u5316\u7f51\u7edc\u7684\u53cc\u8fb9\u7f18\u8f85\u52a9\u8ba1\u7b97\u5378\u8f7d\u4e0e\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u5229\u7528\u65e0\u4eba\u673a\u548c\u4f4e\u8f68\u536b\u661f\u4e3a\u6d77\u4e0a\u81ea\u4e3b\u8239\u8236\u63d0\u4f9b\u5e76\u884c\u8ba1\u7b97\u670d\u52a1\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u964d\u4f4e\u7cfb\u7edf\u80fd\u8017", "motivation": "\u7a7a\u5929\u5730\u6d77\u4e00\u4f53\u5316\u7f51\u7edc\u4e2d\u6d77\u4e8b\u5e94\u7528\u7684\u8ba1\u7b97\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u6d77\u4e0a\u81ea\u4e3b\u8239\u8236\u7684\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\uff0c\u9700\u8981\u5229\u7528\u65e0\u4eba\u673a\u548c\u536b\u661f\u7b49\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u6765\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u80fd\u6548\u7684\u8ba1\u7b97\u9700\u6c42", "method": "\u63d0\u51fa\u53cc\u8fb9\u7f18\u8f85\u52a9\u8ba1\u7b97\u5378\u8f7d\u65b9\u6848\uff0c\u5141\u8bb8\u6d77\u4e0a\u81ea\u4e3b\u8239\u8236\u540c\u65f6\u5411\u65e0\u4eba\u673a\u548c\u4f4e\u8f68\u536b\u661f\u5378\u8f7d\u90e8\u5206\u8ba1\u7b97\u4efb\u52a1\uff1b\u91c7\u7528\u591a\u63a5\u5165\u65b9\u5f0f\uff1b\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u5efa\u6a21\uff0c\u8054\u5408\u4f18\u5316\u5378\u8f7d\u6a21\u5f0f\u3001\u5378\u8f7d\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff1b\u4f7f\u7528\u4ea4\u66ff\u4f18\u5316\u548c\u5206\u5c42\u65b9\u6cd5\u5206\u89e3\u539f\u59cb\u95ee\u9898", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u7684\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u76f8\u6bd4\u57fa\u51c6\u7b97\u6cd5\u5728\u6ee1\u8db3\u5ef6\u8fdf\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u4e86\u7cfb\u7edf\u80fd\u8017", "conclusion": "\u8be5\u53cc\u8fb9\u7f18\u8f85\u52a9\u8ba1\u7b97\u5378\u8f7d\u4e0e\u8d44\u6e90\u5206\u914d\u65b9\u6848\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7a7a\u5929\u5730\u6d77\u4e00\u4f53\u5316\u7f51\u7edc\u4e2d\u7684\u8ba1\u7b97\u9700\u6c42\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u4f18\u5316\u7cfb\u7edf\u80fd\u8017\uff0c\u4e3a\u6d77\u4e8b\u667a\u80fd\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848"}}
{"id": "2512.03421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.03421", "abs": "https://arxiv.org/abs/2512.03421", "authors": ["Hexiang Xu", "Hengyuan Liu", "Yonghao Wu", "Xiaolan Kang", "Xiang Chen", "Yong Liu"], "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "comment": "The paper has been accepted for publication in The Journal of Systems & Software", "summary": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u5728\u6545\u969c\u5b9a\u4f4d\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684\u5148\u8fdb\u6a21\u578b\uff08\u5982OpenAI o3\u548cDeepSeekR1\uff09\u5728\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u65b0\u624b\u7a0b\u5e8f\u5458\u7531\u4e8e\u7ecf\u9a8c\u6709\u9650\uff0c\u5728\u6545\u969c\u5b9a\u4f4d\u4e0a\u9762\u4e34\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u5982SBFL\u548cMBFL\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u80fd\u529b\uff0c\u800cLLM\u51ed\u501f\u5176\u7406\u89e3\u7a0b\u5e8f\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u80fd\u529b\uff0c\u6709\u671b\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u4f7f\u7528Codeflaws\u3001Condefects\u548c\u65b0\u6784\u5efa\u7684BugT\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e866\u4e2a\u95ed\u6e90\u548c7\u4e2a\u5f00\u6e90LLM\u3002BugT\u6570\u636e\u96c6\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u7f13\u89e3\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u6545\u969c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684\u5148\u8fdb\u6a21\u578b\uff08\u5982OpenAI o3\u548cDeepSeekR1\uff09\u5728\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u6700\u5c0f\u3002\u65e0\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\uff08\u5982GPT-4\uff09\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u624d\u80fd\u4fdd\u6301\u6027\u80fd\u3002LLM\u5728\u7b80\u5355\u6545\u969c\u5b9a\u4f4d\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u968f\u7740\u95ee\u9898\u96be\u5ea6\u589e\u52a0\u51c6\u786e\u6027\u4e0b\u964d\u3002\u8fc7\u5ea6\u63a8\u7406\u548c\u8ba1\u7b97\u6210\u672c\u662f\u4e3b\u8981\u6311\u6218\u3002", "conclusion": "LLM\u5728\u63d0\u9ad8\u8c03\u8bd5\u6548\u7387\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u5bf9\u65b0\u624b\u7a0b\u5e8f\u5458\u6709\u663e\u8457\u5e2e\u52a9\u4ef7\u503c\u3002\u4f46\u9700\u8981\u5728\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2512.03565", "categories": ["cs.DC", "cs.CE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.03565", "abs": "https://arxiv.org/abs/2512.03565", "authors": ["Luis Gall", "Samuel James Newcome", "Fabio Alexander Gratl", "Markus M\u00fchlh\u00e4u\u00dfer", "Manish Kumar Mishra", "Hans-Joachim Bungartz"], "title": "Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas", "comment": "20 pages, 8 figures. Submitted to the 5th International Conference on Computational Engineering (ICCE 2024). No changes were made after the peer review process", "summary": "Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.\n  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.\n  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u7d22\u4e86SIMD\u5411\u91cf\u5316\u6280\u672f\u6765\u4f18\u5316\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u7684\u7c92\u5b50\u5bf9\u529b\u8ba1\u7b97\uff0c\u901a\u8fc7\u7814\u7a76\u7c92\u5b50\u503c\u52a0\u8f7d\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u7684\u987a\u5e8f\uff0c\u5e76\u6269\u5c55AutoPas\u7684\u52a8\u6001\u8c03\u4f18\u673a\u5236\u6765\u9009\u62e9\u8fd0\u884c\u65f6\u6700\u4f18\u7684\u5411\u91cf\u5316\u987a\u5e8f\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5728\u539f\u5b50\u5c3a\u5ea6\u4e0a\u4e3a\u7269\u7406\u8fc7\u7a0b\u63d0\u4f9b\u6709\u4ef7\u503c\u89c1\u89e3\uff0c\u4f46\u8ba1\u7b97\u6548\u7387\u9700\u8981\u4f18\u5316\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u6700\u4f18MD\u7b97\u6cd5\u53ef\u80fd\u5728\u8fd0\u884c\u65f6\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6a21\u62df\u7279\u5b9a\u53c2\u6570\uff08\u5982\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u5c45\u8bc6\u522b\u7b97\u6cd5\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u52a8\u6001\u8c03\u4f18\u673a\u5236\u3002", "method": "\u7814\u7a76\u5404\u79cdSIMD\u5411\u91cf\u5316\u6280\u672f\uff0c\u7279\u522b\u662f\u7c92\u5b50\u503c\u52a0\u8f7d\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u7684\u987a\u5e8f\u4f18\u5316\u3002\u6269\u5c55AutoPas\u7c92\u5b50\u6a21\u62df\u5e93\u7684\u52a8\u6001\u8c03\u4f18\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u9009\u62e9\u6700\u4f18\u7684\u5411\u91cf\u5316\u987a\u5e8f\u3002\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u4e0d\u540c\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\u987a\u5e8f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5728\u8fd0\u884c\u65f6\u8003\u8651\u4e0d\u540c\u7684\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\u987a\u5e8f\u76f8\u6bd4AutoPas\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u529b\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u52a8\u6001\u8c03\u4f18\u673a\u5236\u80fd\u591f\u6839\u636e\u6a21\u62df\u53c2\u6570\u9009\u62e9\u6700\u4f18\u7684\u5411\u91cf\u5316\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316SIMD\u5411\u91cf\u5316\u4e2d\u7c92\u5b50\u503c\u7684\u52a0\u8f7d\u987a\u5e8f\uff0c\u5e76\u5b9e\u73b0\u52a8\u6001\u8c03\u4f18\u673a\u5236\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u529b\u8ba1\u7b97\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u8fd0\u884c\u65f6\u53d8\u5316\u7684\u6a21\u62df\u6761\u4ef6\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.03815", "abs": "https://arxiv.org/abs/2512.03815", "authors": ["Shayan Ghasemnezhad", "Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith", "Heanh Sok"], "title": "Runnable Directories: The Solution to the Monorepo vs. Multi-repo Debate", "comment": null, "summary": "Modern software systems increasingly strain traditional codebase organization strategies. Monorepos offer consistency but often suffer from scalability issues and tooling complexity, while multi-repos provide modularity at the cost of coordination and dependency management challenges. As an answer to this trade-off, we present the Causify Dev system, a hybrid approach that integrates key benefits of both. Its central concept is the runnable directory -- a self-contained, independently executable unit with its own development, testing, and deployment lifecycles. Backed by a unified thin environment, shared helper utilities, and containerized Docker-based workflows, runnable directories enable consistent setups, isolated dependencies, and efficient CI/CD processes. The Causify Dev approach provides a practical middle ground between monorepo and multi-repo strategies, improving reliability and maintainability for growing, complex codebases.", "AI": {"tldr": "Causify Dev\u7cfb\u7edf\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5355\u4ed3\u5e93\u548c\u591a\u4ed3\u5e93\u7684\u4f18\u70b9\uff0c\u901a\u8fc7\u53ef\u8fd0\u884c\u76ee\u5f55\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u7684\u4ee3\u7801\u7ec4\u7ec7\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u5bf9\u4f20\u7edf\u4ee3\u7801\u5e93\u7ec4\u7ec7\u7b56\u7565\u63d0\u51fa\u4e86\u6311\u6218\uff1a\u5355\u4ed3\u5e93\u63d0\u4f9b\u4e00\u81f4\u6027\u4f46\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u5de5\u5177\u590d\u6742\u6027\uff1b\u591a\u4ed3\u5e93\u63d0\u4f9b\u6a21\u5757\u5316\u4f46\u9762\u4e34\u534f\u8c03\u548c\u4f9d\u8d56\u7ba1\u7406\u56f0\u96be\u3002\u9700\u8981\u4e00\u79cd\u6298\u4e2d\u65b9\u6848\u6765\u89e3\u51b3\u8fd9\u4e9b\u6743\u8861\u3002", "method": "\u63d0\u51faCausify Dev\u7cfb\u7edf\uff0c\u6838\u5fc3\u6982\u5ff5\u662f\"\u53ef\u8fd0\u884c\u76ee\u5f55\"\u2014\u2014\u81ea\u5305\u542b\u3001\u72ec\u7acb\u53ef\u6267\u884c\u7684\u5355\u5143\uff0c\u5177\u6709\u81ea\u5df1\u7684\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u90e8\u7f72\u751f\u547d\u5468\u671f\u3002\u7cfb\u7edf\u5305\u542b\u7edf\u4e00\u7684\u8f7b\u91cf\u73af\u5883\u3001\u5171\u4eab\u8f85\u52a9\u5de5\u5177\u548c\u57fa\u4e8eDocker\u7684\u5bb9\u5668\u5316\u5de5\u4f5c\u6d41\u3002", "result": "\u53ef\u8fd0\u884c\u76ee\u5f55\u80fd\u591f\u5b9e\u73b0\u4e00\u81f4\u7684\u8bbe\u7f6e\u3001\u9694\u79bb\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u9ad8\u6548\u7684CI/CD\u6d41\u7a0b\u3002\u8be5\u7cfb\u7edf\u5728\u5355\u4ed3\u5e93\u548c\u591a\u4ed3\u5e93\u7b56\u7565\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4e2d\u95f4\u5730\u5e26\u3002", "conclusion": "Causify Dev\u65b9\u6cd5\u4e3a\u4e0d\u65ad\u589e\u957f\u7684\u590d\u6742\u4ee3\u7801\u5e93\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4ee3\u7801\u7ec4\u7ec7\u7b56\u7565\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.03644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03644", "abs": "https://arxiv.org/abs/2512.03644", "authors": ["Bohan Zhao", "Yuanhong Wang", "Chenglin Liu", "Jiagi Pan", "Guang Yang", "Ruitao Liu", "Tingrui Zhang", "Kai Luo", "Wei Xu"], "title": "FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management", "comment": null, "summary": "Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.", "AI": {"tldr": "FFTrainer\u5229\u7528\u7f51\u7edc\u5269\u4f59\u5e26\u5bbd\u5feb\u901f\u4fdd\u5b58\u548c\u52a0\u8f7d\u72b6\u6001\uff0c\u51cf\u5c11LLM\u8bad\u7ec3\u4e2d\u7684\u56de\u6eda\u548c\u6062\u590d\u65f6\u95f4", "motivation": "\u968f\u7740LLM\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\uff0c\u8282\u70b9\u6545\u969c\u3001\u6062\u590d\u65f6\u95f4\u957f\u548c\u5927\u68c0\u67e5\u70b9\u7b49\u95ee\u9898\u964d\u4f4e\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u4f20\u7edf\u7684\u5f02\u6b65\u68c0\u67e5\u70b9\u65b9\u6cd5\u8981\u4e48\u89e6\u53d1\u6602\u8d35\u7684\u56de\u6eda\uff0c\u8981\u4e48\u589e\u52a0\u8fc7\u591a\u5f00\u9500\u3002", "method": "FFTrainer\u5229\u7528\u7f51\u7edc\u5269\u4f59\u5bb9\u91cf\u5feb\u901f\u4fdd\u5b58\u548c\u52a0\u8f7d\u72b6\u6001\uff0c\u9632\u6b62\u56de\u6eda\u5e76\u52a0\u901f\u6062\u590d\u8fc7\u7a0b\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u68c0\u67e5\u70b9\u65b9\u6cd5\uff0cFFTrainer\u5c06\u6062\u590d\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe98%\uff0cGPU\u5229\u7528\u7387\u635f\u5931\u964d\u4f4e\u9ad8\u8fbe68%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u8bad\u7ec3\u3002", "conclusion": "FFTrainer\u901a\u8fc7\u6709\u6548\u5229\u7528\u7f51\u7edc\u8d44\u6e90\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03868", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.03868", "abs": "https://arxiv.org/abs/2512.03868", "authors": ["Shree Hari Bittugondanahalli Indra Kumar", "Lilia Rodrigues Sampaio", "Andr\u00e9 Martin", "Andrey Brito", "Christof Fetzer"], "title": "A Comprehensive Study on the Impact of Vulnerable Dependencies on Open-Source Software", "comment": null, "summary": "Open-source libraries are widely used by software developers to speed up the development of products, however, they can introduce security vulnerabilities, leading to incidents like Log4Shell. With the expanding usage of open-source libraries, it becomes even more imperative to comprehend and address these dependency vulnerabilities. The use of Software Composition Analysis (SCA) tools does greatly help here as they provide a deep insight on what dependencies are used in a project, enhancing the security and integrity in the software supply chain. In order to learn how wide spread vulnerabilities are and how quickly they are being fixed, we conducted a study on over 1k open-source software projects with about 50k releases comprising several languages such as Java, Python, Rust, Go, Ruby, PHP, and JavaScript. Our objective is to investigate the severity, persistence, and distribution of these vulnerabilities, as well as their correlation with project metrics such as team and contributors size, activity and release cycles. In order to perform such analysis, we crawled over 1k projects from github including their version history ranging from 2013 to 2023 using VODA, our SCA tool. Using our approach, we can provide information such as library versions, dependency depth, and known vulnerabilities, and how they evolved over the software development cycle. Being larger and more diverse than datasets used in earlier works and studies, ours provides better insights and generalizability of the gained results. The data collected answers several research questions about the dependency depth and the average time a vulnerability persists. Among other findings, we observed that for most programming languages, vulnerable dependencies are transitive, and a critical vulnerability persists in average for over a year before being fixed.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e861000\u591a\u4e2a\u5f00\u6e90\u9879\u76ee\u76845\u4e07\u6b21\u53d1\u5e03\uff0c\u53d1\u73b0\u6f0f\u6d1e\u4f9d\u8d56\u4e3b\u8981\u662f\u4f20\u9012\u6027\u7684\uff0c\u5173\u952e\u6f0f\u6d1e\u5e73\u5747\u9700\u8981\u4e00\u5e74\u591a\u624d\u80fd\u4fee\u590d\u3002", "motivation": "\u5f00\u6e90\u5e93\u5e7f\u6cdb\u4f7f\u7528\u4f46\u53ef\u80fd\u5f15\u5165\u5b89\u5168\u6f0f\u6d1e\uff08\u5982Log4Shell\uff09\uff0c\u968f\u7740\u4f7f\u7528\u91cf\u589e\u52a0\uff0c\u7406\u89e3\u5e76\u89e3\u51b3\u8fd9\u4e9b\u4f9d\u8d56\u6f0f\u6d1e\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002SCA\u5de5\u5177\u80fd\u5e2e\u52a9\u5206\u6790\u9879\u76ee\u4f9d\u8d56\uff0c\u4f46\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u7814\u7a76\u6765\u4e86\u89e3\u6f0f\u6d1e\u7684\u666e\u904d\u6027\u548c\u4fee\u590d\u901f\u5ea6\u3002", "method": "\u4f7f\u7528VODA\uff08SCA\u5de5\u5177\uff09\u722c\u53d61000\u591a\u4e2aGitHub\u9879\u76ee\u4ece2013\u52302023\u5e74\u7684\u7248\u672c\u5386\u53f2\uff0c\u5206\u6790Java\u3001Python\u3001Rust\u3001Go\u3001Ruby\u3001PHP\u3001JavaScript\u7b49\u591a\u79cd\u8bed\u8a00\u9879\u76ee\u3002\u6536\u96c6\u5e93\u7248\u672c\u3001\u4f9d\u8d56\u6df1\u5ea6\u3001\u5df2\u77e5\u6f0f\u6d1e\u7b49\u4fe1\u606f\uff0c\u5e76\u8ffd\u8e2a\u5176\u5728\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u4e2d\u7684\u6f14\u53d8\u3002", "result": "1. \u5927\u591a\u6570\u7f16\u7a0b\u8bed\u8a00\u4e2d\uff0c\u6f0f\u6d1e\u4f9d\u8d56\u4e3b\u8981\u662f\u4f20\u9012\u6027\u7684\uff08transitive\uff09\uff1b2. \u5173\u952e\u6f0f\u6d1e\u5e73\u5747\u9700\u8981\u8d85\u8fc7\u4e00\u5e74\u65f6\u95f4\u624d\u80fd\u88ab\u4fee\u590d\uff1b3. \u6570\u636e\u96c6\u6bd4\u5148\u524d\u7814\u7a76\u66f4\u5927\u66f4\u591a\u6837\u5316\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6d1e\u5bdf\u529b\u548c\u7ed3\u679c\u6cdb\u5316\u6027\uff1b4. \u56de\u7b54\u4e86\u5173\u4e8e\u4f9d\u8d56\u6df1\u5ea6\u548c\u6f0f\u6d1e\u6301\u7eed\u65f6\u95f4\u7684\u591a\u4e2a\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u5f00\u6e90\u9879\u76ee\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u4e25\u91cd\uff0c\u7279\u522b\u662f\u4f20\u9012\u6027\u4f9d\u8d56\u6f0f\u6d1e\uff0c\u4e14\u4fee\u590d\u5468\u671f\u8fc7\u957f\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u6f0f\u6d1e\u7ba1\u7406\u673a\u5236\u6765\u6539\u5584\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u3002"}}
{"id": "2512.03697", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.03697", "abs": "https://arxiv.org/abs/2512.03697", "authors": ["Rafael Ravedutti Lucio Machado", "Jan Eitzinger", "Georg Hager", "Gerhard Wellein"], "title": "On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs", "comment": "8 pages, 4 figures, conference", "summary": "This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728Fritz\u548cAlex HPC\u96c6\u7fa4\u4e0a\u5bf9\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548cGromacs\u8f6f\u4ef6\u5305\u8fdb\u884c\u80fd\u6548\u5206\u6790\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u4f7f\u7528MPI\u5e76\u884c\u5316\u5728Intel Ice Lake/Sapphire Rapids CPU\u548cNvidia A40/A100 GPU\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c55\u793a\u6d4b\u91cf\u7ed3\u679c\u5e76\u8ba8\u8bba\u5b9e\u9a8c\u4e2d\u7684\u9677\u9631\uff0c\u63d0\u51fa\u672a\u6765\u80fd\u6548\u5206\u6790\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63ed\u793a\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u4e0a\u8fdb\u884c\u80fd\u6548\u5206\u6790\u65f6\u9762\u4e34\u7684\u5b9e\u9645\u6311\u6218\u548c\u9677\u9631\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u4e0d\u540c\u786c\u4ef6\u67b6\u6784\uff08CPU\u548cGPU\uff09\u548c\u4e0d\u540c\u4ee3\u9645\u5904\u7406\u5668\u65f6\u9047\u5230\u7684\u6d4b\u91cf\u548c\u5206\u6790\u56f0\u96be\u3002\u901a\u8fc7\u5b9e\u9645\u6848\u4f8b\u5206\u6790\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u907f\u514d\u5e38\u89c1\u9519\u8bef\uff0c\u63d0\u9ad8\u80fd\u6548\u7814\u7a76\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5728Fritz\u548cAlex HPC\u96c6\u7fa4\u4e0a\uff0c\u4f7f\u7528MPI\u5e76\u884c\u5316\u5728\u5b8c\u6574\u7684CPU\u63d2\u69fd\u4e0a\u8fd0\u884c\u5b9e\u9a8c\uff0c\u786c\u4ef6\u5305\u62ecIntel Ice Lake\u548cSapphire Rapids CPU\uff0c\u4ee5\u53caNvidia A40\u548cA100 GPU\u3002\u4f7f\u7528Likwid\uff08\u9488\u5bf9CPU\uff09\u548cNvidia profiling\u5de5\u5177\uff08\u9488\u5bf9GPU\uff09\u8fdb\u884c\u6027\u80fd\u6307\u6807\u6d4b\u91cf\u548c\u80fd\u6548\u5206\u6790\u3002\u901a\u8fc7\u5bf9\u6bd4\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8bc6\u522b\u6d4b\u91cf\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\u548c\u9677\u9631\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\uff08Intel Ice Lake/Sapphire Rapids CPU\u548cNvidia A40/A100 GPU\uff09\u4e0a\u8fd0\u884c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548cGromacs\u5e94\u7528\u65f6\u83b7\u5f97\u7684\u80fd\u6548\u6d4b\u91cf\u7ed3\u679c\u3002\u63ed\u793a\u4e86\u5728\u4f7f\u7528\u4e0d\u540c\u6027\u80fd\u5206\u6790\u5de5\u5177\u65f6\u9047\u5230\u7684\u6d4b\u91cf\u4e0d\u4e00\u81f4\u6027\u3001\u5de5\u5177\u517c\u5bb9\u6027\u95ee\u9898\u4ee5\u53ca\u8de8\u5e73\u53f0\u6bd4\u8f83\u7684\u6311\u6218\u3002\u5177\u4f53\u7ed3\u679c\u5305\u62ec\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u7684\u80fd\u6548\u8868\u73b0\u5bf9\u6bd4\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u8fdb\u884c\u80fd\u6548\u5206\u6790\u65f6\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5305\u62ec\u6d4b\u91cf\u5de5\u5177\u7684\u9650\u5236\u3001\u786c\u4ef6\u5dee\u5f02\u5e26\u6765\u7684\u6bd4\u8f83\u56f0\u96be\u4ee5\u53ca\u5b9e\u9a8c\u8bbe\u7f6e\u4e2d\u7684\u5e38\u89c1\u9677\u9631\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7ecf\u9a8c\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u80fd\u6548\u5206\u6790\u7814\u7a76\u7684\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\uff0c\u65e8\u5728\u63d0\u9ad8\u6b64\u7c7b\u7814\u7a76\u7684\u51c6\u786e\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u4e3aHPC\u80fd\u6548\u4f18\u5316\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6307\u5bfc\u3002"}}
{"id": "2512.03926", "categories": ["cs.SE", "cs.LO", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.03926", "abs": "https://arxiv.org/abs/2512.03926", "authors": ["Alexander Y. Bai", "Chris Hawblitzel", "Andrea Lattuada"], "title": "Tunable Automation in Automated Program Verification", "comment": null, "summary": "Automated verification tools based on SMT solvers have made significant progress in verifying complex software systems. However, these tools face a fundamental tension between automation and performance when dealing with quantifier instantiation -- the primary source of incompleteness and verification slowdown in SMT-based verifiers. Tools choose between aggressive quantifier instantiation that provides more automation but longer verification times, or conservative instantiation that responds quickly but may require more manual proof hints.\n  We present a mechanism that enables fine-grained control over the availability of quantified facts in verification contexts, allowing developers to selectively tune the level of automation. Our approach lets library authors provide different pre-defined automation levels while giving end-users the ability to further customize quantifier availability at the module, function, or proof context level.\n  We implement our techniques in Verus, a Rust-based verification tool, and evaluate them on multiple openly available codebases. Our empirical analysis demonstrates the automation-performance tradeoff and that selective quantifier management enables developers to select the appropriate level of automation in different contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728SMT\u6c42\u89e3\u5668\u9a8c\u8bc1\u5de5\u5177\u4e2d\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u91cf\u5316\u8bcd\u5b9e\u4f8b\u5316\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u5728\u81ea\u52a8\u5316\u7a0b\u5ea6\u548c\u9a8c\u8bc1\u6027\u80fd\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u6027\u8c03\u4f18\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eSMT\u6c42\u89e3\u5668\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\u5de5\u5177\u5728\u5904\u7406\u91cf\u5316\u8bcd\u5b9e\u4f8b\u5316\u65f6\u9762\u4e34\u81ea\u52a8\u5316\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\uff1a\u6fc0\u8fdb\u5b9e\u4f8b\u5316\u63d0\u4f9b\u66f4\u591a\u81ea\u52a8\u5316\u4f46\u9a8c\u8bc1\u65f6\u95f4\u957f\uff0c\u4fdd\u5b88\u5b9e\u4f8b\u5316\u54cd\u5e94\u5feb\u4f46\u9700\u8981\u66f4\u591a\u624b\u52a8\u8bc1\u660e\u63d0\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5236\uff0c\u5141\u8bb8\u5728\u9a8c\u8bc1\u4e0a\u4e0b\u6587\u4e2d\u5bf9\u91cf\u5316\u4e8b\u5b9e\u7684\u53ef\u7528\u6027\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u5e93\u4f5c\u8005\u53ef\u4ee5\u63d0\u4f9b\u9884\u5b9a\u4e49\u7684\u81ea\u52a8\u5316\u7ea7\u522b\uff0c\u540c\u65f6\u8ba9\u6700\u7ec8\u7528\u6237\u80fd\u591f\u5728\u6a21\u5757\u3001\u51fd\u6570\u6216\u8bc1\u660e\u4e0a\u4e0b\u6587\u7ea7\u522b\u8fdb\u4e00\u6b65\u81ea\u5b9a\u4e49\u91cf\u5316\u8bcd\u53ef\u7528\u6027\u3002", "result": "\u5728\u57fa\u4e8eRust\u7684\u9a8c\u8bc1\u5de5\u5177Verus\u4e2d\u5b9e\u73b0\u4e86\u8be5\u6280\u672f\uff0c\u5e76\u5728\u591a\u4e2a\u5f00\u6e90\u4ee3\u7801\u5e93\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u8bc1\u5206\u6790\u5c55\u793a\u4e86\u81ea\u52a8\u5316-\u6027\u80fd\u6743\u8861\uff0c\u5e76\u8bc1\u660e\u9009\u62e9\u6027\u91cf\u5316\u7ba1\u7406\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u9009\u62e9\u5408\u9002\u7684\u81ea\u52a8\u5316\u7ea7\u522b\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u91cf\u5316\u8bcd\u5b9e\u4f8b\u5316\u63a7\u5236\u673a\u5236\uff0c\u8be5\u7814\u7a76\u89e3\u51b3\u4e86SMT\u9a8c\u8bc1\u5de5\u5177\u4e2d\u81ea\u52a8\u5316\u4e0e\u6027\u80fd\u7684\u6839\u672c\u77db\u76fe\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u6839\u636e\u5177\u4f53\u9a8c\u8bc1\u9700\u6c42\u7075\u6d3b\u9009\u62e9\u9002\u5f53\u7684\u81ea\u52a8\u5316\u7ea7\u522b\u3002"}}
{"id": "2512.03825", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03825", "abs": "https://arxiv.org/abs/2512.03825", "authors": ["Aingeru Ramos", "Jose A Pascual", "Javier Navaridas", "Ivan Coluzza"], "title": "Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods", "comment": "14 pages, 7 figures (5 of them composed by 2 subfigures)", "summary": "Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOpenMP\u548cCUDA\u7684\u5e76\u884c\u5316Metropolis-Hastings\u5e76\u884c\u56de\u706b\u7b97\u6cd5\u5b9e\u73b0\uff0c\u5728CPU\u548cGPU\u4e0a\u5206\u522b\u83b7\u5f97\u4e8652\u500d\u548c986\u500d\u7684\u52a0\u901f\u6bd4\u3002", "motivation": "\u4f20\u7edfMCMC\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6784\u578b\u7a7a\u95f4\u65f6\u91c7\u6837\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u5e76\u884c\u56de\u706b(Parallel Tempering)\u7b49\u6539\u8fdb\u6280\u672f\u867d\u7136\u63d0\u9ad8\u4e86\u7cbe\u5ea6\uff0c\u4f46\u663e\u8457\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u9700\u8981\u901a\u8fc7\u5e76\u884c\u5316\u6765\u62b5\u6d88\u8fd9\u79cd\u8ba1\u7b97\u5f00\u9500\uff0c\u4f7fMCMC/PT\u6280\u672f\u80fd\u66f4\u6709\u6548\u5730\u8fd0\u884c\u5e76\u7814\u7a76\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86Metropolis-Hastings\u5e76\u884c\u56de\u706b\u7b97\u6cd5\u7684\u5e76\u884c\u5b9e\u73b0\uff0c\u4f7f\u7528OpenMP\u8fdb\u884c\u73b0\u4ee3CPU\u7684\u5e76\u884c\u5316\uff0c\u4f7f\u7528CUDA\u8fdb\u884cGPU\u7684\u5e76\u884c\u5316\u3002\u901a\u8fc7\u591a\u4e2a\u526f\u672c\u5e76\u884c\u8fd0\u884c\u5e76\u5b9a\u671f\u4ea4\u6362\u72b6\u6001\u6765\u63d0\u9ad8\u91c7\u6837\u7cbe\u5ea6\u3002", "result": "\u4f7f\u7528OpenMP\u572848\u6838CPU\u4e0a\u83b7\u5f97\u4e86\u6700\u592752\u500d\u7684\u52a0\u901f\u6bd4\uff0c\u4f7f\u7528CUDA\u5728GPU\u4e0a\u83b7\u5f97\u4e86986\u500d\u7684\u52a0\u901f\u6bd4\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e5f\u4e3a\u672a\u6765\u91cf\u5b50\u5b9e\u73b0\u7684\u76f8\u540c\u7b97\u6cd5\u63d0\u4f9b\u4e86\u57fa\u7840\u57fa\u51c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e76\u884c\u5b9e\u73b0\u663e\u8457\u63d0\u9ad8\u4e86MCMC/PT\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u5f97\u80fd\u591f\u66f4\u6709\u6548\u5730\u7814\u7a76\u590d\u6742\u7cfb\u7edf\uff0c\u5e76\u4e3a\u672a\u6765\u91cf\u5b50\u5b9e\u73b0\u63d0\u4f9b\u4e86\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2512.03927", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03927", "abs": "https://arxiv.org/abs/2512.03927", "authors": ["Liujianfu Wang", "Yuyang Du", "Yuchen Pan", "Soung Chang Liew", "Jiacheng Liu", "Kexin Chen"], "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.", "AI": {"tldr": "OD-MoE\uff1a\u4e00\u79cd\u65e0\u9700\u4e13\u5bb6\u7f13\u5b58\u7684\u5206\u5e03\u5f0fMoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6309\u9700\u52a0\u8f7d\u4e13\u5bb6\u53c2\u6570\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5185\u5b58\u5229\u7528", "motivation": "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u9762\u4e34\u5185\u5b58\u9650\u5236\u95ee\u9898\uff0c\u73b0\u6709\u7684\u4e13\u5bb6\u5378\u8f7d\u65b9\u6cd5\u867d\u7136\u5c06\u4e13\u5bb6\u53c2\u6570\u5b58\u50a8\u5728CPU\u5185\u5b58\u4e2d\uff0c\u4f46GPU\u5185\u5b58\u4e2d\u4fdd\u7559\u7684\u4e13\u5bb6\u7f13\u5b58\u5229\u7528\u7387\u4ecd\u7136\u8f83\u4f4e\uff0c\u65e0\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6709\u6548\u8fd0\u884c\u3002", "method": "\u63d0\u51faOD-MoE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8282\u70b9\u4e0a\u5e76\u884c\u5316\u4e13\u5bb6\u52a0\u8f7d\u548c\u4e13\u5bb6\u8ba1\u7b97\uff1b2\uff09\u8d85\u51c6\u786e\u7684\u6a21\u62df\u9884\u6d4b\u5668\uff0c\u5728\u4e13\u5bb6\u8ba1\u7b97\u8fdb\u884c\u65f6\u63d0\u524d\u591a\u5c42\u9884\u6d4b\u4e13\u5bb6\u6fc0\u6d3b\u3002\u901a\u8fc7\u6309\u9700\u52a8\u6001\u52a0\u8f7d\u548c\u53ca\u65f6\u9a71\u9010\u4e13\u5bb6\u53c2\u6570\uff0c\u6700\u5927\u5316GPU\u5185\u5b58\u5229\u7528\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1\uff09OD-MoE\u8fbe\u523099.94%\u7684\u4e13\u5bb6\u6fc0\u6d3b\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\uff1b2\uff09\u4ec5\u4f7f\u75281/3 GPU\u5185\u5b58\u5373\u53ef\u5b9e\u73b0\u5b8c\u5168GPU\u7f13\u5b58MoE\u90e8\u7f72\u7ea675%\u7684\u89e3\u7801\u901f\u5ea6\uff1b3\uff09\u4f7fMoE\u63a8\u7406\u80fd\u591f\u5728GPU\u5185\u5b58\u5c0f\u4e8e1GB\u7684\u8fb9\u7f18\u8282\u70b9\u4e0a\u8fd0\u884c\u3002", "conclusion": "OD-MoE\u901a\u8fc7\u6d88\u9664\u4e13\u5bb6\u7f13\u5b58\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548MoE\u63a8\u7406\uff0c\u4e3aLLM\u65f6\u4ee3\u4f4e\u6210\u672c\u7269\u8054\u7f51\u8bbe\u5907\u7684MoE\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.04054", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.04054", "abs": "https://arxiv.org/abs/2512.04054", "authors": ["Olasupo Ajayi", "Ryan Grant"], "title": "A Chronological Analysis of the Evolution of SmartNICs", "comment": "8 pages, 13 figures, 2 tables, Southern Africa Telecommunication Networks and Applications Conference (SATNAC) 2025", "summary": "Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u667a\u80fd\u7f51\u5361(SNIC)\u8fdb\u884c\u4e8615\u5e74(2010-2024)\u7684\u65f6\u5e8f\u5206\u6790\uff0c\u57fa\u4e8e370\u7bc7\u6587\u732e\u7814\u7a76\u4e86SNIC\u7684\u6f14\u53d8\u3001\u5236\u9020\u5546\u3001\u7528\u4f8b\u548c\u5e94\u7528\u9886\u57df\uff0c\u65e8\u5728\u6f84\u6e05\u5176\u5b9e\u9645\u7528\u9014\u548c\u9002\u7528\u6027\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u8bbe\u5907\u7684\u666e\u53ca\u548c\u5bf9\u66f4\u5feb\u7f51\u7edc\u8bbf\u95ee\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f20\u7edf\u7f51\u5361\u5df2\u5347\u7ea7\u4e3a\u667a\u80fd\u7f51\u5361(SNIC)\uff0c\u80fd\u591f\u5904\u7406\u6d77\u91cf\u6570\u636e\u5e76\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u5904\u7406\u3002\u7136\u800c\uff0c\u5c3d\u7ba1SNIC\u65e5\u76ca\u6d41\u884c\uff0c\u4f46\u5176\u786e\u5207\u7528\u9014\u548c\u9002\u7528\u6027\u4ecd\u5b58\u5728\u4e89\u8bae\uff0c\u7279\u522b\u662f\u968f\u7740\u52a0\u901f\u5668\u96c6\u6210\u5230SNIC\u4e2d\uff0c\u4f7f\u5176\u80fd\u591f\u5206\u62c5\u4e3b\u673aCPU\u7684\u5404\u79cd\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4e89\u8bae\u66f4\u52a0\u590d\u6742\u5316\u3002", "method": "\u91c7\u7528\u65f6\u5e8f\u5206\u6790\u65b9\u6cd5\uff0c\u6536\u96c6\u5e76\u5206\u6790\u4e862010\u5e74\u81f32024\u5e74\u671f\u95f4\u53d1\u8868\u7684370\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u5bf9SNIC\u7684\u6f14\u53d8\u5386\u7a0b\u3001\u5236\u9020\u5546\u60c5\u51b5\u3001\u5177\u4f53\u7528\u4f8b\u548c\u5e94\u7528\u9886\u57df\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5bf915\u5e74\u95f4\u6587\u732e\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u63ed\u793a\u4e86SNIC\u7684\u6280\u672f\u6f14\u8fdb\u8def\u5f84\u3001\u4e3b\u8981\u5236\u9020\u5546\u683c\u5c40\u3001\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4ee5\u53ca\u5728\u4e0d\u540c\u9886\u57df\u7684\u5e94\u7528\u60c5\u51b5\uff0c\u4e3a\u7406\u89e3SNIC\u7684\u5b9e\u9645\u4ef7\u503c\u548c\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5927\u89c4\u6a21\u6587\u732e\u5206\u6790\uff0c\u6f84\u6e05\u4e86\u667a\u80fd\u7f51\u5361\u7684\u53d1\u5c55\u8f68\u8ff9\u548c\u5e94\u7528\u73b0\u72b6\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7406\u89e3SNIC\u7684\u6280\u672f\u6f14\u8fdb\u3001\u5e02\u573a\u683c\u5c40\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u5173\u4e8eSNIC\u9002\u7528\u6027\u7684\u6301\u7eed\u4e89\u8bae\u3002"}}
