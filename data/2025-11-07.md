<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.SE](#cs.SE) [Total: 16]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [GPU-Based Floating-point Adaptive Lossless Compression](https://arxiv.org/abs/2511.04140)
*Zheng Li,Weiyan Wang,Ruiyuan Li,Chao Chen,Xianlei Long,Linjiang Zheng,Quanqing Xu,Chuanhui Yang*

Main category: cs.DB

TL;DR: Falcon是一个基于GPU的浮点自适应无损压缩框架，针对IoT和HPC领域的时间序列数据，解决了数据移动瓶颈、精度保持转换和异常值导致的稀疏性退化三大挑战。


<details>
  <summary>Details</summary>
Motivation: IoT和HPC领域产生大量浮点时间序列数据，需要在不损失精度的情况下高效压缩这些数据，同时利用现代GPU的大规模并行性实现前所未有的吞吐量。

Method: 1) 引入轻量级异步流水线隐藏CPU和GPU间数据传输的I/O延迟；2) 提出具有理论保证的精确快速浮点到整数转换方法；3) 设计自适应稀疏位平面无损编码策略。

Result: 在12个不同数据集上的实验表明，压缩比相比最先进的基于CPU的方法提高了9.1%，压缩吞吐量比最快的基于GPU的竞争对手高2.43倍，解压缩吞吐量高2.4倍。

Conclusion: Falcon框架成功解决了GPU浮点无损压缩的关键挑战，在压缩比和吞吐量方面均显著优于现有方法，为大规模浮点时间序列数据的高效压缩提供了有效解决方案。

Abstract: Domains such as IoT (Internet of Things) and HPC (High Performance Computing)
generate a torrential influx of floating-point time-series data. Compressing
these data while preserving their absolute fidelity is critical, and leveraging
the massive parallelism of modern GPUs offers a path to unprecedented
throughput. Nevertheless, designing such a high-performance GPU-based lossless
compressor faces three key challenges: 1) heterogeneous data movement
bottlenecks, 2) precision-preserving conversion complexity, and 3)
anomaly-induced sparsity degradation. To address these challenges, this paper
proposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN
framework. Specifically, Falcon first introduces a lightweight asynchronous
pipeline, which hides the I/O latency during the data transmission between the
CPU and GPU. Then, we propose an accurate and fast float-to-integer
transformation method with theoretical guarantees, which eliminates the errors
caused by floating-point arithmetic. Moreover, we devise an adaptive sparse
bit-plane lossless encoding strategy, which reduces the sparsity caused by
outliers. Extensive experiments on 12 diverse datasets show that our
compression ratio improves by 9.1% over the most advanced CPU-based method,
with compression throughput 2.43X higher and decompression throughput 2.4X
higher than the fastest GPU-based competitors, respectively.

</details>


### [2] [EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data](https://arxiv.org/abs/2511.04148)
*Xiaobo Zhao,Daniel E. Lucani*

Main category: cs.DB

TL;DR: EntroGD是一种基于熵指导的广义去重压缩框架，将复杂度从O(nd²)降低到O(nd)，在保持压缩性能的同时显著加速配置和分析过程。


<details>
  <summary>Details</summary>
Motivation: 传统的广义去重算法在处理高维数据时面临可扩展性挑战，GreedyGD算法的迭代位选择过程导致O(nd²)的复杂度，在高维数据上性能显著下降。

Method: 采用两步过程：首先生成浓缩样本以保持分析保真度，然后应用熵指导的位选择来最大化压缩效率。

Result: 在18个不同类型和维度的数据集上，EntroGD实现了与GD基压缩器和通用压缩器相当的压缩性能，配置时间比GreedyGD减少最多53.5倍，聚类加速最多31.6倍，且精度损失可忽略。

Conclusion: EntroGD为直接在压缩数据上执行分析提供了高效且可扩展的解决方案。

Abstract: Generalized Deduplication (GD) enables lossless compression with direct
analytics on compressed data by dividing data into \emph{bases} and
\emph{deviations} and performing dictionary encoding on the former. However, GD
algorithms face scalability challenges for high-dimensional data. For example,
the GreedyGD algorithm relies on an iterative bit-selection process across
$d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to
select bits to be used as bases and deviations. Although the $n$ data rows can
be reduced during training at the expense of performance, highly dimensional
data still experiences a marked loss in performance. This paper introduces
EntroGD, an entropy-guided GD framework that reduces complexity of the
bit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step
process. First, it generates condensed samples to preserve analytic fidelity.
Second, it applies entropy-guided bit selection to maximize compression
efficiency. Across 18 datasets of varying types and dimensionalities, EntroGD
achieves compression performance comparable to GD-based and universal
compressors, while reducing configuration time by up to 53.5$\times$ over
GreedyGD and accelerating clustering by up to 31.6$\times$ over the original
data with negligible accuracy loss by performing analytics on the condensed
samples, which are much fewer than original samples. Thus, EntroGD provides an
efficient and scalable solution to performing analytics directly on compressed
data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: OMPILOT是一个专门用于将C++代码翻译为OpenMP的领域特定编码器-解码器转换器，通过结合无监督和监督学习策略，在函数级别实现共享内存并行化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译方面显示出强大能力，但现有方法主要关注循环级转换，缺乏对更广泛语义上下文的捕获。需要专门针对OpenMP并行化的翻译工具。

Method: 使用定制的预训练目标，结合并行构造语义，采用无监督和监督学习策略，在函数级别进行代码翻译。

Result: 提出了OMPBLEU评估指标来专门评估OpenMP并行构造的正确性和质量，解决了传统翻译指标的局限性。

Conclusion: OMPILOT通过领域特定的转换器架构和专门的评估指标，有效提升了C++到OpenMP的代码翻译质量。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [4] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 提出了一种基于马尔可夫链的随机建模方法，用于分析边缘计算中的电源状态转换，通过AI驱动的预测性功率调节优于传统反应式方法，显著提高能效。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽然支持低延迟处理，但由于边缘设备的分布式特性和有限的能源资源，在电源管理方面面临挑战。

Method: 使用马尔可夫链进行随机建模，推导稳态概率并评估能耗，通过蒙特卡洛模拟验证模型，并进行敏感性分析。

Result: AI驱动的预测性功率调节相比传统方法能最小化不必要的状态转换，提高系统响应性，在异构边缘节点间优化工作负载分配，减少能耗差异。

Conclusion: 基于AI的电源管理策略通过预测工作负载需求并优化状态转换，显著提高了边缘计算系统的能源效率。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [5] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 提出了一种新的并行生成策略，通过所有进程在重新分配前协作生成，减少执行时间，同时消除收缩限制，显著降低重新配置成本。


<details>
  <summary>Details</summary>
Motivation: 现有MPI应用的可塑性方法存在局限性：要么重新生成整个应用（成本高），要么在收缩时无法完全释放不需要的进程，阻碍系统资源回收。

Method: 采用并行生成策略，所有进程在重新分配前协作进行生成，并移除收缩限制，使并行系统能更好地适应工作负载。

Result: 在保持竞争性扩展时间（最多1.25倍开销）的同时，实现快速收缩操作，成本降低至少20倍，在异构和同构系统上均得到验证。

Conclusion: 该策略有效解决了MPI应用可塑性中的重新配置成本问题，提高了系统资源利用率和作业执行效率。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [6] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 提出一种在低比特量化下实现动态稀疏推理的技术，通过zigzag量化布局、专用GEMV内核和紧凑运行时机制，在保持精度的同时提升解码吞吐量1.55倍


<details>
  <summary>Details</summary>
Motivation: 在终端设备部署大语言模型面临内存和计算能力限制，而动态稀疏激活特性与分组量化方法存在冲突，需要解决这一矛盾

Method: 采用zigzag模式量化布局组织权重以匹配激活稀疏性，设计专用GEMV内核充分利用并行计算单元，开发紧凑运行时机制收集稀疏索引

Result: 在多种模型规模和硬件配置下，解码吞吐量提升达1.55倍，同时保持与密集量化推理相当的精度

Conclusion: 结构化稀疏和量化可以在商用GPU上有效共存，为终端设备高效部署LLM提供可行方案

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [7] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 提出了一种基于MAPE-K架构的自保护分布式系统，引入了概率性移动拜占庭故障模型，用于分析攻击动态和驱动系统自保护策略。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统面临日益增长的安全威胁，攻击者技能不断提升，漏洞遍布整个系统栈。现有拜占庭故障模型在反映现实场景准确性方面存在局限。

Method: 在MAPE-K架构的分析组件中引入新的概率性移动拜占庭故障模型，通过数学分析计算拜占庭节点数量超过阈值或系统恢复到安全状态的时间。

Result: 建立了数学模型分析拜占庭感染传播速率与自恢复速率之间的关系，并通过仿真验证了系统在此假设下的行为特征。

Conclusion: 提出的概率性移动拜占庭故障模型能够更好地捕捉攻击动态，为分布式系统的自保护和重配置策略提供理论支持。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [8] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 提出动态并发概念，仅在当前系统状态下需要与并发操作仲裁时才使用强同步原语，并给出了动态并发的通用构造方法


<details>
  <summary>Details</summary>
Motivation: 同步是分布式计算可扩展性的主要障碍。并发操作在遇到冲突时需要同步，但冲突通常只在罕见状态下发生，因此需要动态检测冲突以适应当前系统状态

Method: 定义了动态并发概念，并提出了动态并发的通用构造方法，使操作只在必要时使用强同步原语

Result: 提出了动态并发框架，能够根据当前系统状态动态调整同步策略

Conclusion: 动态并发方法可以有效减少不必要的同步开销，提高分布式系统的可扩展性

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Tutorial Debriefing: Applied Statistical Causal Inference in Requirements Engineering](https://arxiv.org/abs/2511.03875)
*Julian Frattini,Hans-Martin Heyn,Robert Feldt,Richard Torkar*

Main category: cs.SE

TL;DR: 论文讨论了软件工程研究中因果推断的重要性，指出需要通过统计因果推断从观察数据中获取证据，因为随机对照试验可能不可行。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究需要将研究成果转化为实践，这要求证明工具、流程或指南与性能指标之间的因果关系。由于随机对照试验在法律、伦理或操作上可能不可行，需要可靠的统计因果推断方法。

Method: 提出使用统计因果推断从观察数据中获取因果关系的证据，作为随机对照试验的替代方案。

Result: 强调了在软件工程研究中应用统计因果推断的必要性和价值。

Conclusion: 软件工程研究社区需要采用统计因果推断方法来验证研究成果的实际效果，特别是在随机对照试验不可行的情况下。

Abstract: As any scientific discipline, the software engineering (SE) research
community strives to contribute to the betterment of the target population of
our research: software producers and consumers. We will only achieve this
betterment if we manage to transfer the knowledge acquired during research into
practice. This transferal of knowledge may come in the form of tools,
processes, and guidelines for software developers. However, the value of these
contributions hinges on the assumption that applying them causes an improvement
of the development process, user experience, or other performance metrics. Such
a promise requires evidence of causal relationships between an exposure or
intervention (i.e., the contributed tool, process or guideline) and an outcome
(i.e., performance metrics). A straight-forward approach to obtaining this
evidence is via controlled experiments in which a sample of a population is
randomly divided into a group exposed to the new tool, process, or guideline,
and a control group. However, such randomized control trials may not be
legally, ethically, or logistically feasible. In these cases, we need a
reliable process for statistical causal inference (SCI) from observational
data.

</details>


### [10] [Collaborative Agents for Automated Program Repair in Ruby](https://arxiv.org/abs/2511.03925)
*Nikta Akbarpour,Mahdieh Sadat Benis,Fatemeh Hendijani Fard,Ali Ouni,Mohamed Aymen Saied*

Main category: cs.SE

TL;DR: RAMP是一个轻量级框架，将程序修复作为Ruby语言的反馈驱动迭代过程，使用协作代理生成测试、反思错误并改进修复方案，无需依赖大型多语言修复数据库或昂贵微调。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化程序修复方法计算成本高且主要关注少数语言，Ruby作为广泛使用的Web开发语言在APR研究中很少受到关注。

Method: RAMP采用多代理协作框架，通过生成针对性测试、错误反思和候选修复迭代优化，直接对Ruby进行轻量级提示和测试驱动反馈。

Result: 在XCodeEval基准测试中，RAMP在Ruby上达到67%的pass@1，优于先前方法，在5次迭代内快速收敛，特别擅长修复错误答案、编译错误和运行时错误。

Conclusion: RAMP为多代理修复策略提供了新见解，并为将基于LLM的调试工具扩展到研究较少的语言奠定了基础。

Abstract: Automated Program Repair (APR) has advanced rapidly with Large Language
Models (LLMs), but most existing methods remain computationally expensive, and
focused on a small set of languages. Ruby, despite its widespread use in web
development and the persistent challenges faced by its developers, has received
little attention in APR research. In this paper, we introduce RAMP, a novel
lightweight framework that formulates program repair as a feedback-driven,
iterative process for Ruby. RAMP employs a team of collaborative agents that
generate targeted tests, reflect on errors, and refine candidate fixes until a
correct solution is found. Unlike prior approaches, RAMP is designed to avoid
reliance on large multilingual repair databases or costly fine-tuning, instead
operating directly on Ruby through lightweight prompting and test-driven
feedback. Evaluation on the XCodeEval benchmark shows that RAMP achieves a
pass@1 of 67% on Ruby, outper-forming prior approaches. RAMP converges quickly
within five iterations, and ablation studies confirm that test generation and
self-reflection are key drivers of its performance. Further analysis shows that
RAMP is particularly effective at repairing wrong answers, compilation errors,
and runtime errors. Our approach provides new insights into multi-agent repair
strategies, and establishes a foundation for extending LLM-based debugging
tools to under-studied languages.

</details>


### [11] [PEFA-AI: Advancing Open-source LLMs for RTL generation using Progressive Error Feedback Agentic-AI](https://arxiv.org/abs/2511.03934)
*Athma Narayanan,Mahesh Subedar,Omesh Tickoo*

Main category: cs.SE

TL;DR: 提出了一种多代理协作的RTL生成流程，通过渐进式错误反馈系统实现无需人工干预的自动代码生成，在开源和闭源LLM上均取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂RTL生成任务中需要人工干预的问题，通过代理协作和自校正机制实现完全自动化的硬件设计流程。

Method: 采用多代理架构结合专用LLM和硬件仿真工具，引入渐进式错误反馈系统(PEFA)，通过迭代错误反馈逐步增加方法复杂度，并包含编译、功能正确性和可综合构造检查。

Result: 在两个开源自然语言到RTL数据集上验证，实现了最先进的通过率，在token数量上高效，有效缩小了开源和闭源LLM之间的性能差距。

Conclusion: 该方法为RTL生成设立了新的基准，展示了代理协作和渐进式错误反馈在复杂硬件设计任务中的有效性。

Abstract: We present an agentic flow consisting of multiple agents that combine
specialized LLMs and hardware simulation tools to collaboratively complete the
complex task of Register Transfer Level (RTL) generation without human
intervention. A key feature of the proposed flow is the progressive error
feedback system of agents (PEFA), a self-correcting mechanism that leverages
iterative error feedback to progressively increase the complexity of the
approach. The generated RTL includes checks for compilation, functional
correctness, and synthesizable constructs. To validate this adaptive approach
to code generation, benchmarking is performed using two opensource natural
language-to-RTL datasets. We demonstrate the benefits of the proposed approach
implemented on an open source agentic framework, using both open- and
closed-source LLMs, effectively bridging the performance gap between them.
Compared to previously published methods, our approach sets a new benchmark,
providing state-of-the-art pass rates while being efficient in token counts.

</details>


### [12] [PSD2Code: Automated Front-End Code Generation from Design Files via Multimodal Large Language Models](https://arxiv.org/abs/2511.04012)
*Yongxi Chen,Lei Chen*

Main category: cs.SE

TL;DR: PSD2Code是一个多模态设计到代码生成方法，通过PSD文件解析和资源对齐来生成生产就绪的React+SCSS代码，解决了现有方法的结构不一致性和资源错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有设计到代码生成方法存在结构不一致、资源错位和生产就绪性有限的问题，需要一种能够准确提取设计信息并生成高质量前端代码的解决方案。

Method: 提出ParseAlignGenerate流水线，从PSD文件中提取层次结构、图层属性和元数据，采用基于约束的对齐策略确保生成元素与设计资源的一致性，并通过结构化提示构建增强可控性和代码质量。

Result: 综合评估显示在代码相似性、视觉保真度和生产就绪性等多个指标上显著优于现有方法，且在不同大语言模型上表现出强大的模型独立性。

Conclusion: 将结构化设计信息与多模态大语言模型相结合，为工业级代码生成提供了有效方法，是设计驱动自动化前端开发的重要进展。

Abstract: Design-to-code generation has emerged as a promising approach to bridge the
gap between design prototypes and deployable frontend code. However, exist?ing
methods often suffer from structural inconsistencies, asset misalignment, and
limited production readiness. This paper presents PSD2Code, a novel multi?modal
approach that leverages PSD file parsing and asset alignment to generate
production-ready React+SCSS code. Our method introduces a ParseAlignGener?ate
pipeline that extracts hierarchical structures, layer properties, and metadata
from PSD files, providing large language models with precise spatial
relation?ships and semantic groupings for frontend code generation. The system
employs a constraint-based alignment strategy that ensures consistency between
generated elements and design resources, while a structured prompt construction
enhances controllability and code quality. Comprehensive evaluation
demonstrates sig?nificant improvements over existing methods across multiple
metrics including code similarity, visual fidelity, and production readiness.
The method exhibits strong model independence across different large language
models, validating the effectiveness of integrating structured design
information with multimodal large language models for industrial-grade code
generation, marking an important step toward design-driven automated frontend
development.

</details>


### [13] [Specification-Guided Vulnerability Detection with Large Language Models](https://arxiv.org/abs/2511.04014)
*Hao Zhu,Jia Li,Cuiyun Gao,Jiaru Qian,Yihong Dong,Huanyu Liu,Lecheng Wang,Ziliang Wang,Xiaolong Hu,Ge Li*

Main category: cs.SE

TL;DR: VulInstruct是一个基于安全规范的漏洞检测方法，通过从历史漏洞中提取安全规范来指导LLMs检测新漏洞，显著提升了漏洞检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在漏洞检测方面表现有限，主要原因是缺乏对安全规范的理解——即代码应如何行为才能保持安全的期望。当代码行为与这些期望不符时，就会形成潜在漏洞。

Method: VulInstruct从两个角度构建规范知识库：(1)从跨项目的高质量补丁中提取通用规范，捕捉基本安全行为；(2)从特定仓库中重复违规中提取领域特定规范。通过检索相关历史案例和规范，让LLMs基于期望的安全行为进行推理。

Result: 在PrimeVul数据集上，VulInstruct达到45.0% F1分数（提升32.7%）和37.7%召回率（提升50.8%），独特检测到24.3%的漏洞——比任何基线方法多2.4倍。在成对评估中相对提升32.3%。

Conclusion: VulInstruct成功发现了一个先前未知的高严重性漏洞(CVE-2025-56538)，证明了其在真实世界漏洞发现中的实用价值。

Abstract: Large language models (LLMs) have achieved remarkable progress in code
understanding tasks. However, they demonstrate limited performance in
vulnerability detection and struggle to distinguish vulnerable code from
patched code. We argue that LLMs lack understanding of security specifications
-- the expectations about how code should behave to remain safe. When code
behavior differs from these expectations, it becomes a potential vulnerability.
However, such knowledge is rarely explicit in training data, leaving models
unable to reason about security flaws. We propose VulInstruct, a
specification-guided approach that systematically extracts security
specifications from historical vulnerabilities to detect new ones. VulInstruct
constructs a specification knowledge base from two perspectives: (i) General
specifications from high-quality patches across projects, capturing fundamental
safe behaviors; and (ii) Domain-specific specifications from repeated
violations in particular repositories relevant to the target code. VulInstruct
retrieves relevant past cases and specifications, enabling LLMs to reason about
expected safe behaviors rather than relying on surface patterns. We evaluate
VulInstruct under strict criteria requiring both correct predictions and valid
reasoning. On PrimeVul, VulInstruct achieves 45.0% F1-score (32.7% improvement)
and 37.7% recall (50.8% improvement) compared to baselines, while uniquely
detecting 24.3% of vulnerabilities -- 2.4x more than any baseline. In pair-wise
evaluation, VulInstruct achieves 32.3% relative improvement. VulInstruct also
discovered a previously unknown high-severity vulnerability (CVE-2025-56538) in
production code, demonstrating practical value for real-world vulnerability
discovery. All code and supplementary materials are available at
https://github.com/zhuhaopku/VulInstruct-temp.

</details>


### [14] [LLM-Driven Adaptive Source-Sink Identification and False Positive Mitigation for Static Analysis](https://arxiv.org/abs/2511.04023)
*Shiyin Lin*

Main category: cs.SE

TL;DR: AdaTaint是一个基于LLM的污点分析框架，通过神经符号推理自适应推断源/汇规范并过滤虚假警报，相比现有方法显著减少误报并提高召回率。


<details>
  <summary>Details</summary>
Motivation: 静态分析在发现软件漏洞方面有效，但存在源-汇规范不完整和误报过多的问题。

Method: 结合LLM推理与符号验证，通过程序事实和约束验证来确保适应性和确定性。

Result: 在多个基准测试中，AdaTaint平均减少43.7%的误报，提高11.2%的召回率，同时保持有竞争力的运行时开销。

Conclusion: 将LLM推理与符号验证相结合为更准确可靠的静态漏洞分析提供了实用路径。

Abstract: Static analysis is effective for discovering software vulnerabilities but
notoriously suffers from incomplete source--sink specifications and excessive
false positives (FPs). We present \textsc{AdaTaint}, an LLM-driven taint
analysis framework that adaptively infers source/sink specifications and
filters spurious alerts through neuro-symbolic reasoning. Unlike LLM-only
detectors, \textsc{AdaTaint} grounds model suggestions in program facts and
constraint validation, ensuring both adaptability and determinism.
  We evaluate \textsc{AdaTaint} on Juliet 1.3, SV-COMP-style C benchmarks, and
three large real-world projects. Results show that \textsc{AdaTaint} reduces
false positives by \textbf{43.7\%} on average and improves recall by
\textbf{11.2\%} compared to state-of-the-art baselines (CodeQL, Joern, and
LLM-only pipelines), while maintaining competitive runtime overhead. These
findings demonstrate that combining LLM inference with symbolic validation
offers a practical path toward more accurate and reliable static vulnerability
analysis.

</details>


### [15] [Benchmarking and Studying the LLM-based Agent System in End-to-End Software Development](https://arxiv.org/abs/2511.04064)
*Zhengran Zeng,Yixin Li,Rui Xie,Wei Ye,Shikun Zhang*

Main category: cs.SE

TL;DR: 该论文构建了E2EDevBench基准测试和混合评估框架，通过实证研究发现当前最先进的软件开发代理只能满足约50%的需求，主要瓶颈在于需求遗漏和验证不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的自主软件开发代理评估面临挑战，包括基准测试过于简单化、不同代理架构难以公平比较等问题。

Method: 构建了具有挑战性的E2EDevBench基准，提出结合测试用例功能评估和细粒度LLM需求验证的混合评估框架，并在统一基础上对三种代表性代理架构进行实证研究。

Result: 最先进的代理在E2EDevBench上只能满足约50%的需求，成功与否关键取决于任务分解和协作的架构策略，主要瓶颈是需求遗漏和验证不足。

Conclusion: 该工作为社区提供了更现实的基准测试、全面评估框架，以及对软件开发代理当前能力和核心挑战的关键见解，指导未来研究增强需求理解和规划能力。

Abstract: The development of LLM-based autonomous agents for end-to-end software
development represents a significant paradigm shift in software engineering.
However, the scientific evaluation of these systems is hampered by significant
challenges, including overly simplistic benchmarks and the difficulty of
conducting fair comparisons between different agent architectures due to
confounding implementation variables. To address these limitations, we first
construct a challenging and dynamically curated E2EDevBench to simulate
realistic development scenarios. Second, we propose a hybrid evaluation
framework that combines test-case-based functional assessment with
fine-grained, LLM-based requirement verification. Using this framework, we
conduct a controlled empirical study on three representative agent
architectures implemented upon a unified foundation to isolate the impact of
workflow design. Our findings reveal that state-of-the-art agents can fulfill
approximately 50\% of requirements on \bench{}, but their success is critically
dependent on the architectural strategy for task decomposition and
collaboration. Furthermore, our analysis indicates that the primary bottleneck
is the omission of requirements and inadequate self-verification. This work
provides the community with a more realistic benchmark, a comprehensive
evaluation framework, and crucial insights into the current capabilities and
core challenges of software development agents, guiding future research toward
enhancing requirement comprehension and planning.

</details>


### [16] [How Natural Language Proficiency Shapes GenAI Code for Software Engineering Tasks](https://arxiv.org/abs/2511.04115)
*Ruksit Rojpaisarnkit,Youmei Fan,Kenichi Matsumoto,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 研究发现英语语言熟练度独立于提示技术，会影响LLM生成代码的熟练度和正确性。高熟练度提示在所有模型中都能产生更正确的代码。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型工具在软件工程中的广泛应用，自然语言提示已成为开发者和大型语言模型之间的关键接口。虽然已有研究关注提示结构，但自然语言熟练度作为影响生成代码质量的因素尚未充分探索。

Method: 使用HumanEval数据集，系统地将164个编程任务的英语提示熟练度从基础到高级进行变化，并测量生成的代码熟练度和正确性。

Result: LLM默认使用中级（B2）自然语言水平。虽然对代码熟练度的影响因模型而异，但高熟练度提示在所有模型中都能一致产生更正确的代码。

Conclusion: 自然语言熟练度是控制代码生成的关键杠杆，帮助开发者定制AI输出并提高解决方案的可靠性。

Abstract: With the widespread adoption of Foundation Model (FM)-powered tools in
software engineering, the natural language prompt has become a critical
interface between developers and Large Language Models (LLMs). While much
research has focused on prompt structure, the natural language proficiency is
an underexplored factor that can influence the quality of generated code. This
paper investigates whether the English language proficiency itself independent
of the prompting technique affects the proficiency and correctness of code
generated by LLMs. Using the HumanEval dataset, we systematically varied the
English proficiency of prompts from basic to advanced for 164 programming tasks
and measured the resulting code proficiency and correctness. Our findings show
that LLMs default to an intermediate (B2) natural language level. While the
effect on the resulting code proficiency was model-dependent, we found that
higher-proficiency prompts consistently yielded more correct code across all
models. These results demonstrate that natural language proficiency is a key
lever for controlling code generation, helping developers tailor AI output and
improve the reliability of solutions.

</details>


### [17] [Are We Aligned? A Preliminary Investigation of the Alignment of Responsible AI Values between LLMs and Human Judgment](https://arxiv.org/abs/2511.04157)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.SE

TL;DR: 研究评估了23个大型语言模型在负责任AI价值观方面与人类群体的对齐程度，发现LLMs与AI从业者更接近，但在价值观声明与实际需求优先级之间存在不一致性


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件工程任务中的广泛应用，需要评估它们与人类在负责任AI价值观上的对齐程度，以确保AI辅助开发的可靠性

Method: 评估23个LLMs在四个任务上的表现：选择关键负责任AI价值观、在特定情境中评级重要性、解决价值观冲突、优先体现价值观的软件需求

Result: LLMs总体上与AI从业者更接近，强调公平、隐私、透明度、安全和问责制，但在价值观声明与实际需求优先级之间存在不一致性

Conclusion: 在需求工程中依赖LLMs存在实际风险，需要系统方法来基准测试、解释和监控AI辅助软件开发中的价值观对齐

Abstract: Large Language Models (LLMs) are increasingly employed in software
engineering tasks such as requirements elicitation, design, and evaluation,
raising critical questions regarding their alignment with human judgments on
responsible AI values. This study investigates how closely LLMs' value
preferences align with those of two human groups: a US-representative sample
and AI practitioners. We evaluate 23 LLMs across four tasks: (T1) selecting key
responsible AI values, (T2) rating their importance in specific contexts, (T3)
resolving trade-offs between competing values, and (T4) prioritizing software
requirements that embody those values. The results show that LLMs generally
align more closely with AI practitioners than with the US-representative
sample, emphasizing fairness, privacy, transparency, safety, and
accountability. However, inconsistencies appear between the values that LLMs
claim to uphold (Tasks 1-3) and the way they prioritize requirements (Task 4),
revealing gaps in faithfulness between stated and applied behavior. These
findings highlight the practical risk of relying on LLMs in requirements
engineering without human oversight and motivate the need for systematic
approaches to benchmark, interpret, and monitor value alignment in AI-assisted
software development.

</details>


### [18] [Explaining Software Vulnerabilities with Large Language Models](https://arxiv.org/abs/2511.04179)
*Oshando Johnson,Alexandra Fomina,Ranjith Krishnamurthy,Vaibhav Chaudhari,Rohith Kumar Shanmuganathan,Eric Bodden*

Main category: cs.SE

TL;DR: SAFE是一个IDE插件，使用GPT-4o来解释SAST工具检测到的漏洞的原因、影响和缓解策略，显著帮助开发人员理解和解决安全漏洞。


<details>
  <summary>Details</summary>
Motivation: SAST工具存在可用性限制，其通用警告信息无法充分向开发人员传达重要信息，导致误解或忽视关键发现。

Method: 采用混合方法，利用LLMs（特别是GPT-4o）来解决SAST可解释性挑战，开发了名为SAFE的IDE插件。

Result: 专家用户研究表明，SAFE生成的解释能显著帮助初级到中级开发人员理解和解决安全漏洞，提高SAST工具的整体可用性。

Conclusion: 基于LLM的方法可以有效解决SAST工具的可解释性问题，帮助开发人员更好地理解和修复安全漏洞。

Abstract: The prevalence of security vulnerabilities has prompted companies to adopt
static application security testing (SAST) tools for vulnerability detection.
Nevertheless, these tools frequently exhibit usability limitations, as their
generic warning messages do not sufficiently communicate important information
to developers, resulting in misunderstandings or oversight of critical
findings. In light of recent developments in Large Language Models (LLMs) and
their text generation capabilities, our work investigates a hybrid approach
that uses LLMs to tackle the SAST explainability challenges. In this paper, we
present SAFE, an Integrated Development Environment (IDE) plugin that leverages
GPT-4o to explain the causes, impacts, and mitigation strategies of
vulnerabilities detected by SAST tools. Our expert user study findings indicate
that the explanations generated by SAFE can significantly assist beginner to
intermediate developers in understanding and addressing security
vulnerabilities, thereby improving the overall usability of SAST tools.

</details>


### [19] [GITER: A Git-Based Declarative Exchange Model Using Kubernetes-Style Custom Resources](https://arxiv.org/abs/2511.04182)
*Christos Tranoris*

Main category: cs.SE

TL;DR: 提出了一种基于Git的轻量级可审计异步信息交换方法，用Git作为协调媒介替代传统API和消息代理，支持跨域、跨组织和隔离环境协作。


<details>
  <summary>Details</summary>
Motivation: 传统API和消息代理在分布式实体间信息交换中存在复杂性和耦合度问题，需要一种更透明、可追溯且保持松耦合的通信方式。

Method: 采用GitOps模式，基于Kubernetes Operators和Custom Resources构建Git通信模型，使用共享仓库作为单一事实源，spec字段记录期望状态，status字段反映实际结果。

Result: 该方法利用Git原生特性（版本控制、提交签名、访问控制）确保了透明度、可追溯性和可重现性，同时保持了系统间的松耦合和自治性。

Conclusion: Git作为声明式通信基板在分布式协作中具有显著优势，但需要在架构设计和实现中权衡其利弊，特别是在与RESTful和代理式集成的对比中。

Abstract: This paper introduces a lightweight and auditable method for asynchronous
information exchange between distributed entities using Git as the coordination
medium. The proposed approach replaces traditional APIs and message brokers
with a Git-based communication model built on the principles of Kubernetes
Operators and Custom Resources (CRs). Each participating entity, designated as
a Publisher or Consumer, interacts through a shared repository that serves as a
single source of truth, where the spec field captures the desired state and the
status field reflects the observed outcome. This pattern extends GitOps beyond
infrastructure management to support cross-domain, inter-organizational, and
air-gapped collaboration scenarios. By leveraging Git native features
(versioning, commit signing, and access control) the model ensures
transparency, traceability, and reproducibility while preserving loose coupling
and autonomy between systems. The paper discusses architectural principles,
implementation considerations, and comparisons with RESTful and broker-based
integrations, highlighting both the advantages and trade-offs of adopting Git
as a declarative communication substrate.

</details>


### [20] [A Tool for Benchmarking Large Language Models' Robustness in Assessing the Realism of Driving Scenarios](https://arxiv.org/abs/2511.04267)
*Jiahui Wu,Chengjie Lu,Aitor Arrieta,Shaukat Ali*

Main category: cs.SE

TL;DR: 提出了DriveRLR基准工具，用于评估LLM在判断驾驶场景真实性方面的鲁棒性，通过生成变异场景和构建提示来测试不同LLM的性能差异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统安全性测试成本高，仿真测试需求增长，但评估仿真场景真实性困难。LLM具有强大的推理和泛化能力，可能通过文本提示评估场景真实性。

Method: 开发DriveRLR工具，生成变异场景变体，构建提示，评估LLM判断驾驶场景真实性的能力和鲁棒性。在DeepScenario数据集上验证，使用GPT-5、Llama 4 Maverick和Mistral Small 3.2三种先进LLM。

Result: DriveRLR有效揭示了不同LLM在鲁棒性方面的差异，证明了其在场景真实性评估中的有效性和实用价值。

Conclusion: DriveRLR不仅能评估LLM鲁棒性，还可作为场景生成的目标函数等应用组件，支持基于仿真的自动驾驶系统测试工作流程。

Abstract: In recent years, autonomous driving systems have made significant progress,
yet ensuring their safety remains a key challenge. To this end, scenario-based
testing offers a practical solution, and simulation-based methods have gained
traction due to the high cost and risk of real-world testing. However,
evaluating the realism of simulated scenarios remains difficult, creating
demand for effective assessment methods. Recent advances show that Large
Language Models (LLMs) possess strong reasoning and generalization
capabilities, suggesting their potential in assessing scenario realism through
scenario-related textual prompts. Motivated by this, we propose DriveRLR, a
benchmark tool to assess the robustness of LLMs in evaluating the realism of
driving scenarios. DriveRLR generates mutated scenario variants, constructs
prompts, which are then used to assess a given LLM's ability and robustness in
determining the realism of driving scenarios. We validate DriveRLR on the
DeepScenario dataset using three state-of-the-art LLMs: GPT-5, Llama 4
Maverick, and Mistral Small 3.2. Results show that DriveRLR effectively reveals
differences in the robustness of various LLMs, demonstrating its effectiveness
and practical value in scenario realism assessment. Beyond LLM robustness
evaluation, DriveRLR can serve as a practical component in applications such as
an objective function to guide scenario generation, supporting simulation-based
ADS testing workflows.

</details>


### [21] [Where Do LLMs Still Struggle? An In-Depth Analysis of Code Generation Benchmarks](https://arxiv.org/abs/2511.04355)
*Amir Molzam Sharifloo,Maedeh Heydari,Parsa Kazerooni,Daniel Maninger,Mira Mezini*

Main category: cs.SE

TL;DR: 分析了四个流行基准测试中LLM在代码生成任务中的失败模式，识别了LLM最可能失败的任务类型和失败原因，揭示了四种常见的LLM弱点模式和导致失败的基准任务复杂性。


<details>
  <summary>Details</summary>
Motivation: 当前基准测试和排行榜主要提供LLM的量化排名，但缺乏对LLM持续失败任务的深入分析，这些信息对于理解当前局限性和指导更强大模型开发至关重要。

Method: 检查四个流行基准测试中的代码生成任务，识别主要LLM最可能失败的任务；分析解决方案代码的静态复杂性是否导致失败；系统检查114个LLM持续失败的任务。

Result: 发现了LLM中四种反复出现的弱点模式，以及基准任务中最常导致失败的常见复杂性。

Conclusion: 通过深入分析LLM在代码生成中的失败模式，为理解当前模型的局限性提供了重要见解，并为开发更强大模型指明了方向。

Abstract: Large Language Models (LLMs) have achieved remarkable success in code
generation, and the race to improve their performance has become a central
focus of AI research. Benchmarks and leaderboards are increasingly popular,
offering quantitative rankings of LLMs. However, they provide limited insight
into the tasks that LLMs consistently fail to solve - information that is
crucial for understanding current limitations and guiding the development of
more capable models. To address this gap, we examined code generation tasks
across four popular benchmarks, identifying those that major LLMs are most
likely to fail. To understand the causes of these failures, we investigated
whether the static complexity of solution code contributes to them, followed by
a systematic inspection of 114 tasks that LLMs consistently struggled with. Our
analysis revealed four recurring patterns of weaknesses in LLMs, as well as
common complications within benchmark tasks that most often lead to failure.

</details>


### [22] [Speed at the Cost of Quality? The Impact of LLM Agent Assistance on Software Development](https://arxiv.org/abs/2511.04427)
*Hao He,Courtney Miller,Shyam Agarwal,Christian Kästner,Bogdan Vasilescu*

Main category: cs.SE

TL;DR: 研究发现Cursor LLM助手能显著但短暂提升开发速度，但同时导致静态分析警告和代码复杂度持续增加，这些因素最终导致长期开发速度下降。


<details>
  <summary>Details</summary>
Motivation: 缺乏关于LLM代理助手对软件开发生产力和质量影响的实证证据，需要量化评估其实际效果。

Method: 采用差分法设计，比较使用Cursor的GitHub项目与匹配的未使用Cursor的控制组项目，并使用面板广义矩估计方法分析长期影响。

Result: Cursor采用导致项目级开发速度显著但短暂提升，同时静态分析警告和代码复杂度显著持续增加，这些因素最终导致长期速度放缓。

Conclusion: LLM助手虽然能短期提升开发效率，但可能以牺牲代码质量为代价，需要平衡短期生产力与长期维护性。

Abstract: Large language models (LLMs) have demonstrated the promise to revolutionize
the field of software engineering. Among other things, LLM agents are rapidly
gaining momentum in their application to software development, with
practitioners claiming a multifold productivity increase after adoption. Yet,
empirical evidence is lacking around these claims. In this paper, we estimate
the causal effect of adopting a widely popular LLM agent assistant, namely
Cursor, on development velocity and software quality. The estimation is enabled
by a state-of-the-art difference-in-differences design comparing
Cursor-adopting GitHub projects with a matched control group of similar GitHub
projects that do not use Cursor. We find that the adoption of Cursor leads to a
significant, large, but transient increase in project-level development
velocity, along with a significant and persistent increase in static analysis
warnings and code complexity. Further panel generalized method of moments
estimation reveals that the increase in static analysis warnings and code
complexity acts as a major factor causing long-term velocity slowdown. Our
study carries implications for software engineering practitioners, LLM agent
assistant designers, and researchers.

</details>


### [23] [EDIT-Bench: Evaluating LLM Abilities to Perform Real-World Instructed Code Edits](https://arxiv.org/abs/2511.04486)
*Wayne Chi,Valerie Chen,Ryan Shar,Aditya Mittal,Jenny Liang,Wei-Lin Chiang,Anastasios Nikolas Angelopoulos,Ion Stoica,Graham Neubig,Ameet Talwalkar,Chris Donahue*

Main category: cs.SE

TL;DR: EDIT-Bench是一个基于真实世界使用场景的代码编辑基准测试，包含545个问题，评估LLM在理解代码上下文、高亮代码和光标位置等真实环境下的代码编辑能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏直接评估LLM代码编辑能力的基准测试，现有数据集往往依赖人工来源，需要基于真实世界用户指令和代码上下文的评估基准。

Method: 收集真实世界的用户指令和代码上下文，构建包含多种自然语言和编程语言、多样化真实使用场景的545个问题集，评估40个不同LLM的性能。

Result: EDIT-Bench是一个具有挑战性的问题集，只有5个模型得分超过60%。模型性能在不同用户指令类别间差异显著，上下文信息的不同级别对任务成功率影响很大，性能差异达11%。

Conclusion: EDIT-Bench强调了在真实上下文环境中评估代码编辑能力的重要性，模型性能受上下文信息影响显著，需要更贴近实际使用场景的评估方法。

Abstract: Instructed code editing, where LLMs directly modify a developer's existing
code based on a user instruction, is becoming a widely used interaction mode in
AI coding assistants. However, few benchmarks directly evaluate this capability
and current datasets often rely on artificial sources. We introduce EDIT-Bench,
a benchmark for evaluating LLM code editing capabilities grounded in real-world
usage, i.e., user instructions and code contexts collected in the wild.
EDIT-Bench comprises of 545 problems, multiple natural and programming
languages, and a diverse set of real-world use cases, ranging from resolving
errors to adding features. EDIT-Bench introduces context-dependent problems
that require the model to understand code context, highlighted code, and cursor
position in addition to the user instruction. We evaluate 40 diverse LLMs and
observe that EDIT-Bench is a challenging set of problems where only 5 models
score over 60%. We find that model performance varies across different
categories of user instructions. Further, we find that varying levels of
contextual information greatly affect task success rate, with performance
varying up to 11%, indicating the importance of evaluating with realistic
context.

</details>


### [24] [Microservices Is Dying, A New Method for Module Division Based on Universal Interfaces](https://arxiv.org/abs/2511.04548)
*Qing Wang,Yong Zhang*

Main category: cs.SE

TL;DR: 提出了一种新的系统设计理念，通过计算模块独立性和设计通用接口来消除模块间依赖，实现了在单进程单体应用中动态加载、卸载和修改任何部分的能力。


<details>
  <summary>Details</summary>
Motivation: 微服务虽然物理隔离了模块，但未能阻止依赖关系的传播和扩散，需要从根本上解决模块间耦合问题。

Method: 从模块变更影响评估出发，提出模块独立性计算方法，设计通用接口作为模块间边界，并实现名为EIGHT的平台架构。

Result: 证明了只要保证模块独立性，即使在单进程单体应用中也能在运行时动态加载、卸载或修改任何部分。

Conclusion: 该架构旨在为日益复杂的系统探索超越微服务和单体架构的新路径。

Abstract: Although microservices have physically isolated modules, they have failed to
prevent the propagation and diffusion of dependencies. To trace the root cause
of the inter-module coupling, this paper, starting from the impact assessment
approach for module changes, proposes a conceptual method for calculating
module independence and utilizes this method to derive the necessary conditions
for module independence. Then, a new system design philosophy and software
engineering methodology is proposed, aimed at eliminating dependencies between
modules. A specific pattern is employed to design a set of universal
interfaces, serving as a universal boundary between modules. Subsequently, this
method is used to implement a platform architecture named EIGHT, demonstrating
that, as long as module independence is guaranteed, even a monolithic
application within a single process can dynamically load, unload, or modify any
part at runtime. Finally, the paper concludes that this architecture aims to
explore a novel path for increasingly complex systems, beyond microservice and
monolithic architectures.

</details>
