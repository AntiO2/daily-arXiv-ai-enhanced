{"id": "2511.08034", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08034", "abs": "https://arxiv.org/abs/2511.08034", "authors": ["Miroslav Popovic", "Marko Popovic", "Pavle Vasiljevic", "Ilija Basicevic"], "title": "Generic Algorithm for Universal TDM Communication Over Inter Satellite Links", "comment": "4 pages, 3 figures, 1 algorithm", "summary": "The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links."}
{"id": "2511.08135", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08135", "abs": "https://arxiv.org/abs/2511.08135", "authors": ["Zhuoheng Ran", "Chong Wu", "Renjie Xu", "Maolin Che", "Hong Yan"], "title": "UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing", "comment": "Accepted on 24 September 2025 at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures."}
{"id": "2511.08158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08158", "abs": "https://arxiv.org/abs/2511.08158", "authors": ["Kelun Lei", "Hailong Yang", "Kaige Zhang", "Kejie Ma", "Yiqing Wang", "Xin You", "Yufan Xu", "Enrique S. Quintana-Orti", "Zhongzhi Luan", "Yi Liu", "Depei Qian"], "title": "\\uline{LO}w-c\\uline{O}st yet High-\\uline{P}erformant \\uline{S}parse Matrix-Matrix Multiplication on Arm SME Architectures", "comment": null, "summary": "Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\\times$ (FP32)/14.4$\\times$ (FP64) against the CPU baseline TACO and 71.3$\\times$ (FP32)/54.8$\\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\\times$ and 33.5$\\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU."}
{"id": "2511.08222", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08222", "abs": "https://arxiv.org/abs/2511.08222", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin", "comment": "25 pages, 9 fugures, 2 tables", "summary": "In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.\n  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases."}
{"id": "2511.07426", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07426", "abs": "https://arxiv.org/abs/2511.07426", "authors": ["Zihao Ding", "Mufeng Zhu", "Yao Liu"], "title": "Network and Systems Performance Characterization of MCP-Enabled LLM Agents", "comment": null, "summary": "Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows."}
