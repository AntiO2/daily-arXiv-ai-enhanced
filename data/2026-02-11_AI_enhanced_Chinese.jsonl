{"id": "2602.09174", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2602.09174", "abs": "https://arxiv.org/abs/2602.09174", "authors": ["Marzieh Barkhordar", "Alireza Tabatabaeian", "Mohammad Sadrosadati", "Christina Giannoula", "Juan Gomez Luna", "Izzat El Hajj", "Onur Mutlu", "Alaa R. Alameldeen"], "title": "ALPHA-PIM: Analysis of Linear Algebraic Processing for High-Performance Graph Applications on a Real Processing-In-Memory System", "comment": null, "summary": "Processing large-scale graph datasets is computationally intensive and time-consuming. Processor-centric CPU and GPU architectures, commonly used for graph applications, often face bottlenecks caused by extensive data movement between the processor and memory units due to low data reuse. As a result, these applications are often memory-bound, limiting both performance and energy efficiency due to excessive data transfers. Processing-In-Memory (PIM) offers a promising approach to mitigate data movement bottlenecks by integrating computation directly within or near memory. Although several previous studies have introduced custom PIM proposals for graph processing, they do not leverage real-world PIM systems.\n  This work aims to explore the capabilities and characteristics of common graph algorithms on a real-world PIM system to accelerate data-intensive graph workloads. To this end, we (1) implement representative graph algorithms on UPMEM's general-purpose PIM architecture; (2) characterize their performance and identify key bottlenecks; (3) compare results against CPU and GPU baselines; and (4) derive insights to guide future PIM hardware design.\n  Our study underscores the importance of selecting optimal data partitioning strategies across PIM cores to maximize performance. Additionally, we identify critical hardware limitations in current PIM architectures and emphasize the need for future enhancements across computation, memory, and communication subsystems. Key opportunities for improvement include increasing instruction-level parallelism, developing improved DMA engines with non-blocking capabilities, and enabling direct interconnection networks among PIM cores to reduce data transfer overheads.", "AI": {"tldr": "\u5728\u771f\u5b9ePIM\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u56fe\u7b97\u6cd5\uff0c\u5206\u6790\u6027\u80fd\u74f6\u9888\u5e76\u4e0eCPU/GPU\u5bf9\u6bd4\uff0c\u4e3a\u672a\u6765PIM\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc", "motivation": "\u4f20\u7edfCPU/GPU\u67b6\u6784\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\u65f6\u9762\u4e34\u6570\u636e\u79fb\u52a8\u74f6\u9888\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u548c\u80fd\u6548\u4f4e\u4e0b\u3002\u867d\u7136\u5df2\u6709PIM\u65b9\u6848\u63d0\u51fa\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u771f\u5b9ePIM\u7cfb\u7edf\u7684\u5b9e\u9645\u8bc4\u4f30\u3002", "method": "\u5728UPMEM\u901a\u7528PIM\u67b6\u6784\u4e0a\u5b9e\u73b0\u4ee3\u8868\u6027\u56fe\u7b97\u6cd5\uff0c\u5206\u6790\u6027\u80fd\u7279\u5f81\u548c\u74f6\u9888\uff0c\u4e0eCPU/GPU\u57fa\u51c6\u5bf9\u6bd4\uff0c\u5e76\u63a8\u5bfc\u786c\u4ef6\u8bbe\u8ba1\u6d1e\u5bdf", "result": "\u7814\u7a76\u53d1\u73b0\u9009\u62e9\u6700\u4f18\u7684\u6570\u636e\u5206\u533a\u7b56\u7565\u5bf9PIM\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u8bc6\u522b\u51fa\u73b0\u6709PIM\u67b6\u6784\u7684\u5173\u952e\u786c\u4ef6\u9650\u5236\uff0c\u5305\u62ec\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u901a\u4fe1\u5b50\u7cfb\u7edf", "conclusion": "\u672a\u6765PIM\u786c\u4ef6\u9700\u8981\u5728\u6307\u4ee4\u7ea7\u5e76\u884c\u6027\u3001\u975e\u963b\u585eDMA\u5f15\u64ce\u548cPIM\u6838\u5fc3\u95f4\u76f4\u63a5\u4e92\u8fde\u7f51\u7edc\u7b49\u65b9\u9762\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5f00\u9500"}}
{"id": "2602.09323", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09323", "abs": "https://arxiv.org/abs/2602.09323", "authors": ["Jie Kong", "Wei Wang", "Jiehan Zhou", "Chen Yu"], "title": "LLM-CoOpt: A Co-Design and Optimization Framework for Efficient LLM Inference on Heterogeneous Platforms", "comment": null, "summary": "Major challenges in LLMs inference remain frequent memory bandwidth bottlenecks, computational redundancy, and inefficiencies in long-sequence processing. To address these issues, we propose LLM-CoOpt, a comprehensive algorithmhardware co-design framework aimed at improving both throughput and latency in LLM inference. LLM-CoOpt integrates three key strategies: (1) Key-Value Cache Optimization, termed Opt-KV, which improves memory access efficiency by optimizing both KV cache write and read paths, and introduces FP8 quantization to reduce memory footprint while maintaining accuracy; (2) Grouped-Query Attention for Computational Efficiency, termed Opt-GQA, which reduces the overall computational complexity by restructuring multi-head self-attention into grouped-query attention with shared key-value projections, enabling higher throughput and lower resource consumption; (3) Paged Attention for Long- Sequence Processing, termed Opt-Pa, which adopts a two-step strategy to first segment long sequences into manageable chunks and then apply lazy memory mapping and computation, significantly reducing memory pressure and improving performance on long-context inputs.Experiments on the LLaMa-13BGPTQ model demonstrate that LLM-CoOpt increases inference throughput by up to 13.43%, reduces latency by up to 16.79%, and maintains model accuracy. These results confirm that LLM-CoOpt provides a practical, high-performance optimization path for real-world inference of large-scale language models.", "AI": {"tldr": "LLM-CoOpt\u662f\u4e00\u4e2a\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7KV\u7f13\u5b58\u4f18\u5316\u3001\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\u6539\u8fdb\u548c\u5206\u9875\u6ce8\u610f\u529b\u4e09\u79cd\u7b56\u7565\uff0c\u63d0\u5347LLM\u63a8\u7406\u7684\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u3001\u8ba1\u7b97\u5197\u4f59\u548c\u957f\u5e8f\u5217\u5904\u7406\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9ad8\u6027\u80fd\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e09\u5408\u4e00\u4f18\u5316\u7b56\u7565\uff1a1) Opt-KV\u4f18\u5316KV\u7f13\u5b58\u8bfb\u5199\u8def\u5f84\u5e76\u5f15\u5165FP8\u91cf\u5316\uff1b2) Opt-GQA\u5c06\u591a\u5934\u81ea\u6ce8\u610f\u529b\u91cd\u6784\u4e3a\u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\uff1b3) Opt-Pa\u91c7\u7528\u4e24\u6b65\u6cd5\u5904\u7406\u957f\u5e8f\u5217\uff0c\u5148\u5206\u5757\u518d\u61d2\u52a0\u8f7d\u3002", "result": "\u5728LLaMa-13B-GPTQ\u6a21\u578b\u4e0a\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad813.43%\uff0c\u5ef6\u8fdf\u964d\u4f4e\u6700\u9ad816.79%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "LLM-CoOpt\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u3001\u9ad8\u6027\u80fd\u7684\u4f18\u5316\u8def\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2602.09435", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09435", "abs": "https://arxiv.org/abs/2602.09435", "authors": ["Joseph M. Hellerstein"], "title": "The Coordination Criterion", "comment": "10 body pages; 24 pages with appendices and references", "summary": "When is coordination intrinsically required by a distributed specification, rather than imposed by a particular protocol or implementation strategy? We give a general answer using minimal assumptions. In an asynchronous message-passing model, we show that a specification admits a coordination-free implementation if and only if it is monotone with respect to history extension under an appropriate order on observable outcomes.\n  This Coordination Criterion is stated directly over Lamport histories -- partially ordered executions under happens-before -- and specification-defined observable outcomes, without assuming any particular programming language, object implementation, or protocol structure. It yields a sharp boundary between specifications that can be implemented without coordination and those for which coordination is unavoidable. The criterion provides a uniform explanation for a range of classical results, including CAP-style impossibility, CALM-style coordination-freedom, agreement and snapshot tasks, transactional isolation levels, and invariant confluence -- all instances of the same underlying semantic phenomenon.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u8c03\u51c6\u5219\uff0c\u7528\u4e8e\u5224\u65ad\u5206\u5e03\u5f0f\u89c4\u8303\u4f55\u65f6\u672c\u8d28\u4e0a\u9700\u8981\u534f\u8c03\uff0c\u800c\u975e\u7279\u5b9a\u534f\u8bae\u6216\u5b9e\u73b0\u7b56\u7565\u5f3a\u52a0\u7684\u3002\u8be5\u51c6\u5219\u8868\u660e\uff1a\u5728\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\u4e2d\uff0c\u89c4\u8303\u5141\u8bb8\u65e0\u534f\u8c03\u5b9e\u73b0\u5f53\u4e14\u4ec5\u5f53\u5b83\u5728\u9002\u5f53\u53ef\u89c2\u5bdf\u7ed3\u679c\u987a\u5e8f\u4e0b\u76f8\u5bf9\u4e8e\u5386\u53f2\u6269\u5c55\u662f\u5355\u8c03\u7684\u3002", "motivation": "\u7814\u7a76\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u534f\u8c03\u7684\u672c\u8d28\u9700\u6c42\uff0c\u533a\u5206\u89c4\u8303\u672c\u8eab\u56fa\u6709\u7684\u534f\u8c03\u9700\u6c42\u4e0e\u7279\u5b9a\u5b9e\u73b0\u7b56\u7565\u5f15\u5165\u7684\u534f\u8c03\uff0c\u4e3a\u7406\u89e3\u4f55\u65f6\u534f\u8c03\u4e0d\u53ef\u907f\u514d\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5728\u5f02\u6b65\u6d88\u606f\u4f20\u9012\u6a21\u578b\u4e2d\uff0c\u57fa\u4e8eLamport\u5386\u53f2\uff08\u504f\u5e8f\u6267\u884c\u4e0b\u7684happens-before\u5173\u7cfb\uff09\u548c\u89c4\u8303\u5b9a\u4e49\u7684\u53ef\u89c2\u5bdf\u7ed3\u679c\uff0c\u63d0\u51fa\u534f\u8c03\u51c6\u5219\uff1a\u89c4\u8303\u5141\u8bb8\u65e0\u534f\u8c03\u5b9e\u73b0\u5f53\u4e14\u4ec5\u5f53\u5b83\u5728\u5386\u53f2\u6269\u5c55\u987a\u5e8f\u4e0b\u662f\u5355\u8c03\u7684\u3002", "result": "\u5efa\u7acb\u4e86\u534f\u8c03\u7684\u6e05\u6670\u8fb9\u754c\uff0c\u7edf\u4e00\u89e3\u91ca\u4e86CAP\u4e0d\u53ef\u80fd\u6027\u3001CALM\u65e0\u534f\u8c03\u6027\u3001\u4e00\u81f4\u6027\u534f\u8bae\u3001\u5feb\u7167\u4efb\u52a1\u3001\u4e8b\u52a1\u9694\u79bb\u7ea7\u522b\u548c\u4e0d\u53d8\u5f0f\u6c47\u5408\u7b49\u4e00\u7cfb\u5217\u7ecf\u5178\u7ed3\u679c\uff0c\u8868\u660e\u5b83\u4eec\u90fd\u662f\u540c\u4e00\u5e95\u5c42\u8bed\u4e49\u73b0\u8c61\u7684\u4e0d\u540c\u5b9e\u4f8b\u3002", "conclusion": "\u534f\u8c03\u51c6\u5219\u4e3a\u5206\u5e03\u5f0f\u89c4\u8303\u662f\u5426\u9700\u8981\u534f\u8c03\u63d0\u4f9b\u4e86\u901a\u7528\u5224\u65ad\u6807\u51c6\uff0c\u57fa\u4e8e\u6700\u5c0f\u5047\u8bbe\uff0c\u4e0d\u4f9d\u8d56\u7279\u5b9a\u7f16\u7a0b\u8bed\u8a00\u3001\u5bf9\u8c61\u5b9e\u73b0\u6216\u534f\u8bae\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u534f\u8c03\u9700\u6c42\u7684\u672c\u8d28\u8bed\u4e49\u7279\u6027\u3002"}}
{"id": "2602.09441", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09441", "abs": "https://arxiv.org/abs/2602.09441", "authors": ["Allen Clement", "Natacha Crooks", "Neil Giridharan", "Alex Shamis"], "title": "It's not a lie if you don't get caught: simplifying reconfiguration in SMR through dirty logs", "comment": null, "summary": "Production state-machine replication (SMR) implementations are complex, multi-layered architectures comprising data dissemination, ordering, execution, and reconfiguration components. Existing research consensus protocols rarely discuss reconfiguration. Those that do tightly couple membership changes to a specific algorithm. This prevents the independent upgrade of individual building blocks and forces expensive downtime when transitioning to new protocol implementations. Instead, modularity is essential for maintainability and system evolution in production deployments. We present Gauss, a reconfiguration engine designed to treat consensus protocols as interchangeable modules. By introducing a distinction between a consensus protocol's inner log and a sanitized outer log exposed to the RSM node, Gauss allows engineers to upgrade membership, failure thresholds, and the consensus protocol itself independently and with minimal global downtime. Our initial evaluation on the Rialo blockchain shows that this separation of concerns enables a seamless evolution of the SMR stack across a sequence of diverse protocol implementations.", "AI": {"tldr": "Gauss\u662f\u4e00\u4e2a\u91cd\u65b0\u914d\u7f6e\u5f15\u64ce\uff0c\u5c06\u5171\u8bc6\u534f\u8bae\u89c6\u4e3a\u53ef\u4e92\u6362\u6a21\u5757\uff0c\u5141\u8bb8\u72ec\u7acb\u5347\u7ea7\u6210\u5458\u8d44\u683c\u3001\u6545\u969c\u9608\u503c\u548c\u5171\u8bc6\u534f\u8bae\u672c\u8eab\uff0c\u6700\u5c0f\u5316\u5168\u5c40\u505c\u673a\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u4ea7\u72b6\u6001\u673a\u590d\u5236\u5b9e\u73b0\u662f\u590d\u6742\u7684\u591a\u5c42\u67b6\u6784\uff0c\u73b0\u6709\u7814\u7a76\u5171\u8bc6\u534f\u8bae\u5f88\u5c11\u8ba8\u8bba\u91cd\u65b0\u914d\u7f6e\uff0c\u5373\u4f7f\u8ba8\u8bba\u4e5f\u5c06\u6210\u5458\u53d8\u66f4\u4e0e\u7279\u5b9a\u7b97\u6cd5\u7d27\u5bc6\u8026\u5408\uff0c\u8fd9\u963b\u788d\u4e86\u72ec\u7acb\u5347\u7ea7\u5355\u4e2a\u6784\u5efa\u5757\uff0c\u5e76\u5728\u8fc7\u6e21\u5230\u65b0\u534f\u8bae\u5b9e\u73b0\u65f6\u5bfc\u81f4\u6602\u8d35\u7684\u505c\u673a\u65f6\u95f4\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5171\u8bc6\u534f\u8bae\u5185\u90e8\u65e5\u5fd7\u548c\u66b4\u9732\u7ed9RSM\u8282\u70b9\u7684\u51c0\u5316\u5916\u90e8\u65e5\u5fd7\u4e4b\u95f4\u7684\u533a\u522b\uff0cGauss\u5141\u8bb8\u5de5\u7a0b\u5e08\u72ec\u7acb\u5347\u7ea7\u6210\u5458\u8d44\u683c\u3001\u6545\u969c\u9608\u503c\u548c\u5171\u8bc6\u534f\u8bae\u672c\u8eab\u3002", "result": "\u5728Rialo\u533a\u5757\u94fe\u4e0a\u7684\u521d\u6b65\u8bc4\u4f30\u8868\u660e\uff0c\u8fd9\u79cd\u5173\u6ce8\u70b9\u5206\u79bb\u4f7f\u5f97SMR\u5806\u6808\u80fd\u591f\u5728\u591a\u79cd\u4e0d\u540c\u534f\u8bae\u5b9e\u73b0\u4e4b\u95f4\u65e0\u7f1d\u6f14\u8fdb\u3002", "conclusion": "\u6a21\u5757\u5316\u5bf9\u4e8e\u751f\u4ea7\u90e8\u7f72\u4e2d\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u7cfb\u7edf\u6f14\u8fdb\u81f3\u5173\u91cd\u8981\uff0cGauss\u901a\u8fc7\u5c06\u5171\u8bc6\u534f\u8bae\u89c6\u4e3a\u53ef\u4e92\u6362\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86SMR\u5806\u6808\u7684\u72ec\u7acb\u5347\u7ea7\u548c\u6700\u5c0f\u5316\u505c\u673a\u65f6\u95f4\u3002"}}
{"id": "2602.09051", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09051", "abs": "https://arxiv.org/abs/2602.09051", "authors": ["Avaljot Singh", "Dushyant Bharadwaj", "Stefanos Baziotis", "Kaushik Varadharajan", "Charith Mendis"], "title": "RuleFlow : Generating Reusable Program Optimizations with LLMs", "comment": null, "summary": "Optimizing Pandas programs is a challenging problem. Existing systems and compiler-based approaches offer reliability but are either heavyweight or support only a limited set of optimizations. Conversely, using LLMs in a per-program optimization methodology can synthesize nontrivial optimizations, but is unreliable, expensive, and offers a low yield. In this work, we introduce a hybrid approach that works in a 3-stage manner that decouples discovery from deployment and connects them via a novel bridge. First, it discovers per-program optimizations (discovery). Second, they are converted into generalised rewrite rules (bridge). Finally, these rules are incorporated into a compiler that can automatically apply them wherever applicable, eliminating repeated reliance on LLMs (deployment). We demonstrate that RuleFlow is the new state-of-the-art (SOTA) Pandas optimization framework on PandasBench, a challenging Pandas benchmark consisting of Python notebooks. Across these notebooks, we achieve a speedup of up to 4.3x over Dias, the previous compiler-based SOTA, and 1914.9x over Modin, the previous systems-based SOTA.\n  Our code is available at https://github.com/ADAPT-uiuc/RuleFlow.", "AI": {"tldr": "RuleFlow\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u4f18\u5316Pandas\u7a0b\u5e8f\uff1a\u53d1\u73b0\u7279\u5b9a\u7a0b\u5e8f\u4f18\u5316\u3001\u8f6c\u6362\u4e3a\u901a\u7528\u91cd\u5199\u89c4\u5219\u3001\u96c6\u6210\u5230\u7f16\u8bd1\u5668\u4e2d\u81ea\u52a8\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709Pandas\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u7cfb\u7edf/\u7f16\u8bd1\u5668\u65b9\u6cd5\u8981\u4e48\u7b28\u91cd\uff0c\u8981\u4e48\u652f\u6301\u6709\u9650\u4f18\u5316\uff1b\u57fa\u4e8eLLM\u7684\u9010\u7a0b\u5e8f\u4f18\u5316\u65b9\u6cd5\u867d\u7136\u80fd\u5408\u6210\u975e\u5e73\u51e1\u4f18\u5316\uff0c\u4f46\u4e0d\u53ef\u9760\u3001\u6602\u8d35\u4e14\u4ea7\u51fa\u7387\u4f4e\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6df7\u5408\u65b9\u6cd5\uff1a1) \u53d1\u73b0\u9636\u6bb5\uff1a\u4f7f\u7528LLM\u53d1\u73b0\u7279\u5b9a\u7a0b\u5e8f\u7684\u4f18\u5316\uff1b2) \u6865\u63a5\u9636\u6bb5\uff1a\u5c06\u53d1\u73b0\u7684\u4f18\u5316\u8f6c\u6362\u4e3a\u901a\u7528\u91cd\u5199\u89c4\u5219\uff1b3) \u90e8\u7f72\u9636\u6bb5\uff1a\u5c06\u89c4\u5219\u96c6\u6210\u5230\u7f16\u8bd1\u5668\u4e2d\uff0c\u81ea\u52a8\u5e94\u7528\u89c4\u5219\u800c\u65e0\u9700\u91cd\u590d\u4f9d\u8d56LLM\u3002", "result": "\u5728PandasBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRuleFlow\u6210\u4e3a\u65b0\u7684SOTA Pandas\u4f18\u5316\u6846\u67b6\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u7f16\u8bd1\u5668SOTA\uff08Dias\uff09\u6700\u9ad8\u52a0\u901f4.3\u500d\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u7cfb\u7edfSOTA\uff08Modin\uff09\u6700\u9ad8\u52a0\u901f1914.9\u500d\u3002", "conclusion": "RuleFlow\u901a\u8fc7\u5c06LLM\u9a71\u52a8\u7684\u4f18\u5316\u53d1\u73b0\u4e0e\u7f16\u8bd1\u5668\u9a71\u52a8\u7684\u89c4\u5219\u5e94\u7528\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684Pandas\u7a0b\u5e8f\u4f18\u5316\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.09038", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09038", "abs": "https://arxiv.org/abs/2602.09038", "authors": ["Yangzhe Peng", "Haiquan Qiu", "Quanming Yao", "Kun He"], "title": "Scaling GraphLLM with Bilevel-Optimized Sparse Querying", "comment": null, "summary": "LLMs have recently shown strong potential in enhancing node-level tasks on text-attributed graphs (TAGs) by providing explanation features. However, their practical use is severely limited by the high computational and monetary cost of repeated LLM queries. To illustrate, naively generating explanations for all nodes on a medium-sized benchmark like Photo (48k nodes) using a representative method (e.g., TAPE) would consume days of processing time. In this paper, we propose Bilevel-Optimized Sparse Querying (BOSQ), a general framework that selectively leverages LLM-derived explanation features to enhance performance on node-level tasks on TAGs. We design an adaptive sparse querying strategy that selectively decides when to invoke LLMs, avoiding redundant or low-gain queries and significantly reducing computation overhead. Extensive experiments on six real-world TAG datasets involving two types of node-level tasks demonstrate that BOSQ achieves orders of magnitude speedups over existing GraphLLM methods while consistently delivering on-par or superior performance.", "AI": {"tldr": "BOSQ\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u7a00\u758f\u67e5\u8be2\u7b56\u7565\uff0c\u9009\u62e9\u6027\u8c03\u7528LLM\u751f\u6210\u89e3\u91ca\u7279\u5f81\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u5728\u6587\u672c\u5c5e\u6027\u56fe\u7684\u8282\u70b9\u7ea7\u4efb\u52a1\u4e0a\u5b9e\u73b0\u6570\u91cf\u7ea7\u52a0\u901f\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u6587\u672c\u5c5e\u6027\u56fe\u8282\u70b9\u4efb\u52a1\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u8ba1\u7b97\u6210\u672c\u548c\u91d1\u94b1\u6210\u672c\u95ee\u9898\uff0c\u4f8b\u5982\u5728\u4e2d\u7b49\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u751f\u6210\u6240\u6709\u8282\u70b9\u7684\u89e3\u91ca\u7279\u5f81\u9700\u8981\u6570\u5929\u65f6\u95f4\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faBOSQ\u6846\u67b6\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u7a00\u758f\u67e5\u8be2\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u9009\u62e9\u6027\u5730\u51b3\u5b9a\u4f55\u65f6\u8c03\u7528LLM\uff0c\u907f\u514d\u5197\u4f59\u6216\u4f4e\u589e\u76ca\u67e5\u8be2\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u6587\u672c\u5c5e\u6027\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBOSQ\u76f8\u6bd4\u73b0\u6709GraphLLM\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5728\u8282\u70b9\u7ea7\u4efb\u52a1\u4e0a\u4fdd\u6301\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "BOSQ\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u6587\u672c\u5c5e\u6027\u56fe\u8282\u70b9\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.09604", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09604", "abs": "https://arxiv.org/abs/2602.09604", "authors": ["Ruimin Shi", "Gabin Schieffer", "Pei-Hung Lin", "Maya Gokhale", "Andreas Herten", "Ivy Peng"], "title": "High-performance Vector-length Agnostic Quantum Circuit Simulations on ARM Processors", "comment": "To be published in IPDPS2026", "summary": "ARM SVE and RISC-V RVV are emerging vector architectures in high-end processors that support vectorization of flexible vector length. In this work, we leverage an important workload for quantum computing, quantum state-vector simulations, to understand whether high-performance portability can be achieved in a vector-length agnostic (VLA) design. We propose a VLA design and optimization techniques critical for achieving high performance, including VLEN-adaptive memory layout adjustment, load buffering, fine-grained loop control, and gate fusion-based arithmetic intensity adaptation. We provide an implementation in Google's Qsim and evaluate five quantum circuits of up to 36 qubits on three ARM processors, including NVIDIA Grace, AWS Graviton3, and Fujitsu A64FX. By defining new metrics and PMU events to quantify vectorization activities, we draw generic insights for future VLA designs. Our single-source implementation of VLA quantum simulations achieves up to 4.5x speedup on A64FX, 2.5x speedup on Grace, and 1.5x speedup on Graviton.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u5728ARM SVE\u548cRISC-V RVV\u7b49\u652f\u6301\u53ef\u53d8\u5411\u91cf\u957f\u5ea6\u7684\u65b0\u5174\u67b6\u6784\u4e0a\u5b9e\u73b0\u91cf\u5b50\u6001\u5411\u91cf\u6a21\u62df\u7684\u9ad8\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u63d0\u51fa\u4e86VLA\u8bbe\u8ba1\u548c\u4f18\u5316\u6280\u672f\uff0c\u5728\u4e09\u79cdARM\u5904\u7406\u5668\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u91cf\u5b50\u6001\u5411\u91cf\u6a21\u62df\u662f\u91cf\u5b50\u8ba1\u7b97\u7684\u91cd\u8981\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u800cARM SVE\u548cRISC-V RVV\u662f\u65b0\u5174\u7684\u652f\u6301\u53ef\u53d8\u5411\u91cf\u957f\u5ea6\u7684\u9ad8\u7aef\u5904\u7406\u5668\u67b6\u6784\u3002\u7814\u7a76\u76ee\u6807\u662f\u63a2\u7d22\u5728\u8fd9\u4e9b\u5411\u91cf\u957f\u5ea6\u65e0\u5173\uff08VLA\uff09\u67b6\u6784\u4e0a\u80fd\u5426\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u53ef\u79fb\u690d\u6027\u3002", "method": "\u63d0\u51fa\u4e86VLA\u8bbe\u8ba1\u548c\u5173\u952e\u4f18\u5316\u6280\u672f\uff1aVLEN\u81ea\u9002\u5e94\u7684\u5185\u5b58\u5e03\u5c40\u8c03\u6574\u3001\u52a0\u8f7d\u7f13\u51b2\u3001\u7ec6\u7c92\u5ea6\u5faa\u73af\u63a7\u5236\u3001\u57fa\u4e8e\u95e8\u878d\u5408\u7684\u7b97\u672f\u5f3a\u5ea6\u9002\u914d\u3002\u5728Google\u7684Qsim\u4e2d\u5b9e\u73b0\uff0c\u5e76\u5728\u4e09\u79cdARM\u5904\u7406\u5668\uff08NVIDIA Grace\u3001AWS Graviton3\u3001Fujitsu A64FX\uff09\u4e0a\u8bc4\u4f30\u4e86\u6700\u591a36\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u4e94\u4e2a\u91cf\u5b50\u7535\u8def\u3002", "result": "\u5b9a\u4e49\u4e86\u65b0\u7684\u6307\u6807\u548cPMU\u4e8b\u4ef6\u6765\u91cf\u5316\u5411\u91cf\u5316\u6d3b\u52a8\uff0c\u4e3a\u672a\u6765VLA\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u901a\u7528\u89c1\u89e3\u3002\u5355\u6e90VLA\u91cf\u5b50\u6a21\u62df\u5b9e\u73b0\uff1a\u5728A64FX\u4e0a\u8fbe\u52304.5\u500d\u52a0\u901f\uff0c\u5728Grace\u4e0a\u8fbe\u52302.5\u500d\u52a0\u901f\uff0c\u5728Graviton\u4e0a\u8fbe\u52301.5\u500d\u52a0\u901f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5728\u5411\u91cf\u957f\u5ea6\u65e0\u5173\u67b6\u6784\u4e0a\u53ef\u4ee5\u5b9e\u73b0\u91cf\u5b50\u6001\u5411\u91cf\u6a21\u62df\u7684\u9ad8\u6027\u80fd\u53ef\u79fb\u690d\u6027\uff0c\u63d0\u51fa\u7684VLA\u8bbe\u8ba1\u548c\u4f18\u5316\u6280\u672f\u6709\u6548\uff0c\u4e3a\u672a\u6765VLA\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.09064", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09064", "abs": "https://arxiv.org/abs/2602.09064", "authors": ["S M Rakib Ul Karim", "Wenyi Lu", "Enock Kasaadha", "Sean Goggins"], "title": "Predicting Open Source Software Sustainability with Deep Temporal Neural Hierarchical Architectures and Explainable AI", "comment": null, "summary": "Open Source Software (OSS) projects follow diverse lifecycle trajectories shaped by evolving patterns of contribution, coordination, and community engagement. Understanding these trajectories is essential for stakeholders seeking to assess project organization and health at scale. However, prior work has largely relied on static or aggregated metrics, such as project age or cumulative activity, providing limited insight into how OSS sustainability unfolds over time. In this paper, we propose a hierarchical predictive framework that models OSS projects as belonging to distinct lifecycle stages grounded in established socio-technical categorizations of OSS development. Rather than treating sustainability solely as project longevity, these lifecycle stages operationalize sustainability as a multidimensional construct integrating contribution activity, community participation, and maintenance dynamics. The framework combines engineered tabular indicators with 24-month temporal activity sequences and employs a multi-stage classification pipeline to distinguish lifecycle stages associated with different coordination and participation regimes. To support transparency, we incorporate explainable AI techniques to examine the relative contribution of feature categories to model predictions. Evaluated on a large corpus of OSS repositories, the proposed approach achieves over 94\\% overall accuracy in lifecycle stage classification. Attribution analyses consistently identify contribution activity and community-related features as dominant signals, highlighting the central role of collective participation dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u5c42\u9884\u6d4b\u6846\u67b6\uff0c\u5c06\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u5efa\u6a21\u4e3a\u57fa\u4e8e\u793e\u4f1a\u6280\u672f\u5206\u7c7b\u7684\u4e0d\u540c\u751f\u547d\u5468\u671f\u9636\u6bb5\uff0c\u4f7f\u7528\u7279\u5f81\u5de5\u7a0b\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u5f3a\u8c03\u8d21\u732e\u6d3b\u52a8\u548c\u793e\u533a\u53c2\u4e0e\u662f\u53ef\u6301\u7eed\u6027\u7684\u6838\u5fc3\u4fe1\u53f7\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u6216\u805a\u5408\u6307\u6807\uff08\u5982\u9879\u76ee\u5e74\u9f84\u6216\u7d2f\u8ba1\u6d3b\u52a8\uff09\uff0c\u65e0\u6cd5\u6df1\u5165\u7406\u89e3\u5f00\u6e90\u8f6f\u4ef6\u53ef\u6301\u7eed\u6027\u5982\u4f55\u968f\u65f6\u95f4\u5c55\u5f00\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u9879\u76ee\u7ec4\u7ec7\u6f14\u53d8\u548c\u5065\u5eb7\u72b6\u51b5\u7684\u52a8\u6001\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u9884\u6d4b\u6846\u67b6\uff1a1) \u57fa\u4e8e\u793e\u4f1a\u6280\u672f\u5206\u7c7b\u5b9a\u4e49\u4e0d\u540c\u7684\u751f\u547d\u5468\u671f\u9636\u6bb5\uff1b2) \u7ed3\u5408\u5de5\u7a0b\u5316\u8868\u683c\u6307\u6807\u548c24\u4e2a\u6708\u65f6\u95f4\u6d3b\u52a8\u5e8f\u5217\uff1b3) \u91c7\u7528\u591a\u9636\u6bb5\u5206\u7c7b\u7ba1\u9053\u533a\u5206\u4e0d\u540c\u534f\u8c03\u548c\u53c2\u4e0e\u673a\u5236\u7684\u751f\u547d\u5468\u671f\u9636\u6bb5\uff1b4) \u96c6\u6210\u53ef\u89e3\u91caAI\u6280\u672f\u5206\u6790\u7279\u5f81\u7c7b\u522b\u5bf9\u9884\u6d4b\u7684\u76f8\u5bf9\u8d21\u732e\u3002", "result": "\u5728\u5927\u578b\u5f00\u6e90\u8f6f\u4ef6\u4ed3\u5e93\u8bed\u6599\u5e93\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u547d\u5468\u671f\u9636\u6bb5\u5206\u7c7b\u4e2d\u8fbe\u5230\u8d85\u8fc794%\u7684\u6574\u4f53\u51c6\u786e\u7387\u3002\u5f52\u56e0\u5206\u6790\u4e00\u81f4\u8bc6\u522b\u51fa\u8d21\u732e\u6d3b\u52a8\u548c\u793e\u533a\u76f8\u5173\u7279\u5f81\u4e3a\u4e3b\u5bfc\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5730\u5c06\u53ef\u6301\u7eed\u6027\u64cd\u4f5c\u5316\u4e3a\u591a\u7ef4\u6784\u9020\uff0c\u5f3a\u8c03\u96c6\u4f53\u53c2\u4e0e\u52a8\u6001\u5728\u5f00\u6e90\u8f6f\u4ef6\u9879\u76ee\u751f\u547d\u5468\u671f\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002\u8d21\u732e\u6d3b\u52a8\u548c\u793e\u533a\u53c2\u4e0e\u662f\u9884\u6d4b\u9879\u76ee\u53d1\u5c55\u9636\u6bb5\u7684\u5173\u952e\u6307\u6807\u3002"}}
{"id": "2602.09039", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09039", "abs": "https://arxiv.org/abs/2602.09039", "authors": ["Sarra Madad"], "title": "Efficient Distance Pruning for Process Suffix Comparison in Prescriptive Process Monitoring", "comment": "Winter Simulation Conference, Dec 2025, Seattle WA, United States", "summary": "Prescriptive process monitoring seeks to recommend actions that improve process outcomes by analyzing possible continuations of ongoing cases. A key obstacle is the heavy computational cost of large-scale suffix comparisons, which grows rapidly with log size. We propose an efficient retrieval method exploiting the triangle inequality: distances to a set of optimized pivots define bounds that prune redundant comparisons. This substantially reduces runtime and is fully parallelizable. Crucially, pruning is exact: the retrieved suffixes are identical to those from exhaustive comparison, thereby preserving accuracy. These results show that metric-based pruning can accelerate suffix comparison and support scalable prescriptive systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e09\u89d2\u4e0d\u7b49\u5f0f\u7684\u6709\u6548\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u67a2\u8f74\u7684\u8ddd\u79bb\u8fb9\u754c\u6765\u526a\u679d\u5197\u4f59\u6bd4\u8f83\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u7d22\u7ed3\u679c\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89c4\u8303\u8fc7\u7a0b\u76d1\u63a7\u9700\u8981\u63a8\u8350\u6539\u5584\u8fc7\u7a0b\u7ed3\u679c\u7684\u884c\u52a8\uff0c\u4f46\u5927\u89c4\u6a21\u540e\u7f00\u6bd4\u8f83\u7684\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\uff0c\u968f\u7740\u65e5\u5fd7\u5927\u5c0f\u7684\u589e\u957f\u800c\u8fc5\u901f\u589e\u52a0\uff0c\u8fd9\u6210\u4e3a\u5173\u952e\u969c\u788d\u3002", "method": "\u5229\u7528\u4e09\u89d2\u4e0d\u7b49\u5f0f\u539f\u7406\uff0c\u901a\u8fc7\u5230\u4e00\u7ec4\u4f18\u5316\u67a2\u8f74\u7684\u8ddd\u79bb\u5b9a\u4e49\u8fb9\u754c\uff0c\u526a\u679d\u5197\u4f59\u6bd4\u8f83\u3002\u8be5\u65b9\u6cd5\u5b8c\u5168\u53ef\u5e76\u884c\u5316\uff0c\u4e14\u526a\u679d\u662f\u7cbe\u786e\u7684\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8fd0\u884c\u65f6\u95f4\uff0c\u68c0\u7d22\u5230\u7684\u540e\u7f00\u4e0e\u7a77\u4e3e\u6bd4\u8f83\u7684\u7ed3\u679c\u5b8c\u5168\u76f8\u540c\uff0c\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002\u8bc1\u660e\u57fa\u4e8e\u5ea6\u91cf\u7684\u526a\u679d\u53ef\u4ee5\u52a0\u901f\u540e\u7f00\u6bd4\u8f83\u3002", "conclusion": "\u57fa\u4e8e\u5ea6\u91cf\u7684\u526a\u679d\u65b9\u6cd5\u80fd\u591f\u52a0\u901f\u540e\u7f00\u6bd4\u8f83\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u89c4\u8303\u8fc7\u7a0b\u76d1\u63a7\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u540e\u7f00\u6bd4\u8f83\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2602.09721", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09721", "abs": "https://arxiv.org/abs/2602.09721", "authors": ["Guowei Liu", "Hongming Li", "Yaning Guo", "Yongxi Lyu", "Mo Zhou", "Yi Liu", "Zhaogeng Li", "Yanpeng Wang"], "title": "Revealing the Challenges of Attention-FFN Disaggregation for Modern MoE Models and Hardware Systems", "comment": null, "summary": "Deploying large-scale MoE models presents challenges in memory capacity and bandwidth for expert activation. While Attention-FFN Disaggregation (AFD) has emerged as a potential architecture to decouple compute and memory resources, its performance boundaries compared to standard large-scale Expert Parallelism (EP) remain underexplored. In this paper, we conduct a systematic analysis of AFD by extending the roofline model to the communication level, correlating interconnect bandwidth, arithmetic intensity, and Hardware FLOPS Utilization (HFU). Our analysis reveals a dead zone on standard clusters: increasing FFN instance count fails to improve HFU as computational workload is capped by scale-out bandwidth, causing operator active time to shrink relative to the fixed latency budget. We further show that AFD's discrete node-level scaling incurs higher imbalance penalties than EP's continuous batch adjustment. Nevertheless, these limitations diminish under specific conditions: Superpod-class hardware with abundant interconnect bandwidth and models with coarse-grained experts and lower sparsity are more likely to benefit from AFD. These findings position AFD as a promising approach for specific hardware-model combinations rather than a universal solution.", "AI": {"tldr": "AFD\u67b6\u6784\u5728\u7279\u5b9a\u786c\u4ef6-\u6a21\u578b\u7ec4\u5408\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u975e\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u53d7\u9650\u4e8e\u5e26\u5bbd\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898", "motivation": "\u63a2\u7d22Attention-FFN\u89e3\u8026\u67b6\u6784(AFD)\u4e0e\u6807\u51c6\u4e13\u5bb6\u5e76\u884c(EP)\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u5206\u6790AFD\u5728\u5185\u5b58\u5bb9\u91cf\u548c\u5e26\u5bbd\u65b9\u9762\u7684\u6f5c\u5728\u4f18\u52bf", "method": "\u5c06\u5c4b\u9876\u7ebf\u6a21\u578b\u6269\u5c55\u5230\u901a\u4fe1\u5c42\u9762\uff0c\u5173\u8054\u4e92\u8fde\u5e26\u5bbd\u3001\u7b97\u672f\u5f3a\u5ea6\u548c\u786c\u4ef6\u6d6e\u70b9\u8fd0\u7b97\u5229\u7528\u7387(HFU)\uff0c\u7cfb\u7edf\u5206\u6790AFD\u67b6\u6784", "result": "\u53d1\u73b0\u6807\u51c6\u96c6\u7fa4\u5b58\u5728\"\u6b7b\u533a\"\uff1a\u589e\u52a0FFN\u5b9e\u4f8b\u6570\u65e0\u6cd5\u63d0\u5347HFU\uff1bAFD\u7684\u79bb\u6563\u8282\u70b9\u7ea7\u6269\u5c55\u6bd4EP\u7684\u8fde\u7eed\u6279\u6b21\u8c03\u6574\u4ea7\u751f\u66f4\u9ad8\u4e0d\u5747\u8861\u60e9\u7f5a\uff1b\u4f46\u5728Superpod\u7ea7\u786c\u4ef6\u548c\u7c97\u7c92\u5ea6\u4e13\u5bb6\u6a21\u578b\u4e0bAFD\u8868\u73b0\u66f4\u597d", "conclusion": "AFD\u662f\u7279\u5b9a\u786c\u4ef6-\u6a21\u578b\u7ec4\u5408\u4e0b\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u800c\u975e\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u5145\u8db3\u4e92\u8fde\u5e26\u5bbd\u548c\u5408\u9002\u6a21\u578b\u7279\u6027\u624d\u80fd\u53d1\u6325\u4f18\u52bf"}}
{"id": "2602.09071", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09071", "abs": "https://arxiv.org/abs/2602.09071", "authors": ["Stefano Balla", "Stefano Zacchiroli", "Thomas Degueule", "Jean-R\u00e9my Falleri", "Romain Robbes"], "title": "DRAGON: Robust Classification for Very Large Collections of Software Repositories", "comment": null, "summary": "The ability to automatically classify source code repositories with ''topics'' that reflect their content and purpose is very useful, especially when navigating or searching through large software collections. However, existing approaches often rely heavily on README files and other metadata, which are frequently missing, limiting their applicability in real-world large-scale settings. We present DRAGON, a repository classifier designed for very large and diverse software collections. It operates entirely on lightweight signals commonly stored in version control systems: file and directory names, and optionally the README when available. In repository classification at scale, DRAGON improves F1@5 from 54.8% to 60.8%, surpassing the state of the art. DRAGON remains effective even when README files are absent, with performance degrading by only 6% w.r.t. when they are present. This robustness makes it practical for real-world settings where documentation is sparse or inconsistent. Furthermore, many of the remaining classification errors are near misses, where predicted labels are semantically close to the correct topics. This property increases the practical value of the predictions in real-world software collections, where suggesting a few related topics can still guide search and discovery. As a byproduct of developing DRAGON, we also release the largest open dataset to date for repository classification, consisting of 825 thousand repositories with associated ground-truth topics, sourced from the Software Heritage archive, providing a foundation for future large-scale and language-agnostic research on software repository understanding.", "AI": {"tldr": "DRAGON\uff1a\u57fa\u4e8e\u8f7b\u91cf\u4fe1\u53f7\uff08\u6587\u4ef6\u540d/\u76ee\u5f55\u540d\uff09\u7684\u4ee3\u7801\u4ed3\u5e93\u5206\u7c7b\u5668\uff0c\u5728README\u7f3a\u5931\u65f6\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5728\u5927\u89c4\u6a21\u8f6f\u4ef6\u96c6\u5408\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u4ee3\u7801\u4ed3\u5e93\u5206\u7c7b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56README\u7b49\u5143\u6570\u636e\uff0c\u4f46\u8fd9\u4e9b\u6587\u4ef6\u7ecf\u5e38\u7f3a\u5931\uff0c\u9650\u5236\u4e86\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6587\u6863\u7a00\u758f\u6216\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5de5\u4f5c\u3002", "method": "DRAGON\u4ec5\u4f7f\u7528\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u4e2d\u5e38\u89c1\u7684\u8f7b\u91cf\u4fe1\u53f7\uff1a\u6587\u4ef6\u548c\u76ee\u5f55\u540d\u79f0\uff0c\u53ef\u9009\u5730\u4f7f\u7528README\uff08\u5f53\u53ef\u7528\u65f6\uff09\u3002\u8be5\u65b9\u6cd5\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u5927\u89c4\u6a21\u548c\u591a\u6837\u5316\u7684\u8f6f\u4ef6\u96c6\u5408\uff0c\u4e0d\u4f9d\u8d56\u5b8c\u6574\u7684\u6587\u6863\u3002", "result": "\u5728\u5927\u89c4\u6a21\u4ed3\u5e93\u5206\u7c7b\u4e2d\uff0cDRAGON\u5c06F1@5\u4ece54.8%\u63d0\u5347\u523060.8%\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002\u5373\u4f7fREADME\u5b8c\u5168\u7f3a\u5931\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d6%\u3002\u8bb8\u591a\u5206\u7c7b\u9519\u8bef\u662f\"\u8fd1\u4f3c\u547d\u4e2d\"\uff0c\u9884\u6d4b\u6807\u7b7e\u5728\u8bed\u4e49\u4e0a\u63a5\u8fd1\u6b63\u786e\u4e3b\u9898\u3002", "conclusion": "DRAGON\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5927\u89c4\u6a21\u4ee3\u7801\u4ed3\u5e93\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6587\u6863\u7a00\u758f\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u540c\u65f6\u53d1\u5e03\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u5f00\u6e90\u4ed3\u5e93\u5206\u7c7b\u6570\u636e\u96c6\uff0882.5\u4e07\u4e2a\u4ed3\u5e93\uff09\uff0c\u4e3a\u672a\u6765\u5927\u89c4\u6a21\u3001\u8bed\u8a00\u65e0\u5173\u7684\u8f6f\u4ef6\u4ed3\u5e93\u7406\u89e3\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.09132", "categories": ["cs.DB", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.09132", "abs": "https://arxiv.org/abs/2602.09132", "authors": ["Jiyong Rao", "Yicheng Qiu", "Jiahui Zhang", "Juntao Deng", "Shangquan Sun", "Fenghua Ling", "Hao Chen", "Nanqing Dong", "Zhangyang Gao", "Siqi Sun", "Yuqiang Li", "Dongzhan Zhou", "Guangyu Wang", "Lijun Wu", "Conghui He", "Xuhong Wang", "Jing Shao", "Xiang Liu", "Yu Zhu", "Mianxin Liu", "Qihao Zheng", "Yinghui Zhang", "Jiamin Wu", "Xiaosong Wang", "Shixiang Tang", "Wenlong Zhang", "Bo Zhang", "Wanli Ouyang", "Runkai Zhao", "Chunfeng Song", "Lei Bai", "Chi Zhang"], "title": "SciDataCopilot: An Agentic Data Preparation Framework for AGI-driven Scientific Discovery", "comment": null, "summary": "The current landscape of AI for Science (AI4S) is predominantly anchored in large-scale textual corpora, where generative AI systems excel at hypothesis generation, literature search, and multi-modal reasoning. However, a critical bottleneck for accelerating closed-loop scientific discovery remains the utilization of raw experimental data. Characterized by extreme heterogeneity, high specificity, and deep domain expertise requirements, raw data possess neither direct semantic alignment with linguistic representations nor structural homogeneity suitable for a unified embedding space. The disconnect prevents the emerging class of Artificial General Intelligence for Science (AGI4S) from effectively interfacing with the physical reality of experimentation. In this work, we extend the text-centric AI-Ready concept to Scientific AI-Ready data paradigm, explicitly formalizing how scientific data is specified, structured, and composed within a computational workflow. To operationalize this idea, we propose SciDataCopilot, an autonomous agentic framework designed to handle data ingestion, scientific intent parsing, and multi-modal integration in a end-to-end manner. By positioning data readiness as a core operational primitive, the framework provides a principled foundation for reusable, transferable systems, enabling the transition toward experiment-driven scientific general intelligence. Extensive evaluations across three heterogeneous scientific domains show that SciDataCopilot improves efficiency, scalability, and consistency over manual pipelines, with up to 30$\\times$ speedup in data preparation.", "AI": {"tldr": "\u63d0\u51faSciDataCopilot\u6846\u67b6\uff0c\u5c06AI-Ready\u6982\u5ff5\u6269\u5c55\u5230\u79d1\u5b66\u6570\u636e\u9886\u57df\uff0c\u89e3\u51b3\u539f\u59cb\u5b9e\u9a8c\u6570\u636e\u5f02\u6784\u6027\u9ad8\u3001\u96be\u4ee5\u4e0eAI\u7cfb\u7edf\u5bf9\u63a5\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u6570\u636e\u51c6\u5907\u81ea\u52a8\u5316\u3002", "motivation": "\u5f53\u524dAI for Science\u4e3b\u8981\u57fa\u4e8e\u6587\u672c\u8bed\u6599\uff0c\u4f46\u539f\u59cb\u5b9e\u9a8c\u6570\u636e\u5b58\u5728\u6781\u7aef\u5f02\u6784\u6027\u3001\u9ad8\u7279\u5f02\u6027\u3001\u9700\u8981\u6df1\u5ea6\u9886\u57df\u77e5\u8bc6\u7b49\u95ee\u9898\uff0c\u7f3a\u4e4f\u4e0e\u8bed\u8a00\u8868\u793a\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u7edf\u4e00\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\u540c\u8d28\u6027\uff0c\u963b\u788d\u4e86AGI4S\u4e0e\u5b9e\u9a8c\u7269\u7406\u73b0\u5b9e\u7684\u5bf9\u63a5\u3002", "method": "\u63d0\u51faScientific AI-Ready\u6570\u636e\u8303\u5f0f\uff0c\u5c06\u79d1\u5b66\u6570\u636e\u5728\u8ba1\u7b97\u5de5\u4f5c\u6d41\u4e2d\u7684\u89c4\u8303\u3001\u7ed3\u6784\u548c\u7ec4\u5408\u65b9\u5f0f\u5f62\u5f0f\u5316\u3002\u5b9e\u73b0SciDataCopilot\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\uff0c\u5904\u7406\u6570\u636e\u6444\u53d6\u3001\u79d1\u5b66\u610f\u56fe\u89e3\u6790\u548c\u591a\u6a21\u6001\u96c6\u6210\uff0c\u4ee5\u7aef\u5230\u7aef\u65b9\u5f0f\u8fd0\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u5f02\u6784\u79d1\u5b66\u9886\u57df\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0cSciDataCopilot\u76f8\u6bd4\u624b\u52a8\u6d41\u7a0b\u63d0\u9ad8\u4e86\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u4e00\u81f4\u6027\uff0c\u6570\u636e\u51c6\u5907\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe30\u500d\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6570\u636e\u51c6\u5907\u5c31\u7eea\u6027\u4f5c\u4e3a\u6838\u5fc3\u64cd\u4f5c\u539f\u8bed\uff0c\u8be5\u6846\u67b6\u4e3a\u53ef\u91cd\u7528\u3001\u53ef\u8f6c\u79fb\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u5411\u5b9e\u9a8c\u9a71\u52a8\u7684\u79d1\u5b66\u901a\u7528\u667a\u80fd\u7684\u8fc7\u6e21\u3002"}}
{"id": "2602.09725", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.09725", "abs": "https://arxiv.org/abs/2602.09725", "authors": ["Liang Mi", "Weijun Wang", "Jinghan Chen", "Ting Cao", "Haipeng Dai", "Yunxin Liu"], "title": "Efficient Remote Prefix Fetching with GPU-native Media ASICs", "comment": null, "summary": "Remote KV cache reuse fetches KV cache for identical contexts from remote storage, avoiding recomputation, accelerating LLM inference. While it excels in high-speed networks, its performance degrades significantly in bandwidth-limited scenarios. Recent studies address this by transmitting KV caches in compressed form, but the associated heavyweight decompression counteracts the KV reuse benefits. In this paper, we propose an efficient and widely deployable remote KV cache reuse solution that leverages GPU-native video codecs. Our system, KVFetcher, enables effective KV cache coding with two techniques. The codec-friendly tensor layout compresses the KV cache in a highly compact video format, enabling fast transmission. The efficient KV fetcher orchestrates the transmission, decoding, and restoration of compressed KV caches in an efficient pipelined manner, eliminating resource contention, masking network fluctuations, and achieving minimum time-to-first-token (TTFT). We prototype KVFetcher on diverse GPUs from high- to low-end. Experiments reveal that it reduces TTFT by up to 3.51 times while maintaining lossless accuracy, compared to SOTA methods.", "AI": {"tldr": "KVFetcher\uff1a\u5229\u7528GPU\u539f\u751f\u89c6\u9891\u7f16\u89e3\u7801\u5668\u5b9e\u73b0\u9ad8\u6548\u7684\u8fdc\u7a0bKV\u7f13\u5b58\u91cd\u7528\uff0c\u5728\u5e26\u5bbd\u53d7\u9650\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u8fdc\u7a0bKV\u7f13\u5b58\u91cd\u7528\u65b9\u6848\u5728\u9ad8\u901f\u7f51\u7edc\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5e26\u5bbd\u53d7\u9650\u573a\u666f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u901a\u8fc7\u538b\u7f29\u4f20\u8f93KV\u7f13\u5b58\u6765\u5e94\u5bf9\uff0c\u4f46\u91cd\u91cf\u7ea7\u7684\u89e3\u538b\u7f29\u8fc7\u7a0b\u62b5\u6d88\u4e86KV\u91cd\u7528\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faKVFetcher\u7cfb\u7edf\uff0c\u91c7\u7528\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u7f16\u89e3\u7801\u53cb\u597d\u7684\u5f20\u91cf\u5e03\u5c40\uff0c\u5c06KV\u7f13\u5b58\u538b\u7f29\u4e3a\u9ad8\u5ea6\u7d27\u51d1\u7684\u89c6\u9891\u683c\u5f0f\uff1b2) \u9ad8\u6548\u7684KV\u63d0\u53d6\u5668\uff0c\u4ee5\u6d41\u6c34\u7ebf\u65b9\u5f0f\u534f\u8c03\u538b\u7f29KV\u7f13\u5b58\u7684\u4f20\u8f93\u3001\u89e3\u7801\u548c\u6062\u590d\uff0c\u6d88\u9664\u8d44\u6e90\u4e89\u7528\u5e76\u63a9\u76d6\u7f51\u7edc\u6ce2\u52a8\u3002", "result": "\u5728\u9ad8\u3001\u4e2d\u3001\u4f4e\u7aefGPU\u4e0a\u539f\u578b\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u8868\u660e\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cKVFetcher\u5c06\u9996\u6b21\u4ee4\u724c\u65f6\u95f4(TTFT)\u6700\u591a\u964d\u4f4e3.51\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u65e0\u635f\u7cbe\u5ea6\u3002", "conclusion": "KVFetcher\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5e7f\u6cdb\u53ef\u90e8\u7f72\u7684\u8fdc\u7a0bKV\u7f13\u5b58\u91cd\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5229\u7528GPU\u539f\u751f\u89c6\u9891\u7f16\u89e3\u7801\u5668\u5728\u5e26\u5bbd\u53d7\u9650\u573a\u666f\u4e0b\u663e\u8457\u52a0\u901fLLM\u63a8\u7406\u3002"}}
{"id": "2602.09185", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.09185", "abs": "https://arxiv.org/abs/2602.09185", "authors": ["Hao Li", "Haoxiang Zhang", "Ahmed E. Hassan"], "title": "AIDev: Studying AI Coding Agents on GitHub", "comment": null, "summary": "AI coding agents are rapidly transforming software engineering by performing tasks such as feature development, debugging, and testing. Despite their growing impact, the research community lacks a comprehensive dataset capturing how these agents are used in real-world projects. To address this gap, we introduce AIDev, a large-scale dataset focused on agent-authored pull requests (Agentic-PRs) in real-world GitHub repositories. AIDev aggregates 932,791 Agentic-PRs produced by five agents: OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code. These PRs span 116,211 repositories and involve 72,189 developers. In addition, AIDev includes a curated subset of 33,596 Agentic-PRs from 2,807 repositories with over 100 stars, providing further information such as comments, reviews, commits, and related issues. This dataset offers a foundation for future research on AI adoption, developer productivity, and human-AI collaboration in the new era of software engineering.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Agentic Software Engineering, Agentic Engineering", "AI": {"tldr": "AIDev\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6536\u96c6\u4e86932,791\u4e2a\u7531AI\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9eGitHub\u9879\u76ee\u4e2d\u521b\u5efa\u7684Pull Request\uff0c\u6db5\u76d65\u79cd\u4e3b\u6d41AI\u4ee3\u7406\uff0c\u4e3a\u7814\u7a76AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u91c7\u7528\u3001\u751f\u4ea7\u529b\u548c\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u57fa\u7840\u3002", "motivation": "AI\u7f16\u7801\u4ee3\u7406\u6b63\u5728\u5feb\u901f\u6539\u53d8\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u4f46\u7814\u7a76\u793e\u533a\u7f3a\u4e4f\u5168\u9762\u6570\u636e\u96c6\u6765\u6355\u6349\u8fd9\u4e9b\u4ee3\u7406\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u521b\u5efa\u80fd\u591f\u53cd\u6620AI\u4ee3\u7406\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u5e94\u7528\u7684\u6570\u636e\u96c6\u3002", "method": "\u4eceGitHub\u6536\u96c6AI\u4ee3\u7406\u521b\u5efa\u7684Pull Request\u6570\u636e\uff0c\u5305\u62ec5\u79cd\u4e3b\u6d41AI\u4ee3\u7406\uff08OpenAI Codex\u3001Devin\u3001GitHub Copilot\u3001Cursor\u3001Claude Code\uff09\u3002\u6570\u636e\u96c6\u5305\u542b932,791\u4e2aAgentic-PR\uff0c\u6d89\u53ca116,211\u4e2a\u4ed3\u5e93\u548c72,189\u540d\u5f00\u53d1\u8005\u3002\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u7cbe\u9009\u5b50\u96c6\uff0c\u5305\u542b33,596\u4e2a\u6765\u81ea2,807\u4e2a\u9ad8\u661f\u4ed3\u5e93\u7684PR\uff0c\u9644\u5e26\u8bc4\u8bba\u3001\u5ba1\u67e5\u3001\u63d0\u4ea4\u548c\u76f8\u5173\u95ee\u9898\u7b49\u8be6\u7ec6\u4fe1\u606f\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86AIDev\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8eAI\u4ee3\u7406\u5728\u771f\u5b9eGitHub\u9879\u76ee\u4e2d\u521b\u5efaPull Request\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u89c4\u6a21\u5e9e\u5927\uff0c\u8986\u76d6\u4e86\u591a\u79cd\u4e3b\u6d41AI\u4ee3\u7406\u548c\u5927\u91cf\u5f00\u53d1\u9879\u76ee\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u57fa\u7840\u3002", "conclusion": "AIDev\u6570\u636e\u96c6\u4e3a\u7814\u7a76AI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91c7\u7528\u3001\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u4ee5\u53ca\u4eba\u673a\u534f\u4f5c\u7684\u65b0\u65f6\u4ee3\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u586b\u8865\u4e86\u8be5\u9886\u57df\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.09572", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.09572", "abs": "https://arxiv.org/abs/2602.09572", "authors": ["Vid Kocijan", "Jinu Sunil", "Jan Eric Lenssen", "Viman Deb", "Xinwei Xe", "Federco Reyes Gomez", "Matthias Fey", "Jure Leskovec"], "title": "Predictive Query Language: A Domain-Specific Language for Predictive Modeling on Relational Databases", "comment": null, "summary": "The purpose of predictive modeling on relational data is to predict future or missing values in a relational database, for example, future purchases of a user, risk of readmission of the patient, or the likelihood that a financial transaction is fraudulent. Typically powered by machine learning methods, predictive models are used in recommendations, financial fraud detection, supply chain optimization, and other systems, providing billions of predictions every day. However, training a machine learning model requires manual work to extract the required training examples - prediction entities and target labels - from the database, which is slow, laborious, and prone to mistakes. Here, we present the Predictive Query Language (PQL), a SQL-inspired declarative language for defining predictive tasks on relational databases. PQL allows specifying a predictive task in a single declarative query, enabling the automatic computation training labels for a large variety of machine learning tasks, such as regression, classification, time-series forecasting, and recommender systems. PQL is already successfully integrated and used in a collection of use cases as part of a predictive AI platform. The versatility of the language can be demonstrated through its many ongoing use cases, including financial fraud, item recommendations, and workload prediction. We demonstrate its versatile design through two implementations; one for small-scale, low-latency use and one that can handle large-scale databases.", "AI": {"tldr": "\u63d0\u51faPredictive Query Language (PQL)\uff0c\u4e00\u79cd\u7c7b\u4f3cSQL\u7684\u58f0\u660e\u5f0f\u8bed\u8a00\uff0c\u7528\u4e8e\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e0a\u5b9a\u4e49\u9884\u6d4b\u4efb\u52a1\uff0c\u81ea\u52a8\u751f\u6210\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6807\u7b7e\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u4e0a\u7684\u9884\u6d4b\u5efa\u6a21\u9700\u8981\u4ece\u6570\u636e\u5e93\u4e2d\u624b\u52a8\u63d0\u53d6\u8bad\u7ec3\u6837\u672c\uff08\u9884\u6d4b\u5b9e\u4f53\u548c\u76ee\u6807\u6807\u7b7e\uff09\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u7f13\u6162\u3001\u8d39\u529b\u4e14\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u8bbe\u8ba1PQL\u8bed\u8a00\uff0c\u5141\u8bb8\u901a\u8fc7\u5355\u4e2a\u58f0\u660e\u5f0f\u67e5\u8be2\u6307\u5b9a\u9884\u6d4b\u4efb\u52a1\uff0c\u81ea\u52a8\u8ba1\u7b97\u5404\u79cd\u673a\u5668\u5b66\u4e60\u4efb\u52a1\uff08\u56de\u5f52\u3001\u5206\u7c7b\u3001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u63a8\u8350\u7cfb\u7edf\uff09\u7684\u8bad\u7ec3\u6807\u7b7e\u3002", "result": "PQL\u5df2\u6210\u529f\u96c6\u6210\u5230\u9884\u6d4bAI\u5e73\u53f0\u4e2d\uff0c\u5728\u91d1\u878d\u6b3a\u8bc8\u3001\u5546\u54c1\u63a8\u8350\u3001\u5de5\u4f5c\u8d1f\u8f7d\u9884\u6d4b\u7b49\u591a\u4e2a\u7528\u4f8b\u4e2d\u5f97\u5230\u5e94\u7528\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e24\u79cd\u5b9e\u73b0\u65b9\u6848\uff1a\u5c0f\u89c4\u6a21\u4f4e\u5ef6\u8fdf\u548c\u5927\u89c4\u6a21\u6570\u636e\u5e93\u5904\u7406\u3002", "conclusion": "PQL\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u5316\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u5173\u7cfb\u6570\u636e\u5e93\u4e0a\u7684\u9884\u6d4b\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u63d0\u53d6\u8bad\u7ec3\u6570\u636e\u7684\u75db\u70b9\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2602.09292", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09292", "abs": "https://arxiv.org/abs/2602.09292", "authors": ["Ana B. M. Bett", "Thais S. Nepomuceno", "Edson OliveiraJr", "Maria Teresa Baldassarre", "Valdemar V. Graciano Neto", "Marcos Kalinowski"], "title": "Towards an OSF-based Registered Report Template for Software Engineering Controlled Experiments", "comment": "Author version of paper accepted at the 3rd International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE@ICSE 2026)", "summary": "Context: The empirical software engineering (ESE) community has contributed to improving experimentation over the years. However, there is still a lack of rigor in describing controlled experiments, hindering reproducibility and transparency. Registered Reports (RR) have been discussed in the ESE community to address these issues. A RR registers a study's hypotheses, methods, and/or analyses before execution, involving peer review and potential acceptance before data collection. This helps mitigate problematic practices such as p-hacking, publication bias, and inappropriate post hoc analysis. Objective: This paper presents initial results toward establishing an RR template for Software Engineering controlled experiments using the Open Science Framework (OSF). Method: We analyzed templates of selected OSF RR types in light of documentation guidelines for controlled experiments. Results: The observed lack of rigor motivated our investigation of OSF-based RR types. Our analysis showed that, although one of the RR types aligned with many of the documentation suggestions contained in the guidelines, none of them covered the guidelines comprehensively. The study also highlights limitations in OSF RR template customization. Conclusion: Despite progress in ESE, planning and documenting experiments still lack rigor, compromising reproducibility. Adopting OSF-based RRs is proposed. However, no currently available RR type fully satisfies the guidelines. Establishing RR-specific guidelines for SE is deemed essential.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5728\u8f6f\u4ef6\u5de5\u7a0b\u63a7\u5236\u5b9e\u9a8c\u4e2d\u91c7\u7528\u6ce8\u518c\u62a5\u544a\uff08RR\uff09\u7684\u73b0\u72b6\uff0c\u53d1\u73b0\u73b0\u6709OSF RR\u6a21\u677f\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3SE\u5b9e\u9a8c\u6587\u6863\u6307\u5357\uff0c\u5efa\u8bae\u5efa\u7acbSE\u4e13\u7528\u7684RR\u6307\u5357\u3002", "motivation": "\u5c3d\u7ba1\u7ecf\u9a8c\u8f6f\u4ef6\u5de5\u7a0b\u793e\u533a\u591a\u5e74\u6765\u6539\u8fdb\u4e86\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u4f46\u63a7\u5236\u5b9e\u9a8c\u7684\u63cf\u8ff0\u4ecd\u7136\u7f3a\u4e4f\u4e25\u8c28\u6027\uff0c\u5f71\u54cd\u4e86\u53ef\u91cd\u590d\u6027\u548c\u900f\u660e\u5ea6\u3002\u6ce8\u518c\u62a5\u544a\uff08RR\uff09\u88ab\u63d0\u51fa\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u8bc4\u4f30\u5176\u5728SE\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "method": "\u5206\u6790Open Science Framework\uff08OSF\uff09\u4e2d\u9009\u5b9a\u7684RR\u6a21\u677f\uff0c\u5bf9\u7167\u8f6f\u4ef6\u5de5\u7a0b\u63a7\u5236\u5b9e\u9a8c\u7684\u6587\u6863\u6307\u5357\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff1a1\uff09\u867d\u7136\u6709\u4e00\u4e2aRR\u7c7b\u578b\u4e0e\u8bb8\u591a\u6587\u6863\u5efa\u8bae\u76f8\u7b26\uff0c\u4f46\u6ca1\u6709\u4e00\u4e2a\u80fd\u5168\u9762\u8986\u76d6\u6307\u5357\u8981\u6c42\uff1b2\uff09OSF RR\u6a21\u677f\u5b9a\u5236\u5b58\u5728\u5c40\u9650\u6027\uff1b3\uff09\u5f53\u524d\u53ef\u7528\u7684RR\u7c7b\u578b\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3SE\u5b9e\u9a8c\u7684\u6587\u6863\u9700\u6c42\u3002", "conclusion": "\u5c3d\u7ba1ESE\u9886\u57df\u6709\u8fdb\u6b65\uff0c\u4f46\u5b9e\u9a8c\u89c4\u5212\u548c\u6587\u6863\u4ecd\u7f3a\u4e4f\u4e25\u8c28\u6027\uff0c\u635f\u5bb3\u4e86\u53ef\u91cd\u590d\u6027\u3002\u5efa\u8bae\u91c7\u7528\u57fa\u4e8eOSF\u7684RR\uff0c\u4f46\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7684RR\u6307\u5357\uff0c\u56e0\u4e3a\u73b0\u6709RR\u7c7b\u578b\u65e0\u6cd5\u5b8c\u5168\u6ee1\u8db3\u9700\u6c42\u3002"}}
{"id": "2602.10027", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.10027", "abs": "https://arxiv.org/abs/2602.10027", "authors": ["Dominik Winecki"], "title": "Optimal Bounds-Only Pruning for Spatial AkNN Joins", "comment": null, "summary": "We propose a bounds-only pruning test for exact Euclidean AkNN joins on partitioned spatial datasets. Data warehouses commonly partition large tables and store row group statistics for them to accelerate searches and joins, rather than maintaining indexes. AkNN joins can benefit from such statistics by constructing bounds and localizing join evaluations to a few partitions before loading them to build spatial indexes. Existing pruning methods are overly conservative for bounds-only spatial data because they do not fully capture its directional semantics, thereby missing opportunities to skip unneeded partitions at the earliest stages of a join. We propose a three-bound proximity test to determine whether all points within a partition have a closer neighbor in one partition than in another, potentially occluded partition. We show that our algorithm is both optimal and efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u8fb9\u754c\u7684\u4e09\u754c\u90bb\u8fd1\u6027\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5206\u533a\u7a7a\u95f4\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cbe\u786e\u6b27\u51e0\u91cc\u5f97AkNN\u8fde\u63a5\uff0c\u901a\u8fc7\u66f4\u7cbe\u786e\u5730\u6355\u6349\u65b9\u5411\u8bed\u4e49\u6765\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u5206\u533a\u52a0\u8f7d\u3002", "motivation": "\u6570\u636e\u4ed3\u5e93\u901a\u5e38\u5bf9\u5927\u578b\u8868\u8fdb\u884c\u5206\u533a\u5e76\u5b58\u50a8\u884c\u7ec4\u7edf\u8ba1\u4fe1\u606f\u4ee5\u52a0\u901f\u641c\u7d22\u548c\u8fde\u63a5\uff0c\u800c\u4e0d\u662f\u7ef4\u62a4\u7d22\u5f15\u3002\u73b0\u6709\u7684\u526a\u679d\u65b9\u6cd5\u5bf9\u4e8e\u4ec5\u8fb9\u754c\u7a7a\u95f4\u6570\u636e\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u5b8c\u5168\u6355\u6349\u5176\u65b9\u5411\u8bed\u4e49\uff0c\u4ece\u800c\u5728\u8fde\u63a5\u7684\u6700\u65e9\u9636\u6bb5\u9519\u8fc7\u4e86\u8df3\u8fc7\u4e0d\u9700\u8981\u5206\u533a\u7684\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u754c\u90bb\u8fd1\u6027\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u5206\u533a\u5185\u6240\u6709\u70b9\u662f\u5426\u5728\u4e00\u4e2a\u5206\u533a\u4e2d\u6709\u6bd4\u53e6\u4e00\u4e2a\u53ef\u80fd\u88ab\u906e\u6321\u7684\u5206\u533a\u66f4\u8fd1\u7684\u90bb\u5c45\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u5206\u533a\u8fb9\u754c\u7edf\u8ba1\u4fe1\u606f\uff0c\u5728\u6784\u5efa\u7a7a\u95f4\u7d22\u5f15\u4e4b\u524d\u5c06\u8fde\u63a5\u8bc4\u4f30\u672c\u5730\u5316\u5230\u5c11\u6570\u5206\u533a\u3002", "result": "\u8be5\u7b97\u6cd5\u65e2\u662f\u6700\u4f18\u7684\u53c8\u662f\u9ad8\u6548\u7684\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u8df3\u8fc7\u4e0d\u9700\u8981\u7684\u5206\u533a\uff0c\u4ece\u800c\u51cf\u5c11\u6570\u636e\u52a0\u8f7d\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u754c\u90bb\u8fd1\u6027\u6d4b\u8bd5\u65b9\u6cd5\u6539\u8fdb\u4e86\u5206\u533a\u7a7a\u95f4\u6570\u636e\u96c6\u4e0a\u7684AkNN\u8fde\u63a5\u6027\u80fd\uff0c\u901a\u8fc7\u66f4\u597d\u5730\u5229\u7528\u8fb9\u754c\u7edf\u8ba1\u4fe1\u606f\u7684\u65b9\u5411\u8bed\u4e49\uff0c\u5728\u8fde\u63a5\u65e9\u671f\u9636\u6bb5\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u526a\u679d\u3002"}}
{"id": "2602.09311", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09311", "abs": "https://arxiv.org/abs/2602.09311", "authors": ["Tao Xiao", "Dong Wang", "Shane McIntosh", "Hideaki Hata", "Yasutaka Kamei"], "title": "Cross-Project Flakiness: A Case Study of the OpenStack Ecosystem", "comment": null, "summary": "Automated regression testing is a cornerstone of modern software development, often contributing directly to code review and Continuous Integration (CI). Yet some tests suffer from flakiness, where their outcomes vary non-deterministically. Flakiness erodes developer trust in test results, wastes computational resources, and undermines CI reliability. While prior research has examined test flakiness within individual projects, its broader ecosystem-wide impact remains largely unexplored. In this paper, we present an empirical study of test flakiness in the OpenStack ecosystem, which focuses on (1) cross-project flakiness, where flaky tests impact multiple projects, and (2) inconsistent flakiness, where a test exhibits flakiness in some projects but remains stable in others. By analyzing 649 OpenStack projects, we identify 1,535 cross-project flaky tests and 1,105 inconsistently flaky tests. We find that cross-project flakiness affects 55% of OpenStack projects and significantly increases both review time and computational costs. Surprisingly, 70% of unit tests exhibit cross-project flakiness, challenging the assumption that unit tests are inherently insulated from issues that span modules like integration and system-level tests. Through qualitative analysis, we observe that race conditions in CI, inconsistent build configurations, and dependency mismatches are the primary causes of inconsistent flakiness. These findings underline the need for better coordination across complex ecosystems, standardized CI configurations, and improved test isolation strategies.", "AI": {"tldr": "\u5bf9OpenStack\u751f\u6001\u7cfb\u7edf\u4e2d\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd55%\u7684\u9879\u76ee\uff0c\u663e\u8457\u589e\u52a0\u5ba1\u67e5\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\uff0c70%\u7684\u5355\u5143\u6d4b\u8bd5\u4e5f\u53d7\u5f71\u54cd\uff0c\u6311\u6218\u4e86\u5355\u5143\u6d4b\u8bd5\u9694\u79bb\u6027\u7684\u5047\u8bbe\u3002", "motivation": "\u81ea\u52a8\u5316\u56de\u5f52\u6d4b\u8bd5\u662f\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u7684\u6838\u5fc3\uff0c\u4f46\u4e0d\u7a33\u5b9a\u7684\u6d4b\u8bd5\u7ed3\u679c\u4f1a\u7834\u574f\u5f00\u53d1\u8005\u4fe1\u4efb\u3001\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u5e76\u635f\u5bb3CI\u53ef\u9760\u6027\u3002\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u9879\u76ee\u5185\u7684\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\uff0c\u4f46\u5176\u5728\u66f4\u5e7f\u6cdb\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5bf9649\u4e2aOpenStack\u9879\u76ee\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\uff1a(1)\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027\u2014\u2014\u4e0d\u7a33\u5b9a\u7684\u6d4b\u8bd5\u5f71\u54cd\u591a\u4e2a\u9879\u76ee\uff1b(2)\u4e0d\u4e00\u81f4\u7684\u4e0d\u7a33\u5b9a\u6027\u2014\u2014\u6d4b\u8bd5\u5728\u67d0\u4e9b\u9879\u76ee\u4e2d\u4e0d\u7a33\u5b9a\u4f46\u5728\u5176\u4ed6\u9879\u76ee\u4e2d\u7a33\u5b9a\u3002\u901a\u8fc7\u5206\u6790\u8bc6\u522b\u51fa1,535\u4e2a\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u548c1,105\u4e2a\u4e0d\u4e00\u81f4\u4e0d\u7a33\u5b9a\u6d4b\u8bd5\u3002", "result": "\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd55%\u7684OpenStack\u9879\u76ee\uff0c\u663e\u8457\u589e\u52a0\u5ba1\u67e5\u65f6\u95f4\u548c\u8ba1\u7b97\u6210\u672c\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c70%\u7684\u5355\u5143\u6d4b\u8bd5\u8868\u73b0\u51fa\u8de8\u9879\u76ee\u4e0d\u7a33\u5b9a\u6027\uff0c\u6311\u6218\u4e86\u5355\u5143\u6d4b\u8bd5\u5929\u751f\u9694\u79bb\u4e8e\u8de8\u6a21\u5757\u95ee\u9898\u7684\u5047\u8bbe\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0cCI\u4e2d\u7684\u7ade\u4e89\u6761\u4ef6\u3001\u4e0d\u4e00\u81f4\u7684\u6784\u5efa\u914d\u7f6e\u548c\u4f9d\u8d56\u4e0d\u5339\u914d\u662f\u4e0d\u4e00\u81f4\u4e0d\u7a33\u5b9a\u6027\u7684\u4e3b\u8981\u539f\u56e0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u751f\u6001\u7cfb\u7edf\u4e2d\u9700\u8981\u66f4\u597d\u7684\u8de8\u9879\u76ee\u534f\u8c03\u3001\u6807\u51c6\u5316\u7684CI\u914d\u7f6e\u548c\u6539\u8fdb\u7684\u6d4b\u8bd5\u9694\u79bb\u7b56\u7565\u3002\u6d4b\u8bd5\u4e0d\u7a33\u5b9a\u6027\u4e0d\u4ec5\u662f\u5355\u4e2a\u9879\u76ee\u95ee\u9898\uff0c\u800c\u662f\u5f71\u54cd\u6574\u4e2a\u751f\u6001\u7cfb\u7edf\u7684\u7cfb\u7edf\u6027\u6311\u6218\u3002"}}
{"id": "2602.09447", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09447", "abs": "https://arxiv.org/abs/2602.09447", "authors": ["Zhirui Zhang", "Hongbo Zhang", "Haoxiang Fei", "Zhiyuan Bao", "Yubin Chen", "Zhengyu Lei", "Ziyue Liu", "Yixuan Sun", "Mingkun Xiao", "Zihang Ye", "Yu Zhang", "Hongcheng Zhu", "Yuxiang Wen", "Heung-Yeung Shum"], "title": "SWE-AGI: Benchmarking Specification-Driven Software Construction with MoonBit in the Era of Autonomous Agents", "comment": "20 pages, 3 figures", "summary": "Although large language models (LLMs) have demonstrated impressive coding capabilities, their ability to autonomously build production-scale software from explicit specifications remains an open question. We introduce SWE-AGI, an open-source benchmark for evaluating end-to-end, specification-driven construction of software systems written in MoonBit. SWE-AGI tasks require LLM-based agents to implement parsers, interpreters, binary decoders, and SAT solvers strictly from authoritative standards and RFCs under a fixed API scaffold. Each task involves implementing 1,000-10,000 lines of core logic, corresponding to weeks or months of engineering effort for an experienced human developer. By leveraging the nascent MoonBit ecosystem, SWE-AGI minimizes data leakage, forcing agents to rely on long-horizon architectural reasoning rather than code retrieval. Across frontier models, gpt-5.3-codex achieves the best overall performance (solving 19/22 tasks, 86.4%), outperforming claude-opus-4.6 (15/22, 68.2%), and kimi-2.5 exhibits the strongest performance among open-source models. Performance degrades sharply with increasing task difficulty, particularly on hard, specification-intensive systems. Behavioral analysis further reveals that as codebases scale, code reading, rather than writing, becomes the dominant bottleneck in AI-assisted development. Overall, while specification-driven autonomous software engineering is increasingly viable, substantial challenges remain before it can reliably support production-scale development.", "AI": {"tldr": "SWE-AGI\u662f\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u57fa\u4e8e\u660e\u786e\u89c4\u8303\u81ea\u4e3b\u6784\u5efa\u751f\u4ea7\u7ea7\u8f6f\u4ef6\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u4f7f\u7528MoonBit\u8bed\u8a00\uff0c\u8981\u6c42\u5b9e\u73b0\u89e3\u6790\u5668\u3001\u89e3\u91ca\u5668\u7b49\u590d\u6742\u7cfb\u7edf\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793aGPT-5.3-Codex\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u968f\u7740\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7801\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u80fd\u5426\u6839\u636e\u660e\u786e\u89c4\u8303\u81ea\u4e3b\u6784\u5efa\u751f\u4ea7\u7ea7\u8f6f\u4ef6\u7cfb\u7edf\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002\u9700\u8981\u8bc4\u4f30LLM\u5728\u7aef\u5230\u7aef\u3001\u89c4\u8303\u9a71\u52a8\u7684\u8f6f\u4ef6\u7cfb\u7edf\u6784\u5efa\u4e2d\u7684\u5b9e\u9645\u80fd\u529b\u3002", "method": "\u5f15\u5165SWE-AGI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528MoonBit\u8bed\u8a00\u7f16\u5199\uff0c\u8981\u6c42LLM\u4ee3\u7406\u6839\u636e\u6743\u5a01\u6807\u51c6\u548cRFC\u4e25\u683c\u5b9e\u73b0\u89e3\u6790\u5668\u3001\u89e3\u91ca\u5668\u3001\u4e8c\u8fdb\u5236\u89e3\u7801\u5668\u548cSAT\u6c42\u89e3\u5668\uff0c\u6bcf\u4e2a\u4efb\u52a1\u6d89\u53ca1,000-10,000\u884c\u6838\u5fc3\u903b\u8f91\u4ee3\u7801\uff0c\u5229\u7528\u65b0\u5174\u7684MoonBit\u751f\u6001\u7cfb\u7edf\u6700\u5c0f\u5316\u6570\u636e\u6cc4\u6f0f\u3002", "result": "GPT-5.3-Codex\u8868\u73b0\u6700\u4f73\uff08\u89e3\u51b319/22\u4efb\u52a1\uff0c86.4%\uff09\uff0c\u4f18\u4e8eClaude-Opus-4.6\uff0815/22\uff0c68.2%\uff09\uff0cKimi-2.5\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u5f3a\u3002\u968f\u7740\u4efb\u52a1\u96be\u5ea6\u589e\u52a0\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u89c4\u8303\u5bc6\u96c6\u7684\u56f0\u96be\u7cfb\u7edf\u4e0a\u3002\u884c\u4e3a\u5206\u6790\u663e\u793a\uff0c\u968f\u7740\u4ee3\u7801\u5e93\u89c4\u6a21\u6269\u5927\uff0c\u4ee3\u7801\u9605\u8bfb\u800c\u975e\u7f16\u5199\u6210\u4e3aAI\u8f85\u52a9\u5f00\u53d1\u7684\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u867d\u7136\u89c4\u8303\u9a71\u52a8\u7684\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u8d8a\u6765\u8d8a\u53ef\u884c\uff0c\u4f46\u5728\u80fd\u591f\u53ef\u9760\u652f\u6301\u751f\u4ea7\u7ea7\u5f00\u53d1\u4e4b\u524d\uff0c\u4ecd\u7136\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u590d\u6742\u89c4\u8303\u7684\u7cfb\u7edf\u65f6\u3002"}}
{"id": "2602.09464", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.09464", "abs": "https://arxiv.org/abs/2602.09464", "authors": ["Haoyu Zhao", "Ziran Yang", "Jiawei Li", "Deyuan He", "Zenan Li", "Chi Jin", "Venugopal V. Veeravalli", "Aarti Gupta", "Sanjeev Arora"], "title": "AlgoVeri: An Aligned Benchmark for Verified Code Generation on Classical Algorithms", "comment": "32 pages", "summary": "Vericoding refers to the generation of formally verified code from rigorous specifications. Recent AI models show promise in vericoding, but a unified methodology for cross-paradigm evaluation is lacking. Existing benchmarks test only individual languages/tools (e.g., Dafny, Verus, and Lean) and each covers very different tasks, so the performance numbers are not directly comparable. We address this gap with AlgoVeri, a benchmark that evaluates vericoding of $77$ classical algorithms in Dafny, Verus, and Lean. By enforcing identical functional contracts, AlgoVeri reveals critical capability gaps in verification systems. While frontier models achieve tractable success in Dafny ($40.3$% for Gemini-3 Flash), where high-level abstractions and SMT automation simplify the workflow, performance collapses under the systems-level memory constraints of Verus ($24.7$%) and the explicit proof construction required by Lean (7.8%). Beyond aggregate metrics, we uncover a sharp divergence in test-time compute dynamics: Gemini-3 effectively utilizes iterative repair to boost performance (e.g., tripling pass rates in Dafny), whereas GPT-OSS saturates early. Finally, our error analysis shows that language design affects the refinement trajectory: while Dafny allows models to focus on logical correctness, Verus and Lean trap models in persistent syntactic and semantic barriers. All data and evaluation code can be found at https://github.com/haoyuzhao123/algoveri.", "AI": {"tldr": "AlgoVeri\u662f\u4e00\u4e2a\u8bc4\u4f30AI\u6a21\u578b\u5728Dafny\u3001Verus\u548cLean\u4e09\u79cd\u9a8c\u8bc1\u7cfb\u7edf\u4e2d\u751f\u6210\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b77\u4e2a\u7ecf\u5178\u7b97\u6cd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u9a8c\u8bc1\u7cfb\u7edf\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u548c\u8bed\u8a00\u8bbe\u8ba1\u5bf9AI\u6a21\u578b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u5404\u81ea\u4e3a\u653f\uff0c\u53ea\u6d4b\u8bd5\u5355\u4e2a\u8bed\u8a00/\u5de5\u5177\uff08\u5982Dafny\u3001Verus\u3001Lean\uff09\uff0c\u4e14\u4efb\u52a1\u5dee\u5f02\u5f88\u5927\uff0c\u5bfc\u81f4\u6027\u80fd\u6570\u636e\u65e0\u6cd5\u76f4\u63a5\u6bd4\u8f83\u3002\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u8de8\u8303\u5f0f\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u521b\u5efaAlgoVeri\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b77\u4e2a\u7ecf\u5178\u7b97\u6cd5\uff0c\u5728Dafny\u3001Verus\u548cLean\u4e09\u79cd\u9a8c\u8bc1\u7cfb\u7edf\u4e2d\u4f7f\u7528\u76f8\u540c\u7684\u529f\u80fd\u5951\u7ea6\u8fdb\u884c\u8bc4\u4f30\u3002\u901a\u8fc7\u5f3a\u5236\u4f7f\u7528\u76f8\u540c\u7684\u529f\u80fd\u89c4\u8303\uff0c\u63ed\u793a\u9a8c\u8bc1\u7cfb\u7edf\u7684\u5173\u952e\u80fd\u529b\u5dee\u8ddd\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728Dafny\u4e2d\u8868\u73b0\u6700\u4f73\uff08Gemini-3 Flash\u8fbe40.3%\uff09\uff0c\u4f46\u5728Verus\u4e2d\u6027\u80fd\u5927\u5e45\u4e0b\u964d\uff0824.7%\uff09\uff0c\u5728Lean\u4e2d\u8868\u73b0\u6700\u5dee\uff087.8%\uff09\u3002Gemini-3\u80fd\u6709\u6548\u5229\u7528\u8fed\u4ee3\u4fee\u590d\u63d0\u5347\u6027\u80fd\uff08\u5982\u5728Dafny\u4e2d\u901a\u8fc7\u7387\u63d0\u9ad8\u4e09\u500d\uff09\uff0c\u800cGPT-OSS\u5219\u65e9\u671f\u9971\u548c\u3002\u8bed\u8a00\u8bbe\u8ba1\u5f71\u54cd\u7ec6\u5316\u8f68\u8ff9\uff1aDafny\u8ba9\u6a21\u578b\u4e13\u6ce8\u4e8e\u903b\u8f91\u6b63\u786e\u6027\uff0c\u800cVerus\u548cLean\u5219\u8ba9\u6a21\u578b\u9677\u5165\u6301\u4e45\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u969c\u788d\u3002", "conclusion": "AlgoVeri\u4e3a\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u8de8\u8303\u5f0f\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u9a8c\u8bc1\u7cfb\u7edf\u5bf9AI\u6a21\u578b\u80fd\u529b\u7684\u663e\u8457\u5f71\u54cd\uff0c\u7279\u522b\u662f\u7cfb\u7edf\u7ea7\u5185\u5b58\u7ea6\u675f\u548c\u663e\u5f0f\u8bc1\u660e\u6784\u9020\u5e26\u6765\u7684\u6311\u6218\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5065\u58ee\u7684\u9a8c\u8bc1\u4ee3\u7801\u751f\u6210\u7814\u7a76\u3002"}}
{"id": "2602.09467", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09467", "abs": "https://arxiv.org/abs/2602.09467", "authors": ["Sota Nakashima", "Masanari Kondo", "Mahmoud Alfadel", "Aly Ahmad", "Toshihiro Nakae", "Hidenori Matsuzaki"], "title": "Toward Linking Declined Proposals and Source Code: An Exploratory Study on the Go Repository", "comment": "11 pages, MSR2026 Technical Track", "summary": "Traceability links are key information sources for software developers, connecting software artifacts (e.g., linking requirements to the corresponding source code). In open-source software (OSS) projects, such links play an important role, particularly between the contributions (e.g., GitHub issues) and the corresponding source code. Through these links, developers can trace the discussions in contributions and uncover design rationales, constraints, and security concerns. Previous studies have mainly examined accepted contributions, while those declined after discussion have been overlooked. The discussions behind declined contributions contain valuable design rationales and implicit knowledge about software decision-making, as the reasons behind the decline often reveal the criteria used to judge what should or should not be implemented. In this study, we present the first attempt to establish traceability links between declined contributions and related source code. We propose an initial linking approach and conduct an empirical analysis of the generated links to discuss factors affecting link generation. As our dataset, we use proposals from the official Go repository, which are GitHub issues used to propose new features or language changes. To link declined proposals to source code, we designed an LLM-driven pipeline. Our results showed that the pipeline selected the correct granularity for each declined proposal with an accuracy of 0.836, and generated correct links at that granularity with a mean precision of 0.643. To clarify the challenges of linking declined proposals, we performed a failure analysis. In the declined proposals where the pipeline failed to generate links, the discussions were often redundant and lacked concrete information (e.g., how the feature should be implemented).", "AI": {"tldr": "\u9996\u4e2a\u7814\u7a76\u5c06\u62d2\u7edd\u7684\u8d21\u732e\uff08\u5982GitHub issue\uff09\u4e0e\u76f8\u5173\u6e90\u4ee3\u7801\u5efa\u7acb\u53ef\u8ffd\u6eaf\u94fe\u63a5\uff0c\u4f7f\u7528LLM\u9a71\u52a8\u7ba1\u9053\u5728Go\u4ed3\u5e93\u4e0a\u9a8c\u8bc1\uff0c\u51c6\u786e\u73870.836\uff0c\u5e73\u5747\u7cbe\u5ea60.643", "motivation": "\u5f00\u6e90\u9879\u76ee\u4e2d\u62d2\u7edd\u7684\u8d21\u732e\u5305\u542b\u6709\u4ef7\u503c\u7684\u8bbe\u8ba1\u539f\u7406\u548c\u51b3\u7b56\u6807\u51c6\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5df2\u63a5\u53d7\u7684\u8d21\u732e\uff0c\u7f3a\u4e4f\u5bf9\u62d2\u7edd\u8d21\u732e\u7684\u53ef\u8ffd\u6eaf\u6027\u94fe\u63a5\u7814\u7a76", "method": "\u63d0\u51faLLM\u9a71\u52a8\u7684\u7ba1\u9053\u65b9\u6cd5\uff0c\u5728Go\u5b98\u65b9\u4ed3\u5e93\u7684GitHub issue\u6570\u636e\u96c6\u4e0a\uff0c\u5c06\u62d2\u7edd\u7684\u63d0\u6848\u94fe\u63a5\u5230\u76f8\u5173\u6e90\u4ee3\u7801\uff0c\u5e76\u8fdb\u884c\u5931\u8d25\u5206\u6790", "result": "\u7ba1\u9053\u5728\u6b63\u786e\u7c92\u5ea6\u9009\u62e9\u4e0a\u51c6\u786e\u73870.836\uff0c\u5728\u76f8\u5e94\u7c92\u5ea6\u4e0b\u751f\u6210\u6b63\u786e\u94fe\u63a5\u7684\u5e73\u5747\u7cbe\u5ea60.643\uff1b\u5931\u8d25\u6848\u4f8b\u4e2d\u8ba8\u8bba\u5197\u4f59\u4e14\u7f3a\u4e4f\u5177\u4f53\u5b9e\u73b0\u4fe1\u606f", "conclusion": "\u9996\u6b21\u5efa\u7acb\u4e86\u62d2\u7edd\u8d21\u732e\u4e0e\u6e90\u4ee3\u7801\u7684\u53ef\u8ffd\u6eaf\u94fe\u63a5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u4f46\u8ba8\u8bba\u8d28\u91cf\u5f71\u54cd\u94fe\u63a5\u751f\u6210\uff0c\u4e3a\u8f6f\u4ef6\u51b3\u7b56\u77e5\u8bc6\u6316\u6398\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84"}}
{"id": "2602.09540", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09540", "abs": "https://arxiv.org/abs/2602.09540", "authors": ["Muxin Tian", "Zhe Wang", "Blair Yang", "Zhenwei Tang", "Kunlun Zhu", "Honghua Dong", "Hanchen Li", "Xinni Xie", "Guangjing Wang", "Jiaxuan You"], "title": "SWE-Bench Mobile: Can Large Language Model Agents Develop Industry-Level Mobile Applications?", "comment": null, "summary": "Can large language model agents develop industry-level mobile applications? We introduce \\textbf{SWE-Bench Mobile}, a benchmark for evaluating coding agents on realistic software engineering tasks derived from a production iOS codebase. Unlike existing benchmarks that focus on isolated problems or bug fixes, SWE-Bench Mobile captures the full complexity of industrial development: multi-modal inputs (PRDs and Figma designs), a large-scale mixed Swift/Objective-C codebase, and comprehensive test suites. We evaluate 22 agent-model configurations across four coding agents -- three commercial (Cursor, Codex, Claude Code) and one open-source (OpenCode) -- and find that even the best configurations achieve only 12\\% task success rate. Our analysis reveals that (1) agent design matters as much as model capability -- the same model shows up to 6$\\times$ performance gap across agents, (2) commercial agents consistently outperform open-source alternatives, and (3) simple ``Defensive Programming'' prompts outperform complex ones by 7.4\\%. These findings highlight a significant gap between current agent capabilities and industrial requirements, while providing actionable insights for practitioners and researchers. We release SWE-Bench Mobile as a \\textit{hosted benchmark challenge} to prevent data contamination and ensure fair evaluation. The public leaderboard and development toolkit are available at https://swebenchmobile.com.", "AI": {"tldr": "SWE-Bench Mobile\u662f\u4e00\u4e2a\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9eiOS\u4ee3\u7801\u5e93\u4e0a\u5f00\u53d1\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6700\u4f73\u914d\u7f6e\u4ec5\u8fbe\u523012%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u8868\u660e\u73b0\u6709\u4ee3\u7406\u4e0e\u5de5\u4e1a\u7ea7\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u95ee\u9898\u6216bug\u4fee\u590d\uff0c\u65e0\u6cd5\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u5728\u771f\u5b9e\u5de5\u4e1a\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9762\u5bf9\u591a\u6a21\u6001\u8f93\u5165\u3001\u5927\u89c4\u6a21\u6df7\u5408\u4ee3\u7801\u5e93\u548c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\u7684\u590d\u6742\u573a\u666f\u3002", "method": "\u521b\u5efaSWE-Bench Mobile\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u6765\u81ea\u751f\u4ea7iOS\u4ee3\u7801\u5e93\u7684\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff0c\u652f\u6301\u591a\u6a21\u6001\u8f93\u5165\uff08PRD\u548cFigma\u8bbe\u8ba1\uff09\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u6df7\u5408Swift/Objective-C\u4ee3\u7801\u5e93\u548c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6\u3002\u8bc4\u4f30\u4e8622\u79cd\u4ee3\u7406-\u6a21\u578b\u914d\u7f6e\uff0c\u6db5\u76d6\u4e09\u79cd\u5546\u4e1a\u4ee3\u7406\uff08Cursor\u3001Codex\u3001Claude Code\uff09\u548c\u4e00\u79cd\u5f00\u6e90\u4ee3\u7406\uff08OpenCode\uff09\u3002", "result": "\u6700\u4f73\u914d\u7f6e\u4ec5\u8fbe\u523012%\u4efb\u52a1\u6210\u529f\u7387\uff1b\u53d1\u73b0\u4ee3\u7406\u8bbe\u8ba1\u91cd\u8981\u6027\u4e0d\u4e9a\u4e8e\u6a21\u578b\u80fd\u529b\uff08\u76f8\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4ee3\u7406\u4e0a\u6027\u80fd\u5dee\u8ddd\u53ef\u8fbe6\u500d\uff09\uff1b\u5546\u4e1a\u4ee3\u7406\u59cb\u7ec8\u4f18\u4e8e\u5f00\u6e90\u66ff\u4ee3\u54c1\uff1b\u7b80\u5355\u7684\"\u9632\u5fa1\u6027\u7f16\u7a0b\"\u63d0\u793a\u6bd4\u590d\u6742\u63d0\u793a\u6027\u80fd\u9ad87.4%\u3002", "conclusion": "\u5f53\u524d\u7f16\u7801\u4ee3\u7406\u80fd\u529b\u4e0e\u5de5\u4e1a\u7ea7\u79fb\u52a8\u5e94\u7528\u5f00\u53d1\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4f46\u7814\u7a76\u4e3a\u4ece\u4e1a\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002\u901a\u8fc7\u53d1\u5e03\u6258\u7ba1\u57fa\u51c6\u6311\u6218\u6765\u9632\u6b62\u6570\u636e\u6c61\u67d3\u5e76\u786e\u4fdd\u516c\u5e73\u8bc4\u4f30\u3002"}}
{"id": "2602.09846", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.09846", "abs": "https://arxiv.org/abs/2602.09846", "authors": ["Malik Abdul Sami", "Zeeshan Rasheed", "Meri Olenius", "Muhammad Waseem", "Kai-Kristian Kemell", "Jussi Rasku", "Pekka Abrahamsson"], "title": "Generative AI Adoption in an Energy Company: Exploring Challenges and Use Cases", "comment": null, "summary": "Organisations are examining how generative AI can support their operational work and decision-making processes. This study investigates how employees in a energy company understand AI adoption and identify areas where AI and LLMs-based agentic workflows could assist daily activities. Data was collected in four weeks through sixteen semi-structured interviews across nine departments, supported by internal documents and researcher observations. The analysis identified areas where employees positioned AI as useful, including reporting work, forecasting, data handling, maintenance-related tasks, and anomaly detection. Participants also described how GenAI and LLM-based tools could be introduced through incremental steps that align with existing workflows. The study provides an overview view of AI adoption in the energy sector and offers a structured basis for identifying entry points for practical implementation and comparative research across industries.", "AI": {"tldr": "\u80fd\u6e90\u516c\u53f8\u5458\u5de5\u8bbf\u8c08\u7814\u7a76\uff0c\u63a2\u8ba8AI\u91c7\u7528\u7406\u89e3\u53caLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u5728\u65e5\u5e38\u5de5\u4f5c\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u9886\u57df", "motivation": "\u7ec4\u7ec7\u6b63\u5728\u63a2\u7d22\u751f\u6210\u5f0fAI\u5982\u4f55\u652f\u6301\u8fd0\u8425\u5de5\u4f5c\u548c\u51b3\u7b56\u8fc7\u7a0b\uff0c\u9700\u8981\u4e86\u89e3\u5458\u5de5\u5bf9AI\u91c7\u7528\u7684\u7406\u89e3\u4ee5\u53ca\u8bc6\u522bAI\u548cLLM\u4ee3\u7406\u5de5\u4f5c\u6d41\u53ef\u534f\u52a9\u65e5\u5e38\u6d3b\u52a8\u7684\u9886\u57df", "method": "\u5728\u80fd\u6e90\u516c\u53f8\u8fdb\u884c\u4e3a\u671f\u56db\u5468\u7684\u7814\u7a76\uff0c\u901a\u8fc716\u4e2a\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u8986\u76d69\u4e2a\u90e8\u95e8\uff0c\u8f85\u4ee5\u5185\u90e8\u6587\u4ef6\u548c\u7814\u7a76\u8005\u89c2\u5bdf", "result": "\u8bc6\u522b\u51fa\u5458\u5de5\u8ba4\u4e3aAI\u6709\u7528\u7684\u9886\u57df\uff1a\u62a5\u544a\u5de5\u4f5c\u3001\u9884\u6d4b\u3001\u6570\u636e\u5904\u7406\u3001\u7ef4\u62a4\u76f8\u5173\u4efb\u52a1\u548c\u5f02\u5e38\u68c0\u6d4b\uff1b\u53c2\u4e0e\u8005\u63cf\u8ff0\u4e86\u5982\u4f55\u901a\u8fc7\u589e\u91cf\u6b65\u9aa4\u5f15\u5165GenAI\u548cLLM\u5de5\u5177\u4ee5\u7b26\u5408\u73b0\u6709\u5de5\u4f5c\u6d41\u7a0b", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u80fd\u6e90\u884c\u4e1aAI\u91c7\u7528\u6982\u51b5\uff0c\u4e3a\u5b9e\u9645\u5b9e\u65bd\u7684\u5207\u5165\u70b9\u8bc6\u522b\u548c\u8de8\u884c\u4e1a\u6bd4\u8f83\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840"}}
{"id": "2602.09892", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09892", "abs": "https://arxiv.org/abs/2602.09892", "authors": ["Jiale Zhao", "Guoxin Chen", "Fanzhe Meng", "Minghao Li", "Jie Chen", "Hui Xu", "Yongshuai Sun", "Xin Zhao", "Ruihua Song", "Yuan Zhang", "Peng Wang", "Cheng Chen", "Jirong Wen", "Kai Jia"], "title": "Immersion in the GitHub Universe: Scaling Coding Agents to Mastery", "comment": null, "summary": "Achieving mastery in real world software engineering tasks is fundamentally bottlenecked by the scarcity of large scale, high quality training data. Scaling such data has been limited by the complexity of environment setup, unit test generation, and problem statement curation. In this paper, we propose ScaleSWE, an automated, sandboxed multi agent workflow designed to construct high quality SWE data at scale. The system coordinates three specialized agents for environment setup, test creation, and problem description synthesis to process 6 million pull requests across 5200 repositories, producing Scale SWE Data: 100k verified SWE instances, the largest such dataset to date. It substantially surpasses existing real world datasets in repository diversity and reflects realistic task complexity. We further demonstrate the dataset utility for training by distilling 71498 high quality trajectories and finetuning Qwen30BA3BInstruct to produce ScaleSWE Agent. Our agent achieves a 64 resolve rate on SWE Bench Verified a nearly three fold improvement over the base model. ScaleSWE provides a scalable, reproducible approach for data construction to advance LLM based software engineering. Scale SWE will be publicly available.", "AI": {"tldr": "ScaleSWE\uff1a\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u6c99\u7bb1\u5316\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6784\u5efa\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u5de5\u7a0b\u6570\u636e\uff0c\u4ece600\u4e07\u4e2aPR\u4e2d\u751f\u621010\u4e07\u4e2a\u5df2\u9a8c\u8bc1\u5b9e\u4f8b\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684\u771f\u5b9e\u4e16\u754cSWE\u6570\u636e\u96c6", "motivation": "\u73b0\u5b9e\u4e16\u754c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u638c\u63e1\u53d7\u5230\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u6839\u672c\u6027\u74f6\u9888\u9650\u5236\uff0c\u73b0\u6709\u6570\u636e\u6269\u5c55\u53d7\u5230\u73af\u5883\u8bbe\u7f6e\u3001\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u548c\u95ee\u9898\u63cf\u8ff0\u6784\u5efa\u590d\u6742\u6027\u7684\u9650\u5236", "method": "\u63d0\u51faScaleSWE\u7cfb\u7edf\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u6c99\u7bb1\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u534f\u8c03\u4e09\u4e2a\u4e13\u95e8\u667a\u80fd\u4f53\uff1a\u73af\u5883\u8bbe\u7f6e\u3001\u6d4b\u8bd5\u521b\u5efa\u548c\u95ee\u9898\u63cf\u8ff0\u5408\u6210\uff0c\u5904\u74065200\u4e2a\u4ed3\u5e93\u4e2d\u7684600\u4e07\u4e2a\u62c9\u53d6\u8bf7\u6c42", "result": "\u751f\u6210Scale SWE Data\uff1a10\u4e07\u4e2a\u5df2\u9a8c\u8bc1\u7684SWE\u5b9e\u4f8b\uff0c\u662f\u76ee\u524d\u6700\u5927\u7684\u6b64\u7c7b\u6570\u636e\u96c6\uff1b\u901a\u8fc7\u5fae\u8c03Qwen30BA3BInstruct\u521b\u5efaScaleSWE Agent\uff0c\u5728SWE Bench Verified\u4e0a\u8fbe\u523064%\u89e3\u51b3\u7387\uff0c\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u5347\u8fd1\u4e09\u500d", "conclusion": "ScaleSWE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u6570\u636e\u6784\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a8\u8fdb\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u8be5\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53ef\u7528"}}
{"id": "2602.09921", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09921", "abs": "https://arxiv.org/abs/2602.09921", "authors": ["Everaldo Silva J\u00fanior", "Lina Marsso", "Ricardo Caldas", "Marsha Chechik", "Gena\u00edna Nunes Rodrigues"], "title": "Operationalizing Human Values in the Requirements Engineering Process of Ethics-Aware Autonomous Systems", "comment": null, "summary": "Operationalizing human values alongside functional and adaptation requirements remains challenging due to their ambiguous, pluralistic, and context-dependent nature. Explicit representations are needed to support the elicitation, analysis, and negotiation of value conflicts beyond traditional software engineering abstractions. In this work, we propose a requirements engineering approach for ethics-aware autonomous systems that captures human values as normative goals and aligns them with functional and adaptation goals. These goals are systematically operationalized into Social, Legal, Ethical, Empathetic, and Cultural (SLEEC) requirements, enabling automated well-formedness checking, conflict detection, and early design-time negotiation. We demonstrate the feasibility of the approach through a medical Body Sensor Network case study.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u4ef7\u503c\u4f5c\u4e3a\u89c4\u8303\u6027\u76ee\u6807\u4e0e\u529f\u80fd\u548c\u9002\u5e94\u76ee\u6807\u5bf9\u9f50\uff0c\u5e76\u7cfb\u7edf\u5316\u4e3aSLEEC\u9700\u6c42\uff0c\u652f\u6301\u81ea\u52a8\u5316\u68c0\u67e5\u548c\u51b2\u7a81\u68c0\u6d4b", "motivation": "\u4eba\u7c7b\u4ef7\u503c\u5177\u6709\u6a21\u7cca\u6027\u3001\u591a\u5143\u6027\u548c\u60c5\u5883\u4f9d\u8d56\u6027\uff0c\u96be\u4ee5\u4e0e\u529f\u80fd\u548c\u9002\u5e94\u9700\u6c42\u534f\u540c\u64cd\u4f5c\uff0c\u9700\u8981\u660e\u786e\u8868\u793a\u6765\u652f\u6301\u4ef7\u503c\u51b2\u7a81\u7684\u8bc6\u522b\u3001\u5206\u6790\u548c\u534f\u5546", "method": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u4f26\u7406\u611f\u77e5\u81ea\u4e3b\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u4ef7\u503c\u6355\u83b7\u4e3a\u89c4\u8303\u6027\u76ee\u6807\uff0c\u4e0e\u529f\u80fd\u548c\u9002\u5e94\u76ee\u6807\u5bf9\u9f50\uff0c\u5e76\u7cfb\u7edf\u5316\u4e3a\u793e\u4f1a\u3001\u6cd5\u5f8b\u3001\u4f26\u7406\u3001\u5171\u60c5\u548c\u6587\u5316(SLEEC)\u9700\u6c42\uff0c\u652f\u6301\u81ea\u52a8\u5316\u826f\u597d\u6027\u68c0\u67e5\u3001\u51b2\u7a81\u68c0\u6d4b\u548c\u65e9\u671f\u8bbe\u8ba1\u65f6\u534f\u5546", "result": "\u901a\u8fc7\u533b\u7597\u4f53\u611f\u7f51\u7edc\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u52a8\u5316\u826f\u597d\u6027\u68c0\u67e5\u3001\u51b2\u7a81\u68c0\u6d4b\u548c\u65e9\u671f\u8bbe\u8ba1\u534f\u5546", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u64cd\u4f5c\u5316\u4eba\u7c7b\u4ef7\u503c\uff0c\u8d85\u8d8a\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u62bd\u8c61\uff0c\u4e3a\u4f26\u7406\u611f\u77e5\u81ea\u4e3b\u7cfb\u7edf\u7684\u9700\u6c42\u5de5\u7a0b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2602.09930", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09930", "abs": "https://arxiv.org/abs/2602.09930", "authors": ["Nishil Amin", "Zhiwei Fei", "Xiang Li", "Justyna Petke", "He Ye"], "title": "JMigBench: A Benchmark for Evaluating LLMs on Source Code Migration (Java 8 to Java 11)", "comment": null, "summary": "We build a benchmark to evaluate large language models (LLMs) for source code migration tasks, specifically upgrading functions from Java 8 to Java 11. We first collected a dataset of function pairs from open-source repositories, but limitations in data quality led us to construct a refined dataset covering eight categories of deprecated APIs. Using this dataset, the Mistral Codestral model was evaluated with CodeBLEU and keyword-based metrics to measure lexical and semantic similarity as well as migration correctness. Results show that the evaluated model (Mistral Codestral) can handle trivial one-to-one API substitutions with moderate success, achieving identical migrations in 11.11% of the cases, but it struggles with more complex migrations such as CORBA or JAX-WS. These findings suggest Mistral Codestral can partially reduce developer effort by automating repetitive migration tasks but cannot yet replace humans within the scope of the JMigBench benchmark. The benchmark and analysis provide a foundation for future work on expanding datasets, refining prompting strategies, and improving migration performance across different LLMs.", "AI": {"tldr": "\u6784\u5efa\u4e86JMigBench\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLM\u5728Java 8\u523011\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Mistral Codestral\u80fd\u5904\u7406\u7b80\u5355API\u66ff\u6362\u4f46\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u8fc1\u79fb\u573a\u666f", "motivation": "\u9700\u8981\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u80fd\u529b\uff0c\u7279\u522b\u662fJava\u7248\u672c\u5347\u7ea7\u573a\u666f\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u81ea\u52a8\u5316\u8fc1\u79fb\u5de5\u5177\u7684\u6709\u6548\u6027\u8bc4\u4f30", "method": "1) \u4ece\u5f00\u6e90\u4ed3\u5e93\u6536\u96c6\u51fd\u6570\u5bf9\u6570\u636e\u96c6\uff1b2) \u6784\u5efa\u5305\u542b8\u7c7b\u5e9f\u5f03API\u7684\u7cbe\u7ec6\u5316\u6570\u636e\u96c6\uff1b3) \u4f7f\u7528Mistral Codestral\u6a21\u578b\u8bc4\u4f30\uff1b4) \u91c7\u7528CodeBLEU\u548c\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u6307\u6807\u8861\u91cf\u8bcd\u6c47\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u8fc1\u79fb\u6b63\u786e\u6027", "result": "Mistral Codestral\u80fd\u4e2d\u7b49\u7a0b\u5ea6\u5904\u7406\u7b80\u5355\u7684\u4e00\u5bf9\u4e00API\u66ff\u6362\uff0811.11%\u6848\u4f8b\u5b9e\u73b0\u5b8c\u5168\u76f8\u540c\u7684\u8fc1\u79fb\uff09\uff0c\u4f46\u5728CORBA\u3001JAX-WS\u7b49\u590d\u6742\u8fc1\u79fb\u573a\u666f\u8868\u73b0\u4e0d\u4f73", "conclusion": "\u8be5\u6a21\u578b\u80fd\u90e8\u5206\u51cf\u5c11\u5f00\u53d1\u8005\u7684\u91cd\u590d\u6027\u8fc1\u79fb\u5de5\u4f5c\uff0c\u4f46\u5c1a\u4e0d\u80fd\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\uff1b\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u672a\u6765\u6269\u5c55\u6570\u636e\u96c6\u3001\u4f18\u5316\u63d0\u793a\u7b56\u7565\u548c\u6539\u8fdbLLM\u8fc1\u79fb\u6027\u80fd\u63d0\u4f9b\u4e86\u57fa\u7840"}}
{"id": "2602.09942", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09942", "abs": "https://arxiv.org/abs/2602.09942", "authors": ["Junjie Luo", "Shangzhou Xia", "Fuyuan Zhang", "Jianjun Zhao"], "title": "QEMI: A Quantum Software Stacks Testing Framework via Equivalence Modulo Inputs", "comment": null, "summary": "As quantum algorithms and hardware continue to evolve, ensuring the correctness of the quantum software stack (QSS) has become increasingly important. However, testing QSSes remains challenging due to the oracle problem, i.e., the lack of a reliable ground truth for expected program behavior. Existing metamorphic testing approaches often rely on equivalent circuit transformations, backend modifications, or parameter tuning to address this issue. In this work, inspired by Equivalence Modulo Inputs (EMI), we propose Quantum EMI (QEMI), a new testing approach for QSSes. Our key contributions include: (1) a random quantum program generator that produces code with dead code based on quantum control-flow structures, and (2) an adaptation of the EMI technique from classical compiler testing to generate variants by removing dead code. By comparing the behavior of these variants, we can detect potential bugs in QSS implementations. We applied QEMI to Qiskit, Q#, and Cirq, and successfully identified 11 crash bugs and 1 behavioral inconsistency. QEMI expands the limited set of testing techniques available for quantum software stacks by going beyond structural transformations and incorporating semantics-preserving ones into quantum program analysis.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50EMI\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5305\u542b\u6b7b\u4ee3\u7801\u7684\u91cf\u5b50\u7a0b\u5e8f\u5e76\u79fb\u9664\u6b7b\u4ee3\u7801\u521b\u5efa\u53d8\u4f53\uff0c\u68c0\u6d4b\u91cf\u5b50\u8f6f\u4ef6\u6808\u4e2d\u7684\u9519\u8bef", "motivation": "\u91cf\u5b50\u8f6f\u4ef6\u6808\u6d4b\u8bd5\u9762\u4e34oracle\u95ee\u9898\uff08\u7f3a\u4e4f\u53ef\u9760\u7684\u771f\u503c\u57fa\u51c6\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7b49\u6548\u7535\u8def\u53d8\u6362\u3001\u540e\u7aef\u4fee\u6539\u6216\u53c2\u6570\u8c03\u4f18\uff0c\u9700\u8981\u65b0\u7684\u6d4b\u8bd5\u6280\u672f", "method": "\u57fa\u4e8eEMI\u601d\u60f3\u63d0\u51fa\u91cf\u5b50EMI\u65b9\u6cd5\uff1a1\uff09\u751f\u6210\u5305\u542b\u6b7b\u4ee3\u7801\u7684\u968f\u673a\u91cf\u5b50\u7a0b\u5e8f\uff1b2\uff09\u901a\u8fc7\u79fb\u9664\u6b7b\u4ee3\u7801\u521b\u5efa\u7a0b\u5e8f\u53d8\u4f53\uff1b3\uff09\u6bd4\u8f83\u53d8\u4f53\u884c\u4e3a\u68c0\u6d4b\u9519\u8bef", "result": "\u5728Qiskit\u3001Q#\u548cCirq\u4e09\u4e2a\u91cf\u5b50\u8f6f\u4ef6\u6808\u4e2d\u6210\u529f\u53d1\u73b011\u4e2a\u5d29\u6e83\u9519\u8bef\u548c1\u4e2a\u884c\u4e3a\u4e0d\u4e00\u81f4\u9519\u8bef", "conclusion": "QEMI\u6269\u5c55\u4e86\u91cf\u5b50\u8f6f\u4ef6\u6808\u6d4b\u8bd5\u6280\u672f\u96c6\uff0c\u8d85\u8d8a\u4e86\u7ed3\u6784\u53d8\u6362\uff0c\u5c06\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\u7eb3\u5165\u91cf\u5b50\u7a0b\u5e8f\u5206\u6790"}}
{"id": "2602.09944", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.09944", "abs": "https://arxiv.org/abs/2602.09944", "authors": ["Xiang Li", "Zhiwei Fei", "Ying Ma", "Jerry Zhang", "Sarro Federica", "He Ye"], "title": "Environment-in-the-Loop: Rethinking Code Migration with LLM-based Agents", "comment": null, "summary": "Modern software systems continuously undergo code upgrades to enhance functionality, security, and performance, and Large Language Models (LLMs) have demonstrated remarkable capabilities in code migration tasks. However, while research on automated code migration which including refactoring, API adaptation, and dependency updates has advanced rapidly, the exploration of the automated environment interaction that must accompany it remains relatively scarce. In practice, code and its environment are intricately intertwined. Relying solely on static analysis of the environment leads to an inadequate understanding of the target setting, prolongs feedback cycles, and consequently causes significant rework and project delays, thereby reducing overall efficiency. We contend that successful software evolution demands a holistic perspective that integrates both code and environment migration. To understand the current landscape and challenges, we first provide an overview of the status of automated environment construction. We then propose a novel framework paradigm that tightly integrates automated environment setup with the code migration workflow. Finally, we explore the challenges and future directions for automated environment interaction within the code migration domain. Our findings emphasize that without automated environment interaction, the automation of code migration is only half complete.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u4ee3\u7801\u8fc1\u79fb\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u672c\u8eab\uff0c\u800c\u5ffd\u89c6\u4e86\u73af\u5883\u4ea4\u4e92\u7684\u81ea\u52a8\u5316\uff0c\u8fd9\u5bfc\u81f4\u8fc1\u79fb\u6548\u7387\u4f4e\u4e0b\u3002\u4f5c\u8005\u63d0\u51fa\u9700\u8981\u5c06\u4ee3\u7801\u8fc1\u79fb\u4e0e\u73af\u5883\u6784\u5efa\u7d27\u5bc6\u7ed3\u5408\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u9700\u8981\u6301\u7eed\u5347\u7ea7\u4ee3\u7801\u4ee5\u589e\u5f3a\u529f\u80fd\u3001\u5b89\u5168\u6027\u548c\u6027\u80fd\uff0cLLM\u5728\u4ee3\u7801\u8fc1\u79fb\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u4ee3\u7801\u8fc1\u79fb\u672c\u8eab\uff08\u91cd\u6784\u3001API\u9002\u914d\u3001\u4f9d\u8d56\u66f4\u65b0\uff09\uff0c\u800c\u5bf9\u5fc5\u987b\u4f34\u968f\u7684\u73af\u5883\u4ea4\u4e92\u81ea\u52a8\u5316\u63a2\u7d22\u4e0d\u8db3\u3002\u4ee3\u7801\u4e0e\u73af\u5883\u7d27\u5bc6\u4ea4\u7ec7\uff0c\u4ec5\u4f9d\u8d56\u9759\u6001\u73af\u5883\u5206\u6790\u4f1a\u5bfc\u81f4\u5bf9\u76ee\u6807\u73af\u5883\u7406\u89e3\u4e0d\u8db3\u3001\u53cd\u9988\u5468\u671f\u5ef6\u957f\uff0c\u4ece\u800c\u9020\u6210\u5927\u91cf\u8fd4\u5de5\u548c\u9879\u76ee\u5ef6\u8fdf\uff0c\u964d\u4f4e\u6574\u4f53\u6548\u7387\u3002", "method": "\u9996\u5148\u6982\u8ff0\u4e86\u81ea\u52a8\u5316\u73af\u5883\u6784\u5efa\u7684\u73b0\u72b6\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u81ea\u52a8\u5316\u73af\u5883\u8bbe\u7f6e\u4e0e\u4ee3\u7801\u8fc1\u79fb\u5de5\u4f5c\u6d41\u7d27\u5bc6\u7ed3\u5408\u7684\u65b0\u6846\u67b6\u8303\u5f0f\uff0c\u6700\u540e\u63a2\u8ba8\u4e86\u4ee3\u7801\u8fc1\u79fb\u9886\u57df\u4e2d\u81ea\u52a8\u5316\u73af\u5883\u4ea4\u4e92\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6ca1\u6709\u81ea\u52a8\u5316\u73af\u5883\u4ea4\u4e92\uff0c\u4ee3\u7801\u8fc1\u79fb\u7684\u81ea\u52a8\u5316\u53ea\u5b8c\u6210\u4e86\u4e00\u534a\u3002\u6210\u529f\u7684\u8f6f\u4ef6\u6f14\u8fdb\u9700\u8981\u91c7\u7528\u6574\u5408\u4ee3\u7801\u548c\u73af\u5883\u8fc1\u79fb\u7684\u6574\u4f53\u89c6\u89d2\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u8f6f\u4ef6\u6f14\u8fdb\u9700\u8981\u4ee3\u7801\u8fc1\u79fb\u4e0e\u73af\u5883\u4ea4\u4e92\u81ea\u52a8\u5316\u7684\u7d27\u5bc6\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u6846\u67b6\u8303\u5f0f\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u6311\u6218\u3002"}}
{"id": "2602.10046", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.10046", "abs": "https://arxiv.org/abs/2602.10046", "authors": ["Doehyun Baek", "Michael Pradel"], "title": "Artisan: Agentic Artifact Evaluation", "comment": null, "summary": "Artifact evaluation has become standard practice in the software engineering community to ensure the reproducibility of research results. However, the current manual process is labor-intensive, and hence, done only as a one-time assessment for a subset of all papers. To support the artifact evaluation effort, we present Artisan, an automated LLM agent for reproducing research results given a paper and its artifact. The approach is enabled by two key contributions: First, we frame the reproduction problem as a code generation task where the goal is to generate a reproduction script that, when executed, reproduces the results reported in a paper. Unlike prior work on automatically reproducing research results in other domains, this formulation allows for running the script independently of the agent and for assessing the reproduction process at a fine-grained level. Second, we design automated judging mechanism that guides the agent toward the expected results without revealing them and that prevent trivial solutions, such as simply copying checked-in results. To evaluate Artisan, we introduce Artisan-Bench, the first benchmark assessing the ability to generate reproduction scripts and the first benchmark for automated artifact evaluation in software engineering. Artisan-Bench comprises 60 tasks derived from 23 software engineering papers, covering different research areas and programming languages. We validate all tasks in Artisan-Bench for reproducibility to ensure that the tasks are feasible. Our experiments show that Artisan is effective, producing 44/60 reproduction scripts and outperforming the best available baseline, a vanilla LLM agent (mini-swe-agent), by 3.14$\\times$ in terms of reproduction scripts generated while taking $0.45 and 48 minutes, on average per task. Artisan also helped uncover 20 new errors in either the paper or artifact.", "AI": {"tldr": "Artisan\uff1a\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u4ee3\u7406\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u590d\u73b0\u7814\u7a76\u7ed3\u679c\u7684\u811a\u672c\uff0c\u76f8\u6bd4\u57fa\u7ebf\u63d0\u53473.14\u500d\u6548\u679c\uff0c\u6210\u672c\u4ec50.45\u7f8e\u5143/\u4efb\u52a1", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u5de5\u4ef6\u8bc4\u4f30\u4f9d\u8d56\u4eba\u5de5\uff0c\u8017\u65f6\u8d39\u529b\u4e14\u53ea\u80fd\u8986\u76d6\u90e8\u5206\u8bba\u6587\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u652f\u6301\u7814\u7a76\u7ed3\u679c\u7684\u53ef\u590d\u73b0\u6027\u9a8c\u8bc1", "method": "\u5c06\u590d\u73b0\u95ee\u9898\u6784\u5efa\u4e3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u8bbe\u8ba1\u81ea\u52a8\u5316\u5224\u65ad\u673a\u5236\u9632\u6b62\u7b80\u5355\u590d\u5236\u7ed3\u679c\uff0c\u521b\u5efaArtisan-Bench\u57fa\u51c6\u5305\u542b60\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1", "result": "Artisan\u572860\u4e2a\u4efb\u52a1\u4e2d\u6210\u529f\u751f\u621044\u4e2a\u590d\u73b0\u811a\u672c\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf(mini-swe-agent)\u63d0\u53473.14\u500d\uff0c\u5e73\u5747\u6210\u672c0.45\u7f8e\u5143/48\u5206\u949f\uff0c\u53d1\u73b020\u4e2a\u65b0\u9519\u8bef", "conclusion": "Artisan\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u7814\u7a76\u7ed3\u679c\u590d\u73b0\u7684\u53ef\u884c\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u8bba\u6587\u6216\u5de5\u4ef6\u4e2d\u7684\u9690\u85cf\u9519\u8bef"}}
