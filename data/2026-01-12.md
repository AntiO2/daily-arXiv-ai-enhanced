<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [DafnyPro: LLM-Assisted Automated Verification for Dafny Programs](https://arxiv.org/abs/2601.05385)
*Debangshu Banerjee,Olivier Bouissou,Stefan Zetzsche*

Main category: cs.SE

TL;DR: DafnyPro是一个推理时框架，通过三个组件增强LLM生成Dafny验证注释的能力，在多个基准测试中显著提升证明成功率，并使小模型也能保持高验证准确率。


<details>
  <summary>Details</summary>
Motivation: Dafny是一种支持形式化验证的编程语言，但编写验证注释（如前置条件、后置条件、循环不变量）对开发者来说具有挑战性。现有的LLM在生成Dafny验证注释时存在准确率不足的问题，需要专门的框架来提升其验证能力。

Method: DafnyPro包含三个关键组件：1) diff-checker防止修改基础程序逻辑；2) pruner移除不必要的循环不变量；3) hint-augmentation系统检索并应用预定义的问题无关证明策略。框架还通过大模型增强后的训练数据微调小模型。

Result: 在四个基准测试（Clover、MBPP-Dafny、HumanEval-Dafny、DafnyBench）上均获得性能提升。在最具挑战性的DafnyBench上，Claude Sonnet 3.5结合DafnyPro达到86%的正确证明率，比基础模型提升16个百分点。微调的7B和14B Qwen模型分别达到68%和70%的正确证明率。

Conclusion: DafnyPro能有效提升LLM生成Dafny验证注释的能力，使大模型在形式化验证任务上表现更好，同时通过知识蒸馏使小模型也能保持较高的验证准确率，为自动化形式化验证提供了实用解决方案。

Abstract: We present DafnyPro, an inference-time framework that enhances LLMs for generating verification annotations in Dafny. DafnyPro comprises three key components: a diff-checker that prevents modifications to base program logic, a pruner that removes unnecessary invariants, and a hint-augmentation system that retrieves and applies predefined, problem-independent proof strategies. We evaluate DafnyPro using Claude Sonnet 3.5 and 3.7 on four benchmarks: Clover, MBPP-Dafny, HumanEval-Dafny, and DafnyBench, achieving consistent performance gains in all cases. Notably, on DafnyBench, the most challenging benchmark, Claude Sonnet 3.5 enhanced with DafnyPro achieves 86% correct proofs, a 16 pp improvement over the base model. We also fine-tune two Qwen models on training data derived from verification attempts by larger models enhanced with DafnyPro. Our 7B and 14B models achieve 68% and 70% correct proofs on DafnyBench, respectively, demonstrating that smaller models can maintain high verification accuracy.

</details>


### [2] [Uncovering Failures in Cyber-Physical System State Transitions: A Fuzzing-Based Approach Applied to sUAS](https://arxiv.org/abs/2601.05449)
*Theodore Chambers,Arturo Miguel Russell Bernal,Michael Vierhauser,Jane Cleland-Huang*

Main category: cs.SE

TL;DR: SaFUZZ是一个状态感知的模糊测试管道，用于验证小型无人机系统在各种条件下的核心行为，通过动态生成故障树来可视化导致故障的状态、模式和环境因素。


<details>
  <summary>Details</summary>
Motivation: 随着小型无人机在多样化且安全关键环境中的部署增加，需要在各种条件下严格验证其机载决策逻辑，确保系统可靠性。

Method: 创建模糊测试规范来检测行为偏差，动态生成关联的故障树以可视化导致故障的状态、模式和环境因素，在高保真仿真环境中进行测试。

Result: 在真实世界无人机系统中验证了SaFUZZ，发现了开发团队之前未检测到的多个故障点，并在物理无人机上进行了实地测试验证。

Conclusion: SaFUZZ能够提供实用且可扩展的方法来发现真实世界无人机应用中的多样化状态转换故障，有助于项目利益相关者分析故障并确定根本原因。

Abstract: The increasing deployment of small Uncrewed Aerial Systems (sUAS) in diverse and often safety-critical environments demands rigorous validation of onboard decision logic under various conditions. In this paper, we present SaFUZZ, a state-aware fuzzing pipeline that validates core behavior associated with state transitions, automated failsafes, and human operator interactions in sUAS applications operating under various timing conditions and environmental disturbances. We create fuzzing specifications to detect behavioral deviations, and then dynamically generate associated Fault Trees to visualize states, modes, and environmental factors that contribute to the failure, thereby helping project stakeholders to analyze the failure and identify its root causes. We validated SaFUZZ against a real-world sUAS system and were able to identify several points of failure not previously detected by the system's development team. The fuzzing was conducted in a high-fidelity simulation environment, and outcomes were validated on physical sUAS in a real-world field testing setting. The findings from the study demonstrated SaFUZZ's ability to provide a practical and scalable approach to uncovering diverse state transition failures in a real-world sUAS application.

</details>


### [3] [Rethinking Basis Path Testing: Mixed Integer Programming Approach for Test Path Set Generation](https://arxiv.org/abs/2601.05463)
*Chao Wei,Xinyi Peng,Yawen Yan,Mao Luo,Ting Cai*

Main category: cs.SE

TL;DR: 将基础路径生成从过程式搜索重构为声明式优化问题，使用混合整数规划生成结构简单的最优路径集


<details>
  <summary>Details</summary>
Motivation: 传统基于贪心图遍历算法（如DFS/BFS）的基础路径测试方法生成次优路径，这种结构上的不足直接阻碍下游测试活动，增加测试数据生成的复杂性和工程师的认知负担

Method: 提出混合整数规划框架，包含两种策略：1）整体MIP模型保证理论最优路径集；2）可扩展的增量MIP策略处理大型复杂拓扑，采用多目标函数优先考虑路径简单性并引入新颖性惩罚以最大化线性独立路径生成

Result: 在真实代码和大规模合成控制流图上的实证评估表明，增量MIP策略在生成完整基础集方面达到100%成功率，同时保持计算效率

Conclusion: 该工作为生成高质量结构"脚手架"提供了基础方法，能够增强后续测试生成工作的效率和效果

Abstract: Basis path testing is a cornerstone of structural testing, yet traditional automated methods, relying on greedy graph-traversal algorithms (e.g., DFS/BFS), often generate sub-optimal paths. This structural inferiority is not a trivial issue; it directly impedes downstream testing activities by complicating automated test data generation and increasing the cognitive load for human engineers. This paper reframes basis path generation from a procedural search task into a declarative optimization problem. We introduce a Mixed Integer Programming (MIP) framework designed to produce a complete basis path set that is globally optimal in its structural simplicity. Our framework includes two complementary strategies: a Holistic MIP model that guarantees a theoretically optimal path set, and a scalable Incremental MIP strategy for large, complex topologies. The incremental approach features a multi-objective function that prioritizes path simplicity and incorporates a novelty penalty to maximize the successful generation of linearly independent paths. Empirical evaluations on both real-code and large-scale synthetic Control Flow Graphs demonstrate that our Incremental MIP strategy achieves a 100\% success rate in generating complete basis sets, while remaining computationally efficient. Our work provides a foundational method for generating a high-quality structural "scaffold" that can enhance the efficiency and effectiveness of subsequent test generation efforts.

</details>


### [4] [STELP: Secure Transpilation and Execution of LLM-Generated Programs](https://arxiv.org/abs/2601.05467)
*Swapnil Shinde,Sahil Wadhwa,Andy Luo,Emily Chen*

Main category: cs.SE

TL;DR: 提出STELP系统，用于安全执行LLM生成的代码，解决生产环境中LLM代码的安全性和可靠性问题


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成方面取得重大进展，但直接将LLM生成的代码用于生产系统存在安全隐患（如漏洞、恶意攻击、幻觉等），而传统安全测试方法和人工审查在实际应用中不可行或不值得信任

Method: 提出STELP（Secure Transpiler and Executor of LLM-Generated Program）系统，能够在受控和安全的环境中执行LLM生成的代码，包括无头代码生成执行和实时执行代码片段等应用场景

Result: 贡献了人工验证的不安全代码片段数据集，在公开数据集上评估了正确性、安全性和延迟，结果表明该方法显著优于现有方法，特别是在安全执行高风险代码片段方面

Conclusion: STELP填补了传统安全测试方法和人工监督的不足，能够安全执行LLM生成的代码，为涉及代码生成的自主生产AI系统提供安全保障

Abstract: Rapid evolution of Large Language Models (LLMs) has achieved major advances in reasoning, planning, and function-calling capabilities. Multi-agentic collaborative frameworks using such LLMs place them at the center of solving software development-related tasks such as code generation. However, direct use of LLM generated code in production software development systems is problematic. The code could be unstable or erroneous and contain vulnerabilities such as data poisoning, malicious attacks, and hallucinations that could lead to widespread system malfunctions. This prohibits the adoption of LLM generated code in production AI systems where human code reviews and traditional secure testing tools are impractical or untrustworthy. In this paper, we discuss safety and reliability problems with the execution of LLM generated code and propose a Secure Transpiler and Executor of LLM-Generated Program (STELP), capable of executing LLM-generated code in a controlled and safe manner. STELP secures autonomous production AI systems involving code generation, filling the critical void left by the impracticality or limitations of traditional secure testing methodologies and human oversight. This includes applications such as headless code generation-execution and LLMs that produce executable code snippets as an action plan to be executed in real time. We contribute a human-validated dataset of insecure code snippets and benchmark our approach on publicly available datasets for correctness, safety, and latency. Our results demonstrate that our approach outperforms an existing method by a significant margin, particularly in its ability to safely execute risky code snippets. Warning: This paper contains malicious code snippets that should be run with caution.

</details>


### [5] [Readability-Robust Code Summarization via Meta Curriculum Learning](https://arxiv.org/abs/2601.05485)
*Wenhao Zeng,Yitian Chai,Hao Zhou,Fandong Meng,Jie Zhou,Xiaodong Gu*

Main category: cs.SE

TL;DR: 提出RoFTCodeSum方法，通过课程学习与元学习结合增强代码摘要模型对低可读性代码的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有代码摘要模型和基准测试仅限于高可读性代码，而现实世界代码往往结构混乱或经过混淆，导致模型性能显著下降

Method: 提出RoFTCodeSum方法，结合课程学习和元学习：基于原始微调数据集创建渐进难度的课程训练集（如混淆函数名和标识符），在每个训练步骤中使用这些渐进挑战性数据集进行元梯度更新

Result: 实验表明RoFTCodeSum在增强对语义扰动的鲁棒性的同时，提高了原始代码上的性能

Conclusion: RoFTCodeSum方法有效提升了代码摘要模型对低可读性代码的鲁棒性，解决了现实世界中代码可读性差的问题

Abstract: Code summarization has emerged as a fundamental technique in the field of program comprehension. While code language models have shown significant advancements, the current models and benchmarks are confined to high-readability code, which contains sufficient semantic cues such as function and variable names. In the real world, however, code is often poorly structured or obfuscated, significantly degrading model performance. In this paper, we first empirically evaluate the robustness of state-of-the-art language models on poor-readability code for the task of code summarization, focusing on (1) their effectiveness, (2) the impact of prompt engineering, and (3) the robustness of different variants. Experimental results reveal that state-of-the-art models-including GPT-4o and DeepSeek-V3 experience a substantial performance drop when faced with poorly readable code, and that prompt engineering and reasoning-enhanced models offer limited improvements. Motivated by these findings, we propose RoFTCodeSum, a novel fine-tuning method that enhances the robustness of code summarization against poorly readable code. RoFTCodeSum marries the concepts of curriculum learning and meta-learning: based on the original dataset for fine-tuning, it creates curricular training sets, e.g., obfuscating function names and identifiers from the code, respectively, that have progressive difficulty in code comprehension. In each training step, the approach meta-updates the gradients using these progressively challenging datasets, thereby optimizing both accuracy and readability robustness simultaneously. Experimental results demonstrate that RoFTCodeSum exhibits increased robustness against semantic perturbation while enhancing performance on the original code.

</details>


### [6] [Evaluating the Use of LLMs for Automated DOM-Level Resolution of Web Performance Issues](https://arxiv.org/abs/2601.05502)
*Gideon Peters,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 该研究评估了9种先进大语言模型在自动化解决网页性能问题方面的效果，发现LLMs在SEO和可访问性问题表现出色，但在性能关键的DOM操作方面效果参差不齐，GPT-4.1表现最佳而GPT-4o-mini表现最差。


<details>
  <summary>Details</summary>
Motivation: 用户对网页体验要求高，但开发者在时间限制下难以满足期望。性能优化是耗时的手动过程，DOM修改尤其复杂。大语言模型的发展为自动化解决这些问题提供了可能。

Method: 从15个流行网页提取DOM树，使用Lighthouse获取性能审计报告，将DOM树和审计结果输入9种LLMs进行分析。研究涵盖7个独特的审计类别。

Result: LLMs在SEO和可访问性问题普遍表现出色。在性能关键的DOM操作方面，GPT-4.1在初始加载、交互性和网络优化方面实现46.52%-48.68%的审计发生率降低，但GPT-4o-mini表现不佳。修改策略主要是添加和位置变更，对视觉稳定性有负面影响。

Conclusion: 大语言模型在自动化网页性能优化方面有潜力，特别是在SEO和可访问性领域，但在性能关键的DOM操作方面需要进一步改进，不同模型表现差异显著。

Abstract: Users demand fast, seamless webpage experiences, yet developers often struggle to meet these expectations within tight constraints. Performance optimization, while critical, is a time-consuming and often manual process. One of the most complex tasks in this domain is modifying the Document Object Model (DOM), which is why this study focuses on it. Recent advances in Large Language Models (LLMs) offer a promising avenue to automate this complex task, potentially transforming how developers address web performance issues. This study evaluates the effectiveness of nine state-of-the-art LLMs for automated web performance issue resolution. For this purpose, we first extracted the DOM trees of 15 popular webpages (e.g., Facebook), and then we used Lighthouse to retrieve their performance audit reports. Subsequently, we passed the extracted DOM trees and corresponding audits to each model for resolution. Our study considers 7 unique audit categories, revealing that LLMs universally excel at SEO & Accessibility issues. However, their efficacy in performance-critical DOM manipulations is mixed. While high-performing models like GPT-4.1 delivered significant reductions in areas like Initial Load, Interactivity, and Network Optimization (e.g., 46.52% to 48.68% audit incidence reductions), others, such as GPT-4o-mini, notably underperformed, consistently. A further analysis of these modifications showed a predominant additive strategy and frequent positional changes, alongside regressions particularly impacting Visual Stability.

</details>


### [7] [LIDL: LLM Integration Defect Localization via Knowledge Graph-Enhanced Multi-Agent Analysis](https://arxiv.org/abs/2601.05539)
*Gou Tan,Zilong He,Min Li,Pengfei Chen,Jieke Shi,Zhensu Sun,Ting Zhang,Danwen Chen,Lwin Khin Shar,Chuanfu Zhang,David Lo*

Main category: cs.SE

TL;DR: LIDL是一个用于定位LLM集成软件缺陷的多智能体框架，通过构建代码知识图谱、融合错误证据和上下文验证，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: LLM集成软件具有概率性和上下文依赖行为，与传统软件不同，引入了新的集成缺陷类型。现有缺陷定位技术无法有效识别这些LLM特定缺陷，因为它们无法捕捉异构工件间的跨层依赖关系，不能利用不完整或误导性的错误跟踪，也缺乏语义推理能力来识别根本原因。

Method: LIDL采用多智能体框架，包含三个核心组件：(1) 构建带有LLM感知注释的代码知识图谱，表示源代码、提示词和配置文件之间的交互边界；(2) 融合LLM推断的三种互补错误证据来源，筛选候选缺陷位置；(3) 应用上下文感知验证，使用反事实推理区分真实根本原因和传播症状。

Result: 在从105个GitHub仓库和16个基于智能体的系统中收集的146个真实缺陷实例上评估LIDL。结果显示LIDL在所有指标上显著优于五个最先进的基线方法，Top-3准确率达到0.64，MAP为0.48，比最佳基线提高了64.1%。同时，LIDL将成本降低了92.5%，实现了高准确性和成本效率。

Conclusion: LIDL通过多智能体框架有效解决了LLM集成软件中的缺陷定位问题，能够捕捉跨层依赖关系、利用不完整错误跟踪并进行语义推理，为LLM软件的质量保证提供了实用解决方案。

Abstract: LLM-integrated software, which embeds or interacts with large language models (LLMs) as functional components, exhibits probabilistic and context-dependent behaviors that fundamentally differ from those of traditional software. This shift introduces a new category of integration defects that arise not only from code errors but also from misaligned interactions among LLM-specific artifacts, including prompts, API calls, configurations, and model outputs. However, existing defect localization techniques are ineffective at identifying these LLM-specific integration defects because they fail to capture cross-layer dependencies across heterogeneous artifacts, cannot exploit incomplete or misleading error traces, and lack semantic reasoning capabilities for identifying root causes.
  To address these challenges, we propose LIDL, a multi-agent framework for defect localization in LLM-integrated software. LIDL (1) constructs a code knowledge graph enriched with LLM-aware annotations that represent interaction boundaries across source code, prompts, and configuration files, (2) fuses three complementary sources of error evidence inferred by LLMs to surface candidate defect locations, and (3) applies context-aware validation that uses counterfactual reasoning to distinguish true root causes from propagated symptoms. We evaluate LIDL on 146 real-world defect instances collected from 105 GitHub repositories and 16 agent-based systems. The results show that LIDL significantly outperforms five state-of-the-art baselines across all metrics, achieving a Top-3 accuracy of 0.64 and a MAP of 0.48, which represents a 64.1% improvement over the best-performing baseline. Notably, LIDL achieves these gains while reducing cost by 92.5%, demonstrating both high accuracy and cost efficiency.

</details>


### [8] [Empirical Characterization of Logging Smells in Machine Learning Code](https://arxiv.org/abs/2601.05540)
*Patrick Loic Foalem,Leuson Da Silva,Foutse Khomh,Ettore Merlo,Heng Li*

Main category: cs.SE

TL;DR: 该研究通过挖掘GitHub上的开源ML仓库，识别机器学习系统中的日志异味（logging smells），并通过从业者调查评估这些异味的严重性和频率。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习组件越来越多地集成到软件系统中，有效的日志记录对于确保模型训练和部署的可复现性、可追溯性和可观测性变得至关重要。然而，目前缺乏对ML系统中实际日志使用情况的实证研究，也不清楚是否存在一致的、有效的日志策略。

Method: 1）对GitHub上开源ML仓库进行大规模挖掘，识别和分类常见的日志异味；2）对ML工程师进行从业者调查，评估已识别异味的感知相关性、严重性和频率。

Result: 研究尚未完成，但预期将提供首个关于ML系统中日志异味的实证分类，揭示实际开发中存在的日志问题。

Conclusion: 该研究将填补ML系统日志实践实证研究的空白，为理解和改进ML开发中的日志记录提供重要基础，尽管结果可能无法完全推广到闭源的工业项目中。

Abstract: \underline{Context:} Logging is a fundamental yet complex practice in software engineering, essential for monitoring, debugging, and auditing software systems. With the increasing integration of machine learning (ML) components into software systems, effective logging has become critical to ensure reproducibility, traceability, and observability throughout model training and deployment. Although various general-purpose and ML-specific logging frameworks exist, little is known about how these tools are actually used in practice or whether ML practitioners adopt consistent and effective logging strategies. To date, no empirical study has systematically characterized recurring bad logging practices--or logging smells--in ML System. \underline{Goal:} This study aims to empirically identify and characterize logging smells in ML systems, providing an evidence-based understanding of how logging is implemented and challenged in practice. \underline{Method:} We propose to conduct a large-scale mining of open-source ML repositories hosted on GitHub to catalogue recurring logging smells. Subsequently, a practitioner survey involving ML engineers will be conducted to assess the perceived relevance, severity, and frequency of the identified smells. \underline{Limitations:} % While The study's limitations include that While our findings may not be generalizable to closed-source industrial projects, we believe our study provides an essential step toward understanding and improving logging practices in ML development.

</details>


### [9] [Understanding LLM-Driven Test Oracle Generation](https://arxiv.org/abs/2601.05542)
*Adam Bodicoat,Gunel Jahangirova,Valerio Terragni*

Main category: cs.SE

TL;DR: 该论文研究利用大语言模型生成测试预言，以解决传统单元测试生成技术无法区分程序正确与错误行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有单元测试生成技术主要生成基于被测类实现行为的回归预言，无法解决"预言问题"——即区分正确与错误程序行为的挑战。随着基础模型特别是大语言模型的兴起，为生成反映预期行为的测试预言提供了新机会。

Method: 通过实证研究，调查不同提示策略和上下文输入水平如何影响LLM生成的预言质量。研究LLM在生成暴露软件故障的测试预言方面的有效性。

Result: 研究结果揭示了LLM基于预言生成的优势和局限性，提供了关于不同提示策略和上下文输入对预言质量影响的见解。

Conclusion: 该研究增进了我们对LLM在基础模型时代生成测试预言能力的理解，为未来在这一领域的研究奠定了基础，推动了由自然语言提示驱动的软件创建和测试（Promptware）的发展。

Abstract: Automated unit test generation aims to improve software quality while reducing the time and effort required for creating tests manually. However, existing techniques primarily generate regression oracles that predicate on the implemented behavior of the class under test. They do not address the oracle problem: the challenge of distinguishing correct from incorrect program behavior. With the rise of Foundation Models (FMs), particularly Large Language Models (LLMs), there is a new opportunity to generate test oracles that reflect intended behavior. This positions LLMs as enablers of Promptware, where software creation and testing are driven by natural-language prompts. This paper presents an empirical study on the effectiveness of LLMs in generating test oracles that expose software failures. We investigate how different prompting strategies and levels of contextual input impact the quality of LLM-generated oracles. Our findings offer insights into the strengths and limitations of LLM-based oracle generation in the FM era, improving our understanding of their capabilities and fostering future research in this area.

</details>


### [10] [An Empirical Study of Policy-as-Code Adoption in Open-Source Software Projects](https://arxiv.org/abs/2601.05555)
*Patrick Loic Foalem,Foutse Khomh,Leuson Da Silva,Ettore Merlo*

Main category: cs.SE

TL;DR: 首次大规模实证研究开源软件中Policy-as-Code工具使用情况，分析399个GitHub仓库，建立包含5大类15子类的PaC使用分类法，揭示工具采用多样性、治理导向和新兴MLOps应用模式。


<details>
  <summary>Details</summary>
Motivation: Policy-as-Code已成为将治理、合规和安全要求嵌入软件系统的基础方法，但软件工程社区缺乏对这些工具在实际开发中使用的实证理解，需要填补这一研究空白。

Method: 分析399个使用9种主流PaC工具的GitHub仓库，采用混合方法：定量分析工具使用和项目特征，定性研究策略文件，并利用LLM辅助分类管道（经专家验证）建立包含5类15子类的PaC使用分类法。

Result: 研究发现PaC采用具有显著多样性：工具常用于早期项目，主要面向治理、配置控制和文档；观察到MLOps管道中的新兴PaC使用模式，以及OPA与Gatekeeper等工具间的强共现模式；分类法揭示了重复出现的治理意图。

Conclusion: 研究结果为从业者和工具开发者提供可操作见解，展示具体使用模式，强调实际PaC使用情况，并推动工具互操作性改进机会，为未来PaC实践研究奠定实证基础。

Abstract: \textbf{Context:} Policy-as-Code (PaC) has become a foundational approach for embedding governance, compliance, and security requirements directly into software systems. While organizations increasingly adopt PaC tools, the software engineering community lacks an empirical understanding of how these tools are used in real-world development practices.
  \textbf{Objective:} This paper aims to bridge this gap by conducting the first large-scale study of PaC usage in open-source software. Our goal is to characterize how PaC tools are adopted, what purposes they serve, and what governance activities they support across diverse software ecosystems.
  \textbf{Method:} We analyzed 399 GitHub repositories using nine widely adopted PaC tools. Our mixed-methods approach combines quantitative analysis of tool usage and project characteristics with a qualitative investigation of policy files. We further employ a Large Language Model (LLM)--assisted classification pipeline, refined through expert validation, to derive a taxonomy of PaC usage consisting of 5 categories and 15 sub-categories.
  \textbf{Results:} Our study reveals substantial diversity in PaC adoption. PaC tools are frequently used in early-stage projects and are heavily oriented toward governance, configuration control, and documentation. We also observe emerging PaC usage in MLOps pipelines and strong co-usage patterns, such as between OPA and Gatekeeper. Our taxonomy highlights recurring governance intents.
  \textbf{Conclusion:} Our findings offer actionable insights for practitioners and tool developers. They highlight concrete usage patterns, emphasize actual PaC usage, and motivate opportunities for improving tool interoperability. This study lays the empirical foundation for future research on PaC practices and their role in ensuring trustworthy, compliant software systems.

</details>


### [11] [Package-Aware Approach for Repository-Level Code Completion in Pharo](https://arxiv.org/abs/2601.05617)
*Omar Abedelkader,Stéphane Ducasse,Oleksandr Zaitsev,Romain Robbes,Guillermo Polito*

Main category: cs.SE

TL;DR: 提出基于包结构的代码补全启发式方法，通过分层搜索（同包→同仓库→全局）提升类名、类变量等全局名称的补全准确性


<details>
  <summary>Details</summary>
Motivation: Pharo现有的语义补全引擎虽然强大，但在建议全局名称（类名、类变量、全局变量）时未考虑仓库结构，无法优先推荐同一包或项目中的相关类，导致所有全局名称被平等对待，缺乏上下文相关性

Method: 提出新的包感知启发式方法，采用结构化搜索策略：首先搜索请求类所在的包，然后扩展到同一仓库中的其他包，最后考虑全局命名空间，形成分层优先级

Result: 初步结果显示平均倒数排名（MRR）有所改善，证明包感知补全比之前的扁平全局方法提供更准确和相关的建议

Conclusion: 通过考虑仓库结构的包感知补全启发式方法能够有效提升代码补全的准确性和相关性，解决了现有系统在全局名称建议方面的局限性

Abstract: Pharo offers a sophisticated completion engine based on semantic heuristics, which coordinates specific fetchers within a lazy architecture. These heuristics can be recomposed to support various activities (e.g., live programming or history usage navigation). While this system is powerful, it does not account for the repository structure when suggesting global names such as class names, class variables, or global variables. As a result, it does not prioritize classes within the same package or project, treating all global names equally. In this paper, we present a new heuristic that addresses this limitation. Our approach searches variable names in a structured manner: it begins with the package of the requesting class, then expands to other packages within the same repository, and finally considers the global namespace. We describe the logic behind this heuristic and evaluate it against the default semantic heuristic and one that directly queries the global namespace. Preliminary results indicate that the Mean Reciprocal Rank (MRR) improves, confirming that package-awareness completions deliver more accurate and relevant suggestions than the previous flat global approach.

</details>


### [12] [A Large Scale Empirical Analysis on the Adherence Gap between Standards and Tools in SBOM](https://arxiv.org/abs/2601.05622)
*Chengjie Wang,Jingzheng Wu,Hao Lyu,Xiang Ling,Tianyue Luo,Yanjun Wu,Chen Zhao*

Main category: cs.SE

TL;DR: 首次大规模实证分析SBOM工具对标准规范的遵循情况，发现当前工具存在合规性不足、一致性差、准确性低等根本性缺陷。


<details>
  <summary>Details</summary>
Motivation: SBOM作为增强软件供应链透明度和安全性的重要工具，已有多个标准和工具出现，但缺乏对这些工具遵循标准规范情况的系统性研究，可能导致合规失败和SBOM使用中断。

Method: 采用两阶段实证分析方法：基线评估和一年期纵向跟踪，使用自动化评估框架SAP，覆盖6个SBOM工具从3,287个真实仓库生成的55,444个SBOM。

Result: 发现当前SBOM工具存在三大根本性限制：1) 对政策要求的合规支持不足；2) 工具一致性差（跨工具包检测一致性率仅7.84%-12.77%，纵向一致性低）；3) 详细软件信息准确性中等至差（如许可证准确性低于20%）。

Conclusion: 当前SBOM工具存在严重的标准遵循差距，需要改进工具实现以提高合规性、一致性和准确性，所有代码和评估结果已开源供进一步研究。

Abstract: A Software Bill of Materials (SBOM) is a machine-readable artifact that systematically organizes software information, enhancing supply chain transparency and security. To facilitate the exchange and utilization of SBOMs, organizations such as the Linux Foundation and OWASP have proposed SBOM standards. Following standards, organizations have developed tools for generating and utilizing SBOMs. However, limited research has examined the adherence of these SBOM tools to standard specifications, a gap that could lead to compliance failures and disruptions in SBOM utilization. This paper presents the first large-scale, two-stage empirical analysis of the adherence gap, using our automated evaluation framework, SAP. The evaluation, comprising a baseline evaluation and a one-year longitudinal follow-up, covers 55,444 SBOMs generated by six SBOM tools from 3,287 real-world repositories. Our analysis reveals persistent, fundamental limitations in current SBOM tools: (1) inadequate compliance support with policy requirements; (2) poor tool consistencies, including inter-tool consistency rates as low as 7.84% to 12.77% for package detection across languages, and significant longitudinal inconsistency, where tools show low consistency with their own prior versions; and (3) mediocre to poor accuracy for detailed software information, e.g., accuracy of package licenses below 20%. We analyze the root causes of these gaps and provide practical solutions. All the code, replication docker image, evaluation results are open sourced at [GitHub](https://github.com/dw763j/SAP) and [Zenodo](https://doi.org/10.5281/zenodo.14998624) for further researches.

</details>


### [13] [Tracing Stereotypes in Pre-trained Transformers: From Biased Neurons to Fairer Models](https://arxiv.org/abs/2601.05663)
*Gianmario Voria,Moses Openja,Foutse Khomh,Gemma Catolino,Fabio Palomba*

Main category: cs.SE

TL;DR: 研究发现预训练Transformer中存在编码社会偏见的"偏见神经元"，通过定位并抑制这些神经元可以在软件工程任务中显著减少偏见，同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: Transformer模型在软件工程中广泛应用，但可能复制和放大社会偏见。现有研究表明可以通过神经元编辑来修改模型行为，因此探索是否存在编码偏见的神经元，并开发可解释的偏见缓解方法

Method: 构建包含九种偏见类型的偏见关系三元组数据集，采用神经元归因策略在BERT模型中追踪和抑制偏见神经元，评估抑制对软件工程任务的影响

Result: 偏见知识集中在少数神经元子集中，抑制这些神经元能显著减少偏见，同时性能损失最小，表明偏见可以在神经元层面被追踪和缓解

Conclusion: Transformer中的偏见可以通过神经元层面的干预来缓解，这为软件工程中的公平性提供了可解释的方法

Abstract: The advent of transformer-based language models has reshaped how AI systems process and generate text. In software engineering (SE), these models now support diverse activities, accelerating automation and decision-making. Yet, evidence shows that these models can reproduce or amplify social biases, raising fairness concerns. Recent work on neuron editing has shown that internal activations in pre-trained transformers can be traced and modified to alter model behavior. Building on the concept of knowledge neurons, neurons that encode factual information, we hypothesize the existence of biased neurons that capture stereotypical associations within pre-trained transformers. To test this hypothesis, we build a dataset of biased relations, i.e., triplets encoding stereotypes across nine bias types, and adapt neuron attribution strategies to trace and suppress biased neurons in BERT models. We then assess the impact of suppression on SE tasks. Our findings show that biased knowledge is localized within small neuron subsets, and suppressing them substantially reduces bias with minimal performance loss. This demonstrates that bias in transformers can be traced and mitigated at the neuron level, offering an interpretable approach to fairness in SE.

</details>


### [14] [Drivora: A Unified and Extensible Infrastructure for Search-based Autonomous Driving Testing](https://arxiv.org/abs/2601.05685)
*Mingfei Cheng,Lionel Briand,Yuan Zhou*

Main category: cs.SE

TL;DR: Drivora是一个基于CARLA的统一可扩展自动驾驶系统测试基础设施，通过OpenScenario统一场景定义、解耦测试引擎、场景执行和ADS集成，支持12种ADS并实现大规模并行仿真。


<details>
  <summary>Details</summary>
Motivation: 现有基于搜索的自动驾驶测试方法存在框架异构问题（不同场景空间、仿真器、ADS），导致跨设置重用和适配需要大量工作，需要一个统一的基础设施来解决这些挑战。

Method: 1. 引入统一场景定义OpenScenario，使用底层可操作参数确保兼容性；2. 解耦测试引擎、场景执行和ADS集成；3. 测试引擎利用进化计算探索新场景；4. 并行执行机制最大化硬件利用率；5. 通过统一接口集成12种ADS。

Result: 开发了Drivora工具，提供统一可扩展的测试基础设施，支持大规模并行仿真，简化了ADS测试流程，代码已在GitHub公开。

Conclusion: Drivora解决了自动驾驶系统测试中的异构框架问题，提供了一个统一、可扩展的基础设施，能够支持现有方法的兼容性和新测试设计的扩展性，显著提高了测试效率和可重用性。

Abstract: Search-based testing is critical for evaluating the safety and reliability of autonomous driving systems (ADSs). However, existing approaches are often built on heterogeneous frameworks (e.g., distinct scenario spaces, simulators, and ADSs), which require considerable effort to reuse and adapt across different settings. To address these challenges, we present Drivora, a unified and extensible infrastructure for search-based ADS testing built on the widely used CARLA simulator. Drivora introduces a unified scenario definition, OpenScenario, that specifies scenarios using low-level, actionable parameters to ensure compatibility with existing methods while supporting extensibility to new testing designs (e.g., multi-autonomous-vehicle testing). On top of this, Drivora decouples the testing engine, scenario execution, and ADS integration. The testing engine leverages evolutionary computation to explore new scenarios and supports flexible customization of core components. The scenario execution can run arbitrary scenarios using a parallel execution mechanism that maximizes hardware utilization for large-scale batch simulation. For ADS integration, Drivora provides access to 12 ADSs through a unified interface, streamlining configuration and simplifying the incorporation of new ADSs. Our tools are publicly available at https://github.com/MingfeiCheng/Drivora.

</details>


### [15] [AIBoMGen: Generating an AI Bill of Materials for Secure, Transparent, and Compliant Model Training](https://arxiv.org/abs/2601.05703)
*Wiebe Vandendriessche,Jordi Thijsman,Laurens D'hooge,Bruno Volckaert,Merlijn Sebrechts*

Main category: cs.SE

TL;DR: AI Bill of Materials (AIBOM) 作为软件物料清单的扩展，为AI模型提供标准化可验证记录，AIBoMGen平台自动化生成签名AIBOM，确保AI系统透明度和安全性


<details>
  <summary>Details</summary>
Motivation: 复杂AI系统的快速采用超过了确保其透明度、安全性和监管合规性的工具发展，需要建立标准化的可验证记录机制

Method: 提出AIBOM概念作为SBOM的扩展，开发AIBoMGen概念验证平台，在训练过程中自动捕获数据集、模型元数据和环境细节，使用加密哈希、数字签名和in-toto证明确保完整性

Result: AIBoMGen能可靠检测所有工件的未授权修改，生成AIBOM的性能开销可忽略不计

Conclusion: AIBoMGen是构建安全透明AI生态系统的基础步骤，有助于实现欧盟AI法案等监管框架的合规性

Abstract: The rapid adoption of complex AI systems has outpaced the development of tools to ensure their transparency, security, and regulatory compliance. In this paper, the AI Bill of Materials (AIBOM), an extension of the Software Bill of Materials (SBOM), is introduced as a standardized, verifiable record of trained AI models and their environments. Our proof-of-concept platform, AIBoMGen, automates the generation of signed AIBOMs by capturing datasets, model metadata, and environment details during training. The training platform acts as a neutral, third-party observer and root of trust. It enforces verifiable AIBOM creation for every job. The system uses cryptographic hashing, digital signatures, and in-toto attestations to ensure integrity and protect against threats such as artifact tampering by dishonest model creators. Our evaluation demonstrates that AIBoMGen reliably detects unauthorized modifications to all artifacts and can generate AIBOMs with negligible performance overhead. These results highlight the potential of AIBoMGen as a foundational step toward building secure and transparent AI ecosystems, enabling compliance with regulatory frameworks like the EUs AI Act.

</details>


### [16] [From Issues to Insights: RAG-based Explanation Generation from Software Engineering Artifacts](https://arxiv.org/abs/2601.05721)
*Daniel Pöttgen,Mersedeh Sadeghi,Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 提出首个基于检索增强生成(RAG)的方法，利用问题跟踪数据生成软件系统解释，在GitHub问题上达到90%与人工解释的一致性。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统日益复杂，传统文档往往过时或不完整，难以提供准确解释。问题跟踪系统包含丰富且持续更新的开发知识，但其解释潜力尚未被挖掘。

Method: 采用检索增强生成(RAG)方法，从问题跟踪数据生成解释。使用开源工具和语言模型构建概念验证系统，利用结构化问题数据生成解释。

Result: 在示例项目的GitHub问题上评估，达到90%与人工编写解释的一致性。系统表现出强大的忠实性和指令遵循能力，确保可靠且基于事实的解释。

Conclusion: RAG方法可以将可解释性扩展到更广泛的软件系统，前提是有问题跟踪数据可用，从而使系统行为更易访问和解释。

Abstract: The increasing complexity of modern software systems has made understanding their behavior increasingly challenging, driving the need for explainability to improve transparency and user trust. Traditional documentation is often outdated or incomplete, making it difficult to derive accurate, context-specific explanations. Meanwhile, issue-tracking systems capture rich and continuously updated development knowledge, but their potential for explainability remains untapped. With this work, we are the first to apply a Retrieval-Augmented Generation (RAG) approach for generating explanations from issue-tracking data. Our proof-of-concept system is implemented using open-source tools and language models, demonstrating the feasibility of leveraging structured issue data for explanation generation. Evaluating our approach on an exemplary project's set of GitHub issues, we achieve 90% alignment with human-written explanations. Additionally, our system exhibits strong faithfulness and instruction adherence, ensuring reliable and grounded explanations. These findings suggest that RAG-based methods can extend explainability beyond black-box ML models to a broader range of software systems, provided that issue-tracking data is available - making system behavior more accessible and interpretable.

</details>


### [17] [StriderSPD: Structure-Guided Joint Representation Learning for Binary Security Patch Detection](https://arxiv.org/abs/2601.05772)
*Qingyuan Li,Chenchen Yu,Chuanyi Li,Xin-Cheng Wen,Cheryl Lee,Cuiyun Gao,Bin Luo*

Main category: cs.SE

TL;DR: StriderSPD是一个用于闭源软件安全补丁检测的框架，通过结合图神经网络和大语言模型，利用汇编代码和伪代码的结构信息来识别安全补丁。


<details>
  <summary>Details</summary>
Motivation: 闭源软件的安全补丁检测面临挑战：现有方法主要针对开源软件，而闭源软件以二进制形式分发；二进制分析常使用汇编代码（语义有限）或伪代码（缺乏结构语法），难以准确学习漏洞修复表示；现有评估方法使用相同项目的训练和测试数据，不符合闭源软件实际情况。

Method: 提出StriderSPD框架，将图分支集成到大语言模型中，利用结构信息指导LLM识别安全补丁。设计了适配器在LLM标记级别对齐汇编代码和伪代码表示，并提出两阶段训练策略解决分支间参数不平衡问题。

Result: 构建了与现有数据集在项目和领域上都不同的二进制SPD基准，并在该基准上广泛评估了StriderSPD。

Conclusion: StriderSPD通过结构引导的联合表示方法，有效解决了闭源软件安全补丁检测的挑战，为实际应用提供了更现实的评估框架。

Abstract: Vulnerabilities severely threaten software systems, making the timely application of security patches crucial for mitigating attacks. However, software vendors often silently patch vulnerabilities with limited disclosure, where Security Patch Detection (SPD) comes to protect software assets. Recently, most SPD studies have targeted Open-Source Software (OSS), yet a large portion of real-world software is closed-source, where patches are distributed as binaries without accessible source code. The limited binary SPD approaches often lift binaries to abstraction levels, i.e., assembly code or pseudo-code. However, assembly code is register-based instructions conveying limited semantics, while pseudo-code lacks parser-compatible grammar to extract structure, both hindering accurate vulnerability-fix representation learning. In addition, previous studies often obtain training and testing data from the same project for evaluation, which fails to reflect closed-source conditions. To alleviate the above challenges, we propose \textbf{\textit{StriderSPD}}, a \underline{Str}ucture-gu\underline{ide}d joint \underline{r}epresentation \underline{SPD} framework of binary code that integrates a graph branch into a large language model (LLM), leveraging structural information to guide the LLM in identifying security patches. Our novel design of the adapters in the graph branch effectively aligns the representations between assembly code and pseudo-code at the LLM's token level. We further present a two-stage training strategy to address the optimization imbalance caused by the large parameter disparity between StriderSPD's two branches, which enables proper branch fitting. To enable more realistic evaluation, we construct a binary SPD benchmark that is disjoint from prior datasets in both projects and domains and extensively evaluate StriderSPD on this benchmark.

</details>


### [18] [EET: Experience-Driven Early Termination for Cost-Efficient Software Engineering Agents](https://arxiv.org/abs/2601.05777)
*Yaoqi Guo,Ying Xiao,Jie M. Zhang,Mark Harman,Yiling Lou,Yang Liu,Zhenpeng Chen*

Main category: cs.SE

TL;DR: EET是一种经验驱动的早期终止方法，通过利用历史经验指导补丁生成和选择过程中的早期终止，显著降低软件工程代理的成本，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的软件工程代理在实践中应用日益广泛，但通常会产生高昂的货币成本。需要一种方法来降低这些成本，同时保持任务性能。

Method: EET从先前的issue-resolution执行中提取结构化经验，并利用这些经验在补丁生成和选择过程中指导早期终止，减少无生产力的迭代。

Result: 在SWE-bench Verified基准测试中，EET将总成本降低了19%-55%（平均32%），分辨率损失可忽略不计（最多0.2%）。平均为11%的问题识别了早期终止机会，API调用、输入token和输出token分别减少了21%、30%和25%。

Conclusion: EET是一种有效的成本优化方法，能够在保持软件工程代理性能的同时显著降低运行成本，为实际部署提供了实用的解决方案。

Abstract: Software engineering (SE) agents powered by large language models are increasingly adopted in practice, yet they often incur substantial monetary cost. We introduce EET, an experience-driven early termination approach that reduces the cost of SE agents while preserving task performance. EET extracts structured experience from prior issue-resolution executions and leverages it to guide early termination during patch generation and selection, reducing unproductive iterations. We evaluate EET on the SWE-bench Verified benchmark across three representative SE agents. EET consistently reduces total cost by 19%-55% (32% on average), with negligible loss in resolution rate (at most 0.2%). These efficiency gains are achieved, on average, by identifying early-termination opportunities for 11% of issues and reducing API calls, input tokens, and output tokens by 21%, 30%, and 25%, respectively. We release the code, prompts, and data at https://github.com/EffiSEAgent/EET.

</details>


### [19] [SSR: Safeguarding Staking Rewards by Defining and Detecting Logical Defects in DeFi Staking](https://arxiv.org/abs/2601.05827)
*Zewei Lin,Jiachi Chen,Jingwen Zhang,Zexu Wang,Yuming Feng,Weizhe Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: SSR工具首次系统研究DeFi质押逻辑缺陷，通过LLM提取质押逻辑构建模型，实现高精度检测，发现22.24%真实合约存在此类缺陷。


<details>
  <summary>Details</summary>
Motivation: DeFi质押存在逻辑缺陷可能导致攻击者非法获取奖励，现有研究缺乏对此类缺陷的系统性定义和检测方法。

Method: 分析64个安全事件和144份审计报告，识别6类逻辑缺陷；开发SSR工具，利用LLM提取质押逻辑信息并构建DeFi质押模型，通过静态分析检测缺陷。

Result: SSR在基准测试中达到92.31%精确率、87.92%召回率和88.85% F1分数；在15,992个真实DeFi质押合约中，检测到3,557个（22.24%）存在至少一个逻辑缺陷。

Conclusion: DeFi质押逻辑缺陷普遍存在，SSR工具能有效检测此类缺陷，为DeFi安全提供重要保障。

Abstract: Decentralized Finance (DeFi) staking is one of the most prominent applications within the DeFi ecosystem, where DeFi projects enable users to stake tokens on the platform and reward participants with additional tokens. However, logical defects in DeFi staking could enable attackers to claim unwarranted rewards by manipulating reward amounts, repeatedly claiming rewards, or engaging in other malicious actions. To mitigate these threats, we conducted the first study focused on defining and detecting logical defects in DeFi staking. Through the analysis of 64 security incidents and 144 audit reports, we identified six distinct types of logical defects, each accompanied by detailed descriptions and code examples. Building on this empirical research, we developed SSR (Safeguarding Staking Reward), a static analysis tool designed to detect logical defects in DeFi staking contracts. SSR utilizes a large language model (LLM) to extract fundamental information about staking logic and constructs a DeFi staking model. It then identifies logical defects by analyzing the model and the associated semantic features. We constructed a ground truth dataset based on known security incidents and audit reports to evaluate the effectiveness of SSR. The results indicate that SSR achieves an overall precision of 92.31%, a recall of 87.92%, and an F1-score of 88.85%. Additionally, to assess the prevalence of logical defects in real-world smart contracts, we compiled a large-scale dataset of 15,992 DeFi staking contracts. SSR detected that 3,557 (22.24%) of these contracts contained at least one logical defect.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Self-Evolving Distributed Memory Architecture for Scalable AI Systems](https://arxiv.org/abs/2601.05569)
*Zixuan Li,Chuanzhen Wang,Haotian Sun*

Main category: cs.DC

TL;DR: 提出了一种三层自演化分布式内存架构，通过协调计算、通信和部署层的内存管理，解决了分布式AI系统中的内存效率问题，相比现有方法显著提升了内存利用率和操作性能。


<details>
  <summary>Details</summary>
Motivation: 分布式AI系统面临跨计算、通信和部署层的关键内存管理挑战：RRAM内存计算受限于设备非理想性和固定阵列大小；去中心化AI框架在NAT约束网络中因静态路由而内存效率低下；多智能体部署系统将应用逻辑与执行环境紧密耦合，阻碍了自适应内存优化。这些问题的根源在于缺乏跨架构层的协调内存管理。

Method: 提出了自演化分布式内存架构（SEDMA），包含三层统一内存管理框架：1）基于设备特性的内存引导矩阵处理与动态分区；2）考虑网络拓扑和计算容量的内存感知对等节点选择；3）通过持续重配置实现运行时自适应部署优化。该框架维护双重内存系统，跟踪长期性能模式和短期工作负载统计。

Result: 在COCO 2017、ImageNet和SQuAD数据集上的实验表明，该方法实现了87.3%的内存利用效率和142.5操作/秒的性能，相比Ray Distributed的72.1%和98.7操作/秒有显著提升。同时将通信延迟降低30.2%至171.2毫秒，资源利用率提升至82.7%。

Conclusion: 该研究的主要贡献包括：跨三个架构层的协调内存管理、工作负载自适应资源分配、以及支持动态系统优化的双重内存架构。这些创新解决了分布式AI系统中的关键内存管理挑战，为可扩展AI系统提供了有效的解决方案。

Abstract: Distributed AI systems face critical memory management challenges across computation, communication, and deployment layers. RRAM based in memory computing suffers from scalability limitations due to device non idealities and fixed array sizes. Decentralized AI frameworks struggle with memory efficiency across NAT constrained networks due to static routing that ignores computational load. Multi agent deployment systems tightly couple application logic with execution environments, preventing adaptive memory optimization. These challenges stem from a fundamental lack of coordinated memory management across architectural layers. We introduce Self Evolving Distributed Memory Architecture for Scalable AI Systems, a three layer framework that unifies memory management across computation, communication, and deployment. Our approach features (1) memory guided matrix processing with dynamic partitioning based on device characteristics, (2) memory aware peer selection considering network topology and computational capacity, and (3) runtime adaptive deployment optimization through continuous reconfiguration. The framework maintains dual memory systems tracking both long term performance patterns and short term workload statistics. Experiments on COCO 2017, ImageNet, and SQuAD show that our method achieves 87.3 percent memory utilization efficiency and 142.5 operations per second compared to Ray Distributed at 72.1 percent and 98.7 operations per second, while reducing communication latency by 30.2 percent to 171.2 milliseconds and improving resource utilization to 82.7 percent. Our contributions include coordinated memory management across three architectural layers, workload adaptive resource allocation, and a dual memory architecture enabling dynamic system optimization.

</details>


### [21] [Performance-Portable Optimization and Analysis of Multiple Right-Hand Sides in a Lattice QCD Solver](https://arxiv.org/abs/2601.05816)
*Shiting Long,Gustavo Ramirez-Hidalgo,Stepan Nassyr,Jose Jimenez-Merchan,Andreas Frommer,Dirk Pleiter*

Main category: cs.DC

TL;DR: 论文扩展DD-αAMG求解器以支持多右端项，优化数据布局提升SIMD利用，在x86和Arm集群上实现性能可移植性，并评估Arm SME指令集的潜力。


<details>
  <summary>Details</summary>
Motivation: 科学计算中稀疏线性系统迭代求解器的高计算成本和内存带宽限制是主要挑战，需要优化数据局部性和数据传输效率。

Method: 扩展DD-αAMG求解器支持多右端项，引入灵活接口支持多种数据布局，实现新的数据布局以优化SIMD利用，在x86和Arm集群上进行评估，并探索Arm SME指令集实现。

Result: 在x86和Arm集群上实现了相似的性能加速，展示了性能可移植性；性能分析揭示了架构约束和编译器行为的复杂性；Arm SME指令集显示出潜在优势。

Conclusion: 通过多右端项支持、数据布局优化和SIMD利用，成功提升了DD-αAMG求解器的性能，并在不同架构上实现了可移植性，同时为Arm SME指令集的应用提供了早期评估。

Abstract: Managing the high computational cost of iterative solvers for sparse linear systems is a known challenge in scientific computing. Moreover, scientific applications often face memory bandwidth constraints, making it critical to optimize data locality and enhance the efficiency of data transport. We extend the lattice QCD solver DD-$α$AMG to incorporate multiple right-hand sides (rhs) for both the Wilson-Dirac operator evaluation and the GMRES solver, with and without odd-even preconditioning. To optimize auto-vectorization, we introduce a flexible interface that supports various data layouts and implement a new data layout for better SIMD utilization. We evaluate our optimizations on both x86 and Arm clusters, demonstrating performance portability with similar speedups. A key contribution of this work is the performance analysis of our optimizations, which reveals the complexity introduced by architectural constraints and compiler behavior. Additionally, we explore different implementations leveraging a new matrix instruction set for Arm called SME and provide an early assessment of its potential benefits.

</details>


### [22] [Multi-Modal Style Transfer-based Prompt Tuning for Efficient Federated Domain Generalization](https://arxiv.org/abs/2601.05955)
*Yuliang Chen,Xi Lin,Jun Wu,Xiangrui Cai,Qiaolun Zhang,Xichun Fan,Jiapeng Xu,Xiu Su*

Main category: cs.DC

TL;DR: FaST-PT是一个联邦域泛化框架，通过多模态风格转移进行本地特征增强，使用双提示模块（全局提示和域提示）捕获知识，并通过域感知提示生成实现高效未见域适应。


<details>
  <summary>Details</summary>
Motivation: 现有联邦域泛化方法面临跨客户端数据异构性挑战，且通信和计算开销较大。需要开发既能处理数据异质性又能高效适应未见域的联邦学习方法。

Method: 1. 轻量级多模态风格转移：在文本监督下转换图像嵌入，扩展训练数据分布；2. 双提示模块：分解为全局提示（捕获跨客户端增强嵌入的通用知识）和域提示（捕获本地数据的域特定知识）；3. 域感知提示生成：为每个样本自适应生成合适提示，通过知识融合促进未见域适应。

Result: 在PACS和DomainNet等四个跨域基准数据集上的实验表明，FaST-PT优于FedDG-GA和DiPrompt等SOTA联邦域泛化方法。消融研究验证了其有效性和效率。

Conclusion: FaST-PT通过本地特征增强和高效未见域适应，成功解决了联邦域泛化中的数据异质性和计算开销问题，在多个数据集上表现出优越性能。

Abstract: Federated Domain Generalization (FDG) aims to collaboratively train a global model across distributed clients that can generalize well on unseen domains. However, existing FDG methods typically struggle with cross-client data heterogeneity and incur significant communication and computation overhead. To address these challenges, this paper presents a new FDG framework, dubbed FaST-PT, which facilitates local feature augmentation and efficient unseen domain adaptation in a distributed manner. First, we propose a lightweight Multi-Modal Style Transfer (MST) method to transform image embedding under text supervision, which could expand the training data distribution and mitigate domain shift. We then design a dual-prompt module that decomposes the prompt into global and domain prompts. Specifically, global prompts capture general knowledge from augmented embedding across clients, while domain prompts capture domain-specific knowledge from local data. Besides, Domain-aware Prompt Generation (DPG) is introduced to adaptively generate suitable prompts for each sample, which facilitates unseen domain adaptation through knowledge fusion. Extensive experiments on four cross-domain benchmark datasets, e.g., PACS and DomainNet, demonstrate the superior performance of FaST-PT over SOTA FDG methods such as FedDG-GA and DiPrompt. Ablation studies further validate the effectiveness and efficiency of FaST-PT.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [23] [Parallel Dynamic Spatial Indexes](https://arxiv.org/abs/2601.05347)
*Ziyang Men,Bo Huang,Yan Gu,Yihan Sun*

Main category: cs.DB

TL;DR: 提出两种并行空间索引结构：P-Orth树和SPaC树家族，用于高效处理动态空间数据的批量更新，相比现有并行索引具有更好的更新性能和查询性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中空间数据集高度动态，需要低延迟的批量更新，但现有并行空间索引在这方面表现不足，缺乏高效支持动态工作负载的并行空间索引结构。

Method: 系统研究并行空间索引，重点关注高动态工作负载的更新性能。选择两种适合低延迟更新的索引类型：Orth-tree和R-tree/BVH，分别提出P-Orth树（并行Orth树）和SPaC树家族（并行R-tree/BVH）。

Result: P-Orth树和SPaC树在批量更新方面优于现有并行kd树和Orth树，同时在查询性能上保持与对应Orth树和R树相当或更好的竞争力。

Conclusion: 提出的两种并行空间索引结构能有效解决动态空间数据的高效批量更新问题，为实际应用提供了高性能的解决方案，并通过全面实验验证了其优越性。

Abstract: Maintaining spatial data (points in two or three dimensions) is crucial and has a wide range of applications, such as graphics, GIS, and robotics. To handle spatial data, many data structures, called spatial indexes, have been proposed, e.g. kd-trees, oct/quadtrees (also called Orth-trees), R-trees, and bounding volume hierarchies (BVHs). In real-world applications, spatial datasets tend to be highly dynamic, requiring batch updates of points with low latency. This calls for efficient parallel batch updates on spatial indexes. Unfortunately, there is very little work that achieves this.
  In this paper, we systematically study parallel spatial indexes, with a special focus on achieving high-performance update performance for highly dynamic workloads. We select two types of spatial indexes that are considered optimized for low-latency updates: Orth-tree and R-tree/BVH. We propose two data structures: the P-Orth tree, a parallel Orth-tree, and the SPaC-tree family, a parallel R-tree/BVH. Both the P-Orth tree and the SPaC-tree deliver superior performance in batch updates compared to existing parallel kd-trees and Orth-trees, while preserving better or competitive query performance relative to their corresponding Orth-tree and R-tree counterparts. We also present comprehensive experiments comparing the performance of various parallel spatial indexes and share our findings at the end of the paper.

</details>


### [24] [Task Cascades for Efficient Unstructured Data Processing](https://arxiv.org/abs/2601.05536)
*Shreya Shankar,Sepanta Zeighami,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 论文提出任务级联框架，通过变化模型、文档部分和操作来优化LLM文档处理成本，相比模型级联平均减少36%成本


<details>
  <summary>Details</summary>
Motivation: 现有模型级联框架只变化模型，但忽略了文档部分选择和操作简化的优化机会，导致成本仍有降低空间

Method: 使用LLM代理生成简化的相关操作并选择相关文档部分，构建数百个候选任务，通过迭代方法组装任务级联

Result: 在8个真实文档处理任务中，任务级联在90%准确率目标下，相比模型级联平均减少36%端到端成本

Conclusion: 任务级联框架通过同时优化模型、文档部分和操作，显著降低了LLM文档处理的成本，并提供统计准确性保证

Abstract: Modern database systems allow users to query or process unstructured text or document columns using LLM-powered functions. Users can express an operation in natural language (e.g., "identify if this review mentions billing issues"), with the system executing the operation on each document, in a row-by-row fashion. One way to reduce cost on a batch of documents is to employ the model cascade framework: a cheap proxy model processes each document, and only uncertain cases are escalated to a more accurate, expensive oracle. However, model cascades miss important optimization opportunities; for example, often only part of a document is needed to answer a query, or other related, but simpler operations (e.g., "is the review sentiment negative?", "does the review mention money?") can be handled by cheap models more effectively than the original operation, while still being correlated with it.
  We introduce the task cascades framework, which generalizes model cascades by varying not just the model, but also the document portion and operation at each stage. Our framework uses an LLM agent to generate simplified, decomposed, or otherwise related operations and selects the most relevant document portions, constructing hundreds of candidate tasks from which it assembles a task cascade. We show that optimal cascade selection is intractable via reduction from Minimum Sum Set Cover, but our iterative approach constructs effective cascades. We also provide an extension that offers statistical accuracy guarantees: the resulting cascade meets a user-defined accuracy target (with respect to the oracle) up to a bounded failure probability. Across eight real-world document processing tasks at a 90% target accuracy, task cascades reduce end-to-end cost by an average of 36% compared to model cascades, at a production scale.

</details>


### [25] [RISE: Rule-Driven SQL Dialect Translation via Query Reduction](https://arxiv.org/abs/2601.05579)
*Xudong Xie,Yuwei Zhang,Wensheng Dou,Yu Gao,Ziyu Cui,Jiansen Song,Rui Yang,Jun Wei*

Main category: cs.DB

TL;DR: RISE：一种基于LLM的SQL方言翻译方法，通过方言感知查询简化、规则提取和应用，显著提升复杂SQL查询的翻译准确率


<details>
  <summary>Details</summary>
Motivation: 传统SQL方言翻译工具依赖人工规则，支持新RDBMS和方言需要大量手动工作。LLM虽然能辅助翻译，但在处理长而复杂的SQL查询时表现不佳。

Method: 提出RISE方法：1）使用方言感知查询简化技术，从复杂查询中移除与方言无关的SQL元素得到简化查询；2）用LLM翻译简化查询；3）自动提取方言翻译规则；4）将规则应用到原始复杂查询中，绕过查询复杂性。

Result: 在TPC-DS和SQLProcBench两个真实基准测试中，RISE分别达到97.98%和100%的准确率，相比基线方法平均提升24.62%和238.41%。

Conclusion: RISE能够准确处理长而复杂的SQL查询，显著优于传统规则工具和现有LLM方法，为云迁移中的SQL方言翻译提供了有效解决方案。

Abstract: Translating SQL dialects across different relational database management systems (RDBMSs) is crucial for migrating RDBMS-based applications to the cloud. Traditional SQL dialect translation tools rely on manually-crafted rules, necessitating significant manual effort to support new RDBMSs and dialects. Although large language models (LLMs) can assist in translating SQL dialects, they often struggle with lengthy and complex SQL queries.
  In this paper, we propose RISE, a novel LLM-based SQL dialect translation approach that can accurately handle lengthy and complex SQL queries. Given a complex source query $Q_c$ that contains a SQL dialect $d$, we first employ a dialect-aware query reduction technique to derive a simplified query $Q_{s}$ by removing $d$-irrelevant SQL elements from $Q_c$. Subsequently, we utilize LLMs to translate $Q_{s}$ into $Q_{s^{'}}$, and automatically extract the translation rule $r_d$ for dialect $d$ based on the relationship between $Q_{s}$ and $Q_{s^{'}}$. By applying $r_d$ to $Q_c$, we can effectively translate the dialect $d$ within $Q_c$, thereby bypassing the complexity of the source query $Q_c$. We evaluate RISE on two real-world benchmarks, i.e., TPC-DS and SQLProcBench, comparing its performance against both the traditional rule-based tools and the LLM-based approaches with respect to translation accuracy. RISE achieves accuracies of 97.98% on TPC-DS and 100% on SQLProcBench, outperforming the baselines by an average improvement of 24.62% and 238.41%, respectively.

</details>


### [26] [Descriptor: Multi-Regional Cloud Honeypot Dataset (MURHCAD)](https://arxiv.org/abs/2601.05813)
*Enrique Feito-Casares,Ismael Gómez-Talal,José-Luis Rojo-Álvarez*

Main category: cs.DB

TL;DR: 本文介绍了一个72小时内收集的13.2万次网络攻击事件的高分辨率蜜网数据集，包含地理分布、协议分析和时间模式，支持网络威胁研究。


<details>
  <summary>Details</summary>
Motivation: 为支持全球网络攻击行为的独立分析，需要高质量、高分辨率的蜜网数据集，以进行异常检测、威胁情报和防御策略研究。

Method: 在微软Azure上部署三个蜜罐（Cowrie、Dionaea、SentryPeer）到四个地理分散的虚拟机，连续72小时收集攻击事件，并添加丰富的元数据（时间戳、IP、地理位置、协议等）。

Result: 收集到132,425个攻击事件，涉及2,438个唯一IP地址和95个国家；发现SIP、Telnet、SMB三种协议主导攻击；观察到07:00和23:00 UTC的流量高峰；地理分布显示不同蜜罐捕获的攻击具有区域特征。

Conclusion: 该数据集结合了高时间分辨率和丰富的地理位置、协议元数据，支持可重复的云规模网络威胁调查，为异常检测、协议滥用研究和威胁情报提供资源。

Abstract: This data article introduces a comprehensive, high-resolution honeynet dataset designed to support standalone analyses of global cyberattack behaviors. Collected over a continuous 72-hour window (June 9 to 11, 2025) on Microsoft Azure, the dataset comprises 132,425 individual attack events captured by three honeypots (Cowrie, Dionaea, and SentryPeer) deployed across four geographically dispersed virtual machines. Each event record includes enriched metadata (UTC timestamps, source/destination IPs, autonomous system and organizational mappings, geolocation coordinates, targeted ports, and honeypot identifiers alongside derived temporal features and standardized protocol classifications). We provide actionable guidance for researchers seeking to leverage this dataset in anomaly detection, protocol-misuse studies, threat intelligence, and defensive policy design. Descriptive statistics highlight significant skew: 2,438 unique source IPs span 95 countries, yet the top 1% of IPs account for 1% of all events, and three protocols dominate: Session Initiation Protocol (SIP), Telnet, Server Message Block (SMB). Temporal analysis uncovers pronounced rush-hour peaks at 07:00 and 23:00 UTC, interspersed with maintenance-induced gaps that reveal operational blind spots. Geospatial mapping further underscores platform-specific biases: SentryPeer captures concentrated SIP floods in North America and Southeast Asia, Cowrie logs Telnet/SSH scans predominantly from Western Europe and the U.S., and Dionaea records SMB exploits around European nodes. By combining fine-grained temporal resolution with rich, contextual geolocation and protocol metadata, this standalone dataset aims to empower reproducible, cloud-scale investigations into evolving cyber threats. Accompanying analysis code and data access details are provided.

</details>


### [27] [The Importance of Parameters in Ranking Functions](https://arxiv.org/abs/2601.06001)
*Christoph Standke,Nikolaos Tziavelis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 该论文研究了在数据库排名中计算列权重的SHAP分数，分析了不同排名函数和效应函数组合下的计算复杂度，发现所有情况都有FPRAS近似算法，但精确计算在多项式时间和#P-hard之间变化。


<details>
  <summary>Details</summary>
Motivation: 研究如何解释数据库排名中列权重的重要性，为排名函数提供可解释性分析，帮助理解特定列权重对元组排名的影响。

Method: 采用Grohe等人的框架，分析三种排名函数（字典序、求和、最小值、最大值）和三种效应函数（全局、top-k、局部）的组合，研究在概率独立有限分布下的计算复杂度。

Result: 所有情况都支持可加性FPRAS近似算法，但精确计算复杂度不同：部分情况可在多项式时间内解决，其他情况是#P-hard问题。这些结果也适用于计算整个列的Shapley值。

Conclusion: 为数据库排名中列权重重要性解释提供了系统的复杂度分析框架，揭示了不同排名和效应函数组合下的计算特性，为实际应用中的近似计算提供了理论依据。

Abstract: How important is the weight of a given column in determining the ranking of tuples in a table? To address such an explanation question about a ranking function, we investigate the computation of SHAP scores for column weights, adopting a recent framework by Grohe et al.[ICDT'24]. The exact definition of this score depends on three key components: (1) the ranking function in use, (2) an effect function that quantifies the impact of using alternative weights on the ranking, and (3) an underlying weight distribution. We analyze the computational complexity of different instantiations of this framework for a range of fundamental ranking and effect functions, focusing on probabilistically independent finite distributions for individual columns.
  For the ranking functions, we examine lexicographic orders and score-based orders defined by the summation, minimum, and maximum functions. For the effect functions, we consider global, top-k, and local perspectives: global measures quantify the divergence between the perturbed and original rankings, top-k measures inspect the change in the set of top-k answers, and local measures capture the impact on an individual tuple of interest. Although all cases admit an additive fully polynomial-time randomized approximation scheme (FPRAS), we establish the complexity of exact computation, identifying which cases are solvable in polynomial time and which are #P-hard. We further show that all complexity results, lower bounds and upper bounds, extend to a related task of computing the Shapley value of whole columns (regardless of their weight).

</details>


### [28] [Database Theory in Action: Direct Access to Query Answers](https://arxiv.org/abs/2601.06013)
*Jiayin Hu,Nikolaos Tziavelis*

Main category: cs.DB

TL;DR: 实现了一个支持多种查询和排序的直接访问系统，并研究了其实际性能表现


<details>
  <summary>Details</summary>
Motivation: 虽然支持直接访问的数据结构时间复杂度已被深入研究，但其实际性能表现却很少受到关注

Method: 实现了一个覆盖广泛查询和排序的系统，用于研究实际性能方面的问题

Result: 能够研究数据库系统的比较性能，以及直接访问与其单次访问对应关系等实际问题

Conclusion: 通过实现直接访问系统，填补了理论研究与实际性能评估之间的空白

Abstract: Direct access asks for the retrieval of query answers by their ranked position, given a query and a desired order. While the time complexity of data structures supporting such accesses has been studied in depth, and efficient algorithms for many queries and common orders are known, their practical performance has received little attention. We provide an implementation covering a wide range of queries and orders; it allows us to investigate intriguing practical aspects, including the comparative performance of database systems and the relationship between direct access and its single-access counterpart.

</details>
