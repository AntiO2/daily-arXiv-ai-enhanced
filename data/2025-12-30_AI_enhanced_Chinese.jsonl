{"id": "2512.22216", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22216", "abs": "https://arxiv.org/abs/2512.22216", "authors": ["Shaunak Samant"], "title": "Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair", "comment": null, "summary": "Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.\n  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.", "AI": {"tldr": "\u5c0f\u578bTransformer\u6a21\u578b\u5728Java\u7a0b\u5e8f\u4fee\u590d\u4e2d\u80fd\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684\u4ee3\u7801\uff0c\u4f46\u8bed\u4e49\u4fee\u590d\u80fd\u529b\u6709\u9650\uff0c\u96f6\u7cbe\u786e\u5339\u914d\u4e14\u5e38\u91cd\u590d\u9519\u8bef\u4ee3\u7801", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u6a21\u578b\u5728\u7a0b\u5e8f\u4fee\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4ecd\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5c0f\u578btransformer\u6a21\u578b\u662f\u5426\u80fd\u6709\u6548\u4fee\u590d\u771f\u5b9e\u4e16\u754c\u7684Java bug\uff0c\u4ee5\u53ca\u8bed\u6cd5\u6b63\u786e\u6027\u662f\u5426\u80fd\u53ef\u9760\u4ee3\u8868\u8bed\u4e49\u6b63\u786e\u6027\u3002", "method": "\u4f7f\u7528CodeT5-small\u6a21\u578b\uff086050\u4e07\u53c2\u6570\uff09\u5728CodeXGLUE\u768452,364\u4e2aJava bug-fix\u5bf9\u4e0a\u5fae\u8c03\uff0c\u901a\u8fc7AST\u89e3\u6790\u8bc4\u4f30token\u7ea7\u6027\u80fd\u548c\u8bed\u6cd5\u6709\u6548\u6027\u3002", "result": "\u6a21\u578b\u6536\u655b\u826f\u597d\uff0c\u8bed\u6cd5\u6b63\u786e\u7387\u7ea694%\uff0c\u4f46\u7cbe\u786e\u5339\u914d\u8bc4\u4f30\u4e0b\u4fee\u590d\u6b63\u786e\u7387\u4e3a\u96f6\u3002\u7ea680%\u60c5\u51b5\u4e0b\u6a21\u578b\u76f4\u63a5\u590d\u5236\u4e86\u9519\u8bef\u7684\u8f93\u5165\u4ee3\u7801\u3002", "conclusion": "\u5c0f\u578btransformer\u6a21\u578b\u80fd\u751f\u6210\u8bed\u6cd5\u6b63\u786e\u7684Java\u4ee3\u7801\uff0c\u4f46\u8bed\u6cd5\u6b63\u786e\u6027\u4e0d\u80fd\u53ef\u9760\u4ee3\u8868\u8bed\u4e49\u6b63\u786e\u6027\u3002\u6a21\u578b\u503e\u5411\u4e8e\u590d\u5236\u8f93\u5165\u800c\u975e\u771f\u6b63\u4fee\u590dbug\uff0c\u8868\u660e\u5f53\u524d\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7a0b\u5e8f\u4fee\u590d\u4e2d\u6548\u679c\u6709\u9650\u3002"}}
{"id": "2512.22244", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.22244", "abs": "https://arxiv.org/abs/2512.22244", "authors": ["Daniyal Ganiuly", "Nurzhau Bolatbek", "Assel Smaiyl"], "title": "Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks", "comment": null, "summary": "Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5728\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u573a\u666f\u4e0b\uff0c\u57fa\u4e8e\u5bf9\u8c61\u7684LiDAR\u653b\u51fb\u5bf9\u7eb5\u5411\u5b89\u5168\u63a7\u5236\u5668\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5373\u4f7f\u77ed\u6682\u7684LiDAR\u8bf1\u5bfc\u7269\u4f53\u5e7b\u89c9\u4e5f\u80fd\u89e6\u53d1\u4e0d\u5b89\u5168\u5236\u52a8\u3001\u5bf9\u771f\u5b9e\u5371\u9669\u54cd\u5e94\u5ef6\u8fdf\u548c\u4e0d\u7a33\u5b9a\u63a7\u5236\u884c\u4e3a\u3002", "motivation": "\u867d\u7136\u5148\u524d\u7814\u7a76\u8868\u660eLiDAR\u611f\u77e5\u53ef\u4ee5\u901a\u8fc7\u57fa\u4e8e\u5bf9\u8c61\u7684\u6b3a\u9a97\u548c\u6ce8\u5165\u653b\u51fb\u8fdb\u884c\u64cd\u7eb5\uff0c\u4f46\u6b64\u7c7b\u653b\u51fb\u5bf9\u8f66\u8f86\u5b89\u5168\u63a7\u5236\u5668\u7684\u5f71\u54cd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u611f\u77e5\u9c81\u68d2\u6027\u548c\u63a7\u5236\u7ea7\u5b89\u5168\u4fdd\u8bc1\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528\u9ad8\u4fdd\u771f\u4eff\u771f\u6846\u67b6\uff0c\u96c6\u6210LiDAR\u611f\u77e5\u3001\u76ee\u6807\u8ddf\u8e2a\u548c\u95ed\u73af\u8f66\u8f86\u63a7\u5236\uff0c\u8bc4\u4f30\u865a\u5047\u548c\u4f4d\u79fb\u7269\u4f53\u68c0\u6d4b\u5982\u4f55\u901a\u8fc7\u611f\u77e5-\u89c4\u5212-\u63a7\u5236\u7ba1\u9053\u4f20\u64ad\u3002\u7814\u7a76\u805a\u7126\u4e8e\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u4e2d\u73b0\u5b9e\u7684\u5207\u5165\u548c\u8ddf\u8f66\u573a\u666f\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1) \u77ed\u65f6LiDAR\u8bf1\u5bfc\u7269\u4f53\u5e7b\u89c9\u53ef\u89e6\u53d1\u4e0d\u5b89\u5168\u5236\u52a8\u3001\u5bf9\u771f\u5b9e\u5371\u9669\u54cd\u5e94\u5ef6\u8fdf\u548c\u4e0d\u7a33\u5b9a\u63a7\u5236\uff1b2) \u5207\u5165\u573a\u666f\u4e2d\uff0c\u4e0e\u826f\u6027\u6761\u4ef6\u76f8\u6bd4\uff0c\u4e0d\u5b89\u5168\u51cf\u901f\u4e8b\u4ef6\u548c\u78b0\u649e\u65f6\u95f4\u8fdd\u89c4\u660e\u663e\u589e\u52a0\uff1b3) \u63a7\u5236\u5668\u5931\u6548\u66f4\u53d7\u6b3a\u9a97\u7269\u4f53\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u800c\u975e\u5355\u7eaf\u7a7a\u95f4\u4e0d\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u63cf\u8ff0\u5bf9\u6297\u6027\u611f\u77e5\u4e0b\u7684\u5b89\u5168\u63a7\u5236\u5668\u5931\u6548\u6a21\u5f0f\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bbe\u8ba1\u653b\u51fb\u611f\u77e5\u7684\u5b89\u5168\u673a\u5236\u548c\u66f4\u9c81\u68d2\u7684LiDAR\u4f9d\u8d56\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u9c81\u68d2\u6027\u4e0e\u63a7\u5236\u7ea7\u5b89\u5168\u4fdd\u8bc1\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002"}}
{"id": "2512.22250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22250", "abs": "https://arxiv.org/abs/2512.22250", "authors": ["Bo Yang", "Yinfen Xia", "Weisong Sun", "Yang Liu"], "title": "Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing", "comment": null, "summary": "In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\\% to 82.76\\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.", "AI": {"tldr": "SQLHD\uff1a\u57fa\u4e8e\u8715\u53d8\u6d4b\u8bd5\u7684\u65e0\u6807\u51c6\u7b54\u6848Text-to-SQL\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u548c\u903b\u8f91\u611f\u77e5\u7684\u8715\u53d8\u5173\u7cfb\u5206\u4e24\u9636\u6bb5\u68c0\u6d4b\u6a21\u5f0f\u94fe\u63a5\u548c\u903b\u8f91\u5408\u6210\u4e2d\u7684\u5e7b\u89c9\u3002", "motivation": "LLM\u5728Text-to-SQL\u4efb\u52a1\u4e2d\u4f1a\u4ea7\u751f\u5e7b\u89c9\u5bfc\u81f4\u9519\u8befSQL\u67e5\u8be2\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6807\u51c6\u7b54\u6848\u4e14\u4e0d\u9002\u7528\u4e8eLLM\u573a\u666f\uff0c\u9700\u8981\u65e0\u76d1\u7763\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u63d0\u51faSQLHD\u65b9\u6cd5\uff0c\u5206\u4e24\u9636\u6bb5\u68c0\u6d4b\uff1a1\uff09\u6a21\u5f0f\u94fe\u63a5\u5e7b\u89c9\u68c0\u6d4b\uff1a\u4f7f\u75288\u4e2a\u7ed3\u6784\u611f\u77e5\u8715\u53d8\u5173\u7cfb\u6270\u52a8\u6bd4\u8f83\u8bcd\u3001\u5b9e\u4f53\u3001\u53e5\u5b50\u7ed3\u6784\u6216\u6570\u636e\u5e93\u6a21\u5f0f\uff1b2\uff09\u903b\u8f91\u5408\u6210\u5e7b\u89c9\u68c0\u6d4b\uff1a\u4f7f\u75289\u4e2a\u903b\u8f91\u611f\u77e5\u8715\u53d8\u5173\u7cfb\u7a81\u53d8\u524d\u7f00\u8bcd\u3001\u6781\u503c\u8868\u8fbe\u5f0f\u3001\u6bd4\u8f83\u8303\u56f4\u6216\u6574\u4e2a\u6570\u636e\u5e93\u3002\u901a\u8fc7\u4ea4\u53c9\u68c0\u67e5LLM\u8f93\u51fa\u4e0e\u6e90\u8f93\u51fa\u6765\u68c0\u6d4b\u8fdd\u53cd\u8715\u53d8\u5173\u7cfb\u7684\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aSQLHD\u5728F1\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u523069.36%\u523082.76%\uff0c\u4f18\u4e8eLLM\u81ea\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522bText-to-SQL\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u3002", "conclusion": "SQLHD\u662f\u4e00\u79cd\u65e0\u9700\u6807\u51c6\u7b54\u6848\u7684\u6709\u6548Text-to-SQL\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8715\u53d8\u6d4b\u8bd5\u5206\u9636\u6bb5\u68c0\u6d4b\u6a21\u5f0f\u94fe\u63a5\u548c\u903b\u8f91\u5408\u6210\u4e2d\u7684\u5e7b\u89c9\uff0c\u5728LLM\u573a\u666f\u4e0b\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.22256", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22256", "abs": "https://arxiv.org/abs/2512.22256", "authors": ["Zhonghao Jiang", "David Lo", "Zhongxin Liu"], "title": "Agentic Software Issue Resolution with Large Language Models: A Survey", "comment": null, "summary": "Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.\n  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86126\u9879\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4efb\u52a1\u5de5\u4f5c\u6d41\u7a0b\u3001\u5efa\u7acb\u4e86\u4e09\u7ef4\u5206\u7c7b\u4f53\u7cfb\uff08\u57fa\u51c6\u3001\u6280\u672f\u3001\u5b9e\u8bc1\u7814\u7a76\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u667a\u80fd\u5f3a\u5316\u5b66\u4e60\u5e26\u6765\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u662f\u8f6f\u4ef6\u7ef4\u62a4\u7684\u5173\u952e\u73af\u8282\uff0c\u4f20\u7edf\u5355\u6b65\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u73b0\u5b9e\u95ee\u9898\u6240\u9700\u7684\u957f\u7a0b\u63a8\u7406\u3001\u8fed\u4ee3\u63a2\u7d22\u548c\u53cd\u9988\u9a71\u52a8\u51b3\u7b56\u3002\u968f\u7740LLM\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u7684\u53d1\u5c55\uff0c\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u6210\u4e3a\u4e3b\u6d41\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8be5\u9886\u57df\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u5bf9126\u9879\u524d\u6cbf\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u6982\u8ff0\u4efb\u52a1\u7684\u4e00\u822c\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5efa\u7acb\u4e09\u7ef4\u5206\u7c7b\u4f53\u7cfb\uff1a\u57fa\u51c6\uff08\u8bc4\u4f30\u6807\u51c6\uff09\u3001\u6280\u672f\uff08\u65b9\u6cd5\u4f53\u7cfb\uff09\u3001\u5b9e\u8bc1\u7814\u7a76\uff08\u5b9e\u9645\u5e94\u7528\uff09\uff0c\u5e76\u5206\u6790\u667a\u80fd\u5f3a\u5316\u5b66\u4e60\u5e26\u6765\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "result": "\u5efa\u7acb\u4e86\u5168\u9762\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u667a\u80fd\u4ee3\u7406\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u8bc6\u522b\u4e86\u667a\u80fd\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u6539\u53d8\u4ee3\u7406\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u5f0f\uff0c\u4e3a\u8fde\u63a5\u4eba\u5de5\u667a\u80fd\u4e0e\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u6865\u6881\u3002", "conclusion": "\u667a\u80fd\u4ee3\u7406\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u4e0d\u4ec5\u6781\u5927\u63d0\u5347\u4e86\u8f6f\u4ef6\u7ef4\u62a4\u6548\u7387\u548c\u8d28\u91cf\uff0c\u4e5f\u4e3a\u9a8c\u8bc1\u4ee3\u7406\u7cfb\u7edf\u7684\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u63d0\u4f9b\u4e86\u73b0\u5b9e\u73af\u5883\u3002\u8bba\u6587\u603b\u7ed3\u4e86\u5173\u952e\u6311\u6218\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.22122", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.22122", "abs": "https://arxiv.org/abs/2512.22122", "authors": ["Lyu Yi", "Weiqi Feng", "Yuanbiao Wang", "Yuhong Kan"], "title": "MonoM: Enhancing Monotonicity in Learned Cardinality Estimators", "comment": null, "summary": "Cardinality estimation is a key component of database query optimization. Recent studies have demonstrated that learned cardinality estimation techniques can surpass traditional methods in accuracy. However, a significant barrier to their adoption in production systems is their tendency to violate fundamental logical principles such as monotonicity. In this paper, we explore how learned models specifically MSCN, a query driven deep learning algorithm can breach monotonicity constraints. To address this, we propose a metric called MonoM, which quantitatively measures how well a cardinality estimator adheres to monotonicity across a given query workload. We also propose a monotonic training framework which includes a workload generator that produces directly comparable queries (one query's predicates are strictly more relaxed than another's, enabling monotonicity inference without actual execution) and a novel regularization term added to the loss function. Experimental results show that our monotonic training algorithm not only enhances monotonicity adherence but also improves cardinality estimation accuracy. This improvement is attributed to the regularization term, which reduces overfitting and improves model generalization.", "AI": {"tldr": "\u63d0\u51faMonoM\u6307\u6807\u91cf\u5316\u57fa\u6570\u4f30\u8ba1\u5668\u7684\u5355\u8c03\u6027\u9075\u5b88\u7a0b\u5ea6\uff0c\u5e76\u63d0\u51fa\u5305\u542b\u53ef\u6bd4\u8f83\u67e5\u8be2\u751f\u6210\u5668\u548c\u6b63\u5219\u5316\u9879\u7684\u5355\u8c03\u8bad\u7ec3\u6846\u67b6\uff0c\u65e2\u63d0\u5347\u5355\u8c03\u6027\u53c8\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\u867d\u7136\u7cbe\u5ea6\u9ad8\uff0c\u4f46\u5728\u751f\u4ea7\u7cfb\u7edf\u4e2d\u5e94\u7528\u7684\u4e3b\u8981\u969c\u788d\u662f\u8fdd\u53cd\u5355\u8c03\u6027\u7b49\u57fa\u672c\u903b\u8f91\u539f\u5219\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u63d0\u51faMonoM\u6307\u6807\u91cf\u5316\u5355\u8c03\u6027\u9075\u5b88\u7a0b\u5ea6\uff1b\u8bbe\u8ba1\u5355\u8c03\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\u751f\u6210\u53ef\u76f4\u63a5\u6bd4\u8f83\u67e5\u8be2\u7684\u5de5\u4f5c\u8d1f\u8f7d\u751f\u6210\u5668\uff0c\u4ee5\u53ca\u5728\u635f\u5931\u51fd\u6570\u4e2d\u6dfb\u52a0\u65b0\u9896\u7684\u6b63\u5219\u5316\u9879\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5355\u8c03\u8bad\u7ec3\u7b97\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86\u5355\u8c03\u6027\u9075\u5b88\u7a0b\u5ea6\uff0c\u8fd8\u63d0\u9ad8\u4e86\u57fa\u6570\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u6b63\u5219\u5316\u9879\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\u5e76\u6539\u5584\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5355\u8c03\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u5668\u8fdd\u53cd\u5355\u8c03\u6027\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4e3a\u751f\u4ea7\u7cfb\u7edf\u90e8\u7f72\u626b\u9664\u4e86\u91cd\u8981\u969c\u788d\u3002"}}
{"id": "2512.22125", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22125", "abs": "https://arxiv.org/abs/2512.22125", "authors": ["Jithin VG", "Ditto PS"], "title": "GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems", "comment": null, "summary": "The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.", "AI": {"tldr": "GPU-Virt-Bench\u662f\u4e00\u4e2a\u5168\u9762\u7684GPU\u865a\u62df\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8f6f\u4ef6GPU\u865a\u62df\u5316\u7cfb\u7edf\uff08\u5982HAMi-core\u548cBUD-FCSP\uff09\u4e0e\u786c\u4ef6MIG\u6280\u672f\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u6db5\u76d656\u4e2a\u6027\u80fd\u6307\u6807\u548c10\u4e2a\u7c7b\u522b\u3002", "motivation": "\u968f\u7740AI\u548cLLM\u63a8\u7406\u7b49GPU\u52a0\u901f\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6fc0\u589e\uff0c\u4e91\u548c\u5bb9\u5668\u73af\u5883\u5bf9\u9ad8\u6548GPU\u8d44\u6e90\u5171\u4eab\u7684\u9700\u6c42\u6025\u5267\u589e\u52a0\u3002\u867d\u7136NVIDIA\u7684MIG\u6280\u672f\u63d0\u4f9b\u786c\u4ef6\u7ea7\u9694\u79bb\uff0c\u4f46\u4ec5\u9002\u7528\u4e8e\u9ad8\u7aef\u6570\u636e\u4e2d\u5fc3GPU\u3002\u8f6f\u4ef6\u865a\u62df\u5316\u65b9\u6848\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u6765\u6307\u5bfc\u751f\u4ea7\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u5f00\u53d1\u4e86GPU-Virt-Bench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b56\u4e2a\u6027\u80fd\u6307\u6807\uff0c\u7ec4\u7ec7\u621010\u4e2a\u7c7b\u522b\uff1a\u5f00\u9500\u3001\u9694\u79bb\u8d28\u91cf\u3001LLM\u7279\u5b9a\u6027\u80fd\u3001\u5185\u5b58\u5e26\u5bbd\u3001\u7f13\u5b58\u884c\u4e3a\u3001PCIe\u541e\u5410\u91cf\u3001\u591aGPU\u901a\u4fe1\u3001\u8c03\u5ea6\u6548\u7387\u3001\u5185\u5b58\u788e\u7247\u5316\u548c\u9519\u8bef\u6062\u590d\u3002\u8be5\u6846\u67b6\u7528\u4e8e\u8bc4\u4f30HAMi-core\u3001BUD-FCSP\u7b49\u8f6f\u4ef6\u865a\u62df\u5316\u65b9\u6848\uff0c\u5e76\u4e0e\u6a21\u62df\u7684MIG\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "GPU-Virt-Bench\u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u6bd4\u8f83\u8f6f\u4ef6\u865a\u62df\u5316\u65b9\u6cd5\u4e0e\u7406\u60f3MIG\u884c\u4e3a\uff0c\u4e3a\u591a\u79df\u6237\u73af\u5883\u4e2d\u90e8\u7f72GPU\u8d44\u6e90\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002\u901a\u8fc7\u5bf9HAMi-core\u3001BUD-FCSP\u548c\u6a21\u62dfMIG\u57fa\u7ebf\u7684\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u751f\u4ea7\u90e8\u7f72\u51b3\u7b56\u6240\u9700\u7684\u5173\u952e\u6027\u80fd\u7279\u5f81\u3002", "conclusion": "GPU-Virt-Bench\u586b\u8865\u4e86GPU\u865a\u62df\u5316\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5e2e\u52a9\u4ece\u4e1a\u8005\u5728\u8f6f\u4ef6\u865a\u62df\u5316\u65b9\u6848\u548c\u786c\u4ef6MIG\u6280\u672f\u4e4b\u95f4\u505a\u51fa\u660e\u667a\u9009\u62e9\uff0c\u4f18\u5316\u591a\u79df\u6237GPU\u8d44\u6e90\u7ba1\u7406\u3002"}}
{"id": "2512.22387", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22387", "abs": "https://arxiv.org/abs/2512.22387", "authors": ["Bhanu Prakash Vangala", "Ali Adibifar", "Tanu Malik", "Ashish Gehani"], "title": "AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents", "comment": null, "summary": "The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.", "AI": {"tldr": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u5e72\u51c0\u73af\u5883\u4e2d\u6267\u884c\u6210\u529f\u7387\u4ec568.3%\uff0c\u5b58\u5728\u5927\u91cf\u9690\u85cf\u4f9d\u8d56\uff0cPython\u6210\u529f\u7387\u6700\u9ad8(89.2%)\uff0cJava\u6700\u4f4e(44.0%)", "motivation": "\u7814\u7a76LLM\u4f5c\u4e3a\u7f16\u7801\u4ee3\u7406\u751f\u6210\u7684\u4ee3\u7801\u5728\u5b9e\u9645\u6267\u884c\u4e2d\u7684\u53ef\u590d\u73b0\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u5176\u5728\u5e72\u51c0\u73af\u5883\u4e2d\u4ec5\u4f7f\u7528\u6a21\u578b\u6307\u5b9a\u4f9d\u8d56\u65f6\u7684\u6267\u884c\u6210\u529f\u7387", "method": "\u4f7f\u7528\u4e09\u79cd\u5148\u8fdbLLM\u7f16\u7801\u4ee3\u7406(Claude Code\u3001OpenAI Codex\u3001Gemini)\uff0c\u57fa\u4e8e100\u4e2a\u6807\u51c6\u5316\u63d0\u793a\u751f\u6210300\u4e2a\u9879\u76ee(Python\u3001JavaScript\u3001Java\u5404100\u4e2a)\uff0c\u5f15\u5165\u4e09\u5c42\u4f9d\u8d56\u6846\u67b6(\u58f0\u79f0\u4f9d\u8d56\u3001\u5de5\u4f5c\u4f9d\u8d56\u3001\u8fd0\u884c\u65f6\u4f9d\u8d56)\u91cf\u5316\u6267\u884c\u53ef\u590d\u73b0\u6027", "result": "\u4ec568.3%\u7684\u9879\u76ee\u80fd\u5f00\u7bb1\u5373\u7528\uff0c\u8bed\u8a00\u95f4\u5dee\u5f02\u663e\u8457\uff1aPython 89.2%\u3001JavaScript 71.6%\u3001Java 44.0%\uff1b\u4ece\u58f0\u660e\u4f9d\u8d56\u5230\u5b9e\u9645\u8fd0\u884c\u65f6\u4f9d\u8d56\u5e73\u5747\u81a8\u80c013.5\u500d\uff0c\u5b58\u5728\u5927\u91cf\u9690\u85cf\u4f9d\u8d56", "conclusion": "LLM\u751f\u6210\u7684\u4ee3\u7801\u5728\u5b9e\u9645\u6267\u884c\u4e2d\u5b58\u5728\u663e\u8457\u7684\u53ef\u590d\u73b0\u6027\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u4f9d\u8d56\u7ba1\u7406\u673a\u5236\uff0c\u7279\u522b\u662f\u5bf9\u4e8eJava\u7b49\u8bed\u8a00\uff0c\u5f53\u524dLLM\u7f16\u7801\u4ee3\u7406\u5728\u4f9d\u8d56\u89c4\u8303\u65b9\u9762\u5b58\u5728\u4e0d\u8db3"}}
{"id": "2512.22364", "categories": ["cs.DB", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22364", "abs": "https://arxiv.org/abs/2512.22364", "authors": ["Saurabh Deochake", "Debajyoti Mukhopadhyay"], "title": "Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries", "comment": null, "summary": "Text-to-SQL systems powered by Large Language Models (LLMs) achieve high accuracy on standard benchmarks, yet existing efficiency metrics such as the Valid Efficiency Score (VES) measure execution time rather than the consumption-based costs of cloud data warehouses. This paper presents the first systematic evaluation of cloud compute costs for LLM-generated SQL queries. We evaluate six state-of-the-art LLMs across 180 query executions on Google BigQuery using the StackOverflow dataset (230GB), measuring bytes processed, slot utilization, and estimated cost. Our analysis yields three key findings: (1) reasoning models process 44.5% fewer bytes than standard models while maintaining equivalent correctness (96.7%-100%); (2) execution time correlates weakly with query cost (r=0.16), indicating that speed optimization does not imply cost optimization; and (3) models exhibit up to 3.4x cost variance, with standard models producing outliers exceeding 36GB per query. We identify prevalent inefficiency patterns including missing partition filters and unnecessary full-table scans, and provide deployment guidelines for cost-sensitive enterprise environments.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u751f\u6210\u7684SQL\u67e5\u8be2\u5728\u4e91\u6570\u636e\u4ed3\u5e93\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u6bd4\u6807\u51c6\u6a21\u578b\u5904\u7406\u5b57\u8282\u6570\u5c1144.5%\uff0c\u6267\u884c\u65f6\u95f4\u4e0e\u6210\u672c\u76f8\u5173\u6027\u5f31\uff0c\u4e0d\u540c\u6a21\u578b\u6210\u672c\u5dee\u5f02\u53ef\u8fbe3.4\u500d\uff0c\u5e76\u8bc6\u522b\u4e86\u5e38\u89c1\u7684\u4f4e\u6548\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709Text-to-SQL\u7cfb\u7edf\u867d\u7136\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u6548\u7387\u6307\u6807\uff08\u5982VES\uff09\u53ea\u5173\u6ce8\u6267\u884c\u65f6\u95f4\uff0c\u5ffd\u7565\u4e86\u4e91\u6570\u636e\u4ed3\u5e93\u7684\u5b9e\u9645\u6d88\u8d39\u6210\u672c\u3002\u4f01\u4e1a\u90e8\u7f72\u9700\u8981\u4e86\u89e3LLM\u751f\u6210SQL\u7684\u771f\u5b9e\u4e91\u6210\u672c\u3002", "method": "\u5728Google BigQuery\u4e0a\u4f7f\u7528StackOverflow\u6570\u636e\u96c6\uff08230GB\uff09\uff0c\u8bc4\u4f306\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u6267\u884c180\u4e2a\u67e5\u8be2\uff0c\u6d4b\u91cf\u5b57\u8282\u5904\u7406\u91cf\u3001\u69fd\u4f4d\u5229\u7528\u7387\u548c\u4f30\u8ba1\u6210\u672c\u3002", "result": "1. \u63a8\u7406\u6a21\u578b\u6bd4\u6807\u51c6\u6a21\u578b\u5904\u7406\u5b57\u8282\u6570\u5c1144.5%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u6b63\u786e\u7387\uff0896.7%-100%\uff09\uff1b2. \u6267\u884c\u65f6\u95f4\u4e0e\u6210\u672c\u76f8\u5173\u6027\u5f31\uff08r=0.16\uff09\uff1b3. \u4e0d\u540c\u6a21\u578b\u6210\u672c\u5dee\u5f02\u53ef\u8fbe3.4\u500d\uff0c\u6807\u51c6\u6a21\u578b\u4ea7\u751f\u8d85\u8fc736GB/\u67e5\u8be2\u7684\u5f02\u5e38\u503c\u3002", "conclusion": "\u9700\u8981\u65b0\u7684\u6210\u672c\u4f18\u5316\u6307\u6807\uff0c\u63a8\u7406\u6a21\u578b\u5728\u6210\u672c\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u8bc6\u522b\u4e86\u7f3a\u5931\u5206\u533a\u8fc7\u6ee4\u5668\u548c\u5168\u8868\u626b\u63cf\u7b49\u4f4e\u6548\u6a21\u5f0f\uff0c\u4e3a\u6210\u672c\u654f\u611f\u7684\u4f01\u4e1a\u73af\u5883\u63d0\u4f9b\u4e86\u90e8\u7f72\u6307\u5357\u3002"}}
{"id": "2512.22135", "categories": ["cs.DC", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22135", "abs": "https://arxiv.org/abs/2512.22135", "authors": ["Zicai Cui", "Zhouyuan Jian", "Weiwen Liu", "Weinan Zhang"], "title": "SoDA: An Efficient Interaction Paradigm for the Agentic Web", "comment": null, "summary": "As the internet evolves from the mobile App-dominated Attention Economy to the Intent-Interconnection of the Agentic Web era, existing interaction modes fail to address the escalating challenges of data lock-in and cognitive overload. Addressing this, we defines a future-oriented user sovereignty interaction paradigm, aiming to realize a fundamental shift from killing time to saving time. Specifically, we argue that decoupling memory from application logic eliminates the structural basis of data lock-in, while shifting from explicit manual instruction to implicit intent alignment resolves cognitive overload by offloading execution complexity. This paradigm is implemented via the Sovereign Digital Avatar (SoDA), which employs an orthogonal decoupling design of storage, computation, and interaction. This establishes the architectural principle of data as a persistent asset, model as a transient tool, fundamentally breaking the platform monopoly on user memory. To support the operation of this new paradigm in zero-trust environments, we design an Intent-Permission Handshake Mechanism based on A2A protocols, utilizing dual-factor (Sensitivity Coefficient and Strictness Parameter) adaptive routing to achieve active risk governance. Empirical evaluation with a high-fidelity simulation environment indicates that this paradigm reduces token consumption by approximately 27-35\\% during cross-platform service migration and complex task execution. Furthermore, in the orchestration of multi-modal complex tasks, it reduces user cognitive load by 72\\% compared to standard Retrieval-Augmented Generation (RAG) architectures, by 88\\% relative to manual workflows, while significantly boosting the Information Signal-to-Noise Ratio (SNR). These results demonstrate that the SoDA is the essential interaction infrastructure for building an efficient, low-friction, and decentralized Agentic Web.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSoDA\uff08\u4e3b\u6743\u6570\u5b57\u5316\u8eab\uff09\u4f5c\u4e3a\u9762\u5411Agentic Web\u65f6\u4ee3\u7684\u65b0\u578b\u4ea4\u4e92\u8303\u5f0f\uff0c\u901a\u8fc7\u5b58\u50a8\u3001\u8ba1\u7b97\u3001\u4ea4\u4e92\u7684\u6b63\u4ea4\u89e3\u8026\u8bbe\u8ba1\uff0c\u89e3\u51b3\u6570\u636e\u9501\u5b9a\u548c\u8ba4\u77e5\u8fc7\u8f7d\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\"\u6d88\u78e8\u65f6\u95f4\"\u5230\"\u8282\u7701\u65f6\u95f4\"\u7684\u6839\u672c\u8f6c\u53d8\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u4ece\u79fb\u52a8App\u4e3b\u5bfc\u7684\u6ce8\u610f\u529b\u7ecf\u6d4e\u5411Agentic Web\u65f6\u4ee3\u7684\u610f\u56fe\u4e92\u8054\u6f14\u8fdb\uff0c\u73b0\u6709\u4ea4\u4e92\u6a21\u5f0f\u65e0\u6cd5\u89e3\u51b3\u65e5\u76ca\u4e25\u91cd\u7684\u6570\u636e\u9501\u5b9a\u548c\u8ba4\u77e5\u8fc7\u8f7d\u95ee\u9898\u3002\u9700\u8981\u5efa\u7acb\u9762\u5411\u672a\u6765\u7684\u7528\u6237\u4e3b\u6743\u4ea4\u4e92\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e3b\u6743\u6570\u5b57\u5316\u8eab\uff08SoDA\uff09\u67b6\u6784\uff0c\u91c7\u7528\u5b58\u50a8\u3001\u8ba1\u7b97\u3001\u4ea4\u4e92\u7684\u6b63\u4ea4\u89e3\u8026\u8bbe\u8ba1\uff0c\u5c06\u6570\u636e\u4f5c\u4e3a\u6301\u4e45\u8d44\u4ea7\u3001\u6a21\u578b\u4f5c\u4e3a\u4e34\u65f6\u5de5\u5177\u3002\u8bbe\u8ba1\u57fa\u4e8eA2A\u534f\u8bae\u7684\u610f\u56fe-\u6743\u9650\u63e1\u624b\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u56e0\u5b50\uff08\u654f\u611f\u7cfb\u6570\u548c\u4e25\u683c\u53c2\u6570\uff09\u81ea\u9002\u5e94\u8def\u7531\u5b9e\u73b0\u4e3b\u52a8\u98ce\u9669\u6cbb\u7406\u3002", "result": "\u5728\u8de8\u5e73\u53f0\u670d\u52a1\u8fc1\u79fb\u548c\u590d\u6742\u4efb\u52a1\u6267\u884c\u4e2d\u51cf\u5c11\u7ea627-35%\u7684token\u6d88\u8017\uff1b\u5728\u591a\u6a21\u6001\u590d\u6742\u4efb\u52a1\u7f16\u6392\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6RAG\u67b6\u6784\u964d\u4f4e72%\u7528\u6237\u8ba4\u77e5\u8d1f\u8f7d\uff0c\u76f8\u6bd4\u624b\u52a8\u5de5\u4f5c\u6d41\u964d\u4f4e88%\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4fe1\u606f\u4fe1\u566a\u6bd4\u3002", "conclusion": "SoDA\u662f\u6784\u5efa\u9ad8\u6548\u3001\u4f4e\u6469\u64e6\u3001\u53bb\u4e2d\u5fc3\u5316Agentic Web\u7684\u5fc5\u5907\u4ea4\u4e92\u57fa\u7840\u8bbe\u65bd\uff0c\u901a\u8fc7\u89e3\u8026\u8bb0\u5fc6\u4e0e\u5e94\u7528\u903b\u8f91\u3001\u4ece\u663e\u5f0f\u624b\u52a8\u6307\u4ee4\u8f6c\u5411\u9690\u5f0f\u610f\u56fe\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u7528\u6237\u4e3b\u6743\u548c\u8ba4\u77e5\u51cf\u8d1f\u3002"}}
{"id": "2512.22418", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22418", "abs": "https://arxiv.org/abs/2512.22418", "authors": ["Yi-Hung Chou", "Boyuan Jiang", "Yi Wen Chen", "Mingyue Weng", "Victoria Jackson", "Thomas Zimmermann", "James A. Jones"], "title": "Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding", "comment": "Accepted for publication at the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2026)", "summary": "Large language models (LLMs) are reshaping software engineering by enabling \"vibe coding,\" in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as \"rolling the dice.\" Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u624e\u6839\u7406\u8bba\u5206\u679020\u4e2a\"\u6c1b\u56f4\u7f16\u7a0b\"\u89c6\u9891\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u4f7f\u7528LLM\u7f16\u7a0b\u65f6\u7684\u884c\u4e3a\u8c31\u7cfb\uff1a\u4ece\u5b8c\u5168\u4f9d\u8d56AI\u4e0d\u68c0\u67e5\u4ee3\u7801\uff0c\u5230\u68c0\u67e5\u5e76\u8c03\u6574\u751f\u6210\u8f93\u51fa\uff0c\u6240\u6709\u4eba\u90fd\u9700\u5e94\u5bf9\u751f\u6210\u7684\u968f\u673a\u6027\uff0c\u8c03\u8bd5\u5e38\u88ab\u63cf\u8ff0\u4e3a\"\u63b7\u9ab0\u5b50\"\u3002", "motivation": "LLM\u6b63\u5728\u91cd\u5851\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u50ac\u751f\u4e86\"\u6c1b\u56f4\u7f16\u7a0b\"\u73b0\u8c61\uff0c\u5373\u5f00\u53d1\u8005\u4e3b\u8981\u901a\u8fc7\u63d0\u793a\u800c\u975e\u7f16\u5199\u4ee3\u7801\u6765\u6784\u5efa\u8f6f\u4ef6\u3002\u5c3d\u7ba1\u88ab\u5e7f\u6cdb\u5ba3\u4f20\u4e3a\u751f\u4ea7\u529b\u7a81\u7834\uff0c\u4f46\u4eba\u4eec\u5bf9\u4ece\u4e1a\u8005\u5982\u4f55\u5b9a\u4e49\u548c\u5b9e\u8df5\u8fd9\u4e9b\u65b9\u6cd5\u77e5\u4e4b\u751a\u5c11\uff0c\u9700\u8981\u7814\u7a76\u8fd9\u4e00\u65b0\u5174\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u624e\u6839\u7406\u8bba\u7814\u7a7620\u4e2a\u6c1b\u56f4\u7f16\u7a0b\u89c6\u9891\uff0c\u5305\u62ec7\u4e2a\u76f4\u64ad\u7f16\u7801\u4f1a\u8bdd\uff08\u7ea616\u5c0f\u65f6\uff0c254\u4e2a\u63d0\u793a\uff09\u548c13\u4e2a\u89c2\u70b9\u89c6\u9891\uff08\u7ea65\u5c0f\u65f6\uff09\uff0c\u8f85\u4ee5\u6d3b\u52a8\u65f6\u957f\u548c\u63d0\u793a\u610f\u56fe\u7684\u989d\u5916\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u884c\u4e3a\u8c31\u7cfb\uff1a\u4e00\u4e9b\u6c1b\u56f4\u7f16\u7801\u8005\u51e0\u4e4e\u5b8c\u5168\u4f9d\u8d56AI\u800c\u4e0d\u68c0\u67e5\u4ee3\u7801\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u68c0\u67e5\u5e76\u8c03\u6574\u751f\u6210\u8f93\u51fa\u3002\u6240\u6709\u65b9\u6cd5\u90fd\u5fc5\u987b\u5e94\u5bf9\u751f\u6210\u7684\u968f\u673a\u6027\uff0c\u8c03\u8bd5\u548c\u4f18\u5316\u5e38\u88ab\u63cf\u8ff0\u4e3a\"\u63b7\u9ab0\u5b50\"\u3002\u4e0d\u540c\u7684\u5fc3\u667a\u6a21\u578b\uff08\u53d7\u7f16\u7801\u8005\u4e13\u4e1a\u77e5\u8bc6\u548cAI\u4f9d\u8d56\u7a0b\u5ea6\u5f71\u54cd\uff09\u5f71\u54cd\u63d0\u793a\u7b56\u7565\u3001\u8bc4\u4f30\u5b9e\u8df5\u548c\u4fe1\u4efb\u6c34\u5e73\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u4e3a\u5de5\u5177\u8bbe\u8ba1\u548c\u6559\u80b2\u63d0\u4f9b\u4e86\u5b9e\u8df5\u673a\u4f1a\u3002\u7814\u7a76\u63ed\u793a\u4e86LLM\u7f16\u7a0b\u5b9e\u8df5\u4e2d\u591a\u6837\u5316\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u6311\u6218\u3002"}}
{"id": "2512.22742", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22742", "abs": "https://arxiv.org/abs/2512.22742", "authors": ["Hanze Meng", "Jianhao Cao", "Rachel Pottinger"], "title": "Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning", "comment": "13 pages, 8 figures", "summary": "Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but their applicability is limited to in-domain settings, as distribution shifts in tables or label spaces require costly re-training from scratch. Recent work has explored prompting generative large language models (LLMs) by framing CTA as a multiple-choice task, but these approaches face two key challenges: (1) model performance is highly sensitive to subtle changes in prompt wording and structure, and (2) annotation F1 scores remain modest. A natural extension is to fine-tune large language models. However, fully fine-tuning these models incurs prohibitive computational costs due to their scale, and the sensitivity to prompts is not eliminated. In this paper, we present a parameter-efficient framework for CTA that trains models over prompt-augmented data via Low-Rank Adaptation (LoRA). Our approach mitigates sensitivity to prompt variations while drastically reducing the number of necessary trainable parameters, achieving robust performance across datasets and templates. Experimental results on recent benchmarks demonstrate that models fine-tuned with our prompt augmentation strategy maintain stable performance across diverse prompt patterns during inference and yield higher weighted F1 scores than those fine-tuned on a single prompt template. These results highlight the effectiveness of parameter-efficient training and augmentation strategies in developing practical and adaptable CTA systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\uff0c\u89e3\u51b3\u5217\u7c7b\u578b\u6807\u6ce8\u4efb\u52a1\u4e2d\u6a21\u578b\u5bf9\u63d0\u793a\u8bcd\u654f\u611f\u4e14\u6027\u80fd\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u57fa\u4e8e\u7f16\u7801\u5668\u7684\u8bed\u8a00\u6a21\u578b\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u9886\u57df\u91cd\u65b0\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\uff1b2) \u4f7f\u7528LLM\u8fdb\u884c\u63d0\u793a\u7684\u65b9\u6cd5\u5bf9\u63d0\u793a\u8bcd\u53d8\u5316\u654f\u611f\u4e14F1\u5206\u6570\u6709\u9650\u3002\u5b8c\u5168\u5fae\u8c03LLM\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u4e14\u65e0\u6cd5\u6d88\u9664\u63d0\u793a\u654f\u611f\u6027\u3002", "method": "\u63d0\u51fa\u53c2\u6570\u9ad8\u6548\u7684CTA\u6846\u67b6\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94(LoRA)\u5728\u63d0\u793a\u589e\u5f3a\u6570\u636e\u4e0a\u8bad\u7ec3\u6a21\u578b\u3002\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u6a21\u677f\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\uff0c\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u5bf9\u63d0\u793a\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u63d0\u793a\u589e\u5f3a\u7b56\u7565\u5fae\u8c03\u7684\u6a21\u578b\u5728\u63a8\u7406\u65f6\u5bf9\u4e0d\u540c\u63d0\u793a\u6a21\u5f0f\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u52a0\u6743F1\u5206\u6570\u9ad8\u4e8e\u4f7f\u7528\u5355\u4e00\u63d0\u793a\u6a21\u677f\u5fae\u8c03\u7684\u6a21\u578b\u3002", "conclusion": "\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u548c\u589e\u5f3a\u7b56\u7565\u5728\u5f00\u53d1\u5b9e\u7528\u4e14\u9002\u5e94\u6027\u5f3a\u7684CTA\u7cfb\u7edf\u4e2d\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u5e73\u8861\u8ba1\u7b97\u6210\u672c\u4e0e\u6027\u80fd\uff0c\u63d0\u9ad8\u6a21\u578b\u5bf9\u63d0\u793a\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.22136", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.22136", "abs": "https://arxiv.org/abs/2512.22136", "authors": ["Mahadev Sunil Kumar", "Arnab Raha", "Debayan Das", "Gopakumar G", "Amitava Mukherjee"], "title": "SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware", "comment": null, "summary": "Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respects hardware limitations and preserves task performance. Our method integrates a structured model pruning with a multi-objective optimization to tailor network capacity to heterogeneous device constraints. We demonstrate this framework using Multi-View Convolutional Neural Network (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets, respectively. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint while reducing inference latency by factors ranging from 1.2x to 5.0x across diverse hardware platforms. These findings suggest that performance-aware, view-adaptive compression provides a viable pathway for deploying complex vision models in distributed edge environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u5206\u5e03\u5f0f\u8fb9\u7f18\u8bbe\u5907\u7684DNN\u9ad8\u6548\u90e8\u7f72\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u526a\u679d\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5728\u6ee1\u8db3\u786c\u4ef6\u7ea6\u675f\u7684\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u5728MVCNN\u4e0a\u5b9e\u73b01.2-5.0\u500d\u7684\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u6df1\u5ea6\u5206\u5e03\u5f0f\u7f51\u7edc\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u53c2\u6570\u91cf\u548c\u8ba1\u7b97\u9700\u6c42\u963b\u788d\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6ee1\u8db3\u786c\u4ef6\u9650\u5236\u53c8\u80fd\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u9ad8\u6548\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u5316\u6a21\u578b\u526a\u679d\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u6839\u636e\u5f02\u6784\u8bbe\u5907\u7ea6\u675f\u8c03\u6574\u7f51\u7edc\u5bb9\u91cf\u3002\u4ee5MVCNN\uff08\u591a\u89c6\u56fe\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u4e3a\u4f8b\uff0c\u91cf\u5316\u5404\u4e2a\u89c6\u56fe\u5bf9\u5206\u7c7b\u51c6\u786e\u7387\u7684\u8d21\u732e\uff0c\u5e76\u76f8\u5e94\u5206\u914d\u526a\u679d\u9884\u7b97\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u6a21\u578b\u5728\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u7684\u51c6\u786e\u7387\u548c\u5185\u5b58\u5360\u7528\u9650\u5236\u7684\u540c\u65f6\uff0c\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e861.2\u500d\u52305.0\u500d\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "\u6027\u80fd\u611f\u77e5\u3001\u89c6\u56fe\u81ea\u9002\u5e94\u7684\u538b\u7f29\u65b9\u6cd5\u4e3a\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u73af\u5883\u4e2d\u90e8\u7f72\u590d\u6742\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2512.22469", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22469", "abs": "https://arxiv.org/abs/2512.22469", "authors": ["Wei Liu", "Chao Peng", "Pengfei Gao", "Aofan Liu", "Wei Zhang", "Haiyan Zhao", "Zhi Jin"], "title": "GraphLocator: Graph-guided Causal Reasoning for Issue Localization", "comment": null, "summary": "The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.", "AI": {"tldr": "GraphLocator\uff1a\u4e00\u79cd\u901a\u8fc7\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u548c\u52a8\u6001\u95ee\u9898\u89e3\u8026\u6765\u89e3\u51b3\u8f6f\u4ef6\u95ee\u9898\u5b9a\u4f4d\u4e2d\u75c7\u72b6-\u539f\u56e0\u4e0d\u5339\u914d\u548c\u4e00\u5bf9\u591a\u4e0d\u5339\u914d\u7684\u65b9\u6cd5", "motivation": "\u8f6f\u4ef6\u95ee\u9898\u5b9a\u4f4d\u4efb\u52a1\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u75c7\u72b6-\u539f\u56e0\u4e0d\u5339\u914d\uff08\u95ee\u9898\u63cf\u8ff0\u4e0d\u660e\u786e\u63ed\u793a\u6839\u672c\u539f\u56e0\uff09\u548c\u4e00\u5bf9\u591a\u4e0d\u5339\u914d\uff08\u5355\u4e2a\u95ee\u9898\u5bf9\u5e94\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u4ee3\u7801\u5b9e\u4f53\uff09\uff0c\u5bfc\u81f4\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u4e0e\u6e90\u4ee3\u7801\u5b9e\u73b0\u4e4b\u95f4\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002", "method": "\u63d0\u51faGraphLocator\u65b9\u6cd5\uff0c\u6784\u5efa\u56e0\u679c\u95ee\u9898\u56fe\uff08CIG\uff09\uff0c\u5176\u4e2d\u9876\u70b9\u8868\u793a\u53d1\u73b0\u7684\u5b50\u95ee\u9898\u53ca\u5176\u5173\u8054\u7684\u4ee3\u7801\u5b9e\u4f53\uff0c\u8fb9\u7f16\u7801\u5b83\u4eec\u4e4b\u95f4\u7684\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\u3002\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u75c7\u72b6\u9876\u70b9\u5b9a\u4f4d\u548c\u52a8\u6001CIG\u53d1\u73b0\uff0c\u9996\u5148\u5728\u4ed3\u5e93\u56fe\u4e2d\u8bc6\u522b\u75c7\u72b6\u4f4d\u7f6e\uff0c\u7136\u540e\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u76f8\u90bb\u9876\u70b9\u52a8\u6001\u6269\u5c55CIG\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a(1) \u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cGraphLocator\u5728\u51fd\u6570\u7ea7\u53ec\u56de\u7387\u5e73\u5747\u63d0\u5347+19.49%\uff0c\u7cbe\u786e\u7387\u63d0\u5347+11.89%\uff1b(2) \u5728\u75c7\u72b6-\u539f\u56e0\u548c\u4e00\u5bf9\u591a\u4e0d\u5339\u914d\u573a\u666f\u4e0b\u5206\u522b\u5b9e\u73b0\u53ec\u56de\u7387\u63d0\u5347+16.44%\u548c+19.18%\uff0c\u7cbe\u786e\u7387\u63d0\u5347+7.78%\u548c+13.23%\uff1b(3) GraphLocator\u751f\u6210\u7684CIG\u5728\u4e0b\u6e38\u89e3\u51b3\u4efb\u52a1\u4e2d\u5e26\u676528.74%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "GraphLocator\u901a\u8fc7\u56e0\u679c\u7ed3\u6784\u53d1\u73b0\u548c\u52a8\u6001\u95ee\u9898\u89e3\u8026\u6709\u6548\u89e3\u51b3\u4e86\u8f6f\u4ef6\u95ee\u9898\u5b9a\u4f4d\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u51c6\u786e\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2512.22838", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.22838", "abs": "https://arxiv.org/abs/2512.22838", "authors": ["Chengying Huan", "Lizheng Chen", "Zhengyi Yang", "Shaonan Ma", "Rong Gu", "Renjie Yao", "Zhibin Wang", "Mingxing Zhang", "Fang Xi", "Jie Tao", "Gang Zhang", "Guihai Chen", "Chen Tian"], "title": "OrchANN: A Unified I/O Orchestration Framework for Skewed Out-of-Core Vector Search", "comment": "13 pages, 30 figures", "summary": "Approximate nearest neighbor search (ANNS) at billion scale is fundamentally an out-of-core problem: vectors and indexes live on SSD, so performance is dominated by I/O rather than compute. Under skewed semantic embeddings, existing out-of-core systems break down: a uniform local index mismatches cluster scales, static routing misguides queries and inflates the number of probed partitions, and pruning is incomplete at the cluster level and lossy at the vector level, triggering \"fetch-to-discard\" reranking on raw vectors.\n  We present OrchANN, an out-of-core ANNS engine that uses an I/O orchestration model for unified I/O governance along the route-access-verify pipeline. OrchANN selects a heterogeneous local index per cluster via offline auto-profiling, maintains a query-aware in-memory navigation graph that adapts to skewed workloads, and applies multi-level pruning with geometric bounds to filter both clusters and vectors before issuing SSD reads. Across five standard datasets under strict out-of-core constraints, OrchANN outperforms four baselines including DiskANN, Starling, SPANN, and PipeANN in both QPS and latency while reducing SSD accesses. Furthermore, OrchANN delivers up to 17.2x higher QPS and 25.0x lower latency than competing systems without sacrificing accuracy.", "AI": {"tldr": "OrchANN\u662f\u4e00\u4e2a\u9762\u5411\u5341\u4ebf\u7ea7\u5411\u91cf\u7684\u5916\u5b58\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u5f15\u64ce\uff0c\u901a\u8fc7I/O\u7f16\u6392\u6a21\u578b\u3001\u5f02\u6784\u672c\u5730\u7d22\u5f15\u3001\u67e5\u8be2\u611f\u77e5\u5bfc\u822a\u56fe\u548c\u591a\u7ea7\u526a\u679d\u6280\u672f\uff0c\u5728SSD\u5b58\u50a8\u73af\u5883\u4e0b\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5341\u4ebf\u7ea7\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u672c\u8d28\u4e0a\u662f\u5916\u5b58\u95ee\u9898\uff0c\u5411\u91cf\u548c\u7d22\u5f15\u5b58\u50a8\u5728SSD\u4e0a\uff0c\u6027\u80fd\u53d7I/O\u4e3b\u5bfc\u800c\u975e\u8ba1\u7b97\u3002\u73b0\u6709\u5916\u5b58\u7cfb\u7edf\u5728\u5904\u7406\u504f\u659c\u8bed\u4e49\u5d4c\u5165\u65f6\u5b58\u5728\u591a\u4e2a\u95ee\u9898\uff1a\u5747\u5300\u672c\u5730\u7d22\u5f15\u4e0e\u805a\u7c7b\u89c4\u6a21\u4e0d\u5339\u914d\u3001\u9759\u6001\u8def\u7531\u8bef\u5bfc\u67e5\u8be2\u5e76\u589e\u52a0\u63a2\u6d4b\u5206\u533a\u6570\u91cf\u3001\u526a\u679d\u4e0d\u5b8c\u6574\u5bfc\u81f4\"\u83b7\u53d6\u540e\u4e22\u5f03\"\u7684\u91cd\u65b0\u6392\u5e8f\u3002", "method": "1. \u91c7\u7528I/O\u7f16\u6392\u6a21\u578b\u7edf\u4e00\u7ba1\u7406\u8def\u7531-\u8bbf\u95ee-\u9a8c\u8bc1\u6d41\u6c34\u7ebf\u7684I/O\u64cd\u4f5c\uff1b2. \u901a\u8fc7\u79bb\u7ebf\u81ea\u52a8\u5206\u6790\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u9009\u62e9\u5f02\u6784\u672c\u5730\u7d22\u5f15\uff1b3. \u7ef4\u62a4\u67e5\u8be2\u611f\u77e5\u7684\u5185\u5b58\u5bfc\u822a\u56fe\u4ee5\u9002\u5e94\u504f\u659c\u5de5\u4f5c\u8d1f\u8f7d\uff1b4. \u5e94\u7528\u5e26\u51e0\u4f55\u8fb9\u754c\u7ea6\u675f\u7684\u591a\u7ea7\u526a\u679d\u6280\u672f\uff0c\u5728\u53d1\u8d77SSD\u8bfb\u53d6\u524d\u8fc7\u6ee4\u805a\u7c7b\u548c\u5411\u91cf\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u548c\u4e25\u683c\u5916\u5b58\u7ea6\u675f\u4e0b\uff0cOrchANN\u5728QPS\u548c\u5ef6\u8fdf\u65b9\u9762\u5747\u4f18\u4e8eDiskANN\u3001Starling\u3001SPANN\u548cPipeANN\u56db\u4e2a\u57fa\u7ebf\u7cfb\u7edf\uff0c\u540c\u65f6\u51cf\u5c11\u4e86SSD\u8bbf\u95ee\u3002\u76f8\u6bd4\u7ade\u4e89\u7cfb\u7edf\uff0cOrchANN\u63d0\u4f9b\u9ad8\u8fbe17.2\u500d\u7684QPS\u63d0\u5347\u548c25.0\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u4e14\u4e0d\u727a\u7272\u51c6\u786e\u6027\u3002", "conclusion": "OrchANN\u901a\u8fc7\u521b\u65b0\u7684I/O\u7f16\u6392\u6a21\u578b\u548c\u81ea\u9002\u5e94\u7d22\u5f15\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5916\u5b58ANNS\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u5728\u5904\u7406\u504f\u659c\u8bed\u4e49\u5d4c\u5165\u65f6\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5341\u4ebf\u7ea7\u5411\u91cf\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22137", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22137", "abs": "https://arxiv.org/abs/2512.22137", "authors": ["Jiangwen Dong", "Jiayu Li", "Wanyu Lin"], "title": "HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration", "comment": null, "summary": "Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.", "AI": {"tldr": "HybridFlow\u662f\u4e00\u4e2a\u8d44\u6e90\u81ea\u9002\u5e94\u7684\u8fb9\u7f18-\u4e91\u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5206\u89e3\u548c\u5e76\u884c\u6267\u884c\u6765\u51cf\u5c11LLM\u63a8\u7406\u5ef6\u8fdf\u548ctoken\u6d88\u8017\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9762\u4e34\u63a8\u7406\u5ef6\u8fdf\u9ad8\u548ctoken\u6d88\u8017\u5927\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u8fb9\u7f18-\u4e91\u534f\u4f5c\u65b9\u6cd5\u91c7\u7528\u7c97\u7c92\u5ea6\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7ec6\u7c92\u5ea6\u63a8\u7406\u5e76\u884c\u6027\uff0c\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "HybridFlow\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4efb\u52a1\u5206\u89e3\u548c\u5e76\u884c\u6267\u884c\uff0c\u52a8\u6001\u5c06\u590d\u6742\u67e5\u8be2\u62c6\u5206\u4e3a\u76f8\u4e92\u4f9d\u8d56\u7684\u5b50\u4efb\u52a1\uff1b2) \u8d44\u6e90\u611f\u77e5\u5b50\u4efb\u52a1\u8def\u7531\uff0c\u901a\u8fc7\u5b66\u4e60\u578b\u8def\u7531\u5668\u6839\u636e\u9884\u6d4b\u7684\u6548\u7528\u589e\u76ca\u548c\u5b9e\u65f6\u9884\u7b97\u72b6\u6001\u81ea\u9002\u5e94\u5730\u5c06\u5b50\u4efb\u52a1\u5206\u914d\u7ed9\u8fb9\u7f18\u6216\u4e91\u7aef\u6a21\u578b\u3002", "result": "\u5728GPQA\u3001MMLU-Pro\u3001AIME\u548cLiveBench-Reasoning\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cHybridFlow\u80fd\u6709\u6548\u51cf\u5c11\u7aef\u5230\u7aef\u63a8\u7406\u65f6\u95f4\u548c\u603b\u4f53token\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "HybridFlow\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u8fb9\u7f18-\u4e91\u534f\u4f5c\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u4f4e\u7684token\u6d88\u8017\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22538", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22538", "abs": "https://arxiv.org/abs/2512.22538", "authors": ["Qingyang Li", "Yibiao Yang", "Maolin Sun", "Jiangchang Wu", "Qingkai Shi", "Yuming Zhou"], "title": "Isolating Compiler Faults via Multiple Pairs of Adversarial Compilation Configurations", "comment": "Accepted at ACM TOSEM", "summary": "Compilers are fundamental to modern software development, making the effective identification and resolution of compiler faults essential. However, localizing these faults to specific source files remains highly challenging due to the complexity and scale of modern compiler infrastructures. In this study, we propose MultiConf, a novel approach that automatically isolates compiler faults by constructing multiple pairs of adversarial compilation configurations. Each adversarial compilation configuration pair consists of a failing configuration and its corresponding passing configuration, which differ in only a small number of fine-grained options. MultiConf generates failing configurations through a lightweight construction process and derives the corresponding passing configurations by selectively disabling bug-related fine-grained options. We then employ a Spectrum-Based Fault Localization (SBFL) formula to rank the suspiciousness of compiler source files. Each adversarial configuration pair independently produces a ranking, which is subsequently aggregated using a weighted voting scheme to derive a final suspiciousness ranking, enabling more accurate and robust fault localization. We evaluate MultiConf on a benchmark of 60 real-world GCC compiler bugs. The results demonstrate that MultiConf significantly outperforms existing compiler fault localization techniques in both effectiveness and efficiency. In particular, MultiConf successfully localizes 27 out of 60 bugs at the Top-1 file level, representing improvements of 35.0% and 28.6% over the two state-of-the-art approaches, Odfl(20) and Basic(21), respectively.", "AI": {"tldr": "MultiConf\uff1a\u4e00\u79cd\u901a\u8fc7\u6784\u5efa\u591a\u5bf9\u5bf9\u6297\u6027\u7f16\u8bd1\u914d\u7f6e\u6765\u81ea\u52a8\u5b9a\u4f4d\u7f16\u8bd1\u5668\u6545\u969c\u7684\u65b0\u65b9\u6cd5\uff0c\u5728GCC\u7f16\u8bd1\u5668bug\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7f16\u8bd1\u5668\u662f\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u7684\u57fa\u7840\uff0c\u4f46\u5b9a\u4f4d\u7f16\u8bd1\u5668\u6545\u969c\u5230\u5177\u4f53\u6e90\u6587\u4ef6\u975e\u5e38\u56f0\u96be\uff0c\u56e0\u4e3a\u73b0\u4ee3\u7f16\u8bd1\u5668\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u4e14\u89c4\u6a21\u5e9e\u5927\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u591a\u5bf9\u5bf9\u6297\u6027\u7f16\u8bd1\u914d\u7f6e\uff08\u6bcf\u4e2a\u5bf9\u5305\u542b\u4e00\u4e2a\u5931\u8d25\u914d\u7f6e\u548c\u5bf9\u5e94\u7684\u901a\u8fc7\u914d\u7f6e\uff0c\u4ec5\u5c11\u6570\u7ec6\u7c92\u5ea6\u9009\u9879\u4e0d\u540c\uff09\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6784\u9020\u8fc7\u7a0b\u751f\u6210\u5931\u8d25\u914d\u7f6e\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u7981\u7528bug\u76f8\u5173\u7ec6\u7c92\u5ea6\u9009\u9879\u63a8\u5bfc\u901a\u8fc7\u914d\u7f6e\uff0c\u7136\u540e\u5e94\u7528\u57fa\u4e8e\u9891\u8c31\u7684\u6545\u969c\u5b9a\u4f4d\u516c\u5f0f\u5bf9\u7f16\u8bd1\u5668\u6e90\u6587\u4ef6\u8fdb\u884c\u53ef\u7591\u5ea6\u6392\u540d\uff0c\u6700\u540e\u901a\u8fc7\u52a0\u6743\u6295\u7968\u65b9\u6848\u805a\u5408\u6392\u540d\u3002", "result": "\u572860\u4e2a\u771f\u5b9e\u4e16\u754cGCC\u7f16\u8bd1\u5668bug\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMultiConf\u5728Top-1\u6587\u4ef6\u7ea7\u522b\u6210\u529f\u5b9a\u4f4d\u4e8627\u4e2abug\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6cd5Odfl(20)\u548cBasic(21)\u5206\u522b\u63d0\u9ad8\u4e8635.0%\u548c28.6%\u3002", "conclusion": "MultiConf\u5728\u7f16\u8bd1\u5668\u6545\u969c\u5b9a\u4f4d\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u4e3a\u7f16\u8bd1\u5668\u8c03\u8bd5\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22893", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.22893", "abs": "https://arxiv.org/abs/2512.22893", "authors": ["Simu Liu", "Kailin Jiao", "Junping Du", "Yawen Li", "Zhe Xue", "Xiaoyang Sean Wang", "Ziqiang Yu", "Yunchuan Shi"], "title": "Time Sensitive Multiple POIs Route Planning on Bus Networks", "comment": null, "summary": "This work addresses a route planning problem constrained by a bus road network that includes the schedules of all buses. Given a query with a starting bus stop and a set of Points of Interest (POIs) to visit, our goal is to find an optimal route on the bus network that allows the user to visit all specified POIs from the starting stop with minimal travel time, which includes both bus travel time and waiting time at bus stops. Although this problem resembles a variant of the Traveling Salesman Problem, it cannot be effectively solved using existing solutions due to the complex nature of bus networks, particularly the constantly changing bus travel times and user waiting times. In this paper, we first propose a modified graph structure to represent the bus network, accommodating the varying bus travel times and their arrival schedules at each stop. Initially, we suggest a brute-force exploration algorithm based on the Dijkstra principle to evaluate all potential routes and determine the best one; however, this approach is too costly for large bus networks. To address this, we introduce the EA-Star algorithm, which focuses on computing the shortest route for promising POI visit sequences. The algorithm includes a terminal condition that halts evaluation once the optimal route is identified, avoiding the need to evaluate all possible POI sequences. During the computation of the shortest route for each POI visiting sequence, it employs the A* algorithm on the modified graph structure, narrowing the search space toward the destination and improving search efficiency. Experiments using New York bus network datasets demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51faEA-Star\u7b97\u6cd5\u89e3\u51b3\u516c\u4ea4\u7f51\u7edc\u4e2d\u7684POI\u8bbf\u95ee\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u8003\u8651\u52a8\u6001\u516c\u4ea4\u65f6\u523b\u8868\u548c\u7b49\u5f85\u65f6\u95f4\uff0c\u76f8\u6bd4\u66b4\u529b\u641c\u7d22\u663e\u8457\u63d0\u5347\u6548\u7387", "motivation": "\u89e3\u51b3\u516c\u4ea4\u7f51\u7edc\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u7528\u6237\u9700\u8981\u4ece\u8d77\u70b9\u51fa\u53d1\u8bbf\u95ee\u591a\u4e2a\u5174\u8da3\u70b9(POI)\uff0c\u4f46\u516c\u4ea4\u7f51\u7edc\u5177\u6709\u52a8\u6001\u53d8\u5316\u7684\u7279\u70b9\uff08\u516c\u4ea4\u884c\u9a76\u65f6\u95f4\u548c\u5230\u7ad9\u65f6\u95f4\u8868\uff09\uff0c\u4f20\u7edf\u65c5\u884c\u5546\u95ee\u9898(TSP)\u89e3\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u590d\u6742\u6027", "method": "1) \u63d0\u51fa\u6539\u8fdb\u7684\u56fe\u7ed3\u6784\u8868\u793a\u516c\u4ea4\u7f51\u7edc\uff0c\u9002\u5e94\u53d8\u5316\u7684\u516c\u4ea4\u884c\u9a76\u65f6\u95f4\u548c\u5230\u7ad9\u65f6\u523b\u8868\uff1b2) \u57fa\u4e8eDijkstra\u539f\u7406\u7684\u66b4\u529b\u641c\u7d22\u7b97\u6cd5\u4f5c\u4e3a\u57fa\u51c6\uff1b3) \u63d0\u51faEA-Star\u7b97\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u8ba1\u7b97\u6709\u5e0c\u671b\u7684POI\u8bbf\u95ee\u5e8f\u5217\u7684\u6700\u77ed\u8def\u5f84\uff0c\u4f7f\u7528A*\u7b97\u6cd5\u5728\u6539\u8fdb\u56fe\u7ed3\u6784\u4e0a\u641c\u7d22\uff0c\u5e76\u8bbe\u7f6e\u7ec8\u6b62\u6761\u4ef6\u907f\u514d\u8bc4\u4f30\u6240\u6709\u53ef\u80fd\u5e8f\u5217", "result": "\u5728\u7ebd\u7ea6\u516c\u4ea4\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u627e\u5230\u8bbf\u95ee\u6240\u6709\u6307\u5b9aPOI\u7684\u6700\u4f18\u8def\u7ebf\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c", "conclusion": "\u63d0\u51fa\u7684EA-Star\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u516c\u4ea4\u7f51\u7edc\u4e2d\u7684\u591aPOI\u8bbf\u95ee\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u8003\u8651\u4e86\u52a8\u6001\u516c\u4ea4\u65f6\u523b\u8868\u548c\u7b49\u5f85\u65f6\u95f4\uff0c\u76f8\u6bd4\u66b4\u529b\u641c\u7d22\u65b9\u6cd5\u5728\u5927\u578b\u516c\u4ea4\u7f51\u7edc\u4e2d\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387"}}
{"id": "2512.22139", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.22139", "abs": "https://arxiv.org/abs/2512.22139", "authors": ["Amur Saqib Pal", "Muhammad Mohsin Ghaffar", "Faisal Shafait", "Christian Weis", "Norbert Wehn"], "title": "HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA", "comment": "Accepted for publication by 25th International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS 2025)", "summary": "Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering real-time performance in safety critical applications due to GPU under-utilization. To address this challenge, we present HLS4PC, a parameterizable HLS framework for FPGA acceleration. Our approach leverages FPGA parallelization and algorithmic optimizations to enable efficient fixed-point implementations of both mapping and NN functions. We explore several hardware-aware compression techniques on a state-of-the-art PointMLP-Elite model, including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning, yielding PointMLP-Lite, a 4x less complex variant with only 2% accuracy drop on ModelNet40. Secondly, we demonstrate that the FPGA acceleration of the PointMLP-Lite results in 3.56x higher throughput than previous works. Furthermore, our implementation achieves 2.3x and 22x higher throughput compared to the GPU and CPU implementations, respectively.", "AI": {"tldr": "HLS4PC\uff1a\u57fa\u4e8eFPGA\u52a0\u901f\u76843D\u70b9\u4e91\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u538b\u7f29\u6280\u672f\u4f18\u5316PointMLP\u6a21\u578b\uff0c\u5b9e\u73b0\u6bd4GPU/CPU\u66f4\u9ad8\u7684\u541e\u5410\u91cf", "motivation": "3D\u70b9\u4e91\u6570\u636e\u7a00\u758f\u3001\u975e\u7ed3\u6784\u5316\u7684\u7279\u6027\u5bfc\u81f4\u4f20\u7edf\u57fa\u4e8eGPU\u7684\u65b9\u6cd5\u5b58\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u9ad8\u3001GPU\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b89\u5168\u5173\u952e\u5e94\u7528\u7684\u5b9e\u65f6\u6027\u8981\u6c42", "method": "\u63d0\u51faHLS4PC\u53c2\u6570\u5316HLS\u6846\u67b6\uff0c\u5229\u7528FPGA\u5e76\u884c\u5316\u548c\u7b97\u6cd5\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u7684\u5b9a\u70b9\u6570\u6620\u5c04\u548c\u795e\u7ecf\u7f51\u7edc\u51fd\u6570\u3002\u91c7\u7528\u786c\u4ef6\u611f\u77e5\u538b\u7f29\u6280\u672f\uff1a\u7528URS\u66ff\u4ee3FPS\u3001\u53c2\u6570\u91cf\u5316\u3001\u5c42\u878d\u5408\u548c\u8f93\u5165\u70b9\u526a\u679d\uff0c\u5c06PointMLP-Elite\u4f18\u5316\u4e3a\u590d\u6742\u5ea6\u964d\u4f4e4\u500d\u7684PointMLP-Lite", "result": "PointMLP-Lite\u5728ModelNet40\u4e0a\u4ec5\u635f\u59312%\u7cbe\u5ea6\uff1bFPGA\u52a0\u901f\u5b9e\u73b0\u6bd4\u5148\u524d\u5de5\u4f5c\u9ad83.56\u500d\u541e\u5410\u91cf\uff0c\u6bd4GPU\u548cCPU\u5b9e\u73b0\u5206\u522b\u9ad82.3\u500d\u548c22\u500d\u541e\u5410\u91cf", "conclusion": "HLS4PC\u6846\u67b6\u901a\u8fc7FPGA\u52a0\u901f\u548c\u786c\u4ef6\u611f\u77e5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u5904\u7406\u7684\u6548\u7387\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.22633", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22633", "abs": "https://arxiv.org/abs/2512.22633", "authors": ["Woorim Han", "Yeongjun Kwak", "Miseon Yu", "Kyeongmin Kim", "Younghan Lee", "Hyungon Moon", "Yunheung Paek"], "title": "Rethinking the Capability of Fine-Tuned Language Models for Automated Vulnerability Repair", "comment": "To appear at ICSE 26. 13 pages", "summary": "Learning-based automated vulnerability repair (AVR) techniques that utilize fine-tuned language models have shown promise in generating vulnerability patches. However, questions remain about their ability to repair unseen vulnerabilities. Our empirical study reveals that state-of-the-art models often overfit to the training set and are evaluated using training, validation, and test sets that are not mutually exclusive. Furthermore, relying on match-based metrics that compare generated patches to reference fixes at the token level has some limitations, failing to account for the possibility of various valid ways to patch the vulnerability. In this paper, we examine the capabilities of state-of-the-art fine-tuned AVR models and the adequacy of match-based evaluation metrics in three ways. First, we apply semantic-preserving transformations to test sets in order to determine whether models truly learn robust vulnerability-repair patterns or simply rely on spurious features. Second, we re-split the training, validation, and test sets to be mutually exclusive and evaluate the models on the revised test set to assess their generalization capabilities. Third, we introduce L-AVRBench, a test-based benchmark tailored for learning-based AVR, to overcome the limitations of match-based metrics and examine the AVR models' true repair capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u6f0f\u6d1e\u4fee\u590d\uff08AVR\uff09\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bc4\u4f30\u6307\u6807\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u6d4b\u8bd5\u57fa\u51c6L-AVRBench\u6765\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u4fee\u590d\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6f0f\u6d1e\u4fee\u590d\u6280\u672f\u867d\u7136\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u4fee\u590d\u672a\u89c1\u6f0f\u6d1e\u7684\u80fd\u529b\u5b58\u5728\u7591\u95ee\u3002\u73b0\u6709\u6a21\u578b\u53ef\u80fd\u8fc7\u62df\u5408\u8bad\u7ec3\u6570\u636e\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u6a21\u578b\u7684\u5b9e\u9645\u4fee\u590d\u80fd\u529b\u3002", "method": "1. \u5bf9\u6d4b\u8bd5\u96c6\u5e94\u7528\u8bed\u4e49\u4fdd\u6301\u53d8\u6362\uff0c\u68c0\u9a8c\u6a21\u578b\u662f\u5426\u5b66\u4e60\u5230\u7a33\u5065\u7684\u6f0f\u6d1e\u4fee\u590d\u6a21\u5f0f\u800c\u975e\u4f9d\u8d56\u865a\u5047\u7279\u5f81\uff1b2. \u91cd\u65b0\u5212\u5206\u4e92\u65a5\u7684\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\uff0c\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff1b3. \u63d0\u51fa\u4e13\u95e8\u9488\u5bf9\u5b66\u4e60\u578bAVR\u7684\u57fa\u4e8e\u6d4b\u8bd5\u7684\u57fa\u51c6L-AVRBench\uff0c\u514b\u670d\u57fa\u4e8e\u5339\u914d\u5ea6\u91cf\u7684\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u6a21\u578b\u7ecf\u5e38\u8fc7\u62df\u5408\u8bad\u7ec3\u96c6\uff0c\u4e14\u73b0\u6709\u8bc4\u4f30\u4f7f\u7528\u7684\u6570\u636e\u96c6\u5212\u5206\u4e0d\u4e92\u65a5\u3002\u57fa\u4e8e\u5339\u914d\u7684\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u8003\u8651\u6f0f\u6d1e\u4fee\u590d\u7684\u591a\u79cd\u6709\u6548\u65b9\u5f0f\u3002", "conclusion": "\u9700\u8981\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u66f4\u51c6\u786e\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u57fa\u4e8e\u5b66\u4e60\u7684AVR\u6a21\u578b\u7684\u771f\u5b9e\u4fee\u590d\u80fd\u529b\uff0cL-AVRBench\u4e3a\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2512.22995", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.22995", "abs": "https://arxiv.org/abs/2512.22995", "authors": ["Prudhvi Gadupudi", "Suman Saha"], "title": "Evolution of Buffer Management in Database Systems: From Classical Algorithms to Machine Learning and Disaggregated Memory", "comment": null, "summary": "Buffer management remains a critical component of database and operating system performance, serving as the primary mechanism for bridging the persistent latency gap between CPU processing speeds and storage access times. This paper provides a comprehensive survey of buffer management evolution spanning four decades of research. We systematically analyze the progression from foundational algorithms like LRU-K, 2Q, LIRS, and ARC to contemporary machine learning-augmented policies and disaggregated memory architectures. Our survey examines the historical OS-DBMS architectural divergence, production system implementations in PostgreSQL, Oracle, and Linux, and emerging trends including eBPF-based kernel extensibility, NVM-aware tiering strategies, and RDMA-enabled memory disaggregation. Through analysis of over 50 seminal papers from leading conferences (SIGMOD, VLDB, OSDI, FAST), we identify key architectural patterns, performance trade-offs, and open research challenges. We conclude by outlining a research direction that integrates machine learning with kernel extensibility mechanisms to enable adaptive, cross-layer buffer management for heterogeneous memory hierarchies in modern database systems.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6570\u636e\u5e93\u548c\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u7f13\u51b2\u533a\u7ba1\u7406\u8fdb\u884c\u4e8640\u5e74\u7814\u7a76\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u4ece\u4f20\u7edf\u7b97\u6cd5\u6f14\u8fdb\u5230\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\u548c\u5206\u89e3\u5185\u5b58\u67b6\u6784\uff0c\u5206\u6790\u4e8650\u591a\u7bc7\u91cd\u8981\u8bba\u6587\uff0c\u5e76\u63d0\u51fa\u4e86\u6574\u5408\u673a\u5668\u5b66\u4e60\u4e0e\u5185\u6838\u53ef\u6269\u5c55\u6027\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7f13\u51b2\u533a\u7ba1\u7406\u662f\u6570\u636e\u5e93\u548c\u64cd\u4f5c\u7cfb\u7edf\u6027\u80fd\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u7528\u4e8e\u5f25\u5408CPU\u5904\u7406\u901f\u5ea6\u4e0e\u5b58\u50a8\u8bbf\u95ee\u65f6\u95f4\u4e4b\u95f4\u7684\u6301\u4e45\u5ef6\u8fdf\u5dee\u8ddd\u3002\u968f\u7740\u6280\u672f\u53d1\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u68b3\u740640\u5e74\u6765\u7684\u7814\u7a76\u6f14\u8fdb\uff0c\u4e3a\u73b0\u4ee3\u5f02\u6784\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e8650\u591a\u7bc7\u6765\u81eaSIGMOD\u3001VLDB\u3001OSDI\u3001FAST\u7b49\u9876\u7ea7\u4f1a\u8bae\u7684\u91cc\u7a0b\u7891\u8bba\u6587\uff0c\u6db5\u76d6\u4eceLRU-K\u30012Q\u3001LIRS\u3001ARC\u7b49\u57fa\u7840\u7b97\u6cd5\u5230\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u7b56\u7565\u548c\u5206\u89e3\u5185\u5b58\u67b6\u6784\u7684\u6f14\u8fdb\u8fc7\u7a0b\uff0c\u5e76\u8003\u5bdf\u4e86PostgreSQL\u3001Oracle\u3001Linux\u7b49\u751f\u4ea7\u7cfb\u7edf\u7684\u5b9e\u73b0\u3002", "result": "\u8bc6\u522b\u4e86\u5173\u952e\u67b6\u6784\u6a21\u5f0f\u3001\u6027\u80fd\u6743\u8861\u548c\u5f00\u653e\u7814\u7a76\u6311\u6218\uff0c\u5305\u62ecOS-DBMS\u67b6\u6784\u5206\u6b67\u7684\u5386\u53f2\u6f14\u53d8\u3001eBPF\u5185\u6838\u53ef\u6269\u5c55\u6027\u3001NVM\u611f\u77e5\u5206\u5c42\u7b56\u7565\u3001RDMA\u5185\u5b58\u5206\u89e3\u7b49\u65b0\u5174\u8d8b\u52bf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6574\u5408\u673a\u5668\u5b66\u4e60\u4e0e\u5185\u6838\u53ef\u6269\u5c55\u673a\u5236\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u5b9e\u73b0\u73b0\u4ee3\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u5f02\u6784\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u81ea\u9002\u5e94\u8de8\u5c42\u7f13\u51b2\u533a\u7ba1\u7406\u3002"}}
{"id": "2512.22142", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22142", "abs": "https://arxiv.org/abs/2512.22142", "authors": ["Leyang Xue", "Meghana Madhyastha", "Myungjin Lee", "Amos Storkey", "Randal Burns", "Mahesh K. Marina"], "title": "On Harnessing Idle Compute at the Edge for Foundation Model Training", "comment": "Extended abstract version of this paper appeared in ACM MobiCom 2025", "summary": "The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.\n  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.\n  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.", "AI": {"tldr": "Cleave\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6df7\u5408\u5f20\u91cf\u5e76\u884c\u548c\u53c2\u6570\u670d\u52a1\u5668\u67b6\u6784\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e0e\u4e91\u7aef\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u5185\u5b58\u9650\u5236\u548c\u901a\u4fe1\u5f00\u9500\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u9ad8\u5ea6\u4e2d\u5fc3\u5316\uff0c\u4f9d\u8d56\u5927\u578b\u4e91\u6570\u636e\u4e2d\u5fc3\uff0c\u6210\u672c\u9ad8\u6602\u3002\u5229\u7528\u8fb9\u7f18\u8bbe\u5907\u7684\u95f2\u7f6e\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u3001\u53ef\u6269\u5c55\u6027\u5dee\u3001\u5185\u5b58\u9650\u5236\u3001\u901a\u4fe1\u5f00\u9500\u5927\u7b49\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8bbe\u5907\u5f02\u6784\u6027\u548c\u52a8\u6001\u6027\u3002", "method": "\u63d0\u51faCleave\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u9009\u62e9\u6027\u6df7\u5408\u5f20\u91cf\u5e76\u884c\u65b9\u6cd5\u7cbe\u7ec6\u5212\u5206\u8bad\u7ec3\u64cd\u4f5c\uff1b2\uff09\u57fa\u4e8e\u53c2\u6570\u670d\u52a1\u5668\u7684\u8bad\u7ec3\u67b6\u6784\u5e94\u5bf9\u8bbe\u5907\u5185\u5b58\u9650\u5236\u5e76\u907f\u514d\u901a\u4fe1\u74f6\u9888\uff1b3\uff09\u901a\u8fc7\u6210\u672c\u4f18\u5316\u6a21\u578b\u6307\u5bfc\u8bbe\u5907\u9009\u62e9\u548c\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\uff0c\u6709\u6548\u5904\u7406\u8bbe\u5907\u5f02\u6784\u6027\u548c\u52a8\u6001\u53d8\u5316\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff1a1\uff09Cleave\u5339\u914d\u4e91\u7aefGPU\u8bad\u7ec3\u6027\u80fd\uff0c\u53ef\u6269\u5c55\u5230\u66f4\u5927\u6a21\u578b\u548c\u6570\u5343\u53f0\u8bbe\u5907\uff1b2\uff09\u652f\u6301\u6bd4\u57fa\u7ebf\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u591a8\u500d\u7684\u8bbe\u5907\uff1b3\uff09\u6bcf\u6279\u6b21\u8bad\u7ec3\u65f6\u95f4\u6bd4\u6700\u5148\u8fdb\u7684\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u5feb10\u500d\uff1b4\uff09\u9ad8\u6548\u5904\u7406\u8bbe\u5907\u6545\u969c\uff0c\u6062\u590d\u901f\u5ea6\u6bd4\u5148\u524d\u65b9\u6cd5\u5feb\u81f3\u5c11100\u500d\u3002", "conclusion": "Cleave\u901a\u8fc7\u521b\u65b0\u7684\u9009\u62e9\u6027\u6df7\u5408\u5f20\u91cf\u5e76\u884c\u548c\u53c2\u6570\u670d\u52a1\u5668\u67b6\u6784\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8bad\u7ec3\u5927\u578b\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8fb9\u7f18\u8bad\u7ec3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u53bb\u4e2d\u5fc3\u5316\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.22701", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22701", "abs": "https://arxiv.org/abs/2512.22701", "authors": ["Sabine Houy", "Bruno Kreyssig", "Alexandre Bartel"], "title": "CFIghter: Automated Control-Flow Integrity Enablement and Evaluation for Legacy C/C++ Systems", "comment": "Under review at IEEE Euro S&P 2026", "summary": "Compiler-based Control-Flow Integrity (CFI) offers strong forward-edge protection but remains challenging to deploy in large C/C++ software due to visibility mismatches, type inconsistencies, and unintended behavioral failures. We present CFIghter, the first fully automated system that enables strict, type-based CFI in real-world projects by detecting, classifying, and repairing unintended policy violations exposed by the test suite. CFIghter integrates whole-program analysis with guided runtime monitoring and iteratively applies the minimal necessary adjustments to CFI enforcement only where required, stopping once all tests pass or remaining failures are deemed unresolvable. We evaluate CFIghter on four GNU projects. It resolves all visibility-related build errors and automatically repairs 95.8% of unintended CFI violations in the large, multi-library util-linux codebase, while retaining strict enforcement at over 89% of indirect control-flow sites. Across all subjects, CFIghter preserves strict type-based CFI for the majority of the codebase without requiring manual source-code changes, relying only on automatically generated visibility adjustments and localized enforcement scopes where necessary. These results show that automated compatibility repair makes strict compiler CFI practically deployable in mature, modular C software.", "AI": {"tldr": "CFIghter\uff1a\u9996\u4e2a\u5168\u81ea\u52a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u4fee\u590d\u6d4b\u8bd5\u5957\u4ef6\u66b4\u9732\u7684\u610f\u5916\u7b56\u7565\u8fdd\u89c4\uff0c\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u5b9e\u73b0\u4e25\u683c\u7c7b\u578b\u5316CFI\uff0c\u65e0\u9700\u624b\u52a8\u4fee\u6539\u6e90\u4ee3\u7801\u3002", "motivation": "\u7f16\u8bd1\u5668\u63a7\u5236\u6d41\u5b8c\u6574\u6027\uff08CFI\uff09\u63d0\u4f9b\u5f3a\u5927\u7684\u524d\u5411\u8fb9\u4fdd\u62a4\uff0c\u4f46\u7531\u4e8e\u53ef\u89c1\u6027\u4e0d\u5339\u914d\u3001\u7c7b\u578b\u4e0d\u4e00\u81f4\u548c\u610f\u5916\u884c\u4e3a\u6545\u969c\uff0c\u5728\u5927\u578bC/C++\u8f6f\u4ef6\u4e2d\u90e8\u7f72\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "CFIghter\u96c6\u6210\u5168\u7a0b\u5e8f\u5206\u6790\u4e0e\u5f15\u5bfc\u5f0f\u8fd0\u884c\u65f6\u76d1\u63a7\uff0c\u8fed\u4ee3\u5e94\u7528\u6700\u5c0f\u5fc5\u8981\u8c03\u6574\u5230CFI\u5f3a\u5236\u6267\u884c\uff0c\u4ec5\u5728\u9700\u8981\u7684\u5730\u65b9\u8fdb\u884c\u8c03\u6574\uff0c\u76f4\u5230\u6240\u6709\u6d4b\u8bd5\u901a\u8fc7\u6216\u5269\u4f59\u6545\u969c\u88ab\u8ba4\u4e3a\u65e0\u6cd5\u89e3\u51b3\u3002", "result": "\u5728\u56db\u4e2aGNU\u9879\u76ee\u4e0a\u8bc4\u4f30\uff1a\u89e3\u51b3\u4e86\u6240\u6709\u53ef\u89c1\u6027\u76f8\u5173\u7684\u6784\u5efa\u9519\u8bef\uff0c\u5728\u5927\u578b\u591a\u5e93util-linux\u4ee3\u7801\u5e93\u4e2d\u81ea\u52a8\u4fee\u590d\u4e8695.8%\u7684\u610f\u5916CFI\u8fdd\u89c4\uff0c\u540c\u65f6\u5728\u8d85\u8fc789%\u7684\u95f4\u63a5\u63a7\u5236\u6d41\u7ad9\u70b9\u4fdd\u6301\u4e25\u683c\u6267\u884c\u3002", "conclusion": "\u81ea\u52a8\u5316\u517c\u5bb9\u6027\u4fee\u590d\u4f7f\u4e25\u683c\u7f16\u8bd1\u5668CFI\u5728\u6210\u719f\u3001\u6a21\u5757\u5316\u7684C\u8f6f\u4ef6\u4e2d\u5b9e\u9645\u53ef\u90e8\u7f72\uff0c\u65e0\u9700\u624b\u52a8\u6e90\u4ee3\u7801\u66f4\u6539\uff0c\u4ec5\u4f9d\u8d56\u81ea\u52a8\u751f\u6210\u7684\u53ef\u89c1\u6027\u8c03\u6574\u548c\u5c40\u90e8\u5316\u6267\u884c\u8303\u56f4\u3002"}}
{"id": "2512.23289", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23289", "abs": "https://arxiv.org/abs/2512.23289", "authors": ["Jiacheng Ding", "Cong Guo", "Xiaofei Zhang"], "title": "ChronoConnect: Tracking Pathways Along Highly Dynamic Vertices in Temporal Graphs", "comment": "4 pages, 4 figures. Demo paper accepted at ICDM 2025", "summary": "With the proliferation of temporal graph data, there is a growing demand for analyzing information propagation patterns during graph evolution. Existing graph analysis systems, mostly based on static snapshots, struggle to effectively capture information flows along the temporal dimension. To address this challenge, we introduce ChronoConnect, a novel system that enables tracking temporal pathways in temporal graph, especially beneficial to downstream mining tasks, e.g., understanding what are the critical pathways in propagating information towards a specific group of vertices. Built on ChronoConnect, users can conveniently configure and execute a variety of temporal traversal algorithms to efficiently analyze information diffusion processes under time constraints. Moreover, ChronoConnect utilizes parallel processing to tackle the explosive size-growth of evolving graphs. We showcase the effectiveness and enhanced performance of ChronoConnect through the implementation of algorithms that track pathways along highly dynamic vertices in temporal graphs. Furthermore, we offer an interactive user interface for graph visualization and query result exploration. We envision ChronoConnect to become a powerful tool for users to examine how information spreads over a temporal graph.", "AI": {"tldr": "ChronoConnect\u662f\u4e00\u4e2a\u7528\u4e8e\u8ffd\u8e2a\u65f6\u5e8f\u56fe\u4e2d\u4fe1\u606f\u4f20\u64ad\u8def\u5f84\u7684\u7cfb\u7edf\uff0c\u652f\u6301\u5e76\u884c\u5904\u7406\u52a8\u6001\u56fe\u6570\u636e\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u754c\u9762\u548c\u591a\u79cd\u65f6\u5e8f\u904d\u5386\u7b97\u6cd5\u914d\u7f6e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9759\u6001\u5feb\u7167\u7684\u56fe\u5206\u6790\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u6355\u6349\u65f6\u5e8f\u7ef4\u5ea6\u4e0a\u7684\u4fe1\u606f\u6d41\u52a8\u6a21\u5f0f\uff0c\u800c\u65f6\u5e8f\u56fe\u6570\u636e\u7684\u666e\u53ca\u9700\u8981\u80fd\u591f\u5206\u6790\u4fe1\u606f\u4f20\u64ad\u6a21\u5f0f\u7684\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1ChronoConnect\u7cfb\u7edf\uff0c\u652f\u6301\u7528\u6237\u914d\u7f6e\u548c\u6267\u884c\u591a\u79cd\u65f6\u5e8f\u904d\u5386\u7b97\u6cd5\uff0c\u5229\u7528\u5e76\u884c\u5904\u7406\u6280\u672f\u5904\u7406\u52a8\u6001\u56fe\u7684\u89c4\u6a21\u7206\u70b8\u589e\u957f\uff0c\u5e76\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u754c\u9762\u3002", "result": "\u901a\u8fc7\u5b9e\u73b0\u8ffd\u8e2a\u65f6\u5e8f\u56fe\u4e2d\u9ad8\u5ea6\u52a8\u6001\u9876\u70b9\u8def\u5f84\u7684\u7b97\u6cd5\uff0c\u5c55\u793a\u4e86ChronoConnect\u7684\u6709\u6548\u6027\u548c\u589e\u5f3a\u6027\u80fd\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5206\u6790\u65f6\u95f4\u7ea6\u675f\u4e0b\u7684\u4fe1\u606f\u6269\u6563\u8fc7\u7a0b\u3002", "conclusion": "ChronoConnect\u5c06\u6210\u4e3a\u7528\u6237\u68c0\u67e5\u4fe1\u606f\u5728\u65f6\u5e8f\u56fe\u4e2d\u5982\u4f55\u4f20\u64ad\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e0b\u6e38\u6316\u6398\u4efb\u52a1\u5982\u7406\u89e3\u5411\u7279\u5b9a\u9876\u70b9\u7fa4\u4f20\u64ad\u4fe1\u606f\u7684\u5173\u952e\u8def\u5f84\u3002"}}
{"id": "2512.22147", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.22147", "abs": "https://arxiv.org/abs/2512.22147", "authors": ["Ruifan Chu", "Anbang Wang", "Xiuxiu Bai", "Shuai Liu", "Xiaoshe Dong"], "title": "GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs", "comment": null, "summary": "In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7aef\u5230\u7aef\u7684LLM\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6700\u5c0f\u53ef\u6267\u884c\u7a0b\u5e8f\u6765\u4f18\u5316GPU\u70ed\u70b9\u5185\u6838\uff0c\u65e0\u9700\u5b8c\u6574\u5e94\u7528\u6784\u5efa\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u6027\u80fd\u63d0\u5347", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e2dGPU\u70ed\u70b9\u5185\u6838\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u4e13\u5bb6\u624b\u52a8\u8c03\u4f18\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u79fb\u690d\u3002\u73b0\u6709LLM\u65b9\u6cd5\u5047\u8bbe\u5185\u6838\u53ef\u4ee5\u5ec9\u4ef7\u7f16\u8bd1\u6267\u884c\uff0c\u4f46\u5728\u5927\u578b\u5e94\u7528\u4e2d\u5b8c\u6574\u6784\u5efa\u548c\u8fd0\u884c\u6210\u672c\u8fc7\u9ad8", "method": "\u4ece\u72ec\u7acb\u63d0\u53d6\u7684\u70ed\u70b9\u5185\u6838\u81ea\u52a8\u6784\u5efa\u6700\u5c0f\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u8fdb\u884c\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\u548c\u8bc4\u4f30\u3002\u96c6\u6210\u81ea\u52a8\u9519\u8bef\u4fee\u590d\u548c\u6027\u80fd\u6a21\u5f0f\u7ee7\u627f\uff0c\u4fee\u590d\u6545\u969c\u3001\u4fdd\u6301\u6b63\u786e\u6027\u3001\u91cd\u7528\u6709\u6548\u7684\u5206\u5757/\u5185\u5b58/\u540c\u6b65\u7b56\u7565\uff0c\u964d\u4f4e\u641c\u7d22\u6210\u672c", "result": "\u5728NVIDIA GPU\u548c\u56fd\u4ea7\u6d77\u5149DCU\u5e73\u53f0\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\uff1aPolyBench\u5728NVIDIA\u4e0a5.05x\uff0c\u5728DCU\u4e0a7.77x\uff0cAMD APP SDK\u4e0a1.77x\uff0c\u4e09\u4e2a\u70ed\u70b9\u5185\u6838\u4e0a1.25x\uff0c\u8d85\u8d8a\u76f4\u63a5LLM\u4f18\u5316", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5b8c\u6574\u6e90\u7801\u4f9d\u8d56\uff0c\u63d0\u4f9b\u8de8\u5e73\u53f0\u53ef\u79fb\u690d\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u3001\u4f4e\u6210\u672c\u7684GPU\u5185\u6838\u4f18\u5316\uff0c\u4e3a\u5927\u89c4\u6a21\u9ad8\u6027\u80fd\u8ba1\u7b97\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u4f18\u5316\u65b9\u6848"}}
{"id": "2512.22753", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22753", "abs": "https://arxiv.org/abs/2512.22753", "authors": ["Moustapha Awwalou Diouf", "Maimouna Tamah Diao", "Iyiola Emmanuel Olatunji", "Abdoul Kader Kabor\u00e9", "Jordan Samhi", "Gervais Mendy", "Samuel Ouya", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "title": "From Rookie to Expert: Manipulating LLMs for Automated Vulnerability Exploitation in Enterprise Software", "comment": null, "summary": "LLMs democratize software engineering by enabling non-programmers to create applications, but this same accessibility fundamentally undermines security assumptions that have guided software engineering for decades. We show in this work how publicly available LLMs can be socially engineered to transform novices into capable attackers, challenging the foundational principle that exploitation requires technical expertise. To that end, we propose RSA (Role-assignment, Scenario-pretexting, and Action-solicitation), a pretexting strategy that manipulates LLMs into generating functional exploits despite their safety mechanisms. Testing against Odoo -- a widely used ERP platform, we evaluated five mainstream LLMs (GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek) and achieved a 100% success rate: tested CVE yielded at least one working exploit within 3-4 prompting rounds. While prior work [13] found LLM-assisted attacks difficult and requiring manual effort, we demonstrate that this overhead can be eliminated entirely.\n  Our findings invalidate core software engineering security principles: the distinction between technical and non-technical actors no longer provides valid threat models; technical complexity of vulnerability descriptions offers no protection when LLMs can abstract it away; and traditional security boundaries dissolve when the same tools that build software can be manipulated to break it. This represents a paradigm shift in software engineering -- we must redesign security practices for an era where exploitation requires only the ability to craft prompts, not understand code.\n  Artifacts available at: https://anonymous.4open.science/r/From-Rookie-to-Attacker-D8B3.", "AI": {"tldr": "LLMs\u4f7f\u975e\u7a0b\u5e8f\u5458\u80fd\u751f\u6210\u529f\u80fd\u6027\u7684\u6f0f\u6d1e\u5229\u7528\u7a0b\u5e8f\uff0c\u6210\u529f\u7387\u8fbe\u5230100%\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u8f6f\u4ef6\u5b89\u5168\u5047\u8bbe", "motivation": "LLMs\u7684\u666e\u53ca\u4f7f\u975e\u7a0b\u5e8f\u5458\u4e5f\u80fd\u521b\u5efa\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f46\u8fd9\u79cd\u53ef\u8bbf\u95ee\u6027\u7834\u574f\u4e86\u6570\u5341\u5e74\u6765\u6307\u5bfc\u8f6f\u4ef6\u5de5\u7a0b\u7684\u5b89\u5168\u5047\u8bbe\u3002\u9700\u8981\u7814\u7a76LLMs\u5982\u4f55\u53ef\u80fd\u88ab\u793e\u4f1a\u5de5\u7a0b\u653b\u51fb\u64cd\u7eb5\uff0c\u5c06\u65b0\u624b\u8f6c\u53d8\u4e3a\u6709\u80fd\u529b\u7684\u653b\u51fb\u8005", "method": "\u63d0\u51faRSA\u7b56\u7565\uff08\u89d2\u8272\u5206\u914d\u3001\u573a\u666f\u4f2a\u88c5\u548c\u884c\u52a8\u8bf1\u5bfc\uff09\uff0c\u901a\u8fc7\u793e\u4f1a\u5de5\u7a0b\u653b\u51fb\u64cd\u7eb5LLMs\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u751f\u6210\u529f\u80fd\u6027\u7684\u6f0f\u6d1e\u5229\u7528\u7a0b\u5e8f\u3002\u5728Odoo ERP\u5e73\u53f0\u4e0a\u6d4b\u8bd5\u4e865\u4e2a\u4e3b\u6d41LLM\uff08GPT-4o\u3001Gemini\u3001Claude\u3001Microsoft Copilot\u548cDeepSeek\uff09", "result": "\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\uff1a\u6240\u6709\u6d4b\u8bd5\u7684CVE\u6f0f\u6d1e\u90fd\u80fd\u57283-4\u8f6e\u63d0\u793a\u5185\u751f\u6210\u81f3\u5c11\u4e00\u4e2a\u53ef\u5de5\u4f5c\u7684\u6f0f\u6d1e\u5229\u7528\u7a0b\u5e8f\u3002\u76f8\u6bd4\u4e4b\u524d\u9700\u8981\u4eba\u5de5\u52aa\u529b\u7684\u7814\u7a76\uff0c\u5b8c\u5168\u6d88\u9664\u4e86\u653b\u51fb\u5f00\u9500", "conclusion": "\u8fd9\u4ee3\u8868\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7684\u8303\u5f0f\u8f6c\u53d8\uff1a\u6280\u672f\u4e0e\u975e\u6280\u672f\u4eba\u5458\u7684\u533a\u5206\u4e0d\u518d\u6709\u6548\uff1b\u6f0f\u6d1e\u63cf\u8ff0\u7684\u6280\u672f\u590d\u6742\u6027\u65e0\u6cd5\u63d0\u4f9b\u4fdd\u62a4\uff1b\u4f20\u7edf\u5b89\u5168\u8fb9\u754c\u74e6\u89e3\u3002\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u5b89\u5168\u5b9e\u8df5\uff0c\u56e0\u4e3a\u73b0\u5728\u653b\u51fb\u53ea\u9700\u8981\u5236\u4f5c\u63d0\u793a\u7684\u80fd\u529b\uff0c\u800c\u4e0d\u9700\u8981\u7406\u89e3\u4ee3\u7801"}}
{"id": "2512.23298", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23298", "abs": "https://arxiv.org/abs/2512.23298", "authors": ["Anbang Song", "Ziqiang Yu", "Wei Liu", "Yating Xu", "Mingjin Tao"], "title": "BRkNN-light: Batch Processing of Reverse k-Nearest Neighbor Queries for Moving Objects on Road Networks", "comment": null, "summary": "The Reverse $k$-Nearest Neighbor (R$k$NN) query over moving objects on road networks seeks to find all moving objects that consider the specified query point as one of their $k$ nearest neighbors. In location based services, many users probably submit R$k$NN queries simultaneously. However, existing methods largely overlook how to efficiently process multiple such queries together, missing opportunities to share redundant computations and thus reduce overall processing costs. To address this, this work is the first to explore batch processing of multiple R$k$NN queries, aiming to minimize total computation by sharing duplicate calculations across queries. To tackle this issue, we propose the BR$k$NN-Light algorithm, which uses rapid verification and pruning strategies based on geometric constraints, along with an optimized range search technique, to speed up the process of identifying the R$k$NNs for each query. Furthermore, it proposes a dynamic distance caching mechanism to enable computation reuse when handling multiple queries, thereby significantly reducing unnecessary computations. Experiments on multiple real-world road networks demonstrate the superiority of the BR$k$NN-Light algorithm on the processing of batch queries.", "AI": {"tldr": "\u63d0\u51faBR$k$NN-Light\u7b97\u6cd5\uff0c\u9996\u6b21\u63a2\u7d22\u9053\u8def\u7f51\u7edc\u4e0a\u79fb\u52a8\u5bf9\u8c61\u7684\u6279\u91cfR$k$NN\u67e5\u8be2\u5904\u7406\uff0c\u901a\u8fc7\u5171\u4eab\u8ba1\u7b97\u51cf\u5c11\u603b\u4f53\u5904\u7406\u6210\u672c", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u5982\u4f55\u9ad8\u6548\u5904\u7406\u591a\u4e2a\u540c\u65f6\u63d0\u4ea4\u7684R$k$NN\u67e5\u8be2\uff0c\u9519\u5931\u4e86\u5171\u4eab\u5197\u4f59\u8ba1\u7b97\u4ee5\u51cf\u5c11\u603b\u4f53\u5904\u7406\u6210\u672c\u7684\u673a\u4f1a", "method": "\u63d0\u51faBR$k$NN-Light\u7b97\u6cd5\uff0c\u91c7\u7528\u57fa\u4e8e\u51e0\u4f55\u7ea6\u675f\u7684\u5feb\u901f\u9a8c\u8bc1\u548c\u526a\u679d\u7b56\u7565\uff0c\u4f18\u5316\u8303\u56f4\u641c\u7d22\u6280\u672f\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u8ddd\u79bb\u7f13\u5b58\u673a\u5236\u5b9e\u73b0\u8ba1\u7b97\u91cd\u7528", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u9053\u8def\u7f51\u7edc\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBR$k$NN-Light\u7b97\u6cd5\u5728\u6279\u91cf\u67e5\u8be2\u5904\u7406\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63a2\u7d22\u6279\u91cfR$k$NN\u67e5\u8be2\u5904\u7406\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u901a\u8fc7\u8ba1\u7b97\u5171\u4eab\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0c\u4e3a\u4f4d\u7f6e\u670d\u52a1\u4e2d\u7684\u9ad8\u6548\u67e5\u8be2\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.22149", "categories": ["cs.DC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.22149", "abs": "https://arxiv.org/abs/2512.22149", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan"], "title": "Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments", "comment": "7 pages, 2 figures", "summary": "Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94GPU\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u7528\u4e8e\u670d\u52a1\u5668\u65e0GPU\u5e73\u53f0\u4e0a\u90e8\u7f72\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u76f8\u6bd4\u8f6e\u8be2\u8c03\u5ea6\u51cf\u5c1185%\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u9759\u6001\u5206\u914d\u76f8\u5f53\u7684\u541e\u5410\u91cf", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u670d\u52a1\u5668\u65e0GPU\u5e73\u53f0\u4e0a\u90e8\u7f72\u9762\u4e34\u8d44\u6e90\u5206\u914d\u6311\u6218\uff0c\u5305\u62ec\u5f02\u6784\u667a\u80fd\u4f53\u5de5\u4f5c\u8d1f\u8f7d\u3001\u4e0d\u540c\u8ba1\u7b97\u9700\u6c42\u4ee5\u53ca\u6210\u672c\u6548\u76ca\u6269\u5c55\u7684\u9700\u6c42", "method": "\u63d0\u51fa\u81ea\u9002\u5e94GPU\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u57fa\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u3001\u667a\u80fd\u4f53\u4f18\u5148\u7ea7\u548c\u6700\u5c0f\u8d44\u6e90\u9700\u6c42\u52a8\u6001\u5206\u914dGPU\u8d44\u6e90\uff0c\u4f7f\u7528O(N)\u590d\u6742\u5ea6\u7b97\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u9002\u5e94", "result": "\u76f8\u6bd4\u8f6e\u8be2\u8c03\u5ea6\u51cf\u5c1185%\u5ef6\u8fdf\uff0c\u4fdd\u6301\u4e0e\u9759\u6001\u5206\u914d\u76f8\u5f53\u7684\u541e\u5410\u91cf\uff0c\u5728\u5ef6\u8fdf\u3001\u6210\u672c\u548cGPU\u5229\u7528\u7387\u6307\u6807\u4e0a\u4f18\u4e8e\u9759\u6001\u5747\u8861\u548c\u8f6e\u8be2\u7b56\u7565", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u670d\u52a1\u5668\u65e0GPU\u57fa\u7840\u8bbe\u65bd\u4e0a\u90e8\u7f72\u6210\u672c\u6548\u76ca\u9ad8\u7684\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.22827", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22827", "abs": "https://arxiv.org/abs/2512.22827", "authors": ["Yue Wu", "Minghao Han", "Ruiyin Li", "Peng Liang", "Amjed Tahir", "Zengyang Li", "Qiong Feng", "Mojtaba Shahin"], "title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework", "comment": "32 pages, 5 images, 7 tables, Manuscript submitted to a Journal (2025)", "summary": "Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.", "AI": {"tldr": "FasterPy\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684Python\u4ee3\u7801\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4f4e\u79e9\u9002\u5e94\u6280\u672f\uff0c\u5728PIE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u548c\u7ef4\u62a4\u7279\u5b9a\u6027\u80fdbug\u7684\u89c4\u5219\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u9002\u7528\u8303\u56f4\u6709\u9650\u3002\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u7a0b\u5e8f\u8868\u793a\u548c\u7cbe\u5fc3\u6784\u5efa\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "FasterPy\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u3002RAG\u7531\u73b0\u6709\u6027\u80fd\u6539\u8fdb\u4ee3\u7801\u5bf9\u548c\u76f8\u5e94\u6027\u80fd\u6d4b\u91cf\u6784\u5efa\u7684\u77e5\u8bc6\u5e93\u652f\u6301\uff0cLoRA\u7528\u4e8e\u589e\u5f3a\u4ee3\u7801\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728Performance Improving Code Edits\uff08PIE\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "FasterPy\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9002\u5e94\u5927\u8bed\u8a00\u6a21\u578b\u6765\u4f18\u5316Python\u4ee3\u7801\u7684\u6267\u884c\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23319", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23319", "abs": "https://arxiv.org/abs/2512.23319", "authors": ["Ziqiang Yu", "Xiaohui Yu", "Yueting Chen", "Wei Liu", "Anbang Song", "Bolong Zheng"], "title": "Flexible Keyword-Aware Top-$k$ Route Search", "comment": null, "summary": "With the rise of Large Language Models (LLMs), tourists increasingly use it for route planning by entering keywords for attractions, instead of relying on traditional manual map services. LLMs provide generally reasonable suggestions, but often fail to generate optimal plans that account for detailed user requirements, given the vast number of potential POIs and possible routes based on POI combinations within a real-world road network. In this case, a route-planning API could serve as an external tool, accepting a sequence of keywords and returning the top-$k$ best routes tailored to user requests. To address this need, this paper introduces the Keyword-Aware Top-$k$ Routes (KATR) query that provides a more flexible and comprehensive semantic to route planning that caters to various user's preferences including flexible POI visiting order, flexible travel distance budget, and personalized POI ratings. Subsequently, we propose an explore-and-bound paradigm to efficiently process KATR queries by eliminating redundant candidates based on estimated score bounds from global to local levels. Extensive experiments demonstrate our approach's superior performance over existing methods across different scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faKATR\u67e5\u8be2\uff0c\u901a\u8fc7\u5173\u952e\u8bcd\u611f\u77e5\u7684top-k\u8def\u7ebf\u89c4\u5212\u89e3\u51b3LLM\u5728\u65c5\u6e38\u8def\u7ebf\u89c4\u5212\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u91c7\u7528\u63a2\u7d22-\u8fb9\u754c\u8303\u5f0f\u9ad8\u6548\u5904\u7406\u67e5\u8be2\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65c5\u6e38\u8def\u7ebf\u89c4\u5212\u4e2d\u7684\u666e\u53ca\uff0c\u7528\u6237\u901a\u8fc7\u8f93\u5165\u666f\u70b9\u5173\u952e\u8bcd\u83b7\u53d6\u5efa\u8bae\uff0c\u4f46LLM\u96be\u4ee5\u5728\u771f\u5b9e\u9053\u8def\u7f51\u7edc\u4e2d\u8003\u8651\u5927\u91cfPOI\u7ec4\u5408\u65f6\u751f\u6210\u6700\u4f18\u8def\u7ebf\uff0c\u9700\u8981\u5916\u90e8\u5de5\u5177\u652f\u6301\u3002", "method": "\u63d0\u51faKATR\u67e5\u8be2\u652f\u6301\u7075\u6d3b\u7684POI\u8bbf\u95ee\u987a\u5e8f\u3001\u65c5\u884c\u8ddd\u79bb\u9884\u7b97\u548c\u4e2a\u6027\u5316POI\u8bc4\u5206\uff1b\u91c7\u7528\u63a2\u7d22-\u8fb9\u754c\u8303\u5f0f\uff0c\u901a\u8fc7\u4ece\u5168\u5c40\u5230\u5c40\u90e8\u7684\u5206\u6570\u8fb9\u754c\u4f30\u8ba1\u6d88\u9664\u5197\u4f59\u5019\u9009\u8def\u7ebf\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "KATR\u67e5\u8be2\u4e3aLLM\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u5168\u9762\u7684\u8def\u7ebf\u89c4\u5212\u8bed\u4e49\uff0c\u63a2\u7d22-\u8fb9\u754c\u8303\u5f0f\u80fd\u9ad8\u6548\u5904\u7406\u590d\u6742\u67e5\u8be2\uff0c\u6ee1\u8db3\u7528\u6237\u591a\u6837\u5316\u9700\u6c42\u3002"}}
{"id": "2512.22168", "categories": ["cs.DC", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.22168", "abs": "https://arxiv.org/abs/2512.22168", "authors": ["Wei Li", "Zhenyu Bai", "Heru Wang", "Pranav Dangi", "Zhiqiang Zhang", "Cheng Tan", "Huiying Lan", "Weng-Fai Wong", "Tulika Mitra"], "title": "TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures", "comment": null, "summary": "Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.", "AI": {"tldr": "TL\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u57fa\u4e8etile\u7684\u7a0b\u5e8f\uff08\u5982Triton\u5185\u6838\uff09\u7f16\u8bd1\u5230\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u4e0a\uff0c\u89e3\u51b3tile\u5b9e\u4f8b\u5728\u5206\u5e03\u5f0f\u6838\u5fc3\u95f4\u7684\u6620\u5c04\u95ee\u9898\uff0c\u63d0\u9ad8\u6570\u636e\u91cd\u7528\u5e76\u51cf\u5c11\u901a\u4fe1\u3002", "motivation": "\u7a7a\u95f4\u6570\u636e\u6d41\u52a0\u901f\u5668\u901a\u8fc7\u663e\u5f0f\u7684\u7f16\u8bd1\u5668\u7ba1\u7406\u6570\u636e\u79fb\u52a8\u51cf\u5c11\u5185\u5b58\u74f6\u9888\uff0c\u4f46\u7aef\u5230\u7aef\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u5230\u786c\u4ef6\u7684\u6620\u5c04\u3002\u73b0\u6709\u7f16\u8bd1\u5668\u4e3b\u8981\u4f18\u5316\u5355\u4e2atile\u5185\u7684\u4ee3\u7801\u751f\u6210\uff0c\u800ctile\u5b9e\u4f8b\u5728\u5206\u5e03\u5f0f\u6838\u5fc3\u95f4\u7684\u6620\u5c04\u95ee\u9898\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684\u53ef\u7f16\u7a0b\u6027\u548c\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "TL\u63d0\u51fa\u4e00\u4e2a\u786c\u4ef6\u8868\u793a\u65b9\u6cd5\uff0c\u6355\u83b7\u4e92\u8fde\u62d3\u6251\u3001\u5185\u5b58\u5c42\u6b21\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u652f\u6301\u7279\u5b9a\u67b6\u6784\u4f18\u5316\u548c\u591a\u6837\u5316\u7a7a\u95f4\u6570\u636e\u6d41\u76ee\u6807\u3002\u57fa\u4e8eMLIR\u751f\u6001\u7cfb\u7edf\u6784\u5efa\uff0c\u5b9a\u4e49\u901a\u7528\u524d\u7aef\u5165\u53e3\u70b9\u548c\u540e\u7aef\u7ec8\u70b9\uff0c\u4e13\u6ce8\u4e8e\u89e3\u51b3tile\u5b9e\u4f8b\u5728\u7a7a\u95f4\u5206\u5e03\u5f0f\u6838\u5fc3\u95f4\u7684\u5206\u5e03\u95ee\u9898\uff0c\u5229\u7528\u7247\u4e0a\u7f51\u7edc\u548c\u5206\u5e03\u5f0f\u5185\u5b58\u589e\u52a0\u6570\u636e\u91cd\u7528\u5e76\u51cf\u5c11\u901a\u4fe1\u3002", "result": "TL\u6846\u67b6\u80fd\u591f\u5c06\u57fa\u4e8etile\u7684\u7a0b\u5e8f\u6709\u6548\u7f16\u8bd1\u5230\u7a7a\u95f4\u6570\u636e\u6d41\u67b6\u6784\u4e0a\uff0c\u901a\u8fc7\u4f18\u5316tile\u5b9e\u4f8b\u5728\u5206\u5e03\u5f0f\u6838\u5fc3\u95f4\u7684\u6620\u5c04\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u91cd\u7528\u5e76\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u4ece\u800c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "TL\u901a\u8fc7\u89e3\u51b3\u7a7a\u95f4\u6570\u636e\u6d41\u52a0\u901f\u5668\u4e2dtile\u5b9e\u4f8b\u5728\u5206\u5e03\u5f0f\u6838\u5fc3\u95f4\u7684\u6620\u5c04\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u6b64\u7c7b\u67b6\u6784\u7684\u53ef\u7f16\u7a0b\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u7a7a\u95f4\u6570\u636e\u6d41\u52a0\u901f\u5668\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2512.22845", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.22845", "abs": "https://arxiv.org/abs/2512.22845", "authors": ["Zan Xu", "Sari Nurfauziyyah", "Anastasia Romanova", "Kaamesh G S", "Yiqun Gao", "Maria Spichkova"], "title": "Towards the analysis of team members well-being", "comment": null, "summary": "Many recent research studies have focused on the well-being of software development team members, as this aspect may be critical not only for productivity and performance at work but also for the physical health and personal life of employees. Many studies agree that an important factor of team member well-being is whether team members feel appreciated and acknowledged for their contributions. This paper presents the results of a project on the team well-being analysis as well as the prototype developed within the project.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5173\u6ce8\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u6210\u5458\u7684\u5e78\u798f\u611f\uff0c\u5f00\u53d1\u4e86\u56e2\u961f\u5e78\u798f\u611f\u5206\u6790\u539f\u578b\u7cfb\u7edf\uff0c\u5f3a\u8c03\u8ba4\u53ef\u548c\u8d5e\u8d4f\u5bf9\u5458\u5de5\u5e78\u798f\u611f\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u6210\u5458\u7684\u5e78\u798f\u611f\u4e0d\u4ec5\u5f71\u54cd\u5de5\u4f5c\u751f\u4ea7\u529b\u548c\u7ee9\u6548\uff0c\u8fd8\u5173\u7cfb\u5230\u5458\u5de5\u7684\u8eab\u4f53\u5065\u5eb7\u548c\u4e2a\u4eba\u751f\u6d3b\u3002\u7814\u7a76\u8868\u660e\uff0c\u56e2\u961f\u6210\u5458\u662f\u5426\u611f\u5230\u81ea\u5df1\u7684\u8d21\u732e\u88ab\u8ba4\u53ef\u548c\u8d5e\u8d4f\u662f\u5f71\u54cd\u5e78\u798f\u611f\u7684\u91cd\u8981\u56e0\u7d20\u3002", "method": "\u5f00\u5c55\u4e86\u4e00\u4e2a\u5173\u4e8e\u56e2\u961f\u5e78\u798f\u611f\u5206\u6790\u7684\u9879\u76ee\uff0c\u5e76\u5728\u9879\u76ee\u4e2d\u5f00\u53d1\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u56e2\u961f\u5e78\u798f\u611f\u5206\u6790\u9879\u76ee\u7684\u7ed3\u679c\u4ee5\u53ca\u5728\u8be5\u9879\u76ee\u4e2d\u5f00\u53d1\u7684\u539f\u578b\u7cfb\u7edf\u3002", "conclusion": "\u56e2\u961f\u6210\u5458\u7684\u8ba4\u53ef\u611f\u548c\u8d21\u732e\u88ab\u8d5e\u8d4f\u662f\u5f71\u54cd\u8f6f\u4ef6\u5f00\u53d1\u56e2\u961f\u5e78\u798f\u611f\u7684\u5173\u952e\u56e0\u7d20\uff0c\u901a\u8fc7\u5206\u6790\u5de5\u5177\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u6539\u5584\u56e2\u961f\u5e78\u798f\u611f\u3002"}}
{"id": "2512.23330", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23330", "abs": "https://arxiv.org/abs/2512.23330", "authors": ["Hadar Rotschield", "Liat Peterfreund"], "title": "Database Theory in Action: From Inexpressibility to Efficiency in GQL's Order-Constrained Paths", "comment": null, "summary": "Pattern matching of core GQL, the new ISO standard for querying property graphs, cannot check whether edge values are increasing along a path, as established in recent work. We present a construc- tive translation that overcomes this limitation by compiling the increasing-edges condition into the input graph. Remarkably, the benefit of this construction goes beyond restoring expressiveness. In our proof-of-concept implementation in Neo4j's Cypher, where such path constraints are expressible but costly, our compiled version runs faster and avoids timeouts. This illustrates how a theoretically motivated translation can not only close an expressiveness gap but also bring practical performance gains.", "AI": {"tldr": "\u5c06GQL\u4e2d\u8def\u5f84\u4e0a\u8fb9\u503c\u9012\u589e\u7684\u6a21\u5f0f\u5339\u914d\u6761\u4ef6\u7f16\u8bd1\u5230\u8f93\u5165\u56fe\u4e2d\uff0c\u65e2\u89e3\u51b3\u4e86\u8868\u8fbe\u6027\u9650\u5236\u53c8\u63d0\u5347\u4e86\u5b9e\u9645\u6027\u80fd", "motivation": "GQL\u6807\u51c6\u5728\u6a21\u5f0f\u5339\u914d\u4e2d\u65e0\u6cd5\u68c0\u67e5\u8def\u5f84\u4e0a\u8fb9\u503c\u662f\u5426\u9012\u589e\uff0c\u5b58\u5728\u8868\u8fbe\u6027\u9650\u5236\u3002\u540c\u65f6\uff0c\u5728\u73b0\u6709\u5b9e\u73b0\uff08\u5982Cypher\uff09\u4e2d\u867d\u7136\u80fd\u8868\u8fbe\u4f46\u6027\u80fd\u4ee3\u4ef7\u9ad8\u3002", "method": "\u63d0\u51fa\u6784\u9020\u6027\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5c06\u8fb9\u503c\u9012\u589e\u6761\u4ef6\u7f16\u8bd1\u5230\u8f93\u5165\u56fe\u4e2d\u3002\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u8f6c\u6362\uff0c\u5c06\u8def\u5f84\u7ea6\u675f\u5d4c\u5165\u5230\u56fe\u7ed3\u6784\u672c\u8eab\u3002", "result": "\u5728Neo4j\u7684Cypher\u4e2d\u5b9e\u73b0\u6982\u5ff5\u9a8c\u8bc1\uff0c\u7f16\u8bd1\u540e\u7684\u7248\u672c\u8fd0\u884c\u66f4\u5feb\u4e14\u907f\u514d\u4e86\u8d85\u65f6\u3002\u4e0d\u4ec5\u6062\u590d\u4e86\u8868\u8fbe\u6027\uff0c\u8fd8\u5e26\u6765\u4e86\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7406\u8bba\u9a71\u52a8\u7684\u7ffb\u8bd1\u4e0d\u4ec5\u80fd\u5f25\u8865\u8868\u8fbe\u6027\u5dee\u8ddd\uff0c\u8fd8\u80fd\u5e26\u6765\u5b9e\u9645\u6027\u80fd\u6536\u76ca\uff0c\u5c55\u793a\u4e86\u7406\u8bba\u6784\u9020\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2512.22173", "categories": ["cs.DC", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.22173", "abs": "https://arxiv.org/abs/2512.22173", "authors": ["Aliaksandr V. Yakutovich", "Jusong Yu", "Daniel Hollas", "Edan Bainglass", "Corsin Battaglia", "Miki Bonacci", "Lucas Fernandez Vilanova", "Stephan Henne", "Anders Kaestner", "Michel Kenzelmann", "Graham Kimbell", "Jakob Lass", "Fabio Lopes", "Daniel G. Mazzone", "Andres Ortega-Guerrero", "Xing Wang", "Nicola Marzari", "Carlo A. Pignedoli", "Giovanni Pizzi"], "title": "AiiDAlab: on the route to accelerate science", "comment": null, "summary": "With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).", "AI": {"tldr": "AiiDAlab\u5e73\u53f0\u5df2\u4ece\u6750\u6599\u79d1\u5b66\u6269\u5c55\u5230\u591a\u5b66\u79d1\uff0c\u901a\u8fc7\u6d4f\u89c8\u5668\u754c\u9762\u7b80\u5316\u590d\u6742\u8ba1\u7b97\u5de5\u4f5c\u6d41\uff0c\u81ea\u52a8\u8ffd\u8e2a\u6a21\u62df\u6765\u6e90\u786e\u4fdd\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u96c6\u6210\u7535\u5b50\u5b9e\u9a8c\u5ba4\u7b14\u8bb0\u672c\u652f\u6301FAIR\u539f\u5219\u3002", "motivation": "\u968f\u7740\u8ba1\u7b97\u80fd\u529b\u589e\u957f\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7814\u7a76\u5de5\u4f5c\u6d41\u6765\u7ba1\u7406\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u4f46\u6267\u884c\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u901a\u5e38\u9700\u8981\u6280\u672f\u4e13\u4e1a\u77e5\u8bc6\u6765\u8bbe\u7f6e\u8f93\u5165\u3001\u89e3\u91ca\u8f93\u51fa\u548c\u5904\u7406\u5e76\u884c\u4ee3\u7801\u6267\u884c\u3002\u73b0\u6709\u5de5\u5177\u5bf9\u975e\u6280\u672f\u7528\u6237\u4e0d\u591f\u53cb\u597d\u3002", "method": "\u5f00\u53d1AiiDAlab\u5e73\u53f0\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684Web\u6d4f\u89c8\u5668\u7528\u6237\u754c\u9762\uff0c\u57fa\u4e8eAiiDA\u5f15\u64ce\u81ea\u52a8\u8ffd\u8e2a\u5b8c\u6574\u6a21\u62df\u6765\u6e90\uff0c\u7b80\u5316\u7528\u6237\u5165\u95e8\u6d41\u7a0b\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u8bbf\u95ee\uff0c\u5904\u7406\u5927\u6570\u636e\u96c6\uff0c\u5e76\u96c6\u6210\u7535\u5b50\u5b9e\u9a8c\u5ba4\u7b14\u8bb0\u672c\u3002", "result": "AiiDAlab\u5df2\u6210\u529f\u6269\u5c55\u5230\u91cf\u5b50\u5316\u5b66\u3001\u5927\u6c14\u5efa\u6a21\u3001\u7535\u6c60\u7814\u7a76\u548c\u5b9e\u9a8c\u6570\u636e\u5206\u6790\u7b49\u591a\u4e2a\u5b66\u79d1\uff0c\u5728\u6559\u80b2\u73af\u5883\u4e2d\u4e5f\u88ab\u79ef\u6781\u4f7f\u7528\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u4e13\u6ce8\u4e8e\u7814\u7a76\u800c\u975e\u8ba1\u7b97\u7ec6\u8282\u3002", "conclusion": "AiiDAlab\u5df2\u6210\u4e3a\u52a0\u901f\u591a\u5b66\u79d1\u79d1\u5b66\u53d1\u73b0\u7684\u5f3a\u5927\u5e73\u53f0\uff0c\u901a\u8fc7\u7b80\u5316\u590d\u6742\u5de5\u4f5c\u6d41\u3001\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u548c\u652f\u6301FAIR\u539f\u5219\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u4e13\u6ce8\u4e8e\u79d1\u5b66\u7814\u7a76\u672c\u8eab\u3002"}}
{"id": "2512.23033", "categories": ["cs.SE", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.23033", "abs": "https://arxiv.org/abs/2512.23033", "authors": ["Fuyad Hasan Bhoyan", "Prashanta Sarker", "Parsia Noor Ethila", "Md. Emon Hossain", "Md Kaviul Hossain", "Md Humaion Kabir Mehedi"], "title": "Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI", "comment": null, "summary": "Early and accurate detection of gallbladder diseases is crucial, yet ultrasound interpretation is challenging. To address this, an AI-driven diagnostic software integrates our hybrid deep learning model MobResTaNet to classify ten categories, nine gallbladder disease types and normal directly from ultrasound images. The system delivers interpretable, real-time predictions via Explainable AI (XAI) visualizations, supporting transparent clinical decision-making. It achieves up to 99.85% accuracy with only 2.24M parameters. Deployed as web and mobile applications using HTML, CSS, JavaScript, Bootstrap, and Flutter, the software provides efficient, accessible, and trustworthy diagnostic support at the point of care", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eAI\u7684\u80c6\u56ca\u75be\u75c5\u8d85\u58f0\u8bca\u65ad\u8f6f\u4ef6\uff0c\u4f7f\u7528MobResTaNet\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u5bf9\u8d85\u58f0\u56fe\u50cf\u8fdb\u884c10\u7c7b\u5206\u7c7b\uff089\u79cd\u75be\u75c5+\u6b63\u5e38\uff09\uff0c\u901a\u8fc7XAI\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u5b9e\u65f6\u9884\u6d4b\uff0c\u90e8\u7f72\u4e3a\u7f51\u9875\u548c\u79fb\u52a8\u5e94\u7528", "motivation": "\u80c6\u56ca\u75be\u75c5\u7684\u65e9\u671f\u51c6\u786e\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8d85\u58f0\u56fe\u50cf\u89e3\u8bfb\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5f00\u53d1AI\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u6765\u6539\u5584\u8bca\u65ad\u6548\u7387\u548c\u51c6\u786e\u6027", "method": "\u5f00\u53d1AI\u9a71\u52a8\u8bca\u65ad\u8f6f\u4ef6\uff0c\u96c6\u6210\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bMobResTaNet\uff0c\u76f4\u63a5\u4ece\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b10\u4e2a\u7c7b\u522b\uff089\u79cd\u80c6\u56ca\u75be\u75c5\u7c7b\u578b\u548c\u6b63\u5e38\uff09\uff0c\u901a\u8fc7\u53ef\u89e3\u91caAI\uff08XAI\uff09\u53ef\u89c6\u5316\u63d0\u4f9b\u5b9e\u65f6\u9884\u6d4b\uff0c\u4f7f\u7528HTML\u3001CSS\u3001JavaScript\u3001Bootstrap\u548cFlutter\u6280\u672f\u90e8\u7f72\u4e3a\u7f51\u9875\u548c\u79fb\u52a8\u5e94\u7528", "result": "\u7cfb\u7edf\u8fbe\u5230\u9ad8\u8fbe99.85%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u4f7f\u75282.24M\u53c2\u6570\uff0c\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u8bbf\u95ee\u4e14\u53ef\u4fe1\u8d56\u7684\u5e8a\u8fb9\u8bca\u65ad\u652f\u6301", "conclusion": "\u8be5AI\u9a71\u52a8\u8bca\u65ad\u8f6f\u4ef6\u80fd\u591f\u4e3a\u80c6\u56ca\u75be\u75c5\u63d0\u4f9b\u51c6\u786e\u3001\u5b9e\u65f6\u4e14\u53ef\u89e3\u91ca\u7684\u8d85\u58f0\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u7f51\u9875\u548c\u79fb\u52a8\u5e94\u7528\u90e8\u7f72\uff0c\u652f\u6301\u900f\u660e\u4e34\u5e8a\u51b3\u7b56\uff0c\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2512.23345", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23345", "abs": "https://arxiv.org/abs/2512.23345", "authors": ["Peiting Xie", "Xiangjun Zai", "Yanping Wu", "Xiaoyang Wang", "Wenjie Zhang", "Lu Qin"], "title": "HL-index: Fast Reachability Query in Hypergraphs", "comment": null, "summary": "Reachability in hypergraphs is essential for mod- eling complex groupwise interactions in real-world applications such as co-authorship, social network, and biological analysis, where relationships go beyond pairwise interactions. In this pa- per, we introduce the notion of s-reachability, where two vertices are s-reachable if there exists a sequence of hyperedges (i.e., a walk) connecting them, such that each pair of consecutive hy- peredges shares at least s vertices. Moreover, we define the max- reachability query as a generalized form of the s-reachability problem, which aims to find the largest value of s that allows one vertex to reach another. To answer max-reachability queries in hypergraphs, we first analyze limitations of the existing vertex-to- vertex and hyperedge-to-hyperedge indexing techniques. We then introduce the HL-index, a compact vertex-to-hyperedge index tailored for the max-reachability problem. To both efficiently and effectively construct a minimal HL-index, we develop a fast covering relationship detection method to eliminate fruitless hypergraph traversals during index construction. A lightweight neighbor-index is further proposed to avoid repeatedly exploring neighbor relationships in hypergraphs and hence accelerate the construction. Extensive experiments on 20 datasets demonstrate the efficiency and scalability of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8d85\u56fe\u4e2d\u7684s\u53ef\u8fbe\u6027\u6982\u5ff5\u548c\u6700\u5927\u53ef\u8fbe\u6027\u67e5\u8be2\u95ee\u9898\uff0c\u5f00\u53d1\u4e86HL-index\u7d22\u5f15\u7ed3\u6784\u548c\u9ad8\u6548\u6784\u5efa\u65b9\u6cd5", "motivation": "\u8d85\u56fe\u4e2d\u7684\u53ef\u8fbe\u6027\u5bf9\u4e8e\u5efa\u6a21\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u7fa4\u4f53\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u5982\u5408\u8457\u5173\u7cfb\u3001\u793e\u4ea4\u7f51\u7edc\u548c\u751f\u7269\u5206\u6790\uff0c\u8fd9\u4e9b\u5173\u7cfb\u8d85\u8d8a\u4e86\u6210\u5bf9\u4ea4\u4e92\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u56fe\u53ef\u8fbe\u6027\u67e5\u8be2\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165s\u53ef\u8fbe\u6027\u6982\u5ff5\uff0c\u5b9a\u4e49\u6700\u5927\u53ef\u8fbe\u6027\u67e5\u8be2\u95ee\u9898\u3002\u63d0\u51faHL-index\uff08\u9876\u70b9\u5230\u8d85\u8fb9\u7d22\u5f15\uff09\uff0c\u5f00\u53d1\u5feb\u901f\u8986\u76d6\u5173\u7cfb\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u6700\u5c0f\u5316\u7d22\u5f15\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u90bb\u5c45\u7d22\u5f15\u6765\u52a0\u901f\u6784\u5efa\u3002", "result": "\u572820\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002HL-index\u80fd\u591f\u6709\u6548\u56de\u7b54\u6700\u5927\u53ef\u8fbe\u6027\u67e5\u8be2\uff0c\u6784\u5efa\u8fc7\u7a0b\u9ad8\u6548\u3002", "conclusion": "\u63d0\u51fa\u7684s\u53ef\u8fbe\u6027\u6982\u5ff5\u548cHL-index\u4e3a\u8d85\u56fe\u4e2d\u7684\u53ef\u8fbe\u6027\u67e5\u8be2\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u7fa4\u4f53\u4ea4\u4e92\u5173\u7cfb\uff0c\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.22174", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22174", "abs": "https://arxiv.org/abs/2512.22174", "authors": ["Muhammad Zeeshan Karamat", "Sadman Saif", "Christiana Chamon Garcia"], "title": "BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs", "comment": null, "summary": "Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.", "AI": {"tldr": "BitFlipScope\u662f\u4e00\u4e2a\u8f6f\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u5728Transformer\u67b6\u6784\u4e2d\u5b9a\u4f4d\u7531\u4f4d\u7ffb\u8f6c\u6545\u969c\u5f15\u8d77\u7684\u53c2\u6570\u635f\u574f\u533a\u57df\uff0c\u652f\u6301\u6709/\u65e0\u53c2\u8003\u6a21\u578b\u4e24\u79cd\u573a\u666f\uff0c\u5e76\u80fd\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6027\u80fd\u6062\u590d\u3002", "motivation": "LLMs\u5728\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\uff0c\u5bb9\u6613\u53d7\u5230\u786c\u4ef6\u9000\u5316\u3001\u5b87\u5b99\u8f90\u5c04\u6216Rowhammer\u653b\u51fb\u7b49\u5f15\u8d77\u7684\u4f4d\u7ffb\u8f6c\u6545\u969c\u5f71\u54cd\uff0c\u8fd9\u4e9b\u6545\u969c\u4f1a\u65e0\u58f0\u5730\u635f\u574f\u5185\u90e8\u53c2\u6570\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u6216\u5371\u9669\u7684\u884c\u4e3a\u3002\u5b9a\u4f4d\u8fd9\u4e9b\u635f\u574f\u5bf9\u4e8e\u8bca\u65ad\u3001\u4fee\u590d\u548c\u6062\u590d\u6a21\u578b\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "BitFlipScope\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u5f53\u6709\u5e72\u51c0\u53c2\u8003\u6a21\u578b\u65f6\uff0c\u901a\u8fc7\u8f93\u51fa\u3001\u9690\u85cf\u72b6\u6001\u548c\u5185\u90e8\u6fc0\u6d3b\u7684\u5dee\u5f02\u5206\u6790\u6765\u68c0\u6d4b\u5f02\u5e38\u884c\u4e3a\uff1b2) \u5f53\u65e0\u53c2\u8003\u6a21\u578b\u65f6\uff0c\u4f7f\u7528\u6b8b\u5dee\u8def\u5f84\u6270\u52a8\u548c\u635f\u5931\u654f\u611f\u6027\u5206\u6790\u76f4\u63a5\u4ece\u635f\u574f\u6a21\u578b\u4e2d\u63a8\u65ad\u6545\u969c\u5f71\u54cd\u533a\u57df\u3002", "result": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u80fd\u6709\u6548\u8fdb\u884c\u6545\u969c\u8bca\u65ad\uff0c\u8fd8\u652f\u6301\u65e0\u9700\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u6027\u80fd\u6062\u590d\uff0c\u4e3a\u5728\u786c\u4ef6\u6613\u51fa\u9519\u548c\u5bf9\u6297\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u4fe1\u8d56\u3001\u5bb9\u9519\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002", "conclusion": "BitFlipScope\u662f\u8fc8\u5411\u5728\u786c\u4ef6\u6613\u51fa\u9519\u548c\u5bf9\u6297\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u4fe1\u8d56\u3001\u5bb9\u9519LLM\u90e8\u7f72\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6545\u969c\u8bca\u65ad\u548c\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2512.23066", "categories": ["cs.SE", "cs.DL"], "pdf": "https://arxiv.org/pdf/2512.23066", "abs": "https://arxiv.org/abs/2512.23066", "authors": ["Houcine Abdelkader Cherief", "Brahim Mahmoudi", "Zacharie Chenail-Larcher", "Naouel Moha", "Quentin Sti'evenart", "Florent Avellaneda"], "title": "An Automated Grey Literature Extraction Tool for Software Engineering", "comment": null, "summary": "Grey literature is essential to software engineering research as it captures practices and decisions that rarely appear in academic venues. However, collecting and assessing it at scale remains difficult because of their heterogeneous sources, formats, and APIs that impede reproducible, large-scale synthesis. To address this issue, we present GLiSE, a prompt-driven tool that turns a research topic prompt into platform-specific queries, gathers results from common software-engineering web sources (GitHub, Stack Overflow) and Google Search, and uses embedding-based semantic classifiers to filter and rank results according to their relevance. GLiSE is designed for reproducibility with all settings being configuration-based, and every generated query being accessible. In this paper, (i) we present the GLiSE tool, (ii) provide a curated dataset of software engineering grey-literature search results classified by semantic relevance to their originating search intent, and (iii) conduct an empirical study on the usability of our tool.", "AI": {"tldr": "GLiSE\u662f\u4e00\u4e2a\u57fa\u4e8e\u63d0\u793a\u9a71\u52a8\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6536\u96c6\u548c\u8bc4\u4f30\u8f6f\u4ef6\u5de5\u7a0b\u7070\u8272\u6587\u732e\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u7c7b\u5668\u8fc7\u6ee4\u548c\u6392\u5e8f\u641c\u7d22\u7ed3\u679c\uff0c\u63d0\u9ad8\u53ef\u91cd\u590d\u6027\u3002", "motivation": "\u7070\u8272\u6587\u732e\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u56e0\u5176\u6765\u6e90\u3001\u683c\u5f0f\u548cAPI\u7684\u5f02\u6784\u6027\uff0c\u5927\u89c4\u6a21\u6536\u96c6\u548c\u8bc4\u4f30\u56f0\u96be\uff0c\u963b\u788d\u4e86\u53ef\u91cd\u590d\u7684\u5927\u89c4\u6a21\u7efc\u5408\u7814\u7a76\u3002", "method": "GLiSE\u5c06\u7814\u7a76\u4e3b\u9898\u63d0\u793a\u8f6c\u6362\u4e3a\u5e73\u53f0\u7279\u5b9a\u67e5\u8be2\uff0c\u4eceGitHub\u3001Stack Overflow\u548cGoogle\u641c\u7d22\u6536\u96c6\u7ed3\u679c\uff0c\u4f7f\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u8bed\u4e49\u5206\u7c7b\u5668\u6839\u636e\u76f8\u5173\u6027\u8fc7\u6ee4\u548c\u6392\u5e8f\u7ed3\u679c\u3002", "result": "\u5f00\u53d1\u4e86GLiSE\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u7070\u8272\u6587\u732e\u641c\u7d22\u7ed3\u679c\u7684\u7cbe\u9009\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u5de5\u5177\u53ef\u7528\u6027\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "conclusion": "GLiSE\u89e3\u51b3\u4e86\u7070\u8272\u6587\u732e\u6536\u96c6\u7684\u53ef\u91cd\u590d\u6027\u548c\u5927\u89c4\u6a21\u7efc\u5408\u95ee\u9898\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2512.23366", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23366", "abs": "https://arxiv.org/abs/2512.23366", "authors": ["Cehua Yang", "Dongyu Xiao", "Junming Lin", "Yuyang Song", "Hanxu Yan", "Shawn Guo", "Wei Zhang", "Jian Yang", "Mingjie Tang", "Bryan Dai"], "title": "AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis", "comment": null, "summary": "The advancement of Text-to-SQL systems is currently hindered by the scarcity of high-quality training data and the limited reasoning capabilities of models in complex scenarios. In this paper, we propose a holistic framework that addresses these issues through a dual-centric approach. From a Data-Centric perspective, we construct an iterative data factory that synthesizes RL-ready data characterized by high correctness and precise semantic-logic alignment, ensured by strict verification. From a Model-Centric perspective, we introduce a novel Agentic Reinforcement Learning framework. This framework employs a Diversity-Aware Cold Start stage to initialize a robust policy, followed by Group Relative Policy Optimization (GRPO) to refine the agent's reasoning via environmental feedback. Extensive experiments on BIRD and Spider benchmarks demonstrate that our synergistic approach achieves state-of-the-art performance among single-model methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53cc\u4e2d\u5fc3\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u5de5\u5382\u5408\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u63d0\u5347Text-to-SQL\u7cfb\u7edf\u6027\u80fd\uff0c\u5728BIRD\u548cSpider\u57fa\u51c6\u4e0a\u8fbe\u5230\u5355\u6a21\u578bSOTA\u3002", "motivation": "Text-to-SQL\u7cfb\u7edf\u9762\u4e34\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u63a8\u7406\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u4ece\u6570\u636e\u548c\u6a21\u578b\u4e24\u4e2a\u89d2\u5ea6\u89e3\u51b3\u3002", "method": "\u91c7\u7528\u53cc\u4e2d\u5fc3\u65b9\u6cd5\uff1a1) \u6570\u636e\u4e2d\u5fc3\u89c6\u89d2\uff1a\u6784\u5efa\u8fed\u4ee3\u6570\u636e\u5de5\u5382\uff0c\u5408\u6210\u5177\u6709\u9ad8\u6b63\u786e\u6027\u548c\u7cbe\u786e\u8bed\u4e49\u903b\u8f91\u5bf9\u9f50\u7684RL\u5c31\u7eea\u6570\u636e\uff1b2) \u6a21\u578b\u4e2d\u5fc3\u89c6\u89d2\uff1a\u63d0\u51fa\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u591a\u6837\u6027\u611f\u77e5\u51b7\u542f\u52a8\u9636\u6bb5\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(GRPO)\u3002", "result": "\u5728BIRD\u548cSpider\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u534f\u540c\u65b9\u6cd5\u5728\u5355\u6a21\u578b\u65b9\u6cd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u5de5\u5382\u548c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u534f\u540c\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86Text-to-SQL\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u63a8\u7406\u80fd\u529b\u9650\u5236\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2512.22180", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22180", "abs": "https://arxiv.org/abs/2512.22180", "authors": ["Alexander K. Chen"], "title": "iOS as Acceleration", "comment": "7 pages main text, 7 pages appendix. Presented at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.", "AI": {"tldr": "\u5229\u7528iOS\u624b\u673a\u4f5c\u4e3a\u5206\u5e03\u5f0f\u5e76\u884c\u8ba1\u7b97\u8d44\u6e90\uff0c\u63d0\u5347\u672c\u5730\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6210\u672c", "motivation": "\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u9700\u8981\u5f3a\u5927\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f46\u672c\u5730\u7cfb\u7edf\u73af\u5883\u53d7\u9650\u3002\u867d\u7136\u4e91\u8ba1\u7b97\u53ef\u89e3\u51b3\u672c\u5730\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u4f46\u5728\u6d89\u53ca\u9690\u79c1\u6570\u636e\u3001\u7269\u7406\u73af\u5883\u4e0d\u53ef\u7528\u6216\u6210\u672c\u8003\u8651\u65f6\uff0c\u4ecd\u9700\u672c\u5730\u8ba1\u7b97\u3002\u79fb\u52a8\u624b\u673a\u4f5c\u4e3a\u666e\u904d\u4f46\u672a\u5145\u5206\u5229\u7528\u7684\u8d44\u6e90\uff0c\u5177\u6709\u6539\u5584\u672c\u5730\u8ba1\u7b97\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u6982\u5ff5\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u7ba1\u9053\u5e76\u884c\u65b9\u6cd5\uff0c\u5229\u7528iOS\u8bbe\u5907\uff08\u7279\u522b\u662f\u8fd1\u5e74\u914d\u5907\u5f3a\u5927\u5904\u7406\u5668\u7684iPhone\uff09\u6765\u52a0\u901f\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u3002\u7cfb\u7edf\u514b\u670d\u4e86iOS\u8bbe\u5907\u7684\u5185\u5b58\u9650\u5236\u3001\u70ed\u8282\u6d41\u548c\u64cd\u4f5c\u7cfb\u7edf\u6c99\u76d2\u7b49\u9650\u5236\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u8f83\u5f31\u8ba1\u7b97\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u52a0\u901f\u4e86\u9002\u5ea6\u89c4\u6a21\u7684\u6a21\u578b\u8bad\u7ec3\u3001\u6279\u91cf\u63a8\u7406\u548c\u4ee3\u7406LRM\u5de5\u5177\u4f7f\u7528\u3002\u5c55\u793a\u4e86iOS\u8bbe\u5907\u4f5c\u4e3a\u5206\u5e03\u5f0f\u8ba1\u7b97\u8d44\u6e90\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u666e\u901a\u79fb\u52a8\u8bbe\u5907\u6709\u6f5c\u529b\u4e3a\u673a\u5668\u5b66\u4e60\u505a\u51fa\u66f4\u5927\u8d21\u732e\u3002\u8ba8\u8bba\u4e86\u5b9e\u9645\u7528\u4f8b\u3001\u9650\u5236\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5229\u7528\u73b0\u6709\u79fb\u52a8\u8bbe\u5907\u8d44\u6e90\u6539\u5584\u672c\u5730\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u73af\u5883\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2512.23327", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23327", "abs": "https://arxiv.org/abs/2512.23327", "authors": ["G\u00f6rkem Giray", "Onur Demir\u00f6rs", "Marcos Kalinowski", "Daniel Mendez"], "title": "An Empirical Study of Generative AI Adoption in Software Engineering", "comment": null, "summary": "Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.", "AI": {"tldr": "GenAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4e3b\u8981\u7528\u4e8e\u5b9e\u73b0\u3001\u9a8c\u8bc1\u3001\u4e2a\u4eba\u8f85\u52a9\u548c\u7ef4\u62a4\u4efb\u52a1\uff0c\u5e26\u6765\u751f\u4ea7\u529b\u63d0\u5347\u4f46\u9762\u4e34\u8f93\u51fa\u53ef\u9760\u6027\u3001\u5b89\u5168\u9690\u79c1\u7b49\u6311\u6218\uff0c\u9884\u8ba1\u4f1a\u91cd\u65b0\u5b9a\u4e49\u800c\u975e\u53d6\u4ee3\u5de5\u7a0b\u5e08\u89d2\u8272\u3002", "motivation": "\u5c3d\u7ba1GenAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u7684\u91c7\u7528\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u3001\u6548\u76ca\u3001\u6311\u6218\u53ca\u5176\u7ec4\u7ec7\u548c\u793e\u4f1a\u5f71\u54cd\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3GenAI\u5728SE\u4e2d\u7684\u91c7\u7528\u73b0\u72b6\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u8c03\u67e5\u5206\u6790GenAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91c7\u7528\u72b6\u6001\uff0c\u5305\u62ec\u91c7\u7528\u7a0b\u5ea6\u3001\u76f8\u5173\u6548\u76ca\u4e0e\u6311\u6218\u3001\u5de5\u5177\u6280\u672f\u5236\u5ea6\u5316\u60c5\u51b5\uff0c\u4ee5\u53ca\u5bf9SE\u4e13\u4e1a\u4eba\u5458\u548c\u793e\u533a\u7684\u957f\u671f\u5f71\u54cd\u9884\u671f\u3002", "result": "\u7ed3\u679c\u663e\u793aGenAI\u5de5\u5177\u88ab\u5e7f\u6cdb\u91c7\u7528\u5e76\u6df1\u5ea6\u878d\u5165\u65e5\u5e38SE\u5de5\u4f5c\uff0c\u7279\u522b\u662f\u5728\u5b9e\u73b0\u3001\u9a8c\u8bc1\u4e0e\u9a8c\u8bc1\u3001\u4e2a\u4eba\u8f85\u52a9\u548c\u7ef4\u62a4\u76f8\u5173\u4efb\u52a1\u4e2d\u3002\u5b9e\u8df5\u8005\u62a5\u544a\u4e86\u663e\u8457\u7684\u6548\u76ca\uff0c\u5305\u62ec\u5468\u671f\u65f6\u95f4\u7f29\u77ed\u3001\u8d28\u91cf\u6539\u8fdb\u3001\u77e5\u8bc6\u5de5\u4f5c\u652f\u6301\u589e\u5f3a\u548c\u751f\u4ea7\u529b\u63d0\u5347\u3002\u7136\u800c\uff0c\u751f\u4ea7\u529b\u548c\u8d28\u91cf\u7684\u5ba2\u89c2\u6d4b\u91cf\u5728\u5b9e\u8df5\u4e2d\u4ecd\u7136\u6709\u9650\u3002\u91cd\u5927\u6311\u6218\u6301\u7eed\u5b58\u5728\uff0c\u5305\u62ec\u8f93\u51fa\u4e0d\u6b63\u786e\u6216\u4e0d\u53ef\u9760\u3001\u63d0\u793a\u5de5\u7a0b\u56f0\u96be\u3001\u9a8c\u8bc1\u5f00\u9500\u3001\u5b89\u5168\u548c\u9690\u79c1\u95ee\u9898\u4ee5\u53ca\u8fc7\u5ea6\u4f9d\u8d56\u98ce\u9669\u3002\u5de5\u5177\u548c\u6280\u672f\u7684\u5236\u5ea6\u5316\u4f3c\u4e4e\u5f88\u5e38\u89c1\uff0c\u4f46\u5dee\u5f02\u5f88\u5927\uff0c\u91cd\u70b9\u5173\u6ce8\u5de5\u5177\u8bbf\u95ee\uff0c\u8f83\u5c11\u5f3a\u8c03\u57f9\u8bad\u548c\u7ba1\u7406\u3002\u5b9e\u8df5\u8005\u671f\u671bGenAI\u91cd\u65b0\u5b9a\u4e49\u800c\u975e\u53d6\u4ee3\u4ed6\u4eec\u7684\u89d2\u8272\uff0c\u540c\u65f6\u5bf9\u5c31\u4e1a\u5e02\u573a\u6536\u7f29\u548c\u6280\u80fd\u8f6c\u53d8\u8868\u793a\u9002\u5ea6\u62c5\u5fe7\u3002", "conclusion": "GenAI\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u91c7\u7528\u5e76\u5e26\u6765\u663e\u8457\u6548\u76ca\uff0c\u4f46\u4ecd\u9762\u4e34\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u8fc7\u5ea6\u4f9d\u8d56\u7b49\u6311\u6218\u3002\u5236\u5ea6\u5316\u7a0b\u5ea6\u4e0d\u4e00\uff0c\u672a\u6765GenAI\u5c06\u91cd\u65b0\u5b9a\u4e49\u800c\u975e\u53d6\u4ee3SE\u89d2\u8272\uff0c\u9700\u8981\u5e73\u8861\u5de5\u5177\u8bbf\u95ee\u4e0e\u9002\u5f53\u57f9\u8bad\u548c\u7ba1\u7406\u3002"}}
{"id": "2512.23399", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23399", "abs": "https://arxiv.org/abs/2512.23399", "authors": ["Mingjin Tao", "Kailin Jiao", "Yawen Li", "Wei Liu", "Ziqiang Yu"], "title": "Distributed Processing of kNN Queries over Moving Objects on Dynamic Road Networks", "comment": "Accepted by the BigComp2026", "summary": "The k Nearest Neighbor (kNN) query over moving objects on road networks is essential for location-based services. Recently, this problem has been studied under road networks with distance as the metric, overlooking fluctuating travel costs. We pioneer the study of the kNN problem within dynamic road networks that account for evolving travel costs. Recognizing the limitations of index-based methods, which become quickly outdated as travel costs change, our work abandons indexes in favor of incremental network expansion on each snapshot of a dynamic road network to search for kNNs. To enhance expansion efficiency, we present DkNN, a distributed algorithm that divides the road network into sub-networks for parallel exploration using Dijkstra's algorithm across relevant regions. This approach effectively addresses challenges related to maintaining global distance accuracy during local, independent subgraph exploration, while minimizing unnecessary searches in irrelevant sub-networks and facilitating the early detection of true kNNs, despite the lack of constant global search monitoring. Implemented on the Storm platform, DkNN demonstrates superior efficiency and effectiveness over traditional methods in real-world road network scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DkNN\u7b97\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u9053\u8def\u7f51\u7edc\u4e2d\u8003\u8651\u65c5\u884c\u6210\u672c\u53d8\u5316\u7684k\u8fd1\u90bb\u67e5\u8be2\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u7f51\u7edc\u6269\u5c55\u65b9\u6cd5\u66ff\u4ee3\u4f20\u7edf\u7d22\u5f15\uff0c\u5728Storm\u5e73\u53f0\u4e0a\u5b9e\u73b0\u5e76\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709kNN\u67e5\u8be2\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9053\u8def\u7f51\u7edc\u4e2d\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5ffd\u7565\u4e86\u65c5\u884c\u6210\u672c\u7684\u52a8\u6001\u53d8\u5316\u3002\u57fa\u4e8e\u7d22\u5f15\u7684\u65b9\u6cd5\u5728\u65c5\u884c\u6210\u672c\u53d8\u5316\u65f6\u5bb9\u6613\u8fc7\u65f6\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u52a8\u6001\u9053\u8def\u7f51\u7edc\u4e2d\u7684kNN\u67e5\u8be2\u95ee\u9898\u3002", "method": "\u653e\u5f03\u7d22\u5f15\u65b9\u6cd5\uff0c\u91c7\u7528\u589e\u91cf\u7f51\u7edc\u6269\u5c55\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u52a8\u6001\u9053\u8def\u7f51\u7edc\u5feb\u7167\u4e0a\u641c\u7d22kNN\u3002\u63d0\u51faDkNN\u5206\u5e03\u5f0f\u7b97\u6cd5\uff0c\u5c06\u9053\u8def\u7f51\u7edc\u5212\u5206\u4e3a\u5b50\u7f51\u7edc\u8fdb\u884c\u5e76\u884c\u63a2\u7d22\uff0c\u4f7f\u7528Dijkstra\u7b97\u6cd5\u5728\u76f8\u5173\u533a\u57df\u641c\u7d22\uff0c\u89e3\u51b3\u5c40\u90e8\u72ec\u7acb\u5b50\u56fe\u63a2\u7d22\u4e2d\u7684\u5168\u5c40\u8ddd\u79bb\u7cbe\u5ea6\u95ee\u9898\u3002", "result": "DkNN\u5728Storm\u5e73\u53f0\u4e0a\u5b9e\u73b0\uff0c\u5728\u771f\u5b9e\u9053\u8def\u7f51\u7edc\u573a\u666f\u4e2d\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4e0d\u76f8\u5173\u5b50\u7f51\u7edc\u7684\u641c\u7d22\uff0c\u65e9\u671f\u68c0\u6d4b\u5230\u771f\u6b63\u7684k\u8fd1\u90bb\u3002", "conclusion": "DkNN\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u9053\u8def\u7f51\u7edc\u4e2d\u8003\u8651\u65c5\u884c\u6210\u672c\u53d8\u5316\u7684kNN\u67e5\u8be2\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u7f51\u7edc\u6269\u5c55\u65b9\u6cd5\u514b\u670d\u4e86\u7d22\u5f15\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4f4d\u7f6e\u670d\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22195", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22195", "abs": "https://arxiv.org/abs/2512.22195", "authors": ["Kun-Woo Shin", "Jay H. Park", "Moonwook Oh", "Yohan Jo", "Jaeyoung Do", "Sang-Won Lee"], "title": "MatKV: Trading Compute for Flash Storage in LLM Inference", "comment": "Accepted for publication in ICDE 2026", "summary": "We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.", "AI": {"tldr": "MatKV\u901a\u8fc7\u9884\u8ba1\u7b97RAG\u6587\u6863\u7684KV\u5411\u91cf\u5e76\u5b58\u50a8\u5728\u95ea\u5b58\u4e2d\uff0c\u5728\u63a8\u7406\u65f6\u76f4\u63a5\u590d\u7528\uff0c\u5c06\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u8017\u51cf\u534a\uff0c\u540c\u65f6\u652f\u6301GPU\u5e76\u884c\u89e3\u7801\u548c\u4f4e\u7aefGPU\u4f7f\u7528\u3002", "motivation": "LLM\u63a8\u7406\u6210\u672c\u5df2\u8d85\u8fc7\u8bad\u7ec3\u6210\u672c\uff0cRAG\u5728\u5904\u7406\u957f\u8f93\u5165\u65f6prefill\u9636\u6bb5\u8ba1\u7b97KV\u5411\u91cf\u80fd\u8017\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u9700\u8981\u63d0\u9ad8RAG\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51faMatKV\u65b9\u6848\uff1a\u9884\u8ba1\u7b97RAG\u6587\u6863\u7684KV\u5411\u91cf\uff0c\u5c06\u7ed3\u679c\u7269\u5316\u5b58\u50a8\u5728\u5ec9\u4ef7\u4f46\u5feb\u901f\u3001\u80fd\u6548\u9ad8\u7684\u95ea\u5b58\u4e2d\uff0c\u63a8\u7406\u65f6\u76f4\u63a5\u52a0\u8f7d\u590d\u7528\u800c\u975e\u91cd\u65b0\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4GPU\u5b8c\u5168\u8ba1\u7b97KV\uff0cMatKV\u5c06RAG\u5de5\u4f5c\u8d1f\u8f7d\u7684\u63a8\u7406\u65f6\u95f4\u548c\u80fd\u8017\u51cf\u534a\uff0c\u4e14\u95ee\u7b54\u4efb\u52a1\u51c6\u786e\u7387\u5f71\u54cd\u4e0d\u5927\u3002\u652f\u6301GPU\u5e76\u884c\u89e3\u7801\u548c\u4f4e\u7aefGPU\u4f7f\u7528\u3002", "conclusion": "MatKV\u80fd\u663e\u8457\u964d\u4f4e\u5927\u89c4\u6a21\u751f\u6210\u5f0fAI\u5e94\u7528\u7684\u6210\u672c\u548c\u80fd\u8017\uff0c\u63d0\u9ad8\u786c\u4ef6\u517c\u5bb9\u6027\uff0c\u4f7fAI\u5e94\u7528\u66f4\u7ecf\u6d4e\u3001\u9ad8\u6548\u3001\u53ef\u8bbf\u95ee\u3002"}}
{"id": "2512.23385", "categories": ["cs.SE", "cs.AI", "cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.23385", "abs": "https://arxiv.org/abs/2512.23385", "authors": ["The Anh Nguyen", "Triet Huynh Minh Le", "M. Ali Babar"], "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?", "comment": "Accepted at the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026) - Research Track", "summary": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790Hugging Face\u548cGitHub\u4e0a\u7684\u5f00\u53d1\u8005\u8ba8\u8bba\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b312,868\u4e2a\u5b89\u5168\u8ba8\u8bba\u7684\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u51faAI\u4f9b\u5e94\u94fe\u4e2d\u768432\u79cd\u5b89\u5168\u95ee\u9898\u548c24\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u63ed\u793a\u4e86AI\u7ec4\u4ef6\u590d\u6742\u4f9d\u8d56\u6027\u548c\u9ed1\u76d2\u7279\u6027\u5e26\u6765\u7684\u5b89\u5168\u6311\u6218\u3002", "motivation": "AI\u6a21\u578b\u548c\u5e94\u7528\u7684\u5feb\u901f\u589e\u957f\u5e26\u6765\u4e86\u590d\u6742\u7684\u5b89\u5168\u683c\u5c40\uff0c\u5f00\u53d1\u8005\u4e0d\u4ec5\u9762\u4e34\u4f20\u7edf\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u95ee\u9898\uff0c\u8fd8\u6709AI\u7279\u6709\u7684\u5b89\u5168\u5a01\u80c1\u3002\u7136\u800c\uff0c\u5b9e\u8df5\u4e2d\u5e38\u89c1\u7684\u5b89\u5168\u95ee\u9898\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u5c1a\u4e0d\u660e\u786e\uff0c\u8fd9\u963b\u788d\u4e86\u9488\u5bf9AI\u4f9b\u5e94\u94fe\u5404\u7ec4\u4ef6\u6709\u6548\u5b89\u5168\u63aa\u65bd\u7684\u5f00\u53d1\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5173\u952e\u8bcd\u5339\u914d\u548c\u4f18\u5316\u7684distilBERT\u5206\u7c7b\u5668\u6784\u5efa\u7ba1\u9053\uff0c\u4eceHugging Face\u548cGitHub\u6536\u96c6\u5f00\u53d1\u8005\u8ba8\u8bba\uff0c\u521b\u5efa\u4e86312,868\u4e2a\u5b89\u5168\u8ba8\u8bba\u7684\u6570\u636e\u96c6\u3002\u5bf9753\u4e2a\u6837\u672c\u8fdb\u884c\u4e3b\u9898\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u7ec6\u7c92\u5ea6\u7684\u5b89\u5168\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u8bc6\u522b\u51fa32\u79cd\u5b89\u5168\u95ee\u9898\u548c24\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u5206\u4e3a\u56db\u4e2a\u4e3b\u9898\uff1a(1)\u7cfb\u7edf\u548c\u8f6f\u4ef6\u3001(2)\u5916\u90e8\u5de5\u5177\u548c\u751f\u6001\u7cfb\u7edf\u3001(3)\u6a21\u578b\u3001(4)\u6570\u636e\u3002\u53d1\u73b0\u8bb8\u591a\u5b89\u5168\u95ee\u9898\u6e90\u4e8eAI\u7ec4\u4ef6\u7684\u590d\u6742\u4f9d\u8d56\u6027\u548c\u9ed1\u76d2\u7279\u6027\uff0c\u7279\u522b\u662f\u6a21\u578b\u548c\u6570\u636e\u76f8\u5173\u7684\u6311\u6218\u5f80\u5f80\u7f3a\u4e4f\u5177\u4f53\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6307\u5bfc\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5e94\u5bf9AI\u4f9b\u5e94\u94fe\u4e2d\u7684\u5b9e\u9645\u5b89\u5168\u5a01\u80c1\uff0c\u586b\u8865\u4e86AI\u5b89\u5168\u5b9e\u8df5\u77e5\u8bc6\u7a7a\u767d\uff0c\u5e76\u63ed\u793a\u4e86\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u7684\u9886\u57df\u3002"}}
{"id": "2512.23491", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.23491", "abs": "https://arxiv.org/abs/2512.23491", "authors": ["Dimitrios Karapiperis", "George Papadakis", "Themis Palpanas", "Vassilios Verykios"], "title": "SPER: Accelerating Progressive Entity Resolution via Stochastic Bipartite Maximization", "comment": null, "summary": "Entity Resolution (ER) is a critical data cleaning task for identifying records that refer to the same real-world entity. In the era of Big Data, traditional batch ER is often infeasible due to volume and velocity constraints, necessitating Progressive ER methods that maximize recall within a limited computational budget. However, existing progressive approaches fail to scale to high-velocity streams because they rely on deterministic sorting to prioritize candidate pairs, a process that incurs prohibitive super-linear complexity and heavy initialization costs. To address this scalability wall, we introduce SPER (Stochastic Progressive ER), a novel framework that redefines prioritization as a sampling problem rather than a ranking problem. By replacing global sorting with a continuous stochastic bipartite maximization strategy, SPER acts as a probabilistic high-pass filter that selects high-utility pairs in strictly linear time. Extensive experiments on eight real-world datasets demonstrate that SPER achieves significant speedups (3x to 6x) over state-of-the-art baselines while maintaining comparable recall and precision.", "AI": {"tldr": "SPER\uff1a\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u91c7\u6837\u7684\u6e10\u8fdb\u5f0f\u5b9e\u4f53\u89e3\u6790\u6846\u67b6\uff0c\u7528\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u6982\u7387\u9ad8\u901a\u8fc7\u6ee4\u5668\u66ff\u4ee3\u4f20\u7edf\u8d85\u7ebf\u6027\u6392\u5e8f\uff0c\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u6d41\u6570\u636e\u4e0b\u7684\u9ad8\u6548\u5b9e\u4f53\u5339\u914d\u3002", "motivation": "\u5728\u5927\u6570\u636e\u65f6\u4ee3\uff0c\u4f20\u7edf\u6279\u5904\u7406\u5b9e\u4f53\u89e3\u6790\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u9ad8\u541e\u5410\u91cf\u6d41\u6570\u636e\uff0c\u73b0\u6709\u6e10\u8fdb\u5f0f\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u6392\u5e8f\u5bfc\u81f4\u8d85\u7ebf\u6027\u590d\u6742\u5ea6\u548c\u9ad8\u6602\u521d\u59cb\u5316\u6210\u672c\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u9ad8\u901f\u6570\u636e\u6d41\u3002", "method": "SPER\u5c06\u4f18\u5148\u7ea7\u95ee\u9898\u91cd\u65b0\u5b9a\u4e49\u4e3a\u91c7\u6837\u95ee\u9898\u800c\u975e\u6392\u5e8f\u95ee\u9898\uff0c\u91c7\u7528\u8fde\u7eed\u968f\u673a\u4e8c\u5206\u6700\u5927\u5316\u7b56\u7565\u66ff\u4ee3\u5168\u5c40\u6392\u5e8f\uff0c\u4f5c\u4e3a\u6982\u7387\u9ad8\u901a\u8fc7\u6ee4\u5668\u5728\u4e25\u683c\u7ebf\u6027\u65f6\u95f4\u5185\u9009\u62e9\u9ad8\u6548\u7528\u5019\u9009\u5bf9\u3002", "result": "\u57288\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSPER\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5b9e\u73b0\u4e863-6\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u53ec\u56de\u7387\u548c\u7cbe\u5ea6\u3002", "conclusion": "SPER\u901a\u8fc7\u968f\u673a\u91c7\u6837\u7b56\u7565\u7a81\u7834\u4e86\u6e10\u8fdb\u5f0f\u5b9e\u4f53\u89e3\u6790\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u4e3a\u9ad8\u541e\u5410\u91cf\u6d41\u6570\u636e\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u5339\u914d\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2512.22215", "categories": ["cs.DC", "cs.MS", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.22215", "abs": "https://arxiv.org/abs/2512.22215", "authors": ["Simone Bn\u00e0", "Giuseppe Giaquinto", "Ettore Fadiga", "Tommaso Zanelli", "Francesco Bottau"], "title": "SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM", "comment": "43 pages", "summary": "High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.", "AI": {"tldr": "SPUMA\u5b9e\u73b0\u4e86OPENFOAM\u5728NVIDIA\u548cAMD GPU\u4e0a\u7684\u5b8c\u6574\u79fb\u690d\uff0c\u91c7\u7528\u4fbf\u643a\u7f16\u7a0b\u6a21\u578b\u548c\u7edf\u4e00\u5185\u5b58\u7ba1\u7406\uff0c\u5728LUMI\u548cLeonardo\u96c6\u7fa4\u4e0a\u5c55\u793a\u4e86\u826f\u597d\u7684\u5f3a/\u5f31\u6269\u5c55\u6027\u548c\u9ad8\u8fbe82%\u7684\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u5c3d\u7ba1GPU\u5728HPC\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u5f00\u6e90CFD\u9886\u57df\uff08\u5982OpenFOAM\uff09\u7684\u7a0b\u5e8f\u53ef\u79fb\u690d\u6027\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u8de8\u5e73\u53f0GPU\u7f16\u7a0b\u548c\u5185\u5b58\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u5f00\u53d1SPUMA\u6846\u67b6\uff0c\u57fa\u4e8e\u4fbf\u643a\u7f16\u7a0b\u6a21\u578b\u5b9e\u73b0OpenFOAM\u7684\u5b8c\u6574GPU\u79fb\u690d\uff0c\u91c7\u7528\u5185\u5b58\u6c60\u7ba1\u7406\u5668\u5229\u7528\u73b0\u4ee3GPU\u7684\u7edf\u4e00\u5185\u5b58\u7279\u6027\uff0c\u652f\u6301NVIDIA\u548cAMD GPU\u3002", "result": "\u5728LUMI\uff08AMD MI250X\uff09\u548cLeonardo\uff08NVIDIA A100\uff09\u96c6\u7fa4\u4e0a\u6d4b\u8bd5\uff1a\u5f3a\u6269\u5c55\u6548\u7387\u8fbe65%\uff08\u6bcfGPU 800\u4e07\u7f51\u683c\uff09\uff0c\u5f31\u6269\u5c55\u6548\u738775-85%\uff0c\u4f7f\u7528AmgX\u65f6\u6548\u7387\u4e0d\u4f4e\u4e8e90%\uff1b\u5355A100 GPU\u6027\u80fd\u76f8\u5f53\u4e8e200-300\u4e2aIntel Sapphire Rapids\u6838\u5fc3\uff1b\u80fd\u8017\u964d\u4f4e\u8fbe82%\u3002", "conclusion": "SPUMA\u6210\u529f\u5b9e\u73b0\u4e86OpenFOAM\u5728\u5f02\u6784GPU\u96c6\u7fa4\u4e0a\u7684\u9ad8\u6548\u79fb\u690d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6027\u80fd\u548c\u80fd\u6548\uff0c\u4e3a\u5f00\u6e90CFD\u5728\u9884\u767e\u4ebf\u4ebf\u6b21\u8ba1\u7b97\u7cfb\u7edf\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.23415", "categories": ["cs.SE", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.23415", "abs": "https://arxiv.org/abs/2512.23415", "authors": ["Vinoth Punniyamoorthy", "Bikesh Kumar", "Sumit Saha", "Lokesh Butra", "Mayilsamy Palanigounder", "Akash Kumar Agarwal", "Kabilan Kannan"], "title": "An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes", "comment": null, "summary": "Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eAIOps\u7684Kubernetes\u667a\u80fd\u5f39\u6027\u4f38\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4fe1\u53f7\u96c6\u6210\u3001SLO\u611f\u77e5\u548c\u6210\u672c\u63a7\u5236\uff0c\u76f8\u6bd4\u539f\u751f\u65b9\u6848\u51cf\u5c11SLO\u8fdd\u89c431%\u3001\u63d0\u5347\u54cd\u5e94\u65f6\u95f424%\u3001\u964d\u4f4e\u6210\u672c18%", "motivation": "Kubernetes\u539f\u751f\u5f39\u6027\u4f38\u7f29\u673a\u5236\u5b58\u5728\u54cd\u5e94\u5f0f\u884c\u4e3a\u3001\u5e94\u7528\u7ea7\u4fe1\u53f7\u5229\u7528\u6709\u9650\u3001\u63a7\u5236\u903b\u8f91\u4e0d\u900f\u660e\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u751f\u4ea7\u73af\u5883\u4e2d\u9891\u7e41\u51fa\u73b0SLO\u8fdd\u89c4\u548c\u6210\u672c\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51fa\u5b89\u5168\u53ef\u89e3\u91ca\u7684\u591a\u4fe1\u53f7\u5f39\u6027\u4f38\u7f29\u6846\u67b6\uff0c\u96c6\u6210SLO\u611f\u77e5\u548c\u6210\u672c\u63a7\u5236\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9700\u6c42\u9884\u6d4b\uff0c\u91c7\u7528AIOps\u539f\u5219\u589e\u5f3aKubernetes\u5f39\u6027\u4f38\u7f29\u80fd\u529b", "result": "\u5728\u4ee3\u8868\u6027\u5fae\u670d\u52a1\u548c\u4e8b\u4ef6\u9a71\u52a8\u8d1f\u8f7d\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u9ed8\u8ba4\u548c\u8c03\u4f18\u7684Kubernetes\u57fa\u7ebf\uff0cSLO\u8fdd\u89c4\u65f6\u957f\u51cf\u5c1131%\uff0c\u4f38\u7f29\u54cd\u5e94\u65f6\u95f4\u63d0\u534724%\uff0c\u57fa\u7840\u8bbe\u65bd\u6210\u672c\u964d\u4f4e18%", "conclusion": "AIOps\u9a71\u52a8\u7684SLO\u4f18\u5148\u5f39\u6027\u4f38\u7f29\u80fd\u663e\u8457\u63d0\u5347Kubernetes\u4e91\u5e73\u53f0\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u64cd\u4f5c\u53ef\u4fe1\u5ea6\uff0c\u5b9e\u73b0SLO\u548c\u6210\u672c\u7ea6\u675f\u7684\u53cc\u91cd\u4f18\u5316"}}
{"id": "2512.22219", "categories": ["cs.DC", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.22219", "abs": "https://arxiv.org/abs/2512.22219", "authors": ["Xinhao Cheng", "Zhihao Zhang", "Yu Zhou", "Jianan Ji", "Jinchen Jiang", "Zepeng Zhao", "Ziruo Xiao", "Zihao Ye", "Yingyi Huang", "Ruihang Lai", "Hongyi Jin", "Bohan Hou", "Mengdi Wu", "Yixin Dong", "Anthony Yip", "Zihao Ye", "Songting Wang", "Wenqin Yang", "Xupeng Miao", "Tianqi Chen", "Zhihao Jia"], "title": "Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs", "comment": null, "summary": "We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage.", "AI": {"tldr": "MPK\u662f\u9996\u4e2a\u5c06\u591aGPU\u6a21\u578b\u63a8\u7406\u81ea\u52a8\u8f6c\u6362\u4e3a\u5355\u4e2a\u9ad8\u6027\u80fd\u5de8\u578b\u5185\u6838\u7684\u7f16\u8bd1\u5668\u548c\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u901a\u8fc7SM\u7ea7\u56fe\u8868\u793a\u5b9e\u73b0\u8de8\u7b97\u5b50\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u7b49\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u7b97\u5b50\u7ea7\u5185\u6838\u5206\u79bb\u7684\u65b9\u5f0f\uff0c\u5bfc\u81f4GPU\u5229\u7528\u7387\u4e0d\u8db3\u548c\u8c03\u5ea6\u5f00\u9500\u5927\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u786c\u4ef6\u6f5c\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u878d\u5408\u591aGPU\u6a21\u578b\u63a8\u7406\u4e3a\u5355\u4e2a\u9ad8\u6027\u80fd\u5185\u6838\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MPK\u5f15\u5165SM\u7ea7\u56fe\u8868\u793a\u6355\u83b7\u5355\u4e2a\u6d41\u591a\u5904\u7406\u5668\u7ea7\u522b\u7684\u6570\u636e\u4f9d\u8d56\uff0c\u652f\u6301\u8de8\u7b97\u5b50\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u548c\u7ec6\u7c92\u5ea6\u5185\u6838\u91cd\u53e0\u3002\u7f16\u8bd1\u5668\u5c06\u5f20\u91cf\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u4f18\u5316\u7684SM\u7ea7\u4efb\u52a1\u56fe\u5e76\u751f\u6210CUDA\u5b9e\u73b0\uff0c\u8fd0\u884c\u65f6\u5728\u5355\u4e2a\u5de8\u578b\u5185\u6838\u5185\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u6267\u884c\u4efb\u52a1\u3002", "result": "MPK\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u5b50\u7ea7LLM\u670d\u52a1\u7cfb\u7edf\uff0c\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe1.7\u500d\uff0c\u5c06LLM\u63a8\u7406\u6027\u80fd\u63a8\u8fd1\u786c\u4ef6\u6781\u9650\u3002", "conclusion": "MPK\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u7aef\u5230\u7aef\u5185\u6838\u878d\u5408\uff0c\u4ee5\u6700\u5c0f\u5f00\u53d1\u5de5\u4f5c\u91cf\u5b9e\u73b0\u4e86\u63a5\u8fd1\u786c\u4ef6\u6781\u9650\u7684LLM\u63a8\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u73b0\u6709\u7f16\u7a0b\u6a21\u578b\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2512.23488", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23488", "abs": "https://arxiv.org/abs/2512.23488", "authors": ["Maria Spichkova"], "title": "Embedding Quality Assurance in project-based learning", "comment": null, "summary": "In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.", "AI": {"tldr": "\u57fa\u4e8e\u5341\u591a\u5e74\u5728\u654f\u6377/Scrum\u73af\u5883\u4e0b\u6559\u6388\u8f6f\u4ef6\u5de5\u7a0b\u8bfe\u7a0b\u7684\u7ecf\u9a8c\uff0c\u5206\u4eab\u8f6f\u4ef6\u8d28\u91cf\u6559\u5b66\u7684\u7ecf\u9a8c\u6559\u8bad\uff0c\u5e76\u63d0\u4f9b\u5728\u57fa\u4e8e\u9879\u76ee\u7684\u654f\u6377\u5b66\u4e60\u4e2d\u5d4c\u5165\u8d28\u91cf\u4fdd\u8bc1\u4e3b\u9898\u7684\u5efa\u8bae\u3002", "motivation": "\u4f5c\u8005\u62e5\u6709\u5341\u591a\u5e74\u5728\u654f\u6377/Scrum\u73af\u5883\u4e0b\u6559\u6388\u8f6f\u4ef6\u5de5\u7a0b\u8bfe\u7a0b\u7684\u7ecf\u9a8c\uff0c\u7279\u522b\u662f\u5728\u6bd5\u4e1a\u5e74\u7ea7\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u548c\u8f6f\u4ef6\u5de5\u7a0b\u9879\u76ee\u7ba1\u7406\u8bfe\u7a0b\u4e2d\u3002\u4ed6\u4eec\u5e0c\u671b\u5206\u4eab\u8fd9\u4e9b\u7ecf\u9a8c\u6559\u8bad\uff0c\u5e2e\u52a9\u5176\u4ed6\u6559\u80b2\u5de5\u4f5c\u8005\u66f4\u597d\u5730\u5728\u654f\u6377\u73af\u5883\u4e2d\u6559\u6388\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u8bc1\u3002", "method": "\u57fa\u4e8e\u5341\u591a\u5e74\u7684\u6559\u5b66\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u603b\u7ed3\u5728\u654f\u6377/Scrum\u73af\u5883\u4e0b\u6559\u6388\u8f6f\u4ef6\u8d28\u91cf\u65b9\u9762\u7684\u7ecf\u9a8c\u6559\u8bad\u3002\u901a\u8fc7\u5206\u6790\u6bd5\u4e1a\u5e74\u7ea7\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u548c\u8f6f\u4ef6\u5de5\u7a0b\u9879\u76ee\u7ba1\u7406\u8bfe\u7a0b\u7684\u6559\u5b66\u5b9e\u8df5\uff0c\u8bc6\u522b\u6709\u6548\u7684\u6559\u5b66\u65b9\u6cd5\u548c\u9762\u4e34\u7684\u6311\u6218\u3002", "result": "\u63d0\u4f9b\u4e86\u5728\u57fa\u4e8e\u9879\u76ee\u7684\u654f\u6377\u5b66\u4e60\u4e2d\u5d4c\u5165\u8d28\u91cf\u4fdd\u8bc1\u4e3b\u9898\u7684\u5177\u4f53\u5efa\u8bae\u3002\u8fd9\u4e9b\u5efa\u8bae\u57fa\u4e8e\u5b9e\u9645\u6559\u5b66\u7ecf\u9a8c\uff0c\u65e8\u5728\u5e2e\u52a9\u6559\u80b2\u5de5\u4f5c\u8005\u66f4\u6709\u6548\u5730\u5c06\u8d28\u91cf\u4fdd\u8bc1\u6982\u5ff5\u6574\u5408\u5230\u654f\u6377\u5f00\u53d1\u73af\u5883\u4e2d\u3002", "conclusion": "\u5728\u654f\u6377/Scrum\u73af\u5883\u4e0b\u6559\u6388\u8f6f\u4ef6\u8d28\u91cf\u9700\u8981\u7279\u5b9a\u7684\u6559\u5b66\u7b56\u7565\u548c\u65b9\u6cd5\u3002\u901a\u8fc7\u5206\u4eab\u8fd9\u4e9b\u7ecf\u9a8c\u6559\u8bad\u548c\u5efa\u8bae\uff0c\u53ef\u4ee5\u5e2e\u52a9\u6559\u80b2\u5de5\u4f5c\u8005\u66f4\u597d\u5730\u8bbe\u8ba1\u548c\u5b9e\u65bd\u76f8\u5173\u8bfe\u7a0b\uff0c\u57f9\u517b\u5b66\u751f\u5728\u654f\u6377\u73af\u5883\u4e2d\u786e\u4fdd\u8f6f\u4ef6\u8d28\u91cf\u7684\u80fd\u529b\u3002"}}
{"id": "2512.22231", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22231", "abs": "https://arxiv.org/abs/2512.22231", "authors": ["Nachiappan Chockalingam", "Akshay Deshpande", "Lokesh Butra", "Ram Sekhar Bodala", "Nitin Saksena", "Adithya Parthasarathy", "Balakrishna Pothineni", "Akash Kumar Agarwal"], "title": "Scalable Cloud-Native Architectures for Intelligent PMU Data Processing", "comment": null, "summary": "Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408AI\u3001\u8fb9\u7f18\u8ba1\u7b97\u4e0e\u4e91\u8ba1\u7b97\u7684\u4e91\u539f\u751f\u67b6\u6784\uff0c\u7528\u4e8e\u5904\u7406\u5927\u89c4\u6a21PMU\u6570\u636e\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u7535\u7f51\u76d1\u63a7\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0fPMU\u6570\u636e\u5904\u7406\u67b6\u6784\u96be\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u7535\u7f51\u4e2dPMU\u90e8\u7f72\u89c4\u6a21\u6269\u5927\u5e26\u6765\u7684\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u8fd0\u884c\u6761\u4ef6\u4e0b\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u6d41\u5904\u7406\u3001\u5bb9\u5668\u5316\u5fae\u670d\u52a1\u548c\u5f39\u6027\u8d44\u6e90\u7f16\u6392\u7684\u4e91\u539f\u751f\u67b6\u6784\uff0c\u96c6\u6210\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u5e76\u5d4c\u5165\u5b89\u5168\u9690\u79c1\u673a\u5236\u3002", "result": "\u5206\u6790\u6a21\u578b\u663e\u793a\u8be5\u67b6\u6784\u53ef\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u54cd\u5e94\u65f6\u95f4\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5927\u89c4\u6a21PMU\u90e8\u7f72\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u541e\u5410\u91cf\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u4e91\u539f\u751f\u67b6\u6784\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u7535\u7f51\u5206\u6790\u63d0\u4f9b\u4e86\u5f3a\u5927\u7075\u6d3b\u7684\u57fa\u7840\uff0c\u80fd\u591f\u6ee1\u8db3\u5927\u89c4\u6a21PMU\u6570\u636e\u5904\u7406\u7684\u9700\u6c42\u3002"}}
{"id": "2512.23498", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23498", "abs": "https://arxiv.org/abs/2512.23498", "authors": ["Henrique De Medeiros", "Denisse Mu\u00f1ante", "Sophie Chabridon", "C\u00e9sar Perdig\u00e3o Batista", "Denis Conan"], "title": "Adaptable Teastore with Energy Consumption Awareness: A Case Study", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "[Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.\n  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.\n  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.\n  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.", "AI": {"tldr": "EnCoMSAS\u5de5\u5177\u7528\u4e8e\u76d1\u63a7\u81ea\u9002\u5e94\u6027\u7cfb\u7edf\u7684\u80fd\u8017\uff0c\u652f\u6301\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574\u4ee5\u63d0\u5347\u80fd\u6548\uff0c\u5bf9\u7cfb\u7edf\u6574\u4f53\u80fd\u8017\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u6301\u7eed\u589e\u957f\uff0c\u4e91\u5e94\u7528\u8fc1\u79fb\u548c\u6570\u5b57\u5185\u5bb9\u6d88\u8d39\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u52a8\u6001\u81ea\u9002\u5e94\u65b9\u6cd5\u6709\u671b\u5728\u8fd0\u884c\u65f6\u964d\u4f4e\u8f6f\u4ef6\u5e94\u7528\u7684\u80fd\u8017\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u80fd\u8017\u76d1\u63a7\u5de5\u5177\u6765\u5b9e\u73b0\u80fd\u8017\u611f\u77e5\u3002", "method": "\u5f00\u53d1EnCoMSAS\u5de5\u5177\u6765\u6536\u96c6\u5206\u5e03\u5f0f\u4e91\u5e94\u7528\u7684\u80fd\u8017\u6570\u636e\uff0c\u652f\u6301SAS\u53d8\u4f53\u7684\u8fd0\u884c\u65f6\u80fd\u8017\u8bc4\u4f30\u3002\u4f7f\u7528Adaptable TeaStore\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u901a\u8fc7Grid5000\u6d4b\u8bd5\u5e73\u53f0\u5728\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u63a8\u8350\u670d\u52a1\u7684\u80fd\u8017\u3002", "result": "EnCoMSAS\u80fd\u6709\u6548\u6536\u96c6\u80fd\u8017\u6570\u636e\u652f\u6301\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574\uff1bCPU\u4f7f\u7528\u7387\u4e0e\u80fd\u8017\u7684\u76f8\u5173\u6027\u9a8c\u8bc1\u4e86\u6d4b\u91cf\u6709\u6548\u6027\uff1b\u80fd\u8017\u53d7\u7b97\u6cd5\u590d\u6742\u5ea6\u548c\u90e8\u7f72\u73af\u5883\u5f71\u54cd\uff1bEnCoMSAS\u5bf9SAS\u751f\u6001\u7cfb\u7edf\u6574\u4f53\u80fd\u8017\u5f71\u54cd\u76f8\u5bf9\u8f83\u5c0f\u3002", "conclusion": "EnCoMSAS\u5de5\u5177\u4e3a\u81ea\u9002\u5e94\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u80fd\u8017\u76d1\u63a7\u80fd\u529b\uff0c\u652f\u6301\u80fd\u8017\u611f\u77e5\u7684\u52a8\u6001\u8c03\u6574\uff0c\u4e14\u5bf9\u7cfb\u7edf\u6574\u4f53\u80fd\u8017\u5f71\u54cd\u6709\u9650\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u8282\u80fd\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002"}}
{"id": "2512.22402", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22402", "abs": "https://arxiv.org/abs/2512.22402", "authors": ["Bhanu Prakash Vangala", "Tanu Malik"], "title": "Efficient Multi-Model Orchestration for Self-Hosted Large Language Models", "comment": null, "summary": "Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.", "AI": {"tldr": "Pick and Spin\u662f\u4e00\u4e2a\u57fa\u4e8eKubernetes\u7684LLM\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u90e8\u7f72\u3001\u81ea\u9002\u5e94\u6269\u7f29\u5bb9\u548c\u6df7\u5408\u8def\u7531\u7b56\u7565\uff0c\u5b9e\u73b0\u81ea\u6258\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ecf\u6d4e\u9ad8\u6548\u90e8\u7f72\uff0c\u76f8\u6bd4\u9759\u6001\u90e8\u7f72\u53ef\u63d0\u534721.6%\u6210\u529f\u7387\u3001\u964d\u4f4e30%\u5ef6\u8fdf\u548c33%GPU\u6210\u672c\u3002", "motivation": "\u7ec4\u7ec7\u81ea\u6258\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34GPU\u5229\u7528\u7387\u4f4e\u3001\u5de5\u4f5c\u8d1f\u8f7d\u8def\u7531\u56f0\u96be\u548c\u53ef\u9760\u6027\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5e73\u8861\u9690\u79c1\u3001\u6210\u672c\u63a7\u5236\u548c\u5b9a\u5236\u5316\u9700\u6c42\u3002", "method": "\u57fa\u4e8eKubernetes\u6784\u5efa\uff0c\u5305\u542b\u7edf\u4e00Helm\u90e8\u7f72\u7cfb\u7edf\u3001\u81ea\u9002\u5e94scale-to-zero\u81ea\u52a8\u5316\u3001\u6df7\u5408\u8def\u7531\u6a21\u5757\uff08\u7ed3\u5408\u5173\u952e\u8bcd\u542f\u53d1\u5f0f\u548c\u8f7b\u91cf\u7ea7DistilBERT\u5206\u7c7b\u5668\uff09\uff0c\u5728\u56db\u4e2a\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u8bc4\u4f30\u4e86\u4e94\u79cd\u63a8\u7406\u7b56\u7565\u548c\u4e24\u79cd\u8def\u7531\u53d8\u4f53\u3002", "result": "\u572831,019\u4e2a\u63d0\u793a\u548c163,720\u6b21\u63a8\u7406\u8fd0\u884c\u7684\u8bc4\u4f30\u4e2d\uff0c\u76f8\u6bd4\u76f8\u540c\u6a21\u578b\u7684\u9759\u6001\u90e8\u7f72\uff0cPick and Spin\u5b9e\u73b0\u4e86\u6700\u9ad821.6%\u7684\u6210\u529f\u7387\u63d0\u5347\u300130%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c33%\u7684GPU\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "Pick and Spin\u4e3a\u81ea\u6258\u7ba1LLM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u7684\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u8def\u7531\u548c\u8d44\u6e90\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u90e8\u7f72\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2512.23499", "categories": ["cs.SE", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.23499", "abs": "https://arxiv.org/abs/2512.23499", "authors": ["Brice Arl\u00e9on Zemtsop Ndadji", "Simon Bliudze", "Cl\u00e9ment Quinton"], "title": "AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "Modern cloud architectures demand self-adaptive capabilities to manage dynamic operational conditions. Yet, existing solutions often impose centralized control models ill-suited to microservices decentralized nature. This paper presents AdaptiFlow, a framework that leverages well-established principles of autonomous computing to provide abstraction layers focused on the Monitor and Execute phases of the MAPE-K loop. By decoupling metrics collection and action execution from adaptation logic, AdaptiFlow enables microservices to evolve into autonomous elements through standardized interfaces, preserving their architectural independence while enabling system-wide adaptability. The framework introduces: (1) Metrics Collectors for unified infrastructure/business metric gathering, (2) Adaptation Actions as declarative actuators for runtime adjustments, and (3) a lightweight Event-Driven and rule-based mechanism for adaptation logic specification. Validation through the enhanced Adaptable TeaStore benchmark demonstrates practical implementation of three adaptation scenarios targeting three levels of autonomy self-healing (database recovery), self-protection (DDoS mitigation), and self-optimization (traffic management) with minimal code modification per service. Key innovations include a workflow for service instrumentation and evidence that decentralized adaptation can emerge from localized decisions without global coordination. The work bridges autonomic computing theory with cloud-native practice, providing both a conceptual framework and concrete tools for building resilient distributed systems. Future work includes integration with formal coordination models and application of adaptation techniques relying on AI agents for proactive adaptation to address complex adaptation scenarios.", "AI": {"tldr": "AdaptiFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u4e3b\u8ba1\u7b97\u539f\u5219\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u76d1\u63a7\u4e0e\u6267\u884c\u9636\u6bb5\uff0c\u4f7f\u5fae\u670d\u52a1\u80fd\u591f\u81ea\u4e3b\u9002\u5e94\u52a8\u6001\u4e91\u73af\u5883\uff0c\u652f\u6301\u81ea\u6108\u3001\u81ea\u4fdd\u62a4\u548c\u81ea\u4f18\u5316\u4e09\u79cd\u81ea\u4e3b\u6027\u7ea7\u522b\u3002", "motivation": "\u73b0\u4ee3\u4e91\u67b6\u6784\u9700\u8981\u81ea\u9002\u5e94\u80fd\u529b\u6765\u7ba1\u7406\u52a8\u6001\u8fd0\u884c\u6761\u4ef6\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u91c7\u7528\u96c6\u4e2d\u5f0f\u63a7\u5236\u6a21\u578b\uff0c\u4e0d\u9002\u5408\u5fae\u670d\u52a1\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u5fae\u670d\u52a1\u67b6\u6784\u72ec\u7acb\u6027\u540c\u65f6\u5b9e\u73b0\u7cfb\u7edf\u8303\u56f4\u9002\u5e94\u6027\u7684\u6846\u67b6\u3002", "method": "AdaptiFlow\u6846\u67b6\u57fa\u4e8eMAPE-K\u5faa\u73af\u7684\u76d1\u63a7\u548c\u6267\u884c\u9636\u6bb5\uff0c\u63d0\u4f9b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u7edf\u4e00\u7684\u57fa\u7840\u8bbe\u65bd/\u4e1a\u52a1\u6307\u6807\u6536\u96c6\u5668\uff1b(2)\u58f0\u660e\u5f0f\u6267\u884c\u5668\u7528\u4e8e\u8fd0\u884c\u65f6\u8c03\u6574\uff1b(3)\u8f7b\u91cf\u7ea7\u4e8b\u4ef6\u9a71\u52a8\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u9002\u5e94\u903b\u8f91\u89c4\u8303\u673a\u5236\u3002\u901a\u8fc7\u6807\u51c6\u5316\u63a5\u53e3\u5b9e\u73b0\u6307\u6807\u6536\u96c6\u548c\u52a8\u4f5c\u6267\u884c\u4e0e\u9002\u5e94\u903b\u8f91\u7684\u89e3\u8026\u3002", "result": "\u901a\u8fc7\u589e\u5f3a\u7684Adaptable TeaStore\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\uff0c\u5b9e\u73b0\u4e86\u4e09\u79cd\u9002\u5e94\u573a\u666f\uff1a\u6570\u636e\u5e93\u6062\u590d\uff08\u81ea\u6108\uff09\u3001DDoS\u7f13\u89e3\uff08\u81ea\u4fdd\u62a4\uff09\u548c\u6d41\u91cf\u7ba1\u7406\uff08\u81ea\u4f18\u5316\uff09\uff0c\u6bcf\u4e2a\u670d\u52a1\u53ea\u9700\u6700\u5c0f\u4ee3\u7801\u4fee\u6539\u3002\u8bc1\u660e\u4e86\u53bb\u4e2d\u5fc3\u5316\u9002\u5e94\u53ef\u4ee5\u901a\u8fc7\u5c40\u90e8\u51b3\u7b56\u5b9e\u73b0\uff0c\u65e0\u9700\u5168\u5c40\u534f\u8c03\u3002", "conclusion": "AdaptiFlow\u5c06\u81ea\u4e3b\u8ba1\u7b97\u7406\u8bba\u4e0e\u4e91\u539f\u751f\u5b9e\u8df5\u76f8\u7ed3\u5408\uff0c\u4e3a\u6784\u5efa\u5f39\u6027\u5206\u5e03\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6982\u5ff5\u6846\u67b6\u548c\u5177\u4f53\u5de5\u5177\u3002\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u4e0e\u6b63\u5f0f\u534f\u8c03\u6a21\u578b\u7684\u96c6\u6210\uff0c\u4ee5\u53ca\u5e94\u7528\u57fa\u4e8eAI\u4ee3\u7406\u7684\u9002\u5e94\u6280\u672f\u6765\u5904\u7406\u66f4\u590d\u6742\u7684\u9002\u5e94\u573a\u666f\u3002"}}
{"id": "2512.22420", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.22420", "abs": "https://arxiv.org/abs/2512.22420", "authors": ["Rui Li", "Zhaoning Zhang", "Libo Zhang", "Huaimin Wang", "Xiang Fu", "Zhiquan Lai"], "title": "Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving", "comment": "6 pages, 11 figures", "summary": "Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.", "AI": {"tldr": "Nightjar\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u8bf7\u6c42\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u63a8\u6d4b\u957f\u5ea6\uff0c\u5728\u5b9e\u65f6\u670d\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u4f7f\u7528\u56fa\u5b9a\u7684\u63a8\u6d4b\u957f\u5ea6\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u8bf7\u6c42\u8d1f\u8f7d\uff0c\u5728\u9ad8\u8d1f\u8f7d\u8ba1\u7b97\u53d7\u9650\u73af\u5883\u4e2d\u53cd\u800c\u4f1a\u56e0\u9a8c\u8bc1\u5f00\u9500\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u5728\u771f\u5b9e\u670d\u52a1\u573a\u666f\u4e2d\u5f62\u6210\u4e86\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51faNightjar\u7b97\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u63a8\u6d4b\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u6839\u636e\u6279\u5904\u7406\u5927\u5c0f\u52a8\u6001\u9009\u62e9\u6700\u4f18\u63a8\u6d4b\u957f\u5ea6\uff0c\u751a\u81f3\u5728\u63a8\u6d4b\u89e3\u7801\u65e0\u76ca\u65f6\u5b8c\u5168\u7981\u7528\u8be5\u529f\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cNightjar\u76f8\u6bd4\u6807\u51c6\u63a8\u6d4b\u89e3\u7801\u5b9e\u73b0\u4e86\u9ad8\u8fbe14.8%\u7684\u541e\u5410\u91cf\u63d0\u5347\u548c20.2%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5728\u5b9e\u65f6\u670d\u52a1\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "Nightjar\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u6d4b\u957f\u5ea6\u89e3\u51b3\u4e86\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u5728\u52a8\u6001\u8d1f\u8f7d\u73af\u5883\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\uff0c\u4e3aLLM\u63a8\u7406\u670d\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.23511", "categories": ["cs.SE", "cs.FL"], "pdf": "https://arxiv.org/pdf/2512.23511", "abs": "https://arxiv.org/abs/2512.23511", "authors": ["Xinyi Zheng", "Ningke Li", "Xiaokun Luan", "Kailong Wang", "Ling Shi", "Meng Sun", "Haoyu Wang"], "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.\n  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.", "AI": {"tldr": "MATP\u662f\u4e00\u4e2a\u901a\u8fc7\u591a\u6b65\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u9a8c\u8bc1LLM\u63a8\u7406\u903b\u8f91\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8f6c\u5316\u4e3a\u4e00\u9636\u903b\u8f91\uff0c\u4f7f\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u8bc4\u4f30\u903b\u8f91\u6709\u6548\u6027\uff0c\u572810,830\u4e2a\u63a8\u7406\u5b9e\u4f8b\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd542\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "LLMs\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u79d1\u5b66\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u6d41\u7545\u8bed\u8a00\u5e38\u63a9\u76d6\u7ec6\u5fae\u903b\u8f91\u9519\u8bef\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u4e8b\u5b9e\u6838\u67e5\u3001\u81ea\u4e00\u81f4\u6027\u65b9\u6cd5\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u9a8c\u8bc1\u65e0\u6cd5\u68c0\u6d4b\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u590d\u6742\u903b\u8f91\u7f3a\u9677\u3002", "method": "MATP\u5c06\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8f6c\u5316\u4e3a\u4e00\u9636\u903b\u8f91\uff0c\u5e94\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u8fdb\u884c\u9010\u6b65\u903b\u8f91\u6709\u6548\u6027\u8bc4\u4f30\uff0c\u8bc6\u522b\u9690\u85cf\u903b\u8f91\u9519\u8bef\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u63a8\u7406\u6b63\u786e\u6027\u5206\u7c7b\u3002", "result": "\u5728\u5305\u542b10,830\u4e2a\u63a8\u7406\u5b9e\u4f8b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u6765\u81eaPrOntoQA-OOD\u3001ProofWriter\u548cFOLIO\u4efb\u52a1\uff0c\u753110\u4e2aLLM\u751f\u6210\uff09\uff0cMATP\u5728\u63a8\u7406\u6b65\u9aa4\u9a8c\u8bc1\u4e0a\u8d85\u8d8a\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\u8d85\u8fc742\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u663e\u793a\u63a8\u7406\u6a21\u578b\u6bd4\u901a\u7528\u6a21\u578b\u4ea7\u751f\u66f4\u903b\u8f91\u4e00\u81f4\u7684\u8f93\u51fa\u3002", "conclusion": "MATP\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u903b\u8f91\u9a8c\u8bc1\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347LLM\u751f\u6210\u63a8\u7406\u7684\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2512.22492", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22492", "abs": "https://arxiv.org/abs/2512.22492", "authors": ["Zhenqian Chen", "Baoquan Zhong", "Xiang Li", "Qing Dai", "Xinkui Zhao", "Miao Ye", "Ren Cheng", "Lufei Zhang", "Jianwei Yin"], "title": "Role-Based Fault Tolerance System for LLM RL Post-Training", "comment": "16 pages, 19 figures", "summary": "RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.\n  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \\textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \\textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \\textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\\% failure injection frequency, RobustRL can achieve an ETTR of over 80\\% compared with the 60\\% in ByteRobust and achieves 8.4\\%-17.4\\% faster in end-to-end training time.", "AI": {"tldr": "RobustRL\u662f\u4e00\u4e2a\u9488\u5bf9LLM RL\u540e\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u89d2\u8272\u9694\u79bb\u3001\u5feb\u901f\u6062\u590d\u548c\u52a8\u6001\u91cd\u8fde\u673a\u5236\uff0c\u5728GPU\u6545\u969c\u65f6\u663e\u8457\u63d0\u5347\u6709\u6548\u8bad\u7ec3\u65f6\u95f4\u6bd4\u4f8b\u3002", "motivation": "RL\u540e\u8bad\u7ec3\u7ed3\u5408\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u73b0\u6709\u5bb9\u9519\u6846\u67b6\u53ea\u9488\u5bf9\u8bad\u7ec3\u6216\u63a8\u7406\u5355\u72ec\u4f18\u5316\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528RL\u5f02\u6b65\u6267\u884c\u7684\u6f5c\u529b\u3002GPU\u6545\u969c\u4f1a\u5bfc\u81f4\u6574\u4e2aRL\u4efb\u52a1\u91cd\u542f\uff0c\u5e26\u6765\u5de8\u5927\u7684\u56de\u653e\u548c\u521d\u59cb\u5316\u5f00\u9500\u3002", "method": "1. \u89d2\u8272\u9694\u79bb\uff1a\u5c06\u8bad\u7ec3\u5668\u3001rollout\u548c\u7ba1\u7406\u89d2\u8272\u89c6\u4e3a\u72ec\u7acb\u5206\u5e03\u5f0f\u5b50\u4efb\u52a1\uff1b2. \u68c0\u6d4b\uff1a\u89d2\u8272\u611f\u77e5\u76d1\u63a7\uff0c\u533a\u5206\u771f\u5b9e\u6545\u969c\u548c\u89d2\u8272\u7279\u5b9a\u884c\u4e3a\uff1b3. \u91cd\u542f\uff1a\u8bad\u7ec3\u5668\u975e\u4e2d\u65ad\u6062\u590d\uff08rollout\u4fdd\u6301\u72b6\u6001\uff09\uff0crollout\u673a\u5668\u9694\u79bb\u66ff\u6362\uff1b4. \u91cd\u8fde\uff1a\u7528UCX\u70b9\u5bf9\u70b9\u52a8\u6001\u901a\u4fe1\u66ff\u6362\u9759\u6001\u96c6\u5408\u901a\u4fe1\uff0c\u5b9e\u73b0\u5feb\u901f\u6743\u91cd\u540c\u6b65\u3002", "result": "\u5728256-GPU\u96c6\u7fa4\u4e0a\uff0c\u4f7f\u7528Qwen3-8B-Math\u5de5\u4f5c\u8d1f\u8f7d\uff0c10%\u6545\u969c\u6ce8\u5165\u9891\u7387\u4e0b\uff0cRobustRL\u7684\u6709\u6548\u8bad\u7ec3\u65f6\u95f4\u6bd4\u4f8b\uff08ETTR\uff09\u8d85\u8fc780%\uff0c\u76f8\u6bd4ByteRobust\u768460%\u663e\u8457\u63d0\u5347\uff0c\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u5feb8.4%-17.4%\u3002", "conclusion": "RobustRL\u901a\u8fc7\u89d2\u8272\u9694\u79bb\u548c\u5feb\u901f\u6062\u590d\u673a\u5236\uff0c\u4e3aRL\u540e\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684GPU\u5bb9\u9519\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.23575", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23575", "abs": "https://arxiv.org/abs/2512.23575", "authors": ["Kenshin Obi", "Takumi Onozawa", "Hiroshi Fujimoto", "Takuya Azumi"], "title": "Model-based Development for Autonomous Driving Software Considering Parallelization", "comment": null, "summary": "In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u5f00\u53d1\uff08MBD\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u5e76\u884c\u5316\u65b9\u6cd5\uff0c\u6269\u5c55\u73b0\u6709MBP\u65b9\u6cd5\u4ee5\u5904\u7406\u590d\u6742\u5904\u7406\uff0c\u51cf\u5c11\u6267\u884c\u65f6\u95f4\uff0c\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u9700\u8981\u5b9e\u65f6\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u79cd\u529f\u80fd\u548c\u590d\u6742\u73af\u5883\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5e76\u884c\u5316\u65b9\u6cd5\u6765\u6ee1\u8db3\u5b9e\u65f6\u6027\u8981\u6c42\u3002", "method": "\u6269\u5c55\u73b0\u6709\u7684\u57fa\u4e8e\u6a21\u578b\u5e76\u884c\u5316\u5668\uff08MBP\uff09\u65b9\u6cd5\uff0c\u5728\u6a21\u578b\u5f00\u53d1\uff08MBD\uff09\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u7684\u5e76\u884c\u5316\uff0c\u7b80\u5316\u590d\u6742\u5904\u7406\u7684\u5b9e\u73b0\u3002", "result": "\u6267\u884c\u65f6\u95f4\u5f97\u5230\u51cf\u5c11\uff0c\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u5f00\u53d1\uff0c\u7279\u522b\u662f\u5728\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eMBD\u7684\u5e76\u884c\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u9002\u5408\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5f00\u53d1\u9700\u6c42\u3002"}}
{"id": "2512.22534", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22534", "abs": "https://arxiv.org/abs/2512.22534", "authors": ["Pawissanutt Lertpongrujikorn"], "title": "Object Abstraction To Streamline Edge-Cloud-Native Application Development", "comment": "This is a doctoral dissertation submitted to University of North Texas", "summary": "Cloud computing has fundamentally transformed application development, yet a gap remains between the serverless promise of simplified deployment and its practical realization due to fragmentation across function runtimes, state management, and orchestration. This dissertation addresses this gap through empirical validation and technical innovation, establishing the Object-as-a-Service (OaaS) paradigm as a unified approach to cloud-native development. Grounded in evidence from three studies - practitioner interviews (21 participants), a human study on developer experience (39 participants), and NSF I-Corps customer discovery (101 interviews across 86 organizations) - this work demonstrates that infrastructure complexity taxes productivity, with practitioners prioritizing automation and maintainability over cost optimization. The dissertation makes five major contributions: (1) the OaaS paradigm unifies resource, state, and workflow management via the Oparaca prototype, demonstrating negligible overhead and state-of-the-art scalability; (2) SLA-driven OaaS enables declarative management of non-functional requirements like availability, consistency, and latency; (3) OaaS-IoT with EdgeWeaver extends the paradigm to the edge-cloud continuum, achieving 31% faster task completion and a 44.5% reduction in lines of code compared to traditional FaaS; (4) commercialization validation establishes a pathway targeting technology SMEs and startups; and (5) an empirical methodology for grounding technical research in validated practitioner needs. By consolidating fragmented abstractions and automating performance optimization, OaaS establishes a foundation for cloud-native platforms that hide infrastructure complexity and empower developers to focus on innovation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faObject-as-a-Service (OaaS)\u8303\u5f0f\uff0c\u901a\u8fc7\u7edf\u4e00\u8d44\u6e90\u3001\u72b6\u6001\u548c\u5de5\u4f5c\u6d41\u7ba1\u7406\u6765\u89e3\u51b3\u4e91\u539f\u751f\u5f00\u53d1\u4e2d\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u5728\u6027\u80fd\u5f00\u9500\u53ef\u5ffd\u7565\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6269\u5c55\u6027\uff0c\u5e76\u6269\u5c55\u5230\u8fb9\u7f18\u8ba1\u7b97\u9886\u57df\u3002", "motivation": "\u867d\u7136\u4e91\u8ba1\u7b97\u5df2\u7ecf\u6539\u53d8\u4e86\u5e94\u7528\u5f00\u53d1\uff0c\u4f46\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u5728\u51fd\u6570\u8fd0\u884c\u65f6\u3001\u72b6\u6001\u7ba1\u7406\u548c\u7f16\u6392\u65b9\u9762\u7684\u788e\u7247\u5316\u963b\u788d\u4e86\u5176\u7b80\u5316\u90e8\u7f72\u7684\u627f\u8bfa\u3002\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\u964d\u4f4e\u4e86\u5f00\u53d1\u8005\u7684\u751f\u4ea7\u529b\uff0c\u5b9e\u8df5\u8005\u66f4\u5173\u6ce8\u81ea\u52a8\u5316\u548c\u53ef\u7ef4\u62a4\u6027\u800c\u975e\u6210\u672c\u4f18\u5316\u3002", "method": "\u57fa\u4e8e\u4e09\u9879\u5b9e\u8bc1\u7814\u7a76\uff1a21\u540d\u4ece\u4e1a\u8005\u8bbf\u8c08\u300139\u540d\u53c2\u4e0e\u8005\u7684\u5f00\u53d1\u8005\u4f53\u9a8c\u7814\u7a76\u3001\u4ee5\u53ca86\u4e2a\u7ec4\u7ec7\u7684101\u6b21\u5ba2\u6237\u8bbf\u8c08\u3002\u6280\u672f\u4e0a\u63d0\u51faOaaS\u8303\u5f0f\uff0c\u5f00\u53d1Oparaca\u539f\u578b\uff0c\u652f\u6301SLA\u9a71\u52a8\u7684\u58f0\u660e\u5f0f\u7ba1\u7406\uff0c\u5e76\u6269\u5c55\u5230\u8fb9\u7f18\u8ba1\u7b97\uff08OaaS-IoT with EdgeWeaver\uff09\u3002", "result": "OaaS\u8303\u5f0f\u5c55\u793a\u4e86\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u548c\u6700\u5148\u8fdb\u7684\u6269\u5c55\u6027\uff1b\u8fb9\u7f18\u8ba1\u7b97\u7248\u672c\u76f8\u6bd4\u4f20\u7edfFaaS\u5b9e\u73b031%\u66f4\u5feb\u7684\u4efb\u52a1\u5b8c\u6210\u548c44.5%\u7684\u4ee3\u7801\u884c\u6570\u51cf\u5c11\uff1b\u5efa\u7acb\u4e86\u9762\u5411\u6280\u672f\u4e2d\u5c0f\u4f01\u4e1a\u548c\u521d\u521b\u4f01\u4e1a\u7684\u5546\u4e1a\u5316\u8def\u5f84\u3002", "conclusion": "OaaS\u901a\u8fc7\u6574\u5408\u788e\u7247\u5316\u62bd\u8c61\u548c\u81ea\u52a8\u5316\u6027\u80fd\u4f18\u5316\uff0c\u4e3a\u4e91\u539f\u751f\u5e73\u53f0\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u9690\u85cf\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u6027\uff0c\u4f7f\u5f00\u53d1\u8005\u80fd\u591f\u4e13\u6ce8\u4e8e\u521b\u65b0\u3002\u8be5\u7814\u7a76\u8fd8\u5efa\u7acb\u4e86\u5c06\u6280\u672f\u7814\u7a76\u57fa\u4e8e\u9a8c\u8bc1\u7684\u5b9e\u8df5\u8005\u9700\u6c42\u7684\u5b9e\u8bc1\u65b9\u6cd5\u3002"}}
{"id": "2512.23605", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23605", "abs": "https://arxiv.org/abs/2512.23605", "authors": ["Kenshin Obi", "Ryo Yoshinaka", "Hiroshi Fujimoto", "Takuya Azumi"], "title": "Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes", "comment": null, "summary": "In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u5f00\u53d1\uff08MBD\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06ROS 2\u517c\u5bb9\u7684Simulink\u6a21\u578b\u81ea\u52a8\u5e76\u884c\u5316\uff0c\u652f\u6301\u591a\u8f93\u5165\u573a\u666f\uff0c\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u3002", "motivation": "\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff08\u7279\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff09\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u4e0d\u65ad\u589e\u52a0\uff0c\u4f20\u7edf\u624b\u52a8\u5e76\u884c\u5316\u9762\u4e34\u6570\u636e\u5b8c\u6574\u6027\u548c\u5e76\u53d1\u95ee\u9898\u6311\u6218\uff0c\u73b0\u6709MBD\u65b9\u6cd5\u96be\u4ee5\u96c6\u6210ROS 2\u7b49\u73b0\u4ee3\u6846\u67b6\u7684\u591a\u8f93\u5165\u573a\u666f\u3002", "method": "\u63d0\u51faMBD\u6846\u67b6\uff0c\u5c06ROS 2\u517c\u5bb9\u7684Simulink\u6a21\u578b\u5206\u7c7b\u4e3a\u4e8b\u4ef6\u9a71\u52a8\u548c\u5b9a\u65f6\u5668\u9a71\u52a8\u7c7b\u578b\u8fdb\u884c\u9488\u5bf9\u6027\u5e76\u884c\u5316\uff0c\u6269\u5c55\u4f20\u7edfMBD\u5e76\u884c\u5316\u65b9\u6cd5\uff0c\u652f\u6301ROS 2\u591a\u8f93\u5165\u6a21\u578b\u7684\u5e76\u884c\u4ee3\u7801\u751f\u6210\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5e94\u7528\u6240\u63d0\u6846\u67b6\u8fdb\u884c\u5e76\u884c\u5316\u540e\uff0c\u6240\u6709\u6a21\u5f0f\u90fd\u663e\u793a\u51fa\u6267\u884c\u65f6\u95f4\u51cf\u5c11\uff0c\u8bc1\u5b9e\u4e86\u5e76\u884c\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86ROS 2\u96c6\u6210\u548c\u591a\u8f93\u5165\u573a\u666f\u4e0b\u7684\u5e76\u884c\u5316\u6311\u6218\uff0c\u6269\u5c55\u4e86MBD\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2512.22560", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.22560", "abs": "https://arxiv.org/abs/2512.22560", "authors": ["Wei Gao", "Yuheng Zhao", "Tianyuan Wu", "Shaopan Xiong", "Weixun Wang", "Dakai An", "Lunxi Cao", "Dilxat Muhtar", "Zichen Liu", "Haizhou Zhao", "Ju Huang", "Siran Yang", "Yongbin Li", "Wenbo Su", "Jiamang Wang", "Lin Qu", "Bo Zheng", "Wei Wang"], "title": "RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure", "comment": "17 pages, 17 figures", "summary": "Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.\n  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\\(\\times\\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.", "AI": {"tldr": "RollArc\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u89e3\u8026\u57fa\u7840\u8bbe\u65bd\u4e0a\u6700\u5927\u5316\u591a\u4efb\u52a1\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u901a\u8fc7\u786c\u4ef6\u4eb2\u548c\u6027\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u3001\u7ec6\u7c92\u5ea6\u5f02\u6b65\u548c\u72b6\u6001\u611f\u77e5\u8ba1\u7b97\u5b9e\u73b01.35-2.05\u500d\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u3002", "motivation": "\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u9ad8\u5ea6\u5f02\u6784\u6027\uff0c\u7ed3\u5408\u4e86\u8ba1\u7b97\u5bc6\u96c6\u578b\u9884\u586b\u5145\u9636\u6bb5\u3001\u5e26\u5bbd\u53d7\u9650\u7684\u89e3\u7801\u548c\u72b6\u6001\u5316\u7684CPU\u5bc6\u96c6\u578b\u73af\u5883\u6a21\u62df\u3002\u4f20\u7edf\u89e3\u8026\u65b9\u6cd5\u4f1a\u5f15\u5165\u5927\u91cf\u540c\u6b65\u5f00\u9500\u548c\u8d44\u6e90\u5229\u7528\u7387\u4e0d\u8db3\u95ee\u9898\u3002", "method": "1) \u786c\u4ef6\u4eb2\u548c\u6027\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\uff1a\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u548c\u5e26\u5bbd\u5bc6\u96c6\u578b\u4efb\u52a1\u8def\u7531\u5230\u6700\u9002\u5408\u7684GPU\u8bbe\u5907\uff1b2) \u7ec6\u7c92\u5ea6\u5f02\u6b65\uff1a\u5728\u8f68\u8ff9\u7ea7\u522b\u7ba1\u7406\u6267\u884c\u4ee5\u51cf\u5c11\u8d44\u6e90\u6c14\u6ce1\uff1b3) \u72b6\u6001\u611f\u77e5\u8ba1\u7b97\uff1a\u5c06\u65e0\u72b6\u6001\u7ec4\u4ef6\uff08\u5982\u5956\u52b1\u6a21\u578b\uff09\u5378\u8f7d\u5230\u65e0\u670d\u52a1\u5668\u57fa\u7840\u8bbe\u65bd\u5b9e\u73b0\u5f39\u6027\u6269\u5c55\u3002", "result": "RollArc\u6709\u6548\u63d0\u9ad8\u4e86\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u76f8\u6bd4\u5355\u4f53\u548c\u540c\u6b65\u57fa\u7ebf\u5b9e\u73b0\u4e861.35-2.05\u500d\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u3002\u5728\u963f\u91cc\u5df4\u5df4\u96c6\u7fa4\u4e0a\u4f7f\u75283000\u591a\u5757GPU\u8bad\u7ec3\u4e86\u6570\u767e\u4ebf\u53c2\u6570\u7684MoE\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "RollArc\u901a\u8fc7\u89e3\u8026\u57fa\u7840\u8bbe\u65bd\u548c\u667a\u80fd\u8c03\u5ea6\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.23495", "categories": ["cs.DC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.23495", "abs": "https://arxiv.org/abs/2512.23495", "authors": ["Eddy Truyen"], "title": "Decoupling Adaptive Control in TeaStore", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u6280\u672f\u65b9\u6cd5\uff08\u8f6f\u4ef6\u67b6\u6784\u3001\u4e91\u539f\u751fOperator\u6a21\u5f0f\u3001\u4f20\u7edf\u7f16\u7a0b\u6280\u672f\uff09\u5b9e\u73b0TeaStore\u5fae\u670d\u52a1\u7684\u81ea\u9002\u5e94\u6027\u63a7\u5236\uff0c\u5206\u6790\u5404\u79cd\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u8868\u8fbe\u4e0e\u7cfb\u7edf\u7ea7\u63a7\u5236\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u591a\u5c42\u67b6\u6784\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5b9e\u73b0\u5fae\u670d\u52a1\u81ea\u9002\u5e94\u6027\u63a7\u5236\u65f6\u9700\u8981\u8003\u8651\u7cfb\u7edf\u4e00\u81f4\u6027\u3001\u89c4\u5212\u6027\u548c\u6a21\u5757\u6027\u7b49\u5173\u952e\u5c5e\u6027\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5c06\u81ea\u9002\u5e94\u63a7\u5236\u903b\u8f91\u4e0e\u4e1a\u52a1\u5e94\u7528\u89e3\u8026\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u5b9e\u73b0\u65b9\u6cd5\u7684\u6743\u8861\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8f6f\u4ef6\u67b6\u6784\u65b9\u6cd5\u3001\u4e91\u539f\u751fOperator\u6a21\u5f0f\u548c\u4f20\u7edf\u7f16\u7a0b\u6280\u672f\u4e09\u79cd\u65b9\u6cd5\uff0c\u7814\u7a76\u5b83\u4eec\u5982\u4f55\u5b9e\u73b0\u81ea\u9002\u5e94\u63a7\u5236\u903b\u8f91\u4e0eTeaStore\u5e94\u7528\u7684\u89e3\u8026\uff0c\u5e76\u8bc4\u4f30\u5404\u79cd\u65b9\u6cd5\u5728\u8868\u8fbe\u7ec6\u7c92\u5ea6\u81ea\u9002\u5e94\u4e0e\u7cfb\u7edf\u7ea7\u63a7\u5236\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5206\u6790\u8868\u660e\u8fd9\u4e9b\u65b9\u6cd5\u5e76\u975e\u4e92\u65a5\uff0c\u53ef\u4ee5\u7ed3\u5408\u5f62\u6210\u591a\u5c42\u67b6\u6784\u7684\u81ea\u9002\u5e94\u5fae\u670d\u52a1\u7cfb\u7edf\uff0c\u5176\u4e2d\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u5c42\u9762\u53d1\u6325\u4f5c\u7528\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u7b56\u7565\u7684\u6709\u6548\u590d\u7528\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8f6f\u4ef6\u67b6\u6784\u65b9\u6cd5\u3001\u4e91\u539f\u751fOperator\u6a21\u5f0f\u548c\u4f20\u7edf\u7f16\u7a0b\u6280\u672f\uff0c\u53ef\u4ee5\u6784\u5efa\u591a\u5c42\u67b6\u6784\u7684\u81ea\u9002\u5e94\u5fae\u670d\u52a1\u7cfb\u7edf\uff0c\u5e73\u8861\u7ec6\u7c92\u5ea6\u8868\u8fbe\u4e0e\u7cfb\u7edf\u7ea7\u63a7\u5236\u7684\u9700\u6c42\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u7b56\u7565\u7684\u6709\u6548\u590d\u7528\u3002"}}
{"id": "2512.22695", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22695", "abs": "https://arxiv.org/abs/2512.22695", "authors": ["Mona Moghadampanah", "Adib Rezaei Shahmirzadi", "Farhana Amin", "Dimitrios S. Nikolopoulos"], "title": "Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference", "comment": null, "summary": "Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u8be6\u7ec6\u5206\u6790\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u80fd\u8017\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u8f93\u5165\u5bfc\u81f417%-94%\u7684\u989d\u5916\u80fd\u8017\uff0c\u5e76\u63d0\u51fa\u9636\u6bb5\u7ea7DVFS\u4f18\u5316\u65b9\u6848", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u6a21\u578b\u57fa\u7840\u4e0a\u589e\u52a0\u4e86\u89c6\u89c9\u7b49\u6a21\u6001\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6a21\u578b\uff0c\u5bf9\u591a\u6a21\u6001\u5e26\u6765\u7684\u80fd\u8017\u6743\u8861\u7f3a\u4e4f\u7406\u89e3\uff0c\u7279\u522b\u662f\u6a21\u6001\u81a8\u80c0\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898", "method": "\u5c06MLLM\u63a8\u7406\u6d41\u7a0b\u5206\u89e3\u4e3a\u89c6\u89c9\u7f16\u7801\u3001\u9884\u586b\u5145\u548c\u89e3\u7801\u4e09\u4e2a\u9636\u6bb5\uff0c\u5728NVIDIA A100 GPU\u4e0a\u8bc4\u4f30\u56db\u79cd\u4ee3\u8868\u6027MLLM\uff0c\u91cf\u5316\u591a\u6a21\u6001\u63a8\u7406\u76f8\u6bd4\u6587\u672c\u57fa\u7ebf\u7684\u989d\u5916\u80fd\u8017\uff0c\u5206\u6790GPU\u529f\u7387\u8f68\u8ff9\u548c\u5229\u7528\u7387", "result": "\u591a\u6a21\u6001\u63a8\u7406\u80fd\u8017\u6bd4\u6587\u672c\u57fa\u7ebf\u9ad817%-94%\uff0c\u80fd\u8017\u74f6\u9888\u56e0\u67b6\u6784\u800c\u5f02\uff08\u8ba1\u7b97\u5bc6\u96c6\u7684\u89c6\u89c9\u7f16\u7801\u5668\u6216\u5927\u578b\u89c6\u89c9\u6807\u8bb0\u5e8f\u5217\uff09\uff0c\u53d1\u73b0GPU\u5728\u63a8\u7406\u4e2d\u5b58\u5728\u663e\u8457\u5229\u7528\u4e0d\u8db3\uff0c\u8f93\u5165\u590d\u6742\u5ea6\u5bfc\u81f4\u4e0d\u540c\u6a21\u578b\u7684\u80fd\u8017\u6269\u5c55\u884c\u4e3a\u5dee\u5f02", "conclusion": "\u9636\u6bb5\u7ea7\u52a8\u6001\u7535\u538b\u9891\u7387\u7f29\u653e\u662f\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u5728\u6027\u80fd\u5f71\u54cd\u8f83\u5c0f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8282\u80fd\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u8282\u80fd\u7684\u591a\u6a21\u6001LLM\u670d\u52a1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u548c\u5177\u4f53\u6307\u5bfc"}}
{"id": "2512.22743", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.22743", "abs": "https://arxiv.org/abs/2512.22743", "authors": ["Ertza Warraich", "Ali Imran", "Annus Zulfiqar", "Shay Vargaftik", "Sonia Fahmy", "Muhammad Shahbaz"], "title": "OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads", "comment": "15 pages", "summary": "As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.\n  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).\n  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.", "AI": {"tldr": "OptiNIC\u662f\u4e00\u4e2a\u4e13\u4e3a\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u8bbe\u8ba1\u7684RDMA\u4f20\u8f93\u534f\u8bae\uff0c\u901a\u8fc7\u6d88\u9664\u91cd\u4f20\u548c\u987a\u5e8f\u4ea4\u4ed8\u8981\u6c42\uff0c\u5229\u7528ML\u5bf9\u6570\u636e\u4e22\u5931\u7684\u5bb9\u5fcd\u6027\u6765\u964d\u4f4e\u5c3e\u5ef6\u8fdf\uff0c\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0fML\u6269\u5c55\u5230\u6570\u5343\u4e2aGPU\uff0c\u96c6\u4f53\u901a\u4fe1\u4e2d\u7684\u5c3e\u5ef6\u8fdf\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u4f20\u7edfRDMA\u4f20\u8f93\uff08\u5982RoCE\u3001IRN\u7b49\uff09\u5f3a\u5236\u4e25\u683c\u7684\u53ef\u9760\u6027\u548c\u987a\u5e8f\u4ea4\u4ed8\uff0c\u4f9d\u8d56\u91cd\u4f20\u548c\u5305\u6392\u5e8f\uff0c\u8fd9\u4e9b\u673a\u5236\u5728ML\u573a\u666f\u4e2d\u5f15\u5165\u4e86\u4e0d\u5fc5\u8981\u7684\u590d\u6742\u6027\u548c\u5ef6\u8fdf\uff0c\u5373\u4f7f\u7f55\u89c1\u7684\u5305\u5ef6\u8fdf\u4e5f\u4f1a\u963b\u585e\u6574\u4e2a\u6a21\u578b\u6d41\u6c34\u7ebf\u3002", "method": "OptiNIC\u91cd\u65b0\u5ba1\u89c6\u4f20\u7edf\u53ef\u9760\u6027\u4fdd\u8bc1\uff0c\u57fa\u4e8eML\u5bf9\u90e8\u5206\u6216\u7f3a\u5931\u6570\u636e\u7684\u5bb9\u5fcd\u6027\uff0c\u4eceNIC\u4e2d\u6d88\u9664\u91cd\u4f20\u548c\u987a\u5e8f\u4ea4\u4ed8\uff0c\u5b9e\u73b0\u5c3d\u529b\u800c\u4e3a\u3001\u4e71\u5e8f\u7684RDMA\u4f20\u8f93\u6a21\u578b\u3002\u5b83\u5f15\u5165\u81ea\u9002\u5e94\u8d85\u65f6\u673a\u5236\u6765\u89e6\u53d1\u8fdb\u5ea6\u63a8\u8fdb\uff0c\u540c\u65f6\u4fdd\u7559\u6807\u51c6\u62e5\u585e\u63a7\u5236\u673a\u5236\uff08\u5982DCQCN\u3001EQDS\u3001Swift\uff09\uff0c\u5c06\u4e22\u5931\u6062\u590d\u8f6c\u79fb\u5230ML\u6d41\u6c34\u7ebf\u672c\u8eab\uff08\u901a\u8fc7Hadamard\u53d8\u6362\u548c\u64e6\u9664\u7f16\u7801\uff09\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0cOptiNIC\u5728\u4e24\u4e2a\u516c\u5171\u4e91\uff08Hyperstack\u548cCloudLab\uff09\u4e0a\uff0c\u5c06\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u65f6\u95f4\u5230\u51c6\u786e\u7387\uff08TTA\uff09\u5206\u522b\u63d0\u53472\u500d\uff0c\u541e\u5410\u91cf\u5206\u522b\u63d0\u53471.6\u500d\u3002\u540c\u65f6\u5c06\u7b2c99\u767e\u5206\u4f4d\u5ef6\u8fdf\u964d\u4f4e3.5\u500d\uff0cBRAM\u4f7f\u7528\u51cf\u5c112.7\u500d\uff0cNIC\u5bb9\u9519\u80fd\u529b\u51e0\u4e4e\u7ffb\u500d\u3002", "conclusion": "OptiNIC\u4e3a\u5206\u5e03\u5f0fML\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u5f39\u6027\u3001\u5c3e\u90e8\u4f18\u5316\u7684RDMA\u4f20\u8f93\u534f\u8bae\uff0c\u901a\u8fc7\u5229\u7528ML\u5e94\u7528\u5bf9\u6570\u636e\u4e22\u5931\u7684\u5bb9\u5fcd\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u5206\u5e03\u5f0fML\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2512.22925", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22925", "abs": "https://arxiv.org/abs/2512.22925", "authors": ["Panlong Wu", "Yifei Zhong", "Danyang Chen", "Ting Wang", "Fangxin Wang"], "title": "Argus: Token Aware Distributed LLM Inference Optimization", "comment": null, "summary": "Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.", "AI": {"tldr": "Argus\uff1a\u9996\u4e2a\u9762\u5411\u5f02\u6784\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u7684token\u611f\u77e5\u5206\u5e03\u5f0fLLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u5ea6\u9884\u6d4b\u548cLyapunov\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u4efb\u52a1\u5378\u8f7d", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u5ffd\u89c6\u4e86\u8fb9\u7f18-\u4e91\u5f02\u6784\u73af\u5883\u7684\u52a8\u6001\u6027\u3001\u968f\u673a\u6027\u548c\u5f02\u6784\u6027\uff0c\u5ffd\u7565\u4e86\u53ef\u53d8\u8f93\u51fatoken\u957f\u5ea6\u548c\u8bbe\u5907\u591a\u6837\u6027\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u63a8\u7406\u65f6\u95f4\u53d8\u5f02\u6027\u5927", "method": "1. \u957f\u5ea6\u611f\u77e5\u8bed\u4e49(LAS)\u6a21\u5757\uff1a\u4f7f\u7528\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u8f93\u51fatoken\u957f\u5ea6\uff1b2. Lyapunov\u5f15\u5bfc\u7684\u5378\u8f7d\u4f18\u5316(LOO)\u6a21\u5757\uff1a\u8003\u8651LLM\u9884\u586b\u5145\u548c\u89e3\u7801\u6210\u672c\uff1b3. \u5e26\u963b\u5c3c\u548c\u62e5\u585e\u63a7\u5236\u7684\u8fed\u4ee3\u5378\u8f7d\u7b97\u6cd5(IODCC)\u89e3\u51b3\u6574\u6570\u975e\u7ebf\u6027\u89c4\u5212\u95ee\u9898", "result": "\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cArgus\u5728\u9ad8\u5ea6\u52a8\u6001\u3001\u5f02\u6784\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u6027\u80fd\u548c\u5353\u8d8a\u7684\u6548\u7387", "conclusion": "Argus\u662f\u9996\u4e2atoken\u611f\u77e5\u7684\u5206\u5e03\u5f0f\u8fb9\u7f18-\u4e91LLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u957f\u5ea6\u9884\u6d4b\u548c\u4f18\u5316\u7684\u4efb\u52a1\u5378\u8f7d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u73af\u5883\u4e2d\u7684\u63a8\u7406\u53d8\u5f02\u6027\u95ee\u9898"}}
{"id": "2512.23029", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.23029", "abs": "https://arxiv.org/abs/2512.23029", "authors": ["Alex Khalil", "Guillaume Heilles", "Maria Parraga", "Simon Heilles"], "title": "Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has been accompanied by a reliance on cloud-based, proprietary systems, raising significant concerns regarding data privacy, operational sovereignty, and escalating costs. This paper investigates the feasibility of deploying a high-performance, private LLM inference server at a cost accessible to Small and Medium Businesses (SMBs). We present a comprehensive benchmarking analysis of a locally hosted, quantized 30-billion parameter Mixture-of-Experts (MoE) model based on Qwen3, running on a consumer-grade server equipped with a next-generation NVIDIA GPU. Unlike cloud-based offerings, which are expensive and complex to integrate, our approach provides an affordable and private solution for SMBs. We evaluate two dimensions: the model's intrinsic capabilities and the server's performance under load. Model performance is benchmarked against academic and industry standards to quantify reasoning and knowledge relative to cloud services. Concurrently, we measure server efficiency through latency, tokens per second, and time to first token, analyzing scalability under increasing concurrent users. Our findings demonstrate that a carefully configured on-premises setup with emerging consumer hardware and a quantized open-source model can achieve performance comparable to cloud-based services, offering SMBs a viable pathway to deploy powerful LLMs without prohibitive costs or privacy compromises.", "AI": {"tldr": "\u672c\u5730\u90e8\u7f72\u91cf\u531630B\u53c2\u6570MoE\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\uff0c\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9ad8\u9690\u79c1\u7684LLM\u63a8\u7406\u65b9\u6848\uff0c\u6027\u80fd\u63a5\u8fd1\u4e91\u670d\u52a1\u3002", "motivation": "\u5f53\u524dLLM\u4e3b\u8981\u4f9d\u8d56\u4e91\u7aef\u4e13\u6709\u7cfb\u7edf\uff0c\u5b58\u5728\u6570\u636e\u9690\u79c1\u3001\u8fd0\u8425\u4e3b\u6743\u548c\u6210\u672c\u9ad8\u6602\u7b49\u95ee\u9898\uff0c\u4e2d\u5c0f\u4f01\u4e1a\u96be\u4ee5\u627f\u53d7\u3002\u9700\u8981\u63a2\u7d22\u4f4e\u6210\u672c\u3001\u9ad8\u9690\u79c1\u7684\u672c\u5730\u90e8\u7f72\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6d88\u8d39\u7ea7\u670d\u52a1\u5668\u642d\u8f7d\u65b0\u4e00\u4ee3NVIDIA GPU\uff0c\u90e8\u7f72\u91cf\u531630B\u53c2\u6570Mixture-of-Experts\u6a21\u578b\uff08\u57fa\u4e8eQwen3\uff09\u3002\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\uff1a\u6a21\u578b\u5185\u5728\u80fd\u529b\uff08\u63a8\u7406\u548c\u77e5\u8bc6\uff09\u548c\u670d\u52a1\u5668\u6027\u80fd\uff08\u5ef6\u8fdf\u3001\u541e\u5410\u91cf\u3001\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\u3001\u5e76\u53d1\u7528\u6237\u6269\u5c55\u6027\uff09\u3002", "result": "\u7cbe\u5fc3\u914d\u7f6e\u7684\u672c\u5730\u90e8\u7f72\u65b9\u6848\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\u91cf\u5316\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u8fbe\u5230\u4e0e\u4e91\u670d\u52a1\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684LLM\u90e8\u7f72\u8def\u5f84\u3002", "conclusion": "\u672c\u5730\u90e8\u7f72\u7684\u91cf\u5316MoE\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u80fd\u591f\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u9ad8\u9690\u79c1\u7684LLM\u63a8\u7406\u670d\u52a1\uff0c\u6027\u80fd\u53ef\u5ab2\u7f8e\u4e91\u670d\u52a1\uff0c\u89e3\u51b3\u4e86\u6210\u672c\u3001\u9690\u79c1\u548c\u8fd0\u8425\u4e3b\u6743\u7b49\u95ee\u9898\u3002"}}
{"id": "2512.23434", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.23434", "abs": "https://arxiv.org/abs/2512.23434", "authors": ["Yongjie Guan"], "title": "Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates", "comment": "14 pages, 10 figures. Includes appendices", "summary": "Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.", "AI": {"tldr": "\u63d0\u51faLocal Rendezvous Hashing (LRH)\uff0c\u901a\u8fc7\u9650\u5236HRW\u9009\u62e9\u5230\u7f13\u5b58\u5c40\u90e8\u7a97\u53e3\u7684C\u4e2a\u76f8\u90bb\u7269\u7406\u8282\u70b9\uff0c\u5728\u4fdd\u6301\u4ee4\u724c\u73af\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u8d1f\u8f7d\u5747\u8861\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4e00\u81f4\u6027\u54c8\u5e0c\u65b9\u6848\u5b58\u5728\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff1a\u57fa\u4e8e\u73af\u7684\u65b9\u6848\u9700\u8981\u5927\u91cf\u865a\u62df\u8282\u70b9\u624d\u80fd\u964d\u4f4e\u5cf0\u503c\u8d1f\u8f7d\u6bd4\uff0c\u800c\u591a\u63a2\u9488\u65b9\u6cd5\u867d\u7136\u6539\u5584\u4e86\u5747\u8861\u6027\u4f46\u5bfc\u81f4\u5185\u5b58\u8bbf\u95ee\u5206\u6563\u3001\u6027\u80fd\u4e0b\u964d\u3002", "method": "LRH\u4fdd\u6301\u4ee4\u724c\u73af\u7ed3\u6784\uff0c\u4f46\u5c06Highest Random Weight (HRW)\u9009\u62e9\u9650\u5236\u5728\u7f13\u5b58\u5c40\u90e8\u7a97\u53e3\u7684C\u4e2a\u76f8\u90bb\u7269\u7406\u8282\u70b9\u3002\u901a\u8fc7\u4e00\u6b21\u4e8c\u5206\u67e5\u627e\u5b9a\u4f4dkey\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u4e0b\u4e00\u4e2a\u4e0d\u540c\u504f\u79fb\u91cf\u679a\u4e3e\u6070\u597dC\u4e2a\u4e0d\u540c\u7684\u5019\u9009\u8282\u70b9\uff0c\u7136\u540e\u9009\u62e9HRW\u80dc\u51fa\u8005\uff08\u53ef\u9009\u52a0\u6743\uff09\u3002\u67e5\u627e\u6210\u672c\u4e3aO(log|R| + C)\u3002", "result": "\u5728N=5000\u8282\u70b9\u3001V=256\u865a\u62df\u8282\u70b9\u3001K=5000\u4e07\u952e\u3001C=8\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLRH\u5c06\u6700\u5927/\u5e73\u5747\u8d1f\u8f7d\u6bd4\u4ece1.2785\u964d\u81f31.0947\uff0c\u8fbe\u523060.05 Mkeys/s\uff0c\u6bd48\u63a2\u9488\u591a\u63a2\u9488\u4e00\u81f4\u6027\u54c8\u5e0c\u5feb\u7ea66.8\u500d\uff088.80 Mkeys/s\uff09\uff0c\u540c\u65f6\u63a5\u8fd1\u5176\u5747\u8861\u6027\uff08\u6700\u5927/\u5e73\u5747\u8d1f\u8f7d\u6bd41.0697\uff09\u3002", "conclusion": "LRH\u5728\u4fdd\u6301\u4ee4\u724c\u73af\u7ed3\u6784\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u7f13\u5b58\u5c40\u90e8\u5316\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u8d1f\u8f7d\u5747\u8861\u6027\u548c\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e00\u81f4\u6027\u54c8\u5e0c\u65b9\u6848\u5728\u8d1f\u8f7d\u5747\u8861\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2512.23439", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.23439", "abs": "https://arxiv.org/abs/2512.23439", "authors": ["Marko Vukoli\u0107", "Orestis Alpos", "Jakov Mitrovski", "Themis Papameletiou", "Nikola Risti\u0107", "Dionysis Zindros"], "title": "Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets", "comment": "31 pages, 13 figures", "summary": "We introduce Bitcoin-IPC, a software stack and protocol that scales Bitcoin towards helping it become the universal Medium of Exchange (MoE) by enabling the permissionless creation of fully programmable Proof-of-Stake (PoS) Layer-2 chains, called subnets, whose stake is denominated in L1 BTC. Bitcoin-IPC subnets rely on Bitcoin L1 for the communication of critical information, settlement, and security.\n  Our design, inspired by SWIFT messaging and embedded within Bitcoin's SegWit mechanism, enables seamless value transfer across L2 subnets, routed through Bitcoin L1. Uniquely, this mechanism reduces the virtual-byte cost per transaction (vB per tx) by up to 23x, compared to transacting natively on Bitcoin L1, effectively increasing monetary transaction throughput from 7 tps to over 160 tps, without requiring any modifications to Bitcoin L1.", "AI": {"tldr": "Bitcoin-IPC\u662f\u4e00\u4e2a\u8f6f\u4ef6\u6808\u548c\u534f\u8bae\uff0c\u901a\u8fc7\u521b\u5efa\u4ee5L1 BTC\u4e3a\u8d28\u62bc\u7684PoS Layer-2\u5b50\u7f51\uff0c\u6269\u5c55\u6bd4\u7279\u5e01\u4f5c\u4e3a\u901a\u7528\u4ea4\u6362\u5a92\u4ecb\u7684\u80fd\u529b\uff0c\u65e0\u9700\u4fee\u6539\u6bd4\u7279\u5e01L1\u5373\u53ef\u5b9e\u73b023\u500d\u4ea4\u6613\u6210\u672c\u964d\u4f4e\u548c160+ tps\u541e\u5410\u91cf\u3002", "motivation": "\u89e3\u51b3\u6bd4\u7279\u5e01\u4f5c\u4e3a\u901a\u7528\u4ea4\u6362\u5a92\u4ecb\uff08MoE\uff09\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7Layer-2\u65b9\u6848\u63d0\u9ad8\u4ea4\u6613\u541e\u5410\u91cf\u3001\u964d\u4f4e\u4ea4\u6613\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6bd4\u7279\u5e01L1\u7684\u5b89\u5168\u6027\u548c\u517c\u5bb9\u6027\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eSWIFT\u6d88\u606f\u4f20\u9012\u673a\u5236\u7684\u8f6f\u4ef6\u6808\uff0c\u5229\u7528\u6bd4\u7279\u5e01\u7684SegWit\u673a\u5236\u5d4c\u5165\u901a\u4fe1\u534f\u8bae\uff0c\u521b\u5efa\u65e0\u9700\u8bb8\u53ef\u7684PoS Layer-2\u5b50\u7f51\uff0c\u8fd9\u4e9b\u5b50\u7f51\u4ee5L1 BTC\u4e3a\u8d28\u62bc\uff0c\u901a\u8fc7\u6bd4\u7279\u5e01L1\u8fdb\u884c\u5173\u952e\u4fe1\u606f\u901a\u4fe1\u3001\u7ed3\u7b97\u548c\u5b89\u5168\u4fdd\u969c\u3002", "result": "\u76f8\u6bd4\u539f\u751f\u6bd4\u7279\u5e01L1\u4ea4\u6613\uff0c\u865a\u62df\u5b57\u8282\u6210\u672c\u964d\u4f4e23\u500d\uff0c\u4ea4\u6613\u541e\u5410\u91cf\u4ece7 tps\u63d0\u5347\u5230\u8d85\u8fc7160 tps\uff0c\u5b9e\u73b0\u4e86\u8de8\u5b50\u7f51\u7684\u65e0\u7f1d\u4ef7\u503c\u8f6c\u79fb\uff0c\u65e0\u9700\u4fee\u6539\u6bd4\u7279\u5e01L1\u534f\u8bae\u3002", "conclusion": "Bitcoin-IPC\u901a\u8fc7\u521b\u65b0\u7684Layer-2\u67b6\u6784\u6210\u529f\u6269\u5c55\u4e86\u6bd4\u7279\u5e01\u7684\u4ea4\u6613\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u6210\u4e3a\u901a\u7528\u4ea4\u6362\u5a92\u4ecb\u7684\u76ee\u6807\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6bd4\u7279\u5e01L1\u7684\u5b89\u5168\u6027\u548c\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u3002"}}
{"id": "2512.23494", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.23494", "abs": "https://arxiv.org/abs/2512.23494", "authors": ["Eddy Truyen", "Wouter Joosen"], "title": "Optimal Configuration of API Resources in Cloud Native Computing", "comment": "In Proceedings WACA 2025, arXiv:2512.22054", "summary": "This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.", "AI": {"tldr": "\u5c06\u73b0\u6709\u79bb\u7ebf\u6027\u80fd\u4f18\u5316\u6846\u67b6\u5e94\u7528\u4e8e\u5fae\u670d\u52a1\u5e94\u7528\u53d1\u5e03\u9636\u6bb5\uff0c\u4f18\u5316CPU\u548c\u5185\u5b58\u8d44\u6e90\u914d\u7f6e\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u4f18\u5316\u7b97\u6cd5\u5728\u91c7\u6837\u6210\u672c\u548c\u6700\u4f18\u914d\u7f6e\u8ddd\u79bb\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5f53\u524d\u5fae\u670d\u52a1\u6027\u80fd\u4f18\u5316\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8fd0\u7ef4\u9636\u6bb5\u7684\u667a\u80fd\u8c03\u5ea6\u548c\u81ea\u52a8\u6269\u7f29\u5bb9\uff0c\u800c\u53d1\u5e03\u9636\u6bb5\u7684\u8d44\u6e90\u914d\u7f6e\u4f18\u5316\u7814\u7a76\u8f83\u5c11\u3002\u5bb9\u5668\u6c34\u5e73\u81ea\u52a8\u6269\u7f29\u5bb9\uff08\u5982\u57fa\u4e8eCPU\u4f7f\u7528\u7387\uff09\u53ef\u80fd\u5bfc\u81f4\u5185\u5b58\u5206\u914d\u4e0d\u5f53\uff0c\u9700\u8981\u5728\u90e8\u7f72\u524d\u5bf9\u4e24\u79cd\u8d44\u6e90\u8fdb\u884c\u7cbe\u7ec6\u8c03\u4f18\u3002", "method": "\u5c06\u73b0\u6709\u79bb\u7ebf\u6027\u80fd\u4f18\u5316\u6846\u67b6\u5e94\u7528\u4e8e\u5fae\u670d\u52a1\u5e94\u7528\u7684\u53d1\u5e03\u9636\u6bb5\uff0c\u4f7f\u7528TeaStore\u5fae\u670d\u52a1\u5e94\u7528\u8fdb\u884c\u8bc4\u4f30\uff0c\u7edf\u8ba1\u6bd4\u8f83\u4e0d\u540c\u4f18\u5316\u7b97\u6cd5\uff0c\u5206\u6790\u56e0\u5b50\u7b5b\u9009\u5bf9\u641c\u7d22\u7a7a\u95f4\u7f29\u51cf\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u5f53\u76ee\u6807\u662f\u627e\u5230\u6700\u4f18\u8d44\u6e90\u914d\u7f6e\u4e14\u91c7\u6837\u9884\u7b97\u6709\u9650\u65f6\uff0c\u524d\u7f6e\u56e0\u5b50\u7b5b\u9009\u6709\u52a9\u4e8e\u7f29\u51cf\u641c\u7d22\u7a7a\u95f4\uff1b2\uff09\u5f53\u9700\u8981\u7edf\u8ba1\u6bd4\u8f83\u4e0d\u540c\u7b97\u6cd5\u65f6\uff0c\u4e5f\u9700\u8981\u5e94\u7528\u7b5b\u9009\u4ee5\u4f7f\u6240\u6709\u6570\u636e\u70b9\u6536\u96c6\u53ef\u884c\uff1b3\uff09\u5f53\u76ee\u6807\u662f\u627e\u5230\u8fd1\u4f3c\u6700\u4f18\u914d\u7f6e\u65f6\uff0c\u4e0d\u8fdb\u884c\u7b5b\u9009\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u53d1\u5e03\u9636\u6bb5\u7684\u5fae\u670d\u52a1\u8d44\u6e90\u914d\u7f6e\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u9886\u57df\uff0c\u4e0d\u540c\u4f18\u5316\u76ee\u6807\u9700\u8981\u91c7\u7528\u4e0d\u540c\u7684\u7b56\u7565\uff08\u662f\u5426\u4f7f\u7528\u56e0\u5b50\u7b5b\u9009\uff09\uff0c\u4e3aDevOps\u751f\u547d\u5468\u671f\u4e2d\u7684\u8d44\u6e90\u914d\u7f6e\u51b3\u7b56\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
