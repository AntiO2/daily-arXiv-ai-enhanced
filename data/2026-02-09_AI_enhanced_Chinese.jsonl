{"id": "2602.06594", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.06594", "abs": "https://arxiv.org/abs/2602.06594", "authors": ["Yichun Wang", "Kristina Irion", "Paul Groth", "Hazar Harmouch"], "title": "Machine Learning Practitioners' Views on Data Quality in Light of EU Regulatory Requirements: A European Online Survey", "comment": null, "summary": "Understanding how data quality aligns with regulatory requirements in machine learning (ML) systems presents a critical challenge for practitioners navigating the evolving EU regulatory landscape. To address this, we first propose a practical framework aligning established data quality dimensions with specific EU regulatory requirements. Second, we conducted a comprehensive online survey with over 180 EU-based data practitioners, investigating their approaches, key challenges, and unmet needs when ensuring data quality in ML systems that align with regulatory requirements. Our findings highlight crucial gaps between current practices and regulatory expectations, underscoring practitioners' need for more integrated data quality tools and better collaboration between technical and legal practitioners. These insights inform recommendations for bridging technical expertise and regulatory compliance, ultimately fostering responsible and trustworthy ML deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5c06\u6570\u636e\u8d28\u91cf\u7ef4\u5ea6\u4e0e\u6b27\u76df\u76d1\u7ba1\u8981\u6c42\u5bf9\u9f50\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8c03\u67e5\u63ed\u793a\u5f53\u524d\u5b9e\u8df5\u4e0e\u76d1\u7ba1\u671f\u671b\u4e4b\u95f4\u7684\u5dee\u8ddd", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u6b27\u76df\u76d1\u7ba1\u73af\u5883\u4e0b\uff0c\u6570\u636e\u8d28\u91cf\u5982\u4f55\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u662f\u5b9e\u8df5\u8005\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u7406\u8bba\u4e0e\u5b9e\u8df5\u8131\u8282\u7684\u95ee\u9898", "method": "1) \u63d0\u51fa\u5c06\u6570\u636e\u8d28\u91cf\u7ef4\u5ea6\u4e0e\u6b27\u76df\u76d1\u7ba1\u8981\u6c42\u5bf9\u9f50\u7684\u5b9e\u7528\u6846\u67b6\uff1b2) \u5bf9180\u591a\u540d\u6b27\u76df\u6570\u636e\u4ece\u4e1a\u8005\u8fdb\u884c\u5728\u7ebf\u8c03\u67e5\uff0c\u4e86\u89e3\u4ed6\u4eec\u5728\u786e\u4fddML\u7cfb\u7edf\u6570\u636e\u8d28\u91cf\u7b26\u5408\u76d1\u7ba1\u8981\u6c42\u65f6\u7684\u505a\u6cd5\u3001\u6311\u6218\u548c\u9700\u6c42", "result": "\u53d1\u73b0\u5f53\u524d\u5b9e\u8df5\u4e0e\u76d1\u7ba1\u671f\u671b\u4e4b\u95f4\u5b58\u5728\u91cd\u8981\u5dee\u8ddd\uff0c\u4ece\u4e1a\u8005\u9700\u8981\u66f4\u96c6\u6210\u7684\u6570\u636e\u8d28\u91cf\u5de5\u5177\u4ee5\u53ca\u6280\u672f\u4e0e\u6cd5\u5f8b\u4ece\u4e1a\u8005\u4e4b\u95f4\u66f4\u597d\u7684\u534f\u4f5c", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f25\u5408\u6280\u672f\u4e13\u957f\u4e0e\u76d1\u7ba1\u5408\u89c4\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u5efa\u8bae\uff0c\u6700\u7ec8\u4fc3\u8fdb\u8d1f\u8d23\u4efb\u548c\u53ef\u4fe1\u7684ML\u90e8\u7f72"}}
{"id": "2602.06721", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.06721", "abs": "https://arxiv.org/abs/2602.06721", "authors": ["Wenxuan Xia", "Mingyu Yang", "Wentao Li", "Wei Wang"], "title": "Filtered Approximate Nearest Neighbor Search Cost Estimation", "comment": "12 pages", "summary": "Hybrid queries combining high-dimensional vector similarity with structured attribute filtering have garnered significant attention across both academia and industry. A critical instance of this paradigm is filtered Approximate k Nearest Neighbor (AKNN) search, where embeddings (e.g., image or text) are queried alongside constraints such as labels or numerical range. While essential for rich retrieval, optimizing these queries remains challenging due to the highly variable search cost induced by combined filters. In this paper, we propose a novel cost estimation framework, E2E, for filtered AKNN search and demonstrate its utility in downstream optimization tasks, specifically early termination. Unlike existing approaches, our model explicitly captures the correlation between the query vector distribution and attribute-value selectivity, yielding significantly higher estimation accuracy. By leveraging these estimates to refine search termination conditions, we achieve substantial performance gains. Experimental results on real-world datasets demonstrate that our approach improves retrieval efficiency by 2x-3x over state-of-the-art baselines while maintaining high search accuracy.", "AI": {"tldr": "\u63d0\u51faE2E\u6210\u672c\u4f30\u8ba1\u6846\u67b6\u7528\u4e8e\u8fc7\u6ee4\u8fd1\u4f3ck\u8fd1\u90bb\u641c\u7d22\uff0c\u901a\u8fc7\u6355\u6349\u67e5\u8be2\u5411\u91cf\u5206\u5e03\u4e0e\u5c5e\u6027\u9009\u62e9\u6027\u7684\u76f8\u5173\u6027\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u4f30\u8ba1\u5e76\u4f18\u5316\u641c\u7d22\u6027\u80fd", "motivation": "\u6df7\u5408\u67e5\u8be2\u7ed3\u5408\u9ad8\u7ef4\u5411\u91cf\u76f8\u4f3c\u6027\u548c\u7ed3\u6784\u5316\u5c5e\u6027\u8fc7\u6ee4\u5728\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u4f18\u5316\u8fd9\u7c7b\u67e5\u8be2\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u7ec4\u5408\u8fc7\u6ee4\u5668\u5bfc\u81f4\u7684\u641c\u7d22\u6210\u672c\u9ad8\u5ea6\u53ef\u53d8", "method": "\u63d0\u51fa\u65b0\u9896\u7684E2E\u6210\u672c\u4f30\u8ba1\u6846\u67b6\uff0c\u663e\u5f0f\u6355\u6349\u67e5\u8be2\u5411\u91cf\u5206\u5e03\u4e0e\u5c5e\u6027\u503c\u9009\u62e9\u6027\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5229\u7528\u8fd9\u4e9b\u4f30\u8ba1\u6765\u4f18\u5316\u641c\u7d22\u7ec8\u6b62\u6761\u4ef6", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5c06\u68c0\u7d22\u6548\u7387\u63d0\u9ad8\u4e862-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u641c\u7d22\u7cbe\u5ea6", "conclusion": "E2E\u6846\u67b6\u901a\u8fc7\u51c6\u786e\u4f30\u8ba1\u8fc7\u6ee4AKNN\u641c\u7d22\u6210\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df7\u5408\u67e5\u8be2\u7684\u4f18\u5316\u6027\u80fd\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u5982\u65e9\u671f\u7ec8\u6b62\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177"}}
{"id": "2602.06057", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06057", "abs": "https://arxiv.org/abs/2602.06057", "authors": ["Satyam Kumar", "Saurabh Jha"], "title": "Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing", "comment": null, "summary": "Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.", "AI": {"tldr": "QEIL\u6846\u67b6\u901a\u8fc7\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\u548c\u5f02\u6784\u7f16\u6392\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u672c\u5730LLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u8986\u76d6\u7387\u548c\u80fd\u6548\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e25\u91cd\u4f9d\u8d56\u4e91\u6216\u6570\u636e\u4e2d\u5fc3\u57fa\u7840\u8bbe\u65bd\uff0c\u9700\u8981\u4f4e\u5ef6\u8fdf\u667a\u80fd\u7cfb\u7edf\u7684\u672c\u5730\u63a8\u7406\u65b9\u6848\u3002", "method": "\u63d0\u51faQEIL\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\uff0c\u663e\u793a\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u5b9e\u73b0\u8d85\u7ebf\u6027\u6548\u7387\u589e\u76ca\uff1b2\uff09\u786c\u4ef6\u611f\u77e5\u8def\u7531\uff0c\u901a\u8fc7\u5206\u6790\u6210\u672c\u6a21\u578b\u8003\u8651\u8ba1\u7b97\u541e\u5410\u91cf\u3001\u5185\u5b58\u5e26\u5bbd\u3001\u529f\u8017\u548c\u70ed\u9650\u5236\uff1b3\uff09\u6027\u80fd-\u80fd\u8017\u6743\u8861\u91cf\u5316\uff0c\u4f7f\u7528\u667a\u80fd\u6bcf\u74e6\u7279\u3001\u80fd\u91cf\u8986\u76d6\u6548\u7387\u7b49\u65b0\u6307\u6807\u3002\u901a\u8fc7\u6e10\u8fdb\u6837\u672c\u590d\u7528\u7edf\u4e00\u7f16\u6392\u5668\u7ec4\u5408\u8fd9\u4e9b\u7ec4\u4ef6\u3002", "result": "\u5728125M\u52302.6B\u53c2\u6570\u7684\u4e94\u4e2a\u6a21\u578b\u7cfb\u5217\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\uff1a7-10.5\u4e2a\u767e\u5206\u70b9\u7684pass@k\u8986\u76d6\u7387\u63d0\u5347\uff0c35.6-78.2%\u7684\u80fd\u91cf\u51cf\u5c11\uff0c68%\u7684\u5e73\u5747\u529f\u8017\u964d\u4f4e\uff08\u6ee1\u8db3\u8fb9\u7f18\u70ed\u9884\u7b97\uff09\uff0c15.8%\u7684\u5ef6\u8fdf\u6539\u5584\uff0c\u96f6\u51c6\u786e\u7387\u635f\u5931\u3002", "conclusion": "\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\u662f\u901a\u7528\u4e14\u67b6\u6784\u65e0\u5173\u7684\uff0c\u5f02\u6784\u8fb9\u7f18\u7f16\u6392\u662f\u80fd\u91cf\u53d7\u9650\u667a\u80fd\u7cfb\u7edf\u7684\u6700\u4f18\u7b56\u7565\u3002"}}
{"id": "2602.06090", "categories": ["cs.SE", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.06090", "abs": "https://arxiv.org/abs/2602.06090", "authors": ["Xiaoxuan Tang", "Jincheng Wang", "Liwei Luo", "Jingxuan Xu", "Sheng Zhou", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "SVRepair: Structured Visual Reasoning for Automated Program Repair", "comment": "16 pages, 3 figures", "summary": "Large language models (LLMs) have recently shown strong potential for Automated Program Repair (APR), yet most existing approaches remain unimodal and fail to leverage the rich diagnostic signals contained in visual artifacts such as screenshots and control-flow graphs. In practice, many bug reports convey critical information visually (e.g., layout breakage or missing widgets), but directly using such dense visual inputs often causes context loss and noise, making it difficult for MLLMs to ground visual observations into precise fault localization and executable patches. To bridge this semantic gap, we propose \\textbf{SVRepair}, a multimodal APR framework with structured visual representation. SVRepair first fine-tunes a vision-language model, \\textbf{Structured Visual Representation (SVR)}, to uniformly transform heterogeneous visual artifacts into a \\emph{semantic scene graph} that captures GUI elements and their structural relations (e.g., hierarchy), providing normalized, code-relevant context for downstream repair. Building on the graph, SVRepair drives a coding agent to localize faults and synthesize patches, and further introduces an iterative visual-artifact segmentation strategy that progressively narrows the input to bug-centered regions to suppress irrelevant context and reduce hallucinations. Extensive experiments across multiple benchmarks demonstrate state-of-the-art performance: SVRepair achieves \\textbf{36.47\\%} accuracy on SWE-Bench M, \\textbf{38.02\\%} on MMCode, and \\textbf{95.12\\%} on CodeVision, validating the effectiveness of SVRepair for multimodal program repair.", "AI": {"tldr": "SVRepair\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7a0b\u5e8f\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u793a\u5c06\u89c6\u89c9\u5de5\u4ef6\u8f6c\u6362\u4e3a\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u5e2e\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u66f4\u597d\u5730\u7406\u89e3\u548c\u4fee\u590d\u5305\u542b\u89c6\u89c9\u4fe1\u606f\u7684bug\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u5927\u591a\u662f\u5355\u6a21\u6001\u7684\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528bug\u62a5\u544a\u4e2d\u5305\u542b\u7684\u4e30\u5bcc\u89c6\u89c9\u4fe1\u606f\uff08\u5982\u622a\u56fe\u3001\u63a7\u5236\u6d41\u56fe\u7b49\uff09\u3002\u76f4\u63a5\u4f7f\u7528\u5bc6\u96c6\u7684\u89c6\u89c9\u8f93\u5165\u4f1a\u5bfc\u81f4\u4e0a\u4e0b\u6587\u4e22\u5931\u548c\u566a\u58f0\uff0c\u4f7f\u5f97\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5c06\u89c6\u89c9\u89c2\u5bdf\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u6545\u969c\u5b9a\u4f4d\u548c\u53ef\u6267\u884c\u8865\u4e01\u3002", "method": "SVRepair\u9996\u5148\u5fae\u8c03\u4e00\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08SVR\uff09\uff0c\u5c06\u5f02\u6784\u7684\u89c6\u89c9\u5de5\u4ef6\u7edf\u4e00\u8f6c\u6362\u4e3a\u8bed\u4e49\u573a\u666f\u56fe\uff0c\u6355\u6349GUI\u5143\u7d20\u53ca\u5176\u7ed3\u6784\u5173\u7cfb\u3002\u57fa\u4e8e\u8be5\u56fe\uff0c\u9a71\u52a8\u7f16\u7801\u4ee3\u7406\u8fdb\u884c\u6545\u969c\u5b9a\u4f4d\u548c\u8865\u4e01\u5408\u6210\uff0c\u5e76\u5f15\u5165\u8fed\u4ee3\u7684\u89c6\u89c9\u5de5\u4ef6\u5206\u5272\u7b56\u7565\uff0c\u9010\u6b65\u5c06\u8f93\u5165\u7f29\u5c0f\u5230bug\u76f8\u5173\u533a\u57df\uff0c\u6291\u5236\u65e0\u5173\u4e0a\u4e0b\u6587\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u5728SWE-Bench M\u4e0a\u8fbe\u523036.47%\u51c6\u786e\u7387\uff0cMMCode\u4e0a38.02%\uff0cCodeVision\u4e0a95.12%\uff0c\u9a8c\u8bc1\u4e86SVRepair\u5728\u591a\u6a21\u6001\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SVRepair\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u89c9\u8868\u793a\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u7a0b\u5e8f\u4fee\u590d\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u5c06\u89c6\u89c9\u4fe1\u606f\u6709\u6548\u8f6c\u5316\u4e3a\u4ee3\u7801\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a0b\u5e8f\u4fee\u590d\u7684\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.06063", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06063", "abs": "https://arxiv.org/abs/2602.06063", "authors": ["Shouyu Du", "Miaoxiang Yu", "Zhiheng Ni", "Jillian Cai", "Qing Yang", "Tao Wei", "Zhenyu Xu"], "title": "Mapping Gemma3 onto an Edge Dataflow Architecture", "comment": "Original Version, data shall be updated", "summary": "We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\\times$ faster prefill and $4.8\\times$ faster decoding versus the iGPU, and $33.5\\times$ and $2.2\\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\\times$ and $222.9\\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728AMD Ryzen AI NPU\u4e0a\u7aef\u5230\u7aef\u90e8\u7f72Gemma3\u7cfb\u5217\u5927\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u73b0\u4ee3NPU\u53ef\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u7528\u3001\u4f4e\u529f\u8017\u7684LLM\u548cVLM\u63a8\u7406\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u8fb9\u7f18\u8bbe\u5907\u7684tiled\u6570\u636e\u6d41\u67b6\u6784\uff08AMD Ryzen AI NPU\uff09\u4e0a\u9ad8\u6548\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u89e3\u51b3\u4f20\u7edfCPU\u548ciGPU\u5728\u8fb9\u7f18\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u548c\u529f\u8017\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u7cfb\u5217\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6280\u672f\uff1a\u9884\u586b\u5145\u9636\u6bb5\u91c7\u7528\u9ad8\u6548\u53cd\u91cf\u5316\u5f15\u64ce\u3001\u4f18\u5316tiled\u77e9\u9635\u4e58\u6cd5\u5185\u6838\u3001\u63d0\u51faFlowQKV\u5206\u5757\u6d41\u6c34\u7ebf\u6ce8\u610f\u529b\u673a\u5236\uff1b\u89e3\u7801\u9636\u6bb5\u5f15\u5165FusedDQP\uff08\u878d\u5408\u53cd\u91cf\u5316\u548c\u6295\u5f71\uff09\u548cFlowKV\uff08\u91cd\u6784\u6ce8\u610f\u529b\u4ee5\u7ef4\u6301\u9ad8\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\uff09\uff0c\u5e76\u7ed3\u5408\u7d27\u51d1\u7684Q4NX 4\u4f4d\u91cf\u5316\u683c\u5f0f\u3002", "result": "\u76f8\u6bd4iGPU\uff0c\u9884\u586b\u5145\u901f\u5ea6\u63d0\u53475.2\u500d\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u53474.8\u500d\uff1b\u76f8\u6bd4CPU\uff0c\u5206\u522b\u63d0\u534733.5\u500d\u548c2.2\u500d\u3002\u80fd\u6548\u6bd4iGPU\u63d0\u534767.2\u500d\uff0c\u6bd4CPU\u63d0\u5347222.9\u500d\u3002", "conclusion": "\u73b0\u4ee3NPU\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u7528\u3001\u4f4e\u529f\u8017\u7684LLM\u548cVLM\u63a8\u7406\uff0c\u4e3a\u5c06\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u6620\u5c04\u5230tiled\u6570\u636e\u6d41\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u84dd\u56fe\u3002"}}
{"id": "2602.06098", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06098", "abs": "https://arxiv.org/abs/2602.06098", "authors": ["Nicolas Menet", "Michael Hersche", "Andreas Krause", "Abbas Rahimi"], "title": "Coding Agents with Environment Interaction: A Theoretical Perspective", "comment": "preprint", "summary": "Coding agents are increasingly utilized in test-driven software development, yet the theoretical mechanisms behind their environment-interaction strategies remain underexplored. We provide a probabilistic framework for two dominant paradigms: code selection after generation using the execution environment, and code generation conditioned on environment feedback. First, we formalize several well-established selection heuristics as environment-aware estimators of code correctness. We theoretically prove that estimators based on fuzzy functional similarity add an inductive bias and strictly dominate estimators based on functional equivalence in terms of signal-to-noise ratio. Second, we frame backprompting as an in-context approximation of Thompson sampling. We derive a novel regret bound for reward functions with unobservable components, theoretically explaining why the effectiveness of backprompting is limited by the ambiguity of the informal task description (an irreducible regret). Using three state-of-the-art open weight models, we corroborate these findings across BigCodeBenchHard, LeetCodeDataset, and QiskitHumanEvalSim. Our formalization also suggests how to improve task descriptions effectively, leading to a new benchmark, QiskitHumanEvalSimX.", "AI": {"tldr": "\u672c\u6587\u4e3a\u7f16\u7801\u4ee3\u7406\u5728\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\u4e2d\u7684\u73af\u5883\u4ea4\u4e92\u7b56\u7565\u63d0\u4f9b\u4e86\u6982\u7387\u6846\u67b6\uff0c\u5206\u6790\u4e86\u4ee3\u7801\u9009\u62e9\u4e0e\u53cd\u9988\u751f\u6210\u4e24\u79cd\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86\u6a21\u7cca\u529f\u80fd\u76f8\u4f3c\u6027\u4f30\u8ba1\u5668\u4f18\u4e8e\u529f\u80fd\u7b49\u4ef7\u4f30\u8ba1\u5668\uff0c\u5e76\u5c06\u53cd\u5411\u63d0\u793a\u6846\u67b6\u4e3aThompson\u91c7\u6837\u7684\u8fd1\u4f3c\u3002", "motivation": "\u7f16\u7801\u4ee3\u7406\u5728\u6d4b\u8bd5\u9a71\u52a8\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u73af\u5883\u4ea4\u4e92\u7b56\u7565\u7684\u7406\u8bba\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u4e3a\u4e24\u79cd\u4e3b\u5bfc\u8303\u5f0f\uff08\u4ee3\u7801\u9009\u62e9\u4e0e\u53cd\u9988\u751f\u6210\uff09\u63d0\u4f9b\u7406\u8bba\u6846\u67b6\uff0c\u4ee5\u7406\u89e3\u5176\u5185\u5728\u673a\u5236\u5e76\u6307\u5bfc\u6539\u8fdb\u3002", "method": "1. \u5c06\u4ee3\u7801\u9009\u62e9\u542f\u53d1\u5f0f\u5f62\u5f0f\u5316\u4e3a\u73af\u5883\u611f\u77e5\u7684\u6b63\u786e\u6027\u4f30\u8ba1\u5668\uff1b2. \u8bc1\u660e\u57fa\u4e8e\u6a21\u7cca\u529f\u80fd\u76f8\u4f3c\u6027\u7684\u4f30\u8ba1\u5668\u5728\u4fe1\u566a\u6bd4\u4e0a\u4e25\u683c\u4f18\u4e8e\u529f\u80fd\u7b49\u4ef7\u4f30\u8ba1\u5668\uff1b3. \u5c06\u53cd\u5411\u63d0\u793a\u6846\u67b6\u4e3aThompson\u91c7\u6837\u7684\u4e0a\u4e0b\u6587\u8fd1\u4f3c\uff1b4. \u63a8\u5bfc\u5177\u6709\u4e0d\u53ef\u89c2\u6d4b\u7ec4\u4ef6\u7684\u5956\u52b1\u51fd\u6570\u7684\u9057\u61be\u754c\uff1b5. \u4f7f\u7528\u4e09\u4e2a\u5148\u8fdb\u5f00\u6e90\u6a21\u578b\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff1a\u6a21\u7cca\u529f\u80fd\u76f8\u4f3c\u6027\u4f30\u8ba1\u5668\u901a\u8fc7\u5f15\u5165\u5f52\u7eb3\u504f\u7f6e\uff0c\u5728\u4fe1\u566a\u6bd4\u4e0a\u4e25\u683c\u4f18\u4e8e\u529f\u80fd\u7b49\u4ef7\u4f30\u8ba1\u5668\uff1b\u53cd\u5411\u63d0\u793a\u7684\u6548\u679c\u53d7\u975e\u6b63\u5f0f\u4efb\u52a1\u63cf\u8ff0\u6a21\u7cca\u6027\u7684\u9650\u5236\uff08\u4e0d\u53ef\u7ea6\u9057\u61be\uff09\u3002\u5b9e\u9a8c\u5728BigCodeBenchHard\u3001LeetCodeDataset\u548cQiskitHumanEvalSim\u4e0a\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\uff0c\u5e76\u57fa\u4e8e\u7406\u8bba\u6d1e\u5bdf\u521b\u5efa\u4e86\u65b0\u57fa\u51c6QiskitHumanEvalSimX\u3002", "conclusion": "\u672c\u6587\u4e3a\u7f16\u7801\u4ee3\u7406\u7684\u73af\u5883\u4ea4\u4e92\u7b56\u7565\u63d0\u4f9b\u4e86\u9996\u4e2a\u6982\u7387\u7406\u8bba\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u7cca\u529f\u80fd\u76f8\u4f3c\u6027\u4f30\u8ba1\u5668\u7684\u4f18\u8d8a\u6027\uff0c\u89e3\u91ca\u4e86\u53cd\u5411\u63d0\u793a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u4efb\u52a1\u63cf\u8ff0\u7684\u65b9\u6cd5\u3002\u8fd9\u4e9b\u7406\u8bba\u6d1e\u5bdf\u6709\u52a9\u4e8e\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u7f16\u7801\u4ee3\u7406\u7cfb\u7edf\u3002"}}
{"id": "2602.06064", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06064", "abs": "https://arxiv.org/abs/2602.06064", "authors": ["Yi-Xiang Hu", "Yuke Wang", "Feng Wu", "Zirui Huang", "Shuli Zeng", "Xiang-Yang Li"], "title": "iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems", "comment": "13 pages, 7 figures,", "summary": "Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\\times$ against strong commercial baselines.", "AI": {"tldr": "iScheduler\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fed\u4ee3\u8c03\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8d44\u6e90\u6295\u8d44\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u5b50\u95ee\u9898\u548c\u5e8f\u5217\u5316\u8fdb\u7a0b\u9009\u62e9\uff0c\u5728\u5de5\u4e1a\u89c4\u6a21\u4efb\u52a1\u4e0a\u5b9e\u73b043\u500d\u52a0\u901f", "motivation": "\u4f20\u7edf\u6df7\u5408\u6574\u6570\u89c4\u5212\u548c\u7ea6\u675f\u89c4\u5212\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u8d44\u6e90\u6295\u8d44\u95ee\u9898\u5b9e\u4f8b\u4e0a\u8ba1\u7b97\u7f13\u6162\uff0c\u4e14\u52a8\u6001\u66f4\u65b0\u9700\u8981\u4f4e\u5ef6\u8fdf\u7684\u8c03\u5ea6\u4fee\u8ba2\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5c06\u8d44\u6e90\u6295\u8d44\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u8fed\u4ee3\u8c03\u5ea6\u6846\u67b6\u8fdb\u884c\u5e8f\u5217\u5316\u8fdb\u7a0b\u9009\u62e9\uff0c\u652f\u6301\u91cd\u7528\u672a\u66f4\u6539\u7684\u8fdb\u7a0b\u8c03\u5ea6", "result": "\u5728\u5305\u542b1000\u4e2a\u5b9e\u4f8b\u30012500-10000\u4e2a\u4efb\u52a1\u7684\u5de5\u4e1a\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ciScheduler\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u8d44\u6e90\u6210\u672c\u7684\u540c\u65f6\uff0c\u5c06\u53ef\u884c\u6027\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe43\u500d", "conclusion": "iScheduler\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u8fed\u4ee3\u8c03\u5ea6\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8d44\u6e90\u6295\u8d44\u95ee\u9898\uff0c\u663e\u8457\u52a0\u901f\u4f18\u5316\u8fc7\u7a0b\u5e76\u652f\u6301\u52a8\u6001\u91cd\u65b0\u914d\u7f6e"}}
{"id": "2602.06223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06223", "abs": "https://arxiv.org/abs/2602.06223", "authors": ["Juan Marcano", "Ashish Samant", "Kai Song", "Lingchao Chen", "Kaelan Mikowicz", "Tim Smyth", "Mengdie Zhang", "Ali Zamani", "Arturo Bravo Rovirosa", "Sowjanya Puligadda", "Srikanth Prodduturi", "Mayank Bansal"], "title": "Scaling Mobile Chaos Testing with AI-Driven Test Execution", "comment": "10 pages of content, 1 page of citations, 7 figures, 6 tables", "summary": "Mobile applications in large-scale distributed systems are susceptible to backend service failures, yet traditional chaos engineering approaches cannot scale mobile testing due to the combinatorial explosion of flows, locations, and failure scenarios that need validation. We present an automated mobile chaos testing system that integrates DragonCrawl, an LLM-based mobile testing platform, with uHavoc, a service-level fault injection system. The key insight is that adaptive AI-driven test execution can navigate mobile applications under degraded backend conditions, eliminating the need to manually write test cases for each combination of user flow, city, and failure type. Since Q1 2024, our system has executed over 180,000 automated chaos tests across 47 critical flows in Uber's Rider, Driver, and Eats applications, representing approximately 39,000 hours of manual testing effort that would be impractical at this scale. We identified 23 resilience risks, with 70% being architectural dependency violations where non-critical service failures degraded core user flows. Twelve issues were severe enough to prevent trip requests or food orders. Two caused application crashes detectable only through mobile chaos testing, not backend testing alone. Automated root cause analysis reduced debugging time from hours to minutes, achieving 88% precision@5 in attributing mobile failures to specific backend services. This paper presents the system design, evaluates its performance under fault injection (maintaining 99% test reliability), and reports operational experience demonstrating that continuous mobile resilience validation is achievable at production scale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u79fb\u52a8\u6df7\u6c8c\u6d4b\u8bd5\u7cfb\u7edf\uff0c\u5c06LLM\u9a71\u52a8\u7684\u79fb\u52a8\u6d4b\u8bd5\u5e73\u53f0DragonCrawl\u4e0e\u670d\u52a1\u7ea7\u6545\u969c\u6ce8\u5165\u7cfb\u7edfuHavoc\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df7\u6c8c\u5de5\u7a0b\u5728\u79fb\u52a8\u6d4b\u8bd5\u4e2d\u56e0\u7528\u6237\u6d41\u3001\u5730\u7406\u4f4d\u7f6e\u548c\u6545\u969c\u573a\u666f\u7ec4\u5408\u7206\u70b8\u800c\u65e0\u6cd5\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u7684\u79fb\u52a8\u5e94\u7528\u5bb9\u6613\u53d7\u5230\u540e\u7aef\u670d\u52a1\u6545\u969c\u7684\u5f71\u54cd\uff0c\u4f46\u4f20\u7edf\u7684\u6df7\u6c8c\u5de5\u7a0b\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u79fb\u52a8\u6d4b\u8bd5\uff0c\u56e0\u4e3a\u9700\u8981\u9a8c\u8bc1\u7684\u7528\u6237\u6d41\u3001\u5730\u7406\u4f4d\u7f6e\u548c\u6545\u969c\u573a\u666f\u7684\u7ec4\u5408\u5448\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u624b\u52a8\u7f16\u5199\u6d4b\u8bd5\u7528\u4f8b\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86DragonCrawl\uff08\u57fa\u4e8eLLM\u7684\u79fb\u52a8\u6d4b\u8bd5\u5e73\u53f0\uff09\u548cuHavoc\uff08\u670d\u52a1\u7ea7\u6545\u969c\u6ce8\u5165\u7cfb\u7edf\uff09\u3002\u6838\u5fc3\u6d1e\u5bdf\u662f\u81ea\u9002\u5e94AI\u9a71\u52a8\u7684\u6d4b\u8bd5\u6267\u884c\u53ef\u4ee5\u5728\u540e\u7aef\u964d\u7ea7\u6761\u4ef6\u4e0b\u5bfc\u822a\u79fb\u52a8\u5e94\u7528\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u7528\u6237\u6d41\u3001\u57ce\u5e02\u548c\u6545\u969c\u7c7b\u578b\u7684\u7ec4\u5408\u624b\u52a8\u7f16\u5199\u6d4b\u8bd5\u7528\u4f8b\u3002", "result": "\u81ea2024\u5e74\u7b2c\u4e00\u5b63\u5ea6\u4ee5\u6765\uff0c\u7cfb\u7edf\u5728Uber\u7684Rider\u3001Driver\u548cEats\u5e94\u7528\u768447\u4e2a\u5173\u952e\u6d41\u7a0b\u4e2d\u6267\u884c\u4e86\u8d85\u8fc718\u4e07\u6b21\u81ea\u52a8\u5316\u6df7\u6c8c\u6d4b\u8bd5\uff0c\u76f8\u5f53\u4e8e\u7ea63.9\u4e07\u5c0f\u65f6\u7684\u624b\u52a8\u6d4b\u8bd5\u5de5\u4f5c\u91cf\u3002\u8bc6\u522b\u4e8623\u4e2a\u5f39\u6027\u98ce\u9669\uff0c\u5176\u4e2d70%\u662f\u67b6\u6784\u4f9d\u8d56\u8fdd\u89c4\uff08\u975e\u5173\u952e\u670d\u52a1\u6545\u969c\u5f71\u54cd\u6838\u5fc3\u7528\u6237\u6d41\uff09\u300212\u4e2a\u95ee\u9898\u4e25\u91cd\u5230\u963b\u6b62\u884c\u7a0b\u8bf7\u6c42\u6216\u98df\u54c1\u8ba2\u5355\uff0c2\u4e2a\u5bfc\u81f4\u4ec5\u901a\u8fc7\u79fb\u52a8\u6df7\u6c8c\u6d4b\u8bd5\u624d\u80fd\u68c0\u6d4b\u5230\u7684\u5e94\u7528\u5d29\u6e83\u3002\u81ea\u52a8\u5316\u6839\u56e0\u5206\u6790\u5c06\u8c03\u8bd5\u65f6\u95f4\u4ece\u6570\u5c0f\u65f6\u7f29\u77ed\u5230\u6570\u5206\u949f\uff0c\u5728\u5c06\u79fb\u52a8\u6545\u969c\u5f52\u56e0\u4e8e\u7279\u5b9a\u540e\u7aef\u670d\u52a1\u65b9\u9762\u8fbe\u523088%\u7684precision@5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u8bbe\u8ba1\u5728\u6545\u969c\u6ce8\u5165\u4e0b\u4fdd\u630199%\u7684\u6d4b\u8bd5\u53ef\u9760\u6027\uff0c\u64cd\u4f5c\u7ecf\u9a8c\u8868\u660e\uff0c\u5728\u751f\u4ea7\u89c4\u6a21\u4e0b\u5b9e\u73b0\u6301\u7eed\u7684\u79fb\u52a8\u5f39\u6027\u9a8c\u8bc1\u662f\u53ef\u884c\u7684\u3002\u81ea\u52a8\u5316\u79fb\u52a8\u6df7\u6c8c\u6d4b\u8bd5\u7cfb\u7edf\u80fd\u591f\u5927\u89c4\u6a21\u8bc6\u522b\u4f20\u7edf\u540e\u7aef\u6d4b\u8bd5\u65e0\u6cd5\u53d1\u73b0\u7684\u79fb\u52a8\u7aef\u7279\u5b9a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u79fb\u52a8\u5e94\u7528\u7684\u5f39\u6027\u3002"}}
{"id": "2602.06069", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06069", "abs": "https://arxiv.org/abs/2602.06069", "authors": ["Dinesh Gopalan", "Ratul Ali"], "title": "HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference", "comment": "7 pages, 3 figures, 2 tables", "summary": "The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.", "AI": {"tldr": "HQP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u526a\u679d\u548c\u91cf\u5316\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b03.12\u500d\u63a8\u7406\u52a0\u901f\u548c55%\u6a21\u578b\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u8bc1\u7cbe\u5ea6\u4e0b\u964d\u5c0f\u4e8e1.5%", "motivation": "\u8fb9\u7f18\u4e91\u73af\u5883\u4e2d\u5bf9\u9ad8\u4fdd\u771f\u5b9e\u65f6\u63a8\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u9650\u5236\uff0c\u9700\u8981\u6fc0\u8fdb\u7684\u6a21\u578b\u4f18\u5316\u65b9\u6cd5", "method": "\u63d0\u51fa\u6df7\u5408\u91cf\u5316\u4e0e\u526a\u679d\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8eFisher\u4fe1\u606f\u77e9\u9635\u8fd1\u4f3c\u7684\u654f\u611f\u5ea6\u611f\u77e5\u7ed3\u6784\u526a\u679d\u7b97\u6cd5\uff0c\u5728\u6ee1\u8db3\u7cbe\u5ea6\u7ea6\u675f\u540e\u8fdb\u884c8\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316", "result": "\u5728NVIDIA Jetson\u8fb9\u7f18\u5e73\u53f0\u4e0a\uff0cMobileNetV3\u548cResNet-18\u6a21\u578b\u8fbe\u52303.12\u500d\u63a8\u7406\u52a0\u901f\u548c55%\u6a21\u578b\u538b\u7f29\uff0c\u7cbe\u5ea6\u4e0b\u964d\u4e25\u683c\u63a7\u5236\u57281.5%\u4ee5\u5185", "conclusion": "HQP\u6846\u67b6\u662f\u786c\u4ef6\u65e0\u5173\u7684\u4f18\u8d8a\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u57fa\u7840\u8bbe\u65bd\u7684\u8d85\u4f4e\u5ef6\u8fdfAI\u90e8\u7f72"}}
{"id": "2602.06310", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06310", "abs": "https://arxiv.org/abs/2602.06310", "authors": ["Aldeida Aleti", "Baishakhi Ray", "Rashina Hoda", "Simin Chen"], "title": "Trustworthy AI Software Engineers", "comment": "The first three authors contributed equally to this work", "summary": "With the rapid rise of AI coding agents, the fundamental premise of what it means to be a software engineer is in question. In this vision paper, we re-examine what it means for an AI agent to be considered a software engineer and then critically think about what makes such an agent trustworthy. \\textit{Grounded} in established definitions of software engineering (SE) and informed by recent research on agentic AI systems, we conceptualise AI software engineers as participants in human-AI SE teams composed of human software engineers and AI models and tools, and we distinguish trustworthiness as a key property of these systems and actors rather than a subjective human attitude. Based on historical perspectives and emerging visions, we identify key dimensions that contribute to the trustworthiness of AI software engineers, spanning technical quality, transparency and accountability, epistemic humility, and societal and ethical alignment. We further discuss how trustworthiness can be evaluated and demonstrated, highlighting a fundamental trust measurement gap: not everything that matters for trust can be easily measured. Finally, we outline implications for the design, evaluation, and governance of AI SE systems, advocating for an ethics-by-design approach to enable appropriate trust in future human-AI SE teams.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8AI\u7f16\u7801\u4ee3\u7406\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u53ef\u4fe1\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u5728\u4eba\u7c7b-AI\u56e2\u961f\u4e2d\u8bc4\u4f30AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u4fe1\u4efb\u5ea6\u7684\u5173\u952e\u7ef4\u5ea6\u548c\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740AI\u7f16\u7801\u4ee3\u7406\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u672c\u8d28\u5b9a\u4e49\u53d7\u5230\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6AI\u4ee3\u7406\u4f5c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u610f\u4e49\uff0c\u5e76\u6df1\u5165\u601d\u8003\u5982\u4f55\u5efa\u7acb\u5bf9AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u4fe1\u4efb\u3002", "method": "\u57fa\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4f20\u7edf\u5b9a\u4e49\u548c\u6700\u65b0AI\u4ee3\u7406\u7cfb\u7edf\u7814\u7a76\uff0c\u5c06AI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u6982\u5ff5\u5316\u4e3a\u4eba\u7c7b-AI\u8f6f\u4ef6\u5de5\u7a0b\u56e2\u961f\u7684\u53c2\u4e0e\u8005\u3002\u901a\u8fc7\u5386\u53f2\u89c6\u89d2\u548c\u65b0\u5174\u613f\u666f\uff0c\u8bc6\u522b\u5f71\u54cdAI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u53ef\u4fe1\u5ea6\u7684\u5173\u952e\u7ef4\u5ea6\u3002", "result": "\u8bc6\u522b\u51faAI\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u53ef\u4fe1\u5ea6\u7684\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u6280\u672f\u8d28\u91cf\u3001\u900f\u660e\u5ea6\u548c\u95ee\u8d23\u5236\u3001\u8ba4\u77e5\u8c26\u900a\u3001\u793e\u4f1a\u4e0e\u4f26\u7406\u5bf9\u9f50\u3002\u540c\u65f6\u6307\u51fa\u4fe1\u4efb\u8bc4\u4f30\u5b58\u5728\u6839\u672c\u6027\u6d4b\u91cf\u7f3a\u53e3\u2014\u2014\u5e76\u975e\u6240\u6709\u5f71\u54cd\u4fe1\u4efb\u7684\u56e0\u7d20\u90fd\u80fd\u8f7b\u6613\u91cf\u5316\u3002", "conclusion": "\u5021\u5bfc\u91c7\u7528\"\u8bbe\u8ba1\u5373\u4f26\u7406\"\u7684\u65b9\u6cd5\u6765\u8bbe\u8ba1\u548c\u6cbb\u7406AI\u8f6f\u4ef6\u5de5\u7a0b\u7cfb\u7edf\uff0c\u4ee5\u5728\u672a\u6765\u4eba\u7c7b-AI\u56e2\u961f\u4e2d\u5efa\u7acb\u9002\u5f53\u7684\u4fe1\u4efb\u5173\u7cfb\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u89e3\u51b3\u4fe1\u4efb\u6d4b\u91cf\u4e0e\u8bc4\u4f30\u7684\u6311\u6218\u3002"}}
{"id": "2602.06070", "categories": ["cs.DC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.06070", "abs": "https://arxiv.org/abs/2602.06070", "authors": ["Nikola Stankovic"], "title": "Computationally Efficient Laplacian CL-colME", "comment": "4 pages, 1 figure", "summary": "Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCL-colME\uff0c\u4e00\u79cd\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u7684\u5206\u6563\u5f0f\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\u65b0\u53d8\u4f53\uff0c\u76f8\u6bd4C-colME\u907f\u514d\u4e86\u8ba1\u7b97\u6602\u8d35\u7684\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u6536\u655b\u6027\u548c\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5206\u6563\u5f0f\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\uff08colME\uff09\u662f\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u57fa\u672c\u4efb\u52a1\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684\u53d8\u4f53B-colME\u548cC-colME\u867d\u7136\u5b9e\u73b0\u4e86\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u4f46C-colME\u4f9d\u8d56\u4e8e\u53cc\u968f\u673a\u5e73\u5747\u77e9\u9635\uff0c\u9700\u8981\u8fdb\u884c\u8ba1\u7b97\u6602\u8d35\u7684\u5f52\u4e00\u5316\u8fc7\u7a0b\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6536\u655b\u6027\u80fd\u53c8\u80fd\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCL-colME\uff08\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u7684\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\uff09\uff0c\u91c7\u7528\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u673a\u5236\u66ff\u4ee3\u53cc\u968f\u673a\u5e73\u5747\u77e9\u9635\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u5f52\u4e00\u5316\u8ba1\u7b97\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5b9e\u73b0\u8282\u70b9\u95f4\u7684\u4fe1\u606f\u4ea4\u6362\u548c\u5171\u8bc6\u8fbe\u6210\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCL-colME\u5728\u4fdd\u6301\u4e0eC-colME\u76f8\u540c\u7684\u6536\u655b\u884c\u4e3a\u548c\u4f30\u8ba1\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u65b0\u65b9\u6cd5\u907f\u514d\u4e86\u5f52\u4e00\u5316\u8fc7\u7a0b\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "CL-colME\u662f\u4e00\u79cd\u6709\u6548\u7684\u5206\u6563\u5f0f\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u7528\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7b97\u6cd5\u6536\u655b\u6027\u548c\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u5927\u89c4\u6a21\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06593", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06593", "abs": "https://arxiv.org/abs/2602.06593", "authors": ["Robert Hutter", "Michael Pradel"], "title": "AgentStepper: Interactive Debugging of Software Development Agents", "comment": null, "summary": "Software development agents powered by large language models (LLMs) have shown great promise in automating tasks like environment setup, issue solving, and program repair. Unfortunately, understanding and debugging such agents remain challenging due to their complex and dynamic nature. Developers must reason about trajectories of LLM queries, tool calls, and code modifications, but current techniques reveal little of this intermediate process in a comprehensible format. The key insight of this paper is that debugging software development agents shares many similarities with conventional debugging of software programs, yet requires a higher level of abstraction that raises the level from low-level implementation details to high-level agent actions. Drawing on this insight, we introduce AgentStepper, the first interactive debugger for LLM-based software engineering agents. AgentStepper enables developers to inspect, control, and interactively manipulate agent trajectories. AgentStepper represents trajectories as structured conversations among an LLM, the agent program, and tools. It supports breakpoints, stepwise execution, and live editing of prompts and tool invocations, while capturing and displaying intermediate repository-level code changes. Our evaluation applies AgentStepper to three state-of-the-art software development agents, ExecutionAgent, SWE-Agent, and RepairAgent, showing that integrating the approach into existing agents requires minor code changes (39-42 edited lines). Moreover, we report on a user study with twelve participants, indicating that AgentStepper improves the ability of participants to interpret trajectories (64% vs. 67% mean performance) and identify bugs in the agent's implementation (17% vs. 60% success rate), while reducing perceived workload (e.g., frustration reduced from 5.4/7.0 to 2.4/7.0) compared to conventional tools.", "AI": {"tldr": "AgentStepper\uff1a\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4ea4\u4e92\u5f0f\u8c03\u8bd5\u5668\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u8868\u793a\u3001\u65ad\u70b9\u3001\u5355\u6b65\u6267\u884c\u7b49\u529f\u80fd\u63d0\u5347\u4ee3\u7406\u8f68\u8ff9\u7684\u53ef\u7406\u89e3\u6027\u548c\u8c03\u8bd5\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8f6f\u4ef6\u5f00\u53d1\u4ee3\u7406\u96be\u4ee5\u8c03\u8bd5\uff0c\u56e0\u4e3a\u5176\u590d\u6742\u7684\u52a8\u6001\u7279\u6027\u4f7f\u5f97\u5f00\u53d1\u8005\u96be\u4ee5\u7406\u89e3LLM\u67e5\u8be2\u3001\u5de5\u5177\u8c03\u7528\u548c\u4ee3\u7801\u4fee\u6539\u7684\u4e2d\u95f4\u8fc7\u7a0b\uff0c\u9700\u8981\u66f4\u9ad8\u5c42\u6b21\u7684\u62bd\u8c61\u6765\u63d0\u5347\u8c03\u8bd5\u6548\u7387\u3002", "method": "\u63d0\u51faAgentStepper\u8c03\u8bd5\u5668\uff0c\u5c06\u4ee3\u7406\u8f68\u8ff9\u8868\u793a\u4e3aLLM\u3001\u4ee3\u7406\u7a0b\u5e8f\u548c\u5de5\u5177\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u5bf9\u8bdd\uff0c\u652f\u6301\u65ad\u70b9\u3001\u5355\u6b65\u6267\u884c\u3001\u5b9e\u65f6\u7f16\u8f91\u63d0\u793a\u548c\u5de5\u5177\u8c03\u7528\uff0c\u540c\u65f6\u6355\u83b7\u5e76\u663e\u793a\u4ed3\u5e93\u7ea7\u522b\u7684\u4ee3\u7801\u53d8\u66f4\u3002", "result": "\u96c6\u6210\u5230\u4e09\u4e2a\u5148\u8fdb\u4ee3\u7406\uff08ExecutionAgent\u3001SWE-Agent\u3001RepairAgent\uff09\u4ec5\u9700\u5c11\u91cf\u4ee3\u7801\u4fee\u6539\uff0839-42\u884c\uff09\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u5de5\u5177\uff0cAgentStepper\u63d0\u5347\u8f68\u8ff9\u89e3\u91ca\u80fd\u529b\uff0864% vs 67%\uff09\uff0c\u63d0\u9ad8bug\u8bc6\u522b\u6210\u529f\u7387\uff0817% vs 60%\uff09\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u5de5\u4f5c\u8d1f\u8377\uff08\u632b\u8d25\u611f\u4ece5.4/7.0\u964d\u81f32.4/7.0\uff09\u3002", "conclusion": "AgentStepper\u901a\u8fc7\u63d0\u4f9b\u7c7b\u4f3c\u4f20\u7edf\u8f6f\u4ef6\u8c03\u8bd5\u7684\u4ea4\u4e92\u5f0f\u4f53\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u57fa\u8f6f\u4ef6\u5f00\u53d1\u4ee3\u7406\u7684\u8c03\u8bd5\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u8005\u7684\u7406\u89e3\u548c\u8c03\u8bd5\u6548\u7387\u3002"}}
{"id": "2602.06071", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06071", "abs": "https://arxiv.org/abs/2602.06071", "authors": ["Rajat Vadiraj Dwaraknath", "Sungyoon Kim", "Mert Pilanci"], "title": "FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs", "comment": null, "summary": "Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.", "AI": {"tldr": "\u63d0\u51faBlockPerm-SJLT\u7a00\u758f\u8349\u56fe\u4e0eFlashSketch CUDA\u5185\u6838\u534f\u540c\u8bbe\u8ba1\uff0c\u89e3\u51b3GPU\u4e0a\u7a00\u758f\u8349\u56fe\u5185\u5b58\u8bbf\u95ee\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u5b9e\u73b01.7\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u8349\u56fe\uff08\u5982\u7a00\u758fJohnson-Lindenstrauss\u53d8\u6362\uff09\u867d\u7136\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5176\u968f\u673a\u7a00\u758f\u6027\u5bfc\u81f4GPU\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4e0d\u89c4\u5219\uff0c\u4e25\u91cd\u5f71\u54cd\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u9650\u5236\u4e86\u5728GPU\u4e0a\u7684\u9ad8\u6548\u5b9e\u73b0\u3002", "method": "\u91c7\u7528\u8349\u56fe-\u5185\u6838\u534f\u540c\u8bbe\u8ba1\uff1a1) \u8bbe\u8ba1\u65b0\u7684\u7a00\u758f\u8349\u56fe\u5bb6\u65cfBlockPerm-SJLT\uff0c\u5176\u7a00\u758f\u7ed3\u6784\u4e13\u95e8\u4f18\u5316GPU\u5185\u5b58\u8bbf\u95ee\uff1b2) \u5f00\u53d1\u5bf9\u5e94\u7684CUDA\u5185\u6838FlashSketch\u9ad8\u6548\u5b9e\u73b0\u8fd9\u4e9b\u8349\u56fe\uff1b3) \u5f15\u5165\u53ef\u8c03\u53c2\u6570\u5e73\u8861GPU\u6548\u7387\u4e0e\u8349\u56fe\u9c81\u68d2\u6027\u3002", "result": "1) \u7406\u8bba\u8bc1\u660eBlockPerm-SJLT\u6ee1\u8db3\u65e0\u610f\u8bc6\u5b50\u7a7a\u95f4\u5d4c\u5165\uff08OSE\uff09\u4fdd\u8bc1\uff1b2) \u5728RandNLA\u57fa\u51c6\u6d4b\u8bd5\u548cGraSS\u6570\u636e\u5f52\u56e0\u6d41\u6c34\u7ebf\u4e2d\uff0cFlashSketch\u5728\u8d28\u91cf\u4e0e\u901f\u5ea6\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b3) \u76f8\u6bd4\u4e4b\u524d\u6700\u4f73GPU\u8349\u56fe\u5b9e\u73b0\uff0c\u83b7\u5f97\u7ea61.7\u500d\u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u8349\u56fe-\u5185\u6838\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u8349\u56fe\u5728GPU\u4e0a\u7684\u6548\u7387\u74f6\u9888\uff0cBlockPerm-SJLT\u548cFlashSketch\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86GPU\u5b9e\u73b0\u6548\u7387\uff0c\u63a8\u52a8\u4e86\u968f\u673a\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u5728GPU\u4e0a\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2602.06671", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06671", "abs": "https://arxiv.org/abs/2602.06671", "authors": ["Shijia Dong", "Haoruo Zhao", "Paul Harvey"], "title": "Code vs Serialized AST Inputs for LLM-Based Code Summarization: An Empirical Study", "comment": "Accepted at the 3rd International Workshop on Large Language Models for Code (LLM4Code 2026), co-located with ICSE 2026", "summary": "Summarizing source code into natural language descriptions (code summarization) helps developers better understand program functionality and reduce the burden of software maintenance. Abstract Syntax Trees (ASTs), as opposed to source code, have been shown to improve summarization quality in traditional encoder-decoder-based code summarization models. However, most large language model (LLM)-based code summarization methods rely on raw code or only incorporate partial AST signals, meaning that the potential of complete AST representation has not been fully explored for LLMs. This paper presents AST(NIT), an AST augmentation and serialization method that preserves lexical details and encodes structural information into LLM-compatible sequences. Experiments with the LLaMA-3.1-8B model on the CodeXGLUE Python dataset show that the proposed serialized ASTs reduce the length of LLM inputs, require shorter training times, and achieve summarization quality comparable to existing approaches.", "AI": {"tldr": "\u63d0\u51faAST(NIT)\u65b9\u6cd5\uff0c\u5c06AST\u5e8f\u5217\u5316\u4ee5\u589e\u5f3aLLM\u7684\u4ee3\u7801\u6458\u8981\u80fd\u529b\uff0c\u76f8\u6bd4\u539f\u59cb\u4ee3\u7801\u51cf\u5c11\u8f93\u5165\u957f\u5ea6\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u6458\u8981\u8d28\u91cf", "motivation": "\u4f20\u7edf\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u5df2\u8bc1\u660eAST\u80fd\u63d0\u5347\u4ee3\u7801\u6458\u8981\u8d28\u91cf\uff0c\u4f46\u73b0\u6709LLM\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u539f\u59cb\u4ee3\u7801\u6216\u4ec5\u4f7f\u7528\u90e8\u5206AST\u4fe1\u53f7\uff0c\u5b8c\u6574\u7684AST\u8868\u793a\u6f5c\u529b\u5c1a\u672a\u5728LLM\u4e2d\u5f97\u5230\u5145\u5206\u63a2\u7d22", "method": "\u63d0\u51faAST(NIT)\u65b9\u6cd5\uff0c\u901a\u8fc7AST\u589e\u5f3a\u548c\u5e8f\u5217\u5316\u6280\u672f\uff0c\u4fdd\u7559\u8bcd\u6c47\u7ec6\u8282\u5e76\u5c06\u7ed3\u6784\u4fe1\u606f\u7f16\u7801\u4e3aLLM\u517c\u5bb9\u7684\u5e8f\u5217\uff0c\u4f7f\u7528LLaMA-3.1-8B\u6a21\u578b\u5728CodeXGLUE Python\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c", "result": "\u5e8f\u5217\u5316\u540e\u7684AST\u80fd\u51cf\u5c11LLM\u8f93\u5165\u957f\u5ea6\uff0c\u7f29\u77ed\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6458\u8981\u8d28\u91cf", "conclusion": "AST(NIT)\u65b9\u6cd5\u6210\u529f\u5c06\u5b8c\u6574\u7684AST\u8868\u793a\u6574\u5408\u5230LLM\u4e2d\uff0c\u4e3a\u4ee3\u7801\u6458\u8981\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.06072", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06072", "abs": "https://arxiv.org/abs/2602.06072", "authors": ["Rui Ning", "Wei Zhang", "Fan Lai"], "title": "PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference", "comment": null, "summary": "Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.", "AI": {"tldr": "PackInfer\u662f\u4e00\u4e2a\u5185\u6838\u7ea7\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f02\u8d28\u6279\u5904\u7406\u8bf7\u6c42\u6253\u5305\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6267\u884c\u7ec4\uff0c\u4f18\u5316LLM\u63a8\u7406\u4e2d\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u6548\u7387\u548cI/O\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347GPU\u5229\u7528\u7387\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u751f\u4ea7\u73af\u5883\u4e2dLLM\u670d\u52a1\u9700\u8981\u6279\u5904\u7406\u5177\u6709\u9ad8\u5ea6\u5f02\u8d28\u5e8f\u5217\u957f\u5ea6\u7684\u8bf7\u6c42\u4ee5\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u73b0\u6709\u6ce8\u610f\u529b\u4f18\u5316\u6280\u672f\uff08\u5982FlashAttention\uff09\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u8bf7\u6c42\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548cI/O\u4e0d\u5e73\u8861\u3001\u62d6\u5c3e\u6548\u5e94\u4e25\u91cd\u4ee5\u53caGPU\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "1. \u5c06\u6279\u5904\u7406\u8bf7\u6c42\u7f16\u6392\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6267\u884c\u7ec4\uff0c\u901a\u8fc7\u5c06\u591a\u4e2a\u8bf7\u6c42\u6253\u5305\u5230\u7edf\u4e00\u7684\u5185\u6838\u542f\u52a8\u4e2d\u6765\u9971\u548cGPU\u5229\u7528\u7387\uff1b2. \u76f4\u63a5\u5728\u6253\u5305\u7684\u67e5\u8be2-\u952e\u533a\u57df\u4e0a\u6784\u5efa\u6ce8\u610f\u529b\u5185\u6838\uff0c\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\u5e76\u5e73\u8861\u7ebf\u7a0b\u5757\u6267\u884c\uff1b3. \u91c7\u7528I/O\u611f\u77e5\u5206\u7ec4\uff0c\u5c06\u5171\u4eab\u524d\u7f00\u7684\u8bf7\u6c42\u5171\u7f6e\uff0c\u5e76\u5c06KV\u7f13\u5b58\u91cd\u7ec4\u4e3a\u7ec4\u8fde\u7eed\u5e03\u5c40\uff0c\u51cf\u5c11\u5185\u5b58\u788e\u7247\u548c\u5197\u4f59\u6570\u636e\u79fb\u52a8\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0cPackInfer\u76f8\u6bd4\u6700\u5148\u8fdb\u7684FlashAttention\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e13.0-20.1%\uff0c\u541e\u5410\u91cf\u63d0\u534720%\u3002", "conclusion": "PackInfer\u901a\u8fc7\u8ba1\u7b97\u548cI/O\u611f\u77e5\u7684\u5f02\u8d28\u6279\u5904\u7406\u63a8\u7406\u6267\u884c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u4ea7LLM\u670d\u52a1\u4e2d\u7684\u6ce8\u610f\u529b\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86GPU\u8d44\u6e90\u5229\u7528\u7387\u548c\u6574\u4f53\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2602.06709", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06709", "abs": "https://arxiv.org/abs/2602.06709", "authors": ["Duong Bui", "Stefan Grintz", "Alexander Berndt", "Thomas Bach"], "title": "Using Large Language Models to Support Automation of Failure Management in CI/CD Pipelines: A Case Study in SAP HANA", "comment": "Accepted for publication in the proceedings of SANER 2026", "summary": "CI/CD pipeline failure management is time-consuming when performed manually. Automating this process is non-trivial because the information required for effective failure management is unstructured and cannot be automatically processed by traditional programs. With their ability to process unstructured data, large language models (LLMs) have shown promising results for automated failure management by previous work. Following these studies, we evaluated whether an LLM-based system could automate failure management in a CI/CD pipeline in the context of a large industrial software project, namely SAP HANA. We evaluated the ability of the LLM-based system to identify the error location and to propose exact solutions that contain no unnecessary actions. To support the LLM in generating exact solutions, we provided it with different types of domain knowledge, including pipeline information, failure management instructions, and data from historical failures. We conducted an ablation study to determine which type of domain knowledge contributed most to solution accuracy. The results show that data from historical failures contributed the most to the system's accuracy, enabling it to produce exact solutions in 92.1% of cases in our dataset. The system correctly identified the error location with 97.4% accuracy when provided with domain knowledge, compared to 84.2% accuracy without it. In conclusion, our findings indicate that LLMs, when provided with data from historical failures, represent a promising approach for automating CI/CD pipeline failure management.", "AI": {"tldr": "LLM\u7ed3\u5408\u5386\u53f2\u6545\u969c\u6570\u636e\u53ef\u81ea\u52a8\u5316CI/CD\u7ba1\u9053\u6545\u969c\u7ba1\u7406\uff0c\u5728\u5de5\u4e1a\u9879\u76ee\u4e2d\u8fbe\u523092.1%\u7684\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\u751f\u6210\u51c6\u786e\u7387\u3002", "motivation": "CI/CD\u7ba1\u9053\u6545\u969c\u7ba1\u7406\u624b\u52a8\u64cd\u4f5c\u8017\u65f6\uff0c\u4f20\u7edf\u7a0b\u5e8f\u65e0\u6cd5\u5904\u7406\u975e\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u800cLLM\u5728\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u9700\u8981\u9a8c\u8bc1\u5176\u5728\u5de5\u4e1a\u9879\u76ee\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u5728SAP HANA\u5927\u578b\u5de5\u4e1a\u9879\u76ee\u4e2d\u8bc4\u4f30LLM\u7cfb\u7edf\uff0c\u63d0\u4f9b\u7ba1\u9053\u4fe1\u606f\u3001\u6545\u969c\u7ba1\u7406\u6307\u4ee4\u548c\u5386\u53f2\u6545\u969c\u6570\u636e\u7b49\u4e0d\u540c\u9886\u57df\u77e5\u8bc6\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u5404\u7c7b\u77e5\u8bc6\u5bf9\u89e3\u51b3\u65b9\u6848\u51c6\u786e\u6027\u7684\u8d21\u732e\u3002", "result": "\u5386\u53f2\u6545\u969c\u6570\u636e\u5bf9\u7cfb\u7edf\u51c6\u786e\u6027\u8d21\u732e\u6700\u5927\uff0c\u4f7f\u7cfb\u7edf\u572892.1%\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u7cbe\u786e\u89e3\u51b3\u65b9\u6848\uff1b\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u65f6\u9519\u8bef\u5b9a\u4f4d\u51c6\u786e\u7387\u8fbe97.4%\uff0c\u65e0\u9886\u57df\u77e5\u8bc6\u65f6\u4e3a84.2%\u3002", "conclusion": "LLM\u7ed3\u5408\u5386\u53f2\u6545\u969c\u6570\u636e\u662f\u81ea\u52a8\u5316CI/CD\u7ba1\u9053\u6545\u969c\u7ba1\u7406\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.06074", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06074", "abs": "https://arxiv.org/abs/2602.06074", "authors": ["Mohammad Umar", "Bharat Tripathi"], "title": "Experimental Analysis of Server-Side Caching for Web Performance", "comment": "4 pages, experimental study, server-side caching", "summary": "Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.", "AI": {"tldr": "\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u5728\u5c0f\u578bWeb\u5e94\u7528\u4e2d\uff0c\u7b80\u5355\u7684\u5185\u5b58\u7f13\u5b58\u80fd\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u65f6\u95f4\uff0c\u9002\u5408\u6559\u80b2\u548c\u7b80\u5355\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u867d\u7136\u7f13\u5b58\u6280\u672f\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5c0f\u578bWeb\u5e94\u7528\u4e2d\u7b80\u5355\u5185\u5b58\u7f13\u5b58\u6548\u679c\u7684\u5b9e\u9a8c\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u63a2\u7d22\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u5728\u5c0f\u578b\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u5b9e\u9a8c\u65b9\u6cd5\u6bd4\u8f83\u4e24\u79cd\u670d\u52a1\u5668\u914d\u7f6e\uff1a\u65e0\u7f13\u5b58\u914d\u7f6e vs \u5e26\u5185\u5b58\u7f13\u5b58\u548c\u56fa\u5b9aTTL\u7684\u914d\u7f6e\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7Web\u670d\u52a1\u5668\u6846\u67b6\uff0c\u5728\u76f8\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u901a\u8fc7\u91cd\u590dHTTP\u8bf7\u6c42\u6d4b\u91cf\u54cd\u5e94\u65f6\u95f4\u3002", "result": "\u7ed3\u679c\u663e\u793a\u7f13\u5b58\u8bf7\u6c42\u7684\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u5728\u63d0\u5347Web\u5e94\u7528\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7b80\u5355\u5185\u5b58\u7f13\u5b58\u80fd\u6709\u6548\u6539\u5584\u5c0f\u578bWeb\u5e94\u7528\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u6559\u80b2\u73af\u5883\u548c\u7b80\u5355\u5e94\u7528\u573a\u666f\uff0c\u5176\u4e2d\u7b80\u5355\u6027\u548c\u53ef\u590d\u73b0\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2602.06831", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06831", "abs": "https://arxiv.org/abs/2602.06831", "authors": ["Marco De Luca", "Domenico Amalfitano", "Anna Rita Fasolino", "Porfirio Tramontana"], "title": "Statistical-Based Metric Threshold Setting Method for Software Fault Prediction in Firmware Projects: An Industrial Experience", "comment": null, "summary": "Ensuring software quality in embedded firmware is critical, especially in safety-critical domains where compliance with functional safety standards (ISO 26262) requires strong guarantees of software reliability. While machine learning-based fault prediction models have demonstrated high accuracy, their lack of interpretability limits their adoption in industrial settings. Developers need actionable insights that can be directly employed in software quality assurance processes and guide defect mitigation strategies. In this paper, we present a structured process for defining context-specific software metric thresholds suitable for integration into fault detection workflows in industrial settings. Our approach supports cross-project fault prediction by deriving thresholds from one set of projects and applying them to independently developed firmware, thereby enabling reuse across similar software systems without retraining or domain-specific tuning. We analyze three real-world C-embedded firmware projects provided by an industrial partner, using Coverity and Understand static analysis tools to extract software metrics. Through statistical analysis and hypothesis testing, we identify discriminative metrics and derived empirical threshold values capable of distinguishing faulty from non-faulty functions. The derived thresholds are validated through an experimental evaluation, demonstrating their effectiveness in identifying fault-prone functions with high precision. The results confirm that the derived thresholds can serve as an interpretable solution for fault prediction, aligning with industry standards and SQA practices. This approach provides a practical alternative to black-box AI models, allowing developers to systematically assess software quality, take preventive actions, and integrate metric-based fault prediction into industrial development workflows to mitigate software faults.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e3a\u5d4c\u5165\u5f0f\u56fa\u4ef6\u5b9a\u4e49\u8f6f\u4ef6\u5ea6\u91cf\u9608\u503c\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684\u6545\u969c\u9884\u6d4b\uff0c\u66ff\u4ee3\u9ed1\u76d2AI\u6a21\u578b", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982ISO 26262\u6807\u51c6\uff09\uff0c\u9700\u8981\u53ef\u9760\u7684\u8f6f\u4ef6\u8d28\u91cf\u4fdd\u8bc1\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u6545\u969c\u9884\u6d4b\u6a21\u578b\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5de5\u4e1a\u5e94\u7528\u3002\u5f00\u53d1\u8005\u9700\u8981\u53ef\u76f4\u63a5\u7528\u4e8eSQA\u6d41\u7a0b\u548c\u7f3a\u9677\u7f13\u89e3\u7b56\u7565\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u6d41\u7a0b\u5b9a\u4e49\u7279\u5b9a\u4e0a\u4e0b\u6587\u7684\u8f6f\u4ef6\u5ea6\u91cf\u9608\u503c\uff1a1\uff09\u5206\u6790\u4e09\u4e2a\u771f\u5b9eC\u5d4c\u5165\u5f0f\u56fa\u4ef6\u9879\u76ee\uff1b2\uff09\u4f7f\u7528Coverity\u548cUnderstand\u9759\u6001\u5206\u6790\u5de5\u5177\u63d0\u53d6\u8f6f\u4ef6\u5ea6\u91cf\uff1b3\uff09\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u548c\u5047\u8bbe\u68c0\u9a8c\u8bc6\u522b\u533a\u5206\u6027\u5ea6\u91cf\uff1b4\uff09\u63a8\u5bfc\u80fd\u533a\u5206\u6545\u969c\u4e0e\u975e\u6545\u969c\u51fd\u6570\u7684\u7ecf\u9a8c\u9608\u503c\uff1b5\uff09\u652f\u6301\u8de8\u9879\u76ee\u6545\u969c\u9884\u6d4b\uff0c\u4ece\u4e00\u4e2a\u9879\u76ee\u96c6\u63a8\u5bfc\u9608\u503c\u5e76\u5e94\u7528\u4e8e\u72ec\u7acb\u5f00\u53d1\u7684\u56fa\u4ef6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\u63a8\u5bfc\u7684\u9608\u503c\u80fd\u6709\u6548\u8bc6\u522b\u6545\u969c\u503e\u5411\u51fd\u6570\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u3002\u8fd9\u4e9b\u9608\u503c\u53ef\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u6545\u969c\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u7b26\u5408\u884c\u4e1a\u6807\u51c6\u548cSQA\u5b9e\u8df5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ed1\u76d2AI\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u7cfb\u7edf\u8bc4\u4f30\u8f6f\u4ef6\u8d28\u91cf\u3001\u91c7\u53d6\u9884\u9632\u63aa\u65bd\uff0c\u5e76\u5c06\u57fa\u4e8e\u5ea6\u91cf\u7684\u6545\u969c\u9884\u6d4b\u96c6\u6210\u5230\u5de5\u4e1a\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\uff0c\u4ee5\u7f13\u89e3\u8f6f\u4ef6\u6545\u969c\u3002"}}
{"id": "2602.06075", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06075", "abs": "https://arxiv.org/abs/2602.06075", "authors": ["Guangyi Liu", "Pengxiang Zhao", "Yaozhen Liang", "Qinyi Luo", "Shunye Tang", "Yuxiang Chai", "Weifeng Lin", "Han Xiao", "WenHao Wang", "Siheng Chen", "Zhengxi Lu", "Gao Wu", "Hao Wang", "Liang Liu", "Yong Liu"], "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments", "comment": "https://lgy0404.github.io/MemGUI-Bench/", "summary": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textbf{\\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/.", "AI": {"tldr": "MemGUI-Bench\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u79fb\u52a8GUI\u4ee3\u7406\u8bb0\u5fc6\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u5728\u8bb0\u5fc6\u76f8\u5173\u4efb\u52a1\u8bc4\u4f30\u4e0a\u7684\u7a7a\u767d\uff0c\u5305\u542b128\u4e2a\u8de8\u65f6\u7a7a\u8bb0\u5fc6\u4efb\u52a1\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8GUI\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u5728\u8bb0\u5fc6\u80fd\u529b\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u53ea\u67095.2-11.8%\u7684\u4efb\u52a1\u6d89\u53ca\u8bb0\u5fc6\uff0c\u4e14\u7f3a\u4e4f\u8de8\u4f1a\u8bdd\u5b66\u4e60\u8bc4\u4f30\u3002\u9700\u8981\u4e13\u95e8\u7684\u8bb0\u5fc6\u4e2d\u5fc3\u5316\u57fa\u51c6\u6765\u5168\u9762\u8bc4\u4f30GUI\u4ee3\u7406\u7684\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "1) \u63d0\u51fa\u7cfb\u7edf\u5316\u8bb0\u5fc6\u5206\u7c7b\u6cd5\uff0c\u5206\u679011\u4e2a\u4ee3\u7406\u76845\u79cd\u67b6\u6784\uff1b2) \u521b\u5efa128\u4e2a\u8de826\u4e2a\u5e94\u7528\u7684\u4efb\u52a1\uff0c\u5176\u4e2d89.8%\u901a\u8fc7\u8de8\u65f6\u7a7a\u8bb0\u5fc6\u4fdd\u6301\u6765\u6311\u6218\u8bb0\u5fc6\u80fd\u529b\uff1b3) \u5f00\u53d1MemGUI-Eval\u81ea\u52a8\u5316\u8bc4\u4f30\u7ba1\u9053\uff0c\u5305\u542b\u6e10\u8fdb\u5f0f\u5ba1\u67e5\u548c7\u4e2a\u5206\u5c42\u6307\u6807\uff1b4) \u5bf911\u4e2a\u6700\u5148\u8fdb\u4ee3\u7406\u8fdb\u884cRQ\u9a71\u52a8\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6240\u6709\u8bc4\u4f30\u7cfb\u7edf\u90fd\u5b58\u5728\u663e\u8457\u8bb0\u5fc6\u7f3a\u9677\uff0c\u8bc6\u522b\u51fa5\u79cd\u4e0d\u540c\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u7efc\u5408\u51fa5\u4e2a\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u542f\u793a\u3002\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\u5c06\u5b8c\u5168\u5f00\u6e90\u5e76\u6301\u7eed\u7ef4\u62a4\u3002", "conclusion": "MemGUI-Bench\u586b\u8865\u4e86\u79fb\u52a8GUI\u4ee3\u7406\u8bb0\u5fc6\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u8bb0\u5fc6\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765GUI\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u6240\u6709\u8d44\u6e90\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2602.06875", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06875", "abs": "https://arxiv.org/abs/2602.06875", "authors": ["Jiangping Huang", "Wenguang Ye", "Weisong Sun", "Jian Zhang", "Mingyue Zhang", "Yang Liu"], "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code", "comment": null, "summary": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.", "AI": {"tldr": "TraceCoder\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u8ffd\u8e2a\u548c\u56e0\u679c\u5206\u6790\u6765\u4fee\u590dLLM\u751f\u6210\u7684\u4ee3\u7801\u9519\u8bef\uff0c\u7ed3\u5408\u5386\u53f2\u6559\u8bad\u5b66\u4e60\u548c\u56de\u6eda\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4fee\u590d\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7801\u4fee\u590d\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u7684\u901a\u8fc7/\u5931\u8d25\u4fe1\u53f7\uff0c\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u884c\u4e3a\u7684\u6df1\u5165\u6d1e\u5bdf\uff0c\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u9519\u8bef\uff0c\u4e14\u4fee\u590d\u8fc7\u7a0b\u5bb9\u6613\u9677\u5165\u4f4e\u6548\u5faa\u73af\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u4fee\u590d\u6846\u67b6\u3002", "method": "TraceCoder\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u6a21\u62df\u4eba\u7c7b\u4e13\u5bb6\u7684\u89c2\u5bdf-\u5206\u6790-\u4fee\u590d\u8fc7\u7a0b\uff1a1) \u5728\u4ee3\u7801\u4e2d\u63d2\u5165\u8bca\u65ad\u63a2\u9488\u6355\u83b7\u7ec6\u7c92\u5ea6\u8fd0\u884c\u65f6\u8ffd\u8e2a\uff1b2) \u5bf9\u8ffd\u8e2a\u8fdb\u884c\u56e0\u679c\u5206\u6790\u5b9a\u4f4d\u6839\u672c\u539f\u56e0\uff1b3) \u5f15\u5165\u5386\u53f2\u6559\u8bad\u5b66\u4e60\u673a\u5236\u4ece\u5931\u8d25\u4fee\u590d\u4e2d\u5b66\u4e60\uff1b4) \u4f7f\u7528\u56de\u6eda\u673a\u5236\u786e\u4fdd\u6bcf\u6b21\u8fed\u4ee3\u90fd\u662f\u4e25\u683c\u6539\u8fdb\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTraceCoder\u76f8\u6bd4\u73b0\u6709\u5148\u8fdb\u57fa\u7ebf\u5728Pass@1\u51c6\u786e\u7387\u4e0a\u53d6\u5f97\u6700\u9ad834.43%\u7684\u76f8\u5bf9\u63d0\u5347\uff1b\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u8fed\u4ee3\u4fee\u590d\u8fc7\u7a0b\u5355\u72ec\u8d21\u732e65.61%\u7684\u76f8\u5bf9\u589e\u76ca\uff1b\u5728\u51c6\u786e\u7387\u548c\u6210\u672c\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u9886\u5148\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002", "conclusion": "TraceCoder\u901a\u8fc7\u6df1\u5ea6\u8fd0\u884c\u65f6\u6d1e\u5bdf\u3001\u56e0\u679c\u5206\u6790\u548c\u5386\u53f2\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u4ee3\u7801\u4fee\u590d\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u81ea\u52a8\u5316\u4ee3\u7801\u4fee\u590d\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06079", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06079", "abs": "https://arxiv.org/abs/2602.06079", "authors": ["Liangyu Wang", "Siqi Zhang", "Junjie Wang", "Yiming Dong", "Bo Zheng", "Zihan Qiu", "Shengkun Tang", "Di Wang", "Rui Men", "Dayiheng Liu"], "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers", "comment": null, "summary": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.", "AI": {"tldr": "Canzona\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f02\u6b65\u8d1f\u8f7d\u5747\u8861\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u77e9\u9635\u4f18\u5316\u5668\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u8026\u903b\u8f91\u4f18\u5316\u5668\u5206\u914d\u4e0e\u7269\u7406\u53c2\u6570\u5206\u5e03\uff0c\u5728256 GPU\u4e0a\u5b9e\u73b01.57\u500d\u7aef\u5230\u7aef\u8fed\u4ee3\u52a0\u901f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u63a8\u52a8\u4e86\u5bf9\u77e9\u9635\u4f18\u5316\u5668\u7684\u5174\u8da3\uff0c\u4f46\u5176\u6574\u4f53\u66f4\u65b0\u9700\u6c42\u4e0e\u5206\u5e03\u5f0f\u6846\u67b6\u4e2d\u7684\u5f20\u91cf\u788e\u7247\u5316\u5b58\u5728\u51b2\u7a81\u3002\u73b0\u6709\u65b9\u6848\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u6216\u8fdd\u53cd\u9ad8\u6548\u901a\u4fe1\u539f\u8bed\u51e0\u4f55\u7ea6\u675f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faCanzona\u6846\u67b6\uff1a1) \u6570\u636e\u5e76\u884c\u4e2d\u91c7\u7528\u03b1\u5e73\u8861\u9759\u6001\u5206\u533a\u7b56\u7565\uff0c\u4fdd\u6301\u539f\u5b50\u6027\u540c\u65f6\u5e73\u8861\u8d1f\u8f7d\uff1b2) \u5f20\u91cf\u5e76\u884c\u4e2d\u8bbe\u8ba1\u5f02\u6b65\u8ba1\u7b97\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u5fae\u7ec4\u8c03\u5ea6\u6279\u91cf\u5904\u7406\u788e\u7247\u5316\u66f4\u65b0\u5e76\u9690\u85cf\u91cd\u6784\u5f00\u9500\u3002", "result": "\u5728Qwen3\u6a21\u578b\u5bb6\u65cf\uff08\u6700\u591a320\u4ebf\u53c2\u6570\uff09\u548c256\u4e2aGPU\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u73b0\u6709\u5e76\u884c\u67b6\u6784\u7684\u6548\u7387\uff0c\u7aef\u5230\u7aef\u8fed\u4ee3\u65f6\u95f4\u52a0\u901f1.57\u500d\uff0c\u4f18\u5316\u5668\u6b65\u9aa4\u5ef6\u8fdf\u964d\u4f4e5.8\u500d\u3002", "conclusion": "Canzona\u6210\u529f\u89e3\u51b3\u4e86\u77e9\u9635\u4f18\u5316\u5668\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6548\u7387\u51b2\u7a81\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5f02\u6b65\u8d1f\u8f7d\u5747\u8861\u6846\u67b6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u5668\u96c6\u6210\u65b9\u6848\u3002"}}
{"id": "2602.06085", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06085", "abs": "https://arxiv.org/abs/2602.06085", "authors": ["Maxim Moraru", "Kamalavasan Kamalakkannan", "Jered Dominguez-Trujillo", "Patrick Diehl", "Atanu Barai", "Julien Loiseau", "Zachary Kent Baker", "Howard Pritchard", "Galen M Shipman"], "title": "LAAFD: LLM-based Agents for Accelerated FPGA Design", "comment": null, "summary": "FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.", "AI": {"tldr": "LAAFD\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u80fd\u591f\u5c06\u901a\u7528C++\u4ee3\u7801\u8f6c\u6362\u4e3a\u4f18\u5316\u7684Vitis HLS\u5185\u6838\uff0c\u5b9e\u73b0\u63a5\u8fd1\u624b\u5de5\u8c03\u4f18\u7684\u6027\u80fd\uff0899.9%\u51e0\u4f55\u5e73\u5747\u6027\u80fd\uff09\uff0c\u663e\u8457\u964d\u4f4eFPGA\u52a0\u901f\u7684\u4e13\u4e1a\u95e8\u69db\u3002", "motivation": "FPGA\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u5177\u6709\u9ad8\u6027\u80fd\u3001\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u91c7\u7528\u3002\u867d\u7136\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u63d0\u9ad8\u4e86\u751f\u4ea7\u529b\uff0c\u4f46\u7ade\u4e89\u6027\u8bbe\u8ba1\u4ecd\u9700\u8981\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6d41\u3002", "method": "LAAFD\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5c06\u901a\u7528C++\u4ee3\u7801\u8f6c\u6362\u4e3a\u4f18\u5316\u7684Vitis HLS\u5185\u6838\u3002\u81ea\u52a8\u5316\u5173\u952e\u8f6c\u6362\u5305\u62ec\u6df1\u5ea6\u6d41\u6c34\u7ebf\u3001\u5411\u91cf\u5316\u548c\u6570\u636e\u6d41\u5206\u533a\uff0c\u5e76\u901a\u8fc7HLS\u534f\u540c\u4eff\u771f\u548c\u7efc\u5408\u53cd\u9988\u5f62\u6210\u95ed\u73af\uff0c\u9a8c\u8bc1\u6b63\u786e\u6027\u5e76\u8fed\u4ee3\u6539\u8fdb\u6267\u884c\u5468\u671f\u3002", "result": "\u5728\u4ee3\u8868HPC\u5e38\u89c1\u8ba1\u7b97\u6a21\u5f0f\u768415\u4e2a\u5185\u6838\u6d4b\u8bd5\u5957\u4ef6\u4e2d\uff0cLAAFD\u76f8\u6bd4\u624b\u5de5\u8c03\u4f18\u7684Vitis HLS\u57fa\u7ebf\u5b9e\u73b0\u4e8699.9%\u7684\u51e0\u4f55\u5e73\u5747\u6027\u80fd\u3002\u5bf9\u4e8e\u6a21\u677f\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\uff0cLAAFD\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8eDSL\u7684HLS\u4ee3\u7801\u751f\u6210\u5668SODA\u76f8\u5f53\uff0c\u540c\u65f6\u751f\u6210\u66f4\u6613\u8bfb\u7684\u5185\u6838\u3002", "conclusion": "LAAFD\u663e\u8457\u964d\u4f4e\u4e86FPGA\u52a0\u901f\u7684\u4e13\u4e1a\u77e5\u8bc6\u95e8\u69db\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u6548\u7387\uff0c\u4e3aFPGA\u5728\u79d1\u5b66\u548c\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u66f4\u5e7f\u6cdb\u91c7\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2602.06498", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06498", "abs": "https://arxiv.org/abs/2602.06498", "authors": ["Arno Geimer"], "title": "BouquetFL: Emulating diverse participant hardware in Federated Learning", "comment": null, "summary": "In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.", "AI": {"tldr": "BouquetFL\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8d44\u6e90\u9650\u5236\u5728\u5355\u673a\u4e0a\u6a21\u62df\u5f02\u6784\u5ba2\u6237\u7aef\u786c\u4ef6\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u5728\u4e0d\u4f9d\u8d56\u591a\u7269\u7406\u8bbe\u5907\u7684\u60c5\u51b5\u4e0b\u7814\u7a76\u7cfb\u7edf\u5f02\u6784\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u5728\u5355\u673a\u4e0a\u8fdb\u884c\u6a21\u62df\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u5ba2\u6237\u7aef\u786c\u4ef6\u5f02\u6784\u6027\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u8fd9\u5bfc\u81f4\u5b9e\u9a8c\u6761\u4ef6\u4e0e\u5b9e\u9645\u60c5\u51b5\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u8d44\u6e90\u9650\u5236\u6765\u6a21\u62df\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\uff0c\u63d0\u4f9b\u4ece\u5e38\u89c1\u6d88\u8d39\u8bbe\u5907\u548c\u5c0f\u578b\u5b9e\u9a8c\u5ba4\u8bbe\u5907\u4e2d\u63d0\u53d6\u7684\u591a\u79cd\u786c\u4ef6\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u771f\u5b9e\u786c\u4ef6\u6d41\u884c\u5ea6\u7684\u81ea\u5b9a\u4e49\u786c\u4ef6\u91c7\u6837\u5668\u3002", "result": "\u5f00\u53d1\u4e86BouquetFL\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5355\u7269\u7406\u673a\u5668\u4e0a\u6a21\u62df\u5f02\u6784\u5ba2\u6237\u7aef\u786c\u4ef6\uff0c\u4f7f\u8054\u90a6\u5b66\u4e60\u5b9e\u9a8c\u66f4\u8d34\u8fd1\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u3002", "conclusion": "BouquetFL\u586b\u8865\u4e86\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u4e2d\u7684\u65b9\u6cd5\u5b66\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u9ad8\u5ea6\u5f02\u6784\u8054\u90a6\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4fbf\u6377\u5de5\u5177\uff0c\u4f7f\u5b9e\u9a8c\u5b9e\u8df5\u66f4\u63a5\u8fd1\u5b9e\u9645\u90e8\u7f72\u6761\u4ef6\u3002"}}
{"id": "2602.06499", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06499", "abs": "https://arxiv.org/abs/2602.06499", "authors": ["Gyeongseo Park", "Eungyeong Lee", "Song-woo Sok", "Myung-Hoon Cha", "Kwangwon Koh", "Baik-Song An", "Hongyeon Kim", "Ki-Dong Kang"], "title": "FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training", "comment": "14 pages,10 figures", "summary": "Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.", "AI": {"tldr": "FCDP \u662f\u4e00\u79cd\u9488\u5bf9\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u524d\u5411\u4f20\u64ad\u53c2\u6570\u7f13\u5b58\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\uff0c\u5e76\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u901a\u8fc7\u5feb\u901f\u8282\u70b9\u5185all-gather\u590d\u7528\uff0c\u51cf\u5c1150%\u7684\u8282\u70b9\u95f4\u901a\u4fe1\uff0c\u5728PEFT\u573a\u666f\u4e0b\u53ef\u51cf\u5c1199%\u4ee5\u4e0a\u7684\u8282\u70b9\u95f4\u6d41\u91cf\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u5728\u5e26\u5bbd\u53d7\u9650\u7684\u786c\u4ef6\u73af\u5883\u4e0b\u5b58\u5728\u74f6\u9888\uff1aZeRO-3\u5728\u8282\u70b9\u95f4\u901a\u4fe1\u5f00\u9500\u5927\uff1bGPU\u5185\u5b58\u7f13\u5b58\u65b9\u6cd5\uff08\u5982MiCS\u3001ZeRO++\uff09\u4f1a\u5bfc\u81f4\u5185\u5b58\u4e0d\u8db3\uff1b\u4e3b\u673a\u5185\u5b58\u5378\u8f7d\u65b9\u6cd5\uff08\u5982ZeRO-Offload\u3001ZeRO-Infinity\uff09\u56e0PCIe\u5f00\u9500\u964d\u4f4e\u541e\u5410\u91cf\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u4e0a\u9ad8\u6548\u5229\u7528\u4e3b\u673a\u5185\u5b58\u7684\u65b9\u6cd5\u3002", "method": "FCDP\u5c06\u4e3b\u673a\u5185\u5b58\u4f5c\u4e3a\u5feb\u901f\u7f13\u5b58\u5c42\u800c\u975e\u6ea2\u51fa\u5c42\uff0c\u7f13\u5b58\u524d\u5411\u4f20\u64ad\u53c2\u6570\u5e76\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u901a\u8fc7\u8282\u70b9\u5185all-gather\u590d\u7528\uff0c\u6d88\u9664\u5197\u4f59\u7684\u8282\u70b9\u95f4\u901a\u4fe1\u3002\u5bf9\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\uff0c\u9009\u62e9\u6027\u901a\u4fe1\u4ec5\u8bad\u7ec3\u53c2\u6570\u4ee5\u6700\u5927\u5316\u7f13\u5b58\u6548\u679c\u3002", "result": "\u5728\u5546\u54c1\u786c\u4ef6\u96c6\u7fa4\u4e0a\uff0cFCDP\u76f8\u6bd4ZeRO-3\u5b9e\u73b0\u9ad8\u8fbe100\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u76f8\u6bd4ZeRO++\u5b9e\u73b051\u500d\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301ZeRO-3\u7684\u6700\u5927\u6279\u5904\u7406\u5927\u5c0f\u3002", "conclusion": "FCDP\u901a\u8fc7\u521b\u65b0\u6027\u5730\u5c06\u4e3b\u673a\u5185\u5b58\u4f5c\u4e3a\u5feb\u901f\u7f13\u5b58\u5c42\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e26\u5bbd\u53d7\u9650\u96c6\u7fa4\u4e0a\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u573a\u666f\u3002"}}
{"id": "2602.06502", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06502", "abs": "https://arxiv.org/abs/2602.06502", "authors": ["Ying Yuan", "Pengfei Zuo", "Bo Wang", "Zhangyu Chen", "Zhipeng Tan", "Zhou Yu"], "title": "DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving", "comment": "23 pages, 15 figures", "summary": "In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\\times$ under the same TTFT SLO constraints compared with SOTA work.", "AI": {"tldr": "DualMap\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u54c8\u5e0c\u51fd\u6570\u5c06\u8bf7\u6c42\u6620\u5c04\u5230\u5019\u9009\u5b9e\u4f8b\uff0c\u667a\u80fd\u9009\u62e9\u66f4\u597d\u7684\u5019\u9009\u5b9e\u4f8b\uff0c\u5728\u5206\u5e03\u5f0fLLM\u670d\u52a1\u4e2d\u540c\u65f6\u5b9e\u73b0\u7f13\u5b58\u4eb2\u548c\u6027\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u5728LLM\u670d\u52a1\u4e2d\uff0c\u8de8\u8bf7\u6c42\u91cd\u7528KV\u7f13\u5b58\u5bf9\u964d\u4f4eTTFT\u548c\u670d\u52a1\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u8c03\u5ea6\u5668\u65e0\u6cd5\u534f\u8c03\u7f13\u5b58\u4eb2\u548c\u6027\u8c03\u5ea6\uff08\u5c06\u76f8\u540c\u524d\u7f00\u7684\u8bf7\u6c42\u653e\u5728\u4e00\u8d77\uff09\u548c\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\uff08\u5c06\u8bf7\u6c42\u5747\u5300\u5206\u5e03\uff09\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u5728\u5355\u4e00\u6620\u5c04\u7a7a\u95f4\u5185\u64cd\u4f5c\u3002", "method": "DualMap\u91c7\u7528\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u54c8\u5e0c\u51fd\u6570\u57fa\u4e8e\u8bf7\u6c42\u63d0\u793a\u5c06\u6bcf\u4e2a\u8bf7\u6c42\u6620\u5c04\u5230\u4e24\u4e2a\u5019\u9009\u5b9e\u4f8b\uff1b2\uff09\u57fa\u4e8e\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u667a\u80fd\u9009\u62e9\u66f4\u597d\u7684\u5019\u9009\u5b9e\u4f8b\uff1b3\uff09\u7ed3\u5408SLO\u611f\u77e5\u8bf7\u6c42\u8def\u7531\u3001\u70ed\u70b9\u611f\u77e5\u91cd\u65b0\u5e73\u8861\u548c\u8f7b\u91cf\u7ea7\u53cc\u54c8\u5e0c\u73af\u6269\u5c55\u4e09\u79cd\u6280\u672f\u6765\u5e94\u5bf9\u52a8\u6001\u548c\u503e\u659c\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u7684TTFT SLO\u7ea6\u675f\u4e0b\uff0cDualMap\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5de5\u4f5c\u5c06\u6709\u6548\u8bf7\u6c42\u5bb9\u91cf\u63d0\u9ad8\u4e86\u6700\u591a2.25\u500d\u3002", "conclusion": "DualMap\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u7f13\u5b58\u4eb2\u548c\u6027\u548c\u8d1f\u8f7d\u5747\u8861\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u5206\u5e03\u5f0fLLM\u670d\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.06555", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06555", "abs": "https://arxiv.org/abs/2602.06555", "authors": ["Lanpei Li", "Massimo Coppola", "Malio Li", "Valerio Besozzi", "Jack Bell", "Vincenzo Lomonaco"], "title": "Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms", "comment": "Accepted at AHPC3 workshop, PDP 2026", "summary": "We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u52a8\u6001\u7ba1\u7406\u7ed3\u6784\u5316\u5e76\u884c\u5904\u7406\u9aa8\u67b6\u7684\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8eFarm\u6a21\u5f0f\uff0c\u7ed3\u5408AI\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u6765\u63d0\u5347\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387", "motivation": "\u5c06HPC\u7ea7\u522b\u7684\u6027\u80fd\u548c\u5f39\u6027\u5f15\u5165\u65e0\u670d\u52a1\u5668\u548c\u8fde\u7eed\u8ba1\u7b97\u73af\u5883\uff0c\u540c\u65f6\u4fdd\u6301\u9aa8\u67b6\u7f16\u7a0b\u7684\u53ef\u7f16\u7a0b\u6027\u4f18\u52bf", "method": "\u57fa\u4e8eOpenFaaS\u5e73\u53f0\u5b9e\u73b0Farm\u6a21\u5f0f\uff0c\u5c06\u5de5\u4f5c\u6c60\u81ea\u52a8\u6269\u5c55\u89c6\u4e3aQoS\u611f\u77e5\u7684\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\uff0c\u6846\u67b6\u5305\u542b\u53ef\u91cd\u7528farm\u6a21\u677f\u548c\u57fa\u4e8eGymnasium\u7684\u76d1\u63a7\u63a7\u5236\u5c42\uff0c\u652f\u6301\u53cd\u5e94\u5f0f\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u5668", "result": "AI\u9a71\u52a8\u7684\u7ba1\u7406\u6bd4\u7eaf\u57fa\u4e8e\u6a21\u578b\u7684\u6027\u80fd\u5f15\u5bfc\u66f4\u80fd\u9002\u5e94\u5e73\u53f0\u7279\u5b9a\u9650\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u8d44\u6e90\u4f7f\u7528\u548c\u7a33\u5b9a\u6269\u5c55\u884c\u4e3a\u7684\u540c\u65f6\u6539\u5584QoS", "conclusion": "AI\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u7ba1\u7406\u5e76\u884c\u5ea6\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5e73\u8861\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387"}}
