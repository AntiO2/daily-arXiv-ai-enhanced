{"id": "2510.12889", "categories": ["cs.DC", "C.2.4"], "pdf": "https://arxiv.org/pdf/2510.12889", "abs": "https://arxiv.org/abs/2510.12889", "authors": ["Wei Da", "Evangelia Kalyvianaki"], "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters", "comment": "single column,20 pages and 8 figures", "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.", "AI": {"tldr": "Dodoor\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u968f\u673a\u5316\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u6279\u91cf\u66f4\u65b0\u7f13\u5b58\u670d\u52a1\u5668\u4fe1\u606f\u548c\u65b0\u578b\u8d1f\u8f7d\u8bc4\u5206\u673a\u5236\uff0c\u5728\u5f02\u6784\u96c6\u7fa4\u4e2d\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u5347\u8c03\u5ea6\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u9700\u8981\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u4efb\u52a1\u8c03\u5ea6\uff0c\u4f46\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u8c03\u5ea6\u5668\u4f9d\u8d56\u5b9e\u65f6\u63a2\u6d4b\u8fdc\u7a0b\u670d\u52a1\u5668\uff0c\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u5927\u3002Dodoor\u65e8\u5728\u901a\u8fc7\u7f13\u5b58\u4fe1\u606f\u548c\u6279\u91cf\u66f4\u65b0\u6765\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u3002", "method": "\u57fa\u4e8e\u52a0\u6743\u7403-\u7bb1\u6a21\u578b\u7684b-batch\u8bbe\u7f6e\uff0c\u4f7f\u7528\u7f13\u5b58\u670d\u52a1\u5668\u4fe1\u606f\u8fdb\u884c\u8c03\u5ea6\u51b3\u7b56\uff0c\u901a\u8fc7\u65b0\u578b\u8d1f\u8f7d\u8bc4\u5206\u673a\u5236\u8861\u91cf\u670d\u52a1\u5668\u4e0e\u4efb\u52a1\u95f4\u7684\u53cd\u4eb2\u548c\u6027\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u5f85\u5904\u7406\u4efb\u52a1\u8ba1\u6570\u65b9\u6cd5\u3002", "result": "\u5728101\u8282\u70b9\u5f02\u6784\u96c6\u7fa4\u6d4b\u8bd5\u4e2d\uff0cDodoor\u5728\u4e24\u4e2a\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5206\u522b\u51cf\u5c11\u8c03\u5ea6\u6d88\u606f55-66%\uff0c\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad833.2%\u548c21.5%\uff0c\u5e73\u5747\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1112.1%\u548c7.2%\uff0c\u5c3e\u90e8\u5ef6\u8fdf\u6539\u558421.9%\u548c24.6%\u3002", "conclusion": "Dodoor\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u548c\u4f18\u5316\u8d1f\u8f7d\u5e73\u8861\uff0c\u5728\u5f02\u6784\u96c6\u7fa4\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e\u7f13\u5b58\u4fe1\u606f\u548c\u65b0\u578b\u8d1f\u8f7d\u8bc4\u5206\u673a\u5236\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.13203", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.13203", "abs": "https://arxiv.org/abs/2510.13203", "authors": ["Mehdi Zekriyapanah Gashti"], "title": "Scrutiny new framework in integrated distributed reliable systems", "comment": null, "summary": "In this paper we represent a new framework for integrated distributed\nsystems. In the proposed framework we have used three parts to increase\nSatisfaction and Performance of this framework. At first we analyse integrated\nsystems and their evolution process and also ERPSD and ERPDRT framework briefly\nthen we explain the new FDIRS framework. Finally we compare the results of\nsimulation of the new framework with presented frameworks. Result showed In\nFIDRS framework, the technique of heterogeneous distributed data base is used\nto improve Performance and speed in responding to users. Finally by using FDIRS\nframework we succeeded to increase Efficiency, Performance and reliability of\nintegrated systems and remove some of previous frameworks problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u6846\u67b6FDIRS\uff0c\u901a\u8fc7\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u6280\u672f\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u548c\u54cd\u5e94\u901f\u5ea6\uff0c\u89e3\u51b3\u4e86\u5148\u524d\u6846\u67b6\u7684\u4e00\u4e9b\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u6846\u67b6\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u6846\u67b6\u6765\u63d0\u5347\u7cfb\u7edf\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u4e09\u90e8\u5206\u7ed3\u6784\u6784\u5efaFDIRS\u6846\u67b6\uff0c\u91c7\u7528\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u6280\u672f\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\uff0c\u5e76\u4e0e\u73b0\u6709\u6846\u67b6ERPSD\u548cERPDRT\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793aFDIRS\u6846\u67b6\u5728\u6548\u7387\u3001\u6027\u80fd\u548c\u53ef\u9760\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5148\u524d\u6846\u67b6\u7684\u4e00\u4e9b\u95ee\u9898\u3002", "conclusion": "FDIRS\u6846\u67b6\u901a\u8fc7\u5f02\u6784\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u6280\u672f\u6709\u6548\u63d0\u9ad8\u4e86\u96c6\u6210\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13223", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.13223", "abs": "https://arxiv.org/abs/2510.13223", "authors": ["Yiyuan He", "Minxian Xu", "Jingfeng Wu", "Jianmin Hu", "Chong Ma", "Min Shen", "Le Chen", "Chengzhong Xu", "Lin Qu", "Kejiang Ye"], "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure", "comment": "23 pages", "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.", "AI": {"tldr": "BanaServe\u662f\u4e00\u4e2a\u52a8\u6001\u7f16\u6392\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u7ea7\u6743\u91cd\u8fc1\u79fb\u3001\u6ce8\u610f\u529b\u7ea7KV\u7f13\u5b58\u8fc1\u79fb\u548c\u5168\u5c40KV\u7f13\u5b58\u5b58\u50a8\u5171\u4eab\u673a\u5236\uff0c\u89e3\u51b3\u89e3\u8026LLM\u670d\u52a1\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u4e0d\u5e73\u8861\u3001\u8d1f\u8f7d\u4e0d\u5747\u548c\u7f13\u5b58\u611f\u77e5\u8def\u7531\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89e3\u8026LLM\u670d\u52a1\u7cfb\u7edf\u9762\u4e34\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u9759\u6001\u8d44\u6e90\u5206\u914d\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u3001\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u56fa\u6709\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3001\u4ee5\u53ca\u524d\u7f00\u7f13\u5b58\u611f\u77e5\u8def\u7531\u5bfc\u81f4\u7684\u8d1f\u8f7d\u5206\u5e03\u503e\u659c\uff0c\u8fd9\u4e9b\u90fd\u4f1a\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u6216\u8fdd\u53cdSLO\u3002", "method": "\u5f15\u5165\u5c42\u7ea7\u6743\u91cd\u8fc1\u79fb\u3001\u6ce8\u610f\u529b\u7ea7KV\u7f13\u5b58\u8fc1\u79fb\u548c\u5168\u5c40KV\u7f13\u5b58\u5b58\u50a8\u5171\u4eab\u673a\u5236\uff0c\u652f\u6301\u7c97\u7c92\u5ea6\uff08\u5c42\u7ea7\uff09\u548c\u7ec6\u7c92\u5ea6\uff08\u6ce8\u610f\u529b\u7ea7\uff09\u8d1f\u8f7d\u91cd\u5206\u914d\uff0c\u6700\u5c0f\u5316\u5ef6\u8fdf\u5f00\u9500\uff0c\u4f7f\u8def\u7531\u5668\u80fd\u591f\u8fdb\u884c\u7eaf\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u3002", "result": "\u76f8\u6bd4vLLM\uff0cBanaServe\u5b9e\u73b0\u4e861.2x-3.9x\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u603b\u5904\u7406\u65f6\u95f4\u964d\u4f4e3.9%-78.4%\uff1b\u76f8\u6bd4DistServe\uff0c\u541e\u5410\u91cf\u63d0\u53471.1x-2.8x\uff0c\u5ef6\u8fdf\u964d\u4f4e1.4%-70.1%\u3002", "conclusion": "BanaServe\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u7f16\u6392\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524d\u89e3\u8026LLM\u670d\u52a1\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u6548\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002"}}
{"id": "2510.13306", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2510.13306", "abs": "https://arxiv.org/abs/2510.13306", "authors": ["Jannick Borowitz", "Ernestine Gro\u00dfmann", "Mattthias Schimek"], "title": "Distributed Reductions for the Maximum Weight Independent Set Problem", "comment": null, "summary": "Finding maximum-weight independent sets in graphs is an important NP-hard\noptimization problem. Given a vertex-weighted graph $G$, the task is to find a\nsubset of pairwise non-adjacent vertices of $G$ with maximum weight. Most\nrecently published practical exact algorithms and heuristics for this problem\nuse a variety of data-reduction rules to compute (near-)optimal solutions.\nApplying these rules results in an equivalent instance of reduced size. An\noptimal solution to the reduced instance can be easily used to construct an\noptimal solution for the original input.\n  In this work, we present the first distributed-memory parallel reduction\nalgorithms for this problem, targeting graphs beyond the scale of previous\nsequential approaches. Furthermore, we propose the first distributed\nreduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight\nindependent set heuristically.\n  In our practical evaluation, our experiments on up to $1024$ processors\ndemonstrate good scalability of our distributed reduce algorithms while\nmaintaining good reduction impact. Our asynchronous reduce-and-peel approach\nachieves an average speedup of $33\\times$ over a sequential state-of-the-art\nreduce-and-peel approach on 36 real-world graphs with a solution quality close\nto the sequential algorithm. Our reduce-and-greedy algorithms even achieve\naverage speedups of up to $50\\times$ at the cost of a lower solution quality.\nMoreover, our distributed approach allows us to consider graphs with more than\none billion vertices and 17 billion edges.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u7f29\u51cf\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u9ad8\u6548\u8ba1\u7b97\u548c\u826f\u597d\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u662f\u4e00\u4e2a\u91cd\u8981\u7684NP\u96be\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709\u7684\u5b9e\u7528\u7b97\u6cd5\u4f7f\u7528\u6570\u636e\u7f29\u51cf\u89c4\u5219\u6765\u6c42\u89e3\uff0c\u4f46\u7f3a\u4e4f\u5206\u5e03\u5f0f\u5e76\u884c\u5b9e\u73b0\uff0c\u65e0\u6cd5\u5904\u7406\u8d85\u5927\u89c4\u6a21\u56fe\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u5185\u5b58\u5e76\u884c\u7f29\u51cf\u7b97\u6cd5\u3001\u5206\u5e03\u5f0freduce-and-greedy\u548creduce-and-peel\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u652f\u6301\u5f02\u6b65\u8ba1\u7b97\u3002", "result": "\u57281024\u4e2a\u5904\u7406\u5668\u4e0a\u6d4b\u8bd5\u663e\u793a\u826f\u597d\u53ef\u6269\u5c55\u6027\uff0creduce-and-peel\u65b9\u6cd5\u76f8\u6bd4\u987a\u5e8f\u7b97\u6cd5\u5e73\u5747\u52a0\u901f33\u500d\uff0creduce-and-greedy\u65b9\u6cd5\u52a0\u901f\u8fbe50\u500d\uff0c\u80fd\u5904\u7406\u8d85\u8fc710\u4ebf\u9876\u70b9\u548c170\u4ebf\u8fb9\u7684\u56fe\u3002", "conclusion": "\u5206\u5e03\u5f0f\u5e76\u884c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\uff0c\u80fd\u591f\u5904\u7406\u524d\u6240\u672a\u6709\u7684\u56fe\u89c4\u6a21\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12803", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.12803", "abs": "https://arxiv.org/abs/2510.12803", "authors": ["Shang Zhou", "Zihan Zheng", "Kaiyuan Liu", "Zeyu Shen", "Zerui Cheng", "Zexing Chen", "Hansen He", "Jianzhu Yao", "Huanzhi Mao", "Qiuyang Mang", "Tianfu Fu", "Beichen Li", "Dongruixuan Li", "Wenhao Chai", "Zhuang Liu", "Aleksandra Korolova", "Peter Henderson", "Natasha Jaques", "Pramod Viswanath", "Saining Xie", "Jingbo Shang"], "title": "AutoCode: LLMs as Problem Setters for Competitive Programming", "comment": "Project page: https://livecodebenchpro.com/projects/autocode/overview", "summary": "Writing competitive programming problems is exacting. Authors must: set\nconstraints, input distributions, and edge cases that rule out shortcuts;\ntarget specific algorithms (e.g., max-flow, dynamic programming, data\nstructures); and calibrate complexity beyond the reach of most competitors. We\nargue that this makes for an ideal test of general large language model\ncapabilities and study whether they can do this reliably. We introduce\nAutoCode, which uses multiple rounds of validation to yield competition-grade\nproblem statements and test cases. On held-out problems, AutoCode test suites\napproach 99% consistency with official judgments, a significant improvement\nover current state-of-the-art methods like HardTests, which achieve less than\n81%. Furthermore, starting with a random seed problem, AutoCode can create\nnovel variants with reference and brute-force solutions. By cross-verifying\nthese generated solutions against test cases, we can further filter out\nmalformed problems. Our system ensures high correctness, as verified by human\nexperts. AutoCode successfully produces novel problems judged by\nGrandmaster-level (top 0.3%) competitive programmers to be of contest quality.", "AI": {"tldr": "AutoCode\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u7ade\u8d5b\u7ea7\u7f16\u7a0b\u95ee\u9898\u53ca\u5176\u6d4b\u8bd5\u7528\u4f8b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u8f6e\u9a8c\u8bc1\u786e\u4fdd\u95ee\u9898\u8d28\u91cf\uff0c\u5728\u4fdd\u7559\u95ee\u9898\u4e0a\u6d4b\u8bd5\u7528\u4f8b\u4e0e\u5b98\u65b9\u8bc4\u5224\u4e00\u81f4\u6027\u63a5\u8fd199%\uff0c\u5e76\u80fd\u751f\u6210\u88ab\u9876\u7ea7\u7a0b\u5e8f\u5458\u8ba4\u53ef\u7684\u65b0\u9896\u95ee\u9898\u3002", "motivation": "\u7f16\u5199\u7ade\u8d5b\u7f16\u7a0b\u95ee\u9898\u9700\u8981\u7cbe\u786e\u8bbe\u7f6e\u7ea6\u675f\u6761\u4ef6\u3001\u8f93\u5165\u5206\u5e03\u548c\u8fb9\u754c\u60c5\u51b5\uff0c\u9488\u5bf9\u7279\u5b9a\u7b97\u6cd5\u5e76\u6821\u51c6\u590d\u6742\u5ea6\uff0c\u8fd9\u4e3a\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u60f3\u573a\u666f\u3002", "method": "\u4f7f\u7528\u591a\u8f6e\u9a8c\u8bc1\u65b9\u6cd5\u751f\u6210\u7ade\u8d5b\u7ea7\u95ee\u9898\u9648\u8ff0\u548c\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4ece\u968f\u673a\u79cd\u5b50\u95ee\u9898\u5f00\u59cb\u521b\u5efa\u65b0\u9896\u53d8\u4f53\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u9a8c\u8bc1\u53c2\u8003\u89e3\u548c\u66b4\u529b\u89e3\u6765\u8fc7\u6ee4\u6709\u7f3a\u9677\u7684\u95ee\u9898\u3002", "result": "\u5728\u4fdd\u7559\u95ee\u9898\u4e0a\uff0cAutoCode\u6d4b\u8bd5\u5957\u4ef6\u4e0e\u5b98\u65b9\u8bc4\u5224\u4e00\u81f4\u6027\u8fbe99%\uff0c\u663e\u8457\u4f18\u4e8eHardTests\u768481%\uff1b\u80fd\u751f\u6210\u88abGrandmaster\u7ea7\uff08\u524d0.3%\uff09\u7a0b\u5e8f\u5458\u8ba4\u53ef\u4e3a\u7ade\u8d5b\u8d28\u91cf\u7684\u65b0\u9896\u95ee\u9898\u3002", "conclusion": "AutoCode\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u53ef\u9760\u5730\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7ade\u8d5b\u7f16\u7a0b\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\uff0c\u4e3a\u7f16\u7a0b\u7ade\u8d5b\u95ee\u9898\u521b\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.13528", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.13528", "abs": "https://arxiv.org/abs/2510.13528", "authors": ["Lo\u00efs Ecoffet", "Veronika Rehn-Sonigo", "Jean-Fran\u00e7ois Couchot", "Catuscia Palamidessi"], "title": "Experiments \\& Analysis of Privacy-Preserving SQL Query Sanitization Systems", "comment": "10 pages, 5 figures, submitted to EDBT 26", "summary": "Analytical SQL queries are essential for extracting insights from relational\ndatabases but concurrently introduce significant privacy risks by potentially\nexposing sensitive information. To mitigate these risks, numerous query\nsanitization systems have been developed, employing diverse approaches that\ncreate a complex landscape for both researchers and practitioners. These\nsystems vary fundamentally in their design, including the underlying privacy\nmodel, such as k-anonymity or Differential Privacy; the protected privacy unit,\nwhether at the tuple- or user-level; and the software architecture, which can\nbe proxy-based or integrated. This paper provides a systematic classification\nof state-of-the-art SQL sanitization systems based on these qualitative\ncriteria and the scope of queries they support. Furthermore, we present a\nquantitative analysis of leading systems, empirically measuring the trade-offs\nbetween data utility, query execution overhead, and privacy guarantees across a\nrange of analytical queries. This work offers a structured overview and\nperformance assessment intended to clarify the capabilities and limitations of\ncurrent privacy-preserving database technologies.", "AI": {"tldr": "\u672c\u6587\u5bf9SQL\u67e5\u8be2\u51c0\u5316\u7cfb\u7edf\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u7c7b\u548c\u5b9a\u91cf\u5206\u6790\uff0c\u8bc4\u4f30\u4e86\u6570\u636e\u6548\u7528\u3001\u67e5\u8be2\u6267\u884c\u5f00\u9500\u548c\u9690\u79c1\u4fdd\u8bc1\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5206\u6790\u578bSQL\u67e5\u8be2\u5728\u63d0\u53d6\u6570\u636e\u5e93\u6d1e\u5bdf\u7684\u540c\u65f6\u5e26\u6765\u663e\u8457\u7684\u9690\u79c1\u98ce\u9669\uff0c\u73b0\u6709\u67e5\u8be2\u51c0\u5316\u7cfb\u7edf\u8bbe\u8ba1\u590d\u6742\u591a\u6837\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u548c\u6027\u80fd\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8e\u9690\u79c1\u6a21\u578b\u3001\u4fdd\u62a4\u5355\u5143\u548c\u8f6f\u4ef6\u67b6\u6784\u7b49\u5b9a\u6027\u6807\u51c6\u5bf9SQL\u51c0\u5316\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5bf9\u9886\u5148\u7cfb\u7edf\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff0c\u6d4b\u91cf\u6570\u636e\u6548\u7528\u3001\u67e5\u8be2\u6267\u884c\u5f00\u9500\u548c\u9690\u79c1\u4fdd\u8bc1\u7684\u6743\u8861\u3002", "result": "\u63d0\u4f9b\u4e86\u5f53\u524d\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u5e93\u6280\u672f\u7684\u7ed3\u6784\u5316\u6982\u89c8\u548c\u6027\u80fd\u8bc4\u4f30\uff0c\u9610\u660e\u4e86\u5404\u7cfb\u7edf\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6846\u67b6\uff0c\u5e2e\u52a9\u4ed6\u4eec\u7406\u89e3\u73b0\u6709SQL\u67e5\u8be2\u51c0\u5316\u7cfb\u7edf\u7684\u7279\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.13447", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.13447", "abs": "https://arxiv.org/abs/2510.13447", "authors": ["Julian Legler", "Sebastian Werner", "Maria C. Borges", "Stefan Tai"], "title": "Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices", "comment": "Accepted at ICSOC 2025", "summary": "Microservice architectures have become the dominant paradigm for cloud-native\nsystems, offering flexibility and scalability. However, this shift has also led\nto increased demand for cloud resources, contributing to higher energy\nconsumption and carbon emissions. While existing research has focused on\nmeasuring fine-grained energy usage of CPU and memory at the container level,\nor on system-wide assessments, these approaches often overlook the energy\nimpact of cross-container service interactions, especially those involving\nnetwork and storage for auxiliary services such as observability and system\nmonitoring. To address this gap, we introduce a service-level energy model that\ncaptures the distributed nature of microservice execution across containers.\nOur model is supported by an experimentation tool that accounts for energy\nconsumption not just in CPU and memory, but also in network and storage\ncomponents. We validate our approach through extensive experimentation with\ndiverse experiment configurations of auxiliary services for a popular\nopen-source cloud-native microservice application. Results show that omitting\nnetwork and storage can lead to an underestimation of auxiliary service energy\nuse by up to 63%, highlighting the need for more comprehensive energy\nassessments in the design of energy-efficient microservice architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u670d\u52a1\u7ea7\u80fd\u8017\u6a21\u578b\uff0c\u4e13\u95e8\u9488\u5bf9\u5fae\u670d\u52a1\u67b6\u6784\u4e2d\u8de8\u5bb9\u5668\u670d\u52a1\u4ea4\u4e92\u7684\u80fd\u8017\u5f71\u54cd\uff0c\u7279\u522b\u662f\u7f51\u7edc\u548c\u5b58\u50a8\u7ec4\u4ef6\u7684\u80fd\u8017\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u7814\u7a76\u53ea\u5173\u6ce8CPU\u548c\u5185\u5b58\u7684\u4e0d\u8db3\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u867d\u7136\u63d0\u4f9b\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u4e91\u8d44\u6e90\u9700\u6c42\uff0c\u5bfc\u81f4\u66f4\u9ad8\u7684\u80fd\u8017\u548c\u78b3\u6392\u653e\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bb9\u5668\u7ea7\u522b\u7684CPU\u548c\u5185\u5b58\u80fd\u8017\uff0c\u5ffd\u7565\u4e86\u8de8\u5bb9\u5668\u670d\u52a1\u4ea4\u4e92\uff08\u7279\u522b\u662f\u7f51\u7edc\u548c\u5b58\u50a8\uff09\u7684\u80fd\u8017\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u670d\u52a1\u7ea7\u80fd\u8017\u6a21\u578b\uff0c\u6355\u6349\u5fae\u670d\u52a1\u5728\u5bb9\u5668\u95f4\u5206\u5e03\u5f0f\u6267\u884c\u7684\u7279\u6027\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9e\u9a8c\u5de5\u5177\u6765\u6d4b\u91cfCPU\u3001\u5185\u5b58\u3001\u7f51\u7edc\u548c\u5b58\u50a8\u7ec4\u4ef6\u7684\u80fd\u8017\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u5ffd\u7565\u7f51\u7edc\u548c\u5b58\u50a8\u4f1a\u5bfc\u81f4\u8f85\u52a9\u670d\u52a1\u80fd\u8017\u4f4e\u4f30\u9ad8\u8fbe63%\uff0c\u7a81\u663e\u4e86\u5728\u80fd\u6548\u5fae\u670d\u52a1\u67b6\u6784\u8bbe\u8ba1\u4e2d\u9700\u8981\u66f4\u5168\u9762\u7684\u80fd\u8017\u8bc4\u4f30\u3002", "conclusion": "\u5fae\u670d\u52a1\u67b6\u6784\u7684\u80fd\u8017\u8bc4\u4f30\u5fc5\u987b\u5305\u542b\u7f51\u7edc\u548c\u5b58\u50a8\u7ec4\u4ef6\uff0c\u5426\u5219\u4f1a\u4e25\u91cd\u4f4e\u4f30\u5b9e\u9645\u80fd\u8017\uff0c\u8fd9\u5bf9\u8bbe\u8ba1\u80fd\u6548\u4f18\u5316\u7684\u4e91\u539f\u751f\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.12948", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.12948", "abs": "https://arxiv.org/abs/2510.12948", "authors": ["Minh Nguyen"], "title": "SpareCodeSearch: Searching for Code Context When You Have No Spare GPU", "comment": "4 pages, 3 figures, 4 tables. Accepted to Context Collection Workshop\n  co-located with ASE'25", "summary": "Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language\nModels (CLMs) by including another module for retrieving relevant context to\nconstruct the input prompt. However, these retrieval modules commonly use\nsemantic search, requiring substantial computational resources for training and\nhosting these embedded models, making them infeasible to integrate into\nlightweight applications such as in-IDE AI-based code completion. In this\nsolution paper, we prove that using keyword-search is sufficient to retrieve\nrelevant and useful code context inside large codebases, without the need for\nextensive GPU resources. The usefulness of code contexts found by our solution\nis demonstrated through their completion results on the Code Context\nCompetition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and\nPython tracks, respectively.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u4f7f\u7528\u5173\u952e\u8bcd\u641c\u7d22\u8db3\u4ee5\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u4e0a\u4e0b\u6587\uff0c\u65e0\u9700GPU\u8d44\u6e90\uff0c\u5728\u4ee3\u7801\u4e0a\u4e0b\u6587\u7ade\u8d5b\u4e2d\u8fbe\u52300.748\u548c0.725\u7684chRF\u5206\u6570\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u4f7f\u7528\u8bed\u4e49\u641c\u7d22\uff0c\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u8bad\u7ec3\u548c\u6258\u7ba1\u5d4c\u5165\u6a21\u578b\uff0c\u96be\u4ee5\u96c6\u6210\u5230\u8f7b\u91cf\u7ea7\u5e94\u7528\u5982IDE\u4ee3\u7801\u8865\u5168\u4e2d\u3002", "method": "\u4f7f\u7528\u5173\u952e\u8bcd\u641c\u7d22\u66ff\u4ee3\u8bed\u4e49\u641c\u7d22\u6765\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u4e0a\u4e0b\u6587\uff0c\u907f\u514d\u5bf9GPU\u8d44\u6e90\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u4ee3\u7801\u4e0a\u4e0b\u6587\u7ade\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKotlin\u548cPython\u8f68\u9053\u5206\u522b\u8fbe\u52300.748\u548c0.725\u7684chRF\u5206\u6570\uff0c\u8bc1\u660e\u5173\u952e\u8bcd\u641c\u7d22\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5173\u952e\u8bcd\u641c\u7d22\u8db3\u4ee5\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u68c0\u7d22\u76f8\u5173\u6709\u7528\u7684\u4ee3\u7801\u4e0a\u4e0b\u6587\uff0c\u65e0\u9700\u6602\u8d35\u7684GPU\u8d44\u6e90\uff0c\u9002\u5408\u8f7b\u91cf\u7ea7\u5e94\u7528\u96c6\u6210\u3002"}}
{"id": "2510.13662", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.13662", "abs": "https://arxiv.org/abs/2510.13662", "authors": ["Mahdi Esmailoghli", "Matthias Weidlich"], "title": "The Past Still Matters: A Temporally-Valid Data Discovery System", "comment": null, "summary": "Over the past decade, the proliferation of public and enterprise data lakes\nhas fueled intensive research into data discovery, aiming to identify the most\nrelevant data from vast and complex corpora to support diverse user tasks.\nSignificant progress has been made through the development of innovative index\nstructures, similarity measures, and querying infrastructures. Despite these\nadvances, a critical aspect remains overlooked: relevance is time-varying.\nExisting discovery methods largely ignore this temporal dimension, especially\nwhen explicit date/time metadata is missing. To fill this gap, we outline a\nvision for a data discovery system that incorporates the temporal dimension of\ndata. Specifically, we define the problem of temporally-valid data discovery\nand argue that addressing it requires techniques for version discovery,\ntemporal lineage inference, change log synthesis, and time-aware data\ndiscovery. We then present a system architecture to deliver these techniques,\nbefore we summarize research challenges and opportunities. As such, we lay the\nfoundation for a new class of data discovery systems, transforming how we\ninteract with evolving data lakes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u65f6\u95f4\u611f\u77e5\u6570\u636e\u53d1\u73b0\u7cfb\u7edf\u7684\u613f\u666f\uff0c\u5f3a\u8c03\u6570\u636e\u76f8\u5173\u6027\u968f\u65f6\u95f4\u53d8\u5316\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5b9a\u4e49\u4e86\u65f6\u95f4\u6709\u6548\u6570\u636e\u53d1\u73b0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u53d1\u73b0\u65b9\u6cd5\u5ffd\u7565\u4e86\u6570\u636e\u76f8\u5173\u6027\u7684\u65f6\u95f4\u53d8\u5316\u7ef4\u5ea6\uff0c\u7279\u522b\u662f\u5728\u7f3a\u4e4f\u660e\u786e\u65f6\u95f4\u5143\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u9650\u5236\u4e86\u6570\u636e\u53d1\u73b0\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u5305\u542b\u7248\u672c\u53d1\u73b0\u3001\u65f6\u95f4\u8c31\u7cfb\u63a8\u65ad\u3001\u53d8\u66f4\u65e5\u5fd7\u5408\u6210\u548c\u65f6\u95f4\u611f\u77e5\u6570\u636e\u53d1\u73b0\u6280\u672f\u7684\u7cfb\u7edf\u67b6\u6784\u3002", "result": "\u4e3a\u65b0\u4e00\u4ee3\u6570\u636e\u53d1\u73b0\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06\u6539\u53d8\u6211\u4eec\u4e0e\u6f14\u8fdb\u6570\u636e\u6e56\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "conclusion": "\u65f6\u95f4\u7ef4\u5ea6\u662f\u6570\u636e\u53d1\u73b0\u7684\u5173\u952e\u56e0\u7d20\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u6280\u672f\u548c\u7cfb\u7edf\u6765\u5e94\u5bf9\u6570\u636e\u76f8\u5173\u6027\u7684\u65f6\u95f4\u53d8\u5316\u7279\u6027\u3002"}}
{"id": "2510.13668", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13668", "abs": "https://arxiv.org/abs/2510.13668", "authors": ["Zhibin Wang", "Zetao Hong", "Xue Li", "Zibo Wang", "Shipeng Li", "Qingkai Meng", "Qing Wang", "Chengying Huan", "Rong Gu", "Sheng Zhong", "Chen Tian"], "title": "Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference", "comment": null, "summary": "Large Language Model (LLM) inference has emerged as a fundamental paradigm.\nIn real-world scenarios, variations in output length cause severe workload\nimbalance in the decode phase, particularly for long-output reasoning tasks.\nExisting systems, such as PD disaggregation architectures, rely on static\nprefill-to-decode scheduling, which often results in SLO violations and OOM\nfailures under evolving decode workloads.\n  In this paper, we propose ARES, an adaptive decoding rescheduling system\npowered by length prediction to anticipate future workloads. Our core\ncontributions include: (1) A lightweight and continuous LLM-native prediction\nmethod that leverages LLM hidden state to model remaining generation length\nwith high precision (reducing MAE by 49.42%) and low overhead (cutting\npredictor parameters by 93.28%); (2) A rescheduling solution in decode phase\nwith : A dynamic balancing mechanism that integrates current and predicted\nworkloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher\ngoodput.", "AI": {"tldr": "ARES\u662f\u4e00\u4e2a\u57fa\u4e8e\u957f\u5ea6\u9884\u6d4b\u7684\u81ea\u9002\u5e94\u89e3\u7801\u91cd\u8c03\u5ea6\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u539f\u751f\u9884\u6d4b\u65b9\u6cd5\u51c6\u786e\u9884\u6d4b\u5269\u4f59\u751f\u6210\u957f\u5ea6\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u5b9e\u73b0\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u8f93\u51fa\u957f\u5ea6\u53d8\u5316\u5bfc\u81f4\u89e3\u7801\u9636\u6bb5\u4e25\u91cd\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u957f\u8f93\u51fa\u63a8\u7406\u4efb\u52a1\u4e2d\u3002\u73b0\u6709\u7cfb\u7edf\u4f7f\u7528\u9759\u6001\u8c03\u5ea6\u7b56\u7565\uff0c\u5728\u53d8\u5316\u7684\u89e3\u7801\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7ecf\u5e38\u5bfc\u81f4SLO\u8fdd\u89c4\u548cOOM\u6545\u969c\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u8fde\u7eed\u7684LLM\u539f\u751f\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u5efa\u6a21\u5269\u4f59\u751f\u6210\u957f\u5ea6\uff1b\u5728\u89e3\u7801\u9636\u6bb5\u5b9e\u65bd\u91cd\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u96c6\u6210\u5f53\u524d\u548c\u9884\u6d4b\u5de5\u4f5c\u8d1f\u8f7d\u7684\u52a8\u6001\u5e73\u8861\u673a\u5236\u3002", "result": "\u9884\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff08MAE\u964d\u4f4e49.42%\uff09\uff0c\u5f00\u9500\u5927\u5e45\u51cf\u5c11\uff08\u9884\u6d4b\u5668\u53c2\u6570\u524a\u51cf93.28%\uff09\uff1bP99 TPOT\u964d\u4f4e74.77%\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe2.24\u500d\u3002", "conclusion": "ARES\u7cfb\u7edf\u901a\u8fc7\u81ea\u9002\u5e94\u89e3\u7801\u91cd\u8c03\u5ea6\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.13078", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13078", "abs": "https://arxiv.org/abs/2510.13078", "authors": ["Tri Minh-Triet Pham", "Diego Elias Costa", "Weiyi Shang", "Jinqiu Yang"], "title": "ADPerf: Investigating and Testing Performance in Autonomous Driving Systems", "comment": "13 pages, accepted by ASE 2025", "summary": "Obstacle detection is crucial to the operation of autonomous driving systems,\nwhich rely on multiple sensors, such as cameras and LiDARs, combined with code\nlogic and deep learning models to detect obstacles for time-sensitive\ndecisions. Consequently, obstacle detection latency is critical to the safety\nand effectiveness of autonomous driving systems. However, the latency of the\nobstacle detection module and its resilience to various changes in the LiDAR\npoint cloud data are not yet fully understood. In this work, we present the\nfirst comprehensive investigation on measuring and modeling the performance of\nthe obstacle detection modules in two industry-grade autonomous driving\nsystems, i.e., Apollo and Autoware. Learning from this investigation, we\nintroduce ADPerf, a tool that aims to generate realistic point cloud data test\ncases that can expose increased detection latency. Increasing latency decreases\nthe availability of the detected obstacles and stresses the capabilities of\nsubsequent modules in autonomous driving systems, i.e., the modules may be\nnegatively impacted by the increased latency in obstacle detection.\n  We applied ADPerf to stress-test the performance of widely used 3D obstacle\ndetection modules in autonomous driving systems, as well as the propagation of\nsuch tests on trajectory prediction modules. Our evaluation highlights the need\nto conduct performance testing of obstacle detection components, especially 3D\nobstacle detection, as they can be a major bottleneck to increased latency of\nthe autonomous driving system. Such an adverse outcome will also further\npropagate to other modules, reducing the overall reliability of autonomous\ndriving systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ADPerf\u5de5\u5177\uff0c\u7528\u4e8e\u6d4b\u91cf\u548c\u5efa\u6a21\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u969c\u788d\u7269\u68c0\u6d4b\u6a21\u5757\u7684\u6027\u80fd\uff0c\u751f\u6210\u80fd\u66b4\u9732\u68c0\u6d4b\u5ef6\u8fdf\u589e\u52a0\u7684\u903c\u771f\u70b9\u4e91\u6570\u636e\u6d4b\u8bd5\u7528\u4f8b\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u591a\u4f20\u611f\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u969c\u788d\u7269\u68c0\u6d4b\uff0c\u4f46\u969c\u788d\u7269\u68c0\u6d4b\u6a21\u5757\u7684\u5ef6\u8fdf\u53ca\u5176\u5bf9\u70b9\u4e91\u6570\u636e\u53d8\u5316\u7684\u5f39\u6027\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u9996\u5148\u5bf9Apollo\u548cAutoware\u4e24\u4e2a\u5de5\u4e1a\u7ea7\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fdb\u884c\u969c\u788d\u7269\u68c0\u6d4b\u6a21\u5757\u6027\u80fd\u7684\u5168\u9762\u8c03\u67e5\uff0c\u7136\u540e\u5f00\u53d1ADPerf\u5de5\u5177\u751f\u6210\u80fd\u589e\u52a0\u68c0\u6d4b\u5ef6\u8fdf\u7684\u903c\u771f\u70b9\u4e91\u6d4b\u8bd5\u6570\u636e\u3002", "result": "ADPerf\u6210\u529f\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u76843D\u969c\u788d\u7269\u68c0\u6d4b\u6a21\u5757\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u5ef6\u8fdf\u589e\u52a0\u5bf9\u8f68\u8ff9\u9884\u6d4b\u6a21\u5757\u7684\u4f20\u64ad\u5f71\u54cd\uff0c\u53d1\u73b03D\u969c\u788d\u7269\u68c0\u6d4b\u53ef\u80fd\u6210\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5ef6\u8fdf\u589e\u52a0\u7684\u4e3b\u8981\u74f6\u9888\u3002", "conclusion": "\u9700\u8981\u5bf9\u969c\u788d\u7269\u68c0\u6d4b\u7ec4\u4ef6\u7279\u522b\u662f3D\u969c\u788d\u7269\u68c0\u6d4b\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\uff0c\u56e0\u4e3a\u5176\u5ef6\u8fdf\u589e\u52a0\u4f1a\u6210\u4e3a\u7cfb\u7edf\u74f6\u9888\uff0c\u5e76\u8fdb\u4e00\u6b65\u4f20\u64ad\u5230\u5176\u4ed6\u6a21\u5757\uff0c\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6574\u4f53\u53ef\u9760\u6027\u3002"}}
{"id": "2510.13724", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13724", "abs": "https://arxiv.org/abs/2510.13724", "authors": ["Aditya Tanikanti", "Benoit C\u00f4t\u00e9", "Yanfei Guo", "Le Chen", "Nickolaus Saint", "Ryan Chard", "Ken Raffenetti", "Rajeev Thakur", "Thomas Uram", "Ian Foster", "Michael E. Papka", "Venkatram Vishwanath"], "title": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access", "comment": null, "summary": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a\nframework enabling Inference-as-a-Service across distributed High-Performance\nComputing (HPC) clusters. FIRST provides cloud-like access to diverse AI\nmodels, like Large Language Models (LLMs), on existing HPC infrastructure.\nLeveraging Globus Auth and Globus Compute, the system allows researchers to run\nparallel inference workloads via an OpenAI-compliant API on private, secure\nenvironments. This cluster-agnostic API allows requests to be distributed\nacross federated clusters, targeting numerous hosted models. FIRST supports\nmultiple inference backends (e.g., vLLM), auto-scales resources, maintains\n\"hot\" nodes for low-latency execution, and offers both high-throughput batch\nand interactive modes. The framework addresses the growing demand for private,\nsecure, and scalable AI inference in scientific workflows, allowing researchers\nto generate billions of tokens daily on-premises without relying on commercial\ncloud infrastructure.", "AI": {"tldr": "FIRST\u662f\u4e00\u4e2a\u8054\u90a6\u63a8\u7406\u8d44\u6e90\u8c03\u5ea6\u5de5\u5177\u5305\uff0c\u4e3a\u5206\u5e03\u5f0f\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u63d0\u4f9b\u7c7b\u4f3c\u4e91\u7684AI\u6a21\u578b\u63a8\u7406\u670d\u52a1\uff0c\u652f\u6301\u591a\u79cd\u63a8\u7406\u540e\u7aef\u548c\u81ea\u52a8\u6269\u7f29\u5bb9\u3002", "motivation": "\u89e3\u51b3\u79d1\u5b66\u5de5\u4f5c\u6d41\u4e2d\u5bf9\u79c1\u6709\u3001\u5b89\u5168\u3001\u53ef\u6269\u5c55AI\u63a8\u7406\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u8ba9\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5728\u672c\u5730\u57fa\u7840\u8bbe\u65bd\u4e0a\u751f\u6210\u6570\u5341\u4ebftoken\uff0c\u800c\u4e0d\u4f9d\u8d56\u5546\u4e1a\u4e91\u670d\u52a1\u3002", "method": "\u5229\u7528Globus Auth\u548cGlobus Compute\uff0c\u901a\u8fc7OpenAI\u517c\u5bb9API\u5728\u79c1\u6709\u5b89\u5168\u73af\u5883\u4e2d\u8fd0\u884c\u5e76\u884c\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u652f\u6301\u591a\u79cd\u63a8\u7406\u540e\u7aef\uff08\u5982vLLM\uff09\uff0c\u81ea\u52a8\u6269\u7f29\u8d44\u6e90\uff0c\u7ef4\u62a4\"\u70ed\"\u8282\u70b9\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u6267\u884c\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u8054\u90a6\u96c6\u7fa4\u4e0a\u5206\u53d1\u8bf7\u6c42\u7684\u80fd\u529b\uff0c\u652f\u6301\u9ad8\u541e\u5410\u91cf\u6279\u5904\u7406\u548c\u4ea4\u4e92\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u73b0\u6709HPC\u57fa\u7840\u8bbe\u65bd\u4e0a\u63d0\u4f9b\u4e91\u5f0fAI\u6a21\u578b\u8bbf\u95ee\u3002", "conclusion": "FIRST\u6846\u67b6\u6210\u529f\u6ee1\u8db3\u4e86\u79d1\u5b66\u8ba1\u7b97\u4e2d\u5bf9\u79c1\u6709\u3001\u5b89\u5168\u3001\u53ef\u6269\u5c55AI\u63a8\u7406\u7684\u9700\u6c42\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5728\u672c\u5730\u73af\u5883\u4e2d\u9ad8\u6548\u8fd0\u884c\u5927\u89c4\u6a21AI\u63a8\u7406\u7684\u80fd\u529b\u3002"}}
{"id": "2510.13106", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.13106", "abs": "https://arxiv.org/abs/2510.13106", "authors": ["Ruoyu Sun", "Da Song", "Jiayang Song", "Yuheng Huang", "Lei Ma"], "title": "TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models", "comment": "4 pages, 2 figures, To appear in ASE 2025 Demo Track", "summary": "As Large Language Models (LLMs) continue to revolutionize Natural Language\nProcessing (NLP) applications, critical concerns about their trustworthiness\npersist, particularly in safety and robustness. To address these challenges, we\nintroduce TRUSTVIS, an automated evaluation framework that provides a\ncomprehensive assessment of LLM trustworthiness. A key feature of our framework\nis its interactive user interface, designed to offer intuitive visualizations\nof trustworthiness metrics. By integrating well-known perturbation methods like\nAutoDAN and employing majority voting across various evaluation methods,\nTRUSTVIS not only provides reliable results but also makes complex evaluation\nprocesses accessible to users. Preliminary case studies on models like\nVicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our\nframework in identifying safety and robustness vulnerabilities, while the\ninteractive interface allows users to explore results in detail, empowering\ntargeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g", "AI": {"tldr": "TRUSTVIS\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u754c\u9762\u53ef\u89c6\u5316LLM\u53ef\u4fe1\u5ea6\u6307\u6807\uff0c\u7ed3\u5408AutoDAN\u7b49\u6270\u52a8\u65b9\u6cd5\u548c\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u6709\u6548\u8bc6\u522b\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u6f0f\u6d1e\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728NLP\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u7b49\u53ef\u4fe1\u5ea6\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u786e\u4fdd\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u96c6\u6210AutoDAN\u7b49\u77e5\u540d\u6270\u52a8\u65b9\u6cd5\uff0c\u91c7\u7528\u591a\u6570\u6295\u7968\u673a\u5236\u6574\u5408\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f00\u53d1\u4ea4\u4e92\u5f0f\u7528\u6237\u754c\u9762\u76f4\u89c2\u5c55\u793a\u53ef\u4fe1\u5ea6\u6307\u6807\u3002", "result": "\u5728Vicuna-7b\u3001Llama2-7b\u548cGPT-3.5\u7b49\u6a21\u578b\u4e0a\u7684\u521d\u6b65\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8bc6\u522b\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u6f0f\u6d1e\u3002", "conclusion": "TRUSTVIS\u6846\u67b6\u4e0d\u4ec5\u63d0\u4f9b\u53ef\u9760\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u7ed3\u679c\uff0c\u8fd8\u901a\u8fc7\u4ea4\u4e92\u5f0f\u754c\u9762\u4f7f\u590d\u6742\u8bc4\u4f30\u8fc7\u7a0b\u5bf9\u7528\u6237\u66f4\u52a0\u53cb\u597d\uff0c\u652f\u6301\u9488\u5bf9\u6027\u7684\u6a21\u578b\u6539\u8fdb\u3002"}}
{"id": "2510.13755", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.13755", "abs": "https://arxiv.org/abs/2510.13755", "authors": ["Timoth\u00e9 Albouy", "Antonio Fern\u00e1ndez Anta", "Chryssis Georgiou", "Nicolas Nicolaou", "Junlang Wang"], "title": "Tight Conditions for Binary-Output Tasks under Crashes", "comment": null, "summary": "This paper explores necessary and sufficient system conditions to solve\ndistributed tasks with binary outputs (\\textit{i.e.}, tasks with output values\nin $\\{0,1\\}$). We focus on the distinct output sets of values a task can\nproduce (intentionally disregarding validity and value multiplicity),\nconsidering that some processes may output no value. In a distributed system\nwith $n$ processes, of which up to $t \\leq n$ can crash, we provide a complete\ncharacterization of the tight conditions on $n$ and $t$ under which every class\nof tasks with binary outputs is solvable, for both synchronous and asynchronous\nsystems. This output-set approach yields highly general results: it unifies\nmultiple distributed computing problems, such as binary consensus and symmetry\nbreaking, and it produces impossibility proofs that hold for stronger task\nformulations, including those that consider validity, account for value\nmultiplicity, or move beyond binary outputs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u89e3\u51b3\u5177\u6709\u4e8c\u8fdb\u5236\u8f93\u51fa\u7684\u5206\u5e03\u5f0f\u4efb\u52a1\u7684\u5fc5\u8981\u548c\u5145\u5206\u7cfb\u7edf\u6761\u4ef6\uff0c\u4e3a\u540c\u6b65\u548c\u5f02\u6b65\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684n\u548ct\u6761\u4ef6\u7279\u5f81\u5316\uff0c\u7edf\u4e00\u4e86\u4e8c\u8fdb\u5236\u5171\u8bc6\u548c\u5bf9\u79f0\u6027\u7834\u574f\u7b49\u591a\u4e2a\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u89e3\u51b3\u4e8c\u8fdb\u5236\u8f93\u51fa\u4efb\u52a1\u7684\u7cfb\u7edf\u6761\u4ef6\uff0c\u91cd\u70b9\u5173\u6ce8\u4efb\u52a1\u53ef\u4ee5\u4ea7\u751f\u7684\u4e0d\u540c\u8f93\u51fa\u503c\u96c6\u5408\uff0c\u5ffd\u7565\u6709\u6548\u6027\u548c\u503c\u591a\u91cd\u6027\uff0c\u8003\u8651\u67d0\u4e9b\u8fdb\u7a0b\u53ef\u80fd\u4e0d\u8f93\u51fa\u503c\u7684\u60c5\u51b5\u3002", "method": "\u91c7\u7528\u8f93\u51fa\u96c6\u65b9\u6cd5\uff0c\u5206\u6790\u5728n\u4e2a\u8fdb\u7a0b\u4e2d\u6700\u591at\u4e2a\u53ef\u80fd\u5d29\u6e83\u7684\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u4e8c\u8fdb\u5236\u8f93\u51fa\u4efb\u52a1\u7684\u53ef\u89e3\u6027\u6761\u4ef6\uff0c\u6db5\u76d6\u540c\u6b65\u548c\u5f02\u6b65\u7cfb\u7edf\u3002", "result": "\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684n\u548ct\u7d27\u6761\u4ef6\u7279\u5f81\u5316\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u5ea6\u901a\u7528\u6027\uff0c\u80fd\u591f\u7edf\u4e00\u591a\u4e2a\u5206\u5e03\u5f0f\u8ba1\u7b97\u95ee\u9898\uff0c\u5e76\u4e3a\u66f4\u5f3a\u7684\u4efb\u52a1\u8868\u8ff0\u63d0\u4f9b\u4e0d\u53ef\u80fd\u6027\u8bc1\u660e\u3002", "conclusion": "\u8f93\u51fa\u96c6\u65b9\u6cd5\u4e3a\u4e8c\u8fdb\u5236\u8f93\u51fa\u5206\u5e03\u5f0f\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5176\u4e0d\u53ef\u80fd\u6027\u8bc1\u660e\u9002\u7528\u4e8e\u8003\u8651\u6709\u6548\u6027\u3001\u503c\u591a\u91cd\u6027\u6216\u8d85\u8d8a\u4e8c\u8fdb\u5236\u8f93\u51fa\u7684\u66f4\u5f3a\u4efb\u52a1\u8868\u8ff0\u3002"}}
{"id": "2510.13128", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13128", "abs": "https://arxiv.org/abs/2510.13128", "authors": ["Yujie Liu", "Mingxuan Zhu", "Shengyu Cheng", "Dan Hao"], "title": "Isolating Compiler Bugs through Compilation Steps Analysis", "comment": null, "summary": "Compilers are essential to software systems, and their bugs can propagate to\ndependent software. Ensuring compiler correctness is critical. However,\nisolating compiler bugs remains challenging due to the internal complexity of\ncompiler execution. Existing techniques primarily mutate compilation inputs to\ngenerate passing and failing tests, but often lack causal analysis of internal\nsteps, limiting their effectiveness.\n  To address this limitation, we propose CompSCAN, a novel compiler bug\nisolation technique that applies analysis over the sequence of compilation\nsteps. CompSCAN follows a three-stage process: (1) extracting the array of\ncompilation steps that leads to the original failure, (2) identifying\nbug-causing steps and collecting corresponding compiler code elements, and (3)\ncalculating suspicious scores for each code element and outputting a suspicious\nranking list as the bug isolation result.\n  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that\nCompSCAN outperforms state-of-the-art techniques in both effectiveness and\nefficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the\nTop-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two\nstate-of-the-art compiler bug isolation techniques, CompSCAN achieves relative\nimprovements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /\n49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs\nfaster on average per bug than both baselines.", "AI": {"tldr": "CompSCAN\u662f\u4e00\u79cd\u65b0\u7684\u7f16\u8bd1\u5668bug\u9694\u79bb\u6280\u672f\uff0c\u901a\u8fc7\u5206\u6790\u7f16\u8bd1\u6b65\u9aa4\u5e8f\u5217\u6765\u8bc6\u522bbug\u539f\u56e0\uff0c\u5728\u771f\u5b9eLLVM\u548cGCC bug\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u7f16\u8bd1\u5668bug\u4f1a\u4f20\u64ad\u5230\u4f9d\u8d56\u8f6f\u4ef6\u4e2d\uff0c\u4f46\u73b0\u6709\u6280\u672f\u7f3a\u4e4f\u5bf9\u5185\u90e8\u6b65\u9aa4\u7684\u56e0\u679c\u5206\u6790\uff0c\u9650\u5236\u4e86bug\u9694\u79bb\u6548\u679c\u3002", "method": "\u4e09\u9636\u6bb5\u8fc7\u7a0b\uff1a\u63d0\u53d6\u5bfc\u81f4\u5931\u8d25\u7684\u7f16\u8bd1\u6b65\u9aa4\u5e8f\u5217\uff0c\u8bc6\u522bbug\u76f8\u5173\u6b65\u9aa4\u5e76\u6536\u96c6\u5bf9\u5e94\u4ee3\u7801\u5143\u7d20\uff0c\u8ba1\u7b97\u53ef\u7591\u5ea6\u5206\u6570\u5e76\u8f93\u51fa\u6392\u540d\u5217\u8868\u3002", "result": "\u5728185\u4e2a\u771f\u5b9ebug\u4e0a\u6d4b\u8bd5\uff0cCompSCAN\u5728Top-1/3/5/10\u6392\u540d\u4e2d\u5206\u522b\u6210\u529f\u9694\u79bb50\u300185\u3001100\u3001123\u4e2abug\uff0c\u76f8\u6bd4ETEM\u548cODFL\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4e14\u8fd0\u884c\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "CompSCAN\u901a\u8fc7\u5206\u6790\u7f16\u8bd1\u6b65\u9aa4\u5e8f\u5217\u6709\u6548\u89e3\u51b3\u4e86\u7f16\u8bd1\u5668bug\u9694\u79bb\u95ee\u9898\uff0c\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2510.13176", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13176", "abs": "https://arxiv.org/abs/2510.13176", "authors": ["Haolin Pan", "Chao Zha", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning", "comment": null, "summary": "Compiler pass selection and phase ordering present a significant challenge in\nachieving optimal program performance, particularly for objectives like code\nsize reduction. Standard compiler heuristics offer general applicability but\noften yield suboptimal, program-specific results due to their one-size-fits-all\nnature. While iterative compilation can find tailored solutions, its\nprohibitive search cost limits practical use. Machine learning approaches\npromise faster inference but frequently struggle with generalization to unseen\nprograms. This paper introduces GRACE, a novel framework for compiler\nauto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE\neffectively curtails the search space by leveraging pass synergies and a\nweighted scoring method to generate initial high-quality candidate sequences\nand a pass pool. It then employs contrastive learning, using pass\nsequence-based data augmentation, to create program embeddings that facilitate\nsimilarity-aware clustering. Evolutionary search within these clusters yields a\ncoreset of $k$ specialized pass sequences designed for robust generalization to\nunseen programs. At test time, GRACE efficiently selects the best coreset\nsequence and refines it using lightweight techniques. Experimental results on\nseven diverse datasets show that GRACE reduces LLVM IR instruction count by an\naverage of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,\nwhile incurring an average tuning time of less than 1s per program,\ndemonstrating its state-of-the-art performance and practical effectiveness.", "AI": {"tldr": "GRACE\u662f\u4e00\u4e2a\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528pass\u534f\u540c\u6548\u5e94\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\uff0c\u5728LLVM IR\u6307\u4ee4\u6570\u4f18\u5316\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8c03\u4f18\u65f6\u95f4\u3002", "motivation": "\u6807\u51c6\u7f16\u8bd1\u5668\u542f\u53d1\u5f0f\u65b9\u6cd5\u901a\u5e38\u4ea7\u751f\u6b21\u4f18\u7ed3\u679c\uff0c\u8fed\u4ee3\u7f16\u8bd1\u641c\u7d22\u6210\u672c\u8fc7\u9ad8\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u627e\u5230\u4f18\u5316\u89e3\u53c8\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u65b9\u6cd5\u3002", "method": "\u5229\u7528pass\u534f\u540c\u6548\u5e94\u548c\u52a0\u6743\u8bc4\u5206\u751f\u6210\u9ad8\u8d28\u91cf\u5019\u9009\u5e8f\u5217\u548cpass\u6c60\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u521b\u5efa\u7a0b\u5e8f\u5d4c\u5165\u8fdb\u884c\u76f8\u4f3c\u6027\u805a\u7c7b\uff0c\u5728\u805a\u7c7b\u5185\u8fdb\u884c\u8fdb\u5316\u641c\u7d22\u5f97\u5230\u6838\u5fc3\u96c6\u5e8f\u5217\uff0c\u6d4b\u8bd5\u65f6\u9009\u62e9\u6700\u4f73\u5e8f\u5217\u5e76\u7528\u8f7b\u91cf\u7ea7\u6280\u672f\u4f18\u5316\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cGRACE\u76f8\u6bd4opt -Oz\u5e73\u5747\u51cf\u5c11LLVM IR\u6307\u4ee4\u657010.09%(LLVM 10.0.0)\u548c10.19%(LLVM 18.1.6)\uff0c\u5e73\u5747\u8c03\u4f18\u65f6\u95f4\u5c0f\u4e8e1\u79d2/\u7a0b\u5e8f\u3002", "conclusion": "GRACE\u5728\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u5b9e\u9645\u6709\u6548\u6027\uff0c\u5728\u663e\u8457\u4f18\u5316\u4ee3\u7801\u5927\u5c0f\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u8c03\u4f18\u901f\u5ea6\u3002"}}
{"id": "2510.13184", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13184", "abs": "https://arxiv.org/abs/2510.13184", "authors": ["Haolin Pan", "Jinyuan Dong", "Mingjie Xing", "Yanjun Wu"], "title": "Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines", "comment": null, "summary": "Compiler optimization relies on sequences of passes to improve program\nperformance. Selecting and ordering these passes automatically, known as\ncompiler auto-tuning, is challenging due to the large and complex search space.\nExisting approaches generally assume a linear sequence of passes, a model\ncompatible with legacy compilers but fundamentally misaligned with the\nhierarchical design of the LLVM New Pass Manager. This misalignment prevents\nthem from guaranteeing the generation of syntactically valid optimization\npipelines. In this work, we present a new auto-tuning framework built from the\nground up for the New Pass Manager. We introduce a formal grammar to define the\nspace of valid nested pipelines and a forest-based data structure for their\nnative representation. Upon this foundation, we develop a structure-aware\nGenetic Algorithm whose operators manipulate these forests directly, ensuring\nthat all candidate solutions are valid by construction. The framework first\nmines synergistic pass relationships to guide the search. An optional\nrefinement stage further explores subtle performance variations arising from\ndifferent valid structural arrangements.\n  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The\ndiscovered pipelines achieve an average of 13.62% additional instruction count\nreduction compared to the standard opt -Oz optimization level, showing that our\nframework is capable of navigating this complex, constrained search space to\nidentify valid and effective pass pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u4e3aLLVM\u65b0Pass Manager\u8bbe\u8ba1\u7684\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u8bed\u6cd5\u5b9a\u4e49\u6709\u6548\u5d4c\u5957\u6d41\u6c34\u7ebf\u7a7a\u95f4\uff0c\u4f7f\u7528\u57fa\u4e8e\u68ee\u6797\u7684\u6570\u636e\u7ed3\u6784\u8868\u793a\uff0c\u5e76\u5f00\u53d1\u7ed3\u6784\u611f\u77e5\u9057\u4f20\u7b97\u6cd5\u786e\u4fdd\u751f\u6210\u6709\u6548\u4f18\u5316\u6d41\u6c34\u7ebf\u3002", "motivation": "\u73b0\u6709\u7f16\u8bd1\u5668\u81ea\u52a8\u8c03\u4f18\u65b9\u6cd5\u5047\u8bbe\u7ebf\u6027pass\u5e8f\u5217\uff0c\u8fd9\u4e0eLLVM\u65b0Pass Manager\u7684\u5206\u5c42\u8bbe\u8ba1\u4e0d\u5339\u914d\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u751f\u6210\u8bed\u6cd5\u6709\u6548\u7684\u4f18\u5316\u6d41\u6c34\u7ebf\u3002", "method": "\u5f15\u5165\u5f62\u5f0f\u5316\u8bed\u6cd5\u5b9a\u4e49\u6709\u6548\u5d4c\u5957\u6d41\u6c34\u7ebf\u7a7a\u95f4\uff0c\u4f7f\u7528\u68ee\u6797\u6570\u636e\u7ed3\u6784\u539f\u751f\u8868\u793a\uff0c\u5f00\u53d1\u7ed3\u6784\u611f\u77e5\u9057\u4f20\u7b97\u6cd5\u76f4\u63a5\u64cd\u4f5c\u8fd9\u4e9b\u68ee\u6797\uff0c\u786e\u4fdd\u5019\u9009\u89e3\u6784\u9020\u6709\u6548\u3002\u6846\u67b6\u9996\u5148\u6316\u6398\u534f\u540cpass\u5173\u7cfb\u6307\u5bfc\u641c\u7d22\uff0c\u53ef\u9009\u7ec6\u5316\u9636\u6bb5\u63a2\u7d22\u4e0d\u540c\u6709\u6548\u7ed3\u6784\u5b89\u6392\u5e26\u6765\u7684\u6027\u80fd\u53d8\u5316\u3002", "result": "\u5728LLVM 18.1.6\u4e0a\u8bc4\u4f30\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u7684\u6d41\u6c34\u7ebf\u76f8\u6bd4\u6807\u51c6opt -Oz\u4f18\u5316\u7ea7\u522b\u5e73\u5747\u5b9e\u73b013.62%\u7684\u989d\u5916\u6307\u4ee4\u6570\u51cf\u5c11\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u590d\u6742\u7ea6\u675f\u641c\u7d22\u7a7a\u95f4\u4e2d\u5bfc\u822a\uff0c\u8bc6\u522b\u6709\u6548\u4e14\u9ad8\u6548\u7684pass\u6d41\u6c34\u7ebf\u3002"}}
{"id": "2510.13423", "categories": ["cs.SE", "cs.MS"], "pdf": "https://arxiv.org/pdf/2510.13423", "abs": "https://arxiv.org/abs/2510.13423", "authors": ["Matthew Sottile", "Mohit Tekriwal", "John Sarracino"], "title": "Towards Richer Challenge Problems for Scientific Computing Correctness", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "Correctness in scientific computing (SC) is gaining increasing attention in\nthe formal methods (FM) and programming languages (PL) community. Existing\nPL/FM verification techniques struggle with the complexities of realistic SC\napplications. Part of the problem is a lack of a common understanding between\nthe SC and PL/FM communities of machine-verifiable correctness challenges and\ndimensions of correctness in SC applications.\n  To address this gap, we call for specialized challenge problems to inform the\ndevelopment and evaluation of FM/PL verification techniques for correctness in\nSC. These specialized challenges are intended to augment existing problems\nstudied by FM/PL researchers for general programs to ensure the needs of SC\napplications can be met. We propose several dimensions of correctness relevant\nto scientific computing, and discuss some guidelines and criteria for designing\nchallenge problems to evaluate correctness in scientific computing.", "AI": {"tldr": "\u63d0\u51fa\u4e3a\u79d1\u5b66\u8ba1\u7b97\u5f00\u53d1\u4e13\u95e8\u7684\u9a8c\u8bc1\u6311\u6218\u95ee\u9898\uff0c\u4ee5\u5f25\u8865\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0e\u7f16\u7a0b\u8bed\u8a00\u793e\u533a\u5728\u79d1\u5b66\u8ba1\u7b97\u6b63\u786e\u6027\u9a8c\u8bc1\u65b9\u9762\u7684\u7406\u89e3\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709PL/FM\u9a8c\u8bc1\u6280\u672f\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u7684\u590d\u6742\u6027\uff0c\u79d1\u5b66\u8ba1\u7b97\u793e\u533a\u4e0ePL/FM\u793e\u533a\u4e4b\u95f4\u7f3a\u4e4f\u5bf9\u673a\u5668\u53ef\u9a8c\u8bc1\u6b63\u786e\u6027\u6311\u6218\u7684\u5171\u540c\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u8bbe\u8ba1\u4e13\u95e8\u7684\u6311\u6218\u95ee\u9898\u6765\u6307\u5bfc\u548c\u8bc4\u4f30\u79d1\u5b66\u8ba1\u7b97\u6b63\u786e\u6027\u7684FM/PL\u9a8c\u8bc1\u6280\u672f\uff0c\u8fd9\u4e9b\u6311\u6218\u95ee\u9898\u65e8\u5728\u8865\u5145\u73b0\u6709\u901a\u7528\u7a0b\u5e8f\u9a8c\u8bc1\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u4e86\u79d1\u5b66\u8ba1\u7b97\u76f8\u5173\u7684\u591a\u4e2a\u6b63\u786e\u6027\u7ef4\u5ea6\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u79d1\u5b66\u8ba1\u7b97\u6b63\u786e\u6027\u8bc4\u4f30\u6311\u6218\u95ee\u9898\u7684\u6307\u5bfc\u539f\u5219\u548c\u6807\u51c6\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u6311\u6218\u95ee\u9898\u6765\u786e\u4fdd\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u7f16\u7a0b\u8bed\u8a00\u9a8c\u8bc1\u6280\u672f\u80fd\u591f\u6ee1\u8db3\u79d1\u5b66\u8ba1\u7b97\u5e94\u7528\u7684\u6b63\u786e\u6027\u9700\u6c42\u3002"}}
{"id": "2510.13424", "categories": ["cs.SE", "D.2.5; G.1.3"], "pdf": "https://arxiv.org/pdf/2510.13424", "abs": "https://arxiv.org/abs/2510.13424", "authors": ["Alexander C. Wilton"], "title": "Verifying a Sparse Matrix Algorithm Using Symbolic Execution", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "Scientific software is, by its very nature, complex. It is mathematical and\nhighly optimized which makes it prone to subtle bugs not as easily detected by\ntraditional testing. We outline how symbolic execution can be used to write\ntests similar to traditional unit tests while providing stronger verification\nguarantees and apply this methodology to a sparse matrix algorithm.", "AI": {"tldr": "\u4f7f\u7528\u7b26\u53f7\u6267\u884c\u6765\u6d4b\u8bd5\u79d1\u5b66\u8f6f\u4ef6\uff0c\u7279\u522b\u662f\u7a00\u758f\u77e9\u9635\u7b97\u6cd5\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u6d4b\u8bd5\u66f4\u5f3a\u7684\u9a8c\u8bc1\u4fdd\u8bc1", "motivation": "\u79d1\u5b66\u8f6f\u4ef6\u5177\u6709\u590d\u6742\u6027\u3001\u6570\u5b66\u6027\u548c\u9ad8\u5ea6\u4f18\u5316\u7684\u7279\u70b9\uff0c\u5bb9\u6613\u4ea7\u751f\u4f20\u7edf\u6d4b\u8bd5\u96be\u4ee5\u53d1\u73b0\u7684\u7ec6\u5fae\u9519\u8bef", "method": "\u91c7\u7528\u7b26\u53f7\u6267\u884c\u65b9\u6cd5\u7f16\u5199\u7c7b\u4f3c\u4f20\u7edf\u5355\u5143\u6d4b\u8bd5\u7684\u6d4b\u8bd5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u7a00\u758f\u77e9\u9635\u7b97\u6cd5", "result": "\u7b26\u53f7\u6267\u884c\u80fd\u591f\u4e3a\u79d1\u5b66\u8f6f\u4ef6\u63d0\u4f9b\u66f4\u5f3a\u7684\u9a8c\u8bc1\u4fdd\u8bc1", "conclusion": "\u7b26\u53f7\u6267\u884c\u662f\u6d4b\u8bd5\u590d\u6742\u79d1\u5b66\u8f6f\u4ef6\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u68c0\u6d4b\u4f20\u7edf\u6d4b\u8bd5\u96be\u4ee5\u53d1\u73b0\u7684\u7ec6\u5fae\u9519\u8bef"}}
{"id": "2510.13561", "categories": ["cs.SE", "cs.AI", "68N30"], "pdf": "https://arxiv.org/pdf/2510.13561", "abs": "https://arxiv.org/abs/2510.13561", "authors": ["Peng Di", "Faqiang Chen", "Xiao Bai", "Hongjun Yang", "Qingfeng Li", "Ganglin Wei", "Jian Mou", "Feng Shi", "Keting Chen", "Peng Tang", "Zhitao Shen", "Zheng Li", "Wenhui Shi", "Junwei Guo", "Hang Yu"], "title": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies", "comment": "23 pages", "summary": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/", "AI": {"tldr": "OpenDerisk\u662f\u4e00\u4e2a\u4e13\u4e3aSRE\u8bbe\u8ba1\u7684\u5f00\u6e90\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u8bca\u65ad\u539f\u751f\u534f\u4f5c\u6a21\u578b\u3001\u53ef\u63d2\u62d4\u63a8\u7406\u5f15\u64ce\u548c\u77e5\u8bc6\u5f15\u64ce\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u8f6f\u4ef6\u95ee\u9898\u7684\u8bca\u65ad\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u590d\u6742\u6027\u4e0d\u65ad\u589e\u52a0\uff0c\u7ed9SRE\u56e2\u961f\u5e26\u6765\u4e86\u4e0d\u53ef\u6301\u7eed\u7684\u64cd\u4f5c\u8d1f\u62c5\uff0c\u9700\u8981\u80fd\u591f\u6a21\u62df\u4e13\u5bb6\u8bca\u65ad\u63a8\u7406\u7684AI\u9a71\u52a8\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u6df1\u5ea6\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u8981\u4e48\u4e0d\u9002\u7528\u4e8eSRE\u7279\u6709\u7684\u4e13\u4e1a\u8c03\u67e5\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86OpenDerisk\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u8bca\u65ad\u539f\u751f\u534f\u4f5c\u6a21\u578b\u3001\u53ef\u63d2\u62d4\u63a8\u7406\u5f15\u64ce\u3001\u77e5\u8bc6\u5f15\u64ce\u548c\u6807\u51c6\u5316\u534f\u8bae(MCP)\uff0c\u4f7f\u4e13\u4e1a\u667a\u80fd\u4f53\u80fd\u591f\u534f\u4f5c\u89e3\u51b3\u590d\u6742\u7684\u591a\u9886\u57df\u95ee\u9898\u3002", "result": "\u5168\u9762\u8bc4\u4f30\u663e\u793aOpenDerisk\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5df2\u5728\u8682\u8681\u96c6\u56e2\u5927\u89c4\u6a21\u751f\u4ea7\u90e8\u7f72\uff0c\u670d\u52a1\u8d85\u8fc73000\u540d\u65e5\u5e38\u7528\u6237\uff0c\u9a8c\u8bc1\u4e86\u5176\u5de5\u4e1a\u7ea7\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5f71\u54cd\u3002", "conclusion": "OpenDerisk\u6210\u529f\u586b\u8865\u4e86SRE\u9886\u57dfAI\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u6a21\u62df\u4e13\u5bb6\u8bca\u65ad\u63a8\u7406\uff0c\u5728\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u4e2d\u8bc1\u660e\u4e86\u5176\u4ef7\u503c\u3002"}}
{"id": "2510.13575", "categories": ["cs.SE", "D.2.5"], "pdf": "https://arxiv.org/pdf/2510.13575", "abs": "https://arxiv.org/abs/2510.13575", "authors": ["Han Fu", "Sigrid Eldh", "Kristian Wiklund", "Andreas Ermedahl", "Philipp Haller", "Cyrille Artho"], "title": "Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code", "comment": "9 pages, 4 figures, conference: 2025 28th Euromicro Conference on\n  Digital System Design (DSD)", "summary": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4fee\u590d\u5de5\u4e1a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e2d\u7684\u7f16\u8bd1\u9519\u8bef\uff0c\u5728CI\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u9ad8\u8fbe63%\u7684\u9519\u8bef\u4fee\u590d\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8c03\u8bd5\u65f6\u95f4\u3002", "motivation": "\u5de5\u4e1a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u5f00\u53d1\u7ecf\u5e38\u5728\u6301\u7eed\u96c6\u6210\u4e2d\u51fa\u73b0\u7f16\u8bd1\u9519\u8bef\uff0c\u73b0\u6709\u4fee\u590d\u6280\u672f\u4f9d\u8d56\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4f46\u4e0d\u53ef\u7f16\u8bd1\u4ee3\u7801\u6ca1\u6709\u6d4b\u8bd5\u7528\u4f8b\u53ef\u7528\u3002", "method": "\u6536\u96c6\u8d85\u8fc740000\u4e2a\u4ea7\u54c1\u6e90\u4ee3\u7801\u63d0\u4ea4\uff0c\u4f7f\u7528\u56db\u79cd\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u5de5\u4e1aCI\u7cfb\u7edf\uff0c\u5e76\u4e0e\u4eba\u5de5\u4fee\u590d\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "LLM\u589e\u5f3a\u7684CI\u7cfb\u7edf\u53ef\u4ee5\u89e3\u51b3\u57fa\u7ebf\u6570\u636e\u96c6\u4e2d63%\u7684\u7f16\u8bd1\u9519\u8bef\uff0c\u5176\u4e2d83%\u7684\u6210\u529f\u4fee\u590d\u88ab\u8ba4\u4e3a\u662f\u5408\u7406\u7684\uff0c\u8c03\u8bd5\u65f6\u95f4\u4ece\u51e0\u5c0f\u65f6\u51cf\u5c11\u52308\u5206\u949f\u5185\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u4fee\u590d\u7f16\u8bd1\u9519\u8bef\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u5de5\u4e1a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5f00\u53d1\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2510.13692", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.13692", "abs": "https://arxiv.org/abs/2510.13692", "authors": ["Deepak A. Cherian"], "title": "Property Testing for Ocean Models. Can We Specify It? (Invited Talk)", "comment": "In Proceedings VSS 2025, arXiv:2510.12314", "summary": "I take inspiration from the property-testing literature, particularly the\nwork of Prof. John Hughes, and explore how such ideas might be applied to\nnumerical models of the ocean. Specifically, I ask whether geophysical fluid\ndynamics (GFD) theory, expressed as property tests, might be used to address\nthe oracle problem of testing the correctness of ocean models. I propose that a\nnumber of simple idealized GFD problems can be framed as property tests. These\nexamples clearly illustrate how physics naturally lends itself to specifying\nproperty tests. Which of these proposed tests might be most feasible and\nuseful, remains to be seen.", "AI": {"tldr": "\u63a2\u8ba8\u5982\u4f55\u5c06\u5c5e\u6027\u6d4b\u8bd5\u601d\u60f3\u5e94\u7528\u4e8e\u6d77\u6d0b\u6570\u503c\u6a21\u578b\uff0c\u5229\u7528\u5730\u7403\u7269\u7406\u6d41\u4f53\u52a8\u529b\u5b66\u7406\u8bba\u4f5c\u4e3a\u5c5e\u6027\u6d4b\u8bd5\u6765\u89e3\u51b3\u6d77\u6d0b\u6a21\u578b\u6b63\u786e\u6027\u9a8c\u8bc1\u7684oracle\u95ee\u9898\u3002", "motivation": "\u4ece\u5c5e\u6027\u6d4b\u8bd5\u6587\u732e\u4e2d\u83b7\u5f97\u542f\u53d1\uff0c\u7279\u522b\u662fJohn Hughes\u6559\u6388\u7684\u5de5\u4f5c\uff0c\u63a2\u7d22\u5982\u4f55\u5c06\u8fd9\u4e9b\u601d\u60f3\u5e94\u7528\u4e8e\u6d77\u6d0b\u6570\u503c\u6a21\u578b\uff0c\u89e3\u51b3\u6a21\u578b\u6b63\u786e\u6027\u9a8c\u8bc1\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5c06\u7b80\u5355\u7406\u60f3\u5316\u7684\u5730\u7403\u7269\u7406\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u6846\u67b6\u5316\u4e3a\u5c5e\u6027\u6d4b\u8bd5\uff0c\u5c55\u793a\u7269\u7406\u5b66\u5982\u4f55\u81ea\u7136\u5730\u652f\u6301\u5c5e\u6027\u6d4b\u8bd5\u7684\u89c4\u8303\u5236\u5b9a\u3002", "result": "\u901a\u8fc7\u793a\u4f8b\u6e05\u6670\u5730\u8bf4\u660e\u4e86\u7269\u7406\u5b66\u5982\u4f55\u81ea\u7136\u5730\u652f\u6301\u5c5e\u6027\u6d4b\u8bd5\u7684\u89c4\u8303\uff0c\u4f46\u54ea\u4e9b\u6d4b\u8bd5\u6700\u53ef\u884c\u548c\u6709\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "\u5730\u7403\u7269\u7406\u6d41\u4f53\u52a8\u529b\u5b66\u7406\u8bba\u53ef\u4ee5\u6709\u6548\u5730\u6846\u67b6\u5316\u4e3a\u5c5e\u6027\u6d4b\u8bd5\uff0c\u4e3a\u89e3\u51b3\u6d77\u6d0b\u6a21\u578b\u6b63\u786e\u6027\u9a8c\u8bc1\u7684oracle\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u5177\u4f53\u5b9e\u65bd\u548c\u5b9e\u7528\u6027\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2510.13697", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.13697", "abs": "https://arxiv.org/abs/2510.13697", "authors": ["Maksim Sapronov", "Evgeniy Glukhov"], "title": "On Pretraining for Project-Level Code Completion", "comment": null, "summary": "Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e0d\u540c\u4ed3\u5e93\u5904\u7406\u7b56\u7565\u5bf9\u4ee3\u7801\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6269\u5c55\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u8bad\u7ec3\u5c11\u91cf\u6570\u636e\uff0c\u5728Long Code Arena\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u7b80\u5355\u6587\u4ef6\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u540c\u6837\u6709\u6548\u3002", "motivation": "\u63a2\u7d22\u4ed3\u5e93\u7ea7\u9884\u8bad\u7ec3\u4e2d\u4e0d\u540c\u5904\u7406\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u4ee3\u7801\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5c06OpenCoder\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4ece4,096\u6269\u5c55\u523016,384 tokens\uff0c\u4f7f\u752810\u4ebftokens\u7684\u4ed3\u5e93\u7ea7\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u6bd4\u8f83\u4e0d\u540c\u4ed3\u5e93\u5904\u7406\u6280\u672f\uff0c\u5e76\u6d4b\u8bd5\u7b80\u5355\u6587\u4ef6\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5c3d\u7ba1\u4f7f\u7528\u8f83\u5c0f\u6570\u636e\u96c6\uff0c\u6a21\u578b\u5728Long Code Arena\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0e\u4f7f\u7528\u66f4\u5927\u6570\u636e\u96c6\u7684\u7ade\u4e89\u6a21\u578b\u76f8\u5f53\uff0c\u5404\u79cd\u4ed3\u5e93\u5904\u7406\u6280\u672f\u6548\u679c\u76f8\u4f3c\uff0c\u4e3b\u8981\u6027\u80fd\u63d0\u5347\u6765\u81eaRoPE\u7f29\u653e\u53c2\u6570\u8c03\u6574\u3002", "conclusion": "\u7b80\u5355\u7684\u6587\u4ef6\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u5728\u539f\u59cb\u5e8f\u5217\u957f\u5ea6\u4e0b\u4ecd\u7136\u975e\u5e38\u6709\u6548\uff0c\u8fd9\u4e3a\u6570\u636e\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u4ed3\u5e93\u7ea7\u4ee3\u7801\u8865\u5168\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
