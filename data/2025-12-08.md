<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Integrating Wearable Data into Process Mining: Event, Case and Activity Enrichment](https://arxiv.org/abs/2512.05203)
*Vinicius Stein Dani,Xixi Lu,Iris Beerepoot*

Main category: cs.DB

TL;DR: 探索将可穿戴设备数据融入事件日志的三种方法：作为事件属性、案例属性或新事件，用于个人生产力与健康分析


<details>
  <summary>Details</summary>
Motivation: 将可穿戴设备收集的健康数据与个人事件日志（如数字日历）相结合，以增强过程挖掘在个人生产力和健康福祉分析中的应用价值

Method: 提出三种方法：1) 将可穿戴数据作为事件属性直接关联到单个事件；2) 作为案例属性使用聚合的日级评分；3) 从可穿戴数据中派生新事件（如睡眠时段、体力活动）。使用真实世界数据（智能手表健康数据与数字日历事件）进行示例说明

Result: 展示了三种方法在实际数据中的应用示例，验证了将可穿戴数据整合到事件日志中的可行性

Conclusion: 整合可穿戴数据到过程挖掘中面临技术和概念上的挑战，但为个人生产力和健康福祉分析提供了新的可能性，需要进一步研究解决这些挑战

Abstract: In this short paper, we explore the enrichment of event logs with data from wearable devices. We discuss three approaches: (1) treating wearable data as event attributes, linking them directly to individual events, (2) treating wearable data as case attributes, using aggregated day-level scores, and (3) introducing new events derived from wearable data, such as sleep episodes or physical activities. To illustrate these approaches, we use real-world data from one person, matching health data from a smartwatch with events extracted from a digital calendar application. Finally, we discuss the technical and conceptual challenges involved in integrating wearable data into process mining for personal productivity and well-being.

</details>


### [2] [Featurized-Decomposition Join: Low-Cost Semantic Joins with Guarantees](https://arxiv.org/abs/2512.05399)
*Sepanta Zeighami,Shreya Shankar,Aditya Parameswaran*

Main category: cs.DB

TL;DR: 提出FDJ方法，通过特征化分解自动提取文本特征并组合成逻辑表达式，显著降低语义连接成本（相比现有方法成本降低10倍），同时保证结果质量。


<details>
  <summary>Details</summary>
Motivation: LLM在处理大型文本数据集时，语义连接任务（如实体匹配、记录分类）需要为每对元组调用LLM，成本高昂。现有方法使用嵌入相似度过滤候选对，但效果有限，因为语义相似度不能可靠预测连接结果。

Method: 提出特征化分解连接（FDJ），自动从文本记录中提取特征，组合成合取范式逻辑表达式（特征化分解），通过廉价的特征比较有效剪除非匹配对。使用LLM自动提取可靠特征并组合成逻辑表达式，同时提供统计保证。

Result: 在真实数据集上的实验显示，相比最先进方法，成本降低高达10倍，同时提供相同的质量保证。

Conclusion: FDJ方法通过特征化分解有效解决了语义连接的高成本问题，在保证质量的同时显著降低计算开销，为LLM在数据处理系统中的实际应用提供了高效解决方案。

Abstract: Large Language Models (LLMs) are being increasingly used within data systems to process large datasets with text fields. A broad class of such tasks involves a semantic join-joining two tables based on a natural language predicate per pair of tuples, evaluated using an LLM. Semantic joins generalize tasks such as entity matching and record categorization, as well as more complex text understanding tasks. A naive implementation is expensive as it requires invoking an LLM for every pair of rows in the cross product. Existing approaches mitigate this cost by first applying embedding-based semantic similarity to filter candidate pairs, deferring to an LLM only when similarity scores are deemed inconclusive. However, these methods yield limited gains in practice, since semantic similarity may not reliably predict the join outcome. We propose Featurized-Decomposition Join (FDJ for short), a novel approach for performing semantic joins that significantly reduces cost while preserving quality. FDJ automatically extracts features and combines them into a logical expression in conjunctive normal form that we call a featurized decomposition to effectively prune out non-matching pairs. A featurized decomposition extracts key information from text records and performs inexpensive comparisons on the extracted features. We show how to use LLMs to automatically extract reliable features and compose them into logical expressions while providing statistical guarantees on the output result-an inherently challenging problem due to dependencies among features. Experiments on real-world datasets show up to 10 times reduction in cost compared with the state-of-the-art while providing the same quality guarantees.

</details>


### [3] [PETGraphDB: A Property Evolution Temporal Graph Data Management System](https://arxiv.org/abs/2512.05417)
*Jinghe Song,Zongyu Zuo,Xuelian Lin,Yang Wang,Shuai Ma*

Main category: cs.DB

TL;DR: PETGraph是一个针对属性演化时序图的数据管理系统，采用有效时间属性图数据模型，通过空间高效的存储和细粒度多级锁机制，显著提升存储效率和查询性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网系统发展，属性演化时序图（节点/边属性频繁变化而拓扑结构基本不变）快速增长，但现有时序图管理方案不适用于此类数据，导致数据建模复杂且查询性能低下。

Method: 采用有效时间时序属性图数据模型支持ACID事务；设计空间高效的时序属性存储结构和细粒度多级锁机制。

Result: PETGraph平均只需当前最佳方案33%的存储空间；在HTAP工作负载中事务吞吐量平均提升58.8倍；查询延迟平均提升267倍。

Conclusion: PETGraph专门针对属性演化时序图设计，通过优化的数据模型、存储结构和锁机制，显著改善了数据建模复杂度和查询性能，为物联网时序图管理提供了高效解决方案。

Abstract: Temporal graphs are graphs whose nodes and edges, together with their associated properties, continuously change over time. With the development of Internet of Things (IoT) systems, a subclass of the temporal graph, i.e., Property Evolution Temporal Graph, in which the value of properties on nodes or edges changes frequently while the graph's topology barely changes, is growing rapidly. However, existing temporal graph management solutions are not oriented to the Property Evolution Temporal Graph data, which leads to highly complex data modeling and low-performance query processing of temporal graph queries. To solve these problems, we developed PETGraph, a data management system for Property Evolution Temporal Graph data. PETGraph adopts a valid-time temporal property graph data model to facilitate data modeling, supporting ACID features with transactions. To improve temporal graph query performance, we designed a space-efficient temporal property storage and a fine-granularity multi-level locking mechanism. Experimental results show that PETGraph requires, on average, only 33% of the storage space needed by the current best data management solution. Additionally, it achieves an average of 58.8 times higher transaction throughput in HTAP workloads compared to the best current solutions and outperforms them by an average of 267 times in query latency.

</details>


### [4] [Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments](https://arxiv.org/abs/2512.05453)
*Luc Moreau,Alfred Rossi,Sophie Stalla-Bourdillon*

Main category: cs.DB

TL;DR: Parajudica是一个基于RDF/SPARQL的开放、模块化、可扩展规则系统，用于在多合规框架下评估数据合规状态


<details>
  <summary>Details</summary>
Motivation: 解决在多个同时适用的合规框架下实施基于策略的数据访问控制（PBAC）的挑战

Method: 开发基于RDF/SPARQL的开放、模块化、可扩展规则系统，包含配套的元模型，应用于现有法律框架和行业标准

Result: 展示了该资源和元模型在现有法律框架和行业标准中的应用价值，为比较框架分析提供见解

Conclusion: Parajudica系统支持合规策略执行、合规监控、数据发现和风险评估等多种应用场景

Abstract: Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment.

</details>


### [5] [Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement](https://arxiv.org/abs/2512.05525)
*Nils Strassenburg,Boris Glavic,Tilmann Rabl*

Main category: cs.DB

TL;DR: 论文提出JITR（即时模型替换）框架，在识别出LLM重复任务时，自动用更便宜的小模型透明替换，以降低成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 企业越来越多地使用大型语言模型（LLMs）来自动化简单重复任务，但LLMs的资源消耗和能耗远高于小模型，而小模型在简单任务上往往能达到相似的预测性能。

Method: 提出JITR框架：1）识别LLM调用中的重复任务；2）用更便宜的替代模型透明替换；3）利用模型搜索和迁移学习高效识别和微调适合特定任务的模型。

Result: 通过原型系统Poodle在示例任务上实现了显著的成本和能耗节省。

Conclusion: JITR框架既保留了LLMs的易用性和低开发成本优势，又能显著节省资源和能源，为解决LLMs过度消耗问题提供了可行方案。

Abstract: Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity](https://arxiv.org/abs/2512.05372)
*Chengjie Ma,Seungeun Oh,Jihong Park,Seong-Lyun Kim*

Main category: cs.DC

TL;DR: FedGMR提出了一种联邦学习框架，通过渐进式模型恢复解决带宽受限客户端在异构环境中的参与问题，提升收敛速度和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在异构联邦学习环境中，带宽受限客户端由于通信容量有限，其小型子模型在训练初期学习快，但后期参数不足，导致收敛缓慢和泛化性能下降，难以有效参与训练过程。

Method: FedGMR采用渐进式模型恢复策略，在训练过程中逐步增加每个客户端的子模型密度；同时开发了针对异步异构联邦学习的掩码感知聚合规则，并提供了收敛性理论保证。

Result: 在FEMNIST、CIFAR-10和ImageNet-100数据集上的实验表明，FedGMR在高异构性和非IID设置下实现了更快的收敛速度和更高的准确率。

Conclusion: FedGMR通过渐进式模型恢复有效解决了带宽受限客户端的参与问题，在异步异构联邦学习环境中显著提升了训练效率和模型性能。

Abstract: Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.

</details>


### [7] [Are Bus-Mounted Edge Servers Feasible?](https://arxiv.org/abs/2512.05543)
*Xuezhi Li,Jiancong He,Ming Xie,Xuyang Chen,Le Chang,Li Jiang,Gui Gui*

Main category: cs.DC

TL;DR: 研究基于真实轨迹数据验证公交车搭载边缘服务器的可行性，通过数学建模和贪心算法优化公交车选择，以有限预算最大化需求点覆盖。


<details>
  <summary>Details</summary>
Motivation: 固定边缘服务器（如路侧单元或基站）部署后位置和容量固定，难以处理时空动态的用户需求。公交车等移动服务器具有增加计算弹性的潜力，但需要验证其实际可行性。

Method: 1. 使用上海公交车/出租车/电信数据集分析公交车和基站的覆盖范围；2. 建立数学模型，设计贪心启发式算法选择有限数量的公交车以最大化需求点覆盖；3. 进行基于轨迹的仿真验证算法性能。

Result: 公交车边缘服务器覆盖了大部分地理区域和需求点，显示出巨大潜力。提出的贪心算法在服务器容量和购买数量等现实约束下，能有效处理动态用户需求。

Conclusion: 公交车搭载的边缘服务器在城市车联网中是可行、有益且有价值的，能够为动态用户需求提供弹性计算支持。

Abstract: Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Model Gateway: Model Management Platform for Model-Driven Drug Discovery](https://arxiv.org/abs/2512.05462)
*Yan-Shiun Wu,Nathan A. Morin*

Main category: cs.SE

TL;DR: Model Gateway是一个用于药物发现流程中管理机器学习和科学计算模型的平台，支持LLM代理和生成式AI工具，实现模型注册、管理、异步执行等功能，在10k+并发客户端测试中达到0%失败率。


<details>
  <summary>Details</summary>
Motivation: 药物发现流程中需要管理大量机器学习和科学计算模型，传统方法难以高效协调这些模型，需要统一的平台来支持LLM代理和生成式AI工具，以加速新药开发。

Method: 开发Model Gateway平台，包含模型所有者控制面板、平台管理工具和API服务，支持动态共识模型（聚合多个科学计算模型）、模型注册管理、信息检索、异步提交执行和结果接收。

Result: 平台在超过10k个同时应用客户端消费模型的扩展测试中达到0%失败率，成功集成到模型驱动的药物发现管道中。

Conclusion: Model Gateway是药物发现流程的基础组成部分，通过成熟的MLOps基础设施和LLM代理与生成式AI工具的集成，有望显著加速新药开发。

Abstract: This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.

</details>


### [9] [Metronome: Differentiated Delay Scheduling for Serverless Functions](https://arxiv.org/abs/2512.05703)
*Zhuangbin Chen,Juzheng Zheng,Zibin Zheng*

Main category: cs.SE

TL;DR: Metronome是一个针对FaaS平台的差异化延迟调度框架，通过预测机制优化数据局部性和基础设施局部性，显著减少函数执行时间。


<details>
  <summary>Details</summary>
Motivation: FaaS平台中的调度优化面临挑战，传统延迟调度方法在serverless环境中效果有限，因为函数执行具有动态性、事件驱动性、异构执行时间和复杂局部性模式。

Method: 提出Metronome框架，采用预测机制识别最优局部性感知节点，使用在线随机森林回归模型预测函数在不同节点上的执行时间，实现智能延迟决策同时防止SLA违规。

Result: 在OpenLambda上的实现显示，Metronome相比基线方法显著提升性能，函数平均执行时间减少64.88%-95.83%，在高并发下仍保持性能优势并确保SLA合规。

Conclusion: Metronome通过预测性差异化延迟调度有效解决了serverless环境中的局部性优化问题，为FaaS平台提供了高效的调度解决方案。

Abstract: Function-as-a-Service (FaaS) computing is an emerging cloud computing paradigm for its ease-of-management and elasticity. However, optimizing scheduling for serverless functions remains challenging due to their dynamic and event-driven nature. While data locality has been proven effective in traditional cluster computing systems through delay scheduling, its application in serverless platforms remains largely unexplored. In this paper, we systematically evaluate existing delay scheduling methods in serverless environments and identify three key observations: 1) delay scheduling benefits vary significantly based on function input characteristics; 2) serverless computing exhibits more complex locality patterns than cluster computing systems, encompassing both data locality and infrastructure locality; and 3) heterogeneous function execution times make rule-based delay thresholds ineffective. Based on these insights, we propose Metronome, a differentiated delay scheduling framework that employs predictive mechanisms to identify optimal locality-aware nodes for individual functions. Metronome leverages an online Random Forest Regression model to forecast function execution times across various nodes, enabling informed delay decisions while preventing SLA violations. Our implementation on OpenLambda shows that Metronome significantly outperforms baselines, achieving 64.88%-95.83% reduction in mean execution time for functions, while maintaining performance advantages under increased concurrency levels and ensuring SLA compliance.

</details>


### [10] [Stellis: A Strategy Language for Purifying Separation Logic Entailments](https://arxiv.org/abs/2512.05159)
*Zhiyi Wang,Xiwei Wu,Yi Fang,Chengtao Li,Hongyi Zhong,Lihan Xie,Qinxiang Cao,Zhenjiang Hu*

Main category: cs.SE

TL;DR: Stellis：一种用于纯化分离逻辑蕴含的策略语言，通过移除空间公式简化验证，支持灵活策略编码并自动生成正确性证明。


<details>
  <summary>Details</summary>
Motivation: 自动证明分离逻辑蕴含是验证中的基础挑战。基于规则的方法依赖分离逻辑规则进行自动化，但这些规则语句不足以描述自动化策略，特别是涉及特定场景中内存布局对齐和消除的情况。

Method: 提出Stellis策略语言，具有强大的匹配机制和灵活的动作描述，能够直接编码各种策略。引入算法为每个策略生成健全性条件，将策略的健全性简化为其健全性条件的正确性。基于机械化的归约健全性定理，原型实现生成整体自动化的正确性证明。

Result: 在包含229个蕴含的基准测试中（来自标准链表数据结构和微内核内存模块的验证），系统自动纯化了95.6%（219/229）的蕴含，使用了5个库中的98个策略。

Conclusion: Stellis在提供灵活性和便利性的同时，也表现出高效性，能够自动纯化大多数分离逻辑蕴含，并通过自动生成证明确保策略的健全性。

Abstract: Automatically proving separation logic entailments is a fundamental challenge in verification. While rule-based methods rely on separation logic rules (lemmas) for automation, these rule statements are insufficient for describing automation strategies, which usually involve the alignment and elimination of corresponding memory layouts in specific scenarios. To overcome this limitation, we propose Stellis, a strategy language for purifying separation logic entailments, i.e., removing all spatial formulas to reduce the entailment to a simpler pure entailment. Stellis features a powerful matching mechanism and a flexible action description, enabling the straightforward encoding of a wide range of strategies. To ensure strategy soundness, we introduce an algorithm that generates a soundness condition for each strategy, thereby reducing the soundness of each strategy to the correctness of its soundness condition. Furthermore, based on a mechanized reduction soundness theorem, our prototype implementation generates correctness proofs for the overall automation. We evaluate our system on a benchmark of 229 entailments collected from verification of standard linked data structures and the memory module of a microkernel, and the evaluation results demonstrate that, with such flexibility and convenience provided, our system is also highly effective, which automatically purifies 95.6% (219 out of 229) of the entailments using 5 libraries with 98 strategies.

</details>


### [11] [Towards A Cultural Intelligence and Values Inferences Quality Benchmark for Community Values and Common Knowledge](https://arxiv.org/abs/2512.05176)
*Brittany Johnson,Erin Reddick,Angela D. R. Smith*

Main category: cs.SE

TL;DR: 该论文提出CIVIQ基准测试，用于评估LLM与美国多元文化社区价值观的对齐程度，而非单一国家对齐。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要对齐西方白人叙事，与多元文化群体存在偏差。现有国家对齐基准（如KorNAT）在美国多元文化背景下效果有限，需要更细粒度的文化对齐评估方法。

Method: 复制研究：将韩国国家LLM对齐基准KorNAT的开发过程，转化为开发CIVIQ（文化智能与价值观推断质量）基准，重点关注社区社会价值观和常识的对齐。

Result: 提出了CIVIQ基准测试，为AI技术文化对齐的研究和开发提供了关键基础，特别针对美国多元文化背景。

Conclusion: 需要超越国家层面的对齐基准，开发针对具体文化社区的对齐评估工具，以促进LLM更好地代表和服务多元文化群体。

Abstract: Large language models (LLMs) have emerged as a powerful technology, and thus, we have seen widespread adoption and use on software engineering teams. Most often, LLMs are designed as "general purpose" technologies meant to represent the general population. Unfortunately, this often means alignment with predominantly Western Caucasian narratives and misalignment with other cultures and populations that engage in collaborative innovation. In response to this misalignment, there have been recent efforts centered on the development of "culturally-informed" LLMs, such as ChatBlackGPT, that are capable of better aligning with historically marginalized experiences and perspectives. Despite this progress, there has been little effort aimed at supporting our ability to develop and evaluate culturally-informed LLMs. A recent effort proposed an approach for developing a national alignment benchmark that emphasizes alignment with national social values and common knowledge. However, given the range of cultural identities present in the United States (U.S.), a national alignment benchmark is an ineffective goal for broader representation. To help fill this gap in this US context, we propose a replication study that translates the process used to develop KorNAT, a Korean National LLM alignment benchmark, to develop CIVIQ, a Cultural Intelligence and Values Inference Quality benchmark centered on alignment with community social values and common knowledge. Our work provides a critical foundation for research and development aimed at cultural alignment of AI technologies in practice.

</details>


### [12] [A Survey of Bugs in AI-Generated Code](https://arxiv.org/abs/2512.05239)
*Ruofan Gao,Amjed Tahir,Peng Liang,Teo Susnjak,Foutse Khomh*

Main category: cs.SE

TL;DR: 该论文系统分析了AI生成代码中的缺陷和错误类型，提供了分类框架和修复策略，为模型改进和质量评估提供参考。


<details>
  <summary>Details</summary>
Motivation: AI代码生成模型被广泛使用以提高生产力，但生成的代码存在质量问题。现有研究对AI生成代码中的缺陷和错误缺乏系统性总结，需要全面分析错误类型、分布、修复策略及其与特定模型的关系。

Method: 系统分析现有AI生成代码文献，建立对生成代码中缺陷和错误的整体理解，提供分类框架。

Result: 建立了AI生成代码中缺陷和错误的分类体系，揭示了不同类型错误的分布特征，分析了与特定模型的关联性，并总结了可能的修复和缓解策略。

Conclusion: 该研究为AI代码生成模型的质量评估和改进提供了系统性参考框架，有助于提升生成代码的可靠性和可维护性。

Abstract: Developers are widely using AI code-generation models, aiming to increase productivity and efficiency. However, there are also quality concerns regarding the AI-generated code. The generated code is produced by models trained on publicly available code, which are known to contain bugs and quality issues. Those issues can cause trust and maintenance challenges during the development process. Several quality issues associated with AI-generated code have been reported, including bugs and defects. However, these findings are often scattered and lack a systematic summary. A comprehensive review is currently lacking to reveal the types and distribution of these errors, possible remediation strategies, as well as their correlation with the specific models. In this paper, we systematically analyze the existing AI-generated code literature to establish an overall understanding of bugs and defects in generated code, providing a reference for future model improvement and quality assessment. We aim to understand the nature and extent of bugs in AI-generated code, and provide a classification of bug types and patterns present in code generated by different models. We also discuss possible fixes and mitigation strategies adopted to eliminate bugs from the generated code.

</details>


### [13] [Learning to Code with Context: A Study-Based Approach](https://arxiv.org/abs/2512.05242)
*Uwe M. Borghoff,Mark Minas,Jannis Schopp*

Main category: cs.SE

TL;DR: 该研究探讨了在软件工程教育中整合生成式AI工具的方法，通过在大学编程项目中开展用户研究，分析了学生使用AI工具的情况，并开发了基于RAG的本地化LLM助手来提供项目上下文支持。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具的快速发展正在改变软件开发方式，软件工程教育需要适应这一变化，确保学生不仅学习传统开发方法，还能有意义且负责任地使用这些新技术。项目制课程为探索AI辅助集成到实际开发实践提供了有效环境。

Method: 在大学编程项目中开展用户研究，学生协作开发电脑游戏，观察他们如何在软件开发不同阶段使用生成式AI工具。同时开发了基于检索增强生成（RAG）的本地部署大型语言模型助手，该系统利用相关文档和源代码来提供项目上下文支持。

Result: 研究调查了参与者在软件开发过程中如何使用生成式AI工具，识别了这些工具最有效的任务类型，分析了学生遇到的挑战。通过RAG系统对模型行为、参数敏感性和常见失败模式进行了定性分析。

Conclusion: 研究结果加深了对教育软件项目中上下文感知AI支持的理解，为未来将基于AI的辅助工具整合到软件工程课程中提供了信息基础，有助于更好地设计AI辅助的软件工程教育。

Abstract: The rapid emergence of generative AI tools is transforming the way software is developed. Consequently, software engineering education must adapt to ensure that students not only learn traditional development methods but also understand how to meaningfully and responsibly use these new technologies. In particular, project-based courses offer an effective environment to explore and evaluate the integration of AI assistance into real-world development practices. This paper presents our approach and a user study conducted within a university programming project in which students collaboratively developed computer games. The study investigates how participants used generative AI tools throughout different phases of the software development process, identifies the types of tasks where such tools were most effective, and analyzes the challenges students encountered. Building on these insights, we further examine a repository-aware, locally deployed large language model (LLM) assistant designed to provide project-contextualized support. The system employs Retrieval-Augmented Generation (RAG) to ground responses in relevant documentation and source code, enabling qualitative analysis of model behavior, parameter sensitivity, and common failure modes. The findings deepen our understanding of context-aware AI support in educational software projects and inform future integration of AI-based assistance into software engineering curricula.

</details>


### [14] [Engagement in Code Review: Emotional, Behavioral, and Cognitive Dimensions in Peer vs. LLM Interactions](https://arxiv.org/abs/2512.05309)
*Adam Alami,Nathan Cassee,Thiago Rocha Silva,Elda Paja,Neil A. Ernst*

Main category: cs.SE

TL;DR: 研究比较了人类同行评审与LLM辅助代码评审中软件工程师的情感反应和行为参与，发现LLM辅助评审将参与重点从情绪管理转向认知负荷管理，AI最适合作为支持伙伴来减少认知和情感负担。


<details>
  <summary>Details</summary>
Motivation: 代码评审是一种社会技术实践，但软件工程师在LLM辅助代码评审中的参与方式与人类同行评审相比尚不明确。需要理解工程师在两种评审模式下的情感反应、参与决策和反馈采纳过程。

Method: 采用两阶段定性研究，涉及20名软件工程师。第一阶段：参与者交换同行评审并接受访谈，了解情感反应和参与决策。第二阶段：引入符合工程师偏好的新提示，探究特征如何影响反应。开发了连接情绪自我调节与行为参与和解决方案的综合模型。

Result: 识别了工程师应对负面反馈的四种情绪自我调节策略：重构、对话调节、回避和防御。参与通过社会校准进行；工程师根据关系氛围和团队规范调整反应和行为。在同行评审中，解决方案轨迹因焦点（个人/双人/团队）和内部意义建构过程而异。LLM辅助评审中，情感成本和自我调节需求较低。当LLM反馈符合工程师认知期望时，参与者报告处理努力减少，采纳倾向可能更高。

Conclusion: LLM辅助评审将参与重点从情绪管理转向认知负荷管理。AI最适合作为支持伙伴来减少认知和情感负担，同时保留人类责任和同行评审等社会技术活动的社会意义。提出了连接情绪自我调节与行为参与和解决方案的综合参与模型。

Abstract: Code review is a socio-technical practice, yet how software engineers engage in Large Language Model (LLM)-assisted code reviews compared to human peer-led reviews is less understood. We report a two-phase qualitative study with 20 software engineers to understand this. In Phase I, participants exchanged peer reviews and were interviewed about their affective responses and engagement decisions. In Phase II, we introduced a new prompt matching engineers' preferences and probed how characteristics shaped their reactions. We develop an integrative account linking emotional self-regulation to behavioral engagement and resolution. We identify self-regulation strategies that engineers use to regulate their emotions in response to negative feedback: reframing, dialogic regulation, avoidance, and defensiveness. Engagement proceeds through social calibration; engineers align their responses and behaviors to the relational climate and team norms. Trajectories to resolution, in the case of peer-led review, vary by locus (solo/dyad/team) and an internal sense-making process. With the LLM-assisted review, emotional costs and the need for self-regulation seem lower. When LLM feedback aligned with engineers' cognitive expectations, participants reported reduced processing effort and a potentially higher tendency to adopt. We show that LLM-assisted review redirects engagement from emotion management to cognitive load management. We contribute an integrative model of engagement that links emotional self-regulation to behavioral engagement and resolution, showing how affective and cognitive processes influence feedback adoption in peer-led and LLM-assisted code reviews. We conclude that AI is best positioned as a supportive partner to reduce cognitive and emotional load while preserving human accountability and the social meaning of peer review and similar socio-technical activities.

</details>


### [15] [WhatsCode: Large-Scale GenAI Deployment for Developer Efficiency at WhatsApp](https://arxiv.org/abs/2512.05314)
*Ke Mao,Timotej Kapus,Cons T Åhs,Matteo Marescotti,Daniel Ip,Ákos Hajdu,Sopot Cela,Aparup Banerjee*

Main category: cs.SE

TL;DR: WhatsCode是一个面向WhatsApp的领域特定AI开发系统，在25个月的工业部署中，从隐私自动化演进为集成端到端功能开发的自主代理工作流，显著提升了隐私验证覆盖率并生成了数千个被接受的代码变更。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助开发工具在工业环境中日益普及，但在合规相关的大规模工业部署方面存在显著的学术研究空白。WhatsApp作为服务超过20亿用户的平台，需要处理数百万行跨平台代码，这为研究AI工具在真实工业环境中的部署提供了重要机会。

Method: WhatsCode是一个领域特定的AI开发系统，支持WhatsApp平台。系统从2023年至2025年共25个月的工业部署中，从针对性的隐私自动化演进为集成端到端功能开发和DevOps流程的自主代理工作流。研究基于实际生产部署数据，分析了系统在不同自动化领域的表现。

Result: WhatsCode取得了显著成效：自动化隐私验证覆盖率从15%提升至53%（3.5倍增长）；识别隐私需求；生成了超过3000个被接受的代码变更，接受率在不同自动化领域从9%到100%不等；完成了692个自动化重构/修复变更、711个框架采用、141个功能开发辅助；在bug分类中保持了86%的精确度。研究还识别出两种稳定的人机协作模式：一键部署（60%高置信度变更）和接管-修订（40%复杂决策）。

Conclusion: 组织因素（如所有权模型、采用动态和风险管理）与技术能力同等重要，对企业在规模AI成功具有决定性作用。研究发现，有效的人机协作而非完全自动化，才是驱动可持续业务影响的关键。这为合规相关环境中大规模AI工具部署提供了基于证据的指导。

Abstract: The deployment of AI-assisted development tools in compliance-relevant, large-scale industrial environments represents significant gaps in academic literature, despite growing industry adoption. We report on the industrial deployment of WhatsCode, a domain-specific AI development system that supports WhatsApp (serving over 2 billion users) and processes millions of lines of code across multiple platforms. Over 25 months (2023-2025), WhatsCode evolved from targeted privacy automation to autonomous agentic workflows integrated with end-to-end feature development and DevOps processes.
  WhatsCode achieved substantial quantifiable impact, improving automated privacy verification coverage 3.5x from 15% to 53%, identifying privacy requirements, and generating over 3,000 accepted code changes with acceptance rates ranging from 9% to 100% across different automation domains. The system committed 692 automated refactor/fix changes, 711 framework adoptions, 141 feature development assists and maintained 86% precision in bug triage. Our study identifies two stable human-AI collaboration patterns that emerged from production deployment: one-click rollout for high-confidence changes (60% of cases) and commandeer-revise for complex decisions (40%). We demonstrate that organizational factors, such as ownership models, adoption dynamics, and risk management, are as decisive as technical capabilities for enterprise-scale AI success. The findings provide evidence-based guidance for large-scale AI tool deployment in compliance-relevant environments, showing that effective human-AI collaboration, not full automation, drives sustainable business impact.

</details>


### [16] [Invisible Load: Uncovering the Challenges of Neurodivergent Women in Software Engineering](https://arxiv.org/abs/2512.05350)
*Munazza Zaib,Wei Wang,Dulaji Hidellaarachchi,Isma Farah Siddiqui*

Main category: cs.SE

TL;DR: 该论文提出了一种混合方法，结合InclusiveMag包容性框架和GenderMag走查流程，专门针对软件工程中神经多样性女性的独特挑战进行研究。


<details>
  <summary>Details</summary>
Motivation: 软件工程中的神经多样性女性面临着性别偏见和神经差异交叉的独特挑战，但目前SE研究尚未系统研究这一群体。误诊、伪装和男性中心的工作文化加剧了压力、倦怠和流失问题。

Method: 提出混合方法学方法：整合InclusiveMag包容性框架和GenderMag走查流程，专门针对神经多样性女性在SE中的情境。设计分为三个阶段：文献综述范围界定、人物角色和分析流程推导、协作研讨会中方法应用。

Result: 通过有针对性的文献综述，将挑战综合为认知、社交、组织、结构和职业发展五个方面，揭示了误诊/延迟诊断和伪装如何加剧排斥问题。

Conclusion: 这些发现为后续阶段奠定了基础，将开发和应用包容性分析方法来支持可操作的变革，以改善神经多样性女性在软件工程领域的处境。

Abstract: Neurodivergent women in Software Engineering (SE) encounter distinctive challenges at the intersection of gender bias and neurological differences. To the best of our knowledge, no prior work in SE research has systematically examined this group, despite increasing recognition of neurodiversity in the workplace. Underdiagnosis, masking, and male-centric workplace cultures continue to exacerbate barriers that contribute to stress, burnout, and attrition. In response, we propose a hybrid methodological approach that integrates InclusiveMag's inclusivity framework with the GenderMag walkthrough process, tailored to the context of neurodivergent women in SE. The overarching design unfolds across three stages, scoping through literature review, deriving personas and analytic processes, and applying the method in collaborative workshops. We present a targeted literature review that synthesize challenges into cognitive, social, organizational, structural and career progression challenges neurodivergent women face in SE, including how under/late diagnosis and masking intensify exclusion. These findings lay the groundwork for subsequent stages that will develop and apply inclusive analytic methods to support actionable change.

</details>


### [17] [BGPFuzz: Automated Configuration Fuzzing of the Border Gateway Protocol](https://arxiv.org/abs/2512.05358)
*Chenlu Zhang,Amirmohammad Pasdar,Van-Thuan Pham*

Main category: cs.SE

TL;DR: BGPFuzz：一种结构感知、有状态的模糊测试框架，用于检测BGP配置错误，无需预定义正确性属性，通过运行时预言机捕获实际异常症状。


<details>
  <summary>Details</summary>
Motivation: 电信网络依赖配置定义路由行为，BGP中的配置错误可能导致严重中断和安全漏洞（如2021年Facebook中断）。现有方法依赖合成或验证，需要成本效益更高的方法来识别由BGP固有复杂性或供应商特定实现导致的配置错误。

Method: 提出BGPFuzz框架：1）结构感知和有状态的模糊测试，系统性地变异BGP配置；2）在虚拟化网络中评估配置效果；3）无需静态分析中的预定义正确性属性；4）通过运行时预言机捕获实际症状（会话重置、黑洞、流量重定向）。

Result: 实验表明BGPFuzz能够可靠地重现和检测已知故障，包括最大前缀违规和子前缀劫持。

Conclusion: BGPFuzz提供了一种无需预定义属性的成本效益方法，通过运行时异常检测来识别BGP配置错误，有效应对BGP复杂性和供应商实现差异带来的配置风险。

Abstract: Telecommunications networks rely on configurations to define routing behavior, especially in the Border Gateway Protocol (BGP), where misconfigurations can lead to severe outages and security breaches, as demonstrated by the 2021 Facebook outage. Unlike existing approaches that rely on synthesis or verification, our work offers a cost-effective method for identifying misconfigurations resulting from BGP's inherent complexity or vendor-specific implementations. We present BGPFuzz, a structure-aware and stateful fuzzing framework that systematically mutates BGP configurations and evaluates their effects in virtualized network. Without requiring predefined correctness properties as in static analysis, BGPFuzz detects anomalies through runtime oracles that capture practical symptoms such as session resets, blackholing, and traffic redirection. Our experiments show that BGPFuzz can reliably reproduce and detect known failures, including max-prefix violations and sub-prefix hijacks.

</details>


### [18] [Legacy Modernization with AI -- Mainframe modernization](https://arxiv.org/abs/2512.05375)
*Sunil Khemka,Arunava Majumdar*

Main category: cs.SE

TL;DR: AI辅助的遗留系统现代化将传统大型机系统转变为灵活、可扩展的智能架构，通过自动化代码重构、智能数据迁移和预测性维护等AI驱动策略，帮助企业迁移到微服务、容器化和混合云平台。


<details>
  <summary>Details</summary>
Motivation: 传统大型机系统虽然可靠，但维护成本高、技能短缺、与云系统集成困难，阻碍了企业的数字化转型和创新发展。

Method: 采用AI驱动的现代化策略：1) 自动化代码重构；2) 智能工具进行数据迁移；3) 预测性维护；4) 机器学习模型分析遗留代码库，识别效率机会，执行自动化测试和部署。

Result: 企业能够顺利迁移到微服务、容器化环境和混合云平台，AI通过生成工作负载平衡和异常检测的洞察，提高运营效率，同时保留核心业务逻辑。

Conclusion: AI在大型机现代化中的应用是数字化转型和企业可持续增长的催化剂，能够实现更快的创新、减少停机时间、增强系统弹性。

Abstract: Artificial Intelligence-assisted legacy modernization is essential in changing the stalwart mainframe systems of the past into flexible, scalable, and smart architecture. While mainframes are generally dependable, they can be difficult to maintain due to their high maintenance costs, the shortage of skills, and the problems in integrating them with cloud-based systems. By adopting AI-driven modernization strategies such as automated code refactoring, migration of data using smart tools, and predictive maintenance, companies can easily move to microservices, containerized environments, and hybrid cloud platforms. Machine learning models have the capability to go through legacy codebases, figure out efficiency opportunities, and carry out automated testing and deployment. Besides that, AI improves the organization's operational efficiency by generating the insights that can be used to level the workload and detect the anomalies. The coupling of the two is not only about saving the core business logic but also about enabling quicker innovation, less downtime, and enhanced system resilience. Therefore, the use of AI in mainframe modernization is a catalyst for digital transformation and enterprise growth that is sustainable over time.

</details>


### [19] [Fuzzing the brain: Automated stress testing for the safety of ML-driven neurostimulation](https://arxiv.org/abs/2512.05383)
*Mara Downing,Matthew Peng,Jacob Granley,Michael Beyeler,Tevfik Bultan*

Main category: cs.SE

TL;DR: 提出一种基于覆盖引导模糊测试的方法，用于系统检测ML驱动的神经刺激系统中的不安全刺激模式，将安全性评估转化为可测量的模型属性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在神经假体设备中用于生成电刺激模式，虽然能提供精确的个性化控制，但直接将模型输出传递到神经组织会引入新的安全风险。需要系统化的方法来检测和表征ML驱动神经刺激系统中的不安全刺激模式。

Method: 将覆盖引导模糊测试技术适配到神经刺激领域，通过扰动模型输入并追踪是否违反电荷密度、瞬时电流或电极共激活等生物物理限制。将编码器视为黑盒，使用覆盖度量来引导探索，量化测试用例在可能输出空间和违规类型上的覆盖广度。

Result: 应用于视网膜和皮层深度刺激编码器时，该方法系统性地揭示了超出安全限制的多样化刺激机制。两种违规输出覆盖度量识别出最多数量和多样性的不安全输出，实现了跨架构和训练策略的可解释比较。

Conclusion: 违规聚焦的模糊测试将安全性评估重构为经验性、可重复的过程。通过将安全性从训练启发式转化为部署模型的可测量属性，为下一代神经接口的循证基准测试、监管准备和伦理保证奠定了基础。

Abstract: Objective: Machine learning (ML) models are increasingly used to generate electrical stimulation patterns in neuroprosthetic devices such as visual prostheses. While these models promise precise and personalized control, they also introduce new safety risks when model outputs are delivered directly to neural tissue. We propose a systematic, quantitative approach to detect and characterize unsafe stimulation patterns in ML-driven neurostimulation systems. Approach: We adapt an automated software testing technique known as coverage-guided fuzzing to the domain of neural stimulation. Here, fuzzing performs stress testing by perturbing model inputs and tracking whether resulting stimulation violates biophysical limits on charge density, instantaneous current, or electrode co-activation. The framework treats encoders as black boxes and steers exploration with coverage metrics that quantify how broadly test cases span the space of possible outputs and violation types. Main results: Applied to deep stimulus encoders for the retina and cortex, the method systematically reveals diverse stimulation regimes that exceed established safety limits. Two violation-output coverage metrics identify the highest number and diversity of unsafe outputs, enabling interpretable comparisons across architectures and training strategies. Significance: Violation-focused fuzzing reframes safety assessment as an empirical, reproducible process. By transforming safety from a training heuristic into a measurable property of the deployed model, it establishes a foundation for evidence-based benchmarking, regulatory readiness, and ethical assurance in next-generation neural interfaces.

</details>


### [20] [Bita: A Conversational Assistant for Fairness Testing](https://arxiv.org/abs/2512.05428)
*Keeryn Johnson,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: Bita是一个基于对话的AI助手，帮助软件测试人员检测AI系统中的偏见，评估测试计划，并生成公平性导向的探索性测试章程。


<details>
  <summary>Details</summary>
Motivation: AI系统中的偏见可能导致不公平和歧视性结果，而现有的公平性测试工具通常难以使用，需要高级专业知识，且对实际工作流程支持有限。

Method: Bita整合了大语言模型与检索增强生成技术，将其响应基于精选的公平性文献，帮助测试人员检测偏见来源、评估测试计划并生成公平性测试章程。

Result: 验证表明Bita能够支持真实世界AI系统的公平性测试任务，提供结构化、可复现的效用证据。

Conclusion: Bita是一个实用的工具，以可访问、系统化且直接适用于工业实践的方式实现了公平性测试的操作化。

Abstract: Bias in AI systems can lead to unfair and discriminatory outcomes, especially when left untested before deployment. Although fairness testing aims to identify and mitigate such bias, existing tools are often difficult to use, requiring advanced expertise and offering limited support for real-world workflows. To address this, we introduce Bita, a conversational assistant designed to help software testers detect potential sources of bias, evaluate test plans through a fairness lens, and generate fairness-oriented exploratory testing charters. Bita integrates a large language model with retrieval-augmented generation, grounding its responses in curated fairness literature. Our validation demonstrates how Bita supports fairness testing tasks on real-world AI systems, providing structured, reproducible evidence of its utility. In summary, our work contributes a practical tool that operationalizes fairness testing in a way that is accessible, systematic, and directly applicable to industrial practice.

</details>


### [21] [Everything is Context: Agentic File System Abstraction for Context Engineering](https://arxiv.org/abs/2512.05470)
*Xiwei Xu,Robert Mao,Quan Bai,Xuewu Gu,Yechao Li,Liming Zhu*

Main category: cs.SE

TL;DR: 该论文提出了一个基于文件系统抽象的情境工程架构，用于统一管理生成式AI系统中的上下文知识、记忆、工具和人类输入，实现可验证、可维护的AI协作系统。


<details>
  <summary>Details</summary>
Motivation: 生成式AI改变了软件系统设计，但现有的情境工程实践（如提示工程、RAG、工具集成）仍然碎片化，产生临时性产物，限制了可追溯性和问责性。需要一种统一、持久的基础设施来管理异构情境工件。

Method: 提出基于Unix"万物皆文件"理念的文件系统抽象，为异构情境工件提供统一的挂载、元数据和访问控制。在开源AIGNE框架中实现可验证的情境工程管道，包含情境构造器、加载器和评估器，用于在令牌约束下组装、交付和验证上下文。

Result: 在AIGNE框架中实现了该架构，展示了两个示例：具有记忆的代理和基于MCP的GitHub助手。该架构为可问责、以人为中心的AI协作提供了可重用的基础，支持可验证、可维护且适合工业环境的GenAI系统。

Conclusion: 文件系统抽象为情境工程提供了统一、持久、可治理的基础设施，使生成式AI成为决策支持中的活跃协作者，同时确保人类作为策展者、验证者和共同推理者的核心角色，建立了可问责、以人为中心的AI协作基础。

Abstract: Generative AI (GenAI) has reshaped software system design by introducing foundation models as pre-trained subsystems that redefine architectures and operations. The emerging challenge is no longer model fine-tuning but context engineering-how systems capture, structure, and govern external knowledge, memory, tools, and human input to enable trustworthy reasoning. Existing practices such as prompt engineering, retrieval-augmented generation (RAG), and tool integration remain fragmented, producing transient artefacts that limit traceability and accountability. This paper proposes a file-system abstraction for context engineering, inspired by the Unix notion that 'everything is a file'. The abstraction offers a persistent, governed infrastructure for managing heterogeneous context artefacts through uniform mounting, metadata, and access control. Implemented within the open-source AIGNE framework, the architecture realises a verifiable context-engineering pipeline, comprising the Context Constructor, Loader, and Evaluator, that assembles, delivers, and validates context under token constraints. As GenAI becomes an active collaborator in decision support, humans play a central role as curators, verifiers, and co-reasoners. The proposed architecture establishes a reusable foundation for accountable and human-centred AI co-work, demonstrated through two exemplars: an agent with memory and an MCP-based GitHub assistant. The implementation within the AIGNE framework demonstrates how the architecture can be operationalised in developer and industrial settings, supporting verifiable, maintainable, and industry-ready GenAI systems.

</details>


### [22] [A Hybrid Approach for EMF Code Generation:Code Templates Meet Large Language Models](https://arxiv.org/abs/2512.05498)
*Xiao He,Ru Chen,Zeqing Zhang,Yanling Wang,Qiuyan Dong*

Main category: cs.SE

TL;DR: iEcoreGen结合EMF模板生成和LLM代码补全，通过分解需求、生成初始代码、序列化规范到文档字符串，再调用LLM完成未实现方法，在代码生成任务中表现优于纯LLM方法。


<details>
  <summary>Details</summary>
Motivation: 模板生成方法能保证正确性但灵活性不足，LLM方法灵活但可能产生错误代码。需要结合两者优势，实现既保证正确性又具有灵活性的代码生成方法。

Method: iEcoreGen采用混合方法：1) 使用EMF的Ecore模型定义系统结构；2) 分解需求推导操作规范；3) 用EMF模板生成初始Java代码；4) 将规范序列化到文档字符串；5) 调用LLM完成未实现方法。

Result: 在20个代码生成任务和5个LLM上的评估显示，iEcoreGen在pass@k指标上优于纯LLM基线，在compilation@k指标上与基线相当。消融研究明确了各组件贡献。

Conclusion: LLM增强的模型驱动开发是提高软件自动化效率的有前景路径，结合模板生成和LLM的优势能实现更可靠的代码生成。

Abstract: Template-based and LLM-based code generation are both key enablers of automated software development. The former provides correctness guarantees but are rigid for complex requirements, whereas LLMs offer high flexibility at the risk of producing faulty code.This paper proposes iEcoreGen, a hybrid approach that integrates Eclipse Modeling Framework (EMF) and LLMs. In EMF, an Ecore model defines a system structure and acts as a blueprint for code-generation.iEcoreGen decomposes requirements to derive operation specifications, uses EMF's template-based generator to produce initial Java code, and serializes specifications into docstrings. LLMs are then invoked to complete and fix unimplemented methods. We assessed iEcoreGen on twenty code-generation tasks across five LLMs. It surpasses LLM-only baselines on pass@k and performs on par with them on compilation@k. An ablation study clarified the contribution of each component of iEcoreGen. Overall, the findings indicate that LLM-enhanced model-driven development is a promising path toward more efficient software automation.

</details>


### [23] [Generative AI in Simulation-Based Test Environments for Large-Scale Cyber-Physical Systems: An Industrial Study](https://arxiv.org/abs/2512.05507)
*Masoud Sadrnezhaad,José Antonio Hernández López,Torvald Mårtensson,Daniel Varro*

Main category: cs.SE

TL;DR: 本文通过跨公司研讨会收集了从业者对生成式AI在大型信息物理系统仿真测试中应用的观点，揭示了当前研究空白，并提出了包含三个优先方向的研究议程。


<details>
  <summary>Details</summary>
Motivation: 大型信息物理系统的质量保证依赖复杂的仿真测试环境，但开发和维护仿真模型需要大量资源。虽然生成式AI在软件测试中已显示出潜力，但在信息物理系统仿真测试中的应用仍未被充分探索。

Method: 基于跨公司研讨会，收集了来自六个组织的从业者观点，分析生成式AI在仿真测试中的应用挑战和机遇。

Result: 研究获得了从业者基于经验的详细见解，识别了当前挑战，并提出了包含三个高优先级方向的研究议程：AI生成的场景和环境模型、CI/CD管道中的模拟器和AI、以及生成式AI在仿真中的可信度。

Conclusion: 虽然生成式AI在仿真测试中具有巨大潜力，但仍存在未解决的挑战。本文旨在通过详细阐述这些问题，指导未来学术界与工业界的合作，促进生成式AI在仿真测试中的负责任应用。

Abstract: Quality assurance for large-scale cyber-physical systems relies on sophisticated test activities using complex test environments investigated with the help of numerous types of simulators. As these systems grow, extensive resources are required to develop and maintain simulation models of hardware and software components, as well as physical environments. Meanwhile, recent advances in generative AI have led to tools that can produce executable test cases for software systems, offering potential benefits such as reducing manual efforts or increasing test coverage. However, the application of generative AI techniques to simulation-based testing of large-scale cyber-physical systems remains underexplored. To better understand this gap, this study captures practitioners' perspectives on leveraging generative AI, based on a cross-company workshop with six organizations. Our contribution is twofold: (1) detailed, experience-based insights into challenges faced by engineers, and (2) a research agenda comprising three high-priority directions: (a) AI-generated scenarios and environment models, (b) simulators and AI in CI/CD pipelines, and (c) trustworthiness in generative AI for simulation. While participants acknowledged substantial potential, they also highlighted unresolved challenges. By detailing these issues, the paper aims to guide future academia-industry collaboration towards the responsible adoption of generative AI in simulation-based testing.

</details>


### [24] [From Challenge to Change: Design Principles for AI Transformations](https://arxiv.org/abs/2512.05533)
*Theocharis Tavantzis,Stefano Lambiase,Daniel Russo,Robert Feldt*

Main category: cs.SE

TL;DR: 提出基于行为软件工程(BSE)的以人为本框架，帮助软件工程组织在早期AI采用阶段应对社会技术复杂性，包含9个维度的具体行动指南。


<details>
  <summary>Details</summary>
Motivation: AI正在重塑软件工程，但现有研究过于关注技术问题，缺乏对团队如何适应和信任AI的深入理解。需要解决AI集成中的人类行为和非技术因素挑战。

Method: 采用混合方法：通过文献综述分析组织变革模型，通过访谈数据主题分析构建框架，并通过调查(N=105)和专家研讨会(N=4)收集初步实践者反馈。

Result: 开发出包含9个维度的框架：AI战略设计、AI战略评估、协作、沟通、治理与伦理、领导力、组织文化、组织动态、技能提升。调查显示技能提升(15.2%)和AI战略设计(15.1%)被认为最重要。

Conclusion: 组织当前优先关注程序性元素如战略设计，而以人为本的防护措施仍不完善。该框架为早期AI采用提供了实用路线图，并突出了未来以人为本AI在SE中的研究方向。

Abstract: The rapid rise of Artificial Intelligence (AI) is reshaping Software Engineering (SE), creating new opportunities while introducing human-centered challenges. Although prior work notes behavioral and other non-technical factors in AI integration, most studies still emphasize technical concerns and offer limited insight into how teams adapt to and trust AI. This paper proposes a Behavioral Software Engineering (BSE)-informed, human-centric framework to support SE organizations during early AI adoption. Using a mixed-methods approach, we built and refined the framework through a literature review of organizational change models and thematic analysis of interview data, producing concrete, actionable steps. The framework comprises nine dimensions: AI Strategy Design, AI Strategy Evaluation, Collaboration, Communication, Governance and Ethics, Leadership, Organizational Culture, Organizational Dynamics, and Up-skilling, each supported by design principles and actions. To gather preliminary practitioner input, we conducted a survey (N=105) and two expert workshops (N=4). Survey results show that Up-skilling (15.2%) and AI Strategy Design (15.1%) received the highest $100-method allocations, underscoring their perceived importance in early AI initiatives. Findings indicate that organizations currently prioritize procedural elements such as strategy design, while human-centered guardrails remain less developed. Workshop feedback reinforced these patterns and emphasized the need to ground the framework in real-world practice. By identifying key behavioral dimensions and offering actionable guidance, this work provides a pragmatic roadmap for navigating the socio-technical complexity of early AI adoption and highlights future research directions for human-centric AI in SE.

</details>


### [25] [Automated Code Review Assignments: An Alternative Perspective of Code Ownership on GitHub](https://arxiv.org/abs/2512.05551)
*Jai Lal Lulla,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: GitHub的CODEOWNERS功能通过自动指定代码文件审查者来增强责任归属，但实际采用情况未知。本研究首次大规模实证分析CODEOWNERS使用情况，发现代码所有者遵守规则、促进更流畅的PR流程，并重新分配审查责任。


<details>
  <summary>Details</summary>
Motivation: 随着软件供应链攻击等外部威胁增加，确保代码责任归属和质量维护变得至关重要。GitHub在2017年引入CODEOWNERS功能来自动指定特定文件的审查者，但对其实际采用和实践情况了解甚少。

Method: 对超过844,000个拉取请求、190万条评论和200万次审查进行大规模实证研究，识别10,287名代码所有者并跟踪他们的审查活动。使用断点回归设计(RDD)分析CODEOWNERS采用对审查动态的影响。

Result: 代码所有者倾向于遵守CODEOWNERS文件中的规则，表现出与传统所有权指标相似的协作行为，但随时间推移促进更流畅、更快的PR工作流程。采用CODEOWNERS后，审查责任从核心开发者重新分配。

Conclusion: CODEOWNERS是改善软件治理和韧性的有前景但未充分利用的机制。项目可以利用这种替代所有权方法来增强开源开发中的安全性、责任归属和工作流程效率。

Abstract: Code ownership is central to ensuring accountability and maintaining quality in large-scale software development. Yet, as external threats such as software supply chain attacks on project health and quality assurance increase, mechanisms for assigning and enforcing responsibility have become increasingly critical. In 2017, GitHub introduced the CODEOWNERS feature, which automatically designates reviewers for specific files to strengthen accountability and protect critical parts of the codebase. Despite its potential, little is known about how CODEOWNERS is actually adopted and practiced. We present the first large-scale empirical study of CODEOWNERS usage across over 844,000 pull requests with 1.9 million comments and over 2 million reviews. We identify 10,287 code owners to track their review activities. Results indicate that codeowners tend to adhere the rules specified in the CODEOWNERS file, exhibit similar collaborative behaviours to traditional metrics of ownership, but tend to contribute to a smoother and faster PR workflow over time. Finally, using regression discontinuity design (RDD) analysis, we find that repositories adopting CODEOWNERS experience shifts in review dynamics, as ownership redistributes review responsibilities away from core developers. Our results position CODEOWNERS as a promising yet underutilized mechanism for improving software governance and resilience. We discuss how projects can leverage this alternative ownership method as a perspective to enhance security, accountability, and workflow efficiency in open-source development.

</details>


### [26] [Executing Discrete/Continuous Declarative Process Specifications via Complex Event Processing](https://arxiv.org/abs/2512.05653)
*Stefan Schönig,Leo Poss,Fabrizio Maria Maggi*

Main category: cs.SE

TL;DR: 提出基于复杂事件处理（CEP）的三层执行架构，实现混合声明式模型的实时执行与强制执行，解决传统BPM无法处理连续传感器数据的局限性


<details>
  <summary>Details</summary>
Motivation: 传统业务流程管理（BPM）专注于离散事件，无法在信息物理环境中整合关键的连续传感器数据。虽然使用信号时序逻辑（STL）的混合声明式规范可以解决这一限制，但现有工作仅限于监控和事后一致性检查，缺乏实时执行和强制执行能力。

Method: 提出基于复杂事件处理（CEP）的三层执行架构，将STL启发的谓词集成到执行流程中，使系统能够基于连续传感器行为主动触发活动并强制执行流程边界。

Result: 该架构实现了混合声明式模型的实时执行与强制执行，填补了混合规范与操作控制之间的鸿沟。

Conclusion: 基于CEP的执行架构成功解决了传统BPM在信息物理环境中的局限性，实现了混合声明式模型的实时操作控制，为连续传感器数据与离散事件的集成提供了有效解决方案。

Abstract: Traditional Business Process Management (BPM) focuses on discrete events and fails to incorporate critical continuous sensor data in cyber-physical environments. Hybrid declarative specifications, utilizing Signal Temporal Logic (STL), address this limitation by allowing constraints over both discrete events and real-valued signals. However, existing work has been limited to monitoring and post-hoc conformance checking. This paper introduces a novel Complex Event Processing (CEP)-based execution architecture that enables the real-time execution and enforcement of hybrid declarative models. Our three-layer approach integrates STL-inspired predicates into the execution flow, allowing the system to actively trigger activities and enforce process boundaries based on continuous sensor behavior. This approach bridges the gap between hybrid specification and operational control.

</details>


### [27] [MicroRacer: Detecting Concurrency Bugs for Cloud Service Systems](https://arxiv.org/abs/2512.05716)
*Zhiling Deng,Juepeng Wang,Zhuangbin Chen*

Main category: cs.SE

TL;DR: MicroRacer：一个非侵入式自动化框架，用于检测微服务环境中的并发bug，通过动态插桩库收集跟踪数据，分析happened-before关系和资源访问模式，并采用三阶段验证过程确认并发问题。


<details>
  <summary>Details</summary>
Motivation: 现代云应用采用微服务架构，端到端用户请求穿越多个不同服务和机器，存在复杂交互，使得云服务系统容易受到并发bug影响，现有检测方法因侵入性和无法处理微服务架构复杂性而不足。

Method: 通过动态插桩广泛使用的库在运行时收集详细跟踪数据，不修改应用代码；利用这些数据分析服务系统中常见操作的happened-before关系和资源访问模式；识别可疑并发操作并采用三阶段验证过程测试和确认并发bug。

Result: 在开源微服务基准测试和复现的工业bug上进行实验，证明了MicroRacer在准确检测和定位并发问题方面的有效性和效率。

Conclusion: MicroRacer为解决微服务环境中并发bug检测的挑战提供了一个非侵入式、自动化的有效解决方案。

Abstract: Modern cloud applications delivering global services are often built on distributed systems with a microservice architecture. In such systems, end-to-end user requests traverse multiple different services and machines, exhibiting intricate interactions. Consequently, cloud service systems are vulnerable to concurrency bugs, which pose significant challenges to their reliability. Existing methods for concurrency bug detection often fall short due to their intrusive nature and inability to handle the architectural complexities of microservices. To address these limitations, we propose MicroRacer, a non-intrusive and automated framework for detecting concurrency bugs in such environments. By dynamically instrumenting widely-used libraries at runtime, MicroRacer collects detailed trace data without modifying the application code. Such data are utilized to analyze the happened-before relationship and resource access patterns of common operations within service systems. Based on this information, MicroRacer identifies suspicious concurrent operations and employs a three-stage validation process to test and confirm concurrency bugs. Experiments on open-source microservice benchmarks with replicated industrial bugs demonstrate MicroRacer's effectiveness and efficiency in accurately detecting and pinpointing concurrency issues.

</details>


### [28] [Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models](https://arxiv.org/abs/2512.05887)
*Sairam Vaidya,Marcel Böhme,Loris D'Antoni*

Main category: cs.SE

TL;DR: Germinator：一种针对可扩展编译器（如MLIR）的方言无关且方言有效的语法引导模糊测试方法，通过自动提取方言语法并结合预训练大语言模型生成种子输入，显著提高测试覆盖率并发现大量未知bug。


<details>
  <summary>Details</summary>
Motivation: 现代可扩展编译器框架（如MLIR）虽然促进了领域特定语言方言的快速创建，但其灵活性使得正确性验证更加困难。现有测试生成方法要么需要为每个方言手动构建种子语料库，要么无法有效针对方言特定特性发现bug。

Method: 提出方言无关且方言有效的语法引导模糊测试方法：1）自动从方言规范中提取语法结构；2）结合预训练大语言模型自动生成覆盖完整方言空间的代表性多样化种子输入；3）使用这些种子引导覆盖率导向的模糊测试器。

Result: 在6个MLIR项目的91个方言上评估，Germinator生成的种子相比语法基线提高10-120%的行覆盖率。发现了88个先前未知的bug（40个已确认），其中23个在之前没有自动化测试生成器的方言中。

Conclusion: Germinator实现了对低资源方言的有效、可控大规模测试，通过自动提取语法和利用预训练大语言模型生成种子，解决了可扩展编译器测试中方言无关性和方言有效性的平衡问题。

Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.

</details>


### [29] [Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures](https://arxiv.org/abs/2512.05908)
*Amirkia Rafiei Oskooei,S. Selcan Yukcu,Mehmet Cevheri Bozoglan,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 提出一种基于自然语言摘要的微服务架构bug定位方法，通过将代码库转换为层次化NL摘要，进行NL-to-NL搜索而非跨模态检索，显著提升定位效果。


<details>
  <summary>Details</summary>
Motivation: 微服务架构中的bug定位面临挑战：自然语言bug报告与代码之间的语义鸿沟、LLM上下文限制、需要先确定正确仓库。现有方法如跨模态检索效果有限。

Method: 将代码库转换为层次化自然语言摘要（文件、目录、仓库级别），采用两阶段搜索：1) 将bug报告路由到相关仓库；2) 在选定仓库内进行自上而下的定位（仓库→目录→文件）。

Result: 在包含46个仓库、110万行代码的工业系统DNext上评估，Pass@10达到0.82，MRR达到0.50，显著优于检索基线以及GitHub Copilot、Cursor等代理RAG系统。

Conclusion: 工程化的自然语言表示比原始源代码更有效于可扩展的bug定位，提供可解释的仓库→目录→文件搜索路径，这对构建企业AI工具的信任至关重要。

Abstract: Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.

</details>
