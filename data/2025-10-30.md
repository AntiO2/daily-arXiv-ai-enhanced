<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives](https://arxiv.org/abs/2510.24943)
*Alfonso Ladino-Rincon,Stephen W. Nesbitt*

Main category: cs.DC

TL;DR: Radar DataTree是首个将WMO FM-301标准从单次雷达体扫描扩展到时间分辨、分析就绪档案的数据集级框架，通过开源架构将运营雷达档案转化为FAIR兼容的云优化数据集。


<details>
  <summary>Details</summary>
Motivation: 天气雷达数据是科学价值最高但结构利用不足的地球观测数据集之一，现有雷达档案分散、供应商特定且不符合FAIR原则，阻碍了大规模研究、可重复性和云原生计算。

Method: 基于FM-301/CfRadial 2.1标准，使用xarray DataTree将雷达体扫描组织为分层、元数据丰富的结构，并序列化为Zarr格式，结合Icechunk实现ACID兼容存储和版本控制。

Result: 在准垂直剖面(QVP)和降水累积工作流等案例研究中展示了显著的性能提升，所有工具和数据集通过Raw2Zarr仓库开放发布。

Conclusion: 这项工作为雷达数据管理、高性能地球科学和AI就绪天气基础设施提供了可重复和可扩展的基础。

Abstract: We introduce Radar DataTree, the first dataset-level framework that extends
the WMO FM-301 standard from individual radar volume scans to time-resolved,
analysis-ready archives. Weather radar data are among the most scientifically
valuable yet structurally underutilized Earth observation datasets. Despite
widespread public availability, radar archives remain fragmented,
vendor-specific, and poorly aligned with FAIR (Findable, Accessible,
Interoperable, Reusable) principles, hindering large-scale research,
reproducibility, and cloud-native computation. Radar DataTree addresses these
limitations with a scalable, open-source architecture that transforms
operational radar archives into FAIR-compliant, cloud-optimized datasets. Built
on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree,
Radar DataTree organizes radar volume scans as hierarchical, metadata-rich
structures and serializes them to Zarr for scalable analysis. Coupled with
Icechunk for ACID-compliant storage and versioning, this architecture enables
efficient, parallel computation across thousands of radar scans with minimal
preprocessing. We demonstrate significant performance gains in case studies
including Quasi-Vertical Profile (QVP) and precipitation accumulation
workflows, and release all tools and datasets openly via the Raw2Zarr
repository. This work contributes a reproducible and extensible foundation for
radar data stewardship, high-performance geoscience, and AI-ready weather
infrastructure.

</details>


### [2] [Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training](https://arxiv.org/abs/2510.25170)
*Kewei Wang,Claire Songhyun Lee,Sunwoo Lee,Vishu Gupta,Jan Balewski,Alex Sim,Peter Nugent,Ankit Agrawal,Alok Choudhary,Kesheng Wu,Wei-keng Liao*

Main category: cs.DC

TL;DR: 提出多分辨率模型融合方法，通过结合低分辨率训练模型与原始分辨率微调，显著减少神经网络训练时间，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络在科学研究中应用广泛，但训练高维大数据样本时计算成本高昂，需要高效的训练方法来降低计算开销。

Method: 多分辨率模型融合方法：先训练低分辨率数据模型，然后与原始分辨率数据结合进行微调，加速模型收敛过程。

Result: 在CosmoFlow和Neuron Inverter两个实际应用中，训练时间分别减少47%和44%，模型精度不受影响。

Conclusion: 多分辨率模型融合方法能显著降低端到端训练时间，同时保持模型的高分辨率洞察力，为科学计算提供高效训练方案。

Abstract: Neural networks are rapidly gaining popularity in scientific research, but
training the models is often very time-consuming. Particularly when the
training data samples are large high-dimensional arrays, efficient training
methodologies that can reduce the computational costs are crucial. To reduce
the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method
that combines models trained on reduced-resolution data and then refined with
data in the original resolution. We demonstrate that these reduced-resolution
models and datasets could be generated quickly. More importantly, the proposed
approach reduces the training time by speeding up the model convergence in each
fusion stage before switching to the final stage of finetuning with data in its
original resolution. This strategy ensures the final model retains
high-resolution insights while benefiting from the computational efficiency of
lower-resolution training. Our experiment results demonstrate that the
multi-resolution model fusion method can significantly reduce end-to-end
training time while maintaining the same model accuracy. Evaluated using two
real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed
method improves the training time by up to 47% and 44%, respectively, as
compared to the original resolution training, while the model accuracy is not
affected.

</details>


### [3] [MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference](https://arxiv.org/abs/2510.25258)
*Xinru Tang,Jingxiang Hou,Dingcheng Jiang,Taiquan Wei,Jiaxin Liu,Jinyi Deng,Huizheng Wang,Qize Yang,Haoran Shang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.DC

TL;DR: 本文针对晶圆级芯片(WSC)上运行MoE模型时的通信不平衡和专家迁移开销问题，提出了ER-Mapping和NI-Balancer两种优化方法，显著提升了MoE模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，MoE技术成为主流，但GPU集群中的跨节点通信开销限制了专家并行的应用。晶圆级芯片提供了高性能网络，但其网状拓扑导致通信压力不平衡，且缺乏片上磁盘导致专家迁移开销大。

Method: 提出了ER-Mapping方法，协同设计注意力层和MoE层的映射以平衡通信压力；提出了NI-Balancer方法，将完整的专家迁移分解为多个步骤，交替利用两层中的冷链路来隐藏迁移开销。

Result: ER-Mapping实现了高达62%的通信减少，NI-Balancer在MoE计算和通信方面分别带来了54%和22%的改进。相比SOTA的NVL72超级节点，WSC平台平均提供了39%更高的每设备MoE性能。

Conclusion: 通过ER-Mapping和NI-Balancer的协同优化，晶圆级芯片平台能够充分发挥其在MoE模型上的潜力，显著提升性能和可扩展性。

Abstract: As large language models (LLMs) continue to scale up, mixture-of-experts
(MoE) has become a common technology in SOTA models. MoE models rely on expert
parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all
communication to dispatch and combine tokens across devices. However, in
widely-adopted GPU clusters, high-overhead cross-node communication makes
all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips
(WSCs) have emerged as a platform integrating numerous devices on a wafer-sized
interposer. WSCs provide a unified high-performance network connecting all
devices, presenting a promising potential for hosting MoE models. Yet, their
network is restricted to a mesh topology, causing imbalanced communication
pressure and performance loss. Moreover, the lack of on-wafer disk leads to
high-overhead expert migration on the critical path.
  To fully unleash this potential, we first propose Entwined Ring Mapping
(ER-Mapping), which co-designs the mapping of attention and MoE layers to
balance communication pressure and achieve better performance. We find that
under ER-Mapping, the distribution of cold and hot links in the attention and
MoE layers is complementary. Therefore, to hide the migration overhead, we
propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert
migration into multiple steps and alternately utilizes the cold links of both
layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%.
NI-Balancer further delivers 54% and 22% improvements in MoE computation and
communication, respectively. Compared with the SOTA NVL72 supernode, the WSC
platform delivers an average 39% higher per-device MoE performance owing to its
scalability to larger EP.

</details>


### [4] [A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon](https://arxiv.org/abs/2510.25277)
*Simon Süwer,Mai Khanh Mai,Christoph Klein,Nicola Götzenberger,Denis Dalić,Andreas Maier,Jan Baumbach*

Main category: cs.DC

TL;DR: 提出一种多阶段安全AI训练方法，通过模拟临床知识图谱设计模型，在联邦学习框架中训练，保护患者隐私的同时实现医疗AI开发。


<details>
  <summary>Details</summary>
Motivation: 临床数据整合对个性化医疗发展至关重要，但GDPR法规限制了小规模罕见病队列数据的使用，需要保护隐私的AI训练方案。

Method: 四阶段方法：1) 在模拟临床知识图谱上设计模型；2) 在FeatureCloud联邦学习框架中准备模型；3) 在医院环境中训练真实数据；4) 执行验证评估脚本仅返回聚合性能指标。

Result: 在TUM.ai Makeathon 2024挑战中成功验证，50名学生无需访问真实数据即开发出患者分类和诊断模型。

Conclusion: 通过联邦学习框架部署安全算法是实现医疗领域隐私保护AI的实用途径。

Abstract: The integration of clinical data offers significant potential for the
development of personalized medicine. However, its use is severely restricted
by the General Data Protection Regulation (GDPR), especially for small cohorts
with rare diseases. High-quality, structured data is essential for the
development of predictive medical AI. In this case study, we propose a novel,
multi-stage approach to secure AI training: (1) The model is designed on a
simulated clinical knowledge graph (cKG). This graph is used exclusively to
represent the structural characteristics of the real cKG without revealing any
sensitive content. (2) The model is then integrated into the FeatureCloud (FC)
federated learning framework, where it is prepared in a single-client
configuration within a protected execution environment. (3) Training then takes
place within the hospital environment on the real cKG, either under the direct
supervision of hospital staff or via a fully automated pipeline controlled by
the hospital. (4) Finally, verified evaluation scripts are executed, which only
return aggregated performance metrics. This enables immediate performance
feedback without sensitive patient data or individual predictions, leaving the
clinic. A fundamental element of this approach involves the incorporation of a
cKG, which serves to organize multi-omics and patient data within the context
of real-world hospital environments. This approach was successfully validated
during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner
Children's Hospital (HCH-LMU): 50 students developed models for patient
classification and diagnosis without access to real data. Deploying secure
algorithms via federated frameworks, such as the FC framework, could be a
practical way of achieving privacy-preserving AI in healthcare.

</details>


### [5] [Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges](https://arxiv.org/abs/2510.25362)
*Georgios L. Stavrinides,Helen D. Karatza*

Main category: cs.DC

TL;DR: 该论文提出数据密集型工作负载的分类方法，并综述了大规模分布式系统中常用的调度技术，探讨了新的调度策略和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大数据爆炸性增长，工作负载变得更加复杂和计算密集，需要有效的调度技术来应对数据密集型应用在并行度、数据局部性、服务质量等方面的挑战。

Method: 提出数据密集型工作负载的分类方法，综述大规模分布式系统中常用的调度方法，介绍文献中提出的新策略。

Result: 提供了数据密集型工作负载的系统分类框架，总结了现有调度方法的优缺点，识别了当前面临的主要挑战。

Conclusion: 数据密集型应用的调度在大规模分布式系统中面临重大挑战，需要开发更有效的调度技术来应对不断增长的计算需求和服务质量要求。

Abstract: With the explosive growth of big data, workloads tend to get more complex and
computationally demanding. Such applications are processed on distributed
interconnected resources that are becoming larger in scale and computational
capacity. Data-intensive applications may have different degrees of parallelism
and must effectively exploit data locality. Furthermore, they may impose
several Quality of Service requirements, such as time constraints and
resilience against failures, as well as other objectives, like energy
efficiency. These features of the workloads, as well as the inherent
characteristics of the computing resources required to process them, present
major challenges that require the employment of effective scheduling
techniques. In this chapter, a classification of data-intensive workloads is
proposed and an overview of the most commonly used approaches for their
scheduling in large-scale distributed systems is given. We present novel
strategies that have been proposed in the literature and shed light on open
challenges and future directions.

</details>


### [6] [Can Like Attract Like? A Study of Homonymous Gathering in Networks](https://arxiv.org/abs/2510.25451)
*Stéphane Devismes,Yoann Dieudonné,Arnaud Labourel*

Main category: cs.DC

TL;DR: 该论文研究了移动代理在分布式网络中的聚集问题，特别关注当代理标签可能重复时的情况。作者完全刻画了可聚集团队的特征，设计了多项式时间算法，并证明了所需公共知识的最优性。


<details>
  <summary>Details</summary>
Motivation: 传统确定性聚集算法假设代理具有互不相同的标签，但现实中标签可能重复。本文旨在探索在标签可能重复的情况下，哪些团队能够被确定性聚集，以及如何高效实现聚集。

Method: 作者首先给出了可聚集团队的完整特征刻画，然后设计了一个时间复杂度为poly(n,logλ)的算法，该算法仅需要代理初始共享O(log log log μ)比特的公共知识。

Result: 论文完全刻画了可聚集团队的特征，提出的算法能够在多项式时间内聚集所有可聚集团队，且所需的公共知识几乎是最优的。作为副产品，还得到了第一个无需公共知识、在标签互异时能在多项式时间内聚集任意规模团队的确定性算法。

Conclusion: 研究表明标签重复并不必然阻碍确定性聚集，通过精心设计的算法和少量公共知识，可以在多项式时间内解决聚集问题。终止检测是扩展算法到任意团队规模时的主要挑战，相关技术具有独立价值。

Abstract: A team of mobile agents, starting from distinct nodes of a network, have to
meet at the same node and declare that they all met. Agents execute the same
algorithm, which they start when activated by an adversary or by an agent
entering their initial node. When activated, agents traverse edges of the
network in synchronous rounds. Their perception and communication are strictly
local. This task, known as gathering, is a central problem in distributed
mobile systems. Most prior work focuses on minimizing its time complexity,
i.e., the worst-case number of rounds between the start of the earliest agent
and the task completion. To break possible symmetries, deterministic solutions
typically assume that agents have pairwise distinct IDs, called labels, known
only to themselves. But must all labels be pairwise distinct to guarantee
deterministic gathering?
  We address this question by considering agents that may share the same label.
A team L is said to be gatherable if, for every initial setting of L, there is
an algorithm that solves gathering. Our contribution is threefold. (1) We give
a full characterization of the gatherable teams. (2) We design an algorithm
that gathers all of them in poly$(n,\log\lambda)$ time, where $n$ (resp.
$\lambda$) is the graph order (resp. the smallest label in L). This algorithm
requires the agents to initially share only $O(\log \log \log \mu)$ bits of
common knowledge, where $\mu$ is the largest label multiplicity in L. (3) We
show this dependency is almost optimal to get a poly$(n,\log\lambda)$-time
complexity.
  As a by-product, we get the first deterministic poly$(n,\log\lambda)$-time
algorithm requiring no common knowledge to gather any team when all labels are
distinct. Known to be achievable for two-agent teams, extending this to any
team size faced a major challenge: termination detection. Our techniques to
address it may be of independent interest.

</details>


### [7] [Holon Streaming: Global Aggregations with Windowed CRDTs](https://arxiv.org/abs/2510.25757)
*Jonas Spenger,Kolya Krafeld,Ruben van Gemeren,Philipp Haller,Paris Carbone*

Main category: cs.DC

TL;DR: Holon Streaming是一个用于全局聚合的精确一次流处理系统，通过窗口化无冲突复制数据类型（Windowed CRDTs）实现可扩展的全局聚合计算，采用去中心化协调机制显著降低延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有系统在全局聚合计算时采用单任务实例或静态聚合树，存在可扩展性限制、瓶颈问题、端到端延迟受最慢路径影响，以及故障和重配置导致的大延迟峰值等问题。

Method: 提出确定性编程模型，使用窗口化无冲突复制数据类型（Windowed CRDTs）作为共享复制状态的新抽象，支持去中心化协调的高效故障恢复算法。

Result: 与现有流处理系统相比，在全局聚合工作负载上实现5倍延迟降低和2倍吞吐量提升，在故障场景下延迟减少11倍。

Conclusion: 证明了确定性去中心化协调的有效性，以及Windowed CRDTs在全局聚合中的实用性。

Abstract: Scaling global aggregations is a challenge for exactly-once stream processing
systems. Current systems implement these either by computing the aggregation in
a single task instance, or by static aggregation trees, which limits
scalability and may become a bottleneck. Moreover, the end-to-end latency is
determined by the slowest path in the tree, and failures and reconfiguration
cause large latency spikes due to the centralized coordination. Towards these
issues, we present Holon Streaming, an exactly-once stream processing system
for global aggregations. Its deterministic programming model uses windowed
conflict-free replicated data types (Windowed CRDTs), a novel abstraction for
shared replicated state. Windowed CRDTs make computing global aggregations
scalable. Furthermore, their guarantees such as determinism and convergence
enable the design of efficient failure recovery algorithms by decentralized
coordination. Our evaluation shows a 5x lower latency and 2x higher throughput
than an existing stream processing system on global aggregation workloads, with
an 11x latency reduction under failure scenarios. The paper demonstrates the
effectiveness of decentralized coordination with determinism, and the utility
of Windowed CRDTs for global aggregations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [8] [ODataX: A Progressive Evolution of the Open Data Protocol](https://arxiv.org/abs/2510.24761)
*Anirudh Ganesh,Nitin Sood*

Main category: cs.DB

TL;DR: 本文分析了OData协议在企业环境外采用受限的原因，并提出了ODataX解决方案，在保持向后兼容的同时通过简化查询语法、性能防护和缓存增强来促进更广泛采用。


<details>
  <summary>Details</summary>
Motivation: OData作为构建RESTful API的标准协议，虽然功能强大且成熟，但主要局限于微软和SAP等企业生态系统，未能获得更广泛的采用。本文旨在识别阻碍其广泛采用的关键障碍。

Method: 提出了ODataX协议，在保持OData v4向后兼容性的基础上，引入渐进式复杂度披露（简化查询语法）、内置性能防护（查询成本估算）和增强缓存机制。

Result: ODataX协议成功解决了原OData协议在企业环境外采用受限的问题，通过平衡企业级查询标准化与现代Web开发对简单性的需求。

Conclusion: ODataX在保持OData强大功能的同时，通过简化使用和性能优化，有望弥合企业级API标准与现代Web开发实践之间的差距，促进更广泛采用。

Abstract: The Open Data Protocol (OData) provides a standardized approach for building
and consuming RESTful APIs with rich query capabilities. Despite its power and
maturity, OData adoption remains confined primarily to enterprise environments,
particularly within Microsoft and SAP ecosystems. This paper analyzes the key
barriers preventing wider OData adoption and introduces ODataX, an evolved
version of the protocol designed to address these limitations. ODataX maintains
backward compatibility with OData v4 while introducing progressive complexity
disclosure through simplified query syntax, built-in performance guardrails via
query cost estimation, and enhanced caching mechanisms. This work aims to
bridge the gap between enterprise-grade query standardization and the
simplicity demanded by modern web development practices.

</details>


### [9] [StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems](https://arxiv.org/abs/2510.25017)
*Qi Lin,Zhenyu Zhang,Viraj Thakkar,Zhenjie Sun,Mai Zheng,Zhichao Cao*

Main category: cs.DB

TL;DR: StorageXTuner是一个基于LLM代理的存储系统自动调优框架，通过四个代理组件实现异构存储引擎的智能调优，相比现有方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 存储系统自动配置困难，参数空间大且工作负载条件多变。现有启发式和ML调优器通常系统特定，需要手动调整，且在系统变化时性能下降。

Method: 采用四代理架构：Executor（沙盒基准测试）、Extractor（性能摘要）、Searcher（基于洞察的配置探索）、Reflector（洞察生成和管理）。结合洞察驱动的树搜索和分层内存，使用轻量级检查器防止不安全操作。

Result: 在RocksDB、LevelDB、CacheLib和MySQL InnoDB上测试，相比默认设置和ELMo-Tune，吞吐量最高提升575%和111%，p99延迟降低88%和56%，收敛所需试验次数更少。

Conclusion: StorageXTuner框架有效解决了存储系统自动调优的挑战，实现了跨系统的可重用性和更好的性能优化。

Abstract: Automatically configuring storage systems is hard: parameter spaces are large
and conditions vary across workloads, deployments, and versions. Heuristic and
ML tuners are often system specific, require manual glue, and degrade under
changes. Recent LLM-based approaches help but usually treat tuning as a
single-shot, system-specific task, which limits cross-system reuse, constrains
exploration, and weakens validation. We present StorageXTuner, an LLM
agent-driven auto-tuning framework for heterogeneous storage engines.
StorageXTuner separates concerns across four agents - Executor (sandboxed
benchmarking), Extractor (performance digest), Searcher (insight-guided
configuration exploration), and Reflector (insight generation and management).
The design couples an insight-driven tree search with layered memory that
promotes empirically validated insights and employs lightweight checkers to
guard against unsafe actions. We implement a prototype and evaluate it on
RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C.
Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up
to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and
56%, and converges with fewer trials.

</details>


### [10] [Time-varying Vector Field Compression with Preserved Critical Point Trajectories](https://arxiv.org/abs/2510.25143)
*Mingze Xia,Yuxiao Li,Pu Jiao,Bei Wang,Xin Liang,Hanqi Guo*

Main category: cs.DB

TL;DR: 提出了一种高效的时变向量场有损压缩框架，能够精确保留所有临界点轨迹，解决了现有方法会扭曲这一关键特征的问题。


<details>
  <summary>Details</summary>
Motivation: 科学模拟和观测产生大量时变向量场数据，存储和传输困难。有损压缩是减少数据量的有效方法，但现有方法会扭曲临界点轨迹这一关键特征。

Method: 扩展了空间临界点保护理论到时空临界点轨迹保护，提出半拉格朗日预测器利用平流主导区域的时空相关性，结合传统Lorenzo预测器提高压缩效率。

Result: 在四个真实科学数据集上测试，压缩比高达124.48倍，比最佳无损压缩器高56.07倍，且能有效保留所有临界点轨迹。

Conclusion: 该方法在保持高压缩比的同时，成功保护了时变向量场中的关键特征——临界点轨迹，优于现有有损和无损压缩方法。

Abstract: Scientific simulations and observations are producing vast amounts of
time-varying vector field data, making it hard to store them for archival
purposes and transmit them for analysis. Lossy compression is considered a
promising approach to reducing these data because lossless compression yields
low compression ratios that barely mitigate the problem. However, directly
applying existing lossy compression methods to timevarying vector fields may
introduce undesired distortions in critical-point trajectories, a crucial
feature that encodes key properties of the vector field. In this work, we
propose an efficient lossy compression framework that exactly preserves all
critical-point trajectories in time-varying vector fields. Our contributions
are threefold. First, we extend the theory for preserving critical points in
space to preserving critical-point trajectories in space-time, and develop a
compression framework to realize the functionality. Second, we propose a
semi-Lagrange predictor to exploit the spatiotemporal correlations in
advectiondominated regions, and combine it with the traditional Lorenzo
predictor for improved compression efficiency. Third, we evaluate our method
against state-of-the-art lossy and lossless compressors using four real-world
scientific datasets. Experimental results demonstrate that the proposed method
delivers up to 124.48X compression ratios while effectively preserving all
critical-point trajectories. This compression ratio is up to 56.07X higher than
that of the best lossless compressors, and none of the existing lossy
compressors can preserve all critical-point trajectories at similar compression
ratios.

</details>


### [11] [DGAI: Decoupled On-Disk Graph-Based ANN Index for Efficient Updates and Queries](https://arxiv.org/abs/2510.25401)
*Jiahao Lou,Quan Yu,Shufeng Gong,Song Yu,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: 提出了一种解耦存储架构来改进基于图的近似最近邻搜索系统，通过三阶段查询机制和增量页面级拓扑重排序策略，显著提高了更新速度和查询效率。


<details>
  <summary>Details</summary>
Motivation: 传统的耦合存储方法在更新图拓扑时会产生大量冗余向量读写，导致无效I/O，影响索引更新效率。

Method: 采用解耦存储架构，设计三阶段查询机制利用多个PQ压缩向量过滤无效I/O和计算，以及增量页面级拓扑重排序策略将新节点插入到包含最相似邻居的页面中。

Result: 解耦架构使插入速度提升10.05倍，删除速度提升6.89倍；三阶段查询和增量重排序使查询效率相比传统耦合架构提升2.66倍。

Conclusion: 解耦存储架构结合优化的查询和重排序策略，能有效减少ANN搜索中的I/O和计算开销，显著提升系统性能。

Abstract: On-disk graph-based indexes are widely used in approximate nearest neighbor
(ANN) search systems for large-scale, high-dimensional vectors. However,
traditional coupled storage methods, which store vectors within the index, are
inefficient for index updates. Coupled storage incurs excessive redundant
vector reads and writes when updating the graph topology, leading to
significant invalid I/O. To address this issue, we propose a decoupled storage
architecture. While a decoupled architecture reduces query performance. To
overcome this limitation, we design two tailored strategies: (i) a three-stage
query mechanism that leverages multiple PQ compressed vectors to filter invalid
I/O and computations, and (ii) an incremental page-level topological reordering
strategy that incrementally inserts new nodes into pages containing their most
similar neighbors to mitigate read amplification. Together, these techniques
substantially reduce both I/O and computational overhead during ANN search.
Experimental results show that the decoupled architecture improves update speed
by 10.05x for insertions and 6.89x for deletions, while the three-stage query
and incremental reordering enhance query efficiency by 2.66x compared to the
traditional coupled architecture.

</details>


### [12] [One Join Order Does Not Fit All: Reducing Intermediate Results with Per-Split Query Plans](https://arxiv.org/abs/2510.25684)
*Yujun He,Hangdong Zhao,Simon Frisk,Yifei Yang,Kevin Kristensen,Paraschos Koutris,Xiangyao Yu*

Main category: cs.DB

TL;DR: SplitJoin框架通过引入split作为一等查询操作符，将输入表分区为重和轻两部分，允许不同数据分区使用不同的查询计划，以减少多连接查询中的中间结果大小。


<details>
  <summary>Details</summary>
Motivation: 最小化中间结果对于高效的多连接查询处理至关重要。虽然Yannakakis算法为无环查询提供了强保证，但循环查询仍然是一个开放挑战。

Method: 提出SplitJoin框架，通过分区输入表为重和轻部分，使不同数据分区能够使用不同的查询计划，利用现有的二元连接引擎减少中间结果大小。系统探索了基于分割优化的设计空间，包括阈值选择、分割策略和分割后的连接排序。

Result: 在DuckDB上，SplitJoin完成了43个社交网络查询（原生为29个），平均运行时间快2.1倍，中间结果小7.9倍（最高分别达13.6倍和74倍）；在Umbra上，完成了45个查询（原生为35个），平均速度提升1.3倍，中间结果小1.2倍（最高分别达6.1倍和2.1倍）。

Conclusion: SplitJoin框架通过引入分割操作符，有效减少了多连接查询中的中间结果大小，显著提升了查询性能，特别是在处理循环查询时表现出色。

Abstract: Minimizing intermediate results is critical for efficient multi-join query
processing. Although the seminal Yannakakis algorithm offers strong guarantees
for acyclic queries, cyclic queries remain an open challenge. In this paper, we
propose SplitJoin, a framework that introduces split as a first-class query
operator. By partitioning input tables into heavy and light parts, SplitJoin
allows different data partitions to use distinct query plans, with the goal of
reducing intermediate sizes using existing binary join engines. We
systematically explore the design space for split-based optimizations,
including threshold selection, split strategies, and join ordering after
splits. Implemented as a front-end to DuckDB and Umbra, SplitJoin achieves
substantial improvements: on DuckDB, SplitJoin completes 43 social network
queries (vs. 29 natively), achieving 2.1x faster runtime and 7.9x smaller
intermediates on average (up to 13.6x and 74x, respectively); on Umbra, it
completes 45 queries (vs. 35), achieving 1.3x speedups and 1.2x smaller
intermediates on average (up to 6.1x and 2.1x, respectively).

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification](https://arxiv.org/abs/2510.24749)
*Aofan Liu,Shiyuan Song,Haoxuan Li,Cehao Yang,Yiyan Qi*

Main category: cs.SE

TL;DR: RepoAlign-Bench是首个专门评估变更请求驱动场景下仓库级代码检索的基准，包含52k标注实例。ReflectCode通过对抗性反射增强的双塔架构，在Top-5准确率和召回率上分别比最先进方法提升12.2%和7.1%。


<details>
  <summary>Details</summary>
Motivation: 现代代码库日益复杂，需要能够理解跨组件变更意图的检索系统，而传统的函数级搜索范式缺乏这种能力。虽然近期研究改进了自然语言查询与代码片段的匹配，但针对特定变更请求检索上下文相关代码仍未被充分探索。

Method: 提出ReflectCode，一种对抗性反射增强的双塔架构，包含解耦的代码编码器和文档编码器组件。通过大语言模型引导的反射，动态整合语法模式、函数依赖关系和语义扩展意图。

Result: 综合实验表明，ReflectCode在Top-5准确率上比最先进基线提升12.2%，在召回率上提升7.1%。

Conclusion: 为上下文感知的代码检索确立了新方向，将检索范式从函数中心匹配转向整体仓库级推理。

Abstract: The escalating complexity of modern codebases has intensified the need for
retrieval systems capable of interpreting cross-component change intents, a
capability fundamentally absent in conventional function-level search
paradigms. While recent studies have improved the alignment between natural
language queries and code snippets, retrieving contextually relevant code for
specific change requests remains largely underexplored. To address this gap, we
introduce RepoAlign-Bench, the first benchmark specifically designed to
evaluate repository-level code retrieval under change request driven scenarios,
encompassing 52k annotated instances. This benchmark shifts the retrieval
paradigm from function-centric matching to holistic repository-level reasoning.
Furthermore, we propose ReflectCode, an adversarial reflection augmented
dual-tower architecture featuring disentangled code_encoder and doc_encoder
components. ReflectCode dynamically integrates syntactic patterns, function
dependencies, and semantic expansion intents through large language model
guided reflection. Comprehensive experiments demonstrate that ReflectCode
achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over
state-of-the-art baselines, establishing a new direction for context-aware code
retrieval.

</details>


### [14] [Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering](https://arxiv.org/abs/2510.24799)
*Filipe R. Cogo,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Compiler.next是一个基于搜索的新型编译器，能够将人类意图自动转换为可工作的软件，通过动态优化认知架构和参数，在准确性、成本和延迟等多个目标间寻找最优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助软件工程工具存在认知过载、工具集成效率低和AI副驾驶能力有限等问题，需要新的范式来推动软件工程3.0时代的发展。

Method: 提出搜索型编译器架构，通过动态优化认知架构组件（如提示词、基础模型配置和系统参数），在多个目标间进行权衡搜索来生成最优软件解决方案。

Result: 构建了Compiler.next的架构框架，将其定位为降低非专家技术门槛、实现可扩展、自适应和可靠AI驱动软件的关键技术。

Conclusion: 该研究为完全自动化、搜索驱动的软件开发奠定了基础，将促进更快的创新和更高效的AI驱动系统发展。

Abstract: The rapid advancement of AI-assisted software engineering has brought
transformative potential to the field of software engineering, but existing
tools and paradigms remain limited by cognitive overload, inefficient tool
integration, and the narrow capabilities of AI copilots. In response, we
propose Compiler.next, a novel search-based compiler designed to enable the
seamless evolution of AI-native software systems as part of the emerging
Software Engineering 3.0 era. Unlike traditional static compilers,
Compiler.next takes human-written intents and automatically generates working
software by searching for an optimal solution. This process involves dynamic
optimization of cognitive architectures and their constituents (e.g., prompts,
foundation model configurations, and system parameters) while finding the
optimal trade-off between several objectives, such as accuracy, cost, and
latency. This paper outlines the architecture of Compiler.next and positions it
as a cornerstone in democratizing software development by lowering the
technical barrier for non-experts, enabling scalable, adaptable, and reliable
AI-powered software. We present a roadmap to address the core challenges in
intent compilation, including developing quality programming constructs,
effective search heuristics, reproducibility, and interoperability between
compilers. Our vision lays the groundwork for fully automated, search-driven
software development, fostering faster innovation and more efficient AI-driven
systems.

</details>


### [15] [A Roadmap for Tamed Interactions with Large Language Models](https://arxiv.org/abs/2510.24819)
*Vincenzo Scotti,Jan Keim,Tobias Hey,Andreas Metzger,Anne Koziolek,Raffaela Mirandola*

Main category: cs.SE

TL;DR: 提出LLM脚本语言(LSL)的概念，旨在通过领域特定语言来规范和控制大语言模型的输出，提高AI应用的可靠性、鲁棒性和可信度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用虽然令人印象深刻，但其不可靠性和产生幻觉内容的倾向阻碍了实际应用。需要软件工程工具来约束LLM输出，提供更强的保证。

Method: 开发领域特定语言(DSL)来编写与LLM的交互脚本，控制LLM输出、强制交互结构化，并与验证、验证和可解释性集成。

Result: 提出了LSL的愿景框架，使LLM交互可编程，并与训练或实现解耦。

Conclusion: LSL可能是改进基于AI应用的关键，通过使LLM交互可编程来解决当前工具碎片化问题，提高LLM软件的可靠性、鲁棒性和可信度。

Abstract: We are witnessing a bloom of AI-powered software driven by Large Language
Models (LLMs). Although the applications of these LLMs are impressive and
seemingly countless, their unreliability hinders adoption. In fact, the
tendency of LLMs to produce faulty or hallucinated content makes them
unsuitable for automating workflows and pipelines. In this regard, Software
Engineering (SE) provides valuable support, offering a wide range of formal
tools to specify, verify, and validate software behaviour. Such SE tools can be
applied to define constraints over LLM outputs and, consequently, offer
stronger guarantees on the generated content. In this paper, we argue that the
development of a Domain Specific Language (DSL) for scripting interactions with
LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based
applications. Currently, LLMs and LLM-based software still lack reliability,
robustness, and trustworthiness, and the tools or frameworks to cope with these
issues suffer from fragmentation. In this paper, we present our vision of LSL.
With LSL, we aim to address the limitations above by exploring ways to control
LLM outputs, enforce structure in interactions, and integrate these aspects
with verification, validation, and explainability. Our goal is to make LLM
interaction programmable and decoupled from training or implementation.

</details>


### [16] [VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus](https://arxiv.org/abs/2510.25015)
*Chuyue Sun,Yican Sun,Daneshvar Amrollahi,Ethan Zhang,Shuvendu Lahiri,Shan Lu,David Dill,Clark Barrett*

Main category: cs.SE

TL;DR: VeriStruct是一个扩展AI辅助自动验证能力的框架，从单函数验证扩展到复杂数据结构模块验证，在Verus中成功验证了128/129个函数（99.2%）。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLMs在理解Verus注释语法和验证特定语义时的困难，以及将AI辅助验证从单函数扩展到复杂数据结构模块的需求。

Method: 使用规划器模块系统生成抽象、类型不变量、规范和证明代码，在提示中嵌入语法指导，并包含修复阶段自动纠正注释错误。

Result: 在11个Rust数据结构模块评估中，成功验证了10个模块，总计128/129个函数（99.2%成功率）。

Conclusion: VeriStruct代表了向自动AI辅助形式验证目标迈出的重要一步，展示了在复杂数据结构验证中的有效性。

Abstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated
verification from single functions to more complex data structure modules in
Verus. VeriStruct employs a planner module to orchestrate the systematic
generation of abstractions, type invariants, specifications, and proof code. To
address the challenge that LLMs often misunderstand Verus' annotation syntax
and verification-specific semantics, VeriStruct embeds syntax guidance within
prompts and includes a repair stage to automatically correct annotation errors.
In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on
ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in
total. These results represent an important step toward the goal of automatic
AI-assisted formal verification.

</details>


### [17] [Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study](https://arxiv.org/abs/2510.25016)
*Mateen Ahmed Abbasi,Petri Ihantola,Tommi Mikkonen,Niko Mäkitalo*

Main category: cs.SE

TL;DR: 该研究提出了Human-AI RE协同模型(HARE-SM)，一个将AI驱动分析与人类监督相结合的概念框架，旨在改进需求工程中的需求获取、分析和验证过程。


<details>
  <summary>Details</summary>
Motivation: 传统需求工程依赖劳动密集型手动流程，容易出错且复杂。AI技术（特别是大语言模型、自然语言处理和生成式AI）提供了变革性解决方案，但同时也带来了算法偏见、缺乏可解释性和伦理问题等挑战。

Method: 采用多阶段研究方法，包括准备需求工程数据集、微调AI模型，以及设计协作式人机工作流程。研究提出了HARE-SM概念框架和早期原型实现。

Result: 建立了研究议程和实际设计方向，为在协作环境中应用智能数据科学技术处理半结构化和非结构化需求工程数据奠定了基础。

Conclusion: HARE-SM框架通过强调透明度、可解释性和偏见缓解，促进了需求工程中AI的伦理使用，为人机协作的需求工程实践提供了理论框架和实现路径。

Abstract: The future of Requirements Engineering (RE) is increasingly driven by
artificial intelligence (AI), reshaping how we elicit, analyze, and validate
requirements. Traditional RE is based on labor-intensive manual processes prone
to errors and complexity. AI-powered approaches, specifically large language
models (LLMs), natural language processing (NLP), and generative AI, offer
transformative solutions and reduce inefficiencies. However, the use of AI in
RE also brings challenges like algorithmic bias, lack of explainability, and
ethical concerns related to automation. To address these issues, this study
introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that
integrates AI-driven analysis with human oversight to improve requirements
elicitation, analysis, and validation. The model emphasizes ethical AI use
through transparency, explainability, and bias mitigation. We outline a
multi-phase research methodology focused on preparing RE datasets, fine-tuning
AI models, and designing collaborative human-AI workflows. This preliminary
study presents the conceptual framework and early-stage prototype
implementation, establishing a research agenda and practical design direction
for applying intelligent data science techniques to semi-structured and
unstructured RE data in collaborative environments.

</details>


### [18] [Automating Benchmark Design](https://arxiv.org/abs/2510.25039)
*Amanda Dsouza,Harit Vishwakarma,Zhengyang Qi,Justin Bauer,Derek Pham,Thomas Walshe,Armin Parchami,Frederic Sala,Paroma Varma*

Main category: cs.SE

TL;DR: BeTaL框架利用LLM自动化动态基准设计，通过参数化基准模板和LLM推理来获得目标属性（如难度和真实性），显著提高了基准设计的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的手工静态基准评估方法容易饱和，而动态基准虽然能随模型发展而进化，但创建和更新成本高昂。

Method: BeTaL框架参数化基准模板的关键设计选择，使用LLM推理参数空间，以成本效益的方式获得目标属性。

Result: 在三个任务和多个目标难度级别上的评估显示，BeTaL产生的基准与期望难度更接近，平均偏差为5.3%到13.2%，比基线方法提高了2-4倍。

Conclusion: BeTaL为动态基准设计提供了一种高效自动化方法，显著改善了基准评估的准确性和适应性。

Abstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents
has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are
the primary tool for assessing model capabilities, but these quickly become
saturated. In contrast, dynamic benchmarks evolve alongside the models they
evaluate, but are expensive to create and continuously update. To address these
challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a
framework that leverages environment design principles to automate the process
of dynamic benchmark design. BeTaL works by parameterizing key design choices
in base benchmark templates and uses LLMs to reason through the resulting
parameter space to obtain target properties (such as difficulty and realism) in
a cost-efficient manner. We validate this approach on its ability to create
benchmarks with desired difficulty levels. Using BeTaL, we create two new
benchmarks and extend a popular agentic benchmark {\tau} -bench. Extensive
evaluation on these three tasks and multiple target difficulty levels shows
that BeTaL produces benchmarks much closer to the desired difficulty, with
average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the
baselines.

</details>


### [19] [Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection](https://arxiv.org/abs/2510.25057)
*Robin Maisch,Larissa Schmid,Timur Sağlam,Nils Niehues*

Main category: cs.SE

TL;DR: 提出了一种基于代码属性图和图变换的新框架，用于增强现有抄袭检测系统对抗重构混淆攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 编程教育中的抄袭检测面临日益复杂的混淆技术挑战，特别是自动化重构攻击。现有系统对基本混淆有抵抗力，但难以应对保持程序行为的结构性修改。

Method: 利用代码属性图和图变换构建可扩展框架，增强最先进的检测器对抗重构混淆的能力。

Result: 在真实学生提交代码上的综合评估显示，该框架在检测抄袭代码方面有显著改进，能够有效对抗算法和AI驱动的混淆攻击。

Conclusion: 该框架为编程教育中的抄袭检测提供了对抗重构混淆攻击的有效解决方案。

Abstract: Plagiarism detection in programming education faces growing challenges due to
increasingly sophisticated obfuscation techniques, particularly automated
refactoring-based attacks. While code plagiarism detection systems used in
education practice are resilient against basic obfuscation, they struggle
against structural modifications that preserve program behavior, especially
caused by refactoring-based obfuscation. This paper presents a novel and
extensible framework that enhances state-of-the-art detectors by leveraging
code property graphs and graph transformations to counteract refactoring-based
obfuscation. Our comprehensive evaluation of real-world student submissions,
obfuscated using both algorithmic and AI-based obfuscation attacks,
demonstrates a significant improvement in detecting plagiarized code.

</details>


### [20] [Adaptive Proof Refinement with LLM-Guided Strategy Selection](https://arxiv.org/abs/2510.25103)
*Minghai Lu,Zhe Zhou,Danning Xie,Songlin Jia,Benjamin Delaware,Tianyi Zhang*

Main category: cs.SE

TL;DR: Adapt是一个基于LLM的动态证明精炼框架，通过LLM引导的决策器根据证明状态和错误上下文动态选择精炼策略，显著提升了定理证明的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有形式化验证方法需要大量人工努力，而LLM生成的证明经常首次尝试就出错，且现有精炼方法使用固定策略，无法根据具体证明问题动态调整。

Method: 提出Adapt框架，使用LLM引导的决策器根据证明助手状态和错误证明的上下文信息，动态选择合适的精炼策略。

Result: 在两个基准测试中，Adapt比最佳基线方法分别多证明了16.63%和18.58%的定理，并在五个不同LLM上展示了良好的泛化能力。

Conclusion: Adapt框架通过动态策略选择显著提升了定理证明的成功率，证明了基于LLM的动态精炼方法的有效性。

Abstract: Formal verification via theorem proving enables the expressive specification
and rigorous proof of software correctness, but it is difficult to scale due to
the significant manual effort and expertise required. While Large Language
Models (LLMs) show potential in proof generation, they frequently produce
incorrect proofs on the first attempt and require additional strategies for
iterative refinement. However, existing approaches employ fixed refinement
strategies and cannot dynamically choose an effective strategy based on the
particular issues in a generated proof, which limits their performance. To
overcome this limitation, we introduce Adapt, a novel proof refinement
framework that leverages an LLM-guided decision-maker to dynamically select a
suitable refinement strategy according to the state of the proof assistant and
available context of an incorrect proof. We evaluate Adapt on two benchmarks
against four existing methods and find that it significantly outperforms the
best baseline on both by proving 16.63% and 18.58% more theorems, respectively.
Furthermore, we demonstrate Adapt's generalizability by evaluating it across
five different LLMs. We also conduct ablation studies to measure the
contribution of each component and compare the trade-offs of alternative
decision-maker designs.

</details>


### [21] [Automated Program Repair Based on REST API Specifications Using Large Language Models](https://arxiv.org/abs/2510.25148)
*Katsuki Yamagishi,Norihiro Yoshida,Erina Makihara,Katsuro Inoue*

Main category: cs.SE

TL;DR: dcFix是一个自动检测和修复REST API误用的方法，通过识别不符合规范的代码片段，结合API规范生成提示，利用大语言模型生成修正代码。


<details>
  <summary>Details</summary>
Motivation: 开发者在测试阶段才能发现REST API规范违反问题，错误信息缺乏有效诊断细节，调试需要反复试错。

Method: 识别不符合规范的代码片段，将其与相关API规范整合到提示中，利用大语言模型生成修正代码。

Result: dcFix能够准确检测API误用，并且在性能上优于基线方法（基线方法在提示中不包含代码片段不符合REST API规范的指示）。

Conclusion: dcFix方法有效解决了REST API误用检测和自动修复的问题，通过结合代码片段和API规范的大语言模型提示，显著提升了修复效果。

Abstract: Many cloud services provide REST API accessible to client applications.
However, developers often identify specification violations only during
testing, as error messages typically lack the detail necessary for effective
diagnosis. Consequently, debugging requires trial and error. This study
proposes dcFix, a method for detecting and automatically repairing REST API
misuses in client programs. In particular, dcFix identifies non-conforming code
fragments, integrates them with the relevant API specifications into prompts,
and leverages a Large Language Model (LLM) to produce the corrected code. Our
evaluation demonstrates that dcFix accurately detects misuse and outperforms
the baseline approach, in which prompts to the LLM omit any indication of code
fragments non conforming to REST API specifications.

</details>


### [22] [Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models](https://arxiv.org/abs/2510.25195)
*Shuochuan Li,Zan Wang,Xiaoning Du,Zhuo Wu,Jiuqiao Yu,Junjie Chen*

Main category: cs.SE

TL;DR: KUMIC是一个基于上下文学习的多意图代码注释生成框架，通过检索机制和思维链优化LLM的知识利用，显著提升注释生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统代码注释生成只能提供通用概述，无法满足开发者对实现洞察和用户对使用说明的多样化需求，因此需要多意图注释生成。

Method: KUMIC框架：1) 设计检索机制获取代码-注释一致性高的演示示例；2) 利用思维链引导LLM关注与特定意图对齐的代码语句；3) 构建从代码到意图特定语句再到注释的映射知识链。

Result: KUMIC在BLEU、METEOR、ROUGE-L和SBERT指标上分别比最先进基线方法提升了14.49%、22.41%、20.72%和12.94%。

Conclusion: KUMIC通过优化知识利用和构建映射知识链，有效解决了LLM在多意图注释生成中构建正确关系的问题，显著提升了生成质量。

Abstract: Code comment generation aims to produce a generic overview of a code snippet,
helping developers understand and maintain code. However, generic summaries
alone are insufficient to meet the diverse needs of practitioners; for example,
developers expect the implementation insights to be presented in an untangled
manner, while users seek clear usage instructions. This highlights the
necessity of multi-intent comment generation. With the widespread adoption of
Large Language Models (LLMs) for code-related tasks, these models have been
leveraged to tackle the challenge of multi-intent comment generation. Despite
their successes, state-of-the-art LLM-based approaches often struggle to
construct correct relationships among intents, code, and comments within a
smaller number of demonstration examples. To mitigate this issue, we propose a
framework named KUMIC for multi-intent comment generation. Built upon
in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize
knowledge utilization for LLMs to generate intent-specific comments.
Specifically, KUMIC first designs a retrieval mechanism to obtain similar
demonstration examples, which exhibit high code-comment consistency. Then,
KUMIC leverages CoT to guide LLMs to focus on statements facilitating the
derivation of code comments aligned with specific intents. In this context,
KUMIC constructs a mapping knowledge chain, linking code to intent-specific
statements to comments, which enables LLMs to follow similar reasoning steps
when generating the desired comments. We conduct extensive experiments to
evaluate KUMIC, and the results demonstrate that KUMIC outperforms
state-of-the-art baselines by 14.49\%, 22.41\%, 20.72\%, and 12.94\% in terms
of BLEU, METEOR, ROUGE-L, and SBERT, respectively.

</details>


### [23] [TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices](https://arxiv.org/abs/2510.25242)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: TECS/Rust-OE是一个内存安全的基于组件的开发框架，通过利用调用流和实时操作系统独占控制机制来优化性能，同时保持代码可重用性。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统变得越来越复杂，需要确保系统可靠性，特别是安全性。现有的TECS/Rust框架由于过度使用独占控制导致性能下降，需要改进。

Method: 提出TECS/Rust-OE框架，利用调用流和实时操作系统独占控制机制，基于组件描述自动生成Rust代码，优化性能而不牺牲可重用性。

Result: 评估显示优化后的独占控制减少了开销，生成的代码具有高可重用性。

Conclusion: TECS/Rust-OE框架成功解决了TECS/Rust的性能问题，在保持内存安全的同时实现了更好的性能和可重用性。

Abstract: The diversification of functionalities and the development of the IoT are
making embedded systems larger and more complex in structure. Ensuring system
reliability, especially in terms of security, necessitates selecting an
appropriate programming language. As part of existing research, TECS/Rust has
been proposed as a framework that combines Rust and component-based development
(CBD) to enable scalable system design and enhanced reliability. This framework
represents system structures using static mutable variables, but excessive
exclusive controls applied to ensure thread safety have led to performance
degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework
utilizing call flows to address these limitations. The proposed Rust code
leverages real-time OS exclusive control mechanisms, optimizing performance
without compromising reusability. Rust code is automatically generated based on
component descriptions. Evaluations demonstrate reduced overhead due to
optimized exclusion control and high reusability of the generated code.

</details>


### [24] [TECS/Rust: Memory-safe Component Framework for Embedded Systems](https://arxiv.org/abs/2510.25270)
*Nao Yoshimura,Hiroshi Oyama,Takuya Azumi*

Main category: cs.SE

TL;DR: 提出了TECS/Rust框架，将Rust的内存安全特性与嵌入式系统组件框架TECS结合，解决C语言在组件化开发中的内存安全问题，同时保持执行效率。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统复杂度增加，组件化开发(CBD)成为解决方案，但C语言存在内存安全问题。需要结合Rust的内存安全特性来解决这些问题。

Method: 开发基于Rust的TECS框架，利用Rust的编译时内存安全特性（生命周期和借用检查），自动生成CBD组件的Rust代码，并支持与实时操作系统的集成。

Result: 生成的代码占实际代码的很大比例，与无框架开发相比执行时间差异极小，框架引入的开销可忽略不计。

Conclusion: TECS/Rust框架成功将Rust的内存安全优势与嵌入式系统组件化开发结合，在保证安全性的同时保持了执行效率。

Abstract: As embedded systems grow in complexity and scale due to increased functional
diversity, component-based development (CBD) emerges as a solution to
streamline their architecture and enhance functionality reuse. CBD typically
utilizes the C programming language for its direct hardware access and
low-level operations, despite its susceptibility to memory-related issues. To
address these concerns, this paper proposes TECS/Rust, a Rust-based framework
specifically designed for TECS, which is a component framework for embedded
systems. It leverages Rust's compile-time memory-safe features, such as
lifetime and borrowing, to mitigate memory vulnerabilities common with C. The
proposed framework not only ensures memory safety but also maintains the
flexibility of CBD, automates Rust code generation for CBD components, and
supports efficient integration with real-time operating systems. An evaluation
of the amount of generated code indicates that the code generated by this paper
framework accounts for a large percentage of the actual code. Compared to code
developed without the proposed framework, the difference in execution time is
minimal, indicating that the overhead introduced by the proposed framework is
negligible.

</details>


### [25] [Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases](https://arxiv.org/abs/2510.25297)
*Hidetake Tanaka,Haruto Tanaka,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究比较了基于属性的测试(PBT)和基于示例的测试(EBT)在检测LLM生成代码边缘案例方面的效果，发现两种方法各有优势，结合使用可将缺陷检测率从68.75%提升至81.25%。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中生成代码的普及，确保LLM生成代码质量变得重要。传统基于示例的测试方法经常遗漏边缘案例，需要探索更有效的测试方法。

Method: 分析了16个HumanEval问题，使用Claude-4-sonnet生成PBT和EBT测试代码，比较两种方法在检测边缘案例方面的表现。

Result: 单独使用PBT或EBT的缺陷检测率为68.75%，但结合两种方法可将检测率提升至81.25%。PBT擅长检测性能问题和通过广泛输入空间探索边缘案例，EBT擅长检测特定边界条件和特殊模式。

Conclusion: 结合PBT和EBT的混合方法可以显著提高LLM生成代码的可靠性，为LLM代码生成的测试策略提供指导。

Abstract: As Large Language Models (LLMs) increasingly generate code in software
development, ensuring the quality of LLM-generated code has become important.
Traditional testing approaches using Example-based Testing (EBT) often miss
edge cases -- defects that occur at boundary values, special input patterns, or
extreme conditions. This research investigates the characteristics of
LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge
cases. We analyze 16 HumanEval problems where standard solutions failed on
extended test cases, generating both PBT and EBT test codes using
Claude-4-sonnet. Our experimental results reveal that while each method
individually achieved a 68.75\% bug detection rate, combining both approaches
improved detection to 81.25\%. The analysis demonstrates complementary
characteristics: PBT effectively detects performance issues and edge cases
through extensive input space exploration, while EBT effectively detects
specific boundary conditions and special patterns. These findings suggest that
a hybrid approach leveraging both testing methods can improve the reliability
of LLM-generated code, providing guidance for test generation strategies in
LLM-based code generation.

</details>


### [26] [Dissect-and-Restore: AI-based Code Verification with Transient Refactoring](https://arxiv.org/abs/2510.25406)
*Changjie Wang,Mariano Scazzariello,Anoud Alshnaka,Roberto Guanciale,Dejan Kostić,Marco Chiesa*

Main category: cs.SE

TL;DR: Prometheus是一个AI辅助的自动化代码验证系统，通过模块化重构将复杂程序分解为可验证的小组件，然后重新组合构建原始程序的证明，显著提高了AI在形式化验证中的有效性。


<details>
  <summary>Details</summary>
Motivation: 形式化验证对构建可靠软件系统至关重要，但需要专业知识且成本高昂。虽然现代AI系统能识别数学证明模式，但如何有效将其集成到形式化验证过程仍是一个开放挑战。

Method: 采用分解-重组工作流：首先将复杂程序逻辑（如嵌套循环）分解为更小可验证组件，验证后重新组合构建原始程序证明。系统通过结构化分解复杂引理指导证明搜索，当自动化工具不足时，用户可提供轻量级自然语言指导。

Result: 在策划的数据集中成功验证了86%的任务（基线为68%）。随着规范复杂度的增加，验证成功率从30%提升到69%；在集成复杂程序的证明大纲时，从25%提升到87%。

Conclusion: 通过模块化重构代码可显著提高AI验证单个组件的有效性，Prometheus系统成功解决了将AI集成到形式化验证过程的挑战。

Abstract: Formal verification is increasingly recognized as a critical foundation for
building reliable software systems. However, the need for specialized expertise
to write precise specifications, navigate complex proof obligations, and learn
annotations often makes verification an order of magnitude more expensive than
implementation. While modern AI systems can recognize patterns in mathematical
proofs and interpret natural language, effectively integrating them into the
formal verification process remains an open challenge. We present Prometheus, a
novel AI-assisted system that facilitates automated code verification with
current AI capabilities in conjunction with modular software engineering
principles (e.g., modular refactoring). Our approach begins by decomposing
complex program logic, such as nested loops, into smaller, verifiable
components. Once verified, these components are recomposed to construct a proof
of the original program. This decomposition-recomposition workflow is
non-trivial. Prometheus addresses this by guiding the proof search through
structured decomposition of complex lemmas into smaller, verifiable sub-lemmas.
When automated tools are insufficient, users can provide lightweight natural
language guidance to steer the proof process effectively. Our evaluation
demonstrates that transiently applying modular restructuring to the code
substantially improves the AI's effectiveness in verifying individual
components. This approach successfully verifies 86% of tasks in our curated
dataset, compared to 68% for the baseline. Gains are more pronounced with
increasing specification complexity, improving from 30% to 69%, and when
integrating proof outlines for complex programs, from 25% to 87%.

</details>


### [27] [What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow](https://arxiv.org/abs/2510.25423)
*Ali Asgari,Annibale Panichella,Pouria Derakhshanfar,Mitchell Olsthoorn*

Main category: cs.SE

TL;DR: 本研究通过分析Stack Overflow上AI智能体开发者的讨论，识别出77个技术挑战，涵盖运行时集成、依赖管理、编排复杂性和评估可靠性等7大领域，并量化了问题的流行度和难度。


<details>
  <summary>Details</summary>
Motivation: AI智能体虽然发展迅速，但开发者在构建、部署和维护这些系统时面临持续且未被充分探索的挑战，需要系统性地识别这些问题。

Method: 通过标签扩展和过滤构建分类法，应用LDA-MALLET进行主题建模，并手动验证和标记主题，分析2021-2025年开发者讨论。

Result: 识别出7大领域77个技术挑战，量化了主题流行度和难度，绘制了智能体开发使用的工具和编程语言图谱，并追踪了其与AI模型和框架发布的演变关系。

Conclusion: 研究结果为从业者、研究人员和教育工作者提供了关于智能体可靠性和开发者支持的具体指导。

Abstract: AI agents have rapidly gained popularity across research and industry as
systems that extend large language models with additional capabilities to plan,
use tools, remember, and act toward specific goals. Yet despite their promise,
developers face persistent and often underexplored challenges when building,
deploying, and maintaining these emerging systems. To identify these
challenges, we study developer discussions on Stack Overflow, the world's
largest developer-focused Q and A platform with about 60 million questions and
answers and 30 million users. We construct a taxonomy of developer challenges
through tag expansion and filtering, apply LDA-MALLET for topic modeling, and
manually validate and label the resulting themes. Our analysis reveals seven
major areas of recurring issues encompassing 77 distinct technical challenges
related to runtime integration, dependency management, orchestration
complexity, and evaluation reliability. We further quantify topic popularity
and difficulty to identify which issues are most common and hardest to resolve,
map the tools and programming languages used in agent development, and track
their evolution from 2021 to 2025 in relation to major AI model and framework
releases. Finally, we present the implications of our results, offering
concrete guidance for practitioners, researchers, and educators on agent
reliability and developer support.

</details>


### [28] [Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies](https://arxiv.org/abs/2510.25506)
*Florian Angermeir,Maximilian Amougou,Mark Kreitz,Andreas Bauer,Matthias Linhuber,Davide Fucci,Fabiola Moyón C.,Daniel Mendez,Tony Gorschek*

Main category: cs.SE

TL;DR: 对ICSE 2024和ASE 2024会议上86篇LLM相关论文的可复现性分析显示，只有18篇提供了研究工件并使用OpenAI模型。在这18篇中，仅5篇适合复现，但没有一篇能完全复现结果，2篇部分可复现，3篇不可复现。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在学术界和工业界的广泛应用，进行可复现的实证研究变得至关重要。本研究旨在评估当前LLM相关研究的可复现性程度，并识别阻碍可复现性的因素。

Method: 分析了ICSE 2024和ASE 2024会议上发表的86篇LLM相关论文，重点关注其中18篇提供了研究工件并使用OpenAI模型的研究，并尝试复现这些研究。

Result: 在18篇研究中，只有5篇适合复现尝试。其中没有一篇能完全复现原始结果，2篇部分可复现，3篇完全不可复现。

Conclusion: 当前LLM研究的可复现性状况令人担忧，需要更严格的研究工件评估和更稳健的研究设计，以确保未来发表成果的可复现价值。

Abstract: Large Language Models have gained remarkable interest in industry and
academia. The increasing interest in LLMs in academia is also reflected in the
number of publications on this topic over the last years. For instance, alone
78 of the around 425 publications at ICSE 2024 performed experiments with LLMs.
Conducting empirical studies with LLMs remains challenging and raises questions
on how to achieve reproducible results, for both other researchers and
practitioners. One important step towards excelling in empirical research on
LLMs and their application is to first understand to what extent current
research results are eventually reproducible and what factors may impede
reproducibility. This investigation is within the scope of our work. We
contribute an analysis of the reproducibility of LLM-centric studies, provide
insights into the factors impeding reproducibility, and discuss suggestions on
how to improve the current state. In particular, we studied the 86 articles
describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86
articles, 18 provided research artefacts and used OpenAI models. We attempted
to replicate those 18 studies. Of the 18 studies, only five were fit for
reproduction. For none of the five studies, we were able to fully reproduce the
results. Two studies seemed to be partially reproducible, and three studies did
not seem to be reproducible. Our results highlight not only the need for
stricter research artefact evaluations but also for more robust study designs
to ensure the reproducible value of future publications.

</details>


### [29] [Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL](https://arxiv.org/abs/2510.25665)
*Ayse Irmak Ercevik,Aidan Dakhama,Melane Navaratnarajah,Yazhuo Cao,Leo Fernandes*

Main category: cs.SE

TL;DR: GreenAFL是一个能量感知的模糊测试框架，通过将功耗纳入模糊测试启发式算法，在保持覆盖率的同时减少自动化测试的环境影响。


<details>
  <summary>Details</summary>
Motivation: 现有灰盒模糊测试方法如AFL++主要关注覆盖率最大化，而不考虑探索不同执行路径的能量成本，导致持续模糊测试活动消耗大量计算资源并产生显著碳足迹。

Method: GreenAFL对传统模糊测试工作流进行两个关键修改：考虑功耗的能量感知语料库最小化，以及引导变异朝向高覆盖率、低能量输入的能量引导启发式算法。

Result: 消融研究表明，使用至少一个修改组件时，能够实现最高覆盖率和最低能量使用。

Conclusion: 将能量消耗纳入模糊测试启发式算法可以有效减少环境足迹，同时保持测试效果。

Abstract: Fuzzing has become a key search-based technique for software testing, but
continuous fuzzing campaigns consume substantial computational resources and
generate significant carbon footprints. Existing grey-box fuzzing approaches
like AFL++ focus primarily on coverage maximisation, without considering the
energy costs of exploring different execution paths. This paper presents
GreenAFL, an energy-aware framework that incorporates power consumption into
the fuzzing heuristics to reduce the environmental impact of automated testing
whilst maintaining coverage. GreenAFL introduces two key modifications to
traditional fuzzing workflows: energy-aware corpus minimisation considering
power consumption when reducing initial corpora, and energy-guided heuristics
that direct mutation towards high-coverage, low-energy inputs. We conduct an
ablation study comparing vanilla AFL++, energy-based corpus minimisation, and
energy-based heuristics to evaluate the individual contributions of each
component. Results show that highest coverage, and lowest energy usage is
achieved whenever at least one of our modifications is used.

</details>


### [30] [A Configuration-First Framework for Reproducible, Low-Code Localization](https://arxiv.org/abs/2510.25692)
*Tim Strnad,Blaž Bertalanič,Carolina Fortuna*

Main category: cs.SE

TL;DR: LOCALIZE是一个低代码、配置优先的无线电定位框架，通过声明式配置和标准化工作流实现可复现的机器学习实验，减少编码工作量，同时保持可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在无线电定位中面临的三重差距：低编码工作量、默认可复现性（包括版本化代码、数据和配置）以及内置可扩展性。现有工具很少能同时满足这三个需求。

Method: 开发了LOCALIZE框架，使用人类可读的配置声明实验，工作流编排器运行从数据准备到报告的标准化流水线，所有工件（数据集、模型、指标和报告）都进行版本控制。

Result: 与普通Jupyter笔记本基线相比，该框架减少了编写工作量，同时保持了相当的运行时间和内存行为。使用蓝牙低功耗数据集的实验表明，随着数据量从1倍增长到10倍，编排开销保持有界。

Conclusion: 该框架使基于机器学习的可复现定位实验变得实用、易访问和可扩展。

Abstract: Machine learning is increasingly permeating radio-based localization
services. To keep results credible and comparable, everyday workflows should
make rigorous experiment specification and exact repeatability the default,
without blocking advanced experimentation. However, in practice, researchers
face a three-way gap that could be filled by a framework that offers (i) low
coding effort for end-to-end studies, (ii) reproducibility by default including
versioned code, data, and configurations, controlled randomness, isolated runs,
and recorded artifacts, and (iii) built-in extensibility so new models,
metrics, and stages can be added with minimal integration effort. Existing
tools rarely deliver all three for machine learning in general and localization
workflows in particular. In this paper we introduce LOCALIZE, a low-code,
configuration-first framework for radio localization in which experiments are
declared in human-readable configuration, a workflow orchestrator runs
standardized pipelines from data preparation to reporting, and all artifacts,
such as datasets, models, metrics, and reports, are versioned. The
preconfigured, versioned datasets reduce initial setup and boilerplate,
speeding up model development and evaluation. The design, with clear extension
points, allows experts to add components without reworking the infrastructure.
In a qualitative comparison and a head-to-head study against a plain Jupyter
notebook baseline, we show that the framework reduces authoring effort while
maintaining comparable runtime and memory behavior. Furthermore, using a
Bluetooth Low Energy dataset, we show that scaling across training data (1x to
10x) keeps orchestration overheads bounded as data grows. Overall, the
framework makes reproducible machine-learning-based localization
experimentation practical, accessible, and extensible.

</details>


### [31] [Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents](https://arxiv.org/abs/2510.25694)
*Jiayi Kuang,Yinghui Li,Xin Zhang,Yangning Li,Di Yin,Xing Sun,Ying Shen,Philip S. Yu*

Main category: cs.SE

TL;DR: Enconda-bench是一个环境配置诊断基准测试，通过过程级轨迹评估来诊断LLM代理在软件环境配置中的细粒度能力，包括规划、错误诊断、反馈修复等环节。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅评估端到端的构建/测试成功率，无法揭示代理在环境配置过程中具体在哪里失败以及失败原因，限制了软件工程代理能力的深入分析。

Method: 通过自动注入真实的README错误构建任务实例，在Docker中进行可扩展的高质量评估，结合过程级分析和端到端可执行性来评估代理能力。

Result: 评估显示，虽然代理能够定位错误，但难以将反馈转化为有效修正，这限制了端到端性能。

Conclusion: Enconda-bench是首个为环境配置提供过程级内部能力评估的框架，为改进软件工程代理提供了可操作的见解。

Abstract: Large language model-based agents show promise for software engineering, but
environment configuration remains a bottleneck due to heavy manual effort and
scarce large-scale, high-quality datasets. Existing benchmarks assess only
end-to-end build/test success, obscuring where and why agents succeed or fail.
We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench,
which provides process-level trajectory assessment of fine-grained agent
capabilities during environment setup-planning, perception-driven error
diagnosis, feedback-driven repair, and action to execute final environment
configuration. Our task instances are automatically constructed by injecting
realistic README errors and are validated in Docker for scalable, high-quality
evaluation. Enconda-bench combines process-level analysis with end-to-end
executability to enable capability assessments beyond aggregate success rates.
Evaluations across state-of-the-art LLMs and agent frameworks show that while
agents can localize errors, they struggle to translate feedback into effective
corrections, limiting end-to-end performance. To our knowledge, Enconda-bench
is the first framework to provide process-level internal capability assessment
for environment configuration, offering actionable insights for improving
software engineering agents.

</details>
