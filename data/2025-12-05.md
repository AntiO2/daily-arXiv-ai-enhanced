<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.DC](#cs.DC) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection](https://arxiv.org/abs/2512.04106)
*Fouad Trad,Ali Chehab*

Main category: cs.SE

TL;DR: 检索增强提示在代码漏洞检测中优于标准少样本提示和微调模型，但微调CodeBERT性能更高但成本更大


<details>
  <summary>Details</summary>
Motivation: 少样本提示在专业任务中实用，但其效果严重依赖于上下文示例的选择和质量，特别是在代码漏洞检测等复杂领域

Method: 使用Gemini-1.5-Flash模型系统评估三种方法：标准少样本提示、基于语义相似示例的检索增强提示、以及基于检索的标签分配

Result: 检索增强提示在20个示例时达到74.05%的F1分数和83.90%的部分匹配准确率，优于零样本提示和微调Gemini模型，但微调CodeBERT达到91.22%的F1分数

Conclusion: 检索增强提示是代码漏洞检测中平衡性能与成本的有效方法，避免了微调的时间和资源消耗，但微调模型在性能上仍有优势

Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.

</details>


### [2] [HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding](https://arxiv.org/abs/2512.04111)
*Hanjun Luo,Chiming Ni,Jiaheng Wen,Zhimu Huang,Yiran Wang,Bingduo Liao,Sylvia Chung,Yingbin Jin,Xinfeng Li,Wenyuan Xu,XiaoFeng Wang,Hanan Salam*

Main category: cs.SE

TL;DR: HAI-Eval是一个评估人机协作编码能力的基准测试，通过45个模板动态生成任务，证明人机协作（31.11%通过率）显著优于单独使用LLM（0.67%）或人类单独工作（18.89%）。


<details>
  <summary>Details</summary>
Motivation: 现有评估系统（传统人类测试或LLM基准）无法捕捉人机协作编码的新范式，它们只关注明确定义的算法问题，而忽略了需要人机协同解决的真实世界问题。

Method: 提出HAI-Eval基准，包含45个"协作必需"问题模板，动态生成任务；提供标准化IDE供人类参与者使用，以及包含450个任务实例的可复现工具包；进行45人参与者的组内研究，比较5个最先进LLM在4种不同人类干预水平下的表现。

Result: 单独LLM通过率仅0.67%，单独人类通过率18.89%，而人机协作将通过率显著提升至31.11%；分析揭示了新兴的协同推理伙伴关系，战略突破可能来自人类或AI。

Conclusion: HAI-Eval不仅为下一代编码代理建立了具有挑战性的基准，还为AI时代评估核心开发者能力提供了基础、可扩展的框架，挑战了传统的人-工具等级观念。

Abstract: LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To bridge this gap, we introduce HAI-Eval, a unified benchmark designed to measure the synergy of human-AI partnership in coding. HAI-Eval's core innovation is its "Collaboration-Necessary" problem templates, which are intractable for both standalone LLMs and unaided humans, but solvable through effective collaboration. Specifically, HAI-Eval uses 45 templates to dynamically create tasks. It also provides a standardized IDE for human participants and a reproducible toolkit with 450 task instances for LLMs, ensuring an ecologically valid evaluation. We conduct a within-subject study with 45 participants and benchmark their performance against 5 state-of-the-art LLMs under 4 different levels of human intervention. Results show that standalone LLMs and unaided participants achieve poor pass rates (0.67% and 18.89%), human-AI collaboration significantly improves performance to 31.11%. Our analysis reveals an emerging co-reasoning partnership. This finding challenges the traditional human-tool hierarchy by showing that strategic breakthroughs can originate from either humans or AI. HAI-Eval establishes not only a challenging benchmark for next-generation coding agents but also a grounded, scalable framework for assessing core developer competencies in the AI era. Our benchmark and interactive demo will be openly accessible.

</details>


### [3] [Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2512.04117)
*Joost Mertens,Joachim Denil*

Main category: cs.SE

TL;DR: 论文提出了一种基于模型验证技术的数字孪生系统异常检测方法，通过验证指标检测物理系统变化，并使用参数估计修正数字孪生模型。


<details>
  <summary>Details</summary>
Motivation: 数字孪生系统面临的主要挑战是确保数字孪生体能够持续准确地反映物理系统的状态。特别是当数字孪生体是模拟物理系统行为的仿真模型时，由于物理系统会因维护、磨损或人为错误而不断演化，需要检测物理系统的变化并相应更新数字孪生体。

Method: 采用基于模型设计的验证技术，提出通用方法：1）使用验证指标检测孪生系统中的异常；2）通过历史数据进行参数估计来修正数字孪生体中的错误。以港口龙门起重机为案例进行验证。

Result: 开发了能够检测物理系统变化的异常检测方法，并展示了通过参数估计修正数字孪生模型的有效性。使用工业相关的龙门起重机案例验证了该方法的实用性。

Conclusion: 基于模型验证技术的方法能够有效检测数字孪生系统中的异常，并通过参数估计修正数字孪生模型，确保数字孪生体持续准确地反映物理系统的演化状态。

Abstract: One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.

</details>


### [4] [DrP: Meta's Efficient Investigations Platform at Scale](https://arxiv.org/abs/2512.04250)
*Shubham Somani,Vanish Talwar,Madhura Parikh,Eduardo Hernandez,Jimmy Wang,Shreya Shah,Chinmay Gandhi,Sanjay Sundarajan,Neeru Sharma,Srikanth Kamath,Nitin Gupta,Benjamin Renard,Ohad Yahalom,Chris Davis*

Main category: cs.SE

TL;DR: DrP是一个端到端的自动化调查框架，通过可编程分析器、可扩展后端和集成插件，显著减少事故平均解决时间(MTTR)和值班工程师负担。


<details>
  <summary>Details</summary>
Motivation: 大规模系统的事故调查通常依赖手动或临时脚本，导致调查效率低下、解决时间长、值班工程师负担重，影响生产力。

Method: 开发DrP框架：1) 提供表达性SDK编写调查剧本(分析器)；2) 可扩展后端执行自动化剧本；3) 插件集成到警报和事故管理工具；4) 后处理系统执行调查后的缓解措施。

Result: 在Meta大规模部署：覆盖300+团队、2000+分析器，每天执行5万次自动化分析。平均MTTR减少20%，部分团队减少超过80%，显著提升值班生产力。

Conclusion: DrP作为端到端自动化调查系统，能有效减少事故解决时间、降低值班负担，已在大规模生产环境中验证其可行性和效果。

Abstract: Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.
  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.

</details>


### [5] [On the Role and Impact of GenAI Tools in Software Engineering Education](https://arxiv.org/abs/2512.04256)
*Qiaolin Qin,Ronnie de Souza Santos,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 该研究调查了软件工程本科生使用生成式AI工具的情况，发现学生主要用于渐进学习和高级实现，面临输出难以理解和适应等挑战，并呼吁更清晰的教学指导和伦理政策。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具如ChatGPT和GitHub Copilot正在改变软件学习和编写方式，在软件工程教育中既带来新机遇，也引发对过度依赖、伦理使用和学习影响的担忧，需要了解学生的实际使用体验。

Method: 对两所大学的130名本科生进行问卷调查，结合结构化李克特量表和开放式问题，从五个维度调查：使用情境、感知益处、挑战、伦理和教学认知。

Result: 学生最常将GenAI用于渐进学习和高级实现，报告的好处包括头脑风暴支持和建立信心；同时面临挑战如不清楚推理过程和难以适应输出；学生强调公平和不当行为等伦理问题，并呼吁更清晰的教学指导。

Conclusion: 生成式AI正在以微妙方式重塑软件工程教育，研究结果强调需要搭建支架、制定伦理政策和适应性教学策略，以确保GenAI支持公平有效的学习。

Abstract: Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.

</details>


### [6] [Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage](https://arxiv.org/abs/2512.04262)
*Nolan Platt,Ethan Luchs,Sehrish Nizamani*

Main category: cs.SE

TL;DR: 研究评估GPT-4o在早期开发阶段进行可用性启发式评估的能力，发现其在问题检测方面具有中等一致性，但在严重性判断上表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 传统的人工专家启发式评估耗时且主观，特别是在开发早期阶段。研究探索大型语言模型能否在开发阶段提供可靠、一致的启发式评估。

Method: 使用Jakob Nielsen的十个可用性启发式原则，对30个开源网站进行评估。通过OpenAI的GPT-4o管道生成超过850个启发式评估，每个网站进行三次独立评估。

Result: 问题检测方面：平均配对Cohen's Kappa为0.50（中等一致性），精确一致率为84%。严重性判断方面：加权Cohen's Kappa平均为0.63，但精确一致率仅为56%，Krippendorff's Alpha接近零。

Conclusion: GPT-4o能够产生内部一致的评估，特别是在识别可用性问题方面，但其严重性判断能力存在变化，实际应用中需要人工监督。研究为改进自动化用户体验评估的一致性提供了基础。

Abstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.

</details>


### [7] [Polynomiogram: An Integrated Framework for Root Visualization and Generative Art](https://arxiv.org/abs/2512.04263)
*Hoang Duc Nguyen,Anh Van Pham,Hien D. Nguyen*

Main category: cs.SE

TL;DR: Polynomiogram框架：一个集成的计算平台，通过多项式根系统探索、可视化和生成艺术，结合科学研究和生成艺术功能。


<details>
  <summary>Details</summary>
Motivation: 创建既能支持科学研究又能用于生成艺术的统一平台，将多项式数学基础应用于科学探索和艺术创作，同时满足教育需求。

Method: 采用灵活采样方案，从用户定义域中抽取两个独立参数，通过生成函数映射到多项式系数；集成NumPy伴侣矩阵求解器进行快速大规模计算，以及MPSolve进行高精度科学验证的双引擎架构。

Result: 使用Kac和Lucas多项式等经典系综验证了数值准确性；应用于三次多项式系统分析其分岔结构；成功生成了类似芙蓉花的自然形态和表达对AI及大语言模型感激之情的个性化艺术作品。

Conclusion: Polynomiogram框架既是探索根现象的科学工具，也是可视化代数与动力系统基本概念的教育辅助工具，同时展示了作为个性化生成艺术工具的潜力。

Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.

</details>


### [8] [Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures](https://arxiv.org/abs/2512.04273)
*Tyler Slater*

Main category: cs.SE

TL;DR: 该研究首次量化了AI生成代码对软件架构侵蚀和技术债务积累的影响，发现开源模型在架构遵循度和代码完整性方面显著落后于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从代码补全工具转变为自主系统架构师，其对软件长期可维护性的影响尚未被量化。现有研究主要关注功能正确性，但缺乏对架构侵蚀和技术债务积累的评估。

Method: 采用比较性试点研究，使用三种先进模型（GPT-5.1、Claude 4.5 Sonnet、Llama 3 8B）在严格的六边形架构约束下实现标准化的图书借阅微服务。通过抽象语法树解析分析架构一致性。

Result: 闭源模型架构遵循度高（GPT-5.1违规率为0%），而开源模型（Llama 3）架构违规率达80%，经常绕过接口适配器创建非法循环依赖。开源模型还表现出"实现懒惰"现象，生成的逻辑代码行数比闭源模型少60%。

Conclusion: 在没有自动化架构检查的情况下，使用较小的开源模型进行系统脚手架会加速结构性技术债务的积累。需要专门的架构检查工具来确保AI生成代码的长期可维护性。

Abstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure "Architectural Erosion" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of "Implementation Laziness," where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.

</details>


### [9] [MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training](https://arxiv.org/abs/2512.04319)
*Zixiao Zhao,Fatemeh H. Fard,Jie JW Wu*

Main category: cs.SE

TL;DR: MANTRA是一个多阶段自适应噪声处理框架，通过噪声诊断和缓解技术嵌入到代码预训练语言模型和代码LLMs的微调过程中，提高模型在软件工程任务中的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 软件工程任务中深度学习模型的可靠应用依赖于高质量训练数据，但大规模存储库不可避免地引入噪声或错误标记的示例，这会降低准确性和鲁棒性。虽然噪声标签学习在其他领域得到广泛研究，但在软件工程和LLMs用于SE任务方面的研究较少。

Method: 提出MANTRA框架：1) 研究不同噪声水平对模型收敛和损失轨迹的影响；2) 应用基于样本损失动态和高斯混合模型聚类的自适应dropout策略，排除持续噪声点同时保留干净数据；3) 将框架嵌入到代码预训练语言模型和代码LLMs的微调过程中。

Result: 在代码摘要和提交意图分类任务上的实验表明：1) 某些LLMs比其他模型对噪声更敏感；2) 使用MANTRA后，所有模型在两个任务中的性能都得到提升；3) MANTRA能够减少训练中数据集错误的影响，节省数据清洗和处理时间，同时最大化微调效果。

Conclusion: MANTRA框架通过将噪声诊断和缓解直接嵌入到微调过程中，有效提高了代码预训练语言模型和代码LLMs在软件工程任务中的鲁棒性和性能，为研究者和实践者提供了一种减少数据集错误影响的有效方法。

Abstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.

</details>


### [10] [Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles](https://arxiv.org/abs/2512.04344)
*Zitong Zhou,Ben Limpanukorn,Hong Jin Kang,Jiyuan Wang,Yaoxuan Wu,Akos Kiss,Renata Hodovan,Miryung Kim*

Main category: cs.SE

TL;DR: TargetFuzz：一种针对单个编译器优化的定向模糊测试方法，通过挖掘和重建优化所需的程序结构关系，提高测试覆盖率和优化触发率。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试方法主要使用优化流水线作为测试框架，存在两个问题：1）阶段排序问题会错过优化间的交互，许多优化甚至不会被调度；2）优化通常只在输入满足特定结构关系时才触发，现有生成器和突变难以产生这些结构。

Method: 提出针对单个优化的定向模糊测试方法。核心思想是利用优化所寻找的"组合风格"（程序构造间的结构关系）。构建基于语法的通用突变模糊测试器TargetFuzz：1）从优化相关语料库中挖掘组合风格；2）通过合成突变在不同上下文中重建这些风格以测试优化逻辑的变体。该方法可通过轻量级语法标注适应新编程语言，自动合成突变器和交叉操作。

Result: 在LLVM和MLIR上的评估显示，TargetFuzz相比基线模糊测试器，在定向模糊测试模式下分别提高了8%和11%的覆盖率，优化触发率分别提高了2.8倍和2.6倍。定向模糊测试是补充性的：它有效测试了所有37个采样的LLVM优化，而流水线模糊测试错过了12个。

Conclusion: TargetFuzz提供了一种有效的编译器优化定向模糊测试方法，能够发现传统流水线测试遗漏的优化交互和触发条件，特别适用于MLIR等模块化框架的快速演进生态系统。

Abstract: Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\times$ and 2.6$\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.

</details>


### [11] [Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration](https://arxiv.org/abs/2512.04445)
*Yanbin Zhang,Hanhui Ye,Yue Bai,Qiming Zhang,Liao Xiang,Wu Mianzhi,Renjun Hu*

Main category: cs.SE

TL;DR: AutoDW是一个支持逐步执行和回滚的文档工作流自动化框架，通过增量规划和多级回滚机制，在长流程任务中保持用户意图和文档上下文的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有代理系统只能执行孤立指令，难以自动化多步骤、会话级的工作流程，因为缺乏对操作过程的控制能力。

Method: 提出AutoDW执行框架，基于用户指令、意图过滤的API候选和文档演化状态进行增量规划，并在参数和API级别实现回滚机制，支持动态修正和容错。

Result: 在包含250个会话和1,708条人工标注指令的基准测试中，AutoDW在指令级和会话级任务上分别达到90%和62%的完成率，比强基线分别提升40%和76%，且对骨干LLM选择和任务难度变化保持鲁棒。

Conclusion: AutoDW通过逐步执行和回滚机制有效解决了文档工作流自动化中的长流程控制问题，显著提升了多步骤任务的完成率。

Abstract: Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW

</details>


### [12] [LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models](https://arxiv.org/abs/2512.04474)
*Jiaqi Sun,Wei Li,Heng Zhang,Chutong Ding,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.SE

TL;DR: LLM-SrcLog：一种主动统一的日志模板解析框架，结合源码分析和数据驱动方法，在准确性和速度间取得理想平衡


<details>
  <summary>Details</summary>
Motivation: 当前日志解析器大多是反应式和日志中心的，仅从日志推断模板，忽视源代码，限制了理解动态日志结构或适应系统演进的能力。此外，基于每条日志的LLM推理成本过高，难以实际部署。

Method: 提出LLM-SrcLog框架：1）部署前直接从源代码提取模板；2）对无可用代码的日志补充数据驱动解析。包含跨函数静态代码分析器重建有意义的日志上下文，基于LLM的白盒模板提取器区分常量与变量，以及结合数据驱动聚类的黑盒模板提取器处理未匹配日志。

Result: 在两个公共基准测试（Hadoop和Zookeeper）和一个大规模工业系统（Sunfire-Compute）上，相比两个LLM基线方法，LLM-SrcLog将平均F1分数提高了2-17%和8-35%。在线解析延迟与数据驱动方法相当，比每条日志LLM解析快约1000倍。

Conclusion: LLM-SrcLog在速度和准确性之间实现了近乎理想的平衡，并通过实际生产环境的案例研究验证了其有效性，为日志解析提供了主动统一的解决方案。

Abstract: Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.

</details>


### [13] [Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding](https://arxiv.org/abs/2512.04538)
*Xinkui Zhao,Rongkai Liu,Yifan Zhang,Chen Zhi,Lufei Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: CoCo是一个通过理解大规模代码仓库多粒度上下文来实现代码补全的新框架，利用静态代码分析提取结构化上下文，采用基于图的多粒度上下文选择机制，并通过结构感知的代码重排器确保语义和结构对齐，在多个基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成（RAG）的代码补全方法通常将代码视为纯自然语言，主要依赖浅层语义匹配，忽略了结构语义和代码特定依赖关系，这限制了它们捕捉控制流和底层意图的能力，从而制约了生成代码的质量。

Method: 1. 使用静态代码分析提取函数、文件和项目级别的结构化上下文，捕捉执行逻辑和语义依赖；2. 采用基于图的多粒度上下文选择机制过滤冗余信息和噪声；3. 将信息一致地转换为自然语言，作为显式上下文提示指导代码补全；4. 引入结构感知的代码重排器机制确保语义和结构层面的对齐。

Result: 在CrossCodeEval和RepoEval基准测试上的广泛实验表明，CoCo始终超越最先进的基线方法，在EM指标上实现了高达20.2%的提升。该框架是模型无关的，可以无缝集成到现有方法中，带来显著的性能提升。

Conclusion: CoCo通过理解大规模代码仓库的多粒度上下文，有效解决了代码补全任务中上下文信息利用不足的问题，通过结构化分析和基于图的选择机制显著提升了代码生成质量，为代码补全领域提供了有效的解决方案。

Abstract: As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.

</details>


### [14] [Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models](https://arxiv.org/abs/2512.04673)
*Gunjan Das,Paheli Bhattacharya,Rishabh Gupta*

Main category: cs.SE

TL;DR: 本文系统评估了通用和代码专用LLM在语言、推理和代码理解方面的跨领域能力，发现代码优化模型在非编码任务中也表现出性能优势。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在通用NLP和领域应用（如代码合成、法律推理、金融）中取得了革命性进展，但现有研究多关注单个模型能力，缺乏系统性的跨领域比较，特别是统一评估语言、推理和代码理解能力的研究仍然不足。

Method: 对5个通用LLM和3个代码专用LLM在6个多样化基准测试上进行全面评估，涵盖语言能力、数学推理和可信度。同时分析模型在CoNaLa数据集上的代码解释行为，比较通用语言模型和代码专用模型的表现。

Result: 研究发现，针对代码优化的模型（如CodeLLaMA变体）展现出强大的推理能力和语法精确性，即使在非编码任务中也能显示出可测量的性能提升，这与Mistral-7B和Llama-3-8B等通用模型形成对比。

Conclusion: 代码专用LLM不仅在代码相关任务中表现出色，其推理和语言能力也能迁移到非编码任务中，这为模型选择和跨领域应用提供了重要见解。

Abstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.

</details>


### [15] [Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap](https://arxiv.org/abs/2512.04680)
*Jialong Li,Mingyue Zhang,Nianyu Li,Danny Weyns,Zhi Jin,Kenji Tei*

Main category: cs.SE

TL;DR: 本文探讨了在自适应性系统中应用生成式AI的潜力、益处与挑战，通过分析四个研究领域的文献，提出了整合GenAI到SAS的研究路线图。


<details>
  <summary>Details</summary>
Motivation: 自适应性系统(SAS)通过监控、分析、规划、执行的反馈循环处理变化和不确定性。生成式AI(GenAI)在数据理解和逻辑推理方面表现出色，这些能力与SAS所需功能高度契合，表明GenAI有潜力增强SAS。然而，在SAS中应用GenAI的具体益处和挑战尚不明确，且由于SAS领域出版物有限、技术应用多样性以及GenAI技术快速演进，全面理解这些益处和挑战较为复杂。

Method: 从四个不同的研究领域收集、筛选和分析文献，将其组织为两个主要类别：1) 围绕MAPE-K反馈循环特定功能增强SAS自主性；2) 在人在环设置中改进人类与SAS之间的交互。

Result: 研究提出了整合GenAI到SAS的研究路线图，突出了需要解决的关键研究挑战，并对GenAI当前不足进行了实践反思，提出了可能的缓解策略。

Conclusion: 本文为研究人员和从业者提供了关于在自适应性系统中应用生成式AI的潜在益处和挑战的全面概览，并制定了解决集成挑战的研究路线图，有助于推动GenAI在SAS领域的应用发展。

Abstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.

</details>


### [16] [POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?](https://arxiv.org/abs/2512.04702)
*Divyansh Pandey,Vyakhya Gupta,Prakhar Singhal,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: POLARIS是一个三层多智能体自适应框架，超越传统反应式自适应，通过集成监控执行、透明推理和元学习层，实现预测性、主动性的系统自适应，标志着向自适应性3.0的转变。


<details>
  <summary>Details</summary>
Motivation: 现代软件生态系统的规模、复杂性、互连性和自主性日益增长，带来了前所未有的不确定性，挑战了传统自适应方法的基础。现有方法（通常是规则驱动控制器或孤立学习组件）难以泛化到新环境或协调分布式子系统，无法应对"未知的未知"问题。

Method: 提出POLARIS三层多智能体自适应框架：1) 低延迟适配层：监控和安全执行；2) 透明推理层：使用工具感知、可解释的智能体生成和验证计划；3) 元层：记录经验并通过元学习改进自适应策略。通过共享知识和预测模型处理不确定性。

Result: 在SWIM和SWITCH两个自适应示例上的初步评估表明，POLARIS始终优于最先进的基线方法。

Conclusion: POLARIS标志着向自适应性3.0的转变，类似于软件3.0：系统不仅从环境中学习，还能推理和演化自身的自适应过程，持续改进以应对新挑战。

Abstract: The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.

</details>


### [17] [Configuration Defects in Kubernetes](https://arxiv.org/abs/2512.05062)
*Yue Zhang,Uchswas Paul,Marcelo d'Amorim,Akond Rahman*

Main category: cs.SE

TL;DR: 对Kubernetes配置缺陷的实证研究：分析了719个缺陷，识别出15个类别，现有工具只能检测8类，开发了新的linter检测严重缺陷，发现了26个未知缺陷


<details>
  <summary>Details</summary>
Motivation: Kubernetes配置容易出错，配置缺陷可能导致严重后果，需要帮助从业者检测和预防这些缺陷

Method: 从2,260个Kubernetes配置脚本中提取719个缺陷，进行定性分析识别缺陷类别，评估8个现有静态分析工具，开发新的linter检测严重缺陷

Result: 识别出15个缺陷类别，现有工具只能检测8类，数据字段相关缺陷检测精度最高，新linter发现了26个已确认的未知缺陷（19个已修复）

Conclusion: 提供了Kubernetes配置脚本缺陷检测和修复技术的建议，数据集和源代码已公开

Abstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [18] [Energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies](https://arxiv.org/abs/2512.04086)
*Sepideh Masoudi,Sebastian Werner,Pierluigi Plebani,Stefan Tai*

Main category: cs.DB

TL;DR: 提出一种建模和估算数据共享管道能耗的新方法，并识别跨管道共享阶段的复用潜力以降低能耗


<details>
  <summary>Details</summary>
Motivation: 当前数据共享管道虽然有许多治理和执行工具，但能源效率问题未得到足够关注，需要解决跨组织数据交换中的能耗优化问题

Method: 开发了一种建模和估算数据共享管道不同执行配置能耗的方法，并识别跨管道共享阶段的复用潜力

Result: 通过仿真实验验证了该方法，显示出跨组织管道优化的潜力，为能源感知执行策略奠定了基础

Conclusion: 该方法为大规模数据共享联盟中的能耗优化提供了新思路，有望实现更节能的数据交换

Abstract: Data-sharing pipelines involve a series of stages that apply policy-based data transformations to enable secure and effective data exchange among organizations. Although numerous tools and platforms exist to manage governance and enforcement in these pipelines, energy efficiency in data exchange has received limited attention. This paper introduces a novel method to model and estimate the energy consumption of different execution configurations in data-sharing pipelines. Additionally, this method identifies reuse potential in shared stages across pipelines that hold the key to reducing energy in large data-sharing federations. We validate this method through simulation experiments, revealing promising potential for cross-organizational pipeline optimization and laying a foundation for energy-conscious execution strategies.

</details>


### [19] [A Fast Ethereum-Compatible Forkless Database](https://arxiv.org/abs/2512.04735)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 提出专为非分叉区块链设计的原生状态数据库，保持以太坊兼容性，性能提升10倍，存储减少99%


<details>
  <summary>Details</summary>
Motivation: 现代区块链采用快速共识协议避免分叉，但以太坊状态数据库为分叉链设计，维护多个状态版本，导致在非分叉链上效率低下。现有实现基于键值存储也不够高效。

Method: 设计原生数据库实现，专门为非分叉区块链优化，同时保持以太坊兼容性

Result: 验证节点速度提升10倍，空间减少99%；归档节点存储需求减少3倍

Conclusion: 为现代非分叉区块链设计专用状态数据库能显著提升性能和存储效率，同时保持与以太坊生态的兼容性

Abstract: The State Database of a blockchain stores account data and enables authentication. Modern blockchains use fast consensus protocols to avoid forking, improving throughput and finality. However, Ethereum's StateDB was designed for a forking chain that maintains multiple state versions. While newer blockchains adopt Ethereum's standard for DApp compatibility, they do not require multiple state versions, making legacy Ethereum databases inefficient for fast, non-forking blockchains. Moreover, existing StateDB implementations have been built on key-value stores (e.g., LevelDB), which make them less efficient.
  This paper introduces a novel state database that is a native database implementation and maintains Ethereum compatibility while being specialized for non-forking blockchains. Our database delivers ten times speedups and 99% space reductions for validators, and a threefold decrease in storage requirements for archive nodes.

</details>


### [20] [High-Performance DBMSs with io_uring: When and How to use it](https://arxiv.org/abs/2512.04859)
*Matthias Jasny,Muhammad El-Hindi,Tobias Ziegler,Viktor Leis,Carsten Binnig*

Main category: cs.DB

TL;DR: 本文研究现代数据库系统如何利用Linux io_uring接口实现高效、低开销的I/O操作，通过两个用例评估其性能优势，并推导出实用设计指南，在PostgreSQL中验证可获得14%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统Linux I/O接口存在局限性，io_uring作为异步系统调用批处理接口，能够统一存储和网络操作，但简单地替换传统接口并不一定能带来性能提升。需要研究io_uring在何种情况下能带来最大效益，以及如何在现代数据库系统中有效使用它。

Method: 通过两个具体用例进行评估：1) 在存储受限的缓冲区管理器中集成io_uring；2) 在网络受限的分析工作负载中使用io_uring进行高吞吐量数据洗牌。进一步分析io_uring的高级功能（如注册缓冲区和直通I/O）对端到端性能的影响。

Result: 研究揭示了低级优化何时能转化为系统级的实际收益，以及架构选择如何影响这些收益。基于这些洞察，推导出使用io_uring设计I/O密集型系统的实用指南，并在PostgreSQL的io_uring集成案例中验证，应用这些指南可获得14%的性能提升。

Conclusion: io_uring在现代数据库系统中具有显著性能潜力，但需要根据具体应用场景和架构选择来有效利用。研究提供了实用的设计指南，帮助系统开发者最大化io_uring的性能优势，PostgreSQL的案例验证了这些指南的有效性。

Abstract: We study how modern database systems can leverage the Linux io_uring interface for efficient, low-overhead I/O. io_uring is an asynchronous system call batching interface that unifies storage and network operations, addressing limitations of existing Linux I/O interfaces. However, naively replacing traditional I/O interfaces with io_uring does not necessarily yield performance benefits. To demonstrate when io_uring delivers the greatest benefits and how to use it effectively in modern database systems, we evaluate it in two use cases: Integrating io_uring into a storage-bound buffer manager and using it for high-throughput data shuffling in network-bound analytical workloads. We further analyze how advanced io_uring features, such as registered buffers and passthrough I/O, affect end-to-end performance. Our study shows when low-level optimizations translate into tangible system-wide gains and how architectural choices influence these benefits. Building on these insights, we derive practical guidelines for designing I/O-intensive systems using io_uring and validate their effectiveness in a case study of PostgreSQL's recent io_uring integration, where applying our guidelines yields a performance improvement of 14%.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [21] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 本文提出了一种面向边缘集群的可持续性感知LLM推理方法，通过碳感知和延迟感知的路由策略，在NVIDIA Jetson Orin NX和Ada 2000设备上平衡推理延迟和碳足迹。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型需要大量计算资源，导致显著的碳排放和运营成本。虽然训练能耗高，但长期环境负担来自推理过程，特别是全球海量查询量。云端推理存在延迟和带宽限制，而边缘集群虽然能实现本地化执行，但面临性能、能效和设备限制之间的权衡。

Method: 提出可持续性感知的LLM推理框架，针对NVIDIA Jetson Orin NX (8GB)和Nvidia Ada 2000 (16GB)边缘设备集群。通过实证基准测试不同提示和批处理配置的能耗和执行时间，开发碳感知和延迟感知的路由策略，将提示路由到特定硬件。比较了贪婪基线策略与碳感知、延迟感知策略。

Result: 实验评估表明，批处理大小为4个提示时能在吞吐量和能效之间取得最佳平衡，更大的批处理大小可能导致GPU内存饱和。碳感知和延迟感知路由策略相比基线贪婪策略能更好地平衡推理延迟和碳足迹。

Conclusion: 边缘集群上的可持续性感知LLM推理通过智能路由策略可以有效平衡性能和环境影响，批处理大小优化是关键因素，为绿色AI推理提供了实用解决方案。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [22] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 评估WebAssembly在浏览器、边缘和云环境中执行无服务器工作流的性能表现，发现AOT编译和实例预热能显著降低启动延迟，小负载时浏览器性能有竞争力，大负载时边缘和云节点的AOT执行表现更优


<details>
  <summary>Details</summary>
Motivation: WebAssembly作为跨平台二进制指令格式，适合在浏览器、边缘节点和云服务器上执行无服务器工作流，但其性能和稳定性受启动开销、运行时执行模型（AOT/JIT编译）以及不同部署环境的资源差异影响，需要系统评估

Method: 使用wasm32-wasi模块在浏览器、边缘和云实例中一致执行无服务器工作流：浏览器中在web worker内执行，边缘和云中通过HTTP shim将帧流式传输到Wasm运行时；测量冷/热启动延迟、每步延迟、工作流完成时间、吞吐量和CPU/内存利用率

Result: AOT编译和实例预热显著降低启动延迟；小负载工作流中，浏览器因完全内存数据交换而具有竞争力性能；随着负载增大，工作流进入计算和内存密集型阶段，边缘和云节点的AOT执行性能明显超越浏览器

Conclusion: WebAssembly无服务器工作流的性能表现高度依赖于负载特性和部署环境，AOT编译和预热策略对性能优化至关重要，需要根据工作流特征和部署场景选择合适的执行环境

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [23] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 该论文对微服务雾边缘计算中的资源管理策略进行了全面调研，重点关注能源效率，系统分类了136项研究，识别了未解决挑战并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增推动了对高效响应服务的需求，雾边缘计算作为分布式范式应运而生，但其资源受限、异构性、动态负载和多样化QoS要求给资源管理带来了挑战。

Method: 系统性地回顾和分类了2020-2024年间136项研究，将其分为五个关键子领域：服务放置、资源供应、任务调度与卸载、资源分配和实例选择。基于优化技术、目标目标以及每种方法的优缺点进行分类。

Result: 识别了现有调研中的空白和未解决挑战，特别是基本资源管理组件之间缺乏协同作用。提出了利用AI驱动优化、量子计算和无服务器计算等有前景的研究方向。

Conclusion: 该调研为研究者和从业者提供了微服务雾边缘计算中资源管理的统一能源感知视角，为更集成、高效和可持续的未来解决方案铺平了道路。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [24] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS是一个基于文件的顺序消息投递系统，使用RPC和RMA通信原语，支持数千消费者，实现秒级或亚秒级全球消息投递


<details>
  <summary>Details</summary>
Motivation: 实时系统需要低延迟消息投递，数据生产者需要将消息可靠地传递给可能分布在多个集群的数千消费者，同时保证顺序性和至少一次投递，避免消费者过载

Method: 设计基于文件的顺序消息投递系统Fast ACS，结合使用远程过程调用（RPC）进行集群间通信和远程内存访问（RMA）进行集群内通信，支持消费者按自身节奏消费消息

Result: 系统已成功部署到数十个生产集群，支持每个集群数千消费者，峰值时达到Tbps级集群内消费者流量，基于消息量和消费者规模，可在几秒甚至亚秒内（p99）将消息投递到全球消费者

Conclusion: Fast ACS是一个高效、可扩展的消息投递系统，能够在低资源成本下实现全球范围内的快速、可靠消息投递，满足大规模实时系统的需求

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [25] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS是一个基于架构参数的确定性分析模型，用于生成高性能GPU GEMM内核，无需运行时自动调优即可达到接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统GPU GEMM内核通常依赖耗时的运行时自动调优来获得高性能，这增加了部署成本和时间开销。需要一种能够基于架构参数直接预测最优配置的方法。

Method: 开发了tritonBLAS分析模型，显式建模架构拓扑、矩阵形状和算法分块行为之间的关系。基于该模型，在Triton框架内实现了轻量级GEMM框架。

Result: 在多种GEMM问题规模上评估，tritonBLAS能达到自动调优解决方案95%以上的性能，同时将自动调优时间降为零。

Conclusion: tritonBLAS是一个实用的替代方案，可用于生产环境中的高性能计算和机器学习工作负载，无需耗时的自动调优过程。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [26] [Scaling MPI Applications on Aurora](https://arxiv.org/abs/2512.04291)
*Huda Ibeid,Anthony-Trung Nguyen,Aditya Nishtala,Premanand Sakarda,Larry Kaplan,Nilakantan Mahadevan,Michael Woodacre,Victor Anisimov,Kalyan Kumaran,JaeHyuk Kwack,Vitali Morozov,Servesh Muralidharan,Scott Parker*

Main category: cs.DC

TL;DR: 本文介绍了Aurora超级计算机的系统设计，特别关注其网络架构和验证方法，展示了该系统在多种基准测试和实际应用中的卓越性能。


<details>
  <summary>Details</summary>
Motivation: Aurora是2024年部署在阿贡国家实验室的三大百亿亿次超级计算机之一，旨在为AI和HPC模拟提供强大的计算能力，推动开放科学发展。本文旨在详细介绍该系统设计，特别是网络架构，并验证其性能表现。

Method: 论文重点介绍了Aurora的系统架构，包括采用Intel Data Center Max Series GPU和Xeon Max Series CPU的节点设计，以及基于HPE Slingshot高性能互连网络的龙形拓扑结构。通过MPI基准测试和多种性能基准（HPL、HPL-MxP、Graph500、HPCG）验证系统性能，并在HACC、AMR-Wind、LAMMPS、FMM等实际应用中测试系统可扩展性。

Result: Aurora在2024年6月Top500榜单中排名第二，在HPL MxP基准测试中排名第一。系统展示了优异的吞吐量、延迟和带宽性能，能够支持大规模节点扩展，为应用提供新的能力水平，实现突破性科学发现。

Conclusion: Aurora超级计算机通过创新的硬件架构和网络设计，成功实现了百亿亿次计算性能，为AI和HPC模拟提供了强大的计算平台，能够支持大规模科学应用并实现突破性科学进展。

Abstract: The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.

</details>


### [27] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: VLCs（虚拟库上下文）是一种进程子单元，用于封装库和资源分配，无需修改库代码即可控制资源使用，避免库间资源争用，提升并行性能。


<details>
  <summary>Details</summary>
Motivation: 现代并行机器日益复杂，程序员依赖软件库组合来利用并行性。但许多库设计时未考虑组合使用，假设独占所有资源，导致并发使用时产生资源争用和性能下降。现有方案需要修改库或操作系统，通常不可行。

Method: 提出虚拟库上下文（VLCs）作为进程子单元，封装库集和相关资源分配。VLCs无需修改库代码即可控制库的资源使用，允许用户在不同库之间划分资源以避免争用，或加载同一库的多个副本以支持并行执行线程不安全的代码。

Result: 实现了C++和Python的VLCs原型，实验表明VLCs在使用OpenMP、OpenBLAS和LibTorch的应用基准测试中实现了最高2.85倍的加速。

Conclusion: VLCs提供了一种轻量级解决方案，能够在同一进程内管理多个库的资源使用，避免资源争用，提升并行性能，且无需修改现有库代码或操作系统。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [28] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: gpuFLOPBench是一个评估LLM预测GPU内核浮点运算次数的基准测试，包含577个CUDA内核，揭示当前模型在预测隐含FLOPs方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代GPU软件开发需要开发者能在运行前预测性能瓶颈，但当前LLM很少测试这种前瞻性推理能力。现有代码助手无法内化硬件特定的微码效应，需要专门的基准测试来评估和改进LLM的性能推理能力。

Method: 创建gpuFLOPBench基准测试，包含从HeCBench中提取的577个CUDA内核，标注真实性能数据和8个执行属性。评估当前闭源推理模型预测单精度和双精度FLOP计数的能力，区分可简单分析的代码与依赖编译器或运行时行为的复杂内核。

Result: 最新LLM在简单内核上能实现完美分类，但在涉及除法、内在数学函数或公共子表达式等隐含FLOPs时，仍会出现多个数量级的错误。这表明现有模型无法内化硬件特定的微码效应。

Conclusion: gpuFLOPBench作为一个专注的测试平台，可用于开发能够像经验丰富的GPU开发者一样严格推理性能的LLM工具。该基准测试揭示了当前代码助手在性能推理方面的核心局限性。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [29] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 提出一种针对稀疏LU分解的结构感知不规则分块方法，通过对角线块特征表征非零元分布，自适应调整块大小以平衡负载，在GPU上相比现有方法获得显著加速。


<details>
  <summary>Details</summary>
Motivation: 稀疏LU分解中，符号分解后的非零元倾向于分布在矩阵的对角线和右下区域。传统的规则二维分块在这种非均匀分布结构上会导致块间工作负载不平衡，且现有矩阵特征无法有效指导分块策略。

Method: 提出一种结构感知的不规则分块方法：1) 引入新颖的对角线块特征来有效表征稀疏矩阵的局部非零元分布；2) 基于此特征提出不规则分块方法，根据局部非零元分布调整块大小；3) 在密集区域使用细粒度块，在稀疏区域使用粗粒度块，充分平衡依赖树中同一层级和跨层级的块非零元数量。

Result: 在单个NVIDIA A100 GPU上，相比PanguLU和最新的SuperLU_DIST，平均加速比分别达到1.50倍和3.32倍。在4个NVIDIA A100 GPU上，相比PanguLU和SuperLU_DIST，加速比分别达到1.40倍和3.84倍。

Conclusion: 提出的结构感知不规则分块方法通过有效表征稀疏矩阵的非零元分布并自适应调整分块大小，显著改善了负载平衡，在GPU上实现了比现有方法更高效的稀疏LU分解性能。

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [30] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: KAI系统通过异步回传协议优化CXL计算内存，减少数据移动开销，提升异构工作负载性能


<details>
  <summary>Details</summary>
Motivation: CXL计算内存虽能减少数据移动开销，但现有卸载机制无法充分利用不同CXL协议模型的权衡，限制了异构工作负载的性能优化

Method: 提出异步回传协议，在底层CXL协议上分层数据和控制传输操作；设计KAI系统实现异步数据移动和轻量级流水线的主机-CCM交互

Result: KAI减少端到端运行时间达50.4%，CCM和主机空闲时间分别平均减少22.11倍和3.85倍

Conclusion: 异步回传协议和KAI系统能有效利用CXL协议权衡，显著提升异构工作负载在分解内存系统中的性能和效率

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [31] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: THz通信与联邦学习结合时，宽带损伤（如波束倾斜、分子吸收）会导致收敛误差，标准聚合方法在频谱空洞处失效，需要SNR加权聚合来恢复收敛。


<details>
  <summary>Details</summary>
Motivation: THz通信与联邦学习结合可实现超快分布式学习，但实际宽带损伤对优化动态的理论影响尚未被充分研究。本文旨在填补这一空白，分析频率选择性THz效应对联邦学习收敛的影响。

Method: 开发了一个多载波随机框架，将本地梯度更新与频率选择性THz效应（包括波束倾斜、分子吸收和抖动）显式耦合。分析了标准无偏聚合下的收敛误差，并提出了SNR加权聚合策略。

Result: 发现了"多样性陷阱"：标准聚合下收敛误差由子载波SNR的调和均值驱动，单个频谱空洞（由严重波束倾斜引起）可使整个带宽对可靠模型更新无效。识别了基本带宽限制，超出临界点会因热噪声和带边增益崩溃而降低收敛。SNR加权聚合可抑制频谱空洞处的方差奇异性，在高波束倾斜情况下恢复收敛。

Conclusion: THz-FL系统的性能受物理层参数显著影响，标准平均聚合在宽带损伤下会失效，需要采用SNR加权聚合策略来确保可靠收敛。研究结果为THz频段联邦学习的实际部署提供了理论指导。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>
