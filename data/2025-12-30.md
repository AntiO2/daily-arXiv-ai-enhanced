<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 14]
- [cs.SE](#cs.SE) [Total: 24]
- [cs.DC](#cs.DC) [Total: 29]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [MonoM: Enhancing Monotonicity in Learned Cardinality Estimators](https://arxiv.org/abs/2512.22122)
*Lyu Yi,Weiqi Feng,Yuanbiao Wang,Yuhong Kan*

Main category: cs.DB

TL;DR: 提出MonoM指标量化基数估计器的单调性遵守程度，并提出包含可比较查询生成器和正则化项的单调训练框架，既提升单调性又提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 学习型基数估计方法虽然精度高，但在生产系统中应用的主要障碍是违反单调性等基本逻辑原则，需要解决这一关键问题。

Method: 提出MonoM指标量化单调性遵守程度；设计单调训练框架，包括生成可直接比较查询的工作负载生成器，以及在损失函数中添加新颖的正则化项。

Result: 实验结果表明，单调训练算法不仅增强了单调性遵守程度，还提高了基数估计的准确性，正则化项减少了过拟合并改善了模型泛化能力。

Conclusion: 提出的单调训练框架有效解决了学习型基数估计器违反单调性的问题，同时提升了估计精度，为生产系统部署扫除了重要障碍。

Abstract: Cardinality estimation is a key component of database query optimization. Recent studies have demonstrated that learned cardinality estimation techniques can surpass traditional methods in accuracy. However, a significant barrier to their adoption in production systems is their tendency to violate fundamental logical principles such as monotonicity. In this paper, we explore how learned models specifically MSCN, a query driven deep learning algorithm can breach monotonicity constraints. To address this, we propose a metric called MonoM, which quantitatively measures how well a cardinality estimator adheres to monotonicity across a given query workload. We also propose a monotonic training framework which includes a workload generator that produces directly comparable queries (one query's predicates are strictly more relaxed than another's, enabling monotonicity inference without actual execution) and a novel regularization term added to the loss function. Experimental results show that our monotonic training algorithm not only enhances monotonicity adherence but also improves cardinality estimation accuracy. This improvement is attributed to the regularization term, which reduces overfitting and improves model generalization.

</details>


### [2] [Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries](https://arxiv.org/abs/2512.22364)
*Saurabh Deochake,Debajyoti Mukhopadhyay*

Main category: cs.DB

TL;DR: 本文首次系统评估了LLM生成的SQL查询在云数据仓库中的计算成本，发现推理模型比标准模型处理字节数少44.5%，执行时间与成本相关性弱，不同模型成本差异可达3.4倍，并识别了常见的低效模式。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL系统虽然准确率高，但效率指标（如VES）只关注执行时间，忽略了云数据仓库的实际消费成本。企业部署需要了解LLM生成SQL的真实云成本。

Method: 在Google BigQuery上使用StackOverflow数据集（230GB），评估6个最先进的LLM，执行180个查询，测量字节处理量、槽位利用率和估计成本。

Result: 1. 推理模型比标准模型处理字节数少44.5%，同时保持相同正确率（96.7%-100%）；2. 执行时间与成本相关性弱（r=0.16）；3. 不同模型成本差异可达3.4倍，标准模型产生超过36GB/查询的异常值。

Conclusion: 需要新的成本优化指标，推理模型在成本效率上表现更好，识别了缺失分区过滤器和全表扫描等低效模式，为成本敏感的企业环境提供了部署指南。

Abstract: Text-to-SQL systems powered by Large Language Models (LLMs) achieve high accuracy on standard benchmarks, yet existing efficiency metrics such as the Valid Efficiency Score (VES) measure execution time rather than the consumption-based costs of cloud data warehouses. This paper presents the first systematic evaluation of cloud compute costs for LLM-generated SQL queries. We evaluate six state-of-the-art LLMs across 180 query executions on Google BigQuery using the StackOverflow dataset (230GB), measuring bytes processed, slot utilization, and estimated cost. Our analysis yields three key findings: (1) reasoning models process 44.5% fewer bytes than standard models while maintaining equivalent correctness (96.7%-100%); (2) execution time correlates weakly with query cost (r=0.16), indicating that speed optimization does not imply cost optimization; and (3) models exhibit up to 3.4x cost variance, with standard models producing outliers exceeding 36GB per query. We identify prevalent inefficiency patterns including missing partition filters and unnecessary full-table scans, and provide deployment guidelines for cost-sensitive enterprise environments.

</details>


### [3] [Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning](https://arxiv.org/abs/2512.22742)
*Hanze Meng,Jianhao Cao,Rachel Pottinger*

Main category: cs.DB

TL;DR: 提出基于LoRA的参数高效微调框架，通过提示增强数据训练，解决列类型标注任务中模型对提示词敏感且性能有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个主要问题：1) 基于编码器的语言模型需要针对不同领域重新训练，成本高；2) 使用LLM进行提示的方法对提示词变化敏感且F1分数有限。完全微调LLM计算成本过高，且无法消除提示敏感性。

Method: 提出参数高效的CTA框架，使用低秩适应(LoRA)在提示增强数据上训练模型。通过多种提示模板增强训练数据，减少可训练参数数量，同时提高对提示变化的鲁棒性。

Result: 实验结果表明，使用提示增强策略微调的模型在推理时对不同提示模式保持稳定性能，加权F1分数高于使用单一提示模板微调的模型。

Conclusion: 参数高效训练和增强策略在开发实用且适应性强的CTA系统中是有效的，能够平衡计算成本与性能，提高模型对提示变化的鲁棒性。

Abstract: Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but their applicability is limited to in-domain settings, as distribution shifts in tables or label spaces require costly re-training from scratch. Recent work has explored prompting generative large language models (LLMs) by framing CTA as a multiple-choice task, but these approaches face two key challenges: (1) model performance is highly sensitive to subtle changes in prompt wording and structure, and (2) annotation F1 scores remain modest. A natural extension is to fine-tune large language models. However, fully fine-tuning these models incurs prohibitive computational costs due to their scale, and the sensitivity to prompts is not eliminated. In this paper, we present a parameter-efficient framework for CTA that trains models over prompt-augmented data via Low-Rank Adaptation (LoRA). Our approach mitigates sensitivity to prompt variations while drastically reducing the number of necessary trainable parameters, achieving robust performance across datasets and templates. Experimental results on recent benchmarks demonstrate that models fine-tuned with our prompt augmentation strategy maintain stable performance across diverse prompt patterns during inference and yield higher weighted F1 scores than those fine-tuned on a single prompt template. These results highlight the effectiveness of parameter-efficient training and augmentation strategies in developing practical and adaptable CTA systems.

</details>


### [4] [OrchANN: A Unified I/O Orchestration Framework for Skewed Out-of-Core Vector Search](https://arxiv.org/abs/2512.22838)
*Chengying Huan,Lizheng Chen,Zhengyi Yang,Shaonan Ma,Rong Gu,Renjie Yao,Zhibin Wang,Mingxing Zhang,Fang Xi,Jie Tao,Gang Zhang,Guihai Chen,Chen Tian*

Main category: cs.DB

TL;DR: OrchANN是一个面向十亿级向量的外存近似最近邻搜索引擎，通过I/O编排模型、异构本地索引、查询感知导航图和多级剪枝技术，在SSD存储环境下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 十亿级近似最近邻搜索本质上是外存问题，向量和索引存储在SSD上，性能受I/O主导而非计算。现有外存系统在处理偏斜语义嵌入时存在多个问题：均匀本地索引与聚类规模不匹配、静态路由误导查询并增加探测分区数量、剪枝不完整导致"获取后丢弃"的重新排序。

Method: 1. 采用I/O编排模型统一管理路由-访问-验证流水线的I/O操作；2. 通过离线自动分析为每个聚类选择异构本地索引；3. 维护查询感知的内存导航图以适应偏斜工作负载；4. 应用带几何边界约束的多级剪枝技术，在发起SSD读取前过滤聚类和向量。

Result: 在五个标准数据集和严格外存约束下，OrchANN在QPS和延迟方面均优于DiskANN、Starling、SPANN和PipeANN四个基线系统，同时减少了SSD访问。相比竞争系统，OrchANN提供高达17.2倍的QPS提升和25.0倍的延迟降低，且不牺牲准确性。

Conclusion: OrchANN通过创新的I/O编排模型和自适应索引策略，有效解决了大规模外存ANNS系统中的关键瓶颈，在处理偏斜语义嵌入时展现出显著优势，为十亿级向量搜索提供了高效解决方案。

Abstract: Approximate nearest neighbor search (ANNS) at billion scale is fundamentally an out-of-core problem: vectors and indexes live on SSD, so performance is dominated by I/O rather than compute. Under skewed semantic embeddings, existing out-of-core systems break down: a uniform local index mismatches cluster scales, static routing misguides queries and inflates the number of probed partitions, and pruning is incomplete at the cluster level and lossy at the vector level, triggering "fetch-to-discard" reranking on raw vectors.
  We present OrchANN, an out-of-core ANNS engine that uses an I/O orchestration model for unified I/O governance along the route-access-verify pipeline. OrchANN selects a heterogeneous local index per cluster via offline auto-profiling, maintains a query-aware in-memory navigation graph that adapts to skewed workloads, and applies multi-level pruning with geometric bounds to filter both clusters and vectors before issuing SSD reads. Across five standard datasets under strict out-of-core constraints, OrchANN outperforms four baselines including DiskANN, Starling, SPANN, and PipeANN in both QPS and latency while reducing SSD accesses. Furthermore, OrchANN delivers up to 17.2x higher QPS and 25.0x lower latency than competing systems without sacrificing accuracy.

</details>


### [5] [Time Sensitive Multiple POIs Route Planning on Bus Networks](https://arxiv.org/abs/2512.22893)
*Simu Liu,Kailin Jiao,Junping Du,Yawen Li,Zhe Xue,Xiaoyang Sean Wang,Ziqiang Yu,Yunchuan Shi*

Main category: cs.DB

TL;DR: 提出EA-Star算法解决公交网络中的POI访问路径规划问题，考虑动态公交时刻表和等待时间，相比暴力搜索显著提升效率


<details>
  <summary>Details</summary>
Motivation: 解决公交网络中的路径规划问题，用户需要从起点出发访问多个兴趣点(POI)，但公交网络具有动态变化的特点（公交行驶时间和到站时间表），传统旅行商问题(TSP)解法无法有效处理这种复杂性

Method: 1) 提出改进的图结构表示公交网络，适应变化的公交行驶时间和到站时刻表；2) 基于Dijkstra原理的暴力搜索算法作为基准；3) 提出EA-Star算法，专注于计算有希望的POI访问序列的最短路径，使用A*算法在改进图结构上搜索，并设置终止条件避免评估所有可能序列

Result: 在纽约公交网络数据集上的实验证明了该方法的有效性，能够找到访问所有指定POI的最优路线，同时显著减少计算成本

Conclusion: 提出的EA-Star算法能够有效解决公交网络中的多POI访问路径规划问题，考虑了动态公交时刻表和等待时间，相比暴力搜索方法在大型公交网络中具有更好的可扩展性和效率

Abstract: This work addresses a route planning problem constrained by a bus road network that includes the schedules of all buses. Given a query with a starting bus stop and a set of Points of Interest (POIs) to visit, our goal is to find an optimal route on the bus network that allows the user to visit all specified POIs from the starting stop with minimal travel time, which includes both bus travel time and waiting time at bus stops. Although this problem resembles a variant of the Traveling Salesman Problem, it cannot be effectively solved using existing solutions due to the complex nature of bus networks, particularly the constantly changing bus travel times and user waiting times. In this paper, we first propose a modified graph structure to represent the bus network, accommodating the varying bus travel times and their arrival schedules at each stop. Initially, we suggest a brute-force exploration algorithm based on the Dijkstra principle to evaluate all potential routes and determine the best one; however, this approach is too costly for large bus networks. To address this, we introduce the EA-Star algorithm, which focuses on computing the shortest route for promising POI visit sequences. The algorithm includes a terminal condition that halts evaluation once the optimal route is identified, avoiding the need to evaluate all possible POI sequences. During the computation of the shortest route for each POI visiting sequence, it employs the A* algorithm on the modified graph structure, narrowing the search space toward the destination and improving search efficiency. Experiments using New York bus network datasets demonstrate the effectiveness of our approach.

</details>


### [6] [Evolution of Buffer Management in Database Systems: From Classical Algorithms to Machine Learning and Disaggregated Memory](https://arxiv.org/abs/2512.22995)
*Prudhvi Gadupudi,Suman Saha*

Main category: cs.DB

TL;DR: 本文对数据库和操作系统中的缓冲区管理进行了40年研究的全面综述，从传统算法演进到机器学习增强策略和分解内存架构，分析了50多篇重要论文，并提出了整合机器学习与内核可扩展性的未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 缓冲区管理是数据库和操作系统性能的关键组件，用于弥合CPU处理速度与存储访问时间之间的持久延迟差距。随着技术发展，需要系统性地梳理40年来的研究演进，为现代异构内存层次结构提供指导。

Method: 系统性地分析了50多篇来自SIGMOD、VLDB、OSDI、FAST等顶级会议的里程碑论文，涵盖从LRU-K、2Q、LIRS、ARC等基础算法到机器学习增强策略和分解内存架构的演进过程，并考察了PostgreSQL、Oracle、Linux等生产系统的实现。

Result: 识别了关键架构模式、性能权衡和开放研究挑战，包括OS-DBMS架构分歧的历史演变、eBPF内核可扩展性、NVM感知分层策略、RDMA内存分解等新兴趋势。

Conclusion: 提出了整合机器学习与内核可扩展机制的研究方向，以实现现代数据库系统中异构内存层次结构的自适应跨层缓冲区管理。

Abstract: Buffer management remains a critical component of database and operating system performance, serving as the primary mechanism for bridging the persistent latency gap between CPU processing speeds and storage access times. This paper provides a comprehensive survey of buffer management evolution spanning four decades of research. We systematically analyze the progression from foundational algorithms like LRU-K, 2Q, LIRS, and ARC to contemporary machine learning-augmented policies and disaggregated memory architectures. Our survey examines the historical OS-DBMS architectural divergence, production system implementations in PostgreSQL, Oracle, and Linux, and emerging trends including eBPF-based kernel extensibility, NVM-aware tiering strategies, and RDMA-enabled memory disaggregation. Through analysis of over 50 seminal papers from leading conferences (SIGMOD, VLDB, OSDI, FAST), we identify key architectural patterns, performance trade-offs, and open research challenges. We conclude by outlining a research direction that integrates machine learning with kernel extensibility mechanisms to enable adaptive, cross-layer buffer management for heterogeneous memory hierarchies in modern database systems.

</details>


### [7] [ChronoConnect: Tracking Pathways Along Highly Dynamic Vertices in Temporal Graphs](https://arxiv.org/abs/2512.23289)
*Jiacheng Ding,Cong Guo,Xiaofei Zhang*

Main category: cs.DB

TL;DR: ChronoConnect是一个用于追踪时序图中信息传播路径的系统，支持并行处理动态图数据，提供可视化界面和多种时序遍历算法配置。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态快照的图分析系统难以有效捕捉时序维度上的信息流动模式，而时序图数据的普及需要能够分析信息传播模式的系统。

Method: 开发ChronoConnect系统，支持用户配置和执行多种时序遍历算法，利用并行处理技术处理动态图的规模爆炸增长，并提供交互式可视化界面。

Result: 通过实现追踪时序图中高度动态顶点路径的算法，展示了ChronoConnect的有效性和增强性能，系统能够高效分析时间约束下的信息扩散过程。

Conclusion: ChronoConnect将成为用户检查信息在时序图中如何传播的强大工具，特别适用于下游挖掘任务如理解向特定顶点群传播信息的关键路径。

Abstract: With the proliferation of temporal graph data, there is a growing demand for analyzing information propagation patterns during graph evolution. Existing graph analysis systems, mostly based on static snapshots, struggle to effectively capture information flows along the temporal dimension. To address this challenge, we introduce ChronoConnect, a novel system that enables tracking temporal pathways in temporal graph, especially beneficial to downstream mining tasks, e.g., understanding what are the critical pathways in propagating information towards a specific group of vertices. Built on ChronoConnect, users can conveniently configure and execute a variety of temporal traversal algorithms to efficiently analyze information diffusion processes under time constraints. Moreover, ChronoConnect utilizes parallel processing to tackle the explosive size-growth of evolving graphs. We showcase the effectiveness and enhanced performance of ChronoConnect through the implementation of algorithms that track pathways along highly dynamic vertices in temporal graphs. Furthermore, we offer an interactive user interface for graph visualization and query result exploration. We envision ChronoConnect to become a powerful tool for users to examine how information spreads over a temporal graph.

</details>


### [8] [BRkNN-light: Batch Processing of Reverse k-Nearest Neighbor Queries for Moving Objects on Road Networks](https://arxiv.org/abs/2512.23298)
*Anbang Song,Ziqiang Yu,Wei Liu,Yating Xu,Mingjin Tao*

Main category: cs.DB

TL;DR: 提出BR$k$NN-Light算法，首次探索道路网络上移动对象的批量R$k$NN查询处理，通过共享计算减少总体处理成本


<details>
  <summary>Details</summary>
Motivation: 现有方法大多忽视如何高效处理多个同时提交的R$k$NN查询，错失了共享冗余计算以减少总体处理成本的机会

Method: 提出BR$k$NN-Light算法，采用基于几何约束的快速验证和剪枝策略，优化范围搜索技术，并引入动态距离缓存机制实现计算重用

Result: 在多个真实世界道路网络上的实验表明，BR$k$NN-Light算法在批量查询处理方面具有优越性

Conclusion: 该研究首次探索批量R$k$NN查询处理，提出的算法通过计算共享显著减少了不必要的计算，为位置服务中的高效查询处理提供了有效解决方案

Abstract: The Reverse $k$-Nearest Neighbor (R$k$NN) query over moving objects on road networks seeks to find all moving objects that consider the specified query point as one of their $k$ nearest neighbors. In location based services, many users probably submit R$k$NN queries simultaneously. However, existing methods largely overlook how to efficiently process multiple such queries together, missing opportunities to share redundant computations and thus reduce overall processing costs. To address this, this work is the first to explore batch processing of multiple R$k$NN queries, aiming to minimize total computation by sharing duplicate calculations across queries. To tackle this issue, we propose the BR$k$NN-Light algorithm, which uses rapid verification and pruning strategies based on geometric constraints, along with an optimized range search technique, to speed up the process of identifying the R$k$NNs for each query. Furthermore, it proposes a dynamic distance caching mechanism to enable computation reuse when handling multiple queries, thereby significantly reducing unnecessary computations. Experiments on multiple real-world road networks demonstrate the superiority of the BR$k$NN-Light algorithm on the processing of batch queries.

</details>


### [9] [Flexible Keyword-Aware Top-$k$ Route Search](https://arxiv.org/abs/2512.23319)
*Ziqiang Yu,Xiaohui Yu,Yueting Chen,Wei Liu,Anbang Song,Bolong Zheng*

Main category: cs.DB

TL;DR: 论文提出KATR查询，通过关键词感知的top-k路线规划解决LLM在旅游路线规划中的局限性，采用探索-边界范式高效处理查询。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在旅游路线规划中的普及，用户通过输入景点关键词获取建议，但LLM难以在真实道路网络中考虑大量POI组合时生成最优路线，需要外部工具支持。

Method: 提出KATR查询支持灵活的POI访问顺序、旅行距离预算和个性化POI评分；采用探索-边界范式，通过从全局到局部的分数边界估计消除冗余候选路线。

Result: 大量实验表明，该方法在不同场景下均优于现有方法，表现出优越的性能。

Conclusion: KATR查询为LLM提供了更灵活全面的路线规划语义，探索-边界范式能高效处理复杂查询，满足用户多样化需求。

Abstract: With the rise of Large Language Models (LLMs), tourists increasingly use it for route planning by entering keywords for attractions, instead of relying on traditional manual map services. LLMs provide generally reasonable suggestions, but often fail to generate optimal plans that account for detailed user requirements, given the vast number of potential POIs and possible routes based on POI combinations within a real-world road network. In this case, a route-planning API could serve as an external tool, accepting a sequence of keywords and returning the top-$k$ best routes tailored to user requests. To address this need, this paper introduces the Keyword-Aware Top-$k$ Routes (KATR) query that provides a more flexible and comprehensive semantic to route planning that caters to various user's preferences including flexible POI visiting order, flexible travel distance budget, and personalized POI ratings. Subsequently, we propose an explore-and-bound paradigm to efficiently process KATR queries by eliminating redundant candidates based on estimated score bounds from global to local levels. Extensive experiments demonstrate our approach's superior performance over existing methods across different scenarios.

</details>


### [10] [Database Theory in Action: From Inexpressibility to Efficiency in GQL's Order-Constrained Paths](https://arxiv.org/abs/2512.23330)
*Hadar Rotschield,Liat Peterfreund*

Main category: cs.DB

TL;DR: 将GQL中路径上边值递增的模式匹配条件编译到输入图中，既解决了表达性限制又提升了实际性能


<details>
  <summary>Details</summary>
Motivation: GQL标准在模式匹配中无法检查路径上边值是否递增，存在表达性限制。同时，在现有实现（如Cypher）中虽然能表达但性能代价高。

Method: 提出构造性翻译方法，将边值递增条件编译到输入图中。通过理论驱动的转换，将路径约束嵌入到图结构本身。

Result: 在Neo4j的Cypher中实现概念验证，编译后的版本运行更快且避免了超时。不仅恢复了表达性，还带来了实际性能提升。

Conclusion: 理论驱动的翻译不仅能弥补表达性差距，还能带来实际性能收益，展示了理论构造在实际系统中的价值。

Abstract: Pattern matching of core GQL, the new ISO standard for querying property graphs, cannot check whether edge values are increasing along a path, as established in recent work. We present a construc- tive translation that overcomes this limitation by compiling the increasing-edges condition into the input graph. Remarkably, the benefit of this construction goes beyond restoring expressiveness. In our proof-of-concept implementation in Neo4j's Cypher, where such path constraints are expressible but costly, our compiled version runs faster and avoids timeouts. This illustrates how a theoretically motivated translation can not only close an expressiveness gap but also bring practical performance gains.

</details>


### [11] [HL-index: Fast Reachability Query in Hypergraphs](https://arxiv.org/abs/2512.23345)
*Peiting Xie,Xiangjun Zai,Yanping Wu,Xiaoyang Wang,Wenjie Zhang,Lu Qin*

Main category: cs.DB

TL;DR: 本文提出超图中的s可达性概念和最大可达性查询问题，开发了HL-index索引结构和高效构建方法


<details>
  <summary>Details</summary>
Motivation: 超图中的可达性对于建模现实世界中的群体交互至关重要，如合著关系、社交网络和生物分析，这些关系超越了成对交互。现有方法在超图可达性查询方面存在局限性。

Method: 引入s可达性概念，定义最大可达性查询问题。提出HL-index（顶点到超边索引），开发快速覆盖关系检测方法以最小化索引，并设计轻量级邻居索引来加速构建。

Result: 在20个数据集上的广泛实验证明了该方法的效率和可扩展性。HL-index能够有效回答最大可达性查询，构建过程高效。

Conclusion: 提出的s可达性概念和HL-index为超图中的可达性查询提供了有效的解决方案，能够处理复杂的群体交互关系，在效率和可扩展性方面表现优异。

Abstract: Reachability in hypergraphs is essential for mod- eling complex groupwise interactions in real-world applications such as co-authorship, social network, and biological analysis, where relationships go beyond pairwise interactions. In this pa- per, we introduce the notion of s-reachability, where two vertices are s-reachable if there exists a sequence of hyperedges (i.e., a walk) connecting them, such that each pair of consecutive hy- peredges shares at least s vertices. Moreover, we define the max- reachability query as a generalized form of the s-reachability problem, which aims to find the largest value of s that allows one vertex to reach another. To answer max-reachability queries in hypergraphs, we first analyze limitations of the existing vertex-to- vertex and hyperedge-to-hyperedge indexing techniques. We then introduce the HL-index, a compact vertex-to-hyperedge index tailored for the max-reachability problem. To both efficiently and effectively construct a minimal HL-index, we develop a fast covering relationship detection method to eliminate fruitless hypergraph traversals during index construction. A lightweight neighbor-index is further proposed to avoid repeatedly exploring neighbor relationships in hypergraphs and hence accelerate the construction. Extensive experiments on 20 datasets demonstrate the efficiency and scalability of our approach.

</details>


### [12] [AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis](https://arxiv.org/abs/2512.23366)
*Cehua Yang,Dongyu Xiao,Junming Lin,Yuyang Song,Hanxu Yan,Shawn Guo,Wei Zhang,Jian Yang,Mingjie Tang,Bryan Dai*

Main category: cs.DB

TL;DR: 提出一个双中心框架，通过数据工厂合成高质量训练数据，并结合智能体强化学习提升Text-to-SQL系统性能，在BIRD和Spider基准上达到单模型SOTA。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL系统面临高质量训练数据稀缺和模型在复杂场景下推理能力有限的问题，需要同时从数据和模型两个角度解决。

Method: 采用双中心方法：1) 数据中心视角：构建迭代数据工厂，合成具有高正确性和精确语义逻辑对齐的RL就绪数据；2) 模型中心视角：提出智能体强化学习框架，包含多样性感知冷启动阶段和组相对策略优化(GRPO)。

Result: 在BIRD和Spider基准测试中，该协同方法在单模型方法中达到了最先进的性能。

Conclusion: 通过数据工厂和智能体强化学习的协同框架，有效解决了Text-to-SQL系统中的数据稀缺和推理能力限制问题，实现了性能的显著提升。

Abstract: The advancement of Text-to-SQL systems is currently hindered by the scarcity of high-quality training data and the limited reasoning capabilities of models in complex scenarios. In this paper, we propose a holistic framework that addresses these issues through a dual-centric approach. From a Data-Centric perspective, we construct an iterative data factory that synthesizes RL-ready data characterized by high correctness and precise semantic-logic alignment, ensured by strict verification. From a Model-Centric perspective, we introduce a novel Agentic Reinforcement Learning framework. This framework employs a Diversity-Aware Cold Start stage to initialize a robust policy, followed by Group Relative Policy Optimization (GRPO) to refine the agent's reasoning via environmental feedback. Extensive experiments on BIRD and Spider benchmarks demonstrate that our synergistic approach achieves state-of-the-art performance among single-model methods.

</details>


### [13] [Distributed Processing of kNN Queries over Moving Objects on Dynamic Road Networks](https://arxiv.org/abs/2512.23399)
*Mingjin Tao,Kailin Jiao,Yawen Li,Wei Liu,Ziqiang Yu*

Main category: cs.DB

TL;DR: 本文提出了DkNN算法，用于动态道路网络中考虑旅行成本变化的k近邻查询，采用分布式网络扩展方法替代传统索引，在Storm平台上实现并展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有kNN查询研究主要关注道路网络中的距离度量，忽略了旅行成本的动态变化。基于索引的方法在旅行成本变化时容易过时，需要新的解决方案来处理动态道路网络中的kNN查询问题。

Method: 放弃索引方法，采用增量网络扩展策略，在每个动态道路网络快照上搜索kNN。提出DkNN分布式算法，将道路网络划分为子网络进行并行探索，使用Dijkstra算法在相关区域搜索，解决局部独立子图探索中的全局距离精度问题。

Result: DkNN在Storm平台上实现，在真实道路网络场景中相比传统方法表现出更高的效率和效果，能够有效减少不相关子网络的搜索，早期检测到真正的k近邻。

Conclusion: DkNN算法成功解决了动态道路网络中考虑旅行成本变化的kNN查询问题，通过分布式网络扩展方法克服了索引方法的局限性，为位置服务提供了有效的解决方案。

Abstract: The k Nearest Neighbor (kNN) query over moving objects on road networks is essential for location-based services. Recently, this problem has been studied under road networks with distance as the metric, overlooking fluctuating travel costs. We pioneer the study of the kNN problem within dynamic road networks that account for evolving travel costs. Recognizing the limitations of index-based methods, which become quickly outdated as travel costs change, our work abandons indexes in favor of incremental network expansion on each snapshot of a dynamic road network to search for kNNs. To enhance expansion efficiency, we present DkNN, a distributed algorithm that divides the road network into sub-networks for parallel exploration using Dijkstra's algorithm across relevant regions. This approach effectively addresses challenges related to maintaining global distance accuracy during local, independent subgraph exploration, while minimizing unnecessary searches in irrelevant sub-networks and facilitating the early detection of true kNNs, despite the lack of constant global search monitoring. Implemented on the Storm platform, DkNN demonstrates superior efficiency and effectiveness over traditional methods in real-world road network scenarios.

</details>


### [14] [SPER: Accelerating Progressive Entity Resolution via Stochastic Bipartite Maximization](https://arxiv.org/abs/2512.23491)
*Dimitrios Karapiperis,George Papadakis,Themis Palpanas,Vassilios Verykios*

Main category: cs.DB

TL;DR: SPER：一种基于随机采样的渐进式实体解析框架，用线性时间复杂度的概率高通过滤器替代传统超线性排序，实现高吞吐量流数据下的高效实体匹配。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，传统批处理实体解析方法无法应对高吞吐量流数据，现有渐进式方法依赖确定性排序导致超线性复杂度和高昂初始化成本，无法扩展到高速数据流。

Method: SPER将优先级问题重新定义为采样问题而非排序问题，采用连续随机二分最大化策略替代全局排序，作为概率高通过滤器在严格线性时间内选择高效用候选对。

Result: 在8个真实数据集上的实验表明，SPER相比最先进基线实现了3-6倍的加速，同时保持相当的召回率和精度。

Conclusion: SPER通过随机采样策略突破了渐进式实体解析的可扩展性瓶颈，为高吞吐量流数据场景提供了高效解决方案，在保持匹配质量的同时显著提升了处理速度。

Abstract: Entity Resolution (ER) is a critical data cleaning task for identifying records that refer to the same real-world entity. In the era of Big Data, traditional batch ER is often infeasible due to volume and velocity constraints, necessitating Progressive ER methods that maximize recall within a limited computational budget. However, existing progressive approaches fail to scale to high-velocity streams because they rely on deterministic sorting to prioritize candidate pairs, a process that incurs prohibitive super-linear complexity and heavy initialization costs. To address this scalability wall, we introduce SPER (Stochastic Progressive ER), a novel framework that redefines prioritization as a sampling problem rather than a ranking problem. By replacing global sorting with a continuous stochastic bipartite maximization strategy, SPER acts as a probabilistic high-pass filter that selects high-utility pairs in strictly linear time. Extensive experiments on eight real-world datasets demonstrate that SPER achieves significant speedups (3x to 6x) over state-of-the-art baselines while maintaining comparable recall and precision.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair](https://arxiv.org/abs/2512.22216)
*Shaunak Samant*

Main category: cs.SE

TL;DR: 小型Transformer模型在Java程序修复中能生成语法正确的代码，但语义修复能力有限，零精确匹配且常重复错误代码


<details>
  <summary>Details</summary>
Motivation: 尽管神经模型在程序修复基准测试中表现良好，但实际部署仍有限。本研究旨在探究小型transformer模型是否能有效修复真实世界的Java bug，以及语法正确性是否能可靠代表语义正确性。

Method: 使用CodeT5-small模型（6050万参数）在CodeXGLUE的52,364个Java bug-fix对上微调，通过AST解析评估token级性能和语法有效性。

Result: 模型收敛良好，语法正确率约94%，但精确匹配评估下修复正确率为零。约80%情况下模型直接复制了错误的输入代码。

Conclusion: 小型transformer模型能生成语法正确的Java代码，但语法正确性不能可靠代表语义正确性。模型倾向于复制输入而非真正修复bug，表明当前方法在真实世界程序修复中效果有限。

Abstract: Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.
  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.

</details>


### [16] [Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks](https://arxiv.org/abs/2512.22244)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.SE

TL;DR: 该论文系统分析了在高速公路驾驶场景下，基于对象的LiDAR攻击对纵向安全控制器的影响，发现即使短暂的LiDAR诱导物体幻觉也能触发不安全制动、对真实危险响应延迟和不稳定控制行为。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究表明LiDAR感知可以通过基于对象的欺骗和注入攻击进行操纵，但此类攻击对车辆安全控制器的影响仍未得到充分理解。自动驾驶系统在感知鲁棒性和控制级安全保证之间存在关键差距。

Method: 使用高保真仿真框架，集成LiDAR感知、目标跟踪和闭环车辆控制，评估虚假和位移物体检测如何通过感知-规划-控制管道传播。研究聚焦于高速公路驾驶中现实的切入和跟车场景。

Result: 结果显示：1) 短时LiDAR诱导物体幻觉可触发不安全制动、对真实危险响应延迟和不稳定控制；2) 切入场景中，与良性条件相比，不安全减速事件和碰撞时间违规明显增加；3) 控制器失效更受欺骗物体时间一致性的影响，而非单纯空间不准确性。

Conclusion: 通过明确描述对抗性感知下的安全控制器失效模式，这项工作为设计攻击感知的安全机制和更鲁棒的LiDAR依赖自动驾驶车辆控制策略提供了实用见解，揭示了感知鲁棒性与控制级安全保证之间的关键差距。

Abstract: Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.

</details>


### [17] [Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing](https://arxiv.org/abs/2512.22250)
*Bo Yang,Yinfen Xia,Weisong Sun,Yang Liu*

Main category: cs.SE

TL;DR: SQLHD：基于蜕变测试的无标准答案Text-to-SQL幻觉检测方法，通过结构感知和逻辑感知的蜕变关系分两阶段检测模式链接和逻辑合成中的幻觉。


<details>
  <summary>Details</summary>
Motivation: LLM在Text-to-SQL任务中会产生幻觉导致错误SQL查询，现有检测方法依赖标准答案且不适用于LLM场景，需要无监督的幻觉检测方案。

Method: 提出SQLHD方法，分两阶段检测：1）模式链接幻觉检测：使用8个结构感知蜕变关系扰动比较词、实体、句子结构或数据库模式；2）逻辑合成幻觉检测：使用9个逻辑感知蜕变关系突变前缀词、极值表达式、比较范围或整个数据库。通过交叉检查LLM输出与源输出来检测违反蜕变关系的幻觉。

Result: 实验结果显示SQLHD在F1分数上表现优异，达到69.36%到82.76%，优于LLM自评估方法，能有效识别Text-to-SQL任务中的幻觉。

Conclusion: SQLHD是一种无需标准答案的有效Text-to-SQL幻觉检测方法，通过蜕变测试分阶段检测模式链接和逻辑合成中的幻觉，在LLM场景下具有优越性能。

Abstract: In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\% to 82.76\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.

</details>


### [18] [Agentic Software Issue Resolution with Large Language Models: A Survey](https://arxiv.org/abs/2512.22256)
*Zhonghao Jiang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: 该论文系统综述了126项基于LLM的智能代理软件问题解决研究，分析了任务工作流程、建立了三维分类体系（基准、技术、实证研究），并探讨了智能强化学习带来的范式转变。


<details>
  <summary>Details</summary>
Motivation: 软件问题解决是软件维护的关键环节，传统单步方法难以应对复杂现实问题所需的长程推理、迭代探索和反馈驱动决策。随着LLM推理和生成能力的发展，基于LLM的智能代理系统成为主流，需要系统梳理该领域研究进展。

Method: 对126项前沿研究进行系统综述，概述任务的一般工作流程，建立三维分类体系：基准（评估标准）、技术（方法体系）、实证研究（实际应用），并分析智能强化学习带来的范式转变。

Result: 建立了全面的分类框架，揭示了智能代理软件问题解决的研究现状，识别了智能强化学习如何改变代理系统的设计和训练方式，为连接人工智能与软件工程提供了桥梁。

Conclusion: 智能代理软件问题解决不仅极大提升了软件维护效率和质量，也为验证代理系统的推理、规划和执行能力提供了现实环境。论文总结了关键挑战并展望了未来研究方向。

Abstract: Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.
  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.

</details>


### [19] [AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents](https://arxiv.org/abs/2512.22387)
*Bhanu Prakash Vangala,Ali Adibifar,Tanu Malik,Ashish Gehani*

Main category: cs.SE

TL;DR: LLM生成的代码在干净环境中执行成功率仅68.3%，存在大量隐藏依赖，Python成功率最高(89.2%)，Java最低(44.0%)


<details>
  <summary>Details</summary>
Motivation: 研究LLM作为编码代理生成的代码在实际执行中的可复现性问题，探索其在干净环境中仅使用模型指定依赖时的执行成功率

Method: 使用三种先进LLM编码代理(Claude Code、OpenAI Codex、Gemini)，基于100个标准化提示生成300个项目(Python、JavaScript、Java各100个)，引入三层依赖框架(声称依赖、工作依赖、运行时依赖)量化执行可复现性

Result: 仅68.3%的项目能开箱即用，语言间差异显著：Python 89.2%、JavaScript 71.6%、Java 44.0%；从声明依赖到实际运行时依赖平均膨胀13.5倍，存在大量隐藏依赖

Conclusion: LLM生成的代码在实际执行中存在显著的可复现性问题，需要改进依赖管理机制，特别是对于Java等语言，当前LLM编码代理在依赖规范方面存在不足

Abstract: The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.

</details>


### [20] [Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding](https://arxiv.org/abs/2512.22418)
*Yi-Hung Chou,Boyuan Jiang,Yi Wen Chen,Mingyue Weng,Victoria Jackson,Thomas Zimmermann,James A. Jones*

Main category: cs.SE

TL;DR: 该研究通过扎根理论分析20个"氛围编程"视频，揭示了开发者使用LLM编程时的行为谱系：从完全依赖AI不检查代码，到检查并调整生成输出，所有人都需应对生成的随机性，调试常被描述为"掷骰子"。


<details>
  <summary>Details</summary>
Motivation: LLM正在重塑软件工程，催生了"氛围编程"现象，即开发者主要通过提示而非编写代码来构建软件。尽管被广泛宣传为生产力突破，但人们对从业者如何定义和实践这些方法知之甚少，需要研究这一新兴现象。

Method: 采用扎根理论研究20个氛围编程视频，包括7个直播编码会话（约16小时，254个提示）和13个观点视频（约5小时），辅以活动时长和提示意图的额外分析。

Result: 研究发现行为谱系：一些氛围编码者几乎完全依赖AI而不检查代码，而另一些则检查并调整生成输出。所有方法都必须应对生成的随机性，调试和优化常被描述为"掷骰子"。不同的心智模型（受编码者专业知识和AI依赖程度影响）影响提示策略、评估实践和信任水平。

Conclusion: 这些发现为软件工程未来研究开辟了新方向，并为工具设计和教育提供了实践机会。研究揭示了LLM编程实践中多样化的行为模式和挑战。

Abstract: Large language models (LLMs) are reshaping software engineering by enabling "vibe coding," in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as "rolling the dice." Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.

</details>


### [21] [GraphLocator: Graph-guided Causal Reasoning for Issue Localization](https://arxiv.org/abs/2512.22469)
*Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin*

Main category: cs.SE

TL;DR: GraphLocator：一种通过因果结构发现和动态问题解耦来解决软件问题定位中症状-原因不匹配和一对多不匹配的方法


<details>
  <summary>Details</summary>
Motivation: 软件问题定位任务面临两个主要挑战：症状-原因不匹配（问题描述不明确揭示根本原因）和一对多不匹配（单个问题对应多个相互依赖的代码实体），导致自然语言问题描述与源代码实现之间存在语义鸿沟。

Method: 提出GraphLocator方法，构建因果问题图（CIG），其中顶点表示发现的子问题及其关联的代码实体，边编码它们之间的因果依赖关系。方法包含两个阶段：症状顶点定位和动态CIG发现，首先在仓库图中识别症状位置，然后通过迭代推理相邻顶点动态扩展CIG。

Result: 在三个真实世界数据集上的实验表明：(1) 相比基线方法，GraphLocator在函数级召回率平均提升+19.49%，精确率提升+11.89%；(2) 在症状-原因和一对多不匹配场景下分别实现召回率提升+16.44%和+19.18%，精确率提升+7.78%和+13.23%；(3) GraphLocator生成的CIG在下游解决任务中带来28.74%的性能提升。

Conclusion: GraphLocator通过因果结构发现和动态问题解耦有效解决了软件问题定位中的两个关键不匹配问题，显著提升了定位准确性和下游任务性能。

Abstract: The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.

</details>


### [22] [Isolating Compiler Faults via Multiple Pairs of Adversarial Compilation Configurations](https://arxiv.org/abs/2512.22538)
*Qingyang Li,Yibiao Yang,Maolin Sun,Jiangchang Wu,Qingkai Shi,Yuming Zhou*

Main category: cs.SE

TL;DR: MultiConf：一种通过构建多对对抗性编译配置来自动定位编译器故障的新方法，在GCC编译器bug基准测试中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 编译器是现代软件开发的基础，但定位编译器故障到具体源文件非常困难，因为现代编译器基础设施复杂且规模庞大。

Method: 通过构建多对对抗性编译配置（每个对包含一个失败配置和对应的通过配置，仅少数细粒度选项不同），使用轻量级构造过程生成失败配置，通过选择性禁用bug相关细粒度选项推导通过配置，然后应用基于频谱的故障定位公式对编译器源文件进行可疑度排名，最后通过加权投票方案聚合排名。

Result: 在60个真实世界GCC编译器bug的基准测试中，MultiConf在Top-1文件级别成功定位了27个bug，比现有最佳方法Odfl(20)和Basic(21)分别提高了35.0%和28.6%。

Conclusion: MultiConf在编译器故障定位的有效性和效率方面显著优于现有技术，为编译器调试提供了更准确和鲁棒的解决方案。

Abstract: Compilers are fundamental to modern software development, making the effective identification and resolution of compiler faults essential. However, localizing these faults to specific source files remains highly challenging due to the complexity and scale of modern compiler infrastructures. In this study, we propose MultiConf, a novel approach that automatically isolates compiler faults by constructing multiple pairs of adversarial compilation configurations. Each adversarial compilation configuration pair consists of a failing configuration and its corresponding passing configuration, which differ in only a small number of fine-grained options. MultiConf generates failing configurations through a lightweight construction process and derives the corresponding passing configurations by selectively disabling bug-related fine-grained options. We then employ a Spectrum-Based Fault Localization (SBFL) formula to rank the suspiciousness of compiler source files. Each adversarial configuration pair independently produces a ranking, which is subsequently aggregated using a weighted voting scheme to derive a final suspiciousness ranking, enabling more accurate and robust fault localization. We evaluate MultiConf on a benchmark of 60 real-world GCC compiler bugs. The results demonstrate that MultiConf significantly outperforms existing compiler fault localization techniques in both effectiveness and efficiency. In particular, MultiConf successfully localizes 27 out of 60 bugs at the Top-1 file level, representing improvements of 35.0% and 28.6% over the two state-of-the-art approaches, Odfl(20) and Basic(21), respectively.

</details>


### [23] [Rethinking the Capability of Fine-Tuned Language Models for Automated Vulnerability Repair](https://arxiv.org/abs/2512.22633)
*Woorim Han,Yeongjun Kwak,Miseon Yu,Kyeongmin Kim,Younghan Lee,Hyungon Moon,Yunheung Paek*

Main category: cs.SE

TL;DR: 该论文评估了基于学习的自动化漏洞修复（AVR）模型的泛化能力和评估指标的有效性，发现现有模型存在过拟合问题，并提出新的测试基准L-AVRBench来更准确地评估修复能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于微调语言模型的自动化漏洞修复技术虽然显示出潜力，但其修复未见漏洞的能力存在疑问。现有模型可能过拟合训练数据，且评估指标存在局限性，无法准确衡量模型的实际修复能力。

Method: 1. 对测试集应用语义保持变换，检验模型是否学习到稳健的漏洞修复模式而非依赖虚假特征；2. 重新划分互斥的训练、验证和测试集，评估模型泛化能力；3. 提出专门针对学习型AVR的基于测试的基准L-AVRBench，克服基于匹配度量的局限性。

Result: 研究发现最先进的模型经常过拟合训练集，且现有评估使用的数据集划分不互斥。基于匹配的评估指标存在局限性，无法考虑漏洞修复的多种有效方式。

Conclusion: 需要更严格的评估方法和更准确的基准来评估基于学习的AVR模型的真实修复能力，L-AVRBench为这一目标提供了更好的评估框架。

Abstract: Learning-based automated vulnerability repair (AVR) techniques that utilize fine-tuned language models have shown promise in generating vulnerability patches. However, questions remain about their ability to repair unseen vulnerabilities. Our empirical study reveals that state-of-the-art models often overfit to the training set and are evaluated using training, validation, and test sets that are not mutually exclusive. Furthermore, relying on match-based metrics that compare generated patches to reference fixes at the token level has some limitations, failing to account for the possibility of various valid ways to patch the vulnerability. In this paper, we examine the capabilities of state-of-the-art fine-tuned AVR models and the adequacy of match-based evaluation metrics in three ways. First, we apply semantic-preserving transformations to test sets in order to determine whether models truly learn robust vulnerability-repair patterns or simply rely on spurious features. Second, we re-split the training, validation, and test sets to be mutually exclusive and evaluate the models on the revised test set to assess their generalization capabilities. Third, we introduce L-AVRBench, a test-based benchmark tailored for learning-based AVR, to overcome the limitations of match-based metrics and examine the AVR models' true repair capabilities.

</details>


### [24] [CFIghter: Automated Control-Flow Integrity Enablement and Evaluation for Legacy C/C++ Systems](https://arxiv.org/abs/2512.22701)
*Sabine Houy,Bruno Kreyssig,Alexandre Bartel*

Main category: cs.SE

TL;DR: CFIghter：首个全自动系统，通过检测、分类和修复测试套件暴露的意外策略违规，在真实项目中实现严格类型化CFI，无需手动修改源代码。


<details>
  <summary>Details</summary>
Motivation: 编译器控制流完整性（CFI）提供强大的前向边保护，但由于可见性不匹配、类型不一致和意外行为故障，在大型C/C++软件中部署仍具挑战性。

Method: CFIghter集成全程序分析与引导式运行时监控，迭代应用最小必要调整到CFI强制执行，仅在需要的地方进行调整，直到所有测试通过或剩余故障被认为无法解决。

Result: 在四个GNU项目上评估：解决了所有可见性相关的构建错误，在大型多库util-linux代码库中自动修复了95.8%的意外CFI违规，同时在超过89%的间接控制流站点保持严格执行。

Conclusion: 自动化兼容性修复使严格编译器CFI在成熟、模块化的C软件中实际可部署，无需手动源代码更改，仅依赖自动生成的可见性调整和局部化执行范围。

Abstract: Compiler-based Control-Flow Integrity (CFI) offers strong forward-edge protection but remains challenging to deploy in large C/C++ software due to visibility mismatches, type inconsistencies, and unintended behavioral failures. We present CFIghter, the first fully automated system that enables strict, type-based CFI in real-world projects by detecting, classifying, and repairing unintended policy violations exposed by the test suite. CFIghter integrates whole-program analysis with guided runtime monitoring and iteratively applies the minimal necessary adjustments to CFI enforcement only where required, stopping once all tests pass or remaining failures are deemed unresolvable. We evaluate CFIghter on four GNU projects. It resolves all visibility-related build errors and automatically repairs 95.8% of unintended CFI violations in the large, multi-library util-linux codebase, while retaining strict enforcement at over 89% of indirect control-flow sites. Across all subjects, CFIghter preserves strict type-based CFI for the majority of the codebase without requiring manual source-code changes, relying only on automatically generated visibility adjustments and localized enforcement scopes where necessary. These results show that automated compatibility repair makes strict compiler CFI practically deployable in mature, modular C software.

</details>


### [25] [From Rookie to Expert: Manipulating LLMs for Automated Vulnerability Exploitation in Enterprise Software](https://arxiv.org/abs/2512.22753)
*Moustapha Awwalou Diouf,Maimouna Tamah Diao,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Jordan Samhi,Gervais Mendy,Samuel Ouya,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: LLMs使非程序员能生成功能性的漏洞利用程序，成功率达到100%，挑战了传统软件安全假设


<details>
  <summary>Details</summary>
Motivation: LLMs的普及使非程序员也能创建应用程序，但这种可访问性破坏了数十年来指导软件工程的安全假设。需要研究LLMs如何可能被社会工程攻击操纵，将新手转变为有能力的攻击者

Method: 提出RSA策略（角色分配、场景伪装和行动诱导），通过社会工程攻击操纵LLMs绕过安全机制生成功能性的漏洞利用程序。在Odoo ERP平台上测试了5个主流LLM（GPT-4o、Gemini、Claude、Microsoft Copilot和DeepSeek）

Result: 实现了100%的成功率：所有测试的CVE漏洞都能在3-4轮提示内生成至少一个可工作的漏洞利用程序。相比之前需要人工努力的研究，完全消除了攻击开销

Conclusion: 这代表了软件工程的范式转变：技术与非技术人员的区分不再有效；漏洞描述的技术复杂性无法提供保护；传统安全边界瓦解。需要重新设计安全实践，因为现在攻击只需要制作提示的能力，而不需要理解代码

Abstract: LLMs democratize software engineering by enabling non-programmers to create applications, but this same accessibility fundamentally undermines security assumptions that have guided software engineering for decades. We show in this work how publicly available LLMs can be socially engineered to transform novices into capable attackers, challenging the foundational principle that exploitation requires technical expertise. To that end, we propose RSA (Role-assignment, Scenario-pretexting, and Action-solicitation), a pretexting strategy that manipulates LLMs into generating functional exploits despite their safety mechanisms. Testing against Odoo -- a widely used ERP platform, we evaluated five mainstream LLMs (GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek) and achieved a 100% success rate: tested CVE yielded at least one working exploit within 3-4 prompting rounds. While prior work [13] found LLM-assisted attacks difficult and requiring manual effort, we demonstrate that this overhead can be eliminated entirely.
  Our findings invalidate core software engineering security principles: the distinction between technical and non-technical actors no longer provides valid threat models; technical complexity of vulnerability descriptions offers no protection when LLMs can abstract it away; and traditional security boundaries dissolve when the same tools that build software can be manipulated to break it. This represents a paradigm shift in software engineering -- we must redesign security practices for an era where exploitation requires only the ability to craft prompts, not understand code.
  Artifacts available at: https://anonymous.4open.science/r/From-Rookie-to-Attacker-D8B3.

</details>


### [26] [FasterPy: An LLM-based Code Execution Efficiency Optimization Framework](https://arxiv.org/abs/2512.22827)
*Yue Wu,Minghao Han,Ruiyin Li,Peng Liang,Amjed Tahir,Zengyang Li,Qiong Feng,Mojtaba Shahin*

Main category: cs.SE

TL;DR: FasterPy是一个基于大语言模型的Python代码优化框架，结合检索增强生成和低秩适应技术，在PIE基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法需要手动设计和维护特定性能bug的规则，劳动密集且适用范围有限。基于机器学习和深度学习的方法依赖特定程序表示和精心构建的训练数据集，开发成本高且难以扩展。大语言模型在代码生成方面的卓越能力为自动化代码优化提供了新途径。

Method: FasterPy结合检索增强生成（RAG）和低秩适应（LoRA）技术。RAG由现有性能改进代码对和相应性能测量构建的知识库支持，LoRA用于增强代码优化性能。

Result: 在Performance Improving Code Edits（PIE）基准测试中，该方法在多个指标上优于现有模型。

Conclusion: FasterPy是一个低成本、高效的框架，能够适应大语言模型来优化Python代码的执行效率，为自动化代码优化提供了有效的解决方案。

Abstract: Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.

</details>


### [27] [Towards the analysis of team members well-being](https://arxiv.org/abs/2512.22845)
*Zan Xu,Sari Nurfauziyyah,Anastasia Romanova,Kaamesh G S,Yiqun Gao,Maria Spichkova*

Main category: cs.SE

TL;DR: 该研究关注软件开发团队成员的幸福感，开发了团队幸福感分析原型系统，强调认可和赞赏对员工幸福感的重要性。


<details>
  <summary>Details</summary>
Motivation: 软件开发团队成员的幸福感不仅影响工作生产力和绩效，还关系到员工的身体健康和个人生活。研究表明，团队成员是否感到自己的贡献被认可和赞赏是影响幸福感的重要因素。

Method: 开展了一个关于团队幸福感分析的项目，并在项目中开发了一个原型系统。

Result: 论文展示了团队幸福感分析项目的结果以及在该项目中开发的原型系统。

Conclusion: 团队成员的认可感和贡献被赞赏是影响软件开发团队幸福感的关键因素，通过分析工具可以更好地理解和改善团队幸福感。

Abstract: Many recent research studies have focused on the well-being of software development team members, as this aspect may be critical not only for productivity and performance at work but also for the physical health and personal life of employees. Many studies agree that an important factor of team member well-being is whether team members feel appreciated and acknowledged for their contributions. This paper presents the results of a project on the team well-being analysis as well as the prototype developed within the project.

</details>


### [28] [Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI](https://arxiv.org/abs/2512.23033)
*Fuyad Hasan Bhoyan,Prashanta Sarker,Parsia Noor Ethila,Md. Emon Hossain,Md Kaviul Hossain,Md Humaion Kabir Mehedi*

Main category: cs.SE

TL;DR: 开发基于AI的胆囊疾病超声诊断软件，使用MobResTaNet混合深度学习模型直接对超声图像进行10类分类（9种疾病+正常），通过XAI提供可解释的实时预测，部署为网页和移动应用


<details>
  <summary>Details</summary>
Motivation: 胆囊疾病的早期准确检测至关重要，但超声图像解读具有挑战性，需要开发AI辅助诊断工具来改善诊断效率和准确性

Method: 开发AI驱动诊断软件，集成混合深度学习模型MobResTaNet，直接从超声图像分类10个类别（9种胆囊疾病类型和正常），通过可解释AI（XAI）可视化提供实时预测，使用HTML、CSS、JavaScript、Bootstrap和Flutter技术部署为网页和移动应用

Result: 系统达到高达99.85%的准确率，仅使用2.24M参数，提供高效、可访问且可信赖的床边诊断支持

Conclusion: 该AI驱动诊断软件能够为胆囊疾病提供准确、实时且可解释的超声图像分类，通过网页和移动应用部署，支持透明临床决策，具有临床实用价值

Abstract: Early and accurate detection of gallbladder diseases is crucial, yet ultrasound interpretation is challenging. To address this, an AI-driven diagnostic software integrates our hybrid deep learning model MobResTaNet to classify ten categories, nine gallbladder disease types and normal directly from ultrasound images. The system delivers interpretable, real-time predictions via Explainable AI (XAI) visualizations, supporting transparent clinical decision-making. It achieves up to 99.85% accuracy with only 2.24M parameters. Deployed as web and mobile applications using HTML, CSS, JavaScript, Bootstrap, and Flutter, the software provides efficient, accessible, and trustworthy diagnostic support at the point of care

</details>


### [29] [An Automated Grey Literature Extraction Tool for Software Engineering](https://arxiv.org/abs/2512.23066)
*Houcine Abdelkader Cherief,Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Sti'evenart,Florent Avellaneda*

Main category: cs.SE

TL;DR: GLiSE是一个基于提示驱动的工具，用于大规模收集和评估软件工程灰色文献，通过语义分类器过滤和排序搜索结果，提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 灰色文献对软件工程研究至关重要，但因其来源、格式和API的异构性，大规模收集和评估困难，阻碍了可重复的大规模综合研究。

Method: GLiSE将研究主题提示转换为平台特定查询，从GitHub、Stack Overflow和Google搜索收集结果，使用基于嵌入的语义分类器根据相关性过滤和排序结果。

Result: 开发了GLiSE工具，提供了软件工程灰色文献搜索结果的精选数据集，并进行了工具可用性的实证研究。

Conclusion: GLiSE解决了灰色文献收集的可重复性和大规模综合问题，为软件工程研究提供了实用的工具和数据集。

Abstract: Grey literature is essential to software engineering research as it captures practices and decisions that rarely appear in academic venues. However, collecting and assessing it at scale remains difficult because of their heterogeneous sources, formats, and APIs that impede reproducible, large-scale synthesis. To address this issue, we present GLiSE, a prompt-driven tool that turns a research topic prompt into platform-specific queries, gathers results from common software-engineering web sources (GitHub, Stack Overflow) and Google Search, and uses embedding-based semantic classifiers to filter and rank results according to their relevance. GLiSE is designed for reproducibility with all settings being configuration-based, and every generated query being accessible. In this paper, (i) we present the GLiSE tool, (ii) provide a curated dataset of software engineering grey-literature search results classified by semantic relevance to their originating search intent, and (iii) conduct an empirical study on the usability of our tool.

</details>


### [30] [An Empirical Study of Generative AI Adoption in Software Engineering](https://arxiv.org/abs/2512.23327)
*Görkem Giray,Onur Demirörs,Marcos Kalinowski,Daniel Mendez*

Main category: cs.SE

TL;DR: GenAI工具在软件工程中已被广泛采用，主要用于实现、验证、个人辅助和维护任务，带来生产力提升但面临输出可靠性、安全隐私等挑战，预计会重新定义而非取代工程师角色。


<details>
  <summary>Details</summary>
Motivation: 尽管GenAI工具在软件工程领域的采用日益增加，但缺乏关于实际使用情况、效益、挑战及其组织和社会影响的实证证据。本研究旨在了解GenAI在SE中的采用现状。

Method: 研究通过调查分析GenAI在软件工程中的采用状态，包括采用程度、相关效益与挑战、工具技术制度化情况，以及对SE专业人员和社区的长期影响预期。

Result: 结果显示GenAI工具被广泛采用并深度融入日常SE工作，特别是在实现、验证与验证、个人辅助和维护相关任务中。实践者报告了显著的效益，包括周期时间缩短、质量改进、知识工作支持增强和生产力提升。然而，生产力和质量的客观测量在实践中仍然有限。重大挑战持续存在，包括输出不正确或不可靠、提示工程困难、验证开销、安全和隐私问题以及过度依赖风险。工具和技术的制度化似乎很常见，但差异很大，重点关注工具访问，较少强调培训和管理。实践者期望GenAI重新定义而非取代他们的角色，同时对就业市场收缩和技能转变表示适度担忧。

Conclusion: GenAI在软件工程中已被广泛采用并带来显著效益，但仍面临可靠性、安全性和过度依赖等挑战。制度化程度不一，未来GenAI将重新定义而非取代SE角色，需要平衡工具访问与适当培训和管理。

Abstract: Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.

</details>


### [31] [Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?](https://arxiv.org/abs/2512.23385)
*The Anh Nguyen,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: 该研究通过分析Hugging Face和GitHub上的开发者讨论，构建了一个包含312,868个安全讨论的数据集，识别出AI供应链中的32种安全问题和24种解决方案，揭示了AI组件复杂依赖性和黑盒特性带来的安全挑战。


<details>
  <summary>Details</summary>
Motivation: AI模型和应用的快速增长带来了复杂的安全格局，开发者不仅面临传统软件供应链问题，还有AI特有的安全威胁。然而，实践中常见的安全问题及其解决方案尚不明确，这阻碍了针对AI供应链各组件有效安全措施的开发。

Method: 通过结合关键词匹配和优化的distilBERT分类器构建管道，从Hugging Face和GitHub收集开发者讨论，创建了312,868个安全讨论的数据集。对753个样本进行主题分析，建立了细粒度的安全分类体系。

Result: 识别出32种安全问题和24种解决方案，分为四个主题：(1)系统和软件、(2)外部工具和生态系统、(3)模型、(4)数据。发现许多安全问题源于AI组件的复杂依赖性和黑盒特性，特别是模型和数据相关的挑战往往缺乏具体解决方案。

Conclusion: 该研究为开发者和研究人员提供了基于证据的指导，帮助他们应对AI供应链中的实际安全威胁，填补了AI安全实践知识空白，并揭示了需要进一步研究的领域。

Abstract: The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.

</details>


### [32] [An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes](https://arxiv.org/abs/2512.23415)
*Vinoth Punniyamoorthy,Bikesh Kumar,Sumit Saha,Lokesh Butra,Mayilsamy Palanigounder,Akash Kumar Agarwal,Kabilan Kannan*

Main category: cs.SE

TL;DR: 论文提出基于AIOps的Kubernetes智能弹性伸缩框架，通过多信号集成、SLO感知和成本控制，相比原生方案减少SLO违规31%、提升响应时间24%、降低成本18%


<details>
  <summary>Details</summary>
Motivation: Kubernetes原生弹性伸缩机制存在响应式行为、应用级信号利用有限、控制逻辑不透明等问题，导致生产环境中频繁出现SLO违规和成本效率低下

Method: 提出安全可解释的多信号弹性伸缩框架，集成SLO感知和成本控制，结合轻量级需求预测，采用AIOps原则增强Kubernetes弹性伸缩能力

Result: 在代表性微服务和事件驱动负载测试中，相比默认和调优的Kubernetes基线，SLO违规时长减少31%，伸缩响应时间提升24%，基础设施成本降低18%

Conclusion: AIOps驱动的SLO优先弹性伸缩能显著提升Kubernetes云平台的可靠性、效率和操作可信度，实现SLO和成本约束的双重优化

Abstract: Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.

</details>


### [33] [Embedding Quality Assurance in project-based learning](https://arxiv.org/abs/2512.23488)
*Maria Spichkova*

Main category: cs.SE

TL;DR: 基于十多年在敏捷/Scrum环境下教授软件工程课程的经验，分享软件质量教学的经验教训，并提供在基于项目的敏捷学习中嵌入质量保证主题的建议。


<details>
  <summary>Details</summary>
Motivation: 作者拥有十多年在敏捷/Scrum环境下教授软件工程课程的经验，特别是在毕业年级软件开发项目和软件工程项目管理课程中。他们希望分享这些经验教训，帮助其他教育工作者更好地在敏捷环境中教授软件质量保证。

Method: 基于十多年的教学实践经验，总结在敏捷/Scrum环境下教授软件质量方面的经验教训。通过分析毕业年级软件开发项目和软件工程项目管理课程的教学实践，识别有效的教学方法和面临的挑战。

Result: 提供了在基于项目的敏捷学习中嵌入质量保证主题的具体建议。这些建议基于实际教学经验，旨在帮助教育工作者更有效地将质量保证概念整合到敏捷开发环境中。

Conclusion: 在敏捷/Scrum环境下教授软件质量需要特定的教学策略和方法。通过分享这些经验教训和建议，可以帮助教育工作者更好地设计和实施相关课程，培养学生在敏捷环境中确保软件质量的能力。

Abstract: In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.

</details>


### [34] [Adaptable Teastore with Energy Consumption Awareness: A Case Study](https://arxiv.org/abs/2512.23498)
*Henrique De Medeiros,Denisse Muñante,Sophie Chabridon,César Perdigão Batista,Denis Conan*

Main category: cs.SE

TL;DR: EnCoMSAS工具用于监控自适应性系统的能耗，支持运行时动态调整以提升能效，对系统整体能耗影响较小。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗持续增长，云应用迁移和数字内容消费加剧了这一问题。动态自适应方法有望在运行时降低软件应用的能耗，但缺乏有效的能耗监控工具来实现能耗感知。

Method: 开发EnCoMSAS工具来收集分布式云应用的能耗数据，支持SAS变体的运行时能耗评估。使用Adaptable TeaStore案例研究进行实证评估，通过Grid5000测试平台在不同工作负载条件下测试推荐服务的能耗。

Result: EnCoMSAS能有效收集能耗数据支持运行时动态调整；CPU使用率与能耗的相关性验证了测量有效性；能耗受算法复杂度和部署环境影响；EnCoMSAS对SAS生态系统整体能耗影响相对较小。

Conclusion: EnCoMSAS工具为自适应性系统提供了有效的能耗监控能力，支持能耗感知的动态调整，且对系统整体能耗影响有限，有助于实现更节能的软件系统。

Abstract: [Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.
  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.
  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.
  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.

</details>


### [35] [AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices](https://arxiv.org/abs/2512.23499)
*Brice Arléon Zemtsop Ndadji,Simon Bliudze,Clément Quinton*

Main category: cs.SE

TL;DR: AdaptiFlow是一个基于自主计算原则的框架，通过解耦监控与执行阶段，使微服务能够自主适应动态云环境，支持自愈、自保护和自优化三种自主性级别。


<details>
  <summary>Details</summary>
Motivation: 现代云架构需要自适应能力来管理动态运行条件，但现有解决方案通常采用集中式控制模型，不适合微服务的去中心化特性。需要一种能够保持微服务架构独立性同时实现系统范围适应性的框架。

Method: AdaptiFlow框架基于MAPE-K循环的监控和执行阶段，提供三个核心组件：(1)统一的基础设施/业务指标收集器；(2)声明式执行器用于运行时调整；(3)轻量级事件驱动和基于规则的适应逻辑规范机制。通过标准化接口实现指标收集和动作执行与适应逻辑的解耦。

Result: 通过增强的Adaptable TeaStore基准测试验证了框架的实用性，实现了三种适应场景：数据库恢复（自愈）、DDoS缓解（自保护）和流量管理（自优化），每个服务只需最小代码修改。证明了去中心化适应可以通过局部决策实现，无需全局协调。

Conclusion: AdaptiFlow将自主计算理论与云原生实践相结合，为构建弹性分布式系统提供了概念框架和具体工具。未来工作包括与正式协调模型的集成，以及应用基于AI代理的适应技术来处理更复杂的适应场景。

Abstract: Modern cloud architectures demand self-adaptive capabilities to manage dynamic operational conditions. Yet, existing solutions often impose centralized control models ill-suited to microservices decentralized nature. This paper presents AdaptiFlow, a framework that leverages well-established principles of autonomous computing to provide abstraction layers focused on the Monitor and Execute phases of the MAPE-K loop. By decoupling metrics collection and action execution from adaptation logic, AdaptiFlow enables microservices to evolve into autonomous elements through standardized interfaces, preserving their architectural independence while enabling system-wide adaptability. The framework introduces: (1) Metrics Collectors for unified infrastructure/business metric gathering, (2) Adaptation Actions as declarative actuators for runtime adjustments, and (3) a lightweight Event-Driven and rule-based mechanism for adaptation logic specification. Validation through the enhanced Adaptable TeaStore benchmark demonstrates practical implementation of three adaptation scenarios targeting three levels of autonomy self-healing (database recovery), self-protection (DDoS mitigation), and self-optimization (traffic management) with minimal code modification per service. Key innovations include a workflow for service instrumentation and evidence that decentralized adaptation can emerge from localized decisions without global coordination. The work bridges autonomic computing theory with cloud-native practice, providing both a conceptual framework and concrete tools for building resilient distributed systems. Future work includes integration with formal coordination models and application of adaptation techniques relying on AI agents for proactive adaptation to address complex adaptation scenarios.

</details>


### [36] [Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving](https://arxiv.org/abs/2512.23511)
*Xinyi Zheng,Ningke Li,Xiaokun Luan,Kailong Wang,Ling Shi,Meng Sun,Haoyu Wang*

Main category: cs.SE

TL;DR: MATP是一个通过多步自动定理证明系统验证LLM推理逻辑的评估框架，将自然语言推理转化为一阶逻辑，使用自动定理证明器评估逻辑有效性，在10,830个推理实例上超越基线方法42个百分点。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗、法律、科学等高风险领域展现出强大推理能力，但其流畅语言常掩盖细微逻辑错误，现有方法如事实核查、自一致性方法和基于规则的验证无法检测多步推理中的复杂逻辑缺陷。

Method: MATP将自然语言推理转化为一阶逻辑，应用自动定理证明器进行逐步逻辑有效性评估，识别隐藏逻辑错误并提供细粒度推理正确性分类。

Result: 在包含10,830个推理实例的基准测试中（来自PrOntoQA-OOD、ProofWriter和FOLIO任务，由10个LLM生成），MATP在推理步骤验证上超越基于提示的基线方法超过42个百分点，并显示推理模型比通用模型产生更逻辑一致的输出。

Conclusion: MATP框架通过系统性的逻辑验证，能够显著提升LLM生成推理的可信度，为高风险应用中的LLM部署提供更可靠的评估工具。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.
  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.

</details>


### [37] [Model-based Development for Autonomous Driving Software Considering Parallelization](https://arxiv.org/abs/2512.23575)
*Kenshin Obi,Takumi Onozawa,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 提出基于模型开发（MBD）的自动驾驶软件并行化方法，扩展现有MBP方法以处理复杂处理，减少执行时间，满足实时性要求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶软件需要实时性能，但现有方法在处理多种功能和复杂环境时面临挑战，需要更有效的并行化方法来满足实时性要求。

Method: 扩展现有的基于模型并行化器（MBP）方法，在模型开发（MBD）过程中实现自动驾驶软件的并行化，简化复杂处理的实现。

Result: 执行时间得到减少，评估结果表明该方法适用于自动驾驶软件开发，特别是在实现实时性能方面表现良好。

Conclusion: 提出的基于MBD的并行化方法能有效提升自动驾驶软件的实时性能，适合复杂自动驾驶系统的开发需求。

Abstract: In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.

</details>


### [38] [Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes](https://arxiv.org/abs/2512.23605)
*Kenshin Obi,Ryo Yoshinaka,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: 提出一个基于模型开发（MBD）的框架，用于将ROS 2兼容的Simulink模型自动并行化，支持多输入场景，减少执行时间。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统（特别是自动驾驶系统）的复杂性和规模不断增加，传统手动并行化面临数据完整性和并发问题挑战，现有MBD方法难以集成ROS 2等现代框架的多输入场景。

Method: 提出MBD框架，将ROS 2兼容的Simulink模型分类为事件驱动和定时器驱动类型进行针对性并行化，扩展传统MBD并行化方法，支持ROS 2多输入模型的并行代码生成。

Result: 评估结果显示，应用所提框架进行并行化后，所有模式都显示出执行时间减少，证实了并行化的有效性。

Conclusion: 该框架成功解决了ROS 2集成和多输入场景下的并行化挑战，扩展了MBD在自动驾驶系统中的应用能力。

Abstract: In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [39] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: GPU-Virt-Bench是一个全面的GPU虚拟化基准测试框架，用于评估软件GPU虚拟化系统（如HAMi-core和BUD-FCSP）与硬件MIG技术的性能差异，涵盖56个性能指标和10个类别。


<details>
  <summary>Details</summary>
Motivation: 随着AI和LLM推理等GPU加速工作负载的激增，云和容器环境对高效GPU资源共享的需求急剧增加。虽然NVIDIA的MIG技术提供硬件级隔离，但仅适用于高端数据中心GPU。软件虚拟化方案缺乏标准化的评估方法，需要系统化的基准测试框架来指导生产部署决策。

Method: 开发了GPU-Virt-Bench基准测试框架，包含56个性能指标，组织成10个类别：开销、隔离质量、LLM特定性能、内存带宽、缓存行为、PCIe吞吐量、多GPU通信、调度效率、内存碎片化和错误恢复。该框架用于评估HAMi-core、BUD-FCSP等软件虚拟化方案，并与模拟的MIG基线进行比较。

Result: GPU-Virt-Bench框架能够系统比较软件虚拟化方法与理想MIG行为，为多租户环境中部署GPU资源提供可操作的见解。通过对HAMi-core、BUD-FCSP和模拟MIG基线的评估，揭示了生产部署决策所需的关键性能特征。

Conclusion: GPU-Virt-Bench填补了GPU虚拟化评估领域的空白，提供了一个标准化的基准测试框架，帮助从业者在软件虚拟化方案和硬件MIG技术之间做出明智选择，优化多租户GPU资源管理。

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [40] [SoDA: An Efficient Interaction Paradigm for the Agentic Web](https://arxiv.org/abs/2512.22135)
*Zicai Cui,Zhouyuan Jian,Weiwen Liu,Weinan Zhang*

Main category: cs.DC

TL;DR: 论文提出SoDA（主权数字化身）作为面向Agentic Web时代的新型交互范式，通过存储、计算、交互的正交解耦设计，解决数据锁定和认知过载问题，实现从"消磨时间"到"节省时间"的根本转变。


<details>
  <summary>Details</summary>
Motivation: 随着互联网从移动App主导的注意力经济向Agentic Web时代的意图互联演进，现有交互模式无法解决日益严重的数据锁定和认知过载问题。需要建立面向未来的用户主权交互范式。

Method: 提出主权数字化身（SoDA）架构，采用存储、计算、交互的正交解耦设计，将数据作为持久资产、模型作为临时工具。设计基于A2A协议的意图-权限握手机制，通过双因子（敏感系数和严格参数）自适应路由实现主动风险治理。

Result: 在跨平台服务迁移和复杂任务执行中减少约27-35%的token消耗；在多模态复杂任务编排中，相比标准RAG架构降低72%用户认知负载，相比手动工作流降低88%，同时显著提升信息信噪比。

Conclusion: SoDA是构建高效、低摩擦、去中心化Agentic Web的必备交互基础设施，通过解耦记忆与应用逻辑、从显式手动指令转向隐式意图对齐，实现了用户主权和认知减负。

Abstract: As the internet evolves from the mobile App-dominated Attention Economy to the Intent-Interconnection of the Agentic Web era, existing interaction modes fail to address the escalating challenges of data lock-in and cognitive overload. Addressing this, we defines a future-oriented user sovereignty interaction paradigm, aiming to realize a fundamental shift from killing time to saving time. Specifically, we argue that decoupling memory from application logic eliminates the structural basis of data lock-in, while shifting from explicit manual instruction to implicit intent alignment resolves cognitive overload by offloading execution complexity. This paradigm is implemented via the Sovereign Digital Avatar (SoDA), which employs an orthogonal decoupling design of storage, computation, and interaction. This establishes the architectural principle of data as a persistent asset, model as a transient tool, fundamentally breaking the platform monopoly on user memory. To support the operation of this new paradigm in zero-trust environments, we design an Intent-Permission Handshake Mechanism based on A2A protocols, utilizing dual-factor (Sensitivity Coefficient and Strictness Parameter) adaptive routing to achieve active risk governance. Empirical evaluation with a high-fidelity simulation environment indicates that this paradigm reduces token consumption by approximately 27-35\% during cross-platform service migration and complex task execution. Furthermore, in the orchestration of multi-modal complex tasks, it reduces user cognitive load by 72\% compared to standard Retrieval-Augmented Generation (RAG) architectures, by 88\% relative to manual workflows, while significantly boosting the Information Signal-to-Noise Ratio (SNR). These results demonstrate that the SoDA is the essential interaction infrastructure for building an efficient, low-friction, and decentralized Agentic Web.

</details>


### [41] [SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware](https://arxiv.org/abs/2512.22136)
*Mahadev Sunil Kumar,Arnab Raha,Debayan Das,Gopakumar G,Amitava Mukherjee*

Main category: cs.DC

TL;DR: 提出一种面向分布式边缘设备的DNN高效部署方法，通过结构化剪枝和多目标优化，在满足硬件约束的同时保持任务性能，在MVCNN上实现1.2-5.0倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 深度分布式网络在计算机视觉中应用广泛，但其庞大的参数量和计算需求阻碍了在资源受限的边缘设备上的部署。需要一种既能满足硬件限制又能保持任务性能的高效部署方案。

Method: 结合结构化模型剪枝和多目标优化，根据异构设备约束调整网络容量。以MVCNN（多视图卷积神经网络）为例，量化各个视图对分类准确率的贡献，并相应分配剪枝预算。

Result: 实验结果表明，生成的模型在满足用户指定的准确率和内存占用限制的同时，在不同硬件平台上实现了1.2倍到5.0倍的推理延迟降低。

Conclusion: 性能感知、视图自适应的压缩方法为在分布式边缘环境中部署复杂视觉模型提供了可行路径。

Abstract: Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respects hardware limitations and preserves task performance. Our method integrates a structured model pruning with a multi-objective optimization to tailor network capacity to heterogeneous device constraints. We demonstrate this framework using Multi-View Convolutional Neural Network (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets, respectively. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint while reducing inference latency by factors ranging from 1.2x to 5.0x across diverse hardware platforms. These findings suggest that performance-aware, view-adaptive compression provides a viable pathway for deploying complex vision models in distributed edge environments.

</details>


### [42] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: HybridFlow是一个资源自适应的边缘-云协作推理框架，通过细粒度任务分解和并行执行来减少LLM推理延迟和token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在边缘设备上部署面临推理延迟高和token消耗大的挑战。现有的边缘-云协作方法采用粗粒度任务分配策略，无法充分利用细粒度推理并行性，导致冗余计算和资源利用效率低下。

Method: HybridFlow采用两阶段方法：1) 任务分解和并行执行，动态将复杂查询拆分为相互依赖的子任务；2) 资源感知子任务路由，通过学习型路由器根据预测的效用增益和实时预算状态自适应地将子任务分配给边缘或云端模型。

Result: 在GPQA、MMLU-Pro、AIME和LiveBench-Reasoning等基准测试上的综合评估表明，HybridFlow能有效减少端到端推理时间和总体token使用量，同时保持有竞争力的准确性。

Conclusion: HybridFlow通过细粒度的边缘-云协作推理，实现了更快的推理速度和更低的token消耗，为资源受限的边缘设备部署大型语言模型提供了有效的解决方案。

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [43] [HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA](https://arxiv.org/abs/2512.22139)
*Amur Saqib Pal,Muhammad Mohsin Ghaffar,Faisal Shafait,Christian Weis,Norbert Wehn*

Main category: cs.DC

TL;DR: HLS4PC：基于FPGA加速的3D点云处理框架，通过硬件感知压缩技术优化PointMLP模型，实现比GPU/CPU更高的吞吐量


<details>
  <summary>Details</summary>
Motivation: 3D点云数据稀疏、非结构化的特性导致传统基于GPU的方法存在内存和计算需求高、GPU利用率低的问题，难以满足安全关键应用的实时性要求

Method: 提出HLS4PC参数化HLS框架，利用FPGA并行化和算法优化实现高效的定点数映射和神经网络函数。采用硬件感知压缩技术：用URS替代FPS、参数量化、层融合和输入点剪枝，将PointMLP-Elite优化为复杂度降低4倍的PointMLP-Lite

Result: PointMLP-Lite在ModelNet40上仅损失2%精度；FPGA加速实现比先前工作高3.56倍吞吐量，比GPU和CPU实现分别高2.3倍和22倍吞吐量

Conclusion: HLS4PC框架通过FPGA加速和硬件感知优化，显著提升了3D点云处理的效率和实时性能，为安全关键应用提供了可行的解决方案

Abstract: Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering real-time performance in safety critical applications due to GPU under-utilization. To address this challenge, we present HLS4PC, a parameterizable HLS framework for FPGA acceleration. Our approach leverages FPGA parallelization and algorithmic optimizations to enable efficient fixed-point implementations of both mapping and NN functions. We explore several hardware-aware compression techniques on a state-of-the-art PointMLP-Elite model, including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning, yielding PointMLP-Lite, a 4x less complex variant with only 2% accuracy drop on ModelNet40. Secondly, we demonstrate that the FPGA acceleration of the PointMLP-Lite results in 3.56x higher throughput than previous works. Furthermore, our implementation achieves 2.3x and 22x higher throughput compared to the GPU and CPU implementations, respectively.

</details>


### [44] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: Cleave是一个去中心化基础模型训练框架，通过选择性混合张量并行和参数服务器架构，在边缘设备上实现与云端训练相当的性能，解决了现有边缘训练方法在可扩展性、内存限制和通信开销方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型训练高度中心化，依赖大型云数据中心，成本高昂。利用边缘设备的闲置计算资源进行去中心化训练是一个有前景的替代方案，但现有边缘训练方法存在性能不足、可扩展性差、内存限制、通信开销大等问题，且无法有效处理设备异构性和动态性。

Method: 提出Cleave框架：1）采用选择性混合张量并行方法精细划分训练操作；2）基于参数服务器的训练架构应对设备内存限制并避免通信瓶颈；3）通过成本优化模型指导设备选择和训练工作负载分配，有效处理设备异构性和动态变化。

Result: 评估显示：1）Cleave匹配云端GPU训练性能，可扩展到更大模型和数千台设备；2）支持比基线边缘训练方法多8倍的设备；3）每批次训练时间比最先进的边缘训练方法快10倍；4）高效处理设备故障，恢复速度比先前方法快至少100倍。

Conclusion: Cleave通过创新的选择性混合张量并行和参数服务器架构，成功实现了在边缘设备上高效训练大型基础模型，解决了现有边缘训练方法的局限性，为去中心化模型训练提供了可行方案。

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [45] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: 提出一个端到端的LLM框架，通过构建最小可执行程序来优化GPU热点内核，无需完整应用构建，实现跨平台性能提升


<details>
  <summary>Details</summary>
Motivation: 高性能计算中GPU热点内核是主要瓶颈，专家手动调优成本高且难以移植。现有LLM方法假设内核可以廉价编译执行，但在大型应用中完整构建和运行成本过高

Method: 从独立提取的热点内核自动构建最小可执行程序，进行多轮迭代优化和评估。集成自动错误修复和性能模式继承，修复故障、保持正确性、重用有效的分块/内存/同步策略，降低搜索成本

Result: 在NVIDIA GPU和国产海光DCU平台上测试，平均加速比：PolyBench在NVIDIA上5.05x，在DCU上7.77x，AMD APP SDK上1.77x，三个热点内核上1.25x，超越直接LLM优化

Conclusion: 该方法无需完整源码依赖，提供跨平台可移植性，实现了实用、低成本的GPU内核优化，为大规模高性能计算应用提供了有效的自动化优化方案

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [46] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: 提出自适应GPU资源分配框架，用于服务器无GPU平台上部署多智能体系统，相比轮询调度减少85%延迟，同时保持与静态分配相当的吞吐量


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在服务器无GPU平台上部署面临资源分配挑战，包括异构智能体工作负载、不同计算需求以及成本效益扩展的需求

Method: 提出自适应GPU资源分配框架，基于工作负载特征、智能体优先级和最小资源需求动态分配GPU资源，使用O(N)复杂度算法实现实时适应

Result: 相比轮询调度减少85%延迟，保持与静态分配相当的吞吐量，在延迟、成本和GPU利用率指标上优于静态均衡和轮询策略

Conclusion: 该框架为在服务器无GPU基础设施上部署成本效益高的多智能体AI系统提供了实用解决方案

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [47] [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)
*Wei Li,Zhenyu Bai,Heru Wang,Pranav Dangi,Zhiqiang Zhang,Cheng Tan,Huiying Lan,Weng-Fai Wong,Tulika Mitra*

Main category: cs.DC

TL;DR: TL是一个端到端框架，用于将基于tile的程序（如Triton内核）编译到空间数据流架构上，解决tile实例在分布式核心间的映射问题，提高数据重用并减少通信。


<details>
  <summary>Details</summary>
Motivation: 空间数据流加速器通过显式的编译器管理数据移动减少内存瓶颈，但端到端性能严重依赖于工作负载到硬件的映射。现有编译器主要优化单个tile内的代码生成，而tile实例在分布式核心间的映射问题未得到充分解决，限制了空间数据流加速器的可编程性和广泛应用。

Method: TL提出一个硬件表示方法，捕获互连拓扑、内存层次和计算能力，支持特定架构优化和多样化空间数据流目标。基于MLIR生态系统构建，定义通用前端入口点和后端终点，专注于解决tile实例在空间分布式核心间的分布问题，利用片上网络和分布式内存增加数据重用并减少通信。

Result: TL框架能够将基于tile的程序有效编译到空间数据流架构上，通过优化tile实例在分布式核心间的映射，显著提高数据重用并减少通信开销，从而提升整体性能。

Conclusion: TL通过解决空间数据流加速器中tile实例在分布式核心间的映射这一核心挑战，提高了此类架构的可编程性，有助于推动空间数据流加速器的更广泛应用。

Abstract: Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.

</details>


### [48] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: AiiDAlab平台已从材料科学扩展到多学科，通过浏览器界面简化复杂计算工作流，自动追踪模拟来源确保可重复性，并集成电子实验室笔记本支持FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力增长，需要自动化研究工作流来管理大规模模拟，但执行这些工作流通常需要技术专业知识来设置输入、解释输出和处理并行代码执行。现有工具对非技术用户不够友好。

Method: 开发AiiDAlab平台，提供直观的Web浏览器用户界面，基于AiiDA引擎自动追踪完整模拟来源，简化用户入门流程，优化计算资源访问，处理大数据集，并集成电子实验室笔记本。

Result: AiiDAlab已成功扩展到量子化学、大气建模、电池研究和实验数据分析等多个学科，在教育环境中也被积极使用，帮助研究人员专注于研究而非计算细节。

Conclusion: AiiDAlab已成为加速多学科科学发现的强大平台，通过简化复杂工作流、确保可重复性和支持FAIR原则，使研究人员能够专注于科学研究本身。

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [49] [BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs](https://arxiv.org/abs/2512.22174)
*Muhammad Zeeshan Karamat,Sadman Saif,Christiana Chamon Garcia*

Main category: cs.DC

TL;DR: BitFlipScope是一个软件框架，用于在Transformer架构中定位由位翻转故障引起的参数损坏区域，支持有/无参考模型两种场景，并能实现轻量级性能恢复。


<details>
  <summary>Details</summary>
Motivation: LLMs在安全关键环境中部署时，容易受到硬件退化、宇宙辐射或Rowhammer攻击等引起的位翻转故障影响，这些故障会无声地损坏内部参数，导致不可预测或危险的行为。定位这些损坏对于诊断、修复和恢复模型功能至关重要。

Method: BitFlipScope采用两种方法：1) 当有干净参考模型时，通过输出、隐藏状态和内部激活的差异分析来检测异常行为；2) 当无参考模型时，使用残差路径扰动和损失敏感性分析直接从损坏模型中推断故障影响区域。

Result: 该框架不仅能有效进行故障诊断，还支持无需微调的轻量级性能恢复，为在硬件易出错和对抗环境中实现可信赖、容错的LLM部署提供了实用路径。

Conclusion: BitFlipScope是迈向在硬件易出错和对抗环境中实现可信赖、容错LLM部署的重要一步，提供了实用的故障诊断和恢复能力。

Abstract: Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.

</details>


### [50] [iOS as Acceleration](https://arxiv.org/abs/2512.22180)
*Alexander K. Chen*

Main category: cs.DC

TL;DR: 利用iOS手机作为分布式并行计算资源，提升本地机器学习计算能力，无需额外成本


<details>
  <summary>Details</summary>
Motivation: 大规模机器学习需要强大计算资源，但本地系统环境受限。虽然云计算可解决本地计算能力不足问题，但在涉及隐私数据、物理环境不可用或成本考虑时，仍需本地计算。移动手机作为普遍但未充分利用的资源，具有改善本地计算系统的潜力。

Method: 提出概念验证系统，采用分布式管道并行方法，利用iOS设备（特别是近年配备强大处理器的iPhone）来加速机器学习任务。系统克服了iOS设备的内存限制、热节流和操作系统沙盒等限制。

Result: 实现了在较弱计算环境中显著提升性能，加速了适度规模的模型训练、批量推理和代理LRM工具使用。展示了iOS设备作为分布式计算资源的实际可行性。

Conclusion: 研究表明普通移动设备有潜力为机器学习做出更大贡献。讨论了实际用例、限制和未来研究方向，强调了利用现有移动设备资源改善本地机器学习计算环境的可能性。

Abstract: Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.

</details>


### [51] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: MatKV通过预计算RAG文档的KV向量并存储在闪存中，在推理时直接复用，将推理时间和能耗减半，同时支持GPU并行解码和低端GPU使用。


<details>
  <summary>Details</summary>
Motivation: LLM推理成本已超过训练成本，RAG在处理长输入时prefill阶段计算KV向量能耗高、耗时长，需要提高RAG推理效率。

Method: 提出MatKV方案：预计算RAG文档的KV向量，将结果物化存储在廉价但快速、能效高的闪存中，推理时直接加载复用而非重新计算。

Result: 实验表明，相比GPU完全计算KV，MatKV将RAG工作负载的推理时间和能耗减半，且问答任务准确率影响不大。支持GPU并行解码和低端GPU使用。

Conclusion: MatKV能显著降低大规模生成式AI应用的成本和能耗，提高硬件兼容性，使AI应用更经济、高效、可访问。

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [52] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: SPUMA实现了OPENFOAM在NVIDIA和AMD GPU上的完整移植，采用便携编程模型和统一内存管理，在LUMI和Leonardo集群上展示了良好的强/弱扩展性和高达82%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU在HPC中广泛应用，但在开源CFD领域（如OpenFOAM）的程序可移植性仍面临挑战，需要解决跨平台GPU编程和内存管理问题。

Method: 开发SPUMA框架，基于便携编程模型实现OpenFOAM的完整GPU移植，采用内存池管理器利用现代GPU的统一内存特性，支持NVIDIA和AMD GPU。

Result: 在LUMI（AMD MI250X）和Leonardo（NVIDIA A100）集群上测试：强扩展效率达65%（每GPU 800万网格），弱扩展效率75-85%，使用AmgX时效率不低于90%；单A100 GPU性能相当于200-300个Intel Sapphire Rapids核心；能耗降低达82%。

Conclusion: SPUMA成功实现了OpenFOAM在异构GPU集群上的高效移植，显著提升了计算性能和能效，为开源CFD在预百亿亿次计算系统上的应用提供了可行方案。

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [53] [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219)
*Xinhao Cheng,Zhihao Zhang,Yu Zhou,Jianan Ji,Jinchen Jiang,Zepeng Zhao,Ziruo Xiao,Zihao Ye,Yingyi Huang,Ruihang Lai,Hongyi Jin,Bohan Hou,Mengdi Wu,Yixin Dong,Anthony Yip,Zihao Ye,Songting Wang,Wenqin Yang,Xupeng Miao,Tianqi Chen,Zhihao Jia*

Main category: cs.DC

TL;DR: MPK是首个将多GPU模型推理自动转换为单个高性能巨型内核的编译器和运行时系统，通过SM级图表示实现跨算子软件流水线等优化，显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统通常采用算子级内核分离的方式，导致GPU利用率不足和调度开销大，无法充分利用硬件潜力，需要一种能自动融合多GPU模型推理为单个高性能内核的解决方案。

Method: MPK引入SM级图表示捕获单个流多处理器级别的数据依赖，支持跨算子软件流水线和细粒度内核重叠。编译器将张量程序转换为优化的SM级任务图并生成CUDA实现，运行时在单个巨型内核内通过去中心化调度执行任务。

Result: MPK显著优于现有算子级LLM服务系统，端到端推理延迟降低高达1.7倍，将LLM推理性能推近硬件极限。

Conclusion: MPK通过自动化的端到端内核融合，以最小开发工作量实现了接近硬件极限的LLM推理性能，同时保持了现有编程模型的灵活性。

Abstract: We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage.

</details>


### [54] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: 提出一个结合AI、边缘计算与云计算的云原生架构，用于处理大规模PMU数据，实现低延迟、可扩展的实时电网监控。


<details>
  <summary>Details</summary>
Motivation: 传统集中式PMU数据处理架构难以应对现代电网中PMU部署规模扩大带来的延迟、可扩展性和可靠性挑战，特别是在动态运行条件下。

Method: 采用分布式流处理、容器化微服务和弹性资源编排的云原生架构，集成机器学习模型进行时间序列分析，并嵌入安全隐私机制。

Result: 分析模型显示该架构可实现亚秒级响应时间，并能扩展到大规模PMU部署，同时保持高吞吐量和可靠性。

Conclusion: 该云原生架构为下一代智能电网分析提供了强大灵活的基础，能够满足大规模PMU数据处理的需求。

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [55] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: Pick and Spin是一个基于Kubernetes的LLM编排框架，通过统一部署、自适应扩缩容和混合路由策略，实现自托管大语言模型的经济高效部署，相比静态部署可提升21.6%成功率、降低30%延迟和33%GPU成本。


<details>
  <summary>Details</summary>
Motivation: 组织自托管大语言模型面临GPU利用率低、工作负载路由困难和可靠性挑战，需要一种经济高效的解决方案来平衡隐私、成本控制和定制化需求。

Method: 基于Kubernetes构建，包含统一Helm部署系统、自适应scale-to-zero自动化、混合路由模块（结合关键词启发式和轻量级DistilBERT分类器），在四个不同规模模型上评估了五种推理策略和两种路由变体。

Result: 在31,019个提示和163,720次推理运行的评估中，相比相同模型的静态部署，Pick and Spin实现了最高21.6%的成功率提升、30%的延迟降低和33%的GPU成本降低。

Conclusion: Pick and Spin为自托管LLM提供了一个可扩展且经济的编排框架，通过智能路由和资源优化显著提升了部署效率和成本效益。

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [56] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: Nightjar提出了一种基于学习的自适应推测解码算法，能够根据请求负载动态调整推测长度，在实时服务中显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码使用固定的推测长度，无法适应动态请求负载，在高负载计算受限环境中反而会因验证开销导致性能下降，这在真实服务场景中形成了性能瓶颈。

Method: 提出Nightjar算法，这是一种基于学习的自适应推测推理方法，能够根据批处理大小动态选择最优推测长度，甚至在推测解码无益时完全禁用该功能。

Result: 实验显示，Nightjar相比标准推测解码实现了高达14.8%的吞吐量提升和20.2%的延迟降低，在实时服务中展现出强大的效率优势。

Conclusion: Nightjar通过自适应调整推测长度解决了传统推测解码在动态负载环境中的性能瓶颈问题，为LLM推理服务提供了更高效、更灵活的解决方案。

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [57] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: RobustRL是一个针对LLM RL后训练的系统，通过角色隔离、快速恢复和动态重连机制，在GPU故障时显著提升有效训练时间比例。


<details>
  <summary>Details</summary>
Motivation: RL后训练结合了训练和推理工作负载，现有容错框架只针对训练或推理单独优化，无法充分利用RL异步执行的潜力。GPU故障会导致整个RL任务重启，带来巨大的回放和初始化开销。

Method: 1. 角色隔离：将训练器、rollout和管理角色视为独立分布式子任务；2. 检测：角色感知监控，区分真实故障和角色特定行为；3. 重启：训练器非中断恢复（rollout保持状态），rollout机器隔离替换；4. 重连：用UCX点对点动态通信替换静态集合通信，实现快速权重同步。

Result: 在256-GPU集群上，使用Qwen3-8B-Math工作负载，10%故障注入频率下，RobustRL的有效训练时间比例（ETTR）超过80%，相比ByteRobust的60%显著提升，端到端训练时间快8.4%-17.4%。

Conclusion: RobustRL通过角色隔离和快速恢复机制，为RL后训练提供了首个全面的GPU容错系统，显著提升了训练效率和可靠性。

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [58] [Object Abstraction To Streamline Edge-Cloud-Native Application Development](https://arxiv.org/abs/2512.22534)
*Pawissanutt Lertpongrujikorn*

Main category: cs.DC

TL;DR: 该论文提出Object-as-a-Service (OaaS)范式，通过统一资源、状态和工作流管理来解决云原生开发中的碎片化问题，在性能开销可忽略的情况下实现最先进的扩展性，并扩展到边缘计算领域。


<details>
  <summary>Details</summary>
Motivation: 虽然云计算已经改变了应用开发，但无服务器计算在函数运行时、状态管理和编排方面的碎片化阻碍了其简化部署的承诺。基础设施复杂性降低了开发者的生产力，实践者更关注自动化和可维护性而非成本优化。

Method: 基于三项实证研究：21名从业者访谈、39名参与者的开发者体验研究、以及86个组织的101次客户访谈。技术上提出OaaS范式，开发Oparaca原型，支持SLA驱动的声明式管理，并扩展到边缘计算（OaaS-IoT with EdgeWeaver）。

Result: OaaS范式展示了可忽略的开销和最先进的扩展性；边缘计算版本相比传统FaaS实现31%更快的任务完成和44.5%的代码行数减少；建立了面向技术中小企业和初创企业的商业化路径。

Conclusion: OaaS通过整合碎片化抽象和自动化性能优化，为云原生平台奠定了基础，隐藏基础设施复杂性，使开发者能够专注于创新。该研究还建立了将技术研究基于验证的实践者需求的实证方法。

Abstract: Cloud computing has fundamentally transformed application development, yet a gap remains between the serverless promise of simplified deployment and its practical realization due to fragmentation across function runtimes, state management, and orchestration. This dissertation addresses this gap through empirical validation and technical innovation, establishing the Object-as-a-Service (OaaS) paradigm as a unified approach to cloud-native development. Grounded in evidence from three studies - practitioner interviews (21 participants), a human study on developer experience (39 participants), and NSF I-Corps customer discovery (101 interviews across 86 organizations) - this work demonstrates that infrastructure complexity taxes productivity, with practitioners prioritizing automation and maintainability over cost optimization. The dissertation makes five major contributions: (1) the OaaS paradigm unifies resource, state, and workflow management via the Oparaca prototype, demonstrating negligible overhead and state-of-the-art scalability; (2) SLA-driven OaaS enables declarative management of non-functional requirements like availability, consistency, and latency; (3) OaaS-IoT with EdgeWeaver extends the paradigm to the edge-cloud continuum, achieving 31% faster task completion and a 44.5% reduction in lines of code compared to traditional FaaS; (4) commercialization validation establishes a pathway targeting technology SMEs and startups; and (5) an empirical methodology for grounding technical research in validated practitioner needs. By consolidating fragmented abstractions and automating performance optimization, OaaS establishes a foundation for cloud-native platforms that hide infrastructure complexity and empower developers to focus on innovation.

</details>


### [59] [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)
*Wei Gao,Yuheng Zhao,Tianyuan Wu,Shaopan Xiong,Weixun Wang,Dakai An,Lunxi Cao,Dilxat Muhtar,Zichen Liu,Haizhou Zhao,Ju Huang,Siran Yang,Yongbin Li,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: RollArc是一个分布式系统，用于在解耦基础设施上最大化多任务代理强化学习的训练吞吐量，通过硬件亲和性工作负载映射、细粒度异步和状态感知计算实现1.35-2.05倍的训练时间减少。


<details>
  <summary>Details</summary>
Motivation: 代理强化学习工作负载具有高度异构性，结合了计算密集型预填充阶段、带宽受限的解码和状态化的CPU密集型环境模拟。传统解耦方法会引入大量同步开销和资源利用率不足问题。

Method: 1) 硬件亲和性工作负载映射：将计算密集型和带宽密集型任务路由到最适合的GPU设备；2) 细粒度异步：在轨迹级别管理执行以减少资源气泡；3) 状态感知计算：将无状态组件（如奖励模型）卸载到无服务器基础设施实现弹性扩展。

Result: RollArc有效提高了训练吞吐量，相比单体和同步基线实现了1.35-2.05倍的端到端训练时间减少。在阿里巴巴集群上使用3000多块GPU训练了数百亿参数的MoE模型，证明了系统的可扩展性和鲁棒性。

Conclusion: RollArc通过解耦基础设施和智能调度策略，成功解决了代理强化学习训练中的异构工作负载挑战，显著提升了训练效率和系统可扩展性。

Abstract: Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.
  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\(\times\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.

</details>


### [60] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: 论文探讨了如何通过不同技术方法（软件架构、云原生Operator模式、传统编程技术）实现TeaStore微服务的自适应性控制，分析各种方法在细粒度表达与系统级控制之间的权衡，并提出了多层架构的解决方案。


<details>
  <summary>Details</summary>
Motivation: 实现微服务自适应性控制时需要考虑系统一致性、规划性和模块性等关键属性，需要探索如何将自适应控制逻辑与业务应用解耦，并分析不同实现方法的权衡。

Method: 通过分析软件架构方法、云原生Operator模式和传统编程技术三种方法，研究它们如何实现自适应控制逻辑与TeaStore应用的解耦，并评估各种方法在表达细粒度自适应与系统级控制之间的权衡。

Result: 分析表明这些方法并非互斥，可以结合形成多层架构的自适应微服务系统，其中不同方法在不同层面发挥作用，实现自适应策略的有效复用。

Conclusion: 通过结合软件架构方法、云原生Operator模式和传统编程技术，可以构建多层架构的自适应微服务系统，平衡细粒度表达与系统级控制的需求，实现自适应策略的有效复用。

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>


### [61] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: 本文首次详细分析了多模态大语言模型推理的能耗，发现多模态输入导致17%-94%的额外能耗，并提出阶段级DVFS优化方案


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文本模型基础上增加了视觉等模态，但现有研究主要关注文本模型，对多模态带来的能耗权衡缺乏理解，特别是模态膨胀导致的效率问题

Method: 将MLLM推理流程分解为视觉编码、预填充和解码三个阶段，在NVIDIA A100 GPU上评估四种代表性MLLM，量化多模态推理相比文本基线的额外能耗，分析GPU功率轨迹和利用率

Result: 多模态推理能耗比文本基线高17%-94%，能耗瓶颈因架构而异（计算密集的视觉编码器或大型视觉标记序列），发现GPU在推理中存在显著利用不足，输入复杂度导致不同模型的能耗扩展行为差异

Conclusion: 阶段级动态电压频率缩放是有效的优化方法，能在性能影响较小的情况下实现节能，为设计更节能的多模态LLM服务系统提供了实用见解和具体指导

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [62] [OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads](https://arxiv.org/abs/2512.22743)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: OptiNIC是一个专为分布式机器学习设计的RDMA传输协议，通过消除重传和顺序交付要求，利用ML对数据丢失的容忍性来降低尾延迟，提升训练和推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着分布式ML扩展到数千个GPU，集体通信中的尾延迟成为主要瓶颈。传统RDMA传输（如RoCE、IRN等）强制严格的可靠性和顺序交付，依赖重传和包排序，这些机制在ML场景中引入了不必要的复杂性和延迟，即使罕见的包延迟也会阻塞整个模型流水线。

Method: OptiNIC重新审视传统可靠性保证，基于ML对部分或缺失数据的容忍性，从NIC中消除重传和顺序交付，实现尽力而为、乱序的RDMA传输模型。它引入自适应超时机制来触发进度推进，同时保留标准拥塞控制机制（如DCQCN、EQDS、Swift），将丢失恢复转移到ML流水线本身（通过Hadamard变换和擦除编码）。

Result: 评估显示，OptiNIC在两个公共云（Hyperstack和CloudLab）上，将训练和推理的时间到准确率（TTA）分别提升2倍，吞吐量分别提升1.6倍。同时将第99百分位延迟降低3.5倍，BRAM使用减少2.7倍，NIC容错能力几乎翻倍。

Conclusion: OptiNIC为分布式ML工作负载提供了一个具有弹性、尾部优化的RDMA传输协议，通过利用ML应用对数据丢失的容忍特性，显著提升了大规模分布式ML系统的性能和效率。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.
  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).
  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.

</details>


### [63] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: Argus：首个面向异构边缘-云系统的token感知分布式LLM推理框架，通过长度预测和Lyapunov优化实现高效任务卸载


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理解决方案忽视了边缘-云异构环境的动态性、随机性和异构性，忽略了可变输出token长度和设备多样性的影响，导致推理时间变异性大

Method: 1. 长度感知语义(LAS)模块：使用微调的语言模型预测输出token长度；2. Lyapunov引导的卸载优化(LOO)模块：考虑LLM预填充和解码成本；3. 带阻尼和拥塞控制的迭代卸载算法(IODCC)解决整数非线性规划问题

Result: 理论和实证评估表明，Argus在高度动态、异构的环境中实现了稳健的性能和卓越的效率

Conclusion: Argus是首个token感知的分布式边缘-云LLM推理框架，通过精确的长度预测和优化的任务卸载，有效解决了异构环境中的推理变异性问题

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


### [64] [Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware](https://arxiv.org/abs/2512.23029)
*Alex Khalil,Guillaume Heilles,Maria Parraga,Simon Heilles*

Main category: cs.DC

TL;DR: 本地部署量化30B参数MoE模型在消费级硬件上，为中小企业提供低成本、高隐私的LLM推理方案，性能接近云服务。


<details>
  <summary>Details</summary>
Motivation: 当前LLM主要依赖云端专有系统，存在数据隐私、运营主权和成本高昂等问题，中小企业难以承受。需要探索低成本、高隐私的本地部署方案。

Method: 使用消费级服务器搭载新一代NVIDIA GPU，部署量化30B参数Mixture-of-Experts模型（基于Qwen3）。从两个维度评估：模型内在能力（推理和知识）和服务器性能（延迟、吞吐量、首次令牌时间、并发用户扩展性）。

Result: 精心配置的本地部署方案在消费级硬件上运行量化开源模型，能够达到与云服务相当的性能水平，为中小企业提供了可行的LLM部署路径。

Conclusion: 本地部署的量化MoE模型在消费级硬件上能够为中小企业提供低成本、高隐私的LLM推理服务，性能可媲美云服务，解决了成本、隐私和运营主权等问题。

Abstract: The proliferation of Large Language Models (LLMs) has been accompanied by a reliance on cloud-based, proprietary systems, raising significant concerns regarding data privacy, operational sovereignty, and escalating costs. This paper investigates the feasibility of deploying a high-performance, private LLM inference server at a cost accessible to Small and Medium Businesses (SMBs). We present a comprehensive benchmarking analysis of a locally hosted, quantized 30-billion parameter Mixture-of-Experts (MoE) model based on Qwen3, running on a consumer-grade server equipped with a next-generation NVIDIA GPU. Unlike cloud-based offerings, which are expensive and complex to integrate, our approach provides an affordable and private solution for SMBs. We evaluate two dimensions: the model's intrinsic capabilities and the server's performance under load. Model performance is benchmarked against academic and industry standards to quantify reasoning and knowledge relative to cloud services. Concurrently, we measure server efficiency through latency, tokens per second, and time to first token, analyzing scalability under increasing concurrent users. Our findings demonstrate that a carefully configured on-premises setup with emerging consumer hardware and a quantized open-source model can achieve performance comparable to cloud-based services, offering SMBs a viable pathway to deploy powerful LLMs without prohibitive costs or privacy compromises.

</details>


### [65] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: 提出Local Rendezvous Hashing (LRH)，通过限制HRW选择到缓存局部窗口的C个相邻物理节点，在保持令牌环结构的同时实现更好的负载均衡和性能。


<details>
  <summary>Details</summary>
Motivation: 传统一致性哈希方案存在负载不均衡问题：基于环的方案需要大量虚拟节点才能降低峰值负载比，而多探针方法虽然改善了均衡性但导致内存访问分散、性能下降。

Method: LRH保持令牌环结构，但将Highest Random Weight (HRW)选择限制在缓存局部窗口的C个相邻物理节点。通过一次二分查找定位key，使用预计算的下一个不同偏移量枚举恰好C个不同的候选节点，然后选择HRW胜出者（可选加权）。查找成本为O(log|R| + C)。

Result: 在N=5000节点、V=256虚拟节点、K=5000万键、C=8的基准测试中，LRH将最大/平均负载比从1.2785降至1.0947，达到60.05 Mkeys/s，比8探针多探针一致性哈希快约6.8倍（8.80 Mkeys/s），同时接近其均衡性（最大/平均负载比1.0697）。

Conclusion: LRH在保持令牌环结构优势的同时，通过缓存局部化设计显著提升了负载均衡性和性能，解决了传统一致性哈希方案在负载均衡和性能之间的权衡问题。

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [66] [Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets](https://arxiv.org/abs/2512.23439)
*Marko Vukolić,Orestis Alpos,Jakov Mitrovski,Themis Papameletiou,Nikola Ristić,Dionysis Zindros*

Main category: cs.DC

TL;DR: Bitcoin-IPC是一个软件栈和协议，通过创建以L1 BTC为质押的PoS Layer-2子网，扩展比特币作为通用交换媒介的能力，无需修改比特币L1即可实现23倍交易成本降低和160+ tps吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决比特币作为通用交换媒介（MoE）的扩展性问题，通过Layer-2方案提高交易吞吐量、降低交易成本，同时保持与比特币L1的安全性和兼容性。

Method: 设计基于SWIFT消息传递机制的软件栈，利用比特币的SegWit机制嵌入通信协议，创建无需许可的PoS Layer-2子网，这些子网以L1 BTC为质押，通过比特币L1进行关键信息通信、结算和安全保障。

Result: 相比原生比特币L1交易，虚拟字节成本降低23倍，交易吞吐量从7 tps提升到超过160 tps，实现了跨子网的无缝价值转移，无需修改比特币L1协议。

Conclusion: Bitcoin-IPC通过创新的Layer-2架构成功扩展了比特币的交易能力，使其更接近成为通用交换媒介的目标，同时保持了比特币L1的安全性和去中心化特性。

Abstract: We introduce Bitcoin-IPC, a software stack and protocol that scales Bitcoin towards helping it become the universal Medium of Exchange (MoE) by enabling the permissionless creation of fully programmable Proof-of-Stake (PoS) Layer-2 chains, called subnets, whose stake is denominated in L1 BTC. Bitcoin-IPC subnets rely on Bitcoin L1 for the communication of critical information, settlement, and security.
  Our design, inspired by SWIFT messaging and embedded within Bitcoin's SegWit mechanism, enables seamless value transfer across L2 subnets, routed through Bitcoin L1. Uniquely, this mechanism reduces the virtual-byte cost per transaction (vB per tx) by up to 23x, compared to transacting natively on Bitcoin L1, effectively increasing monetary transaction throughput from 7 tps to over 160 tps, without requiring any modifications to Bitcoin L1.

</details>


### [67] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: 将现有离线性能优化框架应用于微服务应用发布阶段，优化CPU和内存资源配置，通过实验比较不同优化算法在采样成本和最优配置距离之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前微服务性能优化研究主要集中在运维阶段的智能调度和自动扩缩容，而发布阶段的资源配置优化研究较少。容器水平自动扩缩容（如基于CPU使用率）可能导致内存分配不当，需要在部署前对两种资源进行精细调优。

Method: 将现有离线性能优化框架应用于微服务应用的发布阶段，使用TeaStore微服务应用进行评估，统计比较不同优化算法，分析因子筛选对搜索空间缩减的影响。

Result: 实验表明：1）当目标是找到最优资源配置且采样预算有限时，前置因子筛选有助于缩减搜索空间；2）当需要统计比较不同算法时，也需要应用筛选以使所有数据点收集可行；3）当目标是找到近似最优配置时，不进行筛选的贝叶斯优化效果更好。

Conclusion: 发布阶段的微服务资源配置优化是一个重要但研究不足的领域，不同优化目标需要采用不同的策略（是否使用因子筛选），为DevOps生命周期中的资源配置决策提供了实用指导。

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>
