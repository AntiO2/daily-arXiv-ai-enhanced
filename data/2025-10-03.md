<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration](https://arxiv.org/abs/2510.01379)
*Huashan Chen,Zhenyu Qi,Haotang Li,Hong Chen,Jinfu Chen,Kebin Peng,In Kee Kim,Kyu Hyung Lee,Sen He*

Main category: cs.SE

TL;DR: 提出PerfOrch框架，通过多阶段性能引导的编排机制动态选择最适合的LLM来处理编程任务，相比单一模型方法在代码正确性和运行时性能上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于单一LLM的代码生成方法忽视了不同模型在不同编程语言、算法领域和开发阶段表现出的异构计算优势，需要更智能的模型选择策略。

Method: 基于对17个先进LLM在5种编程语言上的实证研究，开发了PerfOrch框架，采用生成-修复-优化的多阶段工作流程，通过阶段验证和回滚机制动态路由任务到最适合的LLM。

Result: 在HumanEval-X和EffiBench-X基准测试中分别达到96.22%和91.37%的平均正确率，显著超越GPT-4o的78.66%和49.11%；58.76%的问题执行时间得到优化，跨语言中位数加速达17.67%-27.66%。

Conclusion: PerfOrch提供了一种可扩展的生产级自动化软件工程范式，能够适应快速发展的生成式AI环境，无需模型微调即可实现显著性能提升。

Abstract: While Large Language Models (LLMs) have become the predominant paradigm for
automated code generation, current single-model approaches fundamentally ignore
the heterogeneous computational strengths that different models exhibit across
programming languages, algorithmic domains, and development stages. This paper
challenges the single-model convention by introducing a multi-stage,
performance-guided orchestration framework that dynamically routes coding tasks
to the most suitable LLMs within a structured generate-fix-refine workflow. Our
approach is grounded in a comprehensive empirical study of 17 state-of-the-art
LLMs across five programming languages (Python, Java, C++, Go, and Rust) using
HumanEval-X benchmark. The study, which evaluates both functional correctness
and runtime performance metrics (execution time, mean/max memory utilization,
and CPU efficiency), reveals pronounced performance heterogeneity by language,
development stage, and problem category. Guided by these empirical insights, we
present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each
task context through stage-wise validation and rollback mechanisms. Without
requiring model fine-tuning, PerfOrch achieves substantial improvements over
strong single-model baselines: average correctness rates of 96.22% and 91.37%
on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and
49.11%. Beyond correctness gains, the framework delivers consistent performance
optimizations, improving execution time for 58.76% of problems with median
speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The
framework's plug-and-play architecture ensures practical scalability, allowing
new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm
for production-grade automated software engineering that adapts to the rapidly
evolving generative AI landscape.

</details>


### [2] [Deciphering WONTFIX: A Mixed-Method Study on Why GitHub Issues Get Rejected](https://arxiv.org/abs/2510.01514)
*J. Alexander Curtis,Sharadha Kasiviswanathan,Nasir Eisty*

Main category: cs.SE

TL;DR: 该研究分析了GitHub上wontfix标签的普遍性和使用原因，发现约30%的项目使用该标签，主要出现在用户提交的bug报告和功能请求中，并识别出8个常见原因主题。


<details>
  <summary>Details</summary>
Motivation: wontfix标签在GitHub仓库中被广泛使用但理解有限，其对开源软件开发中项目管理和社区动态的影响尚未明确定义。

Method: 采用混合方法，分析定量数据评估wontfix标签的普遍性，分析定性数据探索使用原因。从3,132个最受欢迎的GitHub仓库收集数据，使用开放编码和主题分析对原因进行分类。

Result: 约30%的GitHub项目对某些问题应用wontfix标签，这些问题最常见于用户提交的bug报告和功能请求。识别出8个常见主题，从用户特定控制因素到维护者特定决策。

Conclusion: wontfix标签是GitHub项目中管理资源和指导贡献者工作的关键工具，但也可能阻碍社区参与并掩盖项目管理的透明度。理解这些原因有助于项目经理做出明智决策并促进开源社区内的高效协作。

Abstract: Context: The ``wontfix'' label is a widely used yet narrowly understood tool
in GitHub repositories, indicating that an issue will not be pursued further.
Despite its prevalence, the impact of this label on project management and
community dynamics within open-source software development is not clearly
defined. Objective: This study examines the prevalence and reasons behind
issues being labeled as wontfix across various open-source repositories on
GitHub. Method: Employing a mixed-method approach, we analyze both quantitative
data to assess the prevalence of the wontfix label and qualitative data to
explore the reasoning that it was used. Data were collected from 3,132 of
GitHub's most-popular repositories. Later, we employ open coding and thematic
analysis to categorize the reasons behind wontfix labels, providing a
structured understanding of the issue management landscape. Results: Our
findings show that about 30% of projects on GitHub apply the wontfix label to
some issues. These issues most often occur on user-submitted issues for bug
reports and feature requests. The study identified eight common themes behind
labeling issues as wontfix, ranging from user-specific control factors to
maintainer-specific decisions. Conclusions: The wontfix label is a critical
tool for managing resources and guiding contributor efforts in GitHub projects.
However, it can also discourage community involvement and obscure the
transparency of project management. Understanding these reasons aids project
managers in making informed decisions and fostering efficient collaboration
within open-source communities.

</details>


### [3] [MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model](https://arxiv.org/abs/2510.01635)
*Yifei Chen,Sarra Habchi,Lili Wei*

Main category: cs.SE

TL;DR: MIMIC是一个将多样化人格特征整合到游戏代理中的框架，使代理能够在相似情境下采用不同的游戏策略，从而提高测试覆盖率和游戏交互多样性。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试算法难以应对现代视频游戏的挑战，而现有的强化学习、模仿学习或大语言模型代理往往忽视人类玩家的多样化策略，导致在相似情境下产生重复解决方案，无法触发多样化的游戏交互或发现边缘情况。

Method: MIMIC框架通过将多样化人格特征整合到游戏代理中，使代理能够模仿不同的游戏风格，在相似情境下采用不同的策略。

Result: MIMIC在不同游戏中实现了更高的测试覆盖率和更丰富的游戏交互，在Minecraft中超越了最先进的代理，获得了更高的任务完成率并提供了更多样化的解决方案。

Conclusion: MIMIC在游戏测试方面具有显著潜力，能够通过模仿多样化游戏策略来提升测试效果。

Abstract: Modern video games pose significant challenges for traditional automated
testing algorithms, yet intensive testing is crucial to ensure game quality. To
address these challenges, researchers designed gaming agents using
Reinforcement Learning, Imitation Learning, or Large Language Models. However,
these agents often neglect the diverse strategies employed by human players due
to their different personalities, resulting in repetitive solutions in similar
situations. Without mimicking varied gaming strategies, these agents struggle
to trigger diverse in-game interactions or uncover edge cases.
  In this paper, we present MIMIC, a novel framework that integrates diverse
personality traits into gaming agents, enabling them to adopt different gaming
strategies for similar situations. By mimicking different playstyles, MIMIC can
achieve higher test coverage and richer in-game interactions across different
games. It also outperforms state-of-the-art agents in Minecraft by achieving a
higher task completion rate and providing more diverse solutions. These results
highlight MIMIC's significant potential for effective game testing.

</details>


### [4] [FOSS-chain: using blockchain for Open Source Software license compliance](https://arxiv.org/abs/2510.01740)
*Kypros Iacovou,Georgia M. Kapitsaki,Evangelia Vanezi*

Main category: cs.SE

TL;DR: FOSS-chain是一个基于区块链的开源软件许可证管理平台，旨在解决衍生作品创建时的许可证兼容性问题，自动化许可证合规流程。


<details>
  <summary>Details</summary>
Motivation: 开源软件许可证管理复杂，许可证不兼容可能导致法律纠纷，而区块链技术可以提供透明度和不可篡改的记录机制。

Method: 设计并实现了FOSS-chain网络平台，整合区块链技术，自动化许可证合规流程，覆盖14种开源许可证。

Result: 通过小规模用户研究对初始原型进行评估，初步结果显示出该平台在现实软件系统中应用的潜力。

Conclusion: FOSS-chain展示了区块链技术在开源软件许可证管理中的潜力，能够有效解决许可证兼容性问题。

Abstract: Open Source Software (OSS) is widely used and carries licenses that indicate
the terms under which the software is provided for use, also specifying
modification and distribution rules. Ensuring that users are respecting OSS
license terms when creating derivative works is a complex process. Compliance
issues arising from incompatibilities among licenses may lead to legal
disputes. At the same time, the blockchain technology with immutable entries
offers a mechanism to provide transparency when it comes to licensing and
ensure software changes are recorded. In this work, we are introducing an
integration of blockchain and license management when creating derivative
works, in order to tackle the issue of OSS license compatibility. We have
designed, implemented and performed a preliminary evaluation of FOSS-chain, a
web platform that uses blockchain and automates the license compliance process,
covering 14 OSS licenses. We have evaluated the initial prototype version of
the FOSS-chain platform via a small scale user study. Our preliminary results
are promising, demonstrating the potential of the platform for adaptation on
realistic software systems.

</details>


### [5] [ARENA: A tool for measuring and analysing the energy efficiency of Android apps](https://arxiv.org/abs/2510.01754)
*Hina Anwar*

Main category: cs.SE

TL;DR: ARENA是一个Android开发工具，通过硬件方式测量应用能耗，集成在IDE中简化测量流程。


<details>
  <summary>Details</summary>
Motivation: 硬件能耗测量方法准确但过程复杂耗时，缺乏开源工具支持开发者和研究者进行可靠的硬件能耗测量。

Method: 开发ARENA作为IntelliJ和Android Studio插件，连接物理测量设备，执行测试场景并计算能耗，提供数据聚合、统计分析和可视化功能。

Result: ARENA实现了在IDE环境中直接进行硬件能耗测量，简化了测量流程，支持不同应用或版本间的能耗比较。

Conclusion: ARENA解决了硬件能耗测量复杂性问题，为开发者和研究者提供了便捷可靠的能耗分析工具。

Abstract: To build energy-efficient apps, there is a need to estimate and analyze their
energy consumption in typical usage scenarios. The energy consumption of
Android apps could be estimated via software-based and hardware-based
approaches. Software-based approaches, while easier to implement, are not as
accurate as hardware-based approaches. The process of measuring the energy
consumption of an Android app via a hardware-based approach typically involves
1) setting up a measurement environment, 2) executing the app under test on a
mobile device, 3) recording current/voltage data via a hardware device to
measure energy consumption, and 4) cleaning and aggregating data for analyses,
reports, and visualizations. Specialized scripts are written for selected
hardware and software components to ensure reliable energy measurements. The
energy measurement process is repeated many times and aggregated to remove
noise. These steps make the hardware-based energy measurement process
time-consuming and not easy to adapt or reproduce. There is a lack of
open-source tools available for developers and researchers to take reliable
energy measurements via hardware devices. In this paper, we present and
demonstrate ARENA, a support tool that enables developers and researchers to
connect to a physical measurement device without leaving the comfort of their
IDE. Developers could use ARENA during development to compare energy
consumption between different apps or versions of the same app. ARENA
calculates energy consumption on an Android smartphone by executing a test
scenario on the app under development. Further, ARENA helps aggregate,
statistically analyze, report, and visualize the data, allowing developers and
researchers to dig into the data directly or visually. We implemented ARENA as
an IntelliJ and Android Studio plugin.

</details>


### [6] [Towards Speeding up Program Repair with Non-Autoregressive Model](https://arxiv.org/abs/2510.01825)
*Zhenyu Yang,Yue Pan,Zhen Yang,Zhongxing Yu*

Main category: cs.SE

TL;DR: NARRepair是首个为自动程序修复任务定制的非自回归代码生成模型，通过修复动作预测器、令牌间依赖提取器和两阶段解码器解决NAR方法在APR任务中的质量问题，在修复速度和准确性方面达到最先进的综合性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于自回归的APR技术存在巨大的时间延迟问题，特别是参数较多的模型延迟更严重。非自回归方法可以并行输出目标代码以避免修复延迟，但直接应用于APR任务会导致补丁质量下降。

Method: 提出NARRepair模型，包含三个主要创新：1）修复动作预测器缓解过度修正问题；2）令牌间依赖提取器缓解缺乏令牌间依赖信息问题；3）两阶段解码器缓解缺乏上下文信息问题。

Result: 在三个广泛使用的APR数据集上评估，结果显示：1）在有限修复时间内，NARRepair性能最佳；2）与AR-based APR技术相比，在GPU环境中修复速度提高了1.4-6.4倍。

Conclusion: NARRepair在修复速度和准确性方面实现了最先进的综合性能，成功解决了NAR方法在APR任务中的应用挑战。

Abstract: Enlightened by the success of machine learning techniques in various
application areas, recent years have witnessed a surge of research efforts on
automatic program repair (APR) using machine learning techniques. Previous
machine learning-based APR techniques essentially modified bugs in the
autoregressive (AR) manner, which predicts future values based on past values.
Due to the manner of token-by-token generation, the AR-based APR technique has
a huge time delay. In particular, the delay of the APR model with a large
number of parameters is more serious. To address the issue, we aim to apply the
non-autoregressive (NAR) method to the APR task, which can output target code
in a parallel manner to avoid huge repair delays. However, the naive use of the
NAR manner for the APR task suffers from the issue of compromised patch
quality. To effectively adapt the NAR manner for the APR task, we in this paper
propose NARRepair, the first customized NAR code generation model for the APR
task. The NARRepair model features three major novelties, including 1) the
repair action predictor for alleviating the over-correction issue, 2) the
inter-token dependency extractor for alleviating the issue of lacking
inter-token dependency information, and 3) the two-stage decoder for
alleviating the issue of lacking contextual information. We evaluated NARRepair
on three widely used datasets in the APR community, and the results show that
1) compared to other APR techniques, the NARRepair model has the best
performance within the limited repair time, and 2) compared to AR-based APR
techniques, the repair speed of NARRepair has been increased by 1.4-6.4 times
in the GPU environment. Overall, the results show that NARRepair has achieved
state-of-the-art comprehensive performance in terms of repair speed and
accuracy.

</details>


### [7] [RefFilter: Improving Semantic Conflict Detection via Refactoring-Aware Static Analysis](https://arxiv.org/abs/2510.01960)
*Victor Lira,Paulo Borba,Rodrigo Bonifácio,Galileu Santos e Matheus barbosa*

Main category: cs.SE

TL;DR: RefFilter是一个重构感知的语义干扰检测工具，通过自动检测重构来减少误报，在标记数据集上减少近32%的误报


<details>
  <summary>Details</summary>
Motivation: 现有轻量级静态分析技术在协作软件开发中检测语义干扰时存在高误报率，主要原因是无法有效区分行为保持的代码重构和影响行为的变更

Method: 在现有静态技术基础上整合自动重构检测，从报告中排除行为保持的重构，减少误报同时保持检测覆盖率

Result: 在标记数据集上减少近32%的误报，虽然伴随非显著增加的漏报，但精度提升显著超过召回率的微小权衡

Conclusion: 重构感知的干扰检测是改进现代开发工作流中合并支持的实用有效策略

Abstract: Detecting semantic interference remains a challenge in collaborative software
development. Recent lightweight static analysis techniques improve efficiency
over SDG-based methods, but they still suffer from a high rate of false
positives. A key cause of these false positives is the presence of
behavior-preserving code refactorings, which current techniques cannot
effectively distinguish from changes that impact behavior and can interfere
with others. To handle this problem we present RefFilter, a refactoring-aware
tool for semantic interference detection. It builds on existing static
techniques by incorporating automated refactoring detection to improve
precision. RefFilter discards behavior-preserving refactorings from reports,
reducing false positives while preserving detection coverage. To evaluate
effectiveness and scalability, use two datasets: a labeled dataset with 99
scenarios and ground truth, and a novel dataset of 1,087 diverse merge
scenarios that we have built. Experimental results show that RefFilter reduces
false positives by nearly 32% on the labeled dataset. While this reduction
comes with a non significant increase in false negatives, the overall gain in
precision significantly outweighs the minor trade-off in recall. These findings
demonstrate that refactoring-aware interference detection is a practical and
effective strategy for improving merge support in modern development workflows.

</details>


### [8] [Clarifying Semantics of In-Context Examples for Unit Test Generation](https://arxiv.org/abs/2510.01994)
*Chen Yang,Lin Yang,Ziqi Wang,Dong Wang,Jianyi Zhou,Junjie Chen*

Main category: cs.SE

TL;DR: CLAST是一种通过系统化重构单元测试来提高语义清晰度的技术，能够提升作为上下文学习示例的效用，在测试生成中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于上下文学习的单元测试生成方法受示例质量影响很大，语义不清晰或结构不良的测试示例会导致生成结果不理想。

Method: 通过程序分析和LLM重写相结合的方式，将复杂测试分解为逻辑更清晰的测试，提高语义清晰度。

Result: CLAST在4个开源项目和3个工业项目中评估，完全保留了原始测试的有效性，而UTgen方法在编译成功率、通过率、测试覆盖率和变异得分上分别平均降低了12.90%、35.82%、4.65%和5.07%。85.33%的用户研究参与者更偏好CLAST重构测试的语义清晰度。

Conclusion: CLAST重构的测试作为示例能有效改进基于上下文学习的单元测试生成方法，相比UTgen重构的测试，在生成测试的编译成功率、通过率和覆盖率上分别平均提高了25.97%、28.22%和45.99%。

Abstract: Recent advances in large language models (LLMs) have enabled promising
performance in unit test generation through in-context learning (ICL). However,
the quality of in-context examples significantly influences the effectiveness
of generated tests-poorly structured or semantically unclear test examples
often lead to suboptimal outputs. In this paper, we propose CLAST, a novel
technique that systematically refines unit tests to improve their semantic
clarity, thereby enhancing their utility as in-context examples. The approach
decomposes complex tests into logically clearer ones and improves semantic
clarity through a combination of program analysis and LLM-based rewriting. We
evaluated CLAST on four open-source and three industrial projects. The results
demonstrate that CLAST largely outperforms UTgen, the state-of-the-art
refinement technique, in both preserving test effectiveness and enhancing
semantic clarity. Specifically, CLAST fully retains the original effectiveness
of unit tests, while UTgen reduces compilation success rate (CSR), pass rate
(PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%,
35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user
study preferred the semantic clarity of CLAST-refined tests. Notably,
incorporating CLAST-refined tests as examples effectively improves ICL-based
unit test generation approaches such as RAGGen and TELPA, resulting in an
average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for
generated tests, compared to incorporating UTgen-refined tests. The insights
from the follow-up user study not only reinforce CLAST's potential impact in
software testing practice but also illuminate avenues for future research.

</details>


### [9] [Automatic Generation of Combinatorial Reoptimisation Problem Specifications: A Vision](https://arxiv.org/abs/2510.02002)
*Maximilian Kratz,Steffen Zschaler,Jens Kosiol,Gabriele Taentzer*

Main category: cs.SE

TL;DR: 本文探讨了在上下文因素变化时如何重新优化解决方案的问题，提出了基于模型驱动工程的方法来系统地从原始优化问题规范中推导出重新优化问题。


<details>
  <summary>Details</summary>
Motivation: 当优化问题解决后，如果上下文因素发生变化，需要重新优化解决方案。传统方法需要重新解决整个问题，但重新优化问题具有特殊性：需要最小化对原始解决方案的更改、某些部分可能无法更改、需要生成从原始解决方案到新解决方案的变更脚本。

Method: 采用模型驱动工程方法，特别是使用声明式建模语言和模型转换来高层次地规范优化问题。提出了重新优化问题的初步分类和推导相应重新优化规范的策略，并基于GIPS工具实现了概念验证。

Result: 开发了一个基于GIPS工具的初始概念验证实现，并将其应用于助教分配问题的示例中，展示了该方法在资源分配问题中的可行性。

Conclusion: 模型驱动工程为从原始优化问题规范系统推导重新优化问题提供了新的机会，特别是在组合重新优化问题领域。该方法能够有效处理解决方案适应性问题，并生成相应的变更脚本。

Abstract: Once an optimisation problem has been solved, the solution may need
adaptation when contextual factors change. This challenge, also known as
reoptimisation, has been addressed in various problem domains, such as railway
crew rescheduling, nurse rerostering, or aircraft recovery. This requires a
modified problem to be solved again to ensure that the adapted solution is
optimal in the new context. However, the new optimisation problem differs
notably from the original problem: (i) we want to make only minimal changes to
the original solution to minimise the impact; (ii) we may be unable to change
some parts of the original solution (e.g., because they refer to past
allocations); and (iii) we need to derive a change script from the original
solution to the new solution. In this paper, we argue that Model-Driven
Engineering (MDE) - in particular, the use of declarative modelling languages
and model transformations for the high-level specification of optimisation
problems - offers new opportunities for the systematic derivation of
reoptimisation problems from the original optimisation problem specification.
We focus on combinatorial reoptimisation problems and provide an initial
categorisation of changing problems and strategies for deriving the
corresponding reoptimisation specifications. We introduce an initial
proof-of-concept implementation based on the GIPS (Graph-Based (Mixed) Integer
Linear Programming Problem Specification) tool and apply it to an example
resource-allocation problem: the allocation of teaching assistants to teaching
sessions.

</details>


### [10] [ACM SIGSOFT SEN Empirical Software Engineering: Introducing Our New Regular Column](https://arxiv.org/abs/2510.02007)
*Justus Bogner,Roberto Verdecchia*

Main category: cs.SE

TL;DR: 介绍新的ACM SIGSOFT SEN专栏(SEN-ESE)，旨在讨论实证软件工程研究的元方面，包括最佳实践、统计方法等，通过专家访谈、焦点小组等方式鼓励反思和改进ESE研究。


<details>
  <summary>Details</summary>
Motivation: ESE研究领域虽然成熟但仍需发展，面临研究可重复性、外部有效性有限、评审主观性等问题，且许多研究方面未明确记录，使新人难以掌握。

Method: 通过新的SEN专栏作为讨论平台，采用专家访谈、焦点小组、调查和立场文章等方式收集内容，关注ESE研究的元方面。

Result: 建立了专门讨论ESE研究元方面的论坛，为社区提供了反思和改进研究实践的机会。

Conclusion: 该专栏将成为定期讨论ESE研究中较少触及或隐含话题的场所，邀请社区反馈具有挑战性、争议性或未充分探索的主题。

Abstract: From its early foundations in the 1970s, empirical software engineering (ESE)
has evolved into a mature research discipline that embraces a plethora of
different topics, methodologies, and industrial practices. Despite its
remarkable progress, the ESE research field still needs to keep evolving, as
new impediments, shortcoming, and technologies emerge. Research
reproducibility, limited external validity, subjectivity of reviews, and
porting research results to industrial practices are just some examples of the
drivers for improvements to ESE research. Additionally, several facets of ESE
research are not documented very explicitly, which makes it difficult for
newcomers to pick them up. With this new regular ACM SIGSOFT SEN column
(SEN-ESE), we introduce a venue for discussing meta-aspects of ESE research,
ranging from general topics such as the nature and best practices for
replication packages, to more nuanced themes such as statistical methods,
interview transcription tools, and publishing interdisciplinary research. Our
aim for the column is to be a place where we can regularly spark conversations
on ESE topics that might not often be touched upon or are left implicit.
Contributions to this column will be grounded in expert interviews, focus
groups, surveys, and position pieces, with the goal of encouraging reflection
and improvement in how we conduct, communicate, teach, and ultimately improve
ESE research. Finally, we invite feedback from the ESE community on
challenging, controversial, or underexplored topics, as well as suggestions for
voices you would like to hear from. While we cannot promise to act on every
idea, we aim to shape this column around the community interests and are
grateful for all contributions.

</details>


### [11] [Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection](https://arxiv.org/abs/2510.02165)
*Peter Wauyo,Dalia Bwiza,Alain Murara,Edwin Mugume,Eric Umuhoza*

Main category: cs.SE

TL;DR: 提出基于多模态（CCTV视频和音频）的公共交通欺诈和逃票检测系统，使用ViViT和AST模型提取特征，通过张量融合网络实现跨模态交互，在自定义数据集上达到89.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决公共交通中的欺诈和逃票问题，减少收入损失，提高乘客安全和运营合规性。

Method: 使用Vision Transformer for Video (ViViT)提取视频特征，Audio Spectrogram Transformer (AST)分析音频，采用Tensor Fusion Network (TFN)架构通过2-fold笛卡尔积显式建模单模态和双模态交互。

Result: 在欺诈活动检测中达到89.5%准确率、87.2%精确率和84.0%召回率，显著优于早期融合基线，比最先进交通欺诈检测系统75%的召回率更高。消融研究显示张量融合方法比传统拼接方法在F1分数上提升7.0%，召回率提升8.8%。

Conclusion: 该系统支持实时检测，能有效帮助公共交通运营商减少收入损失，改善乘客安全，确保运营合规。

Abstract: This research introduces a multimodal system designed to detect fraud and
fare evasion in public transportation by analyzing closed circuit television
(CCTV) and audio data. The proposed solution uses the Vision Transformer for
Video (ViViT) model for video feature extraction and the Audio Spectrogram
Transformer (AST) for audio analysis. The system implements a Tensor Fusion
Network (TFN) architecture that explicitly models unimodal and bimodal
interactions through a 2-fold Cartesian product. This advanced fusion technique
captures complex cross-modal dynamics between visual behaviors (e.g.,
tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds).
The system was trained and tested on a custom dataset, achieving an accuracy of
89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent
activities, significantly outperforming early fusion baselines and exceeding
the 75% recall rates typically reported in state-of-the-art transportation
fraud detection systems. Our ablation studies demonstrate that the tensor
fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost
in recall compared to traditional concatenation methods. The solution supports
real-time detection, enabling public transport operators to reduce revenue
loss, improve passenger safety, and ensure operational compliance.

</details>


### [12] [SIEVE: Towards Verifiable Certification for Code-datasets](https://arxiv.org/abs/2510.02166)
*Fatou Ndiaye Mbodji,El-hacen Diallo,Jordan Samhi,Kui Liu,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: SIEVE框架：社区驱动的代码数据集质量认证系统，用可验证的置信卡替代传统数据集卡片，提供统计保证


<details>
  <summary>Details</summary>
Motivation: 现有代码数据集缺乏可验证的质量保证，静态数据集卡片不可审计且无统计保证，导致质量认证困难，团队需构建孤立的清洗管道

Method: 开发SIEVE框架，将属性检查转化为置信卡——机器可读、可验证的证书，包含任意时间有效的统计边界

Result: 提出了研究计划，旨在使SIEVE成熟化，用任意时间可验证的认证替代叙述性卡片

Conclusion: 这一转变预计将降低质量保证成本，增加对代码数据集的信任

Abstract: Code agents and empirical software engineering rely on public code datasets,
yet these datasets lack verifiable quality guarantees. Static 'dataset cards'
inform, but they are neither auditable nor do they offer statistical
guarantees, making it difficult to attest to dataset quality. Teams build
isolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We
present SIEVE, a community-driven framework. It turns per-property checks into
Confidence Cards-machine-readable, verifiable certificates with anytime-valid
statistical bounds. We outline a research plan to bring SIEVE to maturity,
replacing narrative cards with anytime-verifiable certification. This shift is
expected to lower quality-assurance costs and increase trust in code-datasets.

</details>


### [13] [TAIBOM: Bringing Trustworthiness to AI-Enabled Systems](https://arxiv.org/abs/2510.02169)
*Vadim Safronov,Anthony McCaigue,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: 提出了TAIBOM框架，将SBOM原则扩展到AI领域，解决AI系统依赖管理的挑战


<details>
  <summary>Details</summary>
Motivation: 开源软件和AI技术的融合给软件供应链带来了新的复杂性，现有SBOM框架无法捕捉AI系统的动态、数据驱动特性以及数据集、模型和软件组件之间的松散耦合依赖关系

Method: 引入可信AI物料清单(TAIBOM)框架，包括：针对AI组件的结构化依赖模型、跨异构AI管道的完整性声明传播机制、验证组件来源的信任认证过程

Result: TAIBOM支持AI工作流程中的保证、安全性和合规性，相比SPDX和CycloneDX等现有标准具有优势

Conclusion: 这项工作通过结构化软件透明度为可信和可验证的AI系统奠定了基础

Abstract: The growing integration of open-source software and AI-driven technologies
has introduced new layers of complexity into the software supply chain,
challenging existing methods for dependency management and system assurance.
While Software Bills of Materials (SBOMs) have become critical for enhancing
transparency and traceability, current frameworks fall short in capturing the
unique characteristics of AI systems -- namely, their dynamic, data-driven
nature and the loosely coupled dependencies across datasets, models, and
software components. These challenges are compounded by fragmented governance
structures and the lack of robust tools for ensuring integrity, trust, and
compliance in AI-enabled environments.
  In this paper, we introduce Trusted AI Bill of Materials (TAIBOM) -- a novel
framework extending SBOM principles to the AI domain. TAIBOM provides (i) a
structured dependency model tailored for AI components, (ii) mechanisms for
propagating integrity statements across heterogeneous AI pipelines, and (iii) a
trust attestation process for verifying component provenance. We demonstrate
how TAIBOM supports assurance, security, and compliance across AI workflows,
highlighting its advantages over existing standards such as SPDX and CycloneDX.
This work lays the foundation for trustworthy and verifiable AI systems through
structured software transparency.

</details>


### [14] [FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI](https://arxiv.org/abs/2510.02185)
*Paschal C. Amusuo,Dongge Liu,Ricardo Andres Calvo Mendez,Jonathan Metzman,Oliver Chang,James C. Davis*

Main category: cs.SE

TL;DR: 提出两种AI驱动策略来减少OSS-Fuzz-Gen中的误报崩溃：约束驱动的模糊测试驱动生成和基于上下文的崩溃验证，在1500个基准函数上减少虚假崩溃达8%，报告崩溃减少一半以上。


<details>
  <summary>Details</summary>
Motivation: 自动生成的模糊测试驱动经常导致误报崩溃，特别是在需要高度结构化输入和复杂状态要求的函数中，这在工业级模糊测试驱动生成中会损害系统可信度。

Method: 1. 约束驱动的模糊测试驱动生成：主动对函数输入和状态施加约束来指导驱动创建；2. 基于上下文的崩溃验证：反应性分析函数调用者，确定报告的崩溃是否从程序入口点可行。

Result: 在1500个OSS-Fuzz基准函数上，这些策略减少虚假崩溃达8%，报告崩溃减少超过一半，并证明前沿LLM可以作为可靠的程序分析代理。

Conclusion: 研究结果突显了将AI集成到大规模模糊测试管道中的前景和挑战，LLM在程序分析中展现出可靠性。

Abstract: Fuzz testing has become a cornerstone technique for identifying software bugs
and security vulnerabilities, with broad adoption in both industry and
open-source communities. Directly fuzzing a function requires fuzz drivers,
which translate random fuzzer inputs into valid arguments for the target
function. Given the cost and expertise required to manually develop fuzz
drivers, methods exist that leverage program analysis and Large Language Models
to automatically generate these drivers. However, the generated fuzz drivers
frequently lead to false positive crashes, especially in functions highly
structured input and complex state requirements. This problem is especially
crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as
reporting false positive crashes to maintainers impede trust in both the system
and the team.
  This paper presents two AI-driven strategies to reduce false positives in
OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,
constraint-based fuzz driver generation proactively enforces constraints on a
function's inputs and state to guide driver creation. Second, context-based
crash validation reactively analyzes function callers to determine whether
reported crashes are feasible from program entry points. Using 1,500 benchmark
functions from OSS-Fuzz, we show that these strategies reduce spurious crashes
by up to 8%, cut reported crashes by more than half, and demonstrate that
frontier LLMs can serve as reliable program analysis agents. Our results
highlight the promise and challenges of integrating AI into large-scale fuzzing
pipelines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment](https://arxiv.org/abs/2510.01216)
*Preston Vander Vos*

Main category: cs.DC

TL;DR: Odontoceti是一种基于DAG的共识协议，通过将容错率从33%降低到20%来换取性能提升，实现了两轮通信确认、300毫秒延迟和10,000 TPS的高性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决区块链用户对可扩展性的需求，期望快速确认和即时交易处理，通过降低容错率来优化延迟和吞吐量性能。

Method: 采用n=5f+1验证者架构，构建未认证DAG并使用新颖的决策规则提交区块，包含针对崩溃故障的优化机制。

Result: 在现实网络条件下实现300毫秒中位延迟和10,000 TPS，相比现有生产协议延迟改善20-25%，验证了两轮通信确认的性能优势。

Conclusion: 证明了较低容错率共识协议在区块链中的实际可行性，为性能优化提供了新的设计思路。

Abstract: Users of blockchains value scalability, expecting fast confirmations and
immediate transaction processing. Odontoceti, the latest in DAG-based
consensus, addresses these concerns by prioritizing low latency and high
throughput, making a strategic trade-off in security by operating with a 20%
fault tolerance instead of the established 33% level. It is the first DAG-based
protocol to achieve commitment in just two communication rounds, delivering
median latency of 300 milliseconds while processing 10,000 transactions per
second under realistic network conditions. Odontoceti operates with n = 5f + 1
validators and creates an uncertified DAG with a novel decision rule for
committing blocks. The protocol includes an optimization that advances progress
when participants are slow, benefiting crash fault scenarios which are more
common in practice than Byzantine faults. Evaluation results demonstrate 20-25%
latency improvements compared to an existing production protocol, validating
that reducing wave length from three rounds to two rounds yields meaningful
performance benefits. This paper establishes the practical viability of lower
fault tolerance consensus protocols for blockchains.

</details>


### [16] [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)
*Lingling Zeng,Gen Zhang,Jialin Peng,Xiang Xu,Yuan Xu,Lijun Ma*

Main category: cs.DC

TL;DR: Kant是一个高效统一的大规模AI容器集群调度平台，支持训练和推理作业的协同调度，通过Backfill和增强Binpack等策略显著提升资源利用率和调度效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI集群规模扩大和大语言模型训练推理需求快速增长，传统调度系统在资源利用率、调度效率和服务质量方面面临挑战。

Method: 基于Kant系统的实际实现，采用Backfill和增强Binpack等调度策略，系统定义了一套AI集群关键评估指标。

Result: 实验结果显示Kant在数百到数万GPU规模的集群中表现优异，显著提升资源利用率、降低资源碎片和通信开销。

Conclusion: Kant为构建高性能、高可用的AI原生调度基础设施提供了实用的工程方法，已在多个AI数据中心集群中稳定部署。

Abstract: As AI cluster sizes continue to expand and the demand for
large-language-model (LLM) training and inference workloads grows rapidly,
traditional scheduling systems face significant challenges in balancing
resource utilization, scheduling efficiency, and service quality. This paper
presents and evaluates Kant: an efficient unified scheduling platform designed
for large-scale AI container clusters, supporting the co-scheduling of both
training and inference jobs. Based on the practical implementation of the Kant
system, we systematically define a set of key evaluation metrics for AI
clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate
(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution
(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a
foundation for quantitative performance analysis. Experimental results
demonstrate that Kant achieves exceptional performance in clusters ranging from
hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such
as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves
resource utilization and scheduling efficiency, while effectively reducing
resource fragmentation and communication overhead in distributed training. The
system has been deployed in multiple AI data center clusters, where it stably
supports large-scale intelligent computing workloads. This work provides a
practical engineering approach for building high-performance, highly available,
AI-native scheduling infrastructure.

</details>


### [17] [IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol](https://arxiv.org/abs/2510.01260)
*Ningyuan Yang,Guanliang Lyu,Mingchen Ma,Yiyi Lu,Yiming Li,Zhihui Gao,Hancheng Ye,Jianyi Zhang,Tingjun Chen,Yiran Chen*

Main category: cs.DC

TL;DR: 提出了IoT-MCP框架，通过边缘部署的MCP服务器连接LLM和IoT设备，并创建了首个包含114个基础任务和1140个复杂任务的基准测试IoT-MCP Bench。


<details>
  <summary>Details</summary>
Motivation: 解决LLM与IoT系统集成面临的硬件异构性和控制复杂性挑战，Model Context Protocol (MCP) 成为关键推动者。

Method: 实现MCP协议通过边缘部署服务器，构建IoT-MCP框架连接LLM和IoT生态系统。

Result: 在22种传感器类型和6种微控制器单元上的实验验证显示：100%任务成功率，205ms平均响应时间，74KB峰值内存占用。

Conclusion: 该工作提供了开源集成框架和标准化评估方法，为LLM-IoT系统的发展奠定了基础。

Abstract: The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)
systems faces significant challenges in hardware heterogeneity and control
complexity. The Model Context Protocol (MCP) emerges as a critical enabler,
providing standardized communication between LLMs and physical devices. We
propose IoT-MCP, a novel framework that implements MCP through edge-deployed
servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we
introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,
``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel
so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation
across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%
task success rate to generate tool calls that fully meet expectations and
obtain completely accurate results, 205ms average response time, and 74KB peak
memory footprint. This work delivers both an open-source integration framework
(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized
evaluation methodology for LLM-IoT systems.

</details>


### [18] [QScale: Probabilistic Chained Consensus for Moderate-Scale Systems](https://arxiv.org/abs/2510.01536)
*Hasan Heydari,Alysson Bessani,Kartik Nayak*

Main category: cs.DC

TL;DR: 提出了QScale协议，针对中等规模分布式账本系统（几百到一千个节点），实现了每个进程O(κ√n)的通信复杂度、总通信复杂度O(nκ)和O(κ)轮的最佳延迟。


<details>
  <summary>Details</summary>
Motivation: 现有分布式账本协议要么通信复杂度高（如PBFT）只适合小规模系统，要么依赖委员会抽样（如Algorand）只适合大规模系统，都不适合中等规模（几百到一千个节点）的生产系统。

Method: 设计QScale协议，通过优化通信结构实现亚线性通信复杂度，使用安全参数κ来保证安全性和活性。

Result: QScale实现了每个进程O(κ√n)的预期通信复杂度、总通信复杂度O(nκ)和O(κ)轮的最佳延迟，同时以压倒性概率保证安全性和活性。

Conclusion: QScale填补了中等规模分布式账本协议的空白，为生产系统提供了高效的解决方案。

Abstract: Existing distributed ledger protocols either incur a high communication
complexity and are thus suited to systems with a small number of processes
(e.g., PBFT), or rely on committee-sampling-based approaches that only work for
a very large number of processes (e.g., Algorand). Neither of these lines of
work is well-suited for moderate-scale distributed ledgers ranging from a few
hundred to a thousand processes, which are common in production (e.g, Redbelly,
Sui). The goal of this work is to design a distributed ledger with sub-linear
communication complexity per process, sub-quadratic total communication
complexity, and low latency for finalizing a block into the ledger, such that
it can be used for moderate-scale systems. We propose QScale, a protocol in
which every process incurs only $\widetilde{O}(\kappa \sqrt{n})$ communication
complexity per-block in expectation, $\widetilde{O}(n\kappa)$ total
communication complexity per-block in expectation, and a best-case latency of
$O(\kappa)$ rounds while ensuring safety and liveness with overwhelming
probability, with $\kappa$ being a small security parameter.

</details>


### [19] [Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge](https://arxiv.org/abs/2510.01885)
*Jamie Cotter,Ignacio Castineiras,Victor Cionca*

Main category: cs.DC

TL;DR: 提出了一种用于移动边缘设备上低延迟、截止时间约束的DNN卸载解决方案，通过轻量级网络状态表示和动态带宽估计机制来减少延迟并提高任务吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决移动边缘设备上DNN任务卸载的低延迟和截止时间约束问题，特别是在高负载情况下提高系统性能。

Method: 设计了考虑设备可用性、网络链路通信、优先级感知抢占和任务截止时间的调度算法，包括资源可用性表示、网络离散化和动态带宽估计机制。

Result: 在由四个Raspberry Pi 2组成的移动边缘设备系统上实现，与之前的方法相比，在高负载下表现出更好的性能，动态带宽估计有助于任务放置并提高资源稀缺时的任务吞吐量。

Conclusion: 新的低延迟抽象模型在高负载工作负载下表现更好，动态带宽估计机制在资源稀缺时能有效提高任务吞吐量。

Abstract: In this paper, we present a solution for low-latency deadline-constrained DNN
offloading on mobile edge devices. We design a scheduling algorithm with
lightweight network state representation, considering device availability,
communication on the network link, priority-aware pre-emption, and task
deadlines. The scheduling algorithm aims to reduce latency by designing a
resource availability representation, as well as a network discretisation and a
dynamic bandwidth estimation mechanism. We implement the scheduling algorithm
into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,
sampling a waste classification conveyor belt at a set frame rate. The system
is evaluated and compared to a previous approach of ours, which was proven to
outcompete work-stealers and a non-pre-emption based scheduling heuristic under
the aforementioned waste classification scenario. Our findings show the novel
lower latency abstraction models yield better performance under high-volume
workloads, with the dynamic bandwidth estimation assisting the task placement
while, ultimately, increasing task throughput in times of resource scarcity.

</details>


### [20] [Programming RISC-V accelerators via Fortran](https://arxiv.org/abs/2510.02170)
*Nick Brown,Jake Davies,Felix LeClair*

Main category: cs.DC

TL;DR: 本文提出了一种通过Fortran驱动RISC-V加速器的方法，避免为科学计算代码重新开发。


<details>
  <summary>Details</summary>
Motivation: RISC-V加速器具有HPC潜力，但通常需要重新编写代码，而科学计算中的复杂Fortran代码难以重写。

Method: 开发了一种方法，使Fortran代码能够直接驱动RISC-V加速器架构。

Result: 该方法避免了代码重新开发，使现有Fortran科学计算代码能够在RISC-V加速器上运行。

Conclusion: 提出的方法为在RISC-V加速器上运行现有Fortran科学计算代码提供了可行方案。

Abstract: A range of RISC-V based accelerators are available and coming to market, and
there is strong potential for these to be used for High Performance Computing
(HPC) workloads. However, such accelerators tend to provide bespoke programming
models and APIs that require codes to be rewritten. In scientific computing,
where many of the simulation code are highly complex, extensive, and written in
Fortran, this is not realistic. In this extended abstract we present an
approach that enables driving such architectures via Fortran, avoiding code
redevelopment.

</details>
