{"id": "2511.21160", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21160", "abs": "https://arxiv.org/abs/2511.21160", "authors": ["Wu Sai", "Xia Ruichen", "Yang Dingyu", "Wang Rui", "Lai Huihang", "Guan Jiarui", "Bai Jiameng", "Zhang Dongxiang", "Tang Xiu", "Xie Zhongle", "Lu Peng", "Chen Gang"], "title": "MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference", "comment": null, "summary": "The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.", "AI": {"tldr": "MorphingDB\u662f\u4e00\u4e2a\u4efb\u52a1\u4e2d\u5fc3\u7684AI\u539f\u751f\u6570\u636e\u5e93\u7cfb\u7edf\uff0c\u5728PostgreSQL\u4e2d\u81ea\u52a8\u5316\u6a21\u578b\u5b58\u50a8\u3001\u9009\u62e9\u548c\u63a8\u7406\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6a21\u5f0f\u548c\u591a\u7ef4\u5f20\u91cf\u6570\u636e\u7c7b\u578b\u5b9e\u73b0\u7075\u6d3b\u5b58\u50a8\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u6a21\u578b\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u9884\u5d4c\u5165\u548cDAG\u6279\u5904\u7406\u7ba1\u9053\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u539f\u751fDBMS\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u6a21\u578b\u4e2d\u5fc3\u8bbe\u8ba1\u9700\u8981\u624b\u52a8\u9009\u62e9\u914d\u7f6e\u6a21\u578b\u5bfc\u81f4\u5f00\u53d1\u6210\u672c\u9ad8\uff0c\u6216\u4efb\u52a1\u4e2d\u5fc3AutoML\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0eDBMS\u96c6\u6210\u5dee\u7684\u95ee\u9898\u3002", "method": "1. \u5f15\u5165\u4e13\u95e8\u6a21\u5f0f\u548c\u591a\u7ef4\u5f20\u91cf\u6570\u636e\u7c7b\u578b\u652f\u6301BLOB-based\u6a21\u578b\u5b58\u50a8\uff1b2. \u8bbe\u8ba1\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5d4c\u5165\u5386\u53f2\u4efb\u52a1\u6784\u5efa\u8fc1\u79fb\u6027\u5b50\u7a7a\u95f4\uff0c\u5728\u7ebf\u6295\u5f71\u8fdb\u884c\u5b9e\u65f6\u4efb\u52a1\u5904\u7406\uff1b3. \u91c7\u7528\u9884\u5d4c\u5165\u4e0e\u5411\u91cf\u5171\u4eab\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\uff0cDAG\u6279\u5904\u7406\u7ba1\u9053\u4e0e\u6210\u672c\u611f\u77e5\u8c03\u5ea6\u4f18\u5316\u63a8\u7406\u3002", "result": "\u57289\u4e2a\u516c\u5171\u6570\u636e\u96c6\uff08\u5e8f\u5217\u3001NLP\u3001\u56fe\u50cf\u4efb\u52a1\uff09\u4e0a\uff0cMorphingDB\u4f18\u4e8eAI\u539f\u751fDBMS\uff08EvaDB\u3001Madlib\u3001GaussML\uff09\u548cAutoML\u5e73\u53f0\uff08AutoGluon\u3001AutoKeras\u3001AutoSklearn\uff09\uff0c\u5728\u6a21\u578b\u9009\u62e9\u7684\u51c6\u786e\u6027\u3001\u8d44\u6e90\u6d88\u8017\u548c\u65f6\u95f4\u6210\u672c\u4e4b\u95f4\u8fbe\u5230\u826f\u597d\u5e73\u8861\uff0c\u63a8\u7406\u541e\u5410\u91cf\u548c\u8d44\u6e90\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "MorphingDB\u5c55\u793a\u4e86\u5728PostgreSQL\u4e2d\u5b9e\u73b0\u4efb\u52a1\u4e2d\u5fc3AI\u539f\u751f\u6570\u636e\u5e93\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6a21\u578b\u5b58\u50a8\u3001\u9009\u62e9\u548c\u63a8\u7406\uff0c\u5728\u51c6\u786e\u6027\u3001\u8d44\u6e90\u6548\u7387\u548c\u65f6\u95f4\u6210\u672c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2511.21307", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21307", "abs": "https://arxiv.org/abs/2511.21307", "authors": ["Xinyi Zhang", "Liang Liang", "Anastasia Ailamaki", "Jianliang Xu"], "title": "HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads", "comment": "Accepted to SIGMOD 2026. This is the extended technical report", "summary": "Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.", "AI": {"tldr": "HIRE\u662f\u4e00\u79cd\u6df7\u5408\u5185\u5b58\u7d22\u5f15\u7ed3\u6784\uff0c\u7ed3\u5408\u4e86\u4f20\u7edf\u7d22\u5f15\u7684\u7a33\u5065\u6027\u548c\u5b66\u4e60\u7d22\u5f15\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5728\u8303\u56f4\u67e5\u8be2\u541e\u5410\u91cf\u3001\u5c3e\u5ef6\u8fdf\u548c\u6574\u4f53\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b66\u4e60\u7d22\u5f15\u5728\u70b9\u67e5\u8be2\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u9ad8\u5c3e\u5ef6\u8fdf\u3001\u8303\u56f4\u67e5\u8be2\u6027\u80fd\u4e0d\u4f73\u4ee5\u53ca\u5728\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u6548\u679c\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7a33\u5b9a\u9ad8\u6548\u5904\u7406\u5404\u79cd\u67e5\u8be2\u7c7b\u578b\u7684\u7d22\u5f15\u7ed3\u6784\u3002", "method": "HIRE\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a(1)\u81ea\u9002\u5e94\u6df7\u5408\u53f6\u5b50\u8282\u70b9\uff0c(2)\u6a21\u578b\u52a0\u901f\u7684\u5185\u90e8\u8282\u70b9\u914d\u5408\u65e5\u5fd7\u66f4\u65b0\uff0c(3)\u975e\u963b\u585e\u6210\u672c\u9a71\u52a8\u91cd\u6821\u51c6\u673a\u5236\uff0c(4)\u8de8\u5c42\u4f18\u5316\u7684\u6279\u91cf\u52a0\u8f7d\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHIRE\u5728\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534741.7\u500d\uff0c\u5c3e\u5ef6\u8fdf\u964d\u4f4e\u8fbe98%\uff0c\u5728\u8303\u56f4\u67e5\u8be2\u541e\u5410\u91cf\u3001\u5c3e\u5ef6\u8fdf\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u7d22\u5f15\u548c\u4f20\u7edf\u7d22\u5f15\u3002", "conclusion": "HIRE\u6210\u529f\u5730\u5c06\u4f20\u7edf\u7d22\u5f15\u7684\u7a33\u5065\u6027\u4e0e\u5b66\u4e60\u7d22\u5f15\u7684\u9884\u6d4b\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u4e3a\u73b0\u4ee3\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u7d22\u5f15\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Ke\u00dfler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d85\u7ea7\u8ba1\u7b97\u673aRAMSES\u4e0a\u96c6\u6210vLLM\u3001Slurm\u548cKubernetes\u6765\u670d\u52a1LLM\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406100-1000\u4e2a\u5e76\u53d1\u8bf7\u6c42\uff0c\u5ef6\u8fdf\u5f00\u9500\u4ec5\u7ea6500\u6beb\u79d2\u3002", "motivation": "AI\u63a8\u7406\u9700\u6c42\u589e\u957f\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7b49\u6559\u80b2\u9886\u57df\uff0c\u9700\u8981\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u7684\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u4f20\u7edfHPC\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u540c\u6b65\u3001\u9762\u5411\u7528\u6237\u7684\u52a8\u6001AI\u5e94\u7528\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u5728\u8d85\u7ea7\u8ba1\u7b97\u673aRAMSES\u4e0a\u96c6\u6210vLLM\u3001Slurm\u548cKubernetes\u6765\u670d\u52a1\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u3002", "result": "\u521d\u59cb\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u9ad8\u6548\u6269\u5c55\u5904\u7406100\u3001500\u548c1000\u4e2a\u5e76\u53d1\u8bf7\u6c42\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4ec5\u589e\u52a0\u7ea6500\u6beb\u79d2\u7684\u5f00\u9500\u3002", "conclusion": "\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfHPC\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u52a8\u6001AI\u5e94\u7528\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684LLM\u670d\u52a1\u90e8\u7f72\u3002"}}
{"id": "2511.20709", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.20709", "abs": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "comment": null, "summary": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.", "AI": {"tldr": "DUALGAUGE\u662f\u9996\u4e2a\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u8bc4\u4f30LLM\u751f\u6210\u4ee3\u7801\u7684\u5b89\u5168\u6027\u548c\u6b63\u786e\u6027\uff0c\u5305\u542bDUALGAUGE-BENCH\u57fa\u51c6\u5957\u4ef6\u548c\u4ee3\u7406\u7a0b\u5e8f\u6267\u884c\u5668\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u5b89\u5168\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8981\u4e48\u53ea\u8861\u91cf\u6f0f\u6d1e\u51cf\u5c11\uff0c\u8981\u4e48\u5ffd\u89c6\u6b63\u786e\u6027\u4fdd\u6301\uff0c\u6216\u8005\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8bc4\u4f30\u5b89\u5168\u6027\u548c\u529f\u80fd\u6027\uff0c\u65e0\u6cd5\u5b9e\u73b0\u8054\u5408\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86DUALGAUGE\u6846\u67b6\uff0c\u5305\u542b\u624b\u5de5\u9a8c\u8bc1\u7684\u5b89\u5168\u548c\u529f\u80fd\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4f7f\u7528\u6c99\u76d2\u73af\u5883\u8fd0\u884c\u7a0b\u5e8f\uff0c\u5e76\u57fa\u4e8eLLM\u8bc4\u4f30\u6b63\u786e\u6027\u548c\u6f0f\u6d1e\u884c\u4e3a\u3002", "result": "\u5bf9\u5341\u4e2a\u9886\u5148LLM\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5728\u6b63\u786e\u548c\u5b89\u5168\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u7cfb\u7edf\u53ef\u5e2e\u52a9\u901a\u8fc7\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u4e25\u683c\u8bc4\u4f30\u52a0\u901f\u8fdb\u5c55\u3002", "conclusion": "DUALGAUGE\u586b\u8865\u4e86\u5b89\u5168\u4ee3\u7801\u751f\u6210\u8054\u5408\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3aLLM\u751f\u6210\u7684\u4ee3\u7801\u63d0\u4f9b\u4e86\u540c\u65f6\u8bc4\u4f30\u5b89\u5168\u6027\u548c\u6b63\u786e\u6027\u7684\u6807\u51c6\u5316\u65b9\u6cd5\u3002"}}
{"id": "2511.20780", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20780", "abs": "https://arxiv.org/abs/2511.20780", "authors": ["Alison Silva", "Gustavo Callou"], "title": "Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures", "comment": null, "summary": "Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673aPetri\u7f51\u7684\u65b9\u6cd5\u6765\u5206\u6790\u79c1\u6709\u4e91\u73af\u5883\u4e2dNextcloud\u6587\u4ef6\u670d\u52a1\u5668\u7684\u53ef\u7528\u6027\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u5197\u4f59\u7b56\u7565\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4e91\u5b58\u50a8\u5e73\u53f0\u5728\u5b66\u672f\u548c\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u9700\u6c42\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5bfb\u6c42\u516c\u5171\u4e91\u66ff\u4ee3\u65b9\u6848\u7684\u7ec4\u7ec7\u3002\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528Apache CloudStack\u5728\u79c1\u6709\u4e91\u73af\u5883\u4e2d\u6258\u7ba1Nextcloud\u6587\u4ef6\u670d\u52a1\u5668\uff0c\u901a\u8fc7\u968f\u673aPetri\u7f51\u5efa\u6a21\u65b9\u6cd5\u5206\u6790\u56db\u79cd\u67b6\u6784\u914d\u7f6e\uff1a\u57fa\u7ebf\u3001\u4e3b\u673a\u7ea7\u5197\u4f59\u3001\u865a\u62df\u673a\u5197\u4f59\u4ee5\u53ca\u4e24\u8005\u7ec4\u5408\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e3b\u673a\u548c\u865a\u62df\u673a\u7ea7\u522b\u540c\u65f6\u5b9e\u65bd\u5197\u4f59\u7b56\u7565\u80fd\u663e\u8457\u63d0\u9ad8\u53ef\u7528\u6027\u5e76\u51cf\u5c11\u9884\u671f\u505c\u673a\u65f6\u95f4\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u79c1\u6709\u4e91\u53ef\u7528\u6027\u548c\u652f\u6301\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2511.20730", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20730", "abs": "https://arxiv.org/abs/2511.20730", "authors": ["Nehal Afifi", "Christoph Wittig", "Lukas Paehler", "Andreas Lindenmann", "Kai Wolter", "Felix Leitenberger", "Melih Dogru", "Patric Grauberger", "Tobias D\u00fcser", "Albert Albers", "Sven Matthiesen"], "title": "Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities", "comment": null, "summary": "The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u5206\u6790\u4e86\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u4ea7\u54c1\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u53d1\u73b0\u673a\u5668\u5b66\u4e60\u3001\u7edf\u8ba1\u65b9\u6cd5\u5360\u4e3b\u5bfc\uff0c\u6df1\u5ea6\u5b66\u4e60\u5448\u4e0a\u5347\u8d8b\u52bf\uff0c\u4f46\u5728\u9a8c\u8bc1\u9636\u6bb5\u5e94\u7528\u6709\u9650\uff0c\u5b58\u5728\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3001\u8de8\u9636\u6bb5\u53ef\u8ffd\u6eaf\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u4ea7\u54c1\u5f00\u53d1\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\u4f46\u6574\u5408\u96f6\u6563\uff0c\u7f3a\u4e4f\u5173\u4e8e\u5728\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u4f55\u65f6\u4f7f\u7528\u4f55\u79cd\u65b9\u6cd5\u7684\u660e\u786e\u6307\u5bfc\u3002", "method": "\u91c7\u7528PRISMA\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u4f7f\u7528V\u6a21\u578b\u4f5c\u4e3a\u4ea7\u54c1\u5f00\u53d1\u6846\u67b6\uff0c\u5bf9Scopus\u3001Web of Science\u548cIEEE Xplore\uff082014-2024\uff09\u76841689\u6761\u8bb0\u5f55\u8fdb\u884c\u7b5b\u9009\uff0c\u6700\u7ec8\u5206\u6790114\u7bc7\u6587\u732e\u3002", "result": "\u76d1\u7763\u5b66\u4e60\u3001\u805a\u7c7b\u3001\u56de\u5f52\u5206\u6790\u548c\u4ee3\u7406\u5efa\u6a21\u5728\u7cfb\u7edf\u8bbe\u8ba1\u3001\u5b9e\u65bd\u548c\u96c6\u6210\u9636\u6bb5\u666e\u904d\u5e94\u7528\uff0c\u4f46\u5728\u9a8c\u8bc1\u9636\u6bb5\u8d21\u732e\u6709\u9650\uff1b\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u5448\u4e0a\u5347\u8d8b\u52bf\uff1b\u73b0\u6709\u5e94\u7528\u9762\u4e34\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u8de8\u9636\u6bb5\u53ef\u8ffd\u6eaf\u6027\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "conclusion": "\u8fd9\u662f\u5236\u5b9a\u8bbe\u8ba1\u9636\u6bb5\u6307\u5357\u7684\u7b2c\u4e00\u6b65\uff0c\u540e\u7eed\u9700\u8981\u5c06\u8ba1\u7b97\u673a\u79d1\u5b66\u7b97\u6cd5\u6620\u5c04\u5230\u5de5\u7a0b\u8bbe\u8ba1\u95ee\u9898\u548c\u6d3b\u52a8\u4e2d\uff0c\u5e76\u5f00\u53d1\u53ef\u89e3\u91ca\u7684\u6df7\u5408\u6a21\u578b\u3002"}}
{"id": "2511.20834", "categories": ["cs.DC", "cs.AR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.20834", "abs": "https://arxiv.org/abs/2511.20834", "authors": ["Dionysios Adamopoulos", "Anastasia Poulopoulou", "Georgios Goumas", "Christina Giannoula"], "title": "Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks", "comment": null, "summary": "Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.", "AI": {"tldr": "Spira\u662f\u4e00\u4e2a\u57fa\u4e8eGPU\u7684\u7a00\u758f\u5377\u79ef\u5f15\u64ce\uff0c\u901a\u8fc7\u5229\u7528\u4f53\u7d20\u5750\u6807\u7684\u6574\u6570\u6027\u3001\u6709\u754c\u6027\u548c\u51e0\u4f55\u8fde\u7eed\u6027\u7b49\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u7f51\u7edc\u4e2d\u7a00\u758f\u5377\u79ef\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u5377\u79ef\u5f15\u64ce\u672a\u80fd\u5145\u5206\u5229\u7528\u4f53\u7d20\u5750\u6807\u7684\u5173\u952e\u7279\u6027\uff08\u6574\u6570\u6027\u3001\u7a7a\u95f4\u6709\u754c\u6027\u3001\u51e0\u4f55\u8fde\u7eed\u6027\uff09\uff0c\u5bfc\u81f4\u6838\u6620\u5c04\u6784\u5efa\u65f6\u7684\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u5f00\u9500\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u56db\u79cd\u5173\u952e\u6280\u672f\uff1a\u4e00\u6b21\u6027\u641c\u7d22\u7b97\u6cd5\u3001\u538b\u7f29\u539f\u751f\u5904\u7406\u65b9\u6848\u3001\u53cc\u6570\u636e\u6d41\u6267\u884c\u673a\u5236\u3001\u7f51\u7edc\u7ea7\u5e76\u884c\u5316\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u6838\u6620\u5c04\u6784\u5efa\u548c\u7279\u5f81\u8ba1\u7b97\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u7a00\u758f\u5377\u79ef\u5f15\u64ce\uff0cSpira\u5728\u7aef\u5230\u7aef\u63a8\u7406\u4e0a\u5e73\u5747\u63d0\u53471.71\u500d\uff08\u6700\u9ad82.31\u500d\uff09\uff0c\u5728\u9010\u5c42\u6267\u884c\u4e0a\u5e73\u5747\u63d0\u53472.13\u500d\uff08\u6700\u9ad83.32\u500d\uff09\u3002", "conclusion": "Spira\u901a\u8fc7\u5145\u5206\u5229\u7528\u4f53\u7d20\u5750\u6807\u7279\u6027\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7a00\u758f\u5377\u79ef\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a3D\u70b9\u4e91\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20813", "abs": "https://arxiv.org/abs/2511.20813", "authors": ["Simon Hacks"], "title": "Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms", "comment": "17 pages, submitted to CAiSE - International Conference on Advanced information Systems Engineering 2026", "summary": "\"Train While You Fight\" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u652f\u6301\"\u8fb9\u6218\u8fb9\u8bad\"(TWYF)\u6a21\u5f0f\u7684\u5148\u8fdb\u5206\u5e03\u5f0f\u5b66\u4e60\u5e73\u53f0\u7684\u6280\u672f\u9700\u6c42\uff0c\u901a\u8fc7\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\u8bc6\u522b\u4e867\u4e2a\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u7528\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3\u519b\u4e8b\u8bad\u7ec3\u9700\u8981\u4ece\u4f20\u7edf\u7684\u6218\u524d/\u6218\u540e\u8bad\u7ec3\u8f6c\u5411\"\u8fb9\u6218\u8fb9\u8bad\"\u7684\u6301\u7eed\u5b66\u4e60\u6a21\u5f0f\uff0c\u8fd9\u8981\u6c42\u5148\u8fdb\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u5e73\u53f0\u5177\u5907\u65b0\u7684\u6280\u672f\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff1a(i)\u4ecePfPC/\u5317\u7ea6\u6587\u6863\u548c\u8fd1\u671f\u5b9e\u8df5\u4e2d\u63a8\u5bfc\u6311\u6218\uff1b(ii)\u5b9a\u4e49\u89e3\u51b3\u65b9\u6848\u76ee\u6807\uff1b(iii)\u7cfb\u7edf\u6027\u5730\u5c06\u6311\u6218\u6620\u5c04\u5230\u5df2\u9a8c\u8bc1\u7684\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u4e867\u4e2a\u5173\u952e\u6280\u672f\u6311\u6218\uff1a\u4e92\u64cd\u4f5c\u6027\u3001\u5f39\u6027\u3001\u591a\u8bed\u8a00\u652f\u6301\u3001\u6570\u636e\u5b89\u5168\u4e0e\u9690\u79c1\u3001\u53ef\u6269\u5c55\u6027\u3001\u5e73\u53f0\u72ec\u7acb\u6027\u3001\u6a21\u5757\u5316\uff0c\u5e76\u901a\u8fc7\u5fb7\u56fd\u6b66\u88c5\u90e8\u961f\u7684\u56fd\u5bb6\u7528\u4f8b\u8fdb\u884c\u4e86\u6a21\u5f0f\u8bf4\u660e\u3002", "conclusion": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\u80fd\u591f\u6ee1\u8db3\"\u8fb9\u6218\u8fb9\u8bad\"\u6a21\u5f0f\u7684\u6280\u672f\u9700\u6c42\uff0c\u4e3a\u5148\u8fdb\u5206\u5e03\u5f0f\u5b66\u4e60\u5e73\u53f0\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002"}}
{"id": "2511.20975", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20975", "abs": "https://arxiv.org/abs/2511.20975", "authors": ["Yinwei Dai", "Zhuofu Chen", "Anand Iyer", "Ravi Netravali"], "title": "Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows", "comment": null, "summary": "Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\\% and reduces median latency by 32.5--78.9\\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.", "AI": {"tldr": "Aragog\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5de5\u4f5c\u6d41\u914d\u7f6e\u6765\u4f18\u5316\u591a\u9636\u6bb5LLM\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6210\u672c\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u914d\u7f6e\u65b9\u6cd5\u5728\u8bf7\u6c42\u6267\u884c\u524d\u56fa\u5b9a\u914d\u7f6e\uff0c\u65e0\u6cd5\u9002\u5e94\u7cfb\u7edf\u8d1f\u8f7d\u7684\u52a8\u6001\u53d8\u5316\uff0c\u5bfc\u81f4\u5728\u957f\u65f6\u95f4\u6267\u884c\u8fc7\u7a0b\u4e2d\u914d\u7f6e\u53d8\u5f97\u6b21\u4f18\u3002", "method": "\u5c06\u914d\u7f6e\u9009\u62e9\u95ee\u9898\u89e3\u8026\u4e3a\u4e00\u6b21\u6027\u8def\u7531\u6b65\u9aa4\uff08\u8bc6\u522b\u6240\u6709\u4fdd\u6301\u7cbe\u5ea6\u7684\u914d\u7f6e\uff09\u548c\u5ec9\u4ef7\u6bcf\u9636\u6bb5\u8c03\u5ea6\u5668\uff08\u4f7f\u7528\u6700\u65b0\u7cfb\u7edf\u89c2\u5bdf\u9009\u62e9\u914d\u7f6e\uff09\uff0c\u5e76\u5f15\u5165\u52a0\u901f\u7b56\u7565\u3002", "result": "\u5728\u591a\u6837\u5316\u5de5\u4f5c\u6d41\u548c\u6a21\u578b\u5bb6\u65cf\u4e2d\uff0cAragog\u5c06\u6700\u5927\u670d\u52a1\u541e\u5410\u91cf\u63d0\u534750.0-217.0%\uff0c\u5728\u5cf0\u503c\u8bf7\u6c42\u7387\u4e0b\u5c06\u4e2d\u4f4d\u5ef6\u8fdf\u964d\u4f4e32.5-78.9%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u6602\u8d35\u914d\u7f6e\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "Aragog\u8bc1\u660e\u4e86\u5728LLM\u5de5\u4f5c\u6d41\u670d\u52a1\u4e2d\u52a8\u6001\u914d\u7f6e\u9002\u5e94\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u4f18\u5316\u6027\u80fd\u6210\u672c\u6743\u8861\u3002"}}
{"id": "2511.20916", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20916", "abs": "https://arxiv.org/abs/2511.20916", "authors": ["Illia Khudiakov", "Vladyslav Pliuhin", "Sergiy Plankovskyy", "Yevgen Tsegelnyk"], "title": "Application of machine learning for infrastructure reconstruction programs management", "comment": "8 pages, 2 figures, 3 tables", "summary": "The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u51b3\u7b56\u652f\u6301\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u5de5\u7a0b\u57fa\u7840\u8bbe\u65bd\u91cd\u5efa\u9879\u76ee\u7684\u7ba1\u7406\u6548\u7387\uff0c\u901a\u8fc7\u7cfb\u7edf\u5efa\u6a21\u548c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u6765\u4f18\u5316\u9879\u76ee\u67b6\u6784\u548c\u5de5\u4f5c\u5206\u89e3\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u5de5\u7a0b\u57fa\u7840\u8bbe\u65bd\u91cd\u5efa\u9879\u76ee\u7ba1\u7406\u5de5\u5177\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u9002\u5e94\u6a21\u578b\u6765\u6539\u8fdb\u9879\u76ee\u67b6\u6784\u548c\u5de5\u4f5c\u5206\u89e3\u7ed3\u6784\u7684\u521b\u5efa\u8fc7\u7a0b\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u9879\u76ee\u76ee\u6807\u548c\u7cfb\u7edf\u7c7b\u578b\u3002", "method": "\u7ed3\u5408\u7cfb\u7edf\u5efa\u6a21\u5de5\u5177\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u81ea\u9002\u5e94\u6a21\u578b\uff0c\u5305\u542b\u51b3\u7b56\u8005\u504f\u597d\u3001\u51b3\u7b56\u4efb\u52a1\u3001\u8f93\u5165\u6570\u636e\u96c6\u548c\u5e94\u7528\u8f6f\u4ef6\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5386\u53f2\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u9884\u6d4b\u76ee\u6807\u51fd\u6570\u503c\u3002", "result": "\u5f00\u53d1\u4e86\u57fa\u4e8eMicrosoft Azure Machine Learning Studio\u7684\u529f\u80fd\u7ec4\u4ef6\uff0c\u786e\u5b9a\u4e86\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5e76\u7ed9\u51fa\u4e86\u8bc4\u4f30\u7ed3\u679c\uff0c\u6a21\u578b\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u5de5\u7a0b\u7cfb\u7edf\u7c7b\u578b\u548c\u9879\u76ee\u76ee\u6807\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u6a21\u578b\u53ef\u6709\u6548\u5e94\u7528\u4e8e\u70ed\u529b\u3001\u71c3\u6c14\u3001\u7535\u529b\u4f9b\u5e94\u3001\u4f9b\u6c34\u548c\u6392\u6c34\u7b49\u5de5\u7a0b\u7cfb\u7edf\u91cd\u5efa\u9879\u76ee\u7684\u7ba1\u7406\uff0c\u63d0\u9ad8\u4e86\u51b3\u7b56\u8fc7\u7a0b\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2511.20982", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20982", "abs": "https://arxiv.org/abs/2511.20982", "authors": ["Junhan Liao", "Minxian Xu", "Wanyi Zheng", "Yan Wang", "Kejiang Ye", "Rajkumar Buyya", "Chengzhong Xu"], "title": "A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving", "comment": "14 pages", "summary": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.", "AI": {"tldr": "DOPD\u662f\u4e00\u4e2a\u52a8\u6001LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574prefill\u548cdecoding\u5b9e\u4f8b\u5206\u914d\u6bd4\u4f8b\uff0c\u89e3\u51b3\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u54cd\u5e94\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u63a8\u7406\u7cfb\u7edf\u5c06prefill\u548cdecoding\u9636\u6bb5\u89e3\u8026\u5230\u4e0d\u540cGPU\u4e0a\uff0c\u4f46\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4f1a\u5bfc\u81f4\u4e24\u4e2a\u5b9e\u4f8b\u7c7b\u578b\u4e4b\u95f4\u7684\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u4e0d\u5e73\u8861\uff0c\u5f71\u54cdSLO\u8fbe\u6210\u3002", "method": "\u63d0\u51faDOPD\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5b9e\u65f6\u8d1f\u8f7d\u76d1\u63a7\u52a8\u6001\u8c03\u6574prefill-to-decoding\u6bd4\u4f8b\uff0c\u7ed3\u5408\u9002\u5f53\u7684\u8bf7\u6c42\u8c03\u5ea6\u7b56\u7565\uff0c\u89e3\u51b3\u5b9e\u4f8b\u95f4\u4e0d\u5e73\u8861\u548c\u6df7\u5408\u957f\u5ea6\u8bf7\u6c42\u4e0b\u7684\u8d44\u6e90\u5206\u914d\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u76f8\u6bd4vLLM\u548cDistServe\uff0cDOPD\u5c06\u7cfb\u7edf\u541e\u5410\u91cf\u63d0\u5347\u81f31.5\u500d\uff0cP90 TTFT\u964d\u4f4e67.5%\uff0cP90 TPOT\u964d\u4f4e22.8%\uff0cSLO\u8fbe\u6210\u7387\u8d85\u8fc799%\u4e14\u4f7f\u7528\u66f4\u5c11\u989d\u5916\u8d44\u6e90\u3002", "conclusion": "DOPD\u901a\u8fc7\u52a8\u6001P/D\u8c03\u6574\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684LLM\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u8d44\u6e90\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.20933", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20933", "abs": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "D\u00e1niel Varr\u00f3", "Tushar Sharma"], "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "comment": "18 figures", "summary": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "AI": {"tldr": "LLMs\u5bf9\u8f6f\u4ef6\u8bbe\u8ba1\u6982\u5ff5\u7684\u7406\u89e3\u5b58\u5728\u8106\u5f31\u6027\u548c\u4e0d\u5bf9\u79f0\u6027\uff1a\u8026\u5408\u5206\u6790\u5728\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u800c\u5185\u805a\u5206\u6790\u5728\u6307\u5bfc\u4efb\u52a1\u4e2d\u76f8\u5bf9\u7a33\u5065\u4f46\u65e0\u6307\u5bfc\u65f6\u4e5f\u4f1a\u5931\u6548\u3002", "motivation": "\u8bc4\u4f30LLMs\u5bf9\u8f6f\u4ef6\u8bbe\u8ba1\u6838\u5fc3\u6982\u5ff5\uff08\u5185\u805a\u6027\u548c\u8026\u5408\u6027\uff09\u7684\u7406\u89e3\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u5f00\u53d1\u73af\u5883\u4e2d\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u8bbe\u8ba1\u4e0d\u826f\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u6d4b\u8bd5DeepSeek-R1\u6a21\u578b\u5bb6\u65cf\u5728\u4e0d\u540c\u6307\u5bfc\u7ea7\u522b\uff08\u9a8c\u8bc1\u3001\u5f15\u5bfc\u3001\u5f00\u653e\u5f0f\u751f\u6210\uff09\u548c\u4e0d\u540c\u4e0a\u4e0b\u6587\u566a\u58f0\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u5bf9\u4e24\u4e2a\u6982\u5ff5\u6709\u57fa\u672c\u7406\u89e3\uff0c\u4f46\u5b9e\u9645\u77e5\u8bc6\u8106\u5f31\u4e14\u9ad8\u5ea6\u4e0d\u5bf9\u79f0\uff1a\u8026\u5408\u5206\u6790\u5728\u566a\u58f0\u5f00\u653e\u5f0f\u573a\u666f\u4e2dF1\u5206\u6570\u4e0b\u964d\u8d8550%\uff0c\u5185\u805a\u5206\u6790\u5728\u5f15\u5bfc\u4efb\u52a1\u4e2d\u5bf9\u5185\u90e8\u566a\u58f0\u7a33\u5065\u4f46\u65e0\u6307\u5bfc\u65f6\u5931\u6548\u3002", "conclusion": "LLMs\u80fd\u53ef\u9760\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\uff0c\u4f46\u5728\u566a\u58f0\u73b0\u5b9e\u73af\u5883\u4e2d\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u66f4\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u7a0b\u5e8f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2511.21018", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21018", "abs": "https://arxiv.org/abs/2511.21018", "authors": ["Antonis Psistakis"], "title": "Handling of Memory Page Faults during Virtual-Address RDMA", "comment": "Antonis Psistakis, Master of Science (MSc) Thesis 2019. The abstract and text were lightly revised in 2025 to comply with arXiv formatting guidelines", "summary": "Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.\n  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.\n  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.\n  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u73b0\u4e86\u4e00\u79cd\u4e0eDMA\u5f15\u64ce\u96c6\u6210\u7684\u9875\u9762\u9519\u8bef\u5904\u7406\u673a\u5236\uff0c\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u65b9\u6848\u89e3\u51b3RDMA\u901a\u4fe1\u4e2d\u7684\u9875\u9762\u9519\u8bef\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5185\u5b58\u56fa\u5b9a\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfRDMA\u6280\u672f\u65e0\u6cd5\u5bb9\u5fcd\u9875\u9762\u9519\u8bef\uff0c\u901a\u5e38\u91c7\u7528\u5185\u5b58\u56fa\u5b9a\u6280\u672f\uff0c\u4f46\u8fd9\u5bfc\u81f4\u7f16\u7a0b\u590d\u6742\u6027\u589e\u52a0\u3001\u5185\u5b58\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u65e0\u6cd5\u5b8c\u5168\u9632\u6b62\u9875\u9762\u9519\u8bef\u3002", "method": "\u5728ExaNeSt\u9879\u76ee\u4e2d\uff0c\u901a\u8fc7ARM SMMU\u68c0\u6d4b\u9875\u9762\u9519\u8bef\uff0c\u5f00\u53d1\u786c\u4ef6-\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u4fee\u6539Linux SMMU\u9a71\u52a8\u3001\u5f00\u53d1\u65b0\u8f6f\u4ef6\u5e93\u3001\u8c03\u6574DMA\u5f15\u64ce\u786c\u4ef6\u548c\u8c03\u5ea6\u903b\u8f91\u3002", "result": "\u5728ExaNeSt\u7684QFDB\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4e0e\u5185\u5b58\u56fa\u5b9a\u548c\u9884\u9519\u8bef\u5904\u7406\u7b49\u66ff\u4ee3\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u7684\u9875\u9762\u9519\u8bef\u5904\u7406\u673a\u5236\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3RDMA\u901a\u4fe1\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u95ee\u9898\u3002"}}
{"id": "2511.20955", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20955", "abs": "https://arxiv.org/abs/2511.20955", "authors": ["Sanchit Kaul", "Kevin Nhu", "Jason Eissayou", "Ivan Eser", "Victor Borup"], "title": "SpaceX: Exploring metrics with the SPACE model for developer productivity", "comment": "Code available at https://github.com/knhu/ECS260Project", "summary": "This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7SPACE\u6846\u67b6\u548c\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u5f00\u6e90\u4ee3\u7801\u5e93\uff0c\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u4e0e\u63d0\u4ea4\u9891\u7387\u6b63\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u590d\u5408\u751f\u4ea7\u529b\u8bc4\u5206(CPS)\u6765\u66f4\u5168\u9762\u5730\u8861\u91cf\u5f00\u53d1\u8005\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5355\u4e00\u7ef4\u5ea6\u7684\u751f\u4ea7\u529b\u542f\u53d1\u5f0f\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u591a\u7ef4\u5ea6\u7684\u751f\u4ea7\u529b\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5e7f\u4e49\u7ebf\u6027\u6df7\u5408\u6a21\u578b(GLM)\u548c\u57fa\u4e8eRoBERTa\u7684\u60c5\u611f\u5206\u7c7b\uff0c\u901a\u8fc7\u6316\u6398\u5f00\u6e90\u4ee3\u7801\u5e93\u6570\u636e\u6765\u6784\u5efa\u7efc\u5408\u751f\u4ea7\u529b\u6307\u6807\u3002", "result": "\u53d1\u73b0\u8d1f\u9762\u60c5\u611f\u72b6\u6001\u4e0e\u63d0\u4ea4\u9891\u7387\u5b58\u5728\u663e\u8457\u6b63\u76f8\u5173\uff0c\u8868\u660e\u5b58\u5728\u7531\u632b\u8d25\u611f\u9a71\u52a8\u7684\u8fed\u4ee3\u4fee\u590d\u5faa\u73af\uff1b\u8d21\u732e\u8005\u4e92\u52a8\u62d3\u6251\u5206\u6790\u6bd4\u4f20\u7edf\u57fa\u4e8e\u6570\u91cf\u7684\u6307\u6807\u80fd\u66f4\u597d\u5730\u6620\u5c04\u534f\u4f5c\u52a8\u6001\u3002", "conclusion": "\u63d0\u51fa\u4e86\u590d\u5408\u751f\u4ea7\u529b\u8bc4\u5206(CPS)\u6765\u89e3\u51b3\u5f00\u53d1\u8005\u6548\u7387\u7684\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4e3a\u751f\u4ea7\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u6846\u67b6\u3002"}}
{"id": "2511.21022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21022", "abs": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "comment": null, "summary": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e8610\u79cd\u6a21\u578b\u7f16\u8f91\u6280\u672f\u7528\u4e8e\u66f4\u65b0LLMs\u4e2d\u8fc7\u65f6\u7684API\u77e5\u8bc6\uff0c\u63d0\u51fa\u4e86AdaLoRA-L\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u5206\u901a\u7528API\u5c42\u548c\u7279\u5b9aAPI\u5c42\u6765\u63d0\u5347\u7f16\u8f91\u7279\u5f02\u6027\u3002", "motivation": "LLMs\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u7ecf\u5e38\u751f\u6210\u5df2\u5f03\u7528\u7684API\uff0c\u56e0\u4e3a\u5176\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u65f6\u6548\u6027\u95ee\u9898\u3002\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u800c\u73b0\u6709\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u662f\u5426\u80fd\u6709\u6548\u66f4\u65b0API\u77e5\u8bc6\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6784\u5efaEDAPIBench\u57fa\u51c6\uff0c\u5305\u542b70+\u4e2a\u5f03\u7528API\u548c3000+\u7f16\u8f91\u5b9e\u4f8b\u3002\u6bd4\u8f8310\u79cdSOTA\u6a21\u578b\u7f16\u8f91\u6280\u672f\uff0c\u5e76\u63d0\u51faAdaLoRA-L\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u533a\u5206'\u901a\u7528API\u5c42'\u548c'\u7279\u5b9aAPI\u5c42'\u6765\u9650\u5236\u7f16\u8f91\u8303\u56f4\u3002", "result": "AdaLoRA\u5728\u751f\u6210\u6b63\u786eAPI\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u7279\u5f02\u6027\u4e0d\u8db3\u3002AdaLoRA-L\u663e\u8457\u63d0\u9ad8\u4e86\u7279\u5f02\u6027\uff0c\u540c\u65f6\u5728\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\u4e0a\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u7f16\u8f91\u6280\u672f\u53ef\u4ee5\u6709\u6548\u66f4\u65b0LLMs\u4e2d\u7684API\u77e5\u8bc6\uff0cAdaLoRA-L\u901a\u8fc7\u5206\u5c42\u7f16\u8f91\u7b56\u7565\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7279\u5f02\u6027\u3002"}}
{"id": "2511.21431", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21431", "abs": "https://arxiv.org/abs/2511.21431", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Yueqiang Chen", "Baoguo He", "Hongfeng Sun", "Ziqing Yin", "Shangchao Su", "Zhiyan Cui", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Jianwei He", "Jiesong Ma", "Weikang Huang", "Jianglei Tong", "Dongdong Gao", "Jian Zhang", "Hong Tian"], "title": "MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training", "comment": null, "summary": "The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.", "AI": {"tldr": "MemFine\u662f\u4e00\u4e2a\u5185\u5b58\u611f\u77e5\u7684\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u91cd\u8ba1\u7b97\u7b56\u7565\u89e3\u51b3MoE\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5728\u5185\u5b58\u53d7\u9650\u7684GPU\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u7684\u5927\u89c4\u6a21MoE\u8bad\u7ec3\u3002", "motivation": "\u5927\u89c4\u6a21MoE\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u4e25\u91cd\u7684\u5185\u5b58\u74f6\u9888\uff0c\u52a8\u6001\u4ee4\u724c\u8def\u7531\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u4f1a\u9020\u6210GPU\u5185\u5b58\u6ea2\u51fa\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u8d1f\u8f7d\u5747\u8861\u65b9\u6cd5\u4f1a\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\u4e14\u5728\u5185\u5b58\u53d7\u9650\u786c\u4ef6\u4e0a\u5931\u6548\u3002", "method": "MemFine\u5c06\u4ee4\u724c\u5206\u5e03\u548c\u4e13\u5bb6\u8ba1\u7b97\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5757\uff0c\u91c7\u7528\u5206\u5757\u91cd\u8ba1\u7b97\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5185\u5b58\u6a21\u578b\u52a8\u6001\u4f18\u5316\u4ee5\u5e73\u8861\u5185\u5b58\u6548\u7387\u548c\u541e\u5410\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMemFine\u76f8\u6bd4\u5b8c\u5168\u91cd\u8ba1\u7b97\u57fa\u7ebf\u51cf\u5c11\u6fc0\u6d3b\u5185\u5b5848.03%\uff0c\u63d0\u5347\u541e\u5410\u91cf4.42%\uff0c\u80fd\u591f\u5728\u5185\u5b58\u53d7\u9650\u7684GPU\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u7684\u5927\u89c4\u6a21MoE\u8bad\u7ec3\u3002", "conclusion": "MemFine\u6709\u6548\u89e3\u51b3\u4e86MoE\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5728\u6709\u9650\u5185\u5b58\u786c\u4ef6\u4e0a\u8bad\u7ec3\u5927\u89c4\u6a21MoE\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.21151", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21151", "abs": "https://arxiv.org/abs/2511.21151", "authors": ["M. Alecci", "P. Jim\u00e9nez", "J. Samhi", "T. Bissyand\u00e9", "J. Klein"], "title": "Exploring Hidden Geographic Disparities in Android Apps", "comment": null, "summary": "While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.\n  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.\n  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Android\u5e94\u7528\u5b58\u5728\u5730\u7406\u5dee\u5f02\u73b0\u8c61\uff1aGeoTwins\uff08\u529f\u80fd\u76f8\u4f3c\u4f46\u4e0d\u540c\u5730\u533a\u7684\u5e94\u7528\u53d8\u4f53\uff09\u548cApp Bundle\u57fa\u7840\u6587\u4ef6\u7684\u533a\u57df\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5f71\u54cd\u5b89\u5168\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u63a2\u7d22\u79fb\u52a8\u5e94\u7528\u884c\u4e3a\u7684\u5730\u7406\u5dee\u5f02\uff0c\u63ed\u793a\u73b0\u6709\u7814\u7a76\u4e2d\u88ab\u5ffd\u89c6\u7684\u533a\u57df\u6027\u5b89\u5168\u3001\u9690\u79c1\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5206\u5e03\u5f0f\u5e94\u7528\u6536\u96c6\u7ba1\u9053\uff0c\u8de8\u591a\u4e2a\u5730\u533a\u6536\u96c6\u5e76\u5206\u6790\u6570\u5343\u4e2a\u5e94\u7528\uff0c\u8bc6\u522bGeoTwins\u548cApp Bundle\u7684\u533a\u57df\u5dee\u5f02\u3002", "result": "\u53d1\u73b081,963\u4e2aGeoTwins\uff0c\u63ed\u793a\u4e86\u5e94\u7528\u6743\u9650\u3001\u7b2c\u4e09\u65b9\u5e93\u548c\u9690\u79c1\u58f0\u660e\u7684\u533a\u57df\u5dee\u5f02\uff0c\u4ee5\u53caApp Bundle\u57fa\u7840\u6587\u4ef6\u7684\u9690\u85cf\u5b9a\u5236\u5316\u3002", "conclusion": "\u79fb\u52a8\u8f6f\u4ef6\u5b58\u5728\u7cfb\u7edf\u6027\u533a\u57df\u5dee\u5f02\uff0c\u8fd9\u5bf9\u7814\u7a76\u4eba\u5458\u3001\u5f00\u53d1\u8005\u3001\u5e73\u53f0\u67b6\u6784\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u9700\u8981\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.21535", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21535", "abs": "https://arxiv.org/abs/2511.21535", "authors": ["Morteza Sadeghi"], "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation", "comment": null, "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u6570\u636e\u5197\u4f59\u6539\u5584MLFMA\u4e2dP2P\u7b97\u5b50\u7684GPU\u5185\u5b58\u5c40\u90e8\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u5c40\u90e8\u6027\u5ea6\u91cf\u7684\u5206\u6790\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u8d8b\u52bf\uff0c\u5728\u7535\u78c1\u6c42\u89e3\u5668\u548c\u6052\u661f\u52a8\u529b\u5b66\u4ee3\u7801\u4e2d\u9a8c\u8bc1\uff0c\u83b7\u5f97\u6700\u9ad87\u500d\u5185\u6838\u52a0\u901f\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u91cd\u7ec4\u5f00\u9500\uff0c\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f\u4ec5\u4e3a1.04\u500d\u3002", "motivation": "MLFMA\u4e2d\u7684\u8fd1\u573a(P2P)\u7b97\u5b50\u5728GPU\u4e0a\u56e0\u5185\u5b58\u5c40\u90e8\u6027\u5dee\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u6539\u5584\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5f15\u5165\u6570\u636e\u5197\u4f59\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5206\u6563\uff0c\u63d0\u51fa\u7ed3\u5408\u6570\u636e\u91cf\u548c\u8bbf\u95ee\u5206\u6563\u5ea6\u7684\u5c40\u90e8\u6027\u5ea6\u91cf\u5206\u6790\u6a21\u578b\uff0c\u5728DBIM-MLFMA\u7535\u78c1\u6c42\u89e3\u5668\u548cPhotoNs-2.0\u6052\u661f\u52a8\u529b\u5b66\u4ee3\u7801\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5185\u6838\u901f\u5ea6\u6700\u9ad8\u63d0\u53477\u500d\uff0c\u7f13\u5b58\u884c\u4e3a\u663e\u8457\u6539\u5584\uff0c\u4f46\u6570\u636e\u91cd\u7ec4\u5f00\u9500\u9650\u5236\u4e86\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f\u4ec5\u4e3a1.04\u500d\uff0c\u5206\u6790\u6a21\u578b\u80fd\u53ef\u9760\u6355\u6349\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u548c\u5bc6\u5ea6\u4e0b\u7684\u6027\u80fd\u8d8b\u52bf\u3002", "conclusion": "\u6570\u636e\u5197\u4f59\u53ef\u6709\u6548\u63d0\u5347GPU\u4e0aP2P\u7b97\u5b50\u6027\u80fd\uff0c\u524d\u63d0\u662f\u5c40\u90e8\u6027\u6536\u76ca\u8d85\u8fc7\u6570\u636e\u79fb\u52a8\u6210\u672c\uff0c\u8be5\u6280\u672f\u53ef\u6700\u5c0f\u5316\u4ee3\u7801\u4fee\u6539\u6ce8\u5165\u73b0\u6709\u5b9e\u73b0\u3002"}}
{"id": "2511.21197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.21197", "abs": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "comment": null, "summary": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "AI": {"tldr": "\u5f00\u53d1\u8005\u5c06AI\u8f85\u52a9bug\u68c0\u6d4b\u5de5\u5177\u89c6\u4e3a\"bug\u4fa6\u63a2\"\uff0c\u5c06\u53ef\u8bfb\u6027\u8bc4\u4f30\u5de5\u5177\u89c6\u4e3a\"\u8d28\u91cf\u6559\u7ec3\"\uff0c\u4fe1\u4efb\u53d6\u51b3\u4e8e\u89e3\u91ca\u6e05\u6670\u5ea6\u3001\u65f6\u673a\u548c\u7528\u6237\u63a7\u5236\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684IDE\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5c3d\u7ba1AI\u8f85\u52a9\u5de5\u5177\u5728\u6280\u672f\u7279\u6027\u4e0a\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u5f00\u53d1\u8005\u5982\u4f55\u5fc3\u7406\u5efa\u6a21\u8fd9\u4e9b\u5de5\u5177\u4ee5\u53ca\u4e0d\u5339\u914d\u5982\u4f55\u5f71\u54cd\u4fe1\u4efb\u3001\u63a7\u5236\u548c\u91c7\u7528\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u901a\u8fc76\u4e2a\u5171\u540c\u8bbe\u8ba1\u5de5\u4f5c\u574a\u4e0e58\u540d\u5f00\u53d1\u8005\u5408\u4f5c\uff0c\u5f15\u51fa\u4ed6\u4eec\u5bf9AI\u8f85\u52a9bug\u68c0\u6d4b\u548c\u53ef\u8bfb\u6027\u529f\u80fd\u7684\u5fc3\u7406\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u8005\u5c06bug\u68c0\u6d4b\u5de5\u5177\u89c6\u4e3a\u53ea\u8b66\u544a\u5173\u952e\u95ee\u9898\u7684bug\u4fa6\u63a2\uff0c\u8981\u6c42\u900f\u660e\u5ea6\u3001\u53ef\u64cd\u4f5c\u53cd\u9988\u548c\u4fe1\u5fc3\u63d0\u793a\uff1b\u53ef\u8bfb\u6027\u5de5\u5177\u5219\u88ab\u89c6\u4e3a\u63d0\u4f9b\u60c5\u5883\u5316\u3001\u4e2a\u6027\u5316\u548c\u6e10\u8fdb\u6307\u5bfc\u7684\u8d28\u91cf\u6559\u7ec3\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5e73\u8861\u5e72\u6270\u4e0e\u652f\u6301\u3001\u7b80\u6d01\u4e0e\u6df1\u5ea6\u3001\u81ea\u52a8\u5316\u4e0e\u4eba\u7c7b\u80fd\u52a8\u6027\u7684\u4eba\u7c7b\u4e2d\u5fc3AI\u5728IDE\u4e2d\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2511.21380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21380", "abs": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "comment": null, "summary": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "AI": {"tldr": "\u9996\u4e2a\u5173\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6570\u636e\u96c6\u9002\u5e94\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8eGPT-4.1\u548cClaude Sonnet 4\u7684Copilot\u5728\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u5de5\u4ef6\u9002\u5e94\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u5316\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u5de5\u4ef6\u7684\u8de8\u6570\u636e\u96c6\u9002\u5e94\u5bf9\u4e8e\u53ef\u6269\u5c55\u6027\u548c\u53ef\u590d\u73b0\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u8f83\u5c11\u3002\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u671b\u901a\u8fc7\u534f\u8c03\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u5de5\u5177\u4ea4\u4e92\u6765\u81ea\u52a8\u5316\u590d\u6742\u5f00\u53d1\u5de5\u4f5c\u6d41\u3002", "method": "\u4f7f\u7528\u4e94\u9636\u6bb5\u8bc4\u4f30\u6d41\u6c34\u7ebf\uff08\u6587\u4ef6\u7406\u89e3\u3001\u4ee3\u7801\u7f16\u8f91\u3001\u547d\u4ee4\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u6700\u7ec8\u6267\u884c\uff09\uff0c\u8bc4\u4f30Copilot\u5728ROCODE\u548cLogHub2.0\u7b49\u57fa\u51c6\u4ed3\u5e93\u4e2d\u7684\u8868\u73b0\uff0c\u6d4b\u91cf\u6210\u529f\u7387\uff0c\u5206\u6790\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u8bc4\u4f30\u63d0\u793a\u5e72\u9884\u7b56\u7565\u3002", "result": "\u5f53\u524d\u7cfb\u7edf\u80fd\u8bc6\u522b\u5173\u952e\u6587\u4ef6\u5e76\u751f\u6210\u90e8\u5206\u9002\u5e94\uff0c\u4f46\u5f88\u5c11\u4ea7\u751f\u529f\u80fd\u6b63\u786e\u7684\u5b9e\u73b0\u3002\u63d0\u793a\u5e72\u9884\uff08\u7279\u522b\u662f\u63d0\u4f9b\u6267\u884c\u9519\u8bef\u4fe1\u606f\u548c\u53c2\u8003\u4ee3\u7801\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u771f\u5b9e\u7ed3\u679c\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\uff08\u4ece7.25%\u523067.14%\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5728\u6570\u636e\u96c6\u9002\u5e94\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u81ea\u6821\u6b63\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5177\u4f53\u65b9\u5411\u3002"}}
{"id": "2511.21382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21382", "abs": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "comment": "33 pages, 8 figures", "summary": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "AI": {"tldr": "\u672c\u6587\u5bf9115\u7bc7\u5173\u4e8e\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7684\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6d4b\u8bd5\u751f\u6210\u751f\u547d\u5468\u671f\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u4e86\u6838\u5fc3\u751f\u6210\u7b56\u7565\u548c\u589e\u5f3a\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5316\u6d4b\u8bd5\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u6765\u751f\u6210\u771f\u5b9e\u8f93\u5165\u548c\u65ad\u8a00\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5229\u7528\u5176\u4ee3\u7801\u8bed\u4e49\u548c\u7f16\u7a0b\u6a21\u5f0f\u7684\u6570\u636e\u9a71\u52a8\u77e5\u8bc6\u6765\u5f25\u8865\u8fd9\u4e00\u5c40\u9650\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u67902021\u5e745\u6708\u81f32025\u5e748\u6708\u95f4\u7684115\u7bc7\u51fa\u7248\u7269\uff0c\u63d0\u51fa\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u751f\u547d\u5468\u671f\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63d0\u793a\u5de5\u7a0b\u5df2\u6210\u4e3a\u4e3b\u5bfc\u7b56\u7565\uff08\u536089%\u7684\u7814\u7a76\uff09\uff0c\u8fed\u4ee3\u9a8c\u8bc1\u548c\u4fee\u590d\u5faa\u73af\u6210\u4e3a\u786e\u4fdd\u9c81\u68d2\u6027\u7684\u6807\u51c6\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7f16\u8bd1\u548c\u6267\u884c\u901a\u8fc7\u7387\u3002", "conclusion": "\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u5e94\u671d\u7740\u81ea\u4e3b\u6d4b\u8bd5\u4ee3\u7406\u548c\u6df7\u5408\u7cfb\u7edf\u65b9\u5411\u53d1\u5c55\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u76f8\u7ed3\u5408\u3002"}}
