<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation](https://arxiv.org/abs/2510.06414)
*Abdur Rehman Anwar Qureshi,Adrian Rebmann,Timotheus Kampik,Matthias Weidlich,Mathias Weske*

Main category: cs.DB

TL;DR: 提出一种将命令式流程模型转换为可在关系数据库上执行的SQL查询的方法，用于一致性检查，弥合传统流程建模与数据驱动流程分析之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统命令式流程模型（如Petri网）难以直接应用于包含结构化流程执行数据的关系数据库，导致流程模型在数据驱动流程分析中未被充分利用。

Method: 开发一种将命令式流程模型转换为宽松流程数据查询（特别是SQL查询）的方法，用于在关系数据库上进行一致性检查。

Result: 展示了命令式流程模型在数据驱动流程管理中的持续相关性，以及行为足迹和其他声明式方法在整合基于模型和数据驱动流程管理中的重要性。

Conclusion: 该方法成功弥合了传统流程建模与数据驱动流程分析之间的差距，证明了命令式流程模型在数据驱动流程管理中的价值。

Abstract: Business process management is increasingly practiced using data-driven
approaches. Still, classical imperative process models, which are typically
formalized using Petri nets, are not straightforwardly applicable to the
relational databases that contain much of the available structured process
execution data. This creates a gap between the traditional world of process
modeling and recent developments around data-driven process analysis,
ultimately leading to the under-utilization of often readily available process
models. In this paper, we close this gap by providing an approach for
translating imperative models into relaxed process data queries, specifically
SQL queries executable on relational databases, for conformance checking. Our
results show the continued relevance of imperative process models to
data-driven process management, as well as the importance of behavioral
footprints and other declarative approaches for integrating model-based and
data-driven process management.

</details>


### [2] [Automated Discovery of Test Oracles for Database Management Systems Using LLMs](https://arxiv.org/abs/2510.06663)
*Qiuyang Mang,Runyuan He,Suyang Zhong,Xiaoxuan Liu,Huanchen Zhang,Alvin Cheung*

Main category: cs.DB

TL;DR: Argus是一个基于LLM的自动化DBMS测试框架，通过约束抽象查询生成等效查询对来发现数据库系统中的逻辑错误。


<details>
  <summary>Details</summary>
Motivation: 传统数据库测试中的测试预言设计需要人工参与，限制了自动化测试的完全自动化。LLMs虽然具有创造力，但存在幻觉问题且成本高昂，需要一种方法来约束其使用。

Method: 使用约束抽象查询（包含占位符和实例化条件的SQL骨架），通过LLM生成语义等效的骨架对，用SQL等价求解器进行形式化验证，最后用LLM合成具体SQL片段实例化。

Result: 在5个广泛测试的DBMS中发现了40个未知错误，其中35个是逻辑错误，36个已确认，26个已修复。

Conclusion: Argus成功实现了自动化测试预言的发现和实例化，显著提升了DBMS测试的自动化水平，证明了LLM在数据库测试中的有效性。

Abstract: Since 2020, automated testing for Database Management Systems (DBMSs) has
flourished, uncovering hundreds of bugs in widely-used systems. A cornerstone
of these techniques is test oracle, which typically implements a mechanism to
generate equivalent query pairs, thereby identifying bugs by checking the
consistency between their results. However, while applying these oracles can be
automated, their design remains a fundamentally manual endeavor. This paper
explores the use of large language models (LLMs) to automate the discovery and
instantiation of test oracles, addressing a long-standing bottleneck towards
fully automated DBMS testing. Although LLMs demonstrate impressive creativity,
they are prone to hallucinations that can produce numerous false positive bug
reports. Furthermore, their significant monetary cost and latency mean that LLM
invocations should be limited to ensure that bug detection is efficient and
economical.
  To this end, we introduce Argus, a novel framework built upon the core
concept of the Constrained Abstract Query - a SQL skeleton containing
placeholders and their associated instantiation conditions (e.g., requiring a
placeholder to be filled by a boolean column). Argus uses LLMs to generate
pairs of these skeletons that are asserted to be semantically equivalent. This
equivalence is then formally proven using a SQL equivalence solver to ensure
soundness. Finally, the placeholders within the verified skeletons are
instantiated with concrete, reusable SQL snippets that are also synthesized by
LLMs to efficiently produce complex test cases. We implemented Argus and
evaluated it on five extensively tested DBMSs, discovering 40 previously
unknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already
fixed by the developers.

</details>


### [3] [Relational Database Distillation: From Structured Tables to Condensed Graph Data](https://arxiv.org/abs/2510.06980)
*Xinyi Gao,Jingxi Zhang,Lijian Chen,Tong Chen,Lizhen Cui,Hongzhi Yin*

Main category: cs.DB

TL;DR: 提出关系数据库蒸馏(RDD)方法，将大规模关系数据库压缩为紧凑的异构图，同时保持预测能力，解决现有图表示学习方法存储开销大和训练时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 关系数据库中的图表示学习方法虽然性能优异，但由于数据库规模庞大和跨表消息传递的计算负担，存在存储开销过大和训练时间过长的问题。

Method: 通过将多模态列信息保存为节点特征，主外键关系编码为异质边，保持数据保真度和关系结构。设计基于核岭回归和伪标签的目标函数，避免传统双层蒸馏框架的低效问题。

Result: 在多个真实世界关系数据库上的实验表明，该方法显著减小数据规模，同时在分类和回归任务上保持竞争力的性能。

Conclusion: 关系数据库蒸馏为关系数据库的可扩展学习提供了有效途径，能够在保持性能的同时大幅降低存储和计算成本。

Abstract: Relational databases (RDBs) underpin the majority of global data management
systems, where information is structured into multiple interdependent tables.
To effectively use the knowledge within RDBs for predictive tasks, recent
advances leverage graph representation learning to capture complex inter-table
relations as multi-hop dependencies. Despite achieving state-of-the-art
performance, these methods remain hindered by the prohibitive storage overhead
and excessive training time, due to the massive scale of the database and the
computational burden of intensive message passing across interconnected tables.
To alleviate these concerns, we propose and study the problem of Relational
Database Distillation (RDD). Specifically, we aim to distill large-scale RDBs
into compact heterogeneous graphs while retaining the predictive power (i.e.,
utility) required for training graph-based models. Multi-modal column
information is preserved through node features, and primary-foreign key
relations are encoded via heterogeneous edges, thereby maintaining both data
fidelity and relational structure. To ensure adaptability across diverse
downstream tasks without engaging the traditional, inefficient bi-level
distillation framework, we further design a kernel ridge regression-guided
objective with pseudo-labels, which produces quality features for the distilled
graph. Extensive experiments on multiple real-world RDBs demonstrate that our
solution substantially reduces the data size while maintaining competitive
performance on classification and regression tasks, creating an effective
pathway for scalable learning with RDBs.

</details>


### [4] [On the Expressiveness of Languages for Querying Property Graphs in Relational Databases](https://arxiv.org/abs/2510.07062)
*Hadar Rotschield,Liat Peterfreund*

Main category: cs.DB

TL;DR: SQL/PGQ标准在属性图查询中的表达能力分析，揭示了图创建在表达能力中的核心作用，形成了严格的层次结构，最终覆盖所有NL查询。


<details>
  <summary>Details</summary>
Motivation: 研究SQL/PGQ标准在属性图查询中的表达能力，特别是图创建操作对表达能力的影响，以及不同片段之间的层次关系。

Method: 通过形式化分析SQL/PGQ的三个片段：只读核心、读写扩展以及具有更丰富视图定义的扩展变体，比较它们的表达能力。

Result: 只读片段严格弱于读写片段，读写片段仍低于NL复杂度类。扩展视图定义后，扩展片段恰好捕获NL，形成了严格的层次结构。在有序结构上，该层次结构会坍缩。

Conclusion: 图创建在属性图查询中扮演核心角色，视图构造是决定表达能力的关键因素，SQL/PGQ片段形成了严格的表达能力层次，其并集覆盖所有NL查询。

Abstract: SQL/PGQ is the emerging ISO standard for querying property graphs defined as
views over relational data. We formalize its expressive power across three
fragments: the read-only core, the read-write extension, and an extended
variant with richer view definitions. Our results show that graph creation
plays a central role in determining the expressiveness. The read-only fragment
is strictly weaker than the read-write fragment, and the latter is still below
the complexity class NL. Extending view definitions with arbitrary arity
identifiers closes this gap: the extended fragment captures exactly NL. This
yields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL
queries. On ordered structures the hierarchy collapses: once arity-2
identifiers are allowed, higher arities add no power, mirroring the classical
transitive-closure collapse and underscoring the central role of view
construction in property graph querying.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems](https://arxiv.org/abs/2510.06343)
*Fikret Mert Gültekin,Oscar Lilja,Ranim Khojah,Rebekka Wohlrab,Marvin Damschen,Mazen Mohamad*

Main category: cs.SE

TL;DR: 本文探讨了在林业领域使用本地部署的大型语言模型（LLMs）结合检索增强生成技术来支持网络安全风险评估，同时满足数据保护要求。


<details>
  <summary>Details</summary>
Motivation: 在安全关键软件系统中，网络安全活动至关重要，但网络安全专家通常稀缺，导致工作负荷高，需要为工程师提供支持工具。

Method: 采用设计科学研究方法，通过访谈、互动会议和问卷调查，涉及12位专家参与大型项目研究。

Result: LLMs能够协助网络安全专家生成初步风险评估、识别威胁并进行冗余检查，但需要人工监督确保准确性和合规性。

Conclusion: 尽管存在信任问题，专家愿意在特定评估和辅助角色中使用LLMs，而非完全依赖其生成能力，这鼓励在安全关键领域使用基于LLM的代理支持风险评估过程。

Abstract: In safety-critical software systems, cybersecurity activities become
essential, with risk assessment being one of the most critical. In many
software teams, cybersecurity experts are either entirely absent or represented
by only a small number of specialists. As a result, the workload for these
experts becomes high, and software engineers would need to conduct
cybersecurity activities themselves. This creates a need for a tool to support
cybersecurity experts and engineers in evaluating vulnerabilities and threats
during the risk assessment process. This paper explores the potential of
leveraging locally hosted large language models (LLMs) with retrieval-augmented
generation to support cybersecurity risk assessment in the forestry domain
while complying with data protection and privacy requirements that limit
external data sharing. We performed a design science study involving 12 experts
in interviews, interactive sessions, and a survey within a large-scale project.
The results demonstrate that LLMs can assist cybersecurity experts by
generating initial risk assessments, identifying threats, and providing
redundancy checks. The results also highlight the necessity for human oversight
to ensure accuracy and compliance. Despite trust concerns, experts were willing
to utilize LLMs in specific evaluation and assistance roles, rather than solely
relying on their generative capabilities. This study provides insights that
encourage the use of LLM-based agents to support the risk assessment process of
cyber-physical systems in safety-critical domains.

</details>


### [6] [Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study](https://arxiv.org/abs/2510.06363)
*Ololade Babatunde,Tomisin Ayodabo,Raqibul Raqibul*

Main category: cs.SE

TL;DR: 该研究通过引入和评估定制的基于Git的提交系统，解决了高等教育中传统作业提交方法的挑战，显著提升了作业跟踪、协作和提交效率。


<details>
  <summary>Details</summary>
Motivation: 解决高等教育中传统作业提交方法在跟踪、协作和效率方面的挑战，探索分布式版本控制在教育环境中的应用价值。

Method: 采用迭代软件开发和人本设计方法，在真实大学环境中集成定制Git系统，通过可用性测试和学生反馈进行实证评估。

Result: 85%的教师认为Git系统更易用，84%的学生偏好该系统；提交和评审时间减少38%，存储需求降低48%；学生在分布式版本控制工作流中获得积极体验。

Conclusion: 基于Git的提交系统为教育环境中集成分布式版本控制提供了实用见解，有效提升了教师监督和学生参与度，特别是在软件工程及相关学科中。

Abstract: This study addresses challenges in traditional assignment submission methods
used in higher education by introducing and evaluating a customized Git-based
submission system. Employing iterative software development and user-centered
design methodologies, the system was integrated within a real-world university
environment. Empirical evaluation, including usability testing and student
feedback, indicated significant improvements in assignment tracking,
collaboration, and submission efficiency. Students reported positive
experiences using distributed version control workflows, highlighting improved
learning outcomes and reduced administrative burden. Challenges related to
initial adoption and student learning curves were identified and mitigated
through iterative improvements. The proposed system contributes practical
insights for integrating distributed version control into educational settings,
enhancing both instructor oversight and student engagement in software
engineering and related disciplines. Based on our results, the research showed
that 85% of instructors found the git based system easier to use, with 84% of
students preferring it over traditional methods, as it provides a 38% reduction
in time taken for submission and review, while also leading to a 48% reduction
in storage requirements.

</details>


### [7] [Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review](https://arxiv.org/abs/2510.06483)
*Judith Michael,Lukas Netz,Bernhard Rumpe,Ingo Müller,John Grundy,Shavindra Wickramathilaka,Hourieh Khalajzadeh*

Main category: cs.SE

TL;DR: 对30篇关于模型驱动工程（MDE）如何解决视觉障碍可访问性的研究进行系统文献综述，发现当前MDE研究在支持视觉相关可访问性方面存在不足，缺乏具体建模技术、功能完整的系统演示，以及实证验证薄弱。


<details>
  <summary>Details</summary>
Motivation: 软件应用经常为有可访问性需求的用户（如视觉障碍者）设置障碍。模型驱动工程（MDE）通过系统化的代码推导，提供了将可访问性问题整合到软件开发中的系统方法，同时减少人工工作量。

Method: 进行系统文献综述，从447篇初步确定的论文中筛选出30篇符合纳入标准的主要研究进行分析。

Result: 约三分之二的研究参考了Web内容可访问性指南（WCAG），但项目特定的适应性和最终用户验证阻碍了在MDE中的更广泛采用。分析的研究主要建模用户界面结构、交互和导航、用户能力、需求和上下文信息，但只有少数研究具体说明了如何整合可访问性需求的具体建模技术或展示了功能完整的系统。

Conclusion: 当前MDE研究在支持视觉相关可访问性方面不足，需要更有效地将视觉障碍支持嵌入MDE过程，并提出了研究议程。

Abstract: Software applications often pose barriers for users with accessibility needs,
e.g., visual impairments. Model-driven engineering (MDE), with its systematic
nature of code derivation, offers systematic methods to integrate accessibility
concerns into software development while reducing manual effort. This paper
presents a systematic literature review on how MDE addresses accessibility for
vision impairments. From 447 initially identified papers, 30 primary studies
met the inclusion criteria. About two-thirds reference the Web Content
Accessibility Guidelines (WCAG), yet their project-specific adaptions and
end-user validations hinder wider adoption in MDE. The analyzed studies model
user interface structures, interaction and navigation, user capabilities,
requirements, and context information. However, only few specify concrete
modeling techniques on how to incorporate accessibility needs or demonstrate
fully functional systems. Insufficient details on MDE methods, i.e.,
transformation rules or code templates, hinder the reuse, generalizability, and
reproducibility. Furthermore, limited involvement of affected users and limited
developer expertise in accessibility contribute to weak empirical validation.
Overall, the findings indicate that current MDE research insufficiently
supports vision-related accessibility. Our paper concludes with a research
agenda outlining how support for vision impairments can be more effectively
embedded in MDE processes.

</details>


### [8] [Beyond More Context: How Granularity and Order Drive Code Completion Quality](https://arxiv.org/abs/2510.06606)
*Uswat Yusuf,Genevieve Caumartin,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文提出了一种基于静态分析的代码块检索方法，用于改进代码补全中的上下文收集策略，在ASE 2025挑战赛中相比文件级检索提升了6%的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码补全任务中需要充分且相关的上下文信息，但在大型代码库中面临两个挑战：LLM上下文长度限制无法包含所有文件，以及生成代码质量对噪声或无关上下文高度敏感。

Method: 开发并评估了文件和代码块级别的检索策略，重点研究了上下文大小和文件排序对LLM性能的影响，引入了基于静态分析的代码块检索方法。

Result: 上下文的数量和顺序显著影响模型性能，基于静态分析的代码块检索相比最佳文件检索策略提升了6%，相比无上下文基线在Python任务中提升了16%。

Conclusion: 检索粒度、排序策略和混合方法对于为实际开发场景构建有效的上下文收集管道至关重要。

Abstract: Context plays an important role in the quality of code completion, as Large
Language Models (LLMs) require sufficient and relevant information to assist
developers in code generation tasks. However, composing a relevant context for
code completion poses challenges in large repositories: First, the limited
context length of LLMs makes it impractical to include all repository files.
Second, the quality of generated code is highly sensitive to noisy or
irrelevant context. In this paper, we present our approach for the ASE 2025
Context Collection Challenge. The challenge entails outperforming JetBrains
baselines by designing effective retrieval and context collection strategies.
We develop and evaluate a series of experiments that involve retrieval
strategies at both the file and chunk levels. We focus our initial experiments
on examining the impact of context size and file ordering on LLM performance.
Our results show that the amount and order of context can significantly
influence the performance of the models. We introduce chunk-based retrieval
using static analysis, achieving a 6% improvement over our best file-retrieval
strategy and a 16% improvement over the no-context baseline for Python in the
initial phase of the competition. Our results highlight the importance of
retrieval granularity, ordering and hybrid strategies in developing effective
context collection pipelines for real-world development scenarios.

</details>


### [9] [AISysRev -- LLM-based Tool for Title-abstract Screening](https://arxiv.org/abs/2510.06708)
*Aleksi Huotala,Miikka Kuutila,Olli-Pekka Turtio,Mika Mäntylä*

Main category: cs.SE

TL;DR: 开发了AiSysRev工具，利用LLM辅助系统综述中的文献筛选，通过实验发现LLM能有效处理简单案例，但边界案例仍需人工干预。


<details>
  <summary>Details</summary>
Motivation: 系统综述中的文献筛选工作量大且耗时，需要自动化工具来减轻人工负担，特别是在快速综述中。

Method: 开发基于LLM的AiSysRev网络应用，支持零样本和少样本筛选，通过OpenRouter接入多种LLM，提供人工筛选界面作为参考。

Result: 在137篇论文的试验中，发现论文可分为四类：简单包含、简单排除、边界包含和边界排除，LLM在边界案例中容易出错。

Conclusion: LLM不能完全替代人工判断，但能显著减轻大规模科学文献评估的负担，特别是在简单案例筛选方面。

Abstract: Systematic reviews are a standard practice for summarizing the state of
evidence in software engineering. Conducting systematic reviews is laborious,
especially during the screening or study selection phase, where the number of
papers can be overwhelming. During this phase, papers are assessed against
inclusion and exclusion criteria based on their titles and abstracts. Recent
research has demonstrated that large language models (LLMs) can perform
title-abstract screening at a level comparable to that of a master's student.
While LLMs cannot be fully trusted, they can help, for example, in Rapid
Reviews, which try to expedite the review process. Building on recent research,
we developed AiSysRev, an LLM-based screening tool implemented as a web
application running in a Docker container. The tool accepts a CSV file
containing paper titles and abstracts. Users specify inclusion and exclusion
criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev
supports both zero-shot and few-shot screening, and also allows for manual
screening through interfaces that display LLM results as guidance for human
reviewers.We conducted a trial study with 137 papers using the tool. Our
findings indicate that papers can be classified into four categories: Easy
Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary
cases, where LLMs are prone to errors, highlight the need for human
intervention. While LLMs do not replace human judgment in systematic reviews,
they can significantly reduce the burden of assessing large volumes of
scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:
https://github.com/EvoTestOps/AISysRev

</details>


### [10] [LLM Company Policies and Policy Implications in Software Organizations](https://arxiv.org/abs/2510.06718)
*Ranim Khojah,Mazen Mohamad,Linda Erlenhov,Francisco Gomes de Oliveira Neto,Philipp Leitner*

Main category: cs.SE

TL;DR: 研究11家公司如何制定LLM聊天机器人使用政策及其影响因素，帮助管理者安全地将聊天机器人整合到开发工作流程中。


<details>
  <summary>Details</summary>
Motivation: 软件组织采用大型语言模型聊天机器人存在风险，需要制定清晰的政策来确保安全使用。

Method: 研究11家公司制定聊天机器人使用政策的过程和影响因素。

Result: 识别了影响公司制定LLM聊天机器人政策的关键因素。

Conclusion: 研究结果有助于管理者在开发工作流程中安全地集成聊天机器人。

Abstract: The risks associated with adopting large language model (LLM) chatbots in
software organizations highlight the need for clear policies. We examine how 11
companies create these policies and the factors that influence them, aiming to
help managers safely integrate chatbots into development workflows.

</details>


### [11] [Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis](https://arxiv.org/abs/2510.06844)
*Nicole Hoess,Carlos Paradis,Rick Kazman,Wolfgang Mauerer*

Main category: cs.SE

TL;DR: 该研究通过复制三个软件演化分析研究，使用四种独立挖掘工具比较数据提取、分析结果和结论，发现工具设计和实现中的技术细节会导致显著差异，影响研究有效性。


<details>
  <summary>Details</summary>
Motivation: 研究软件仓库挖掘工具在复杂分析管道中的有效性威胁，评估不同工具对相同研究问题在数据、结果和结论上的一致性。

Method: 通过轻量级文献综述选择三个关于协作协调、软件维护和软件质量的研究，使用四种系统选择的独立挖掘工具进行正式复制，定量和定性比较提取的数据、分析结果和结论。

Result: 发现工具设计和实现中的众多技术细节在复杂挖掘管道中累积，可能导致提取的基线数据、其衍生数据、后续统计分析结果以及特定情况下的结论存在显著差异。

Conclusion: 用户必须仔细选择工具并评估其局限性，以充分评估有效性范围。建议重用工具，研究人员和工具作者可通过复制包和比较研究促进可重用性并减少不确定性。

Abstract: Context: Mining software repositories is a popular means to gain insights
into a software project's evolution, monitor project health, support decisions
and derive best practices. Tools supporting the mining process are commonly
applied by researchers and practitioners, but their limitations and agreement
are often not well understood.
  Objective: This study investigates some threats to validity in complex tool
pipelines for evolutionary software analyses and evaluates the tools' agreement
in terms of data, study outcomes and conclusions for the same research
questions.
  Method: We conduct a lightweight literature review to select three studies on
collaboration and coordination, software maintenance and software quality from
high-ranked venues, which we formally replicate with four independent,
systematically selected mining tools to quantitatively and qualitatively
compare the extracted data, analysis results and conclusions.
  Results: We find that numerous technical details in tool design and
implementation accumulate along the complex mining pipelines and can cause
substantial differences in the extracted baseline data, its derivatives,
subsequent results of statistical analyses and, under specific circumstances,
conclusions.
  Conclusions: Users must carefully choose tools and evaluate their limitations
to assess the scope of validity in an adequate way. Reusing tools is
recommended. Researchers and tool authors can promote reusability and help
reducing uncertainties by reproduction packages and comparative studies
following our approach.

</details>


### [12] [An empirical study on declined proposals: why are these proposals declined?](https://arxiv.org/abs/2510.06984)
*Masanari Kondo,Mahmoud Alfadel,Shane McIntosh,Yasutaka Kamei,Naoyasu Ubayashi*

Main category: cs.SE

TL;DR: 该研究分析了Go编程语言项目的提案系统，发现提案被拒绝的比例高于接受，且处理周期长。通过分析1091个提案，建立了9种拒绝原因的分类法，并证明GPT模型可以在讨论早期预测提案结果。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目的提案过程资源密集且常导致贡献者挫败感，特别是当提案被拒绝时缺乏明确反馈。目前对提案拒绝原因的理解有限，限制了流程优化和有效指导贡献者的机会。

Method: 对Go项目的1091个提案进行混合方法实证研究，量化提案结果，建立拒绝原因分类法，并评估大型语言模型预测这些结果的能力。

Result: 提案被拒绝的比例高于接受，平均解决时间超过一个月。仅14.7%的被拒绝提案会重新提交。确定了9个主要拒绝原因，包括重复、用例有限或违反项目原则等。GPT模型在部分评论情况下可预测拒绝决策（F1分数=0.71）。

Conclusion: 研究揭示了提案过程的低效性，并提供了通过早期筛选和基于过去拒绝原因的结构化理解来改善贡献者体验和审阅者工作负载的可操作机会。

Abstract: Design-level decisions in open-source software (OSS) projects are often made
through structured mechanisms such as proposals, which require substantial
community discussion and review. Despite their importance, the proposal process
is resource-intensive and often leads to contributor frustration, especially
when proposals are declined without clear feedback. Yet, the reasons behind
proposal rejection remain poorly understood, limiting opportunities to
streamline the process or guide contributors effectively. This study
investigates the characteristics and outcomes of proposals in the Go
programming language to understand why proposals are declined and how such
outcomes might be anticipated. We conduct a mixed-method empirical study on
1,091 proposals submitted to the Go project. We quantify proposal outcomes,
build a taxonomy of decline reasons, and evaluate large language models (LLMs)
for predicting these outcomes. We find that proposals are more often declined
than accepted, and resolution typically takes over a month. Only 14.7% of
declined proposals are ever resubmitted. Through qualitative coding, we
identify nine key reasons for proposal decline, such as duplication, limited
use cases, or violations of project principles. This taxonomy can help
contributors address issues in advance, e.g., checking for existing
alternatives can reduce redundancy. We also demonstrate that GPT-based models
can predict decline decisions early in the discussion (F1 score = 0.71 with
partial comments), offering a practical tool for prioritizing review effort.
Our findings reveal inefficiencies in the proposal process and highlight
actionable opportunities for improving both contributor experience and reviewer
workload by enabling early triage and guiding contributors to strengthen their
proposals using a structured understanding of past decline reasons.

</details>


### [13] [Human-aligned AI Model Cards with Weighted Hierarchy Architecture](https://arxiv.org/abs/2510.06989)
*Pengyue Yang,Haolin Jin,Qingwen Zeng,Jiawen Wen,Harry Rao,Huaming Chen*

Main category: cs.SE

TL;DR: 提出了CRAI-MCF框架，从静态披露转向可操作、人类对齐的文档，解决大语言模型发现和采用中的挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生态快速发展，但模型发现和采用面临挑战，现有文档框架存在静态、定性为主、缺乏定量比较机制等问题。

Method: 基于价值敏感设计，分析240个开源项目提炼217个参数，构建八模块价值对齐架构，引入定量充分性标准。

Result: 开发了CRAI-MCF框架，平衡技术、伦理和操作维度，支持严格的跨模型比较。

Conclusion: CRAI-MCF框架使从业者能够更有效地评估、选择和采用大语言模型，提高信心和操作完整性。

Abstract: The proliferation of Large Language Models (LLMs) has led to a burgeoning
ecosystem of specialized, domain-specific models. While this rapid growth
accelerates innovation, it has simultaneously created significant challenges in
model discovery and adoption. Users struggle to navigate this landscape due to
inconsistent, incomplete, and imbalanced documentation across platforms.
Existing documentation frameworks, such as Model Cards and FactSheets, attempt
to standardize reporting but are often static, predominantly qualitative, and
lack the quantitative mechanisms needed for rigorous cross-model comparison.
This gap exacerbates model underutilization and hinders responsible adoption.
To address these shortcomings, we introduce the Comprehensive Responsible AI
Model Card Framework (CRAI-MCF), a novel approach that transitions from static
disclosures to actionable, human-aligned documentation. Grounded in Value
Sensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240
open-source projects, distilling 217 parameters into an eight-module,
value-aligned architecture. Our framework introduces a quantitative sufficiency
criterion to operationalize evaluation and enables rigorous cross-model
comparison under a unified scheme. By balancing technical, ethical, and
operational dimensions, CRAI-MCF empowers practitioners to efficiently assess,
select, and adopt LLMs with greater confidence and operational integrity.

</details>


### [14] [Building an Open AIBOM Standard in the Wild](https://arxiv.org/abs/2510.07070)
*Gopi Krishnan Rajbahadur,Keheliya Gallaba,Elyas Rashno,Arthit Suriyawongkul,Karen Bennet,Kate Stewart,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本文介绍了AI物料清单(AIBOM)规范的开发经验，这是SPDX软件物料清单标准的AI扩展，用于捕获AI组件如数据集和训练工件。通过行动研究方法，记录了全球多利益相关方参与的过程，并通过法规对齐、用例映射、访谈和案例研究进行验证。


<details>
  <summary>Details</summary>
Motivation: 现代软件工程日益依赖开放社区驱动的标准，但在AI系统等快速发展的领域中，此类标准的创建过程仍未被充分探索。需要为AI组件制定标准化的物料清单规范。

Method: 采用行动研究(AR)方法，组织全球多利益相关方(超过90名贡献者)参与结构化AR周期，开发AIBOM规范。通过四种互补方法进行验证：法规标准对齐、行业用例映射、半结构化从业者访谈和工业案例研究。

Result: 成功开发了经过验证的AIBOM规范，该规范与主要法规(如欧盟AI法案)和伦理标准(如IEEE 7000)保持一致，并映射到六个行业用例。规范捕获了AI组件如数据集和迭代训练工件。

Conclusion: 除了交付经过验证的AIBOM规范外，本文还记录了在真实环境中构建规范的过程，反思了其与AR周期的对齐，并提炼了可为软件工程社区未来标准化工作提供参考的经验教训。

Abstract: Modern software engineering increasingly relies on open, community-driven
standards, yet how such standards are created in fast-evolving domains like
AI-powered systems remains underexplored. This paper presents a detailed
experience report on the development of the AI Bill of Materials AIBOM
specification, an extension of the ISO/IEC 5962:2021 Software Package Data
Exchange (SPDX) software bill of materials (SBOM) standard, which captures AI
components such as datasets and iterative training artifacts. Framed through
the lens of Action Research (AR), we document a global, multi-stakeholder
effort involving over 90 contributors and structured AR cycles. The resulting
specification was validated through four complementary approaches: alignment
with major regulations and ethical standards (e.g., EU AI Act and IEEE 7000
standards), systematic mapping to six industry use cases, semi-structured
practitioner interviews, and an industrial case study. Beyond delivering a
validated artefact, our paper documents the process of building the AIBOM
specification in the wild, and reflects on how it aligns with the AR cycle, and
distills lessons that can inform future standardization efforts in the software
engineering community.

</details>


### [15] [Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe](https://arxiv.org/abs/2510.07189)
*Junjie Li,Fazle Rabbi,Bo Yang,Song Wang,Jinqiu Yang*

Main category: cs.SE

TL;DR: Secure-Instruct是一个自动合成高质量漏洞和安全代码示例的框架，通过指令微调提升LLM的安全代码生成能力，在安全性和功能性方面都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成的代码存在安全隐患，现有方法（如SafeCoder）受限于有限且不平衡的数据集，影响其有效性和泛化能力。

Method: 提出Secure-Instruct框架，自动合成漏洞和安全代码示例，生成微调指令，并对LLM进行指令微调，使任务描述与安全代码生成能力对齐。

Result: 在CWEBench上，Secure-Instruct使安全代码生成显著提升，平均安全比率比预训练模型提高14.3%，比SafeCoder高出7.6%。在CWEval上，Func-Sec@1指标分别比预训练模型提高14%（CodeLlama-7B）和5.8%（Mistral-7B），比SafeCoder分别高出15.8%和6.8%。

Conclusion: Secure-Instruct不仅能提高生成代码的安全性，还能提升其功能性正确性，为安全代码生成提供了有效的解决方案。

Abstract: Although Large Language Models (LLMs) show promising solutions to automated
code generation, they often produce insecure code that threatens software
security. Current approaches (e.g., SafeCoder) to improve secure code
generation suffer from limited and imbalanced datasets, reducing their
effectiveness and generalizability. In this work, we present Secure-Instruct, a
novel framework that automatically synthesizes high-quality vulnerable and
secure code examples, generates fine-tuning instructions, and instruction-tunes
LLMs to align task description and secure code generation abilities. We
evaluate Secure-Instruct on four representative LLMs using two benchmarks: our
own CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44
CWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning
dataset, while CWEval covers 31 CWEs with 119 manually verified
security-critical tasks. We find that Secure-Instruct improves not only the
security but also the functional correctness of the generated code. On
CWEBench, Secure-Instruct substantially improves secure code generation, giving
a 14.3% average increase in secure ratio over the pretrained models and
outperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%
increase for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained
models, and surpasses SafeCoder by 15.8% and 6.8% respectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [DiLi: A Lock-Free Asynchronously Distributable Linked List](https://arxiv.org/abs/2510.06387)
*Raaghav Ravishankar,Sandeep Kulkarni,Sathya Peri,Gokarna Sharma*

Main category: cs.DC

TL;DR: DiLi是一个条件无锁、可线性化、可分布式的链表数据结构，支持异步动态分区和负载均衡，在多机环境下吞吐量线性扩展。


<details>
  <summary>Details</summary>
Motivation: 解决现代数据库在单机吞吐量不足时，需要分布式数据结构来避免硬件更换的停机时间，同时克服静态分区带来的负载不均问题。

Method: 提出条件无锁概念，扩展无锁计算以包含进程间通信假设；设计DiLi分布式链表，支持动态分区和负载均衡；采用二分查找分区方案加有限线性遍历的搜索方法。

Result: DiLi在单机性能与最先进的无锁并发搜索结构相当，在多机环境下吞吐量随机器数量线性扩展。

Conclusion: DiLi成功实现了可动态分区和负载均衡的分布式无锁数据结构，在保持单机性能的同时实现了多机线性扩展。

Abstract: Modern databases use dynamic search structures that store a huge amount of
data, and often serve them using multi-threaded algorithms to support the
ever-increasing throughput needs. When this throughput need exceeds the
capacity of the machine hosting the structure, one either needs to replace the
underlying hardware (an option that is typically not viable and introduces a
long down time) or make the data structure distributed. Static partitioning of
the data structure for distribution is not desirable, as it is prone to uneven
load distribution over time, and having to change the partitioning scheme later
will require downtime.
  Since a distributed data structure, inherently, relies on communication
support from the network stack and operating systems, we introduce the notion
of conditional lock-freedom that extends the notion of lock-free computation
with reasonable assumptions about communication between processes. We present
DiLi, a conditional lock-free, linearizable, and distributable linked list that
can be asynchronously and dynamically (1) partitioned into multiple sublists
and (2) load balanced by distributing sublists across multiple machines. DiLi
contains primitives for these that also maintain the lock-free property of the
underlying search structure that supports find, remove, and insert of a key as
the client operations.
  Searching for an item in DiLi is by a novel traversal that involves a binary
search on the partitioning scheme, and then a linear traversal on a limitable
number of linked nodes. As a result, we are able to empirically show that DiLi
performs as well as the state-of-the-art lock-free concurrent search structures
that are based off of a linked list when executed on a single-machine. We also
show that the throughput of DiLi scales linearly with the number of machines
that host it.

</details>


### [17] [Adaptive Protein Design Protocols and Middleware](https://arxiv.org/abs/2510.06396)
*Aymen Alsaadi,Jonathan Ash,Mikhail Titov,Matteo Turilli,Andre Merzky,Shantenu Jha,Sagar Khare*

Main category: cs.DC

TL;DR: IMPRESS是一个集成机器学习和高性能计算的蛋白质设计平台，通过动态资源分配和异步工作负载执行，提高蛋白质设计质量和效率。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列和结构空间极其庞大，传统计算方法需要大量计算资源进行采样，难以实现生成结构与预测结构之间的收敛。

Method: 开发IMPRESS平台，将AI与高性能计算任务耦合，实现自适应蛋白质设计协议和支持计算基础设施。

Result: 提高了蛋白质设计质量的一致性，并由于动态资源分配和异步工作负载执行而增强了蛋白质设计的吞吐量。

Conclusion: IMPRESS通过集成AI/ML与高性能计算，有效解决了蛋白质设计中的计算资源挑战，提升了设计效率和可靠性。

Abstract: Computational protein design is experiencing a transformation driven by
AI/ML. However, the range of potential protein sequences and structures is
astronomically vast, even for moderately sized proteins. Hence, achieving
convergence between generated and predicted structures demands substantial
computational resources for sampling. The Integrated Machine-learning for
Protein Structures at Scale (IMPRESS) offers methods and advanced computing
systems for coupling AI to high-performance computing tasks, enabling the
ability to evaluate the effectiveness of protein designs as they are developed,
as well as the models and simulations used to generate data and train models.
This paper introduces IMPRESS and demonstrates the development and
implementation of an adaptive protein design protocol and its supporting
computing infrastructure. This leads to increased consistency in the quality of
protein design and enhanced throughput of protein design due to dynamic
resource allocation and asynchronous workload execution.

</details>


### [18] [MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases](https://arxiv.org/abs/2510.06404)
*Raaghav Ravishankar,Sandeep Kulkarni,Nitin H Vaidya*

Main category: cs.DC

TL;DR: 提出了分布式事务一致性快照(DTCS)算法，用于弱一致性分布式数据库的检查点，以最小开销(O(n)消息和单个计数器)确保强一致性快照。


<details>
  <summary>Details</summary>
Motivation: 弱一致性分布式数据库存在用户未预期的异常，传统检查点方法会导致不一致性或过高开销，需要一种高效检查点机制来验证所需不变性。

Method: 定义大小最小检查点概念，设计仅需O(n)新消息和单个计数器的算法，在完全复制数据库中创建强一致性快照。

Result: DTCS算法以最小开销提供强一致性快照，显著优于现有分布式系统和内存数据库的检查点算法。

Conclusion: DTCS通过一系列强一致性快照总结计算过程，使异常发生时只需关注异常时间点周围的快照，提高了弱一致性系统的可调试性。

Abstract: We focus on the problem of checkpointing in fully replicated weakly
consistent distributed databases, which we refer to as Distributed Transaction
Consistent Snapshot (DTCS). A typical example of such a system is a main-memory
database that provides strong eventual consistency. This problem is important
and challenging for several reasons: (1) eventual consistency often creates
anomalies that the users do not anticipate. Hence, frequent checkpoints to
ascertain desired invariants is highly beneficial in their use, and (2)
traditional checkpoints lead to significant overhead and/or inconsistencies. By
showing that the traditional checkpoint leads to inconsistencies or excessive
overhead, we define the notion of size-minimal checkpointing for fully
replicated databases. We present an algorithm for checkpointing with minimal
checkpointing overhead (only O(n) new messages and addition of a single counter
for existing messages). It also provides a significant benefit over existing
checkpointing algorithms for distributed systems and main-memory databases.
  A key benefit of DTCS is that it summarizes the computation by a sequence of
snapshots that are strongly consistent even though the underlying computation
is weakly consistent. In essence, when anomalies arise in an eventually
consistent system, DTCS enables one to concentrate solely on the snapshots
surrounding the time point of the anomaly.

</details>


### [19] [REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum](https://arxiv.org/abs/2510.06675)
*Xu Bai,Muhammed Tawfiqul Islam,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: REACH是一种基于强化学习的微服务动态重调度算法，用于在云边连续体中优化微服务放置，以应对资源波动和性能变化，显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 云计算难以满足新兴延迟敏感应用的实时性需求，云边连续体结合了边缘资源的响应性和云的可扩展性，但异构动态的计算资源给微服务最优放置带来挑战。

Method: 提出REACH算法，使用强化学习实时动态调整微服务放置，以适应分布式基础设施中波动的资源可用性和性能变化。

Result: 在真实测试平台上的广泛实验表明，REACH在三个基准MSA应用中分别将平均端到端延迟降低了7.9%、10%和8%，同时有效缓解了延迟波动和峰值。

Conclusion: REACH算法通过强化学习驱动的动态重调度，能够有效优化云边连续体中的微服务放置，显著提升应用性能并降低延迟。

Abstract: Cloud computing, despite its advantages in scalability, may not always fully
satisfy the low-latency demands of emerging latency-sensitive pervasive
applications. The cloud-edge continuum addresses this by integrating the
responsiveness of edge resources with cloud scalability. Microservice
Architecture (MSA) characterized by modular, loosely coupled services, aligns
effectively with this continuum. However, the heterogeneous and dynamic
computing resource poses significant challenges to the optimal placement of
microservices. We propose REACH, a novel rescheduling algorithm that
dynamically adapts microservice placement in real time using reinforcement
learning to react to fluctuating resource availability, and performance
variations across distributed infrastructures. Extensive experiments on a
real-world testbed demonstrate that REACH reduces average end-to-end latency by
7.9%, 10%, and 8% across three benchmark MSA applications, while effectively
mitigating latency fluctuations and spikes.

</details>


### [20] [Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices](https://arxiv.org/abs/2510.06882)
*Boris Sedlak,Philipp Raith,Andrea Morichetta,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 提出了MUDAP多维度自动扩展平台，支持服务级和资源级的细粒度垂直扩展，通过RASK智能体基于结构知识回归分析优化多服务执行。


<details>
  <summary>Details</summary>
Motivation: 边缘设备资源有限，现有自动扩展机制仅关注资源扩展，无法满足竞争服务的SLO需求，需要支持多维度扩展。

Method: 开发MUDAP平台支持跨维度的细粒度垂直扩展，引入基于结构知识回归分析(RASK)的扩展智能体，学习处理环境的连续回归模型以推断最优扩展动作。

Result: 与Kubernetes VPA和强化学习智能体相比，RASK仅需20次迭代即可学习准确回归模型，在单边缘设备上扩展9个服务时，SLO违规减少28%，支持最高请求负载。

Conclusion: MUDAP和RASK方法通过多维度弹性扩展，显著提升了边缘设备上多服务的SLO满足率和性能表现。

Abstract: Edge devices have limited resources, which inevitably leads to situations
where stream processing services cannot satisfy their needs. While existing
autoscaling mechanisms focus entirely on resource scaling, Edge devices require
alternative ways to sustain the Service Level Objectives (SLOs) of competing
services. To address these issues, we introduce a Multi-dimensional Autoscaling
Platform (MUDAP) that supports fine-grained vertical scaling across both
service- and resource-level dimensions. MUDAP supports service-specific scaling
tailored to available parameters, e.g., scale data quality or model size for a
particular service. To optimize the execution across services, we present a
scaling agent based on Regression Analysis of Structural Knowledge (RASK). The
RASK agent efficiently explores the solution space and learns a continuous
regression model of the processing environment for inferring optimal scaling
actions. We compared our approach with two autoscalers, the Kubernetes VPA and
a reinforcement learning agent, for scaling up to 9 services on a single Edge
device. Our results showed that RASK can infer an accurate regression model in
merely 20 iterations (i.e., observe 200s of processing). By increasingly adding
elasticity dimensions, RASK sustained the highest request load with 28% less
SLO violations, compared to baselines.

</details>


### [21] [GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs](https://arxiv.org/abs/2510.06902)
*Ayesha Afzal,Anna Kahler,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 对四种NVIDIA GPU加速器（A40、A100、L4、L40）在GROMACS分子动力学模拟中的性能分析，包括频率缩放和功率限制下的表现。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟性能严重依赖硬件选择和配置，需要为大规模MD工作流提供GPU硬件选择和性能优化的实用指导。

Method: 使用六个代表性GROMACS生物分子工作负载和两个合成基准测试（Pi Solver计算绑定和STREAM Triad内存绑定），研究GPU图形时钟频率缩放和功率限制下的性能表现。

Result: 较小GROMACS系统显示强频率敏感性，较大系统快速饱和并变得内存绑定；在功率限制下，性能保持稳定直到达到特定阈值，高端GPU如A100在降低功率预算下仍能维持接近最大性能。

Conclusion: 研究结果为在功率限制下选择GPU硬件和优化GROMACS性能提供了实用指导。

Abstract: Molecular dynamics simulations are essential tools in computational
biophysics, but their performance depend heavily on hardware choices and
configuration. In this work, we presents a comprehensive performance analysis
of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six
representative GROMACS biomolecular workloads alongside two synthetic
benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We
investigate how performance scales with GPU graphics clock frequency and how
workloads respond to power capping. The two synthetic benchmarks define the
extremes of frequency scaling: Pi Solver shows ideal compute scalability, while
STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance
in context. Our results reveal distinct frequency scaling behaviors: Smaller
GROMACS systems exhibit strong frequency sensitivity, while larger systems
saturate quickly, becoming increasingly memory bound. Under power capping,
performance remains stable until architecture- and workload-specific thresholds
are reached, with high-end GPUs like the A100 maintaining near-maximum
performance even under reduced power budgets. Our findings provide practical
guidance for selecting GPU hardware and optimizing GROMACS performance for
large-scale MD workflows under power constraints.

</details>


### [22] [Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic](https://arxiv.org/abs/2510.06998)
*Martin Wilhelm,Franz Freitag,Max Tzschoppe,Thilo Pionteck*

Main category: cs.DC

TL;DR: 本文提出了一个用于异构计算系统的灵活评估框架，能够基于抽象任务图描述收集真实世界的makespan结果，分析现有分析方法预测实际makespan的能力，并识别异构系统中的常见挑战。


<details>
  <summary>Details</summary>
Motivation: 异构计算系统结合通用处理器和专用加速器在现代应用中越来越重要，但预测任务映射变化对整体makespan的影响具有挑战性。模拟器需要完整任务实现，而分析方法虽然快速但过于抽象，理论与实践之间存在差距。

Method: 开发了一个高度灵活的评估框架，用于包含CPU、GPU和FPGA的异构系统，能够基于抽象任务图描述收集真实世界的makespan结果。

Result: 分析了现有分析方法预测实际makespan的程度，并识别了异构系统中由高层特性（如数据传输开销和设备拥塞）引起的常见挑战。

Conclusion: 该框架有助于开发快速makespan预测算法，弥合了异构计算系统中理论与实践之间的差距。

Abstract: Heterogeneous computing systems, which combine general-purpose processors
with specialized accelerators, are increasingly important for optimizing the
performance of modern applications. A central challenge is to decide which
parts of an application should be executed on which accelerator or, more
generally, how to map the tasks of an application to available devices.
Predicting the impact of a change in a task mapping on the overall makespan is
non-trivial. While there are very capable simulators, these generally require a
full implementation of the tasks in question, which is particularly
time-intensive for programmable logic. A promising alternative is to use a
purely analytical function, which allows for very fast predictions, but
abstracts significantly from reality. Bridging the gap between theory and
practice poses a significant challenge to algorithm developers. This paper aims
to aid in the development of rapid makespan prediction algorithms by providing
a highly flexible evaluation framework for heterogeneous systems consisting of
CPUs, GPUs and FPGAs, which is capable of collecting real-world makespan
results based on abstract task graph descriptions. We analyze to what extent
actual makespans can be predicted by existing analytical approaches.
Furthermore, we present common challenges that arise from high-level
characteristics such as data transfer overhead and device congestion in
heterogeneous systems.

</details>
