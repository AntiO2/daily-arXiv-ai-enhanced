<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 13]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.SE](#cs.SE) [Total: 14]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Topological Relational Theory: A Simplicial-Complex View of Functional Dependencies, Lossless Decomposition, and Acyclicity](https://arxiv.org/abs/2602.21213)
*Bilge Senturk,Faruk Alpay*

Main category: cs.DB

TL;DR: 该论文提出了一种基于拓扑学的关系模式设计方法，将函数依赖编码为抽象单纯复形的单纯形，通过同调不变量诊断循环依赖结构。


<details>
  <summary>Details</summary>
Motivation: 传统的关系模式设计方法缺乏对多属性交互和循环依赖结构的系统性分析工具，需要新的数学框架来诊断和优化模式设计。

Method: 将函数依赖编码为抽象单纯复形（依赖复形），利用同调理论（Betti数）分析依赖结构；定义单纯正规形式（SNF）作为同调无环性条件；将经典的无损连接准则重新表述为拓扑变形收缩问题。

Result: 建立了依赖复形的同调性质与模式设计质量的关系；证明了无损连接准则的拓扑等价形式；展示了神经复形中的1-循环如何阻碍连接树结构；提出了基于Betti数的轻量级模式诊断算法。

Conclusion: 拓扑方法为关系模式设计提供了新的分析工具，能够诊断循环依赖结构并补充传统的FD-chase测试，具有实际的计算应用价值。

Abstract: We develop a topological lens on relational schema design by encoding functional dependencies (FDs) as simplices of an abstract simplicial complex. This dependency complex exposes multi-attribute interactions and enables homological invariants (Betti numbers) to diagnose cyclic dependency structure. We define Simplicial Normal Form (SNF) as homological acyclicity of the dependency complex in positive dimensions, i.e., vanishing reduced homology for all $n \ge 1$. SNF is intentionally weaker than contractibility and does not identify homology with homotopy. For decompositions, we give a topological reformulation of the classical binary lossless-join criterion: assuming dependency preservation, a decomposition is lossless exactly when the intersection attributes form a key for at least one component. Topologically, this yields a strong deformation retraction that trivializes the relevant Mayer--Vietoris boundary map. For multiway decompositions, we show how the nerve of a cover by induced subcomplexes provides a computable certificate: a 1-cycle in the nerve (detected by $H_1$) obstructs join-tree structure and aligns with cyclic join behavior in acyclic-scheme theory. Finally, we discuss an algorithmic consequence: Betti numbers of the dependency complex (or of a decomposition nerve) can be computed from boundary matrices and used as a lightweight schema diagnostic to localize "unexplained" dependency cycles, complementing standard FD-chase tests.

</details>


### [2] [Premature Dimensional Collapse and Tensor-based Execution Paths for High-Dimensional Relational Operations in Cost-Based Database Systems](https://arxiv.org/abs/2602.21237)
*Il-Sun Chang*

Main category: cs.DB

TL;DR: 论文提出通过张量执行路径延迟中间表示的线性化，解决内存受限时数据库执行不稳定和尾部延迟问题


<details>
  <summary>Details</summary>
Motivation: 现代基于成本的DBMS在高维关系操作触发内存机制转换（如哈希表溢出和外部物化）时，经常出现执行不稳定和尾部延迟放大问题。作者发现结构性问题在于中间表示在内存压力下过早线性化，导致不成比例的I/O放大和类似相变的延迟行为。

Method: 提出基于张量的执行路径，通过延迟过早线性化和结构化中间布局来保持高维局部性。使用修改的PostgreSQL原型和受控微基准测试进行验证。

Result: 在受限内存设置下（如work_mem=1MB），传统执行可能溢出数百兆字节并超过数秒的P99延迟，而提出的路径保持稳定执行并将P99延迟降低到亚秒级。

Conclusion: 表示时机是执行稳定性的重要设计变量，补充了传统专注于基数估计和算子吞吐量的优化工作。

Abstract: Modern cost-based DBMSs frequently exhibit execution instability and tail-latency amplification when high-dimensional relational operations trigger memory-regime transitions such as hash-table spilling and external materialization. We identify a structural failure mode in which intermediate representations are prematurely linearized under memory pressure, causing disproportionate I/O amplification and phase-transition-like latency behavior. To mitigate this, we propose a tensor-based execution path that delays premature linearization and preserves higher-dimensional locality through late materialization and structured intermediate layouts. Using a modified PostgreSQL-based prototype and controlled microbenchmarks, we show that under constrained memory settings (e.g., work_mem=1MB) conventional execution can spill hundreds of megabytes and exceed multi-second P99 latency, while the proposed path maintains stable execution and reduces P99 latency to sub-second levels. Our results suggest that representation timing is a first-class design variable for execution stability, complementing traditional optimization efforts focused on cardinality estimation and operator throughput.

</details>


### [3] [PiPNN: Ultra-Scalable Graph-Based Nearest Neighbor Indexing](https://arxiv.org/abs/2602.21247)
*Tobias Rubel,Richard Wen,Laxman Dhulipala,Lars Gottesbüren,Rajesh Jayaram,Jakub Łącki*

Main category: cs.DB

TL;DR: PiPNN是一种超可扩展的图构建算法，通过HashPrune在线剪枝技术避免传统图方法的搜索瓶颈，构建速度比现有方法快11-13倍，首次实现单机20分钟内构建十亿级数据集的高质量ANN索引。


<details>
  <summary>Details</summary>
Motivation: 当前最快的近似最近邻搜索索引（如HNSW和Vamana）构建速度极慢，因为它们依赖随机访问密集的beam搜索，存在"搜索瓶颈"。需要一种能够快速构建高质量图索引的算法。

Method: PiPNN的核心创新是HashPrune在线剪枝算法，它动态维护稀疏边集合。通过将数据集划分为重叠子问题，使用密集矩阵乘法内核高效进行批量距离比较，并将边子集流式输入HashPrune，保证构建过程中的内存有界。

Result: PiPNN构建速度比Vamana快11.6倍，比HNSW快12.9倍，比MIRAGE快19.1倍，比FastKCNA快17.3倍。首次实现单机20分钟内构建十亿级数据集的高质量ANN索引，且产生的索引具有更高的查询吞吐量。

Conclusion: PiPNN通过HashPrune算法解决了图构建中的搜索瓶颈问题，实现了超可扩展的图构建，显著提升了构建速度而不牺牲索引质量，为大规模ANN应用提供了实用解决方案。

Abstract: The fastest indexes for Approximate Nearest Neighbor Search today are also the slowest to build: graph-based methods like HNSW and Vamana achieve state-of-the-art query performance but have large construction times due to relying on random-access-heavy beam searches. We introduce PiPNN (Pick-in-Partitions Nearest Neighbors), an ultra-scalable graph construction algorithm that avoids this ``search bottleneck'' that existing graph-based methods suffer from.
  PiPNN's core innovation is HashPrune, a novel online pruning algorithm which dynamically maintains sparse collections of edges. HashPrune enables PiPNN to partition the dataset into overlapping sub-problems, efficiently perform bulk distance comparisons via dense matrix multiplication kernels, and stream a subset of the edges into HashPrune. HashPrune guarantees bounded memory during index construction which permits PiPNN to build higher quality indices without the use of extra intermediate memory.
  PiPNN builds state-of-the-art indexes up to 11.6x faster than Vamana (DiskANN) and up to 12.9x faster than HNSW. PiPNN is significantly more scalable than recent algorithms for fast graph construction. PiPNN builds indexes at least 19.1x faster than MIRAGE and 17.3x than FastKCNA while producing indexes that achieve higher query throughput. PiPNN enables us to build, for the first time, high-quality ANN indexes on billion-scale datasets in under 20 minutes using a single multicore machine.

</details>


### [4] [BuffCut: Prioritized Buffered Streaming Graph Partitioning](https://arxiv.org/abs/2602.21248)
*Linus Baumgärtner,Adil Chhabra,Marcelo Fonseca Faraj,Christian Schulz*

Main category: cs.DB

TL;DR: BuffCut：一种带缓冲的流式图划分器，通过优先级缓冲和批处理多级分配，显著减少边割数量，在对抗性流顺序下尤其有效。


<details>
  <summary>Details</summary>
Motivation: 传统流式图划分器对数据流顺序高度敏感，边割数量通常远高于内存方法。需要一种能在流式处理中保持高质量划分的方法。

Method: 采用有界优先级缓冲区延迟决策，通过迭代插入最高优先级节点构建高局部性批次，然后使用多级划分算法分配每个批次。

Result: 在真实和合成图上的实验显示，BuffCut始终优于现有缓冲流式方法。相比最强基线，边割减少20.8%，运行速度快2.9倍，内存使用减少11.3倍。

Conclusion: BuffCut通过结合优先级缓冲和批处理多级分配，显著缩小了流式与内存划分器之间的质量差距，特别是在对抗性流顺序下表现优异。

Abstract: Streaming graph partitioners enable resource-efficient and massively scalable partitioning, but one-pass assignment heuristics are highly sensitive to stream order and often yield substantially higher edge cuts than in-memory methods. We present BuffCut, a buffered streaming partitioner that narrows this quality gap, particularly when stream ordering is adversarial, by combining prioritized buffering with batch-wise multilevel assignment. BuffCut maintains a bounded priority buffer to delay poorly informed decisions and regulate the order in which nodes are considered for assignment. It incrementally constructs high-locality batches of configurable size by iteratively inserting the highest-priority nodes from the buffer into the batch, effectively recovering locality structure from the stream. Each batch is then assigned via a multilevel partitioning algorithm. Experiments on diverse real-world and synthetic graphs show that BuffCut consistently outperforms state-of-the-art buffered streaming methods. Compared to the strongest prioritized buffering baseline, BuffCut achieves 20.8% fewer edge cuts while running 2.9 times faster and using 11.3 times less memory. Against the next-best buffered method, it reduces edge cut by 15.8% with only modest overheads of 1.8 times runtime and 1.09 times memory.

</details>


### [5] [Quality of Descriptive Information on Cultural Heritage Objects: Definition and Empirical Evaluation](https://arxiv.org/abs/2602.21249)
*Markus Matoni,Arno Kesper,Gabriele Taentzer*

Main category: cs.DB

TL;DR: 该论文针对文化遗产领域描述性信息的数据质量问题，提出了专门的质量维度定义，并通过真实世界数据问题进行了实证验证。


<details>
  <summary>Details</summary>
Motivation: 数据质量对数据处理至关重要，但目前缺乏广泛接受、领域无关的数据质量定义。现有框架多为特定领域定制，文化遗产领域描述性信息的质量评估方法发展不足，且现有定义多为理论性，缺乏基于真实世界数据问题的实证验证。

Method: 首先基于对现有质量维度的深入分析，定义了一套专门针对文化遗产对象描述性信息的质量维度，并通过领域特定示例进行说明。然后使用文化遗产领域真实世界数据质量问题的精选集，对所提出的质量定义进行实证评估。

Result: 通过实证评估验证了所提出的数据质量定义，形成了文化遗产领域全面的数据质量定义。

Conclusion: 该研究填补了文化遗产领域数据质量定义的空白，提供了经过实证验证的、专门针对文化遗产描述性信息的质量维度框架。

Abstract: Effective data processing depends on the quality of the underlying data. However, quality issues such as inconsistencies and uncertainties, can significantly impede the processing and subsequent use of data. Despite the centrality of data quality to a wide range of computational tasks, there is currently no broadly accepted, domain-independent consensus on the definition of data quality. Existing frameworks primarily define data quality in ways that are tailored to specific domains, data types, or contexts of use. Although quality assessment frameworks exist for specific domains, such as electronic health record data and linked data, corresponding approaches for descriptive information about cultural heritage objects remain underdeveloped. Moreover, existing quality definitions are often theoretical in nature and lack empirical validation based on real-world data problems. In this paper, we address these limitations by first defining a set of quality dimensions specifically designed to capture the characteristics of descriptive information about cultural heritage objects. Our definition is based on an in-depth analysis of existing dimensions and is illustrated through domain-specific examples. We then evaluate the practical applicability of our proposed quality definition using a curated set of real-world data quality problems from the cultural heritage domain. This empirical evaluation substantiates our definition of data quality, resulting in a comprehensive definition of data quality in this domain.

</details>


### [6] [Both Ends Count! Just How Good are LLM Agents at "Text-to-Big SQL"?](https://arxiv.org/abs/2602.21480)
*Germán T. Eizaguirre,Lars Tissen,Marc Sánchez-Artigas*

Main category: cs.DB

TL;DR: 该论文提出"Text-to-Big SQL"概念，针对现有文本到SQL基准测试在大数据场景下的不足，引入新的评估指标，并通过实验证明传统指标在大数据环境中不够充分。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL研究和大数据领域虽然都有广泛基准测试，但缺乏联合评估。实际应用中，文本到SQL系统常嵌入大数据工作流，但现有基准测试范围狭窄，忽略了数据规模扩大时的成本和性能影响。小数据集上的微小翻译错误在大数据场景下会导致显著的成本和延迟开销，而现有指标完全忽略了这一问题。

Method: 引入新颖且具有代表性的Text-to-Big SQL评估指标，重点关注生产级LLM代理（数据库无关的系统，适应多样化用户需求）。通过对前沿模型进行广泛评估，分析传统文本到SQL指标在大数据场景下的不足。

Result: 研究表明文本到SQL指标在大数据场景下不够充分，而提出的Text-to-Big SQL指标能准确反映执行效率、成本以及数据规模的影响。同时提供了LLM特定洞察，包括细粒度的跨模型延迟和成本比较。

Conclusion: 需要专门针对大数据场景的文本到SQL评估指标，传统指标无法捕捉数据规模扩大时的实际成本和性能影响。提出的Text-to-Big SQL指标为实际生产环境提供了更准确的评估框架。

Abstract: Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as "Text-to-Big SQL". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics.
  In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on production-level LLM agents, a database-agnostic system adaptable to diverse user needs. Via an extensive evaluation of frontier models, we show that text-to-SQL metrics are insufficient for Big Data. In contrast, our proposed text-to-Big SQL metrics accurately reflect execution efficiency, cost, and the impact of data scale. Furthermore, we provide LLM-specific insights, including fine-grained, cross-model comparisons of latency and cost.

</details>


### [7] [I/O Optimizations for Graph-Based Disk-Resident Approximate Nearest Neighbor Search: A Design Space Exploration](https://arxiv.org/abs/2602.21514)
*Liang Li,Shufeng Gong,Yanan Yang,Yiduo Wang,Jie Wu*

Main category: cs.DB

TL;DR: 本文提出一个面向SSD的近似最近邻搜索I/O优先框架，通过内存布局、磁盘布局和搜索算法三个维度的系统组合，显著减少I/O开销并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 基于SSD的近似最近邻搜索中，I/O操作占查询延迟的70-90%，成为主要瓶颈。现有方法缺乏对I/O优化的系统化框架，需要从存储角度重新思考ANN设计。

Method: 提出I/O优先框架，从三个维度组织技术：内存布局、磁盘布局和搜索算法。建立页面级复杂度模型解释页面局部性和路径长度如何共同决定页面读取。通过一致实现验证模型，并量化单因素效应和跨维度协同作用。

Result: 内存驻留导航和动态宽度提供最强的独立增益；页面洗牌和页面搜索单独效果弱但互补性强；系统化组合OctopusANN显著减少I/O，在Recall@10=90%时比Starling吞吐量高4.1-37.9%，比DiskANN高87.5-149.5%。

Conclusion: 为基于磁盘的ANN提供了可操作的指导原则，强调在不同并发水平和精度约束下选择存储中心或混合设计，倡导系统化组合而非孤立调整以推动性能边界。

Abstract: Approximate nearest neighbor (ANN) search on SSD-backed indexes is increasingly I/O-bound (I/O accounts for 70--90\% of query latency). We present an I/O-first framework for disk-based ANN that organizes techniques along three dimensions: memory layout, disk layout, and search algorithm. We introduce a page-level complexity model that explains how page locality and path length jointly determine page reads, and we validate the model empirically. Using consistent implementations across four public datasets, we quantify both single-factor effects and cross-dimensional synergies. We find that (i) memory-resident navigation and dynamic width provide the strongest standalone gains; (ii) page shuffle and page search are weak alone but complementary together; and (iii) a principled composition, OctopusANN, substantially reduces I/O and achieves 4.1--37.9\% higher throughput than the state-of-the-art system Starling and 87.5--149.5\% higher throughput than DiskANN at matched Recall@10=90\%. Finally, we distill actionable guidelines for selecting storage-centric or hybrid designs across diverse concurrency levels and accuracy constraints, advocating systematic composition rather than isolated tweaks when pushing the performance frontier of disk-based ANN.

</details>


### [8] [RAC: Relation-Aware Cache Replacement for Large Language Models](https://arxiv.org/abs/2602.21547)
*Yuchong Wu,Zihuan Xu,Wangze Ni,Peng Cheng,Lei Chen,Xuemin Lin,Heng Tao Shen,Kui Ren*

Main category: cs.DB

TL;DR: RAC是一种基于语义关系的LLM服务缓存替换策略，利用主题流行度和结构重要性信号，在有限容量下显著提升缓存命中率。


<details>
  <summary>Details</summary>
Motivation: LLM服务扩展面临成本和延迟挑战，现有缓存替换策略主要依赖近期性和频率等有限窗口统计信息，这些信号对现实LLM工作负载（具有长重用距离和稀疏局部重复性）不够鲁棒。

Method: 提出关系感知缓存(RAC)，一种在线驱逐策略，利用请求间的语义关系指导驱逐决策。RAC综合两种关系感知信号：1) 主题流行度：在主题层面聚合访问证据以捕获长时域重用；2) 结构重要性：利用局部主题内依赖结构区分条目的未来重用价值。

Result: 广泛评估显示RAC在不同工作负载下保持高有效性，在缓存命中率方面持续超越最先进基线20%-30%。

Conclusion: RAC通过利用语义关系信号有效解决了LLM工作负载中传统缓存策略的局限性，显著提升了缓存性能。

Abstract: The scaling of Large Language Model (LLM) services faces significant cost and latency challenges, making effective caching under tight capacity crucial. Existing cache replacement policies, from heuristics to learning-based methods, predominantly rely on limited-window statistics such as recency and frequency. We show these signals are not robust for real-world LLM workloads, which exhibit long reuse distances and sparse local recurrence.
  To address these limitations, we propose Relation-Aware Cache (RAC), an online eviction strategy that leverages semantic relations among requests to guide eviction decisions. RAC synthesizes two relation-aware signals: (1) Topical Prevalence, which aggregates access evidence at the topic level to capture long-horizon reuse; and (2) Structural Importance, which leverages local intra-topic dependency structure to discriminate entries by their future reuse value. Extensive evaluations show that RAC maintains high effectiveness across diverse workloads, consistently surpassing state-of-the-art baselines by 20%--30% in cache hit ratio.

</details>


### [9] [Epoch-based Optimistic Concurrency Control in Geo-replicated Databases](https://arxiv.org/abs/2602.21566)
*Yunhao Mao,Harunari Takata,Michail Bachras,Yuqiu Zhang,Shiquan Zhang,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DB

TL;DR: Minerva是一个用于地理分布式数据库的统一分布式并发控制协议，通过解耦数据传播与提交过程、使用确定性重执行解决冲突，显著提升了高延迟网络下的可序列化事务性能。


<details>
  <summary>Details</summary>
Motivation: 地理分布式应用需要高可用性和可靠性，但在高延迟网络下实现高性能的可序列化事务面临巨大挑战，主要源于分布式原子提交、并发控制和容错复制协议中的过度协调问题。

Method: 1. 采用基于epoch的异步复制协议，解耦数据传播与提交过程，支持连续事务复制；2. 使用乐观并发控制，允许副本并行执行事务且无需协调提交；3. 通过确定性重执行而非中止事务来解决冲突；4. 构建冲突图并使用最大权重独立集算法选择最优事务子集进行提交，最小化重执行事务数量。

Result: Minerva在可扩展性实验中比最先进的复制数据库实现了超过3倍的吞吐量提升，在TPC-C基准测试的高网络延迟模拟中实现了2.8倍的吞吐量提升。

Conclusion: Minerva通过创新的异步复制协议和确定性冲突解决机制，有效解决了地理分布式数据库中高性能可序列化事务的挑战，显著提升了系统吞吐量。

Abstract: Geo-distribution is essential for modern online applications to ensure service reliability and high availability. However, supporting high-performance serializable transactions in geo-replicated databases remains a significant challenge. This difficulty stems from the extensive over-coordination inherent in distributed atomic commitment, concurrency control, and fault-tolerance replication protocols under high network latency.
  To address these challenges, we introduce Minerva, a unified distributed concurrency control designed for highly scalable multi-leader replication. Minerva employs a novel epoch-based asynchronous replication protocol that decouples data propagation from the commitment process, enabling continuous transaction replication. Optimistic concurrency control is used to allow any replicas to execute transactions concurrently and commit without coordination. In stead of aborting transactions when conflicts are detected, Minerva uses deterministic re-execution to resolve conflicts, ensuring serializability without sacrificing performance. To further enhance concurrency, we construct a conflict graph and use a maximum weight independent set algorithm to select the optimal subset of transactions for commitment, minimizing the number of re-executed transactions. Our evaluation demonstrates that Minerva significantly outperforms state-of-the-art replicated databases, achieving over $3\times$ higher throughput in scalability experiments and $2.8\times$ higher throughput during a high network latency simulation with the TPC-C benchmark.

</details>


### [10] [Towards Autonomous Graph Data Analytics with Analytics-Augmented Generation](https://arxiv.org/abs/2602.21604)
*Qiange Wang,Chaoyi Chen,Jingqi Gao,Zihan Wang,Yanfeng Zhang,Ge Yu*

Main category: cs.DB

TL;DR: 论文提出Analytics-Augmented Generation (AAG)新范式，将分析计算作为首要关注点，让LLM作为知识驱动的分析协调者，实现从自然语言意图到自动化执行的可解释端到端图数据分析。


<details>
  <summary>Details</summary>
Motivation: 当前基于检索或代码生成的LLM智能体无法实现可靠的端到端图数据分析。虽然LLM具备强大的推理能力，但面向非专家用户的实用图分析需要明确的分析基础来支持意图到执行的转换、任务感知的图构建以及跨多样图算法的可靠执行。

Method: 提出Analytics-Augmented Generation (AAG)新范式：1) 将分析计算作为首要关注点；2) 将LLM定位为知识驱动的分析协调者；3) 集成知识驱动的任务规划、算法中心的LLM-分析交互和任务感知的图构建。

Result: AAG能够实现端到端的图分析流水线，将自然语言用户意图转化为自动化执行和可解释的结果。

Conclusion: 需要超越单纯的检索或代码生成为中心的LLM智能体，通过AAG新范式将分析计算作为核心关注点，让LLM在知识驱动下协调分析过程，实现可靠、可解释的端到端图数据分析。

Abstract: This paper argues that reliable end-to-end graph data analytics cannot be achieved by retrieval- or code-generation-centric LLM agents alone. Although large language models (LLMs) provide strong reasoning capabilities, practical graph analytics for non-expert users requires explicit analytical grounding to support intent-to-execution translation, task-aware graph construction, and reliable execution across diverse graph algorithms. We envision Analytics-Augmented Generation (AAG) as a new paradigm that treats analytical computation as a first-class concern and positions LLMs as knowledge-grounded analytical coordinators. By integrating knowledge-driven task planning, algorithm-centric LLM-analytics interaction, and task-aware graph construction, AAG enables end-to-end graph analytics pipelines that translate natural-language user intent into automated execution and interpretable results.

</details>


### [11] [RAMSeS: Robust and Adaptive Model Selection for Time-Series Anomaly Detection Algorithms](https://arxiv.org/abs/2602.21766)
*Mohamed Abdelmaksoud,Sheng Ding,Andrey Morozov,Ziawasch Abedjan*

Main category: cs.DB

TL;DR: RAMSeS框架通过双分支策略解决时间序列异常检测的跨域适应性问题：一个分支使用遗传算法优化堆叠集成，另一个分支使用多种技术自适应选择最佳单一检测器。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据在不同领域差异很大，通用的异常检测器不实用。现有方法在一个数据集上表现良好但难以迁移到其他领域，因为异常的定义是上下文相关的。关键挑战是设计一个在特定上下文中表现良好，同时能适应不同领域数据复杂性的方法。

Method: RAMSeS框架包含两个分支：(1) 使用遗传算法优化的堆叠集成，利用互补检测器的优势；(2) 自适应模型选择分支，使用汤普森采样、生成对抗网络的鲁棒性测试和蒙特卡洛模拟等技术识别最佳单一检测器。

Result: 评估显示RAMSeS在F1分数上优于先前的方法。

Conclusion: RAMSeS的双重策略既利用了多个模型的集体优势，又能适应数据集特定的特征，解决了时间序列异常检测的跨域适应性问题。

Abstract: Time-series data vary widely across domains, making a universal anomaly detector impractical. Methods that perform well on one dataset often fail to transfer because what counts as an anomaly is context dependent. The key challenge is to design a method that performs well in specific contexts while remaining adaptable across domains with varying data complexities. We present the Robust and Adaptive Model Selection for Time-Series Anomaly Detection RAMSeS framework. RAMSeS comprises two branches: (i) a stacking ensemble optimized with a genetic algorithm to leverage complementary detectors. (ii) An adaptive model-selection branch identifies the best single detector using techniques including Thompson sampling, robustness testing with generative adversarial networks, and Monte Carlo simulations. This dual strategy exploits the collective strength of multiple models and adapts to dataset-specific characteristics. We evaluate RAMSeS and show that it outperforms prior methods on F1.

</details>


### [12] [Quantum Computing for Query Containment of Conjunctive Queries](https://arxiv.org/abs/2602.21803)
*Luisa Gerlach,Tobias Köppl,Renè Zander,Nicole Schweikardt,Stefanie Scherzinger*

Main category: cs.DB

TL;DR: 首次将量子计算应用于集合语义下合取查询的查询包含问题，提出可运行于门基量子硬件和量子退火器的优化问题形式化方法


<details>
  <summary>Details</summary>
Motivation: 查询包含是数据库研究的基础问题，虽然理论研究广泛，但由于计算复杂度高甚至不可判定，商业数据库系统未能充分利用其优化潜力。量子计算为此问题提供了新的计算视角。

Method: 提出将查询包含问题形式化为优化问题的新方法，该方法可在门基量子硬件上求解，某些情况下可直接映射到量子退火器。提供了原型实现并在模拟器和量子设备上评估。

Result: 实验成功证明该方法正确且在当前量子硬件限制内具有可扩展性，展示了量子优化能有效解决查询包含问题。

Conclusion: 首次将量子计算应用于查询包含问题，为这一经典数据库问题贡献了新的计算视角，证明了量子优化在此领域的有效性。

Abstract: We address the problem of checking query containment, a foundational problem in database research. Although extensively studied in theory research, optimization opportunities arising from query containment are not fully leveraged in commercial database systems, due to the high computational complexity and sometimes even undecidability of the underlying decision problem. In this article, we present the first approach to applying quantum computing to the query containment problem for conjunctive queries under set semantics. We propose a novel formulation as an optimization problem that can be solved on gate-based quantum hardware, and in some cases directly maps to quantum annealers. We formally prove this formulation to be correct and present a prototype implementation which we evaluate using simulator software as well as quantum devices. Our experiments successfully demonstrate that our approach is sound and scales within the current limitations of quantum hardware. In doing so, we show that quantum optimization can effectively address this problem. Thereby, we contribute a new computational perspective on the query containment problem.

</details>


### [13] [Detecting Logic Bugs of Join Optimizations in DBMS](https://arxiv.org/abs/2602.21955)
*Xiu Tang,Sai Wu,Dongxiang Zhang,Feifei Li,Gang Chen*

Main category: cs.DB

TL;DR: TQS是一个针对多表连接查询逻辑bug的测试框架，通过数据引导的模式查询生成和知识引导的查询空间探索，在24小时内检测到115个bug。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成的测试工具仅限于单表查询，对于涉及多表连接操作符的查询存在研究空白，而DBMS中的逻辑bug通常由查询优化器的不当实现引起。

Method: TQS包含两个关键组件：1) 数据引导的模式和查询生成(DSG)，采用数据库规范化技术生成测试模式，维护位图索引进行结果跟踪，并人为插入噪声；2) 知识引导的查询空间探索(KQE)，将问题建模为同构图集发现，结合图嵌入和加权随机游走进行查询生成。

Result: 在MySQL、MariaDB、TiDB和行业领先的云原生数据库X-DB上评估，TQS在24小时内成功检测到115个逻辑bug：MySQL 31个、MariaDB 30个、TiDB 31个、X-DB 23个。

Conclusion: TQS能有效检测数据库管理系统中连接优化的逻辑bug，填补了多表连接查询测试的研究空白，展示了在实际DBMS中的实用价值。

Abstract: Generation-based testing techniques have shown their effectiveness in detecting logic bugs of DBMS, which are often caused by improper implementation of query optimizers. Nonetheless, existing generation-based debug tools are limited to single-table queries and there is a substantial research gap regarding multi-table queries with join operators. In this paper, we propose TQS, a novel testing framework targeted at detecting logic bugs derived by queries involving multi-table joins. Given a target DBMS, TQS achieves the goal with two key components: Data-guided Schema and Query Generation (DSG) and Knowledge-guided Query Space Exploration (KQE). DSG addresses the key challenge of multi-table query debugging: how to generate ground-truth (query, result) pairs for verification. It adopts the database normalization technique to generate a testing schema and maintains a bitmap index for result tracking. To improve debug efficiency, DSG also artificially inserts some noises into the generated data. To avoid repetitive query space search, KQE forms the problem as isomorphic graph set discovery and combines the graph embedding and weighted random walk for query generation. We evaluated TQS on four popular DBMSs: MySQL, MariaDB, TiDB and the gray release of an industry-leading cloud-native database, anonymized as X-DB. Experimental results show that TQS is effective in finding logic bugs of join optimization in database management systems. It successfully detected 115 bugs within 24 hours, including 31 bugs in MySQL, 30 in MariaDB, 31 in TiDB, and 23 in X-DB respectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [General Convex Agreement with Near-Optimal Communication](https://arxiv.org/abs/2602.21411)
*Marc Dufay,Diana Ghinea,Anton Paramonov*

Main category: cs.DC

TL;DR: 提出了针对抽象凸空间的确定性同步凸协议，实现了接近最优的通信复杂度：对于有限凸空间为O(L·n log n)，对于欧几里得空间为O(L·n^{1+o(1)})，同时保持最优轮复杂度O(n)。


<details>
  <summary>Details</summary>
Motivation: 凸协议（CA）相比拜占庭协议（BA）要求输出位于诚实方输入的凸包内，适用于需要聚合但输入不必完全一致的实用场景（如鲁棒学习、传感器融合）。现有CA协议通信复杂度为Θ(Ln²)，与BA的下界Ω(Ln)存在差距，需要为抽象凸空间设计通信高效的协议。

Method: 使用提取器图获得确定性委员会分配方案，能够抵抗自适应敌手攻击。协议设计针对抽象凸空间，当L=Ω(n·κ)时，实现有限凸空间的O(L·n log n)通信和欧几里得空间的O(L·n^{1+o(1)})通信。

Result: 实现了接近最优的通信复杂度：有限凸空间O(L·n log n)，欧几里得空间O(L·n^{1+o(1)})，轮复杂度为最优的O(n)。当输入长度L已知时，实现接近最优的容错性t < n/(ω+ε)；L未知时，容错性为t < n/(ω+ε+1)。

Conclusion: 该工作填补了抽象凸空间凸协议通信复杂度的理论空白，通过提取器图技术实现了确定性委员会分配，为凸协议提供了通信高效的解决方案，并可应用于并行拜占庭协议。

Abstract: Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \emph{communication}: standard approaches for CA have a communication complexity of $Θ(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $Ω(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.
  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = Ω(n \cdot κ)$, where $κ$ is a security parameter, we achieve $O(L\cdot n\log n)$ communication for finite convexity spaces and $O(L\cdot n^{1+o(1)})$ communication for Euclidean spaces $\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(ω+\varepsilon)$ for any constant $\varepsilon>0$, where $ω$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(ω+\varepsilon+1)$ for any constant $\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.
  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.

</details>


### [15] [DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference](https://arxiv.org/abs/2602.21548)
*Yongtong Wu,Shaoyuan Chen,Yinmin Zhong,Rilin Huang,Yixuan Tan,Wentao Zhang,Liyue Zhang,Shangyan Zhou,Yuxuan Liu,Shunfeng Zhou,Mingxing Zhang,Xin Jin,Panpan Huang*

Main category: cs.DC

TL;DR: DualPath通过引入双路径KV-Cache加载机制，解决了多轮代理式LLM推理中KV-Cache存储I/O瓶颈问题，显著提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多轮代理式LLM推理的性能越来越受KV-Cache存储I/O而非计算限制。在流行的解耦架构中，从外部存储加载大量KV-Cache导致存储NIC带宽饱和，而解码引擎的NIC却闲置，这种不对称严重限制了系统整体吞吐量。

Method: DualPath引入双路径KV-Cache加载机制：除了传统的存储到预填充路径外，新增存储到解码路径，将KV-Cache加载到解码引擎，然后通过RDMA在计算网络上高效传输到预填充引擎。该系统结合优化的数据路径（避免网络拥塞和延迟关键模型执行通信的干扰）以及全局调度器动态平衡预填充和解码引擎的负载。

Result: 在三个模型的生产代理工作负载评估中，DualPath将离线推理吞吐量提升高达1.87倍，在线服务吞吐量平均提升1.96倍，且不违反SLO。

Conclusion: DualPath通过创新的双路径KV-Cache加载机制有效解决了多轮LLM推理中的存储I/O瓶颈，显著提升了系统吞吐量，为代理式推理系统提供了高效的解决方案。

Abstract: The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.
  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.
  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87$\times$ on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96$\times$ without violating SLO.

</details>


### [16] [Multi-Layer Scheduling for MoE-Based LLM Reasoning](https://arxiv.org/abs/2602.21626)
*Yifan Sun,Gholamreza Haffar,Minxian Xu,Rajkumar Buyya,Adel N. Toosi*

Main category: cs.DC

TL;DR: 提出针对MoE大语言模型服务的多层调度框架，在请求、引擎和专家三个层面优化调度策略，相比vLLM显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务面临计算和延迟挑战，现有调度策略如FCFS和RR无法充分利用系统资源，存在队头阻塞和负载不均衡问题，而MoE模型又带来了专家并行和路由复杂性的新挑战

Method: 提出三层调度框架：1) 请求层采用SJF和优先级感知老化算法；2) 引擎层基于前缀token负载、KV缓存利用率和用户粘性设计负载感知分发策略；3) 专家层缓解专家热点并策略性放置层间专家依赖

Result: 在100多个不同工作负载分布的实验中，相比vLLM框架，TTFT延迟降低17.8%，TPOT延迟降低13.3%

Conclusion: 针对MoE大语言模型服务的多层调度框架能有效优化系统资源利用，显著降低服务延迟，为高效的大模型服务提供解决方案

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.

</details>


### [17] [Lamport's Arrow of Time: The Category Mistake in Logical Clocks](https://arxiv.org/abs/2602.21730)
*Paul Borrill*

Main category: cs.DC

TL;DR: 该论文批判Lamport的happens-before关系保留了未检验的假设：因果性诱导全局有向无环图，将认识论构造与本体论主张混淆。作者提出基于互信息守恒而非时间优先的分布式一致性原语。


<details>
  <summary>Details</summary>
Motivation: 揭示Lamport的happens-before关系中隐含的深层假设：因果性形成全局有向无环图，这种"仅向前时间"结构将逻辑消息排序的认识论构造与物理因果性的本体论主张混淆。

Method: 运用Ryle的范畴错误分析，追溯这一混淆在香农信道模型、TLA+、贝尔定理、FLP不可能性定理和CAP定理中的体现，结合相对论和不确定因果顺序研究，论证自然只允许局部因果结构。

Result: 论证Lamport的形式主义保留了未检验的假设，自然只允许局部因果结构，不确定因果顺序研究表明自然允许没有明确定义因果顺序的关联，需要新的分布式一致性基础。

Conclusion: 提出互信息守恒而非时间优先应作为分布式一致性的更基本原语，需要超越全局有向无环图假设的新分布式系统理论基础。

Abstract: Lamport's 1978 paper introduced the happens-before relation and logical clocks, freeing distributed systems from dependence on synchronized physical clocks. This is widely understood as a move away from Newtonian absolute time. We argue that Lamport's formalism retains a deeper and largely unexamined assumption: that causality induces a globally well-defined directed acyclic graph (DAG) over events -- a forward-in-time-only (FITO) structure that functions as an arrow of time embedded at the semantic level. Following Ryle's analysis of category mistakes, we show that this assumption conflates an epistemic construct (the logical ordering of messages) with an ontic claim (that physical causality is globally acyclic and monotonic). We trace this conflation through Shannon's channel model, TLA+, Bell's theorem, and the impossibility results of Fischer-Lynch-Paterson and Brewer's CAP theorem. We then show that special and general relativity permit only local causal structure, and that recent work on indefinite causal order demonstrates that nature admits correlations with no well-defined causal ordering. We propose that mutual information conservation, rather than temporal precedence, provides a more fundamental primitive for distributed consistency.

</details>


### [18] [DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism](https://arxiv.org/abs/2602.21788)
*Yifan Niu,Han Xiao,Dongyi Liu,Wei Zhou,Jia Li*

Main category: cs.DC

TL;DR: 提出动态混合并行（DHP）策略，解决多模态大语言模型训练中数据异构性导致的负载不均衡问题，相比现有方法提升训练吞吐量1.36倍


<details>
  <summary>Details</summary>
Motivation: 现实世界多模态数据集高度异构，现有静态并行策略在数据异构情况下存在严重负载不均衡、冗余通信和硬件利用率低下问题

Method: 提出动态混合并行（DHP）策略，自适应地重构通信组和并行度；推广非2的幂次并行度，开发多项式时间算法生成近似最优并行策略，每批次训练仅毫秒级开销

Result: DHP显著优于Megatron-LM和DeepSpeed，训练吞吐量提升最高达1.36倍，在大规模NPU集群上保持接近线性的扩展效率

Conclusion: DHP能有效应对极端数据变异性，保持高硬件效率，为多模态大语言模型的长上下文扩展提供高效并行训练方案

Abstract: Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.

</details>


### [19] [A task-based data-flow methodology for programming heterogeneous systems with multiple accelerator APIs](https://arxiv.org/abs/2602.21897)
*Aleix Boné,Alejandro Aguirre,David Álvarez,Pedro J. Martinez-Ferrer,Vicenç Beltran*

Main category: cs.DC

TL;DR: 提出基于任务的数据流方法和任务感知API（TA-libs），结合nOS-V线程库，实现多加速器编程模型的无缝集成与高效协同


<details>
  <summary>Details</summary>
Motivation: 异构节点（多核CPU+多种加速器）成为HPC和AI基础设施常态，但不同加速器API（CUDA、SYCL、Triton等）各有抽象、执行语义和同步机制，组合使用易错且费力

Method: 1. 采用基于任务的数据流方法，将应用表示为DAG图；2. 引入任务感知API（TASYCL、TACUDA）将加速器调用提升为一等任务；3. 使用nOS-V统一线程管理，避免多运行时线程争用

Result: 任务感知库与nOS-V库使单个应用能透明高效地利用多种加速器编程模型，方法适用于当前异构节点，并可扩展至未来更复杂的异构系统

Conclusion: 提出的方法解决了多加速器编程模型集成难题，为异构计算提供了统一高效的编程框架，具有良好的可扩展性和实用性

Abstract: Heterogeneous nodes that combine multi-core CPUs with diverse accelerators are rapidly becoming the norm in both high-performance computing (HPC) and AI infrastructures. Exploiting these platforms, however, requires orchestrating several low-level accelerator APIs such as CUDA, SYCL, and Triton. In some occasions they can be combined with optimized vendor math libraries: e.g., cuBLAS and oneAPI. Each API or library introduces its own abstractions, execution semantics, and synchronization mechanisms. Combining them within a single application is therefore error-prone and labor-intensive. We propose reusing a task-based data-flow methodology together with Task-Aware APIs (TA-libs) to overcome these limitations and facilitate the seamless integration of multiple accelerator programming models, while still leveraging the best-in-class kernels offered by each API.
  Applications are expressed as a directed acyclic graph (DAG) of host tasks and device kernels managed by an OpenMP/OmpSs-2 runtime. We introduce Task-Aware SYCL (TASYCL) and leverage Task-Aware CUDA (TACUDA), which elevate individual accelerator invocations to first-class tasks. When multiple native runtimes coexist on the same multi-core CPU, they contend for threads, leading to oversubscription and performance variability. To address this, we unify their thread management under the nOS-V tasking and threading library, to which we contribute a new port of the PoCL (Portable OpenCL) runtime.
  These results demonstrate that task-aware libraries, coupled with the nOS-V library, enable a single application to harness multiple accelerator programming models transparently and efficiently. The proposed methodology is immediately applicable to current heterogeneous nodes and is readily extensible to future systems that integrate even richer combinations of CPUs, GPUs, FPGAs, and AI accelerators.

</details>


### [20] [Energy Efficient Federated Learning with Hyperdimensional Computing over Wireless Communication Networks](https://arxiv.org/abs/2602.21949)
*Yahao Ding,Yinchao Yang,Jiaxiang Wang,Zhaohui Yang,Dusit Niyato,Zhu Han,Mohammad Shikh-Bahaei*

Main category: cs.DC

TL;DR: 提出FL-HDC-DP框架，通过超维计算和差分隐私降低联邦学习的能耗，优化资源分配实现83.3%的节能，同时减少通信轮数。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在无线边缘网络中面临高计算成本和隐私挑战，特别是对于资源受限的用户。需要一种既能保护隐私又能降低能耗的高效解决方案。

Method: 提出FL-HDC-DP框架：1) 使用超维计算(HDC)替代复杂神经网络更新，简化本地训练；2) 应用差分隐私(DP)保护传输模型信息；3) 通过优化HDC维度、传输时间、带宽、发射功率和CPU频率来最小化总能耗。

Result: 相比基线方法，提出的框架实现了高达83.3%的总能耗降低，同时达到约90%的准确率，通信轮数比神经网络基线减少约3.5倍。

Conclusion: FL-HDC-DP框架有效解决了无线边缘联邦学习的能耗和隐私问题，通过超维计算简化计算，差分隐私保护隐私，联合资源优化显著降低能耗，为资源受限环境提供了实用解决方案。

Abstract: In this paper, we investigate a problem of minimizing total energy consumption for secure federated learning (FL) over wireless edge networks. To address the high computational cost and privacy challenges in conventional FL with neural networks (NN) for resource-constrained users, we propose a novel FL with hyperdimensional computing and differential privacy (FL-HDC-DP) framework. In the considered model, each edge user employs hyperdimensional computing (HDC) for local training, which replaces complex neural updates with simple hypervector operations, and applies differential privacy (DP) noise to protect transmitted model information. We optimize the total energy of computation and communication under both latency and privacy constraints. We formulate the problem as an optimization that minimizes the total energy of all users by jointly allocating HDC dimension, transmission time, system bandwidth, transmit power, and CPU frequency. To solve this problem, a sigmoid-variant function is proposed to characterize the relationship between the HDC dimension and the convergence rounds required to reach a target accuracy. Based on this model, we develop two alternating optimization algorithms, where closed-form expressions for time, frequency, bandwidth, and power allocations are derived at each iteration. Since the iterative algorithm requires a feasible initialization, we construct a feasibility problem and obtain feasible initial resource parameters by solving a per round transmission time minimization problem. Simulation results demonstrate that the proposed FL-HDC-DP framework achieves up to 83.3% total energy reduction compared with the baseline, while attaining about 90% accuracy in approximately 3.5X fewer communication rounds than the NN baseline.

</details>


### [21] [IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs](https://arxiv.org/abs/2602.22017)
*Chris Egersdoerfer,Arnav Sareen,Jean Luca Bez,Suren Byna,Dongkuan,Xu,Dong Dai*

Main category: cs.DC

TL;DR: IOAgent：基于LLM的HPC I/O性能自动诊断工具，通过模块化预处理、RAG领域知识集成和树状合并技术，为科学家提供专家级的I/O问题诊断和交互式分析。


<details>
  <summary>Details</summary>
Motivation: 随着HPC存储系统复杂度增加，科学家面临I/O性能优化挑战，而I/O专家资源有限，无法满足数据密集型应用需求。LLM的快速发展为构建自动化诊断工具提供了可能，但存在上下文窗口限制、领域知识缺乏和幻觉生成等问题。

Method: IOAgent采用三层架构：1) 模块化预处理器处理Darshan跟踪文件；2) RAG领域知识集成器整合HPC I/O专业知识；3) 树状合并器准确诊断I/O问题。系统提供详细诊断依据和交互式问答界面。

Result: 在首个开放诊断测试套件TraceBench上评估显示，IOAgent匹配或超越现有最先进的I/O诊断工具，提供准确有用的诊断结果。系统不依赖特定LLM，在专有和开源LLM上表现一致良好。

Conclusion: IOAgent通过系统化方法解决了LLM在HPC I/O诊断中的关键挑战，有望成为科学家应对复杂HPC I/O子系统的强大工具，提高科学家的生产力和I/O性能优化能力。

Abstract: As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.

</details>


### [22] [PASTA: A Modular Program Analysis Tool Framework for Accelerators](https://arxiv.org/abs/2602.22103)
*Mao Lin,Hyeran Jeon,Keren Zhou*

Main category: cs.DC

TL;DR: PASTA是一个低开销、模块化的加速器程序分析工具框架，通过抽象底层API和深度学习框架，提供统一接口进行多级运行时事件分析，支持快速原型开发自定义工具。


<details>
  <summary>Details</summary>
Motivation: 现代计算系统中硬件加速器日益复杂多样，需要灵活、低开销的程序分析工具来应对不同加速器和深度学习框架的挑战。

Method: PASTA框架抽象底层分析API和深度学习框架，提供统一接口捕获和分析多级运行时事件，采用可扩展设计支持快速原型开发，利用GPU加速后端降低开销。

Result: 在NVIDIA和AMD GPU上评估显示，PASTA具有广泛适用性，在NVIDIA GPU上比传统分析工具快达1.3×10^4倍，显著降低开销，同时提供详细性能洞察。

Conclusion: PASTA在可用性、可扩展性和效率之间取得了实用平衡，非常适合现代基于加速器的计算环境。

Abstract: The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program AnalysiS Tool Framework for Accelerators. PASTA abstracts over low-level profiling APIs and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. Its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. We demonstrate the utility of PASTA by developing several analysis tools, including a deep learning workload characterization tool and a UVM optimization tool. Through extensive evaluation on mainstream deep learning workloads tested on NVIDIA and AMD GPUs under both single- and multi-GPU scenarios, we demonstrate PASTA's broad applicability. On NVIDIA GPUs, we further show that PASTA provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its GPU-accelerated backend. PASTA strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.

</details>


### [23] [LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models](https://arxiv.org/abs/2602.22158)
*Minqiu Sun,Xin Huang,Luanzheng Guo,Nathan R. Tallent,Kento Sato,Dong Dai*

Main category: cs.DC

TL;DR: LLMTailor是一个检查点合并框架，通过选择性保存显著更新的层来减少大语言模型训练中的检查点开销，可将检查点大小减少4.3倍，时间加快2.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有检查点方法需要定期存储整个模型和优化器状态，导致巨大的存储开销和资源争用。研究发现LLM各层的更新高度不均匀，有些层变化显著，有些层相对稳定甚至不变，这为选择性检查点提供了机会。

Method: 提出LLMTailor检查点合并框架，通过过滤和组装不同检查点中的层来形成复合检查点。该框架支持不同的选择性检查点策略，能够对权重和优化器状态进行细粒度控制。

Result: 评估显示LLMTailor能有效减少检查点大小（如Llama3.1-8B减少4.3倍）和检查点时间（如Qwen2.5-7B加快2.8倍），同时保持模型质量。

Conclusion: LLMTailor通过利用LLM层更新的不均匀性，实现了高效的选择性检查点，解决了现有工具缺乏细粒度控制的问题，显著降低了训练中的检查点开销。

Abstract: Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI](https://arxiv.org/abs/2602.21251)
*Clemens Pohle*

Main category: cs.SE

TL;DR: AgenticTyper是一个基于LLM的智能系统，通过迭代错误修正和转译比较来为JavaScript代码库自动添加TypeScript类型，在20分钟内解决了81K行代码中的633个类型错误。


<details>
  <summary>Details</summary>
Motivation: 传统JavaScript系统缺乏类型安全，维护风险高。虽然TypeScript可以提供帮助，但手动添加类型成本高昂。现有自动化类型研究主要关注类型推断，很少解决类型检查设置、定义生成、错误识别或仓库规模的行为正确性问题。

Method: 使用基于大型语言模型（LLM）的智能代理系统，通过迭代错误修正和行为保持（通过转译比较）来填补上述研究空白。

Result: 在两个专有代码库（81K行代码）上的评估显示，AgenticTyper在20分钟内解决了所有633个初始类型错误，将手动工作量从一整天减少到几乎为零。

Conclusion: AgenticTyper有效地解决了JavaScript到TypeScript迁移中的自动化类型添加问题，显著减少了人工工作量并提高了效率。

Abstract: Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day.

</details>


### [25] [From Ad-Hoc Scripts to Orchestrated Pipelines: Architecting a Resilient ELT Framework for Developer Productivity Metrics](https://arxiv.org/abs/2602.21568)
*Yuvraj Agrawal,Pallav Jain*

Main category: cs.SE

TL;DR: 论文介绍了从临时Cron作业迁移到基于DAG编排和Medallion架构的ELT管道，以解决开发者生产力仪表板数据可靠性问题


<details>
  <summary>Details</summary>
Motivation: 开发者生产力仪表板对于可视化DevOps性能指标至关重要，但数据可靠性问题经常削弱其效用。早期平台中的临时摄取脚本（Cron作业）导致"静默故障"，数据缺口数日未被发现，侵蚀了组织信任。

Method: 从传统调度迁移到使用有向无环图（DAG）编排和Medallion架构的稳健提取-加载-转换（ELT）管道。具体包括：解耦数据提取与转换、实现不可变原始历史记录用于指标重定义、实施基于状态的依赖管理。

Result: 通过将指标管道视为生产级分布式系统，实现了可持续的工程分析。ELT管道提供了操作优势，包括更好的数据可靠性、故障检测和指标重定义能力。

Conclusion: 将指标管道视为生产级分布式系统是可持续工程分析的先决条件。从传统调度迁移到基于DAG编排的ELT管道解决了数据可靠性问题，并支持指标重定义和更好的依赖管理。

Abstract: Developer Productivity Dashboards are essential for visualizing DevOps performance metrics such as Deployment Frequency and Change Failure Rate (DORA). However, the utility of these dashboards is frequently undermined by data reliability issues. In early iterations of our platform, ad-hoc ingestion scripts (Cron jobs) led to "silent failures," where data gaps went undetected for days, eroding organizational trust. This paper reports on our experience migrating from legacy scheduling to a robust Extract-Load-Transform (ELT) pipeline using Directed Acyclic Graph (DAG) orchestration and Medallion Architecture. We detail the operational benefits of decoupling data extraction from transformation, the necessity of immutable raw history for metric redefinition, and the implementation of state-based dependency management. Our experience suggests that treating the metrics pipeline as a production-grade distributed system is a prerequisite for sustainable engineering analytics.

</details>


### [26] [Structurally Aligned Subtask-Level Memory for Software Engineering Agents](https://arxiv.org/abs/2602.21611)
*Kangning Shen,Jingyuan Zhang,Chenxi Sun,Wencong Zeng,Yang Yue*

Main category: cs.SE

TL;DR: 提出Structurally Aligned Subtask-Level Memory方法，通过细粒度子任务级记忆对齐，解决LLM软件工程代理中实例级记忆粒度不匹配问题，在SWE-bench Verified上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM软件工程代理的记忆机制通常采用粗粒度的实例级存储和检索，将整个问题解决过程作为原子单位。然而，这种粒度不匹配会导致误导性检索，特别是当表面描述相似的任务在特定阶段需要不同的推理逻辑时。

Method: 提出Structurally Aligned Subtask-Level Memory方法，将记忆的存储、检索和更新与代理的功能分解对齐。通过细粒度的子任务级记忆机制，更好地支持长时程推理。

Result: 在SWE-bench Verified上的大量实验表明，该方法始终优于普通代理和强大的实例级记忆基线，平均将Pass@1提高了4.7个百分点（如Gemini 2.5 Pro上提高6.8个百分点）。性能增益随着交互步骤的增加而增长。

Conclusion: 细粒度的子任务级记忆对齐能有效解决实例级记忆的粒度不匹配问题，利用过去的经验有益于复杂软件工程任务中的长时程推理，为LLM软件工程代理的记忆机制提供了更好的设计方向。

Abstract: Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance granularity, treating the entire problem-solving episode as the atomic unit of storage and retrieval. We empirically demonstrate that instance-level memory suffers from a fundamental granularity mismatch, resulting in misguided retrieval when tasks with similar surface descriptions require distinct reasoning logic at specific stages. To address this, we propose Structurally Aligned Subtask-Level Memory, a method that aligns memory storage, retrieval, and updating with the agent's functional decomposition. Extensive experiments on SWE-bench Verified demonstrate that our method consistently outperforms both vanilla agents and strong instance-level memory baselines across diverse backbones, improving mean Pass@1 over the vanilla agent by +4.7 pp on average (e.g., +6.8 pp on Gemini 2.5 Pro). Performance gains grow with more interaction steps, showing that leveraging past experience benefits long-horizon reasoning in complex software engineering tasks.

</details>


### [27] [Uncertainty Modeling for SysML v2](https://arxiv.org/abs/2602.21641)
*Man Zhang,Yunyang Li,Tao Yue*

Main category: cs.SE

TL;DR: 该论文提出将PSUM不确定性建模标准集成到SysML v2中的系统扩展，使系统建模能够显式表示不确定性并进行传播分析。


<details>
  <summary>Details</summary>
Motivation: 现代工程系统（如信息物理系统、自主系统、微服务系统）在动态和部分可观测环境中运行，存在固有不确定性。虽然OMG发布了PSUM不确定性建模标准，但新一代系统建模语言SysML v2缺乏与PSUM对齐的原生构造来支持不确定性表示。

Method: 提出SysML v2的系统扩展，将PSUM元模型集成到其建模框架中。该扩展能够显式指定不确定性来源、结构化表征不确定性，并在系统模型中保持不确定性传播的一致性，同时保持与SysML v2语法和语义的符合性。

Result: 通过七个案例研究验证了该方法。结果表明，提出的扩展（PSUM-SysMLv2）具有表达力，适用于不确定性感知的MBSE，并可能支持不确定性和不确定性传播分析。

Conclusion: 该研究成功地将PSUM标准集成到SysML v2中，为基于模型的系统工程提供了标准化、形式化的不确定性建模能力，填补了SysML v2在不确定性表示方面的空白。

Abstract: Uncertainty is inherent in modern engineered systems, including cyber-physical systems, autonomous systems, and large-scale software-intensive infrastructures (such as microservice-based systems) operating in dynamic and partially observable environments. The recent publication of Precise Semantics for Uncertainty Modeling (PSUM) by the Object Management Group represents the first standardized specification for uncertainty modeling within the Model-Based Systems Engineering (MBSE) community, providing formally defined semantics for representing and reasoning about uncertainty in models. In parallel, the second version of Systems Modeling Language (SysML v2) was released as the next-generation systems modeling language, offering improved semantic rigor and reusability, yet lacking native constructs aligned with PSUM for first-class uncertainty representation. This paper proposes a systematic extension of SysML v2 that incorporates the PSUM metamodel into its modeling framework. The extension enables explicit specification of indeterminacy sources, structured characterization of uncertainties, and consistent propagation of uncertainty within system models, while preserving conformance with SysML v2 syntax and semantics. We validate the approach through seven case studies. Results demonstrate that the proposed extension (PSUM-SysMLv2) is expressive and applicable for uncertainty-aware MBSE, and potentially enables uncertainty and uncertainty propagation analyses.

</details>


### [28] [AkiraRust: Re-thinking LLM-aided Rust Repair Using a Feedback-guided Thinking Switch](https://arxiv.org/abs/2602.21681)
*Renshuang Jiang,Yichong Wang,Pan Dong,Xiaoxiang Fang,Zhenling Duan,Tinglue Wang,Yuchen Hu,Jie Yu,Zhe Jiang*

Main category: cs.SE

TL;DR: AkiraRust是一个基于LLM的Rust程序修复框架，通过有限状态机和双模式推理策略实现运行时语义自适应的未定义行为修复，达到92%的语义正确性和2.2倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有Rust代码修复框架大多受限于僵化的模板或缺乏可执行语义基础，导致上下文感知有限和语义不正确。需要一种能够深度理解语义并实现准确可靠修复的方法。

Method: AkiraRust采用有限状态机动态调整检测和修复流程以适应运行时语义条件，引入双模式推理策略协调多个智能体的快慢思考，每个智能体映射到FSM状态，通过波形驱动转换控制器管理状态切换、回滚决策和语义检查点。

Result: 实验结果显示AkiraRust达到约92%的语义正确性，相比最先进技术平均加速2.2倍。

Conclusion: AkiraRust通过结合有限状态机和双模式推理，实现了上下文感知和运行时自适应的Rust程序修复，显著提高了语义正确性和修复效率。

Abstract: Eliminating undefined behaviors (UBs) in Rust programs requires a deep semantic understanding to enable accurate and reliable repair. While existing studies have demonstrated the potential of LLMs to support Rust code analysis and repair, most frameworks remain constrained by inflexible templates or lack grounding in executable semantics, resulting in limited contextual awareness and semantic incorrectness. Here, we present AkiraRust, an LLM-driven repair and verification framework that incorporates a finite-state machine to dynamically adapt its detection and repair flow to runtime semantic conditions. AkiraRust introduces a dual-mode reasoning strategy that coordinates fast and slow thinking across multiple agents. Each agent is mapped to an FSM state, and a waveform-driven transition controller manages state switching, rollback decisions, and semantic check pointing, enabling context-aware and runtime-adaptive repair. Experimental results show that AkiraRust achieves about 92% semantic correctness and delivers a 2.2x average speedup compared to SOTA.

</details>


### [29] [EditFlow: Benchmarking and Optimizing Code Edit Recommendation Systems via Reconstruction of Developer Flows](https://arxiv.org/abs/2602.21697)
*Chenyan Liu,Yun Lin,Jiaxin Chang,Jiawei Liu,Binhang Qi,Bo Jiang,Zhiyong Huang,Jin Song Dong*

Main category: cs.SE

TL;DR: EditFlow：通过重构开发者编辑流程来优化代码编辑推荐系统，解决AI辅助开发中技术准确性与开发效率脱节的问题


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码编辑方面虽然技术准确率高，但实际使用中开发者完成任务速度反而降低19%，超过68.81%的推荐会打断开发者的思维流程。问题根源在于现有方法使用静态提交快照，缺乏时间信息，导致模型优化的是最终结果而非与开发者自然推理过程一致的增量、上下文敏感的步骤。

Method: 提出EditFlow框架，通过重构开发者编辑流程来基准测试和优化后续代码编辑推荐系统。解决三个关键挑战：1) 收集反映开发者流程的编辑顺序数据；2) 创建数字孪生模拟来基准测试推荐性能；3) 为异构系统开发统一的优化策略，使所有模型具备思维流程意识。

Result: 论文未在摘要中提供具体实验结果，但提出了解决现有问题的系统框架和方法论。

Conclusion: EditFlow通过关注开发者的编辑流程而非最终结果，旨在弥合技术准确性与开发效率之间的差距，使AI代码编辑推荐系统更好地与开发者的自然思维过程保持一致。

Abstract: Large language models (LLMs) for code editing have achieved remarkable progress, yet recent empirical studies reveal a fundamental disconnect between technical accuracy and developer productivity. Despite their strong benchmark performance, developers complete tasks 19% slower when using AI assistance, with over 68.81% of recommendations disrupting their mental flow. This misalignment stems from the use of static commit snapshots that lack temporal information, causing models to optimize for end results rather than the incremental, context-sensitive steps that align with developers' natural reasoning process.
  To bridge this gap, we present EditFlow, which benchmarks and optimizes subsequent code edit recommendation systems through the reconstruction of developer editing flows. EditFlow addresses three key challenges. First, collecting edit-order data that reflects developers' flow is inherently difficult: manual annotation introduces prohibitive overhead, while development logs capture only single trajectories instead of all plausible editing flows. Second, benchmarking recommendation performance against developers' ongoing editing flow requires a digital-twin-like simulation that can faithfully simulate the editing process. Third, existing heterogeneous systems vary drastically in scale and architecture, posing challenges for developing a unified optimization strategy that endows all models with mental-flow awareness regardless of design or capability.
  ......

</details>


### [30] [Proto-ML: An IDE for ML Solution Prototyping](https://arxiv.org/abs/2602.21734)
*Selin Coban,Miguel Perez,Horst Lichter*

Main category: cs.SE

TL;DR: Proto-ML是一个专为机器学习原型开发设计的集成开发环境，通过三个扩展包解决现有工具在协作和知识复用方面的不足，提高原型开发效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习原型开发工具在协作和知识复用方面支持不足，缺乏有效的利益相关者参与、跨项目知识重用和统一工具支持，导致原型开发效率低下。

Method: Proto-ML IDE包含三个扩展包：原型实现、分析和知识管理。这些扩展支持从评估原型质量到整合利益相关者视角的全过程，提供结构化文档化和知识共享框架。

Result: 初步用户反馈表明Proto-ML能够提高原型开发效率，促进更透明和可复用的机器学习解决方案开发。

Conclusion: Proto-ML通过统一框架解决了机器学习原型开发中的关键缺陷，增强了协作和知识复用能力，为更高效的机器学习解决方案开发提供了支持。

Abstract: Prototyping plays a critical role in the development of machine learning (ML) solutions, yet existing tools often provide limited support for effective collaboration and knowledge reuse among stakeholders. This paper introduces Proto-ML, an IDE designed to strengthen ML prototyping workflows. By addressing key deficiencies such as insufficient stakeholder involvement, limited cross-project knowledge reuse, and fragmented tool support, Proto-ML offers a unified framework that enables structured documentation of prototyping activities and promotes knowledge sharing across projects.
  The Proto-ML IDE consists of three extension bundles: prototype implementation, analysis, and knowledge management. These extensions support tasks ranging from evaluating prototype quality against defined criteria to incorporating stakeholder perspectives throughout the development process. Preliminary user feedback suggests that Proto-ML can increase prototyping efficiency and foster more transparent and reusable ML solution development.

</details>


### [31] [An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention](https://arxiv.org/abs/2602.21800)
*Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 该论文研究如何改进大语言模型在长代码序列上的上下文长度外推能力，特别针对代码补全任务，通过零样本推理方法优化位置编码和注意力机制。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程自动化工具中取得了显著进展，能够执行代码生成、补全和翻译等任务，但其有效性受到固定上下文长度的限制，难以泛化到长领域特定代码序列。

Method: 研究零样本、仅推理的方法，旨在改进位置编码和优化注意力机制，以促进代码中的上下文长度外推。

Result: 论文旨在对当前促进代码上下文长度外推的方法进行全面分析，特别是在长代码补全任务背景下。

Conclusion: 通过优化位置编码和注意力机制，可以提升大语言模型在长代码序列上的泛化能力，从而改善代码相关任务的性能。

Abstract: The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.

</details>


### [32] [An Empirical Study of Bugs in Modern LLM Agent Frameworks](https://arxiv.org/abs/2602.21806)
*Xinxue Zhu,Jiacong Wu,Xiaoyu Zhang,Tianlin Li,Yanzhou Mu,Juan Zhai,Chao Shen,Yang Liu*

Main category: cs.SE

TL;DR: 对CrewAI和LangChain的998个bug报告进行实证研究，构建了15个根本原因和7个可观察症状的分类体系，发现代理框架bug主要源于API误用、API不兼容和文档不同步。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在现实应用中的广泛采用，理解底层代理框架中的bug变得至关重要。现有工作主要关注代理级故障，忽视了框架级bug，需要填补这一研究空白。

Method: 对CrewAI和LangChain的998个bug报告进行实证研究，构建了一个包含15个根本原因和7个可观察症状的分类体系，涵盖代理生命周期的五个阶段：代理初始化、感知、自我行动、相互交互和演化。

Result: 研究发现代理框架bug主要源于API误用、API不兼容和文档不同步，主要集中在"自我行动"阶段。症状通常表现为功能错误、崩溃和构建失败，反映了任务进展和控制流的中断。

Conclusion: 该研究填补了代理框架级bug研究的空白，为理解和改进LLM代理框架的可靠性提供了重要见解，强调了API设计、兼容性和文档维护的重要性。

Abstract: LLM agents have been widely adopted in real-world applications, relying on agent frameworks for workflow execution and multi-agent coordination. As these systems scale, understanding bugs in the underlying agent frameworks becomes critical. However, existing work mainly focuses on agent-level failures, overlooking framework-level bugs. To address this gap, we conduct an empirical study of 998 bug reports from CrewAI and LangChain, constructing a taxonomy of 15 root causes and 7 observable symptoms across five agent lifecycle stages: 'Agent Initialization','Perception', 'Self-Action', 'Mutual Interaction' and 'Evolution'. Our findings show that agent framework bugs mainly arise from 'API misuse', 'API incompatibility', and 'Documentation Desync', largely concentrated in the 'Self-Action' stage. Symptoms typically appear as 'Functional Error', 'Crash', and 'Build Failure', reflecting disruptions to task progression and control flow.

</details>


### [33] [From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models](https://arxiv.org/abs/2602.21833)
*Norman Peitek,Julia Hess,Sven Apel*

Main category: cs.SE

TL;DR: GPT5.1用于Java代码重构的系统研究显示，迭代重构呈现先重构后稳定的收敛模式，表明LLM对"最优可读性"代码有内在理解，且不同代码变体间收敛模式稳健。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多用于自动化代码重构，但其重构质量可能存在不一致和不可预测的行为。本文旨在系统研究LLM在代码重构方面的能力，特别关注提高代码可读性。

Method: 使用GPT5.1对230个Java代码片段进行大规模实验，每个片段在三种不同提示策略下进行五次迭代重构。将重构过程中的细粒度代码变化分为实现、语法和注释级转换，随后研究功能正确性并用新代码片段测试结果稳健性。

Result: 1. 迭代代码重构呈现先重构后稳定的收敛模式，表明LLM对"最优可读性"代码版本有内在理解；2. 收敛模式在不同代码变体间相当稳健；3. 针对特定可读性因素的明确提示对重构动态略有影响。

Conclusion: 这些发现为评估LLM辅助代码重构的可靠性提供了实证基础，为未来研究开辟了道路，包括跨模型比较分析和系统评估LLM重构代码的其他软件质量维度。

Abstract: Large language models (LLMs) are increasingly used for automated code refactoring tasks. Although these models can quickly refactor code, the quality may exhibit inconsistencies and unpredictable behavior. In this article, we systematically study the capabilities of LLMs for code refactoring with a specific focus on improving code readability.
  We conducted a large-scale experiment using GPT5.1 with 230 Java snippets, each systematically varied and refactored regarding code readability across five iterations under three different prompting strategies. We categorized fine-grained code changes during the refactoring into implementation, syntactic, and comment-level transformations. Subsequently, we investigated the functional correctness and tested the robustness of the results with novel snippets.
  Our results reveal three main insights: First, iterative code refactoring exhibits an initial phase of restructuring followed by stabilization. This convergence tendency suggests that LLMs possess an internalized understanding of an "optimally readable" version of code. Second, convergence patterns are fairly robust across different code variants. Third, explicit prompting toward specific readability factors slightly influences the refactoring dynamics.
  These insights provide an empirical foundation for assessing the reliability of LLM-assisted code refactoring, which opens pathways for future research, including comparative analyses across models and a systematic evaluation of additional software quality dimensions in LLM-refactored code.

</details>


### [34] [Enhancing LLM-Based Test Generation by Eliminating Covered Code](https://arxiv.org/abs/2602.21997)
*WeiZhe Xu,Mengyu Liu,Fanxin Kong*

Main category: cs.SE

TL;DR: 提出了一种基于LLM的可扩展单元测试生成方法，通过上下文信息检索和迭代测试生成，解决了现有方法在复杂方法上覆盖率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的测试生成方法在处理小型代码片段时表现良好，但在面对复杂方法时效果不佳，主要受限于上下文长度和推理能力下降的问题。

Method: 采用两阶段方法：1) 上下文信息检索，结合LLM和静态分析获取复杂方法的相关上下文信息；2) 迭代测试生成与代码消除，反复为代码切片生成测试，跟踪覆盖率，并选择性移除已覆盖的代码段。

Result: 在开源项目上的综合评估表明，该方法在复杂方法上超越了最先进的基于LLM和基于搜索的方法，实现了更高的覆盖率。

Conclusion: 提出的可扩展LLM单元测试生成方法通过上下文检索和迭代简化策略，有效解决了复杂方法测试生成的挑战，显著提高了覆盖率。

Abstract: Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.

</details>


### [35] [Detecting UX smells in Visual Studio Code using LLMs](https://arxiv.org/abs/2602.22020)
*Andrés Rodriguez,Juan Cruz Gardey,Alejandra Garrido*

Main category: cs.SE

TL;DR: 提出基于LLM的方法，通过挖掘GitHub用户反馈来检测VS Code中的UX异味，发现主要问题集中在信息性、清晰度、直观性和效率方面


<details>
  <summary>Details</summary>
Motivation: 集成开发环境对开发者体验至关重要，但对其可用性和用户体验的实证研究仍然有限，需要系统化的方法来识别和分类IDE中的UX问题

Method: 采用LLM辅助方法，从GitHub仓库挖掘VS Code的用户反馈问题，使用验证过的分类法和专家评审来识别和分类重复出现的UX异味

Result: 研究发现大多数UX异味集中在信息性、清晰度、直观性和效率方面，这些正是开发者最看重的质量属性

Conclusion: LLM辅助方法能有效检测IDE中的UX问题，为改进开发者体验提供了实证基础，未来可扩展应用到其他开发工具

Abstract: Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.

</details>


### [36] [Visual Milestone Planning in a Hybrid Development Context](https://arxiv.org/abs/2602.22076)
*Eduardo Miranda*

Main category: cs.SE

TL;DR: VMP是一种可视化里程碑规划方法，使用敏捷术语帮助敏捷实践者采用，作为混合开发流程的前端。它通过团队直接操作规划构件来促进对工作方法的共同理解和承诺。


<details>
  <summary>Details</summary>
Motivation: 为敏捷实践者提供一种易于采用的混合开发流程规划方法，通过可视化协作促进团队对工作方法的共同理解和承诺。

Method: 1) 建立产品待办事项列表和识别相关里程碑；2) 使用里程碑规划矩阵记录产品待办事项到里程碑的分配；3) 将代表工作的便签分组到称为工作包的时间盒中；4) 在资源和时间缩放的调度画布上安排工作包，类似俄罗斯方块游戏。

Result: 提出了一种结合敏捷和传统规划方法的可视化协作规划框架，使团队能够直观地创建和管理开发计划。

Conclusion: VMP方法为敏捷团队提供了一种有效的可视化规划工具，促进了团队协作和共同理解，可作为混合开发流程的前端规划方法。

Abstract: This paper explains the Visual Milestone Planning (VMP) method using an agile vocabulary to facilitate its adoption by agile practitioners as a front end for a hybrid development process. VMP is a visual and collaborative planning approach which promotes a shared understanding of the work approach and commitment through the direct manipulation by team members of the reified planning constructs involved in the development of the plan. Once the product backlog has been established and relevant milestones identified, a novel construct called the milestone planning matrix is used to document the allocation of product backlog items to milestones. The milestones due dates are later determined by grouping sticky notes representing the work to be performed into time-boxes called work packages and accommodating them on a resource and time scaled scheduling canvas very much as it would be done in a Tetris game.

</details>


### [37] [SWE-Protégé: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents](https://arxiv.org/abs/2602.22124)
*Patrick Tser Jern Kon,Archana Pradeep,Ang Chen,Alexander P. Ellis,Warren Hunt,Zijian Wang,John Yang,Samuel Thompson*

Main category: cs.SE

TL;DR: SWE-Protégé：一个通过专家-学徒协作框架提升小语言模型在软件工程任务上性能的后训练方法


<details>
  <summary>Details</summary>
Motivation: 小语言模型在成本和延迟方面有优势，但在SWE-bench等长视野软件工程任务上表现不佳，存在动作循环和低解决率问题。需要一种方法让SLM既能保持决策自主性，又能有效利用专家模型的指导。

Method: 提出SWE-Protégé框架，将软件修复重构为专家-学徒协作问题。SLM作为唯一决策者，学习选择性向专家模型寻求指导、识别停滞状态、并遵循专家反馈。结合专家增强轨迹的监督微调和强化学习，明确抑制退化循环和无成效的专家协作。

Result: 在Qwen2.5-Coder-7B-Instruct上轻量后训练，在SWE-bench Verified上达到42.4% Pass@1，比之前SLM最佳结果提升25.4%，同时稀疏使用专家协助（约4次调用/任务，占总token的11%）。

Conclusion: SWE-Protégé框架有效解决了SLM在软件工程任务中的动作循环问题，通过智能的专家协作机制显著提升了性能，同时保持了SLM的成本和延迟优势。

Abstract: Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Protégé, a post-training framework that reframes software repair as an expert-protégé collaboration problem. In SWE-Protégé, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).

</details>
