<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [FlashMap: A Flash Optimized Key-Value Store](https://arxiv.org/abs/2511.08826)
*Zonglin Guo,Tony Givargis*

Main category: cs.DB

TL;DR: FlashMap是一个针对闪存固态硬盘优化的高性能键值存储系统，在单台数据中心级服务器上实现了出色的吞吐量性能。


<details>
  <summary>Details</summary>
Motivation: 随着现代计算对响应性和可扩展性需求的增长，键值存储已成为数据基础设施的关键组件。闪存固态硬盘的普及为优化键值存储性能提供了新的机会。

Method: 开发了FlashMap，一个专门为闪存固态硬盘优化的键值存储系统，通过针对SSD特性进行设计优化。

Result: 实验结果显示，FlashMap在100字节负载下平均实现了1980万次插入和2380万次随机查找每秒的吞吐量。

Conclusion: FlashMap证明了针对闪存固态硬盘专门优化的键值存储系统能够实现卓越的性能，为高性能应用提供了有效的解决方案。

Abstract: Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.

</details>


### [2] [Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration](https://arxiv.org/abs/2511.09001)
*Yuka Haruki,Shigeru Ishikura,Kazuya Demachi,Teruaki Hayashi*

Main category: cs.DB

TL;DR: 提出了一种结合表格结构信息和上下文元素（列描述、外部知识）的图嵌入技术，用于数据集成中的模式匹配和实体解析任务，在多种数据集特性下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据集成需求增长，但现有自动化方法对数据集特性的影响研究不足，且方法组合有限。需要更有效处理不同数据集特性的集成方法。

Method: 开发了上下文图嵌入技术，整合表格数据的结构细节和上下文元素（列描述、外部知识），在具有不同特性（领域特异性、数据规模、缺失率、重叠率）的数据集上进行测试。

Result: 该方法在多种数据集特性下持续超越现有图基方法，特别是在数值比例高或缺失数据多的困难场景中表现优异。但在语义相似但不同的列上仍存在失败案例。

Conclusion: 上下文嵌入提高了匹配可靠性，数据集特性显著影响集成结果。这些发现有助于开发支持实际企业应用的实用数据集成系统。

Abstract: As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications.

</details>


### [3] [Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking](https://arxiv.org/abs/2511.09052)
*Yu Wang,Hui Wang,Jiake Ge,Xin Wang*

Main category: cs.DB

TL;DR: 提出了三种核心创新来扩展GNN-PE到分布式系统：动态负载均衡机制、多GPU协作缓存策略和查询计划排序方法，显著提升分布式子图匹配效率。


<details>
  <summary>Details</summary>
Motivation: 现有GNN-PE框架在单机上能实现高效精确匹配，但缺乏针对分布式环境的可扩展性和优化，无法满足大规模图上的计算需求。

Method: 1) 轻量级动态相关性感知负载均衡和热迁移机制；2) 基于在线增量学习的多GPU协作动态缓存策略；3) 基于支配嵌入剪枝潜力的查询计划排序方法；结合METIS分区和并行离线预处理。

Result: 在分布式场景（数十台机器）中实现了"最小边切割+负载均衡+不间断查询"，显著提高了分布式子图匹配的效率和稳定性。

Conclusion: 通过三项核心技术创新，成功将GNN-PE框架扩展到分布式系统，解决了大规模图上精确子图匹配的挑战。

Abstract: Exact subgraph matching on large-scale graphs remains a chal- lenging problem due to high computational complexity and dis- tributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed envi- ronments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, mem- ory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves "minimum edge cut + load balancing + non-interruptible queries" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.

</details>


### [4] [CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System](https://arxiv.org/abs/2511.09262)
*Jiaping Cao,Ting Sun,Man Lung Yiu,Xiao Yan,Bo Tang*

Main category: cs.DB

TL;DR: CheetahGIS是一个基于Apache Flink Stateful Functions构建的可扩展高效系统，用于处理海量移动对象上的流式空间查询。


<details>
  <summary>Details</summary>
Motivation: 现有系统在处理大量移动对象和实时空间查询时存在局限性，需要构建一个可扩展且高效的系统来应对这些挑战。

Method: 采用模块化架构，基于Apache Flink Stateful Functions构建，使用轻量级全局网格索引、元数据同步策略和负载均衡机制等优化技术。

Result: 系统实现了优秀的可扩展性，能够高效处理三种代表性流式查询（对象查询、范围计数查询和k近邻查询）。

Conclusion: CheetahGIS通过模块化设计和优化策略，成功解决了海量移动对象实时空间查询的可扩展性和效率问题。

Abstract: Spatial data analytics systems are widely studied in both the academia and industry. However, existing systems are limited when handling a large number of moving objects and real time spatial queries. In this work, we architect a scalable and efficient system CheetahGIS to process streaming spatial queries over massive moving objects. In particular, CheetahGIS is built upon Apache Flink Stateful Functions (StateFun), an API for building distributed streaming applications with an actor-like model. CheetahGIS enjoys excellent scalability due to its modular architecture, which clearly decomposes different components and allows scaling individual components. To improve the efficiency and scalability of CheetahGIS, we devise a suite of optimizations, e.g., lightweight global grid-based index, metadata synchroniza tion strategies, and load balance mechanisms. We also formulate a generic paradigm for spatial query processing in CheetahGIS, and verify its generality by processing three representative streaming queries (i.e., object query, range count query, and k nearest neighbor query). We conduct extensive experiments on both real and synthetic datasets to evaluate CheetahGIS.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Triage in Software Engineering: A Systematic Review of Research and Practice](https://arxiv.org/abs/2511.08607)
*Yongxin Zhao,Shenglin Zhang,Yujia Wu,Yuxin Sun,Yongqian Sun,Dan Pei,Chetan Bansal,Minghua Ma*

Main category: cs.SE

TL;DR: 本文对2004年至今的234篇论文进行了全面综述，深入分析了软件系统故障分诊的基本概念、系统架构和问题陈述，总结了开源数据集和评估指标，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统复杂度的增长，故障分诊已成为系统运维的基本流程。海量异构数据使得有效的故障分诊对于维护系统可靠性、可维护性和快速响应问题变得不可或缺。

Method: 通过比较学术和工业研究的不同目标，分析工业实践的实证研究，识别限制故障分诊系统实际部署的主要障碍。总结广泛采用的开源数据集和评估指标。

Result: 识别了限制故障分诊系统实际部署的主要障碍，提供了统一的分诊有效性测量视角，所有综述论文和项目可在GitHub仓库获取。

Conclusion: 本文为实践者提供了方法选择和性能评估的指导，并指出了促进学术创新与工业应用更紧密结合的未来方向和新兴机会。

Abstract: As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.

</details>


### [6] [Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis](https://arxiv.org/abs/2511.08644)
*Punit Kumar,Asif Imran,Tevfik Kosar*

Main category: cs.SE

TL;DR: 比较Pandas、Polars和Dask三个Python数据处理库在深度学习训练和推理管道中的性能表现


<details>
  <summary>Details</summary>
Motivation: 研究这些库在GPU工作负载下的交互表现，填补现有文献在数据加载、预处理和批次供给等关键阶段的性能分析空白

Method: 测量运行时、内存使用、磁盘使用和能耗等关键性能指标，使用多种机器学习模型和数据集进行测试

Result: 未在摘要中明确说明具体结果

Conclusion: 未在摘要中明确说明具体结论

Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.

</details>


### [7] [An insight into the technical debt-fix trade off in software backporting](https://arxiv.org/abs/2511.09000)
*Jarin Tasnim,Debasish Chakroborti,Chanchal K. Roy,Kevin A. Schneider*

Main category: cs.SE

TL;DR: 该研究分析了87个仓库中105,396个回移植提交的技术债务，发现约4.3%的回移植引入了新的技术债务，不同生态系统和开发阶段的技术债务模式存在差异。


<details>
  <summary>Details</summary>
Motivation: 研究回移植过程中技术债务的产生，识别何时以及为何在稳定源代码的回移植过程中会产生新的技术债务。

Method: 分析了来自Apache、Eclipse和Python三个软件生态系统中87个仓库的31,076个回移植源头的105,396个提交。

Result: 约4.3%的回移植引入了新的技术债务；Apache贡献了最多的绝对实例，而Python和Eclipse的债务与提交比率几乎是Apache的三倍；不同生态系统在不同发布阶段的技术债务模式不同。

Conclusion: 回移植过程中确实会产生技术债务，特别是在经验不足、工作负荷高或非所有者的开发者进行回移植时更容易引入技术债务。

Abstract: Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.

</details>


### [8] [Test Plan Generation for Live Testing of Cloud Services](https://arxiv.org/abs/2511.09038)
*Oussama Jebbar,Ferhat Khendek,Maria Toeroe*

Main category: cs.SE

TL;DR: 提出一种自动化测试计划生成方法，旨在减少生产环境中测试活动可能引起的服务中断


<details>
  <summary>Details</summary>
Motivation: 手动设计测试计划繁琐且容易出错，特别是在大型复杂系统中更为困难。生产环境中的测试需要在不对生产流量造成不可接受干扰的情况下进行

Method: 提出自动化测试计划生成方法，包括测试配置选择/生成、测试配置部署规划、创建测试运行计划、选择减少干扰风险的策略等

Result: 通过案例研究展示了该方法的不同方面

Conclusion: 自动化测试计划生成可以减少生产环境中测试活动引起的服务中断

Abstract: Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.

</details>


### [9] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: 开发了一个针对工业PLC编程的低数据领域代码助手，通过RAG、多模型竞争和即时编译验证，无需微调大模型即可生成高质量代码


<details>
  <summary>Details</summary>
Motivation: PLC使用专有代码方言，难以训练代码助手；现有LLM不了解特定功能块和项目代码；企业对云服务不信任，需要本地化解决方案

Method: 使用检索增强生成(RAG)、多模型竞争、推理和自动纠错，通过即时编译验证代码有效性，对小模型进行微调用于边缘设备

Result: 通过广泛的提示工程和定向检索，在低数据领域实现了高质量的代码生成，提供了代码编译统计和用户评分评估

Conclusion: RAG支持的代码助手可以在低数据领域工作，通过提示工程和定向检索实现有效代码生成

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [10] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: 提出SPLVD方法，通过自步学习从易到难动态选择训练数据，提升软件漏洞检测准确率


<details>
  <summary>Details</summary>
Motivation: 现有深度学习漏洞检测方法准确率有限，主要原因是训练数据质量低

Method: SPLVD使用专门设计的数据选择器，根据训练阶段动态选择源代码，模拟人类从易到难的学习过程

Result: 在三个基准数据集上分别达到89.2%、68.7%和43.5%的F1分数，在OpenHarmony项目中达到90.9%的精确率

Conclusion: SPLVD能有效提升软件漏洞检测性能，具有实际应用价值

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [11] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: AILINKPREVIEWER工具利用LLM生成PR中链接的预览，通过对比三种方法发现上下文摘要方法在指标上表现最佳，但用户研究显示大多数参与者更喜欢非上下文摘要。


<details>
  <summary>Details</summary>
Motivation: 代码审查中链接通常被丢弃，限制了信息的丰富性并增加了认知负担，需要工具来提供链接预览以增强代码审查效率。

Method: 开发AILINKPREVIEWER工具，利用LLM基于PR元数据（标题、描述、评论和链接内容）生成链接预览，比较了上下文LLM摘要、非上下文LLM摘要和基于元数据的预览三种方法。

Result: 上下文摘要在BLEU、BERTScore和压缩比等指标上表现最佳，但用户研究中大多数参与者更喜欢非上下文摘要，表明指标性能与感知可用性之间存在权衡。

Conclusion: LLM驱动的链接预览有潜力提高代码审查效率，为开发人员和软件工程自动化提供更丰富的上下文。

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [12] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: 本研究探索使用大型语言模型辅助用例建模，通过集成开源LLM和高级提示工程技术从软件需求中提取参与者和用例，实验显示建模时间减少60%且质量相当。


<details>
  <summary>Details</summary>
Motivation: 手动创建用例模型耗时费力，实践中常被跳过，需要自动化方法来提高效率。

Method: 集成开源LLM，采用高级提示工程技术从软件需求中系统提取参与者和用例，并与传统手动方法进行对比研究。

Result: 建模时间减少60%，模型质量与传统方法相当，参与者认为该方法在过程中提供了有价值的指导。

Conclusion: LLM辅助的用例建模方法能显著提高效率，同时保持质量，为软件工程实践提供了有价值的自动化支持。

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [13] [Decoding the Configuration of AI Coding Agents: Insights from Claude Code Projects](https://arxiv.org/abs/2511.09268)
*Helio Victor F. Santos,Vitor Costa,Joao Eduardo Montandon,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 对Claude Code配置文件的实证研究，分析了328个配置文件中的软件工程关注点和实践模式。


<details>
  <summary>Details</summary>
Motivation: 虽然智能代码助手承诺提升生产力，但其行为和效果严重依赖配置文件，而对这些配置工件的结构和内容了解甚少。

Method: 收集并分析328个公开Claude Code项目的配置文件，识别其中指定的软件工程关注点、实践以及这些关注点在单个文件中的共现模式。

Result: 研究强调了在智能体配置文件中定义广泛关注点和实践的重要性，特别是指定智能体应遵循的架构。

Conclusion: 配置文件中需要明确定义各种软件工程关注点，其中架构规范尤为重要。

Abstract: Agentic code assistants are a new generation of AI systems capable of performing end-to-end software engineering tasks. While these systems promise unprecedented productivity gains, their behavior and effectiveness depend heavily on configuration files that define architectural constraints, coding practices, and tool usage policies. However, little is known about the structure and content of these configuration artifacts. This paper presents an empirical study of the configuration ecosystem of Claude Code, one of the most widely used agentic coding systems. We collected and analyzed 328 configuration files from public Claude Code projects to identify (i) the software engineering concerns and practices they specify and (ii) how these concerns co-occur within individual files. The results highlight the importance of defining a wide range of concerns and practices in agent configuration files, with particular emphasis on specifying the architecture the agent should follow.

</details>


### [14] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: Routesplain是首个针对软件相关任务的LLM路由器，通过提取可解释概念进行路由决策，在准确性和成本方面优于单个模型，并达到或超过所有黑盒基线。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在软件相关任务上表现差异显著，通过路由查询到合适的LLM可以提高响应质量并降低成本，但现有工作主要关注通用LLM路由，缺乏针对软件任务的专用路由器。

Method: Routesplain从每个查询中提取人类可解释的概念（如任务类型、领域、推理复杂度），仅基于这些概念进行路由决策，提供可理解的解释依据。

Result: 在16个最先进LLM和8个软件相关任务上的评估显示，Routesplain在准确性和成本方面均优于单个模型，且等于或超过所有黑盒基线方法。

Conclusion: Routesplain证明了基于可解释概念的路由方法在软件任务中的有效性，概念级干预为路由器的进一步改进指明了方向。

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: 首个通过MLIR中的OpenMP目标指令实现选择性代码卸载到FPGA的实现，结合MLIR OpenMP方言和HLS方言提供可移植的FPGA编译流程。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律放缓，FPGA等异构计算平台在加速HPC工作负载方面日益受到关注，需要更灵活的FPGA加速方案。

Method: 将MLIR OpenMP方言与高级综合(HLS)方言结合，支持任何MLIR兼容前端(如Flang)，利用现有MLIR构建模块减少开发工作量。

Result: 实现了基于指令的FPGA加速，支持通过标准OpenMP指令手动优化卸载内核，展示了MLIR生态系统的可组合性优势。

Conclusion: 建立了一个灵活且可扩展的基于指令的FPGA加速路径，集成在MLIR生态系统中。

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [16] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: 数据中心通过能源资源解耦电力容量和电网负载来创建灵活性，优化分布和管理可显著降低电网碳排放，经济上可行但需要电网干预。


<details>
  <summary>Details</summary>
Motivation: AI和云数据中心的高能耗加剧了碳足迹问题，数据中心恒定电力需求与波动性可再生能源的矛盾阻碍电网脱碳，需要数据中心灵活性来改善可再生能源消纳。

Method: 定义和计算数据中心负载解耦的功率和能量需求，评估解耦资源的分布和管理方法，包括站点差异和电网合作（单向信息共享与双向共享控制）。

Result: 优化分布可实现98%的潜在电网碳减排，仅需70%的总解耦需求；电网合作管理使碳减排效果提升1.4倍；经济上平均收益大于本地解耦成本，但站点间差异需要电网干预。

Conclusion: 数据中心负载解耦是降低碳足迹的有效策略，优化分布和电网合作能最大化效益，经济可行性高但需要政策支持解决站点差异问题。

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [17] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: 评估四大云提供商在HPC CPU性能和成本方面的表现，AWS性能最佳但价格昂贵，OCI最经济但速度较慢，ARM实例在不同提供商间表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 比较虚拟化云基础设施中HPC风格的CPU性能和成本，帮助用户根据工作负载优先级选择最优实例类型。

Method: 使用SPEC ACCEL套件中的OpenMP工作负载，在AWS、Azure、GCP和OCI四大云提供商上测试Intel、AMD和ARM实例类型，对比按需和一年折扣定价。

Result: AWS在所有三种实例类型中运行时间最短但收费最高；OCI在所有CPU系列中最经济但运行较慢；Azure表现中等；GCP从Intel切换到AMD时性能显著提升，但ARM实例比其AMD实例慢两倍且更贵；AWS的ARM实例比其Intel和AMD实例快49%。

Conclusion: 实例选择和提供商选择在运行时间和价格上产生显著差异，应根据工作负载优先级（原始速度或成本最小化）指导实例类型决策。

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [18] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: 提出了企业级隐私保护联邦学习框架APPFL的设计愿景，旨在解决从本地原型到分布式部署的规模化挑战，支持跨异构计算环境的无缝扩展。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在科学领域具有重要应用价值，但现有框架难以同时满足用户友好性、可扩展性和隐私保护要求，特别是在从本地原型到分布式部署的过渡中存在挑战。

Method: 基于APPFL框架开发经验，提出企业级FL框架的关键能力：可扩展本地仿真、无缝部署过渡、跨异构基础设施部署、多级抽象、全面隐私安全保护。

Result: 提出了实现企业级隐私保护联邦学习框架的架构设计，能够支持从个人设备到云集群和HPC系统的分布式部署。

Conclusion: 该框架旨在弥合研究原型与企业级部署之间的差距，为科学领域提供可扩展、可靠且隐私保护的人工智能解决方案。

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [19] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG是一个软件框架，通过将MIG的一对一分配模型改为一对多，利用主机共享内存实现跨MIG实例的集合通信，无需硬件修改，显著提升GPU集群利用率。


<details>
  <summary>Details</summary>
Motivation: 多租户GPU集群中，NVIDIA MIG技术的硬件刚性和传统一对一分配模型导致严重的资源碎片化和集群利用率低下问题。

Method: 提出Flex-MIG软件框架，采用一对多分配模型，通过主机共享内存实现跨MIG实例的集合通信，无需硬件修改。

Result: 消除了需要排空的重配置操作，减少了碎片化，在多样化跟踪测试中使makespan提升高达17%。

Conclusion: 将MIG的操作模型重新设计为软件协调层，可以显著提高集群效率。

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [20] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: 本文提出了一种新的用户空间任务调度方法CES，通过结合交换调度来优化协程同步性能，相比现有方法能带来3-8倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代编程语言广泛支持协程进行高并行或异步应用，用户空间同步避免了重量级系统调用，但现有同步原语仍采用内核级调度视角，引入了不必要的延迟，限制了吞吐量。

Method: 开发了Combine-and-Exchange Scheduling (CES)方法，确保竞争临界区保持在同一个执行线程上，同时将可并行工作均匀分配到其他线程。

Result: 该方法可应用于多种现有语言和库，在应用基准测试中实现了3倍性能提升，在微基准测试中实现了8倍性能提升。

Conclusion: CES方法重新思考了用户空间任务的同步机制，通过优化调度策略显著提升了协程同步性能。

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [21] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: 提出了Cyclic Memory Protection (CMP)方法，在保持严格FIFO语义、无界容量和锁自由进度的同时，消除了协调开销，显著提升了高并发场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无锁队列实现在并发环境下变得异常复杂，协调机制往往主导设计，保护开销在高并发AI工作负载中成为主要瓶颈。

Method: 采用循环内存保护(CMP)机制，通过有界保护窗口提供实用的回收保证，无需协调即可实现严格FIFO队列。

Result: CMP在高度竞争环境下比最先进的无锁队列性能提升1.72-4倍，并能扩展到数百个线程。

Conclusion: 高并发队列可以在不削弱队列语义的情况下回归其基本简单性，CMP为此提供了可行方案。

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [22] [SPADA: A Spatial Dataflow Architecture Programming Language](https://arxiv.org/abs/2511.09447)
*Lukas Gianinazzi,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.DC

TL;DR: SPADA是一个针对空间数据流架构的编程语言，通过抽象底层架构细节，提供对数据放置、数据流模式和异步操作的精确控制，显著减少代码量并实现近理想的弱扩展性。


<details>
  <summary>Details</summary>
Motivation: 空间数据流架构在AI和科学计算中表现出色，但编程困难，需要显式协调数据移动和异步计算。现有编程模型忽视了这些架构的独特能力，特别是高效的数据流和复杂路由管理。

Method: 提出SPADA编程语言，建立严格的数据流语义框架，定义路由正确性、数据竞争和死锁，并设计针对Cerebras CSL的多级降低编译器。

Result: SPADA将复杂并行模式的代码量减少6-8倍，在三个数量级上实现近理想的弱扩展性，并作为GT4Py stencil DSL的高级编程接口和中间表示。

Conclusion: SPADA通过统一空间数据流架构的编程模型，推进了这些高性能计算平台的理论基础和实践可用性。

Abstract: Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.
  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.
  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.

</details>


### [23] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: 使用CSP过程代数对Python联邦学习测试床中的第三个通用算法（TDM通信）进行形式化验证，通过模型检查器PAT证明其死锁自由性和成功终止性


<details>
  <summary>Details</summary>
Motivation: 为Python联邦学习测试床框架中的第三个通用算法（TDM通信）提供形式化验证，确保其正确性和可靠性

Method: 采用两阶段方法：第一阶段构建忠实反映Python代码的CSP模型，第二阶段使用模型检查器PAT自动验证算法的安全性和活性属性

Result: 成功证明了第三个通用算法的死锁自由性（安全性）和成功终止性（活性）

Conclusion: 通过形式化验证方法证实了Python联邦学习测试床中TDM通信算法的正确性

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>
