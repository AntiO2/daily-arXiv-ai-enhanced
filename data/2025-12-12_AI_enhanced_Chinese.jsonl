{"id": "2512.09942", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09942", "abs": "https://arxiv.org/abs/2512.09942", "authors": ["Zhiming Liang", "Bin Chen", "Litao Ye", "Chen Sun", "Shuo Wang", "Zhe Peng"], "title": "A study of the spectrum resource leasing method based on ERC4907 extension", "comment": null, "summary": "The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.", "AI": {"tldr": "M-ERC4907\u6269\u5c55\u4e86ERC4907\u6807\u51c6\uff0c\u652f\u6301\u591a\u65f6\u95f4\u6bb5\u6279\u91cf\u914d\u7f6e\u548c\u591a\u7528\u6237\u540c\u65f6\u6388\u6743\uff0c\u89e3\u51b3\u4e86\u5355\u7528\u6237\u5355\u65f6\u95f4\u6bb5\u6388\u6743\u7684\u9650\u5236\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u94fe\u4e0a\u4ea4\u6613\u548cGas\u6d88\u8017\u3002", "motivation": "ERC4907\u6807\u51c6\u867d\u7136\u652f\u6301\u53ef\u79df\u8d41NFT\uff0c\u4f46\u4ec5\u9650\u4e8e\u5355\u7528\u6237\u3001\u5355\u65f6\u95f4\u6bb5\u6388\u6743\uff0c\u8fd9\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u65f6\u95f4\u6bb5\u8c03\u5ea6\u573a\u666f\u4e2d\u4e25\u91cd\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faM-ERC4907\u6269\u5c55\u65b9\u6cd5\uff0c\u5f15\u5165\u652f\u6301\u591a\u65f6\u95f4\u6bb5\u6279\u91cf\u914d\u7f6e\u548c\u591a\u7528\u6237\u540c\u65f6\u6388\u6743\u7684\u65b0\u529f\u80fd\uff0c\u6d88\u9664ERC4907\u7684\u4e25\u683c\u987a\u5e8f\u6388\u6743\u7ea6\u675f\u3002", "result": "\u5728Remix\u5f00\u53d1\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cM-ERC4907\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u94fe\u4e0a\u4ea4\u6613\u548c\u603b\u4f53Gas\u6d88\u8017\uff0c\u63d0\u5347\u4e86\u53ef\u6269\u5c55\u6027\u548c\u8d44\u6e90\u5206\u914d\u6548\u7387\u3002", "conclusion": "M-ERC4907\u6269\u5c55\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ERC4907\u5728\u591a\u65f6\u95f4\u6bb5\u8c03\u5ea6\u573a\u666f\u4e2d\u7684\u9650\u5236\uff0c\u901a\u8fc7\u6279\u91cf\u914d\u7f6e\u548c\u5e76\u884c\u6388\u6743\u673a\u5236\u63d0\u9ad8\u4e86NFT\u79df\u8d41\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.09946", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09946", "abs": "https://arxiv.org/abs/2512.09946", "authors": ["Hung-Yueh Chiang", "Bokun Wang", "Diana Marculescu"], "title": "ELANA: A Simple Energy and Latency Analyzer for LLMs", "comment": null, "summary": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.", "AI": {"tldr": "ELANA\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u8f7b\u91cf\u7ea7LLM\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5927\u5c0f\u3001KV\u7f13\u5b58\u3001\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u652f\u6301Hugging Face\u6240\u6709\u516c\u5f00\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u591aGPU\u548c\u8fb9\u7f18GPU\u5e73\u53f0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u5230\u4e91GPU\u96c6\u7fa4\u7684\u5404\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u5ef6\u8fdf\u548c\u529f\u8017\u662f\u4e3b\u8981\u7ea6\u675f\uff0c\u9700\u8981\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u6765\u4f18\u5316\u6a21\u578b\u90e8\u7f72\u6548\u7387\u548c\u4e0b\u4e00\u4ee3\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u5b66\u672f\u53cb\u597d\u7684\u6027\u80fd\u5206\u6790\u5de5\u5177ELANA\uff0c\u652f\u6301\u5206\u6790\u6a21\u578b\u5927\u5c0f\u3001KV\u7f13\u5b58\u5927\u5c0f\u3001\u9884\u586b\u5145\u5ef6\u8fdf(TTFT)\u3001\u751f\u6210\u5ef6\u8fdf(TPOT)\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf(TTLT)\uff0c\u652f\u6301Hugging Face\u6240\u6709\u516c\u5f00\u6a21\u578b\uff0c\u63d0\u4f9b\u547d\u4ee4\u884c\u754c\u9762\u548c\u53ef\u9009\u7684\u80fd\u8017\u65e5\u5fd7\u8bb0\u5f55\u3002", "result": "\u5f00\u6e90\u4e86ELANA\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u591aGPU\u548c\u8fb9\u7f18GPU\u5e73\u53f0\uff0c\u5b8c\u5168\u517c\u5bb9Hugging Face API\uff0c\u53ef\u8f7b\u677e\u5b9a\u5236\u6216\u9002\u914d\u538b\u7f29\u6216\u4f4e\u6bd4\u7279\u6a21\u578b\uff0c\u9002\u5408\u9ad8\u6548LLM\u7814\u7a76\u6216\u5c0f\u89c4\u6a21\u6982\u5ff5\u9a8c\u8bc1\u3002", "conclusion": "ELANA\u662f\u4e00\u4e2a\u7b80\u5355\u5b9e\u7528\u7684LLM\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u4f18\u5316\u6a21\u578b\u90e8\u7f72\u6548\u7387\u548c\u4fc3\u8fdb\u9ad8\u6548LLM\u7814\u7a76\uff0c\u7279\u522b\u9002\u5408\u5b66\u672f\u7814\u7a76\u548c\u5c0f\u89c4\u6a21\u6982\u5ff5\u9a8c\u8bc1\u3002"}}
{"id": "2512.09957", "categories": ["cs.DC", "cs.CR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09957", "abs": "https://arxiv.org/abs/2512.09957", "authors": ["Bethel Hall", "Owen Ungaro", "William Eiers"], "title": "CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models", "comment": "10 pages", "summary": "Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demon- strated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.", "AI": {"tldr": "CloudFix\uff1a\u9996\u4e2a\u7ed3\u5408\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0eLLM\u7684\u4e91\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u81ea\u52a8\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u6545\u969c\u5b9a\u4f4d\u548cLLM\u751f\u6210\u4fee\u590d\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u4fee\u590d\u51c6\u786e\u7387\u3002", "motivation": "\u4e91\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7684\u624b\u52a8\u7f16\u5199\u548c\u66f4\u65b0\u5bb9\u6613\u51fa\u9519\u4e14\u8017\u65f6\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u6f0f\u6d1e\u3002\u73b0\u6709\u7b26\u53f7\u5206\u6790\u65b9\u6cd5\u5728\u4e91\u73af\u5883\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800cLLM\u5728\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u4fee\u590d\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\u3002", "method": "CloudFix\u7ed3\u5408\u5f62\u5f0f\u5316\u65b9\u6cd5\u4e0eLLM\uff1a\u9996\u5148\u4f7f\u7528\u57fa\u4e8e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7684\u6545\u969c\u5b9a\u4f4d\u8bc6\u522b\u7b56\u7565\u4e2d\u7684\u9519\u8bef\u8bed\u53e5\uff0c\u7136\u540e\u5229\u7528LLM\u751f\u6210\u6f5c\u5728\u4fee\u590d\u65b9\u6848\uff0c\u6700\u540e\u901a\u8fc7SMT\u6c42\u89e3\u5668\u9a8c\u8bc1\u4fee\u590d\u7684\u6b63\u786e\u6027\u3002", "result": "\u5728\u5305\u542b282\u4e2a\u771f\u5b9eAWS\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cCloudFix\u5728\u4e0d\u540c\u8bf7\u6c42\u89c4\u6a21\u4e0b\u5747\u4f18\u4e8e\u57fa\u7ebf\u5b9e\u73b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4fee\u590d\u51c6\u786e\u7387\u3002", "conclusion": "CloudFix\u662f\u9996\u4e2a\u5229\u7528LLM\u8fdb\u884c\u7b56\u7565\u4fee\u590d\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86LLM\u5728\u8bbf\u95ee\u63a7\u5236\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4e91\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\u7684\u9ad8\u6548\u81ea\u52a8\u5316\u4fee\u590d\u3002"}}
{"id": "2512.09961", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09961", "abs": "https://arxiv.org/abs/2512.09961", "authors": ["Jinyu Chen", "Long Shi", "Taotao Wang", "Jiaheng Wang", "Wei Zhang"], "title": "TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0", "comment": null, "summary": "The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.", "AI": {"tldr": "\u63d0\u51faTDC-Cache\u6846\u67b6\uff0c\u7ed3\u5408DRL-DC\u548cPoCL\u5171\u8bc6\uff0c\u89e3\u51b3Web3.0\u53bb\u4e2d\u5fc3\u5316\u7f13\u5b58\u4e2d\u7684\u6548\u7387\u548c\u5b89\u5168\u95ee\u9898", "motivation": "Web3.0\u4ece\u4e2d\u5fc3\u5316\u8f6c\u5411\u53bb\u4e2d\u5fc3\u5316\u7ed3\u6784\uff0c\u8d4b\u4e88\u7528\u6237\u6570\u636e\u81ea\u4e3b\u6743\uff0c\u4f46\u9762\u4e34\u5197\u4f59\u6570\u636e\u590d\u5236\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\u548c\u6570\u636e\u4e0d\u4e00\u81f4\u5e26\u6765\u7684\u5b89\u5168\u6f0f\u6d1e", "method": "\u5f00\u53d1TDC-Cache\u4e24\u5c42\u67b6\u6784\uff1aDON\u5c42\u4f5c\u4e3a\u53ef\u4fe1\u4e2d\u4ecb\u5e73\u53f0\uff1b\u63d0\u51faDRL-DC\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u4f18\u5316\u5206\u5e03\u5f0f\u9884\u8a00\u673a\u7f13\u5b58\u7b56\u7565\uff1b\u5f00\u53d1PoCL\u5171\u8bc6\u673a\u5236\u7ef4\u62a4\u7f13\u5b58\u51b3\u7b56\u4e00\u81f4\u6027", "result": "\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747\u8bbf\u95ee\u5ef6\u8fdf\u964d\u4f4e20%\uff0c\u7f13\u5b58\u547d\u4e2d\u7387\u6700\u9ad8\u63d0\u534718%\uff0c\u5e73\u5747\u5171\u8bc6\u6210\u529f\u7387\u63d0\u9ad810%", "conclusion": "\u672c\u6587\u9996\u6b21\u63a2\u7d22Web3.0\u53bb\u4e2d\u5fc3\u5316\u7f13\u5b58\u6846\u67b6\u548c\u7b56\u7565\uff0cTDC-Cache\u80fd\u6709\u6548\u63d0\u5347\u7f13\u5b58\u6548\u7387\u5e76\u589e\u5f3a\u7cfb\u7edf\u5bf9\u6297\u6027\u5a01\u80c1\u7684\u97e7\u6027"}}
{"id": "2512.10217", "categories": ["cs.DB", "cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.10217", "abs": "https://arxiv.org/abs/2512.10217", "authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Dan Suciu"], "title": "PANDAExpress: a Simpler and Faster PANDA Algorithm", "comment": null, "summary": "PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.\n  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\\tilde O(\\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power.", "AI": {"tldr": "PANDAExpress\u6539\u8fdbPANDA\u7b97\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684\u6982\u7387\u4e0d\u7b49\u5f0f\u548c\u52a8\u6001\u8d85\u5e73\u9762\u5206\u5272\u65b9\u6848\uff0c\u6d88\u9664\u4e86polylog(N)\u56e0\u5b50\uff0c\u5728\u4fdd\u6301\u901a\u7528\u6027\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u4e13\u7528\u7b97\u6cd5\u76f8\u5f53\u7684\u8fd0\u884c\u65f6\u95f4\u3002", "motivation": "PANDA\u7b97\u6cd5\u867d\u7136\u901a\u7528\u5f3a\u5927\uff0c\u4f46\u5b58\u5728\u8f83\u5927\u7684polylog(N)\u56e0\u5b50\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u5b9e\u7528\uff0c\u65e0\u6cd5\u8fbe\u5230\u4e13\u7528\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5f31\u70b9\u3002", "method": "1. \u63d0\u51fa\u65b0\u7684\u6982\u7387\u4e0d\u7b49\u5f0f\u6765\u4e0a\u754cDDRs\u5728\u4efb\u610f\u5ea6\u7ea6\u675f\u4e0b\u7684\u8f93\u51fa\u5927\u5c0f\uff1b2. \u57fa\u4e8e\u8be5\u8bc1\u660e\u5f00\u53d1PANDAExpress\u7b97\u6cd5\uff0c\u4f7f\u7528\u52a8\u6001\u6784\u5efa\u7684\u4efb\u610f\u8d85\u5e73\u9762\u5206\u5272\uff08\u800c\u975ePANDA\u7684\u8f74\u5e73\u884c\u8d85\u5e73\u9762\uff09\uff0c\u6839\u636e\u6570\u636e\u504f\u659c\u7edf\u8ba1\u8fdb\u884c\u81ea\u9002\u5e94\u5212\u5206\u3002", "result": "PANDAExpress\u6d88\u9664\u4e86PANDA\u4e2d\u7684polylog(N)\u56e0\u5b50\uff0c\u5728\u4fdd\u6301\u5904\u7406\u4efb\u610f\u5ea6\u7ea6\u675f\u3001\u81ea\u7531\u53d8\u91cf\u3001CQs\u548cDDRs\u7b49\u901a\u7528\u6027\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u590d\u6742\u4e13\u7528\u7b97\u6cd5\u76f8\u5f53\u7684\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "PANDAExpress\u89e3\u51b3\u4e86PANDA\u7684\u4e3b\u8981\u6027\u80fd\u5f31\u70b9\uff0c\u5728\u4fdd\u6301\u7b97\u6cd5\u901a\u7528\u6027\u548c\u5f3a\u5927\u529f\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u4f18\u4e13\u7528\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f7f\u901a\u7528\u67e5\u8be2\u5904\u7406\u6846\u67b6\u53d8\u5f97\u5b9e\u7528\u3002"}}
{"id": "2512.10079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10079", "abs": "https://arxiv.org/abs/2512.10079", "authors": ["Federico Formica", "Mark Lawford", "Claudio Menghi"], "title": "Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives", "comment": null, "summary": "Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u57fa\u4e8e\u641c\u7d22\u7684\u8f6f\u4ef6\u6d4b\u8bd5\uff08SBST\uff09\u4e2d\u96c6\u6210\u9886\u57df\u77e5\u8bc6\u7684\u6700\u65b0\u5b9e\u9a8c\u7ed3\u679c\uff0c\u7a81\u51fa\u5c55\u793a\u4e86\u5927\u80c6\u4e14\u610f\u5916\u7684\u53d1\u73b0\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "motivation": "SBST\u80fd\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4f46\u7f3a\u4e4f\u5de5\u7a0b\u5e08\u7684\u9886\u57df\u77e5\u8bc6\u3002\u867d\u7136\u5df2\u6709\u6280\u672f\u5c1d\u8bd5\u5c06\u9886\u57df\u77e5\u8bc6\u96c6\u6210\u5230SBST\u6846\u67b6\u4e2d\uff0c\u4f46\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u57fa\u4e8e\u6700\u65b0\u5b9e\u9a8c\u7ed3\u679c\u63a2\u7d22\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u56de\u987e\u548c\u5206\u6790\u6700\u8fd1\u5173\u4e8eSBST\u4e2d\u96c6\u6210\u9886\u57df\u77e5\u8bc6\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u91cd\u70b9\u5173\u6ce8\u90a3\u4e9b\u5927\u80c6\u4e14\u610f\u5916\u7684\u53d1\u73b0\uff0c\u5bf9\u73b0\u6709\u6280\u672f\u8fdb\u884c\u6279\u5224\u6027\u53cd\u601d\u3002", "result": "\u8bba\u6587\u7a81\u51fa\u4e86SBST\u96c6\u6210\u9886\u57df\u77e5\u8bc6\u5b9e\u9a8c\u4e2d\u7684\u610f\u5916\u7ed3\u679c\uff0c\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u4f20\u7edf\u8ba4\u77e5\uff0c\u4e3a\u91cd\u65b0\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u3002", "conclusion": "\u9700\u8981\u4ece\u65b0\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u7684SBST\u6280\u672f\uff0c\u57fa\u4e8e\u5b9e\u9a8c\u53d1\u73b0\u63d0\u51fa\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2512.09963", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09963", "abs": "https://arxiv.org/abs/2512.09963", "authors": ["Phuong Tran", "Tzu-Hao Liu", "Long Tan Le", "Tung-Anh Nguyen", "Van Quan La", "Eason Yu", "Han Shu", "Choong Seon Hong", "Nguyen H. Tran"], "title": "GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference", "comment": "Accepted at INFOCOM 2026", "summary": "Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.", "AI": {"tldr": "GOODSPEED\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u4f18\u5316LLM\u63a8\u7406\u7684\u597d\u541e\u5410\u91cf\uff0c\u5728\u5f02\u6784\u8349\u7a3f\u670d\u52a1\u5668\u95f4\u5b9e\u73b0\u6bd4\u4f8b\u516c\u5e73\u7684\u8d44\u6e90\u5206\u914d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u7ed9\u5b9e\u65f6\u63a8\u7406\u5e26\u6765\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u591a\u7528\u6237\u670d\u52a1\u5668\u63a8\u6d4b\u89e3\u7801\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u3002\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u6280\u672f\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u9ad8\u597d\u541e\u5410\u91cf\uff08\u6709\u6548\u63a5\u53d7\u4ee4\u724c\u7387\uff09\u548c\u5728\u591a\u4e2a\u8349\u7a3f\u670d\u52a1\u5668\u95f4\u7684\u516c\u5e73\u6027\u3002", "method": "GOODSPEED\u91c7\u7528\u4e2d\u5fc3\u9a8c\u8bc1\u670d\u52a1\u5668\u534f\u8c03\u4e00\u7ec4\u5f02\u6784\u8349\u7a3f\u670d\u52a1\u5668\u7684\u67b6\u6784\u3002\u8349\u7a3f\u670d\u52a1\u5668\u8fd0\u884c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63a8\u6d4b\u4ee4\u724c\uff0c\u9a8c\u8bc1\u670d\u52a1\u5668\u5e76\u884c\u5904\u7406\u6240\u6709\u8349\u7a3f\u670d\u52a1\u5668\u7684\u8f93\u51fa\u3002\u6846\u67b6\u5305\u542b\u68af\u5ea6\u8c03\u5ea6\u7b97\u6cd5\uff0c\u52a8\u6001\u5206\u914d\u4ee4\u724c\u9a8c\u8bc1\u4efb\u52a1\uff0c\u901a\u8fc7\u6700\u5927\u5316\u5bf9\u6570\u6548\u7528\u51fd\u6570\u786e\u4fdd\u670d\u52a1\u5668\u95f4\u7684\u6bd4\u4f8b\u516c\u5e73\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u6d41\u4f53\u6837\u672c\u8def\u5f84\u5206\u6790\uff0cGOODSPEED\u5728\u7a33\u6001\u6761\u4ef6\u4e0b\u6536\u655b\u5230\u6700\u4f18\u597d\u541e\u5410\u91cf\u5206\u914d\uff0c\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u4e14\u6709\u754c\u8bef\u5dee\u3002\u8be5\u6846\u67b6\u4e3a\u5206\u5e03\u5f0fLLM\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u516c\u5e73\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "GOODSPEED\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u548c\u516c\u5e73\u8d44\u6e90\u5206\u914d\uff0c\u89e3\u51b3\u4e86\u591a\u670d\u52a1\u5668\u73af\u5883\u4e2dLLM\u63a8\u7406\u7684\u597d\u541e\u5410\u91cf\u548c\u516c\u5e73\u6027\u6311\u6218\uff0c\u4e3a\u5206\u5e03\u5f0f\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u6846\u67b6\u3002"}}
{"id": "2512.10621", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.10621", "abs": "https://arxiv.org/abs/2512.10621", "authors": ["Siwoo Song", "Wonseok Shin", "Kunsoo Park", "Giuseppe F. Italiano", "Zhengyi Yang", "Wenjie Zhang"], "title": "Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint", "comment": null, "summary": "A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4ea4\u96c6\u7ea6\u675f\u3001\u5019\u9009\u8d85\u8fb9\u7a7a\u95f4\u548c\u5339\u914d-\u8fc7\u6ee4\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u8d85\u56fe\u80fd\u591f\u5efa\u6a21\u6d89\u53ca\u591a\u4e2a\u9876\u70b9\u7684\u590d\u6742\u5173\u7cfb\uff0c\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\u662f\u5bfb\u627e\u67e5\u8be2\u8d85\u56fe\u5728\u6570\u636e\u8d85\u56fe\u4e2d\u6240\u6709\u540c\u6784\u5d4c\u5165\u7684\u57fa\u672c\u95ee\u9898\u3002\u73b0\u6709\u7b97\u6cd5\u5728\u5904\u7406\u590d\u6742\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\u65f6\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5f15\u5165\u4ea4\u96c6\u7ea6\u675f\u4f5c\u4e3a\u6709\u6548\u5d4c\u5165\u7684\u5fc5\u8981\u5145\u5206\u6761\u4ef6\uff0c\u52a0\u901f\u9a8c\u8bc1\u8fc7\u7a0b\uff1b2. \u8bbe\u8ba1\u5019\u9009\u8d85\u8fb9\u7a7a\u95f4\u6570\u636e\u7ed3\u6784\u5b58\u50a8\u67e5\u8be2\u8d85\u8fb9\u4e0e\u6570\u636e\u8d85\u8fb9\u4e4b\u95f4\u7684\u6f5c\u5728\u6620\u5c04\uff1b3. \u63d0\u51fa\u5339\u914d-\u8fc7\u6ee4\u6846\u67b6\uff0c\u5728\u56de\u6eaf\u8fc7\u7a0b\u4e2d\u4ea4\u66ff\u8fdb\u884c\u5339\u914d\u548c\u8fc7\u6ee4\u64cd\u4f5c\uff0c\u4ec5\u4fdd\u7559\u517c\u5bb9\u7684\u5019\u9009\u6620\u5c04\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u67e5\u8be2\u5904\u7406\u65f6\u95f4\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u53ef\u8fbe\u6570\u91cf\u7ea7\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\u7b97\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u7ea6\u675f\u6761\u4ef6\u3001\u6570\u636e\u7ed3\u6784\u548c\u6846\u67b6\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\uff0c\u4e3a\u5904\u7406\u590d\u6742\u5173\u7cfb\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10173", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10173", "abs": "https://arxiv.org/abs/2512.10173", "authors": ["Mantas Baksys", "Stefan Zetzsche", "Olivier Bouissou", "Remi Delmas", "Soonho Kong"], "title": "ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis", "comment": null, "summary": "Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.", "AI": {"tldr": "ATLAS\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5e26\u9a8c\u8bc1\u7684Dafny\u7a0b\u5e8f\u89e3\u51b3LLM\u7a0b\u5e8f\u9a8c\u8bc1\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u9a8c\u8bc1\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5df2\u9a8c\u8bc1\u4ee3\u7801\u7684\u8bad\u7ec3\u6570\u636e\u963b\u788d\u4e86\u8fdb\u5c55\u3002\u9700\u8981\u89e3\u51b3\u6570\u636e\u74f6\u9888\u95ee\u9898\u3002", "method": "ATLAS\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u5927\u89c4\u6a21\u5408\u6210\u5df2\u9a8c\u8bc1\u7684Dafny\u7a0b\u5e8f\uff0c\u5305\u62ec\u89c4\u8303\u3001\u5b9e\u73b0\u548c\u8bc1\u660e\u3002\u901a\u8fc7\u5c06\u5408\u6210\u8fc7\u7a0b\u5206\u89e3\u4e3a\u591a\u4e2a\u4e13\u95e8\u4efb\u52a1\uff0c\u4ece\u6bcf\u4e2a\u9a8c\u8bc1\u7a0b\u5e8f\u4e2d\u63d0\u53d6\u591a\u4e2a\u8bad\u7ec3\u793a\u4f8b\u3002", "result": "\u751f\u6210\u4e862.7K\u4e2a\u5df2\u9a8c\u8bc1\u7a0b\u5e8f\uff0c\u63d0\u53d6\u4e86\u8d85\u8fc719K\u4e2a\u8bad\u7ec3\u793a\u4f8b\uff08\u5e73\u5747\u6bcf\u4e2a\u7a0b\u5e8f\u8d85\u8fc77\u4e2a\uff09\u3002\u5728Qwen 2.5 7B Coder\u4e0a\u5fae\u8c03\u540e\uff0c\u5728DafnyBench\u4e0a\u63d0\u534723\u4e2a\u767e\u5206\u70b9\uff0c\u5728DafnySynthesis\u4e0a\u63d0\u534750\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5408\u6210\u7684\u5df2\u9a8c\u8bc1\u4ee3\u7801\u80fd\u6709\u6548\u589e\u5f3aLLM\u7684\u5f62\u5f0f\u9a8c\u8bc1\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u7a0b\u5e8f\u9a8c\u8bc1\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2512.10236", "categories": ["cs.DC", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10236", "abs": "https://arxiv.org/abs/2512.10236", "authors": ["Shagnik Pal", "Shaizeen Aga", "Suchita Pati", "Mahzabeen Islam", "Lizy K. John"], "title": "Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap", "comment": null, "summary": "As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.", "AI": {"tldr": "FiCCO\u63d0\u51fa\u7ec6\u7c92\u5ea6\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u6280\u672f\uff0c\u901a\u8fc7\u6bd4\u5206\u7247\u7ea7\u66f4\u6df1\u4e00\u5c42\u7684\u91cd\u53e0\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u7f51\u7edc\u62d3\u6251\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u6d41\u89e3\u9501\u91cd\u53e0\u673a\u4f1a\uff0c\u5e76\u901a\u8fc7GPU DMA\u5378\u8f7d\u901a\u4fe1\u51cf\u5c11\u4e89\u7528\uff0c\u5b9e\u73b0\u6700\u9ad81.6\u500d\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u5206\u5e03\u5f0fML\u8bad\u7ec3\u548c\u63a8\u7406\u4e2d\uff0c\u5206\u7247\u5e76\u884c\u6280\u672f\u666e\u904d\u5b58\u5728\u6570\u636e\u4f9d\u8d56\u7684\u901a\u4fe1-\u8ba1\u7b97\u64cd\u4f5c\uff0c\u901a\u4fe1\u66b4\u9732\u5bfc\u81f4\u6027\u80fd\u635f\u5931\u9ad8\u8fbe\u7406\u60f3\u6027\u80fd\u76841.7\u500d\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5728\u5206\u7247\u7ea7\u522b\u8fdb\u884c\u7c97\u7c92\u5ea6\u91cd\u53e0\uff0c\u4f46\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faFiCCO\uff08\u7ec6\u7c92\u5ea6\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\uff09\u6280\u672f\uff1a1\uff09\u5206\u6790ML\u64cd\u4f5c\u5206\u89e3\u5bfc\u81f4\u7684\u6548\u7387\u635f\u5931\uff1b2\uff09\u8bbe\u8ba1FiCCO\u8c03\u5ea6\u65b9\u6848\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff1b3\uff09\u5c06\u8c03\u5ea6\u65b9\u6848\u4e0e\u76f8\u5e94\u7684\u6548\u7387\u7279\u5f81\u53e0\u52a0\uff1b4\uff09\u8bbe\u8ba1\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e3a\u4e0d\u540cML\u64cd\u4f5c\u9009\u62e9\u5b9a\u5236\u8c03\u5ea6\uff1b5\uff09\u4f7f\u7528GPU DMA\u5f15\u64ce\u5378\u8f7d\u901a\u4fe1\u4ee5\u51cf\u5c11\u4e89\u7528\u3002", "result": "\u5728\u771f\u5b9eML\u90e8\u7f72\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u63d0\u51fa\u7684\u5b9a\u5236\u8c03\u5ea6\u65b9\u6848\u5b9e\u73b0\u6700\u9ad81.6\u500d\u52a0\u901f\uff0c\u542f\u53d1\u5f0f\u65b9\u6cd5\u572881%\u7684\u672a\u89c1\u573a\u666f\u4e2d\u63d0\u4f9b\u51c6\u786e\u6307\u5bfc\u3002", "conclusion": "FiCCO\u901a\u8fc7\u7ec6\u7c92\u5ea6\u91cd\u53e0\u6269\u5c55\u4e86\u6267\u884c\u8c03\u5ea6\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7ed3\u5408\u6548\u7387\u635f\u5931\u5206\u6790\u548cGPU DMA\u5378\u8f7d\uff0c\u80fd\u591f\u4e3a\u4e0d\u540cML\u64cd\u4f5c\u9009\u62e9\u6700\u4f18\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0fML\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2512.10218", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10218", "abs": "https://arxiv.org/abs/2512.10218", "authors": ["Thanosan Prathifkumar", "Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?", "comment": null, "summary": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0SWE-Bench-Verified\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u56e0\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u5bfc\u81f4\u6a21\u578b\u8868\u73b0\u865a\u9ad8\uff0c\u4e0d\u80fd\u771f\u5b9e\u53cd\u6620\u89e3\u51b3\u5b9e\u9645\u8f6f\u4ef6\u95ee\u9898\u7684\u80fd\u529b", "motivation": "SWE-Bench-Verified\u4f5c\u4e3a\u8bc4\u4f30LLMs\u89e3\u51b3GitHub\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53ef\u80fd\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u5bfc\u81f4\u5206\u6570\u53cd\u6620\u7684\u662f\u8bad\u7ec3\u8bb0\u5fc6\u800c\u975e\u5b9e\u9645\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b", "method": "\u6d4b\u8bd5\u4e24\u4e2a\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u7684Claude\u6a21\u578b\uff0c\u8ba9\u5b83\u4eec\u4ec5\u51ed\u95ee\u9898\u6587\u672c\u5b9a\u4f4d\u76f8\u5173\u6587\u4ef6\uff0c\u7136\u540e\u52a0\u4e0a\u6587\u4ef6\u8def\u5f84\u4fe1\u606f\uff0c\u5e76\u5728BeetleBox\u548cSWE-rebench\u4e0a\u8fdb\u884c\u76f8\u540c\u6d4b\u8bd5", "result": "\u6a21\u578b\u5728SWE-Bench-Verified\u4e0a\u7684\u8868\u73b0\u6bd4\u5728\u5176\u4ed6\u57fa\u51c6\u4e0a\u597d3\u500d\uff0c\u5b9a\u4f4d\u7f16\u8f91\u6587\u4ef6\u7684\u80fd\u529b\u5f3a6\u500d\uff0c\u8868\u660e\u6a21\u578b\u53ef\u80fd\u5728\u8bad\u7ec3\u4e2d\u89c1\u8fc7\u8fd9\u4e9b\u4efb\u52a1\uff0c\u57fa\u51c6\u5206\u6570\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u80fd\u529b", "conclusion": "\u4f9d\u8d56\u65e7\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u98ce\u9669\uff0c\u5e94\u8f6c\u5411\u8003\u8651\u6570\u636e\u6c61\u67d3\u95ee\u9898\u7684\u65b0\u6570\u636e\u96c6\uff0c\u907f\u514d\u8bef\u5bfc\u6027\u8bc4\u4f30\u548c\u9009\u62e9\u504f\u5411\u7279\u5b9a\u6a21\u578b\u800c\u975e\u4f18\u79c0\u4ee3\u7406\u8bbe\u8ba1"}}
{"id": "2512.10271", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10271", "abs": "https://arxiv.org/abs/2512.10271", "authors": ["Shruti Dongare", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Nannan Zhao", "Diego Melendez Maita", "Ali R. Butt"], "title": "Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters", "comment": null, "summary": "Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.", "AI": {"tldr": "RLTune\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u4f5c\u4e1a\u8c03\u5ea6\u6846\u67b6\uff0c\u80fd\u5728\u5f02\u6784GPU\u96c6\u7fa4\u4e0a\u52a8\u6001\u4f18\u5316\u4f5c\u4e1a\u4f18\u5148\u7ea7\u548c\u8d44\u6e90\u5206\u914d\uff0c\u65e0\u9700\u4f5c\u4e1a\u7ea7\u5206\u6790\u5373\u53ef\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u4e91\u5e73\u53f0\u627f\u8f7d\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f46GPU\u96c6\u7fa4\u5f02\u6784\u6027\u589e\u5f3a\u548c\u5e94\u7528\u7279\u6027\u4e0d\u900f\u660e\u7ed9\u73b0\u6709\u8c03\u5ea6\u5668\u5e26\u6765\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u79bb\u7ebf\u5206\u6790\u6216\u5e94\u7528\u7279\u5b9a\u5047\u8bbe\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "method": "RLTune\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\u548c\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u7684\u4f5c\u4e1a\u5230\u8282\u70b9\u6620\u5c04\u3002RL\u7ec4\u4ef6\u52a8\u6001\u786e\u5b9a\u4f5c\u4e1a\u4f18\u5148\u7ea7\uff0cMILP\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u5171\u540c\u4f18\u5316\u7cfb\u7edf\u76ee\u6807\u5982\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u3001\u6392\u961f\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "\u57fa\u4e8e\u5fae\u8f6fPhilly\u3001Helios\u548c\u963f\u91cc\u5df4\u5df4\u7684\u5927\u89c4\u6a21\u751f\u4ea7\u8f68\u8ff9\u8bad\u7ec3\uff0cRLTune\u5c06GPU\u5229\u7528\u7387\u63d0\u5347\u9ad8\u8fbe20%\uff0c\u6392\u961f\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe81%\uff0c\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u9ad8\u8fbe70%\u3002", "conclusion": "RLTune\u65e0\u9700\u4f5c\u4e1a\u7ea7\u5206\u6790\u5373\u53ef\u8de8\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u6cdb\u5316\uff0c\u4e3a\u4e91\u63d0\u4f9b\u5546\u63d0\u4f9b\u4e86\u53ef\u5927\u89c4\u6a21\u90e8\u7f72\u7684\u9ad8\u6548\u3001\u516c\u5e73\u3001\u53ef\u6301\u7eed\u7684\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10238", "abs": "https://arxiv.org/abs/2512.10238", "authors": ["Antu Saha"], "title": "Studying and Automating Issue Resolution for Software Quality", "comment": "3 pages", "summary": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e09\u4e2a\u65b9\u5411\u63d0\u5347\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u6548\u7387\uff1a1) \u5229\u7528LLM\u589e\u5f3a\u95ee\u9898\u62a5\u544a\u8d28\u91cf\uff1b2) \u5b9e\u8bc1\u5206\u6790\u4f20\u7edf\u4e0eAI\u8f85\u52a9\u5f00\u53d1\u5de5\u4f5c\u6d41\uff1b3) \u81ea\u52a8\u5316\u95ee\u9898\u5b9a\u4f4d\u548c\u89e3\u51b3\u65b9\u6848\u8bc6\u522b\u4efb\u52a1\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u95ee\u9898\u89e3\u51b3\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u4f4e\u8d28\u91cf\u95ee\u9898\u62a5\u544a\u3001\u5bf9\u5b9e\u9645\u5de5\u4f5c\u6d41\u7406\u89e3\u6709\u9650\u3001\u7f3a\u4e4f\u81ea\u52a8\u5316\u652f\u6301\u3002\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u4e86\u8f6f\u4ef6\u8d28\u91cf\u548c\u7ef4\u62a4\u6548\u7387\u3002", "method": "\u91c7\u7528\u4e09\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\uff1a1) \u5229\u7528LLM\u63a8\u7406\u548c\u5e94\u7528\u7279\u5b9a\u4fe1\u606f\u63d0\u5347\u95ee\u9898\u62a5\u544a\u8d28\u91cf\uff1b2) \u5b9e\u8bc1\u5206\u6790\u4f20\u7edf\u548cAI\u589e\u5f3a\u7cfb\u7edf\u4e2d\u7684\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\uff1b3) \u7ed3\u5408ML\u3001DL\u548cLLM\u6280\u672f\u81ea\u52a8\u5316\u95ee\u9898\u5b9a\u4f4d\u548c\u89e3\u51b3\u65b9\u6848\u8bc6\u522b\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6d1e\u5bdf\u3001\u5b9e\u7528\u5de5\u5177\u548c\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u66f4\u53ef\u7ef4\u62a4\u548c\u9ad8\u8d28\u91cf\u7684\u8f6f\u4ef6\u7cfb\u7edf\uff0c\u63a8\u8fdbAI\u9a71\u52a8\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7efc\u5408\u65b9\u6cd5\u63d0\u5347\u95ee\u9898\u62a5\u544a\u8d28\u91cf\u3001\u7406\u89e3\u5f00\u53d1\u5de5\u4f5c\u6d41\u5e76\u81ea\u52a8\u5316\u8ba4\u77e5\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u8be5\u7814\u7a76\u4e3aAI\u9a71\u52a8\u7684\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u9ad8\u8d28\u91cf\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002"}}
{"id": "2512.10312", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10312", "abs": "https://arxiv.org/abs/2512.10312", "authors": ["Julian Rodriguez", "Piotr Lopez", "Emiliano Lerma", "Rafael Medrano", "Jacobo Hernandez"], "title": "High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments", "comment": "8 pages, 3 figures", "summary": "This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.", "AI": {"tldr": "\u8be5\u6587\u6863\u62a5\u544a\u4e86\u5927\u6570\u636e\u8bfe\u7a0b\u4e2d\u5b9e\u65bd\u7684\u5b9e\u8df5\u548c\u65b9\u6cd5\u5e8f\u5217\uff0c\u5305\u62ecEpsilon\u6570\u636e\u96c6\u5904\u7406\u3001\u6587\u672c\u5206\u6790\u5206\u7c7b\u3001\u7535\u5f71\u7279\u5f81\u5206\u6790\u4ee5\u53caApache Spark\u5206\u5e03\u5f0f\u96c6\u7fa4\u7684\u6280\u672f\u5b9e\u73b0\u3002", "motivation": "\u8be5\u6587\u6863\u65e8\u5728\u8bb0\u5f55\u5927\u6570\u636e\u8bfe\u7a0b\u4e2d\u7684\u5b9e\u8df5\u6d41\u7a0b\u548c\u65b9\u6cd5\u8bba\uff0c\u5c55\u793a\u4ece\u6570\u636e\u5904\u7406\u5230\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4\u642d\u5efa\u7684\u5b8c\u6574\u5b66\u4e60\u8def\u5f84\uff0c\u4e3a\u5927\u6570\u636e\u6280\u672f\u5e94\u7528\u63d0\u4f9b\u5b9e\u9645\u6848\u4f8b\u53c2\u8003\u3002", "method": "\u91c7\u7528\u5206\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u5c0f\u7ec4\u548c\u4e2a\u4eba\u7b56\u7565\u5904\u7406Epsilon\u6570\u636e\u96c6\uff1b2) \u4f7f\u7528RestMex\u8fdb\u884c\u6587\u672c\u5206\u6790\u548c\u5206\u7c7b\uff1b3) \u4f7f\u7528IMDb\u8fdb\u884c\u7535\u5f71\u7279\u5f81\u5206\u6790\uff1b4) \u5728Linux\u7cfb\u7edf\u4e0a\u4f7f\u7528Scala\u8bed\u8a00\u5b9e\u73b0Apache Spark\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4\u3002", "result": "\u6587\u6863\u8be6\u7ec6\u8bb0\u5f55\u4e86\u5b8c\u6574\u7684\u5927\u6570\u636e\u5904\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u96c6\u5904\u7406\u3001\u6587\u672c\u5206\u6790\u3001\u7535\u5f71\u7279\u5f81\u5206\u6790\u7684\u6280\u672f\u5b9e\u73b0\uff0c\u4ee5\u53ca\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4\u7684\u6210\u529f\u642d\u5efa\u548c\u914d\u7f6e\u3002", "conclusion": "\u8be5\u8bfe\u7a0b\u5b9e\u8df5\u5c55\u793a\u4e86\u5927\u6570\u636e\u5904\u7406\u7684\u5168\u6d41\u7a0b\u65b9\u6cd5\uff0c\u4ece\u57fa\u7840\u6570\u636e\u5904\u7406\u5230\u9ad8\u7ea7\u5206\u5e03\u5f0f\u8ba1\u7b97\uff0c\u4e3a\u5927\u6570\u636e\u6280\u672f\u5b66\u4e60\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u5b9e\u8df5\u6846\u67b6\u548c\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2512.10393", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10393", "abs": "https://arxiv.org/abs/2512.10393", "authors": ["Guoqiang Chen", "Lingyun Ying", "Ziyang Song", "Daguang Liu", "Qiang Wang", "Zhiqi Wang", "Li Hu", "Shaoyin Cheng", "Weiming Zhang", "Nenghai Yu"], "title": "Cross-modal Retrieval Models for Stripped Binary Analysis", "comment": null, "summary": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.", "AI": {"tldr": "BinSeek\uff1a\u9996\u4e2a\u7528\u4e8e\u5265\u79bb\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u7684\u4e24\u9636\u6bb5\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u6a21\u578b\u548c\u91cd\u6392\u5e8f\u6a21\u578b\u5b9e\u73b0\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u9ad8\u6548\u5339\u914d", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u4ece\u6570\u5343\u4e2a\u5265\u79bb\u7b26\u53f7\u4fe1\u606f\u7684\u4e8c\u8fdb\u5236\u51fd\u6570\u4e2d\u68c0\u7d22\u76f8\u5173\u4ee3\u7801\u975e\u5e38\u56f0\u96be\uff0c\u8fd9\u4e0d\u540c\u4e8e\u6e90\u4ee3\u7801\u68c0\u7d22\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u7b26\u53f7\u4fe1\u606f", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff1a1) BinSeekEmbedding\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5b66\u4e60\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff1b2) BinSeek-Reranker\u901a\u8fc7\u4e0a\u4e0b\u6587\u589e\u5f3a\u4ed4\u7ec6\u5224\u65ad\u5019\u9009\u4ee3\u7801\u4e0e\u63cf\u8ff0\u7684\u76f8\u5173\u6027\u3002\u4f7f\u7528LLM\u6570\u636e\u5408\u6210\u7ba1\u9053\u81ea\u52a8\u5316\u8bad\u7ec3\u6570\u636e\u6784\u5efa", "result": "BinSeek\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728Rec@3\u6307\u6807\u4e0a\u8d85\u8d8a\u540c\u89c4\u6a21\u6a21\u578b31.42%\uff0c\u5728MRR@3\u4e0a\u8d85\u8d8a27.17%\uff0c\u751a\u81f3\u9886\u5148\u53c2\u6570\u89c4\u6a21\u592716\u500d\u7684\u5148\u8fdb\u901a\u7528\u6a21\u578b", "conclusion": "BinSeek\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u5265\u79bb\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u7684\u4e24\u9636\u6bb5\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e8c\u8fdb\u5236\u4ee3\u7801\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6"}}
{"id": "2512.10425", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10425", "abs": "https://arxiv.org/abs/2512.10425", "authors": ["Fan Yu", "Guodong Li", "Si Wu", "Weijun Fang", "Sihuang Hu"], "title": "Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability", "comment": null, "summary": "Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.", "AI": {"tldr": "\u63d0\u51faCP-LRCs\uff08\u7ea7\u8054\u5947\u5076\u6821\u9a8cLRCs\uff09\uff0c\u901a\u8fc7\u5728\u5c40\u90e8\u5947\u5076\u6821\u9a8c\u5757\u4e4b\u95f4\u5d4c\u5165\u7ed3\u6784\u5316\u4f9d\u8d56\u5173\u7cfb\uff0c\u89e3\u51b3\u5bbd\u6761\u5e26LRCs\u4e2d\u4fee\u590d\u6210\u672c\u9ad8\u548c\u53ef\u9760\u6027\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u5bbd\u6761\u5e26\u7ea0\u5220\u7801\u5728\u5927\u578b\u5b58\u50a8\u7cfb\u7edf\u4e2d\u88ab\u5e7f\u6cdb\u91c7\u7528\u4ee5\u964d\u4f4e\u5b58\u50a8\u5f00\u9500\uff0c\u4f46\u73b0\u6709\u7684\u5c40\u90e8\u53ef\u4fee\u590d\u7801\uff08LRCs\uff09\u5b58\u5728\u7ed3\u6784\u9650\u5236\uff1a\u6269\u5927\u7684\u5c40\u90e8\u7ec4\u589e\u52a0\u5355\u8282\u70b9\u4fee\u590d\u6210\u672c\uff0c\u591a\u8282\u70b9\u6545\u969c\u9891\u7e41\u89e6\u53d1\u6602\u8d35\u7684\u5168\u5c40\u4fee\u590d\uff0c\u53ef\u9760\u6027\u6025\u5267\u4e0b\u964d\u3002\u6839\u672c\u539f\u56e0\u662f\u5c40\u90e8\u548c\u5168\u5c40\u5947\u5076\u6821\u9a8c\u5757\u72ec\u7acb\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u5728\u4fee\u590d\u8fc7\u7a0b\u4e2d\u534f\u540c\u5de5\u4f5c\u3002", "method": "\u63d0\u51fa\u7ea7\u8054\u5947\u5076\u6821\u9a8cLRCs\uff08CP-LRCs\uff09\uff0c\u901a\u8fc7\u5728\u6240\u6709\u5c40\u90e8\u5947\u5076\u6821\u9a8c\u5757\u4e0a\u5206\u89e3\u5168\u5c40\u5947\u5076\u6821\u9a8c\u5757\uff0c\u5728\u5947\u5076\u6821\u9a8c\u5757\u4e4b\u95f4\u5d4c\u5165\u7ed3\u6784\u5316\u4f9d\u8d56\u5173\u7cfb\uff0c\u5f62\u6210\u7ea7\u8054\u5947\u5076\u6821\u9a8c\u7ec4\u3002\u63d0\u4f9b\u901a\u7528\u7684\u7cfb\u6570\u751f\u6210\u6846\u67b6\uff0c\u5f00\u53d1\u5229\u7528\u7ea7\u8054\u7279\u6027\u7684\u4fee\u590d\u7b97\u6cd5\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3aCP-Azure\u548cCP-Uniform\u4e24\u79cd\u5b9e\u73b0\u3002", "result": "\u5728\u963f\u91cc\u4e91\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCP-LRCs\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5355\u8282\u70b9\u6545\u969c\u4fee\u590d\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe41%\uff0c\u53cc\u8282\u70b9\u6545\u969c\u4fee\u590d\u65f6\u95f4\u51cf\u5c1126%\u3002\u540c\u65f6\u4fdd\u6301\u4e86MDS\u7ea7\u522b\u7684\u5bb9\u9519\u80fd\u529b\u3002", "conclusion": "CP-LRCs\u901a\u8fc7\u5947\u5076\u6821\u9a8c\u5757\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u4f9d\u8d56\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u5bbd\u6761\u5e26LRCs\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5e26\u5bbd\u7684\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u4fee\u590d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u53ef\u9760\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5b58\u50a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10415", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10415", "abs": "https://arxiv.org/abs/2512.10415", "authors": ["Devanshu Sahoo", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Dhruv Kumar"], "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation", "comment": "Under Review", "summary": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.", "AI": {"tldr": "\u9996\u6b21\u5927\u89c4\u6a21\u7814\u7a76\u5b66\u672f\u73af\u5883\u4e2dLLM\u4ee3\u7801\u8bc4\u4f30\u5668\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u63d0\u51fa\u5b66\u672f\u8d8a\u72f1\u6982\u5ff5\uff0c\u521b\u5efa25K\u5bf9\u6297\u6027\u63d0\u4ea4\u6570\u636e\u96c6\uff0c\u8bc4\u4f306\u4e2aLLM\u7684\u8106\u5f31\u6027", "motivation": "LLM\u4f5c\u4e3a\u4ee3\u7801\u81ea\u52a8\u8bc4\u4f30\u5668\u5728\u5b66\u672f\u73af\u5883\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5b66\u751f\u53ef\u80fd\u4f7f\u7528\u5bf9\u6297\u6027\u63d0\u793a\u7b56\u7565\u8bf1\u5bfc\u8bef\u5224\u4ee5\u83b7\u53d6\u4e0d\u6b63\u5f53\u5b66\u672f\u4f18\u52bf\uff0c\u9700\u8981\u7814\u7a76\u5176\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027", "method": "\u7cfb\u7edf\u6027\u5730\u5c0620\u591a\u79cd\u8d8a\u72f1\u7b56\u7565\u9002\u914d\u5230\u5b66\u672f\u4ee3\u7801\u8bc4\u4f30\u573a\u666f\uff0c\u521b\u5efa\u5305\u542b25K\u5bf9\u6297\u6027\u63d0\u4ea4\u7684\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u4e09\u4e2a\u8d8a\u72f1\u6307\u6807\uff08\u8d8a\u72f1\u6210\u529f\u7387\u3001\u5206\u6570\u81a8\u80c0\u3001\u5371\u5bb3\u6027\uff09\uff0c\u57286\u4e2aLLM\u4e0a\u8fdb\u884c\u5168\u9762\u8bc4\u4f30", "result": "\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5b58\u5728\u663e\u8457\u8106\u5f31\u6027\uff0c\u7279\u522b\u662f\u5bf9\u8bf4\u670d\u6027\u548c\u89d2\u8272\u626e\u6f14\u7c7b\u653b\u51fb\uff08\u8d8a\u72f1\u6210\u529f\u7387\u9ad8\u8fbe97%\uff09\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u9c81\u68d2\u7684LLM\u5b66\u672f\u4ee3\u7801\u8bc4\u4f30\u5668\u5960\u5b9a\u57fa\u7840", "conclusion": "\u5b66\u672f\u8d8a\u72f1\u653b\u51fb\u5bf9LLM\u4ee3\u7801\u8bc4\u4f30\u5668\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5957\u4ef6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840"}}
{"id": "2512.10443", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10443", "abs": "https://arxiv.org/abs/2512.10443", "authors": ["Sabtain Ahmad", "Meerzhan Kanatbekova", "Ivona Brandic", "Atakan Aral"], "title": "Clustered Federated Learning with Hierarchical Knowledge Distillation", "comment": null, "summary": "Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\\%.", "AI": {"tldr": "CFLHKD\uff1a\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u578b\u4e2a\u6027\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u53cc\u5c42\u805a\u5408\u548c\u96c6\u7fa4\u95f4\u77e5\u8bc6\u5171\u4eab\uff0c\u63d0\u5347\u5f02\u6784IoT\u73af\u5883\u4e0b\u7684\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u5b58\u5728\u788e\u7247\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u72ec\u7acb\u8bad\u7ec3\u5168\u5c40\u6a21\u578b\uff0c\u65e0\u6cd5\u5229\u7528\u96c6\u7fa4\u95f4\u7684\u96c6\u4f53\u6d1e\u5bdf\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u96c6\u7fa4\u4e2a\u6027\u5316\uff0c\u53c8\u80fd\u4fc3\u8fdb\u96c6\u7fa4\u95f4\u77e5\u8bc6\u5171\u4eab\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCFLHKD\u65b9\u6848\uff0c\u91c7\u7528\u5c42\u6b21\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u805a\u5408\uff08\u8fb9\u7f18\u96c6\u7fa4\u7279\u5b9a\u6a21\u578b\u548c\u4e91\u7aef\u7edf\u4e00\u5168\u5c40\u6a21\u578b\uff09\u548c\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5b9e\u73b0\u96c6\u7fa4\u95f4\u77e5\u8bc6\u5171\u4eab\u540c\u65f6\u4fdd\u6301\u96c6\u7fa4\u4e2a\u6027\u5316\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cCFLHKD\u5728\u96c6\u7fa4\u7279\u5b9a\u6a21\u578b\u548c\u5168\u5c40\u6a21\u578b\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u8fbe\u52303.32-7.57%\u3002", "conclusion": "CFLHKD\u901a\u8fc7\u5c42\u6b21\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfCFL\u7684\u788e\u7247\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u96c6\u7fa4\u95f4\u77e5\u8bc6\u5171\u4eab\u4e0e\u4e2a\u6027\u5316\u7684\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.10452", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10452", "abs": "https://arxiv.org/abs/2512.10452", "authors": ["Yang Yang", "Li Kuang", "Jiakun Liu", "Zhongxin Liu", "Yingjie Xia", "David Lo"], "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval", "comment": "Accepted by the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "summary": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.", "AI": {"tldr": "UniCoR\uff1a\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u7edf\u4e00\u4ee3\u7801\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u5bf9\u6bd4\u5b66\u4e60\u548c\u5206\u5e03\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u89e3\u51b3\u6df7\u5408\u67e5\u8be2\u4ee3\u7801\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u3001\u6a21\u6001\u878d\u5408\u4f4e\u6548\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u5f31\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u68c0\u7d22\u65b9\u6cd5\u5728\u6df7\u5408\u67e5\u8be2\uff08\u81ea\u7136\u8bed\u8a00+\u4ee3\u7801\u7247\u6bb5\uff09\u548c\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\uff1b2\uff09\u6df7\u5408\u68c0\u7d22\u4e2d\u7684\u6a21\u6001\u878d\u5408\u4f4e\u6548\uff1b3\uff09\u8de8\u8bed\u8a00\u573a\u666f\u6cdb\u5316\u80fd\u529b\u5f31\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5229\u7528\u6df7\u5408\u67e5\u8be2\u5e76\u5177\u5907\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u7684\u4ee3\u7801\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUniCoR\u81ea\u76d1\u7763\u6846\u67b6\uff1a1\uff09\u591a\u89c6\u89d2\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u4ece\u4ee3\u7801-\u4ee3\u7801\u3001\u81ea\u7136\u8bed\u8a00-\u4ee3\u7801\u3001\u81ea\u7136\u8bed\u8a00-\u81ea\u7136\u8bed\u8a00\u591a\u4e2a\u89c6\u89d2\u5bf9\u9f50\u8868\u793a\uff0c\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u548c\u6a21\u6001\u878d\u5408\uff1b2\uff09\u8868\u793a\u5206\u5e03\u4e00\u81f4\u6027\u5b66\u4e60\u6a21\u5757\uff0c\u663e\u5f0f\u5bf9\u9f50\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u7279\u5f81\u5206\u5e03\uff0c\u5b9e\u73b0\u8bed\u8a00\u65e0\u5173\u7684\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u7ecf\u9a8c\u57fa\u51c6\u548c\u5927\u89c4\u6a21\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniCoR\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff0cMRR\u5e73\u5747\u63d0\u53478.64%\uff0cMAP\u5e73\u5747\u63d0\u534711.54%\u3002\u5728\u6df7\u5408\u4ee3\u7801\u68c0\u7d22\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "UniCoR\u901a\u8fc7\u7edf\u4e00\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u67e5\u8be2\u4ee3\u7801\u68c0\u7d22\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u8bed\u4e49\u7406\u89e3\u3001\u6a21\u6001\u878d\u5408\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u4ee3\u7801\u68c0\u7d22\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10576", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10576", "abs": "https://arxiv.org/abs/2512.10576", "authors": ["Xinhang Chen", "Chao Zhang", "Jiahuan He", "Wei Liu", "Jianming Zhang", "Wenlong Zhou", "Xiao Li", "Pai Zeng", "Shiyong Li", "Yuanpan Qian", "Dong Li", "Zhaogeng Li"], "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp", "comment": null, "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.", "AI": {"tldr": "ESS\u7cfb\u7edf\u901a\u8fc7\u5c06Latent-Cache\u5378\u8f7d\u5230CPU\u5185\u5b58\uff0c\u89e3\u51b3\u4e86DeepSeek-V3.2-Exp\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684GPU\u5185\u5b58\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e3\u7801\u9636\u6bb5\u541e\u5410\u91cf\u3002", "motivation": "DeepSeek-V3.2-Exp\u867d\u7136\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u964d\u4f4e\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f46\u89e3\u7801\u9636\u6bb5\u7684PD\u89e3\u8026\u4ecd\u7136\u662f\u4e3b\u8981\u74f6\u9888\u3002\u74f6\u9888\u6e90\u4e8eLatent-Cache\u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u4e0eGPU\u5185\u5b58\u5bb9\u91cf\u6709\u9650\u7684\u51b2\u7a81\uff0c\u8fd9\u9650\u5236\u4e86\u6279\u5904\u7406\u5927\u5c0f\u5e76\u6291\u5236\u4e86\u89e3\u7801\u9636\u6bb5\u541e\u5410\u91cf\u3002", "method": "\u63d0\u51faESS\uff08Extended Sparse Server\uff09\u7cfb\u7edf\uff0c\u91c7\u7528\u5378\u8f7d\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\uff0c\u9009\u62e9\u6027\u5730\u5c06Latent-Cache\u5378\u8f7d\u5230CPU\u5185\u5b58\uff0c\u540c\u65f6\u5c06\u5ef6\u8fdf\u5173\u952e\u7ec4\u4ef6\u4fdd\u7559\u5728GPU\u4e0a\u3002\u901a\u8fc7\u91ca\u653eGPU\u5185\u5b58\uff0cESS\u4f7f\u6279\u5904\u7406\u5927\u5c0f\u6269\u5c55\u4e0eGPU\u5185\u5b58\u7ea6\u675f\u89e3\u8026\u3002", "result": "\u9ad8\u4fdd\u771f\u6a21\u62df\u663e\u793a\uff0cESS\u572832K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u63d0\u4f9b69.4%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u63d0\u4f9b\u9ad8\u8fbe123%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5bf9\u5927\u4e0a\u4e0b\u6587\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6709\u6548\u6027\u3002", "conclusion": "ESS\u662f\u957f\u4e0a\u4e0b\u6587LLM\u670d\u52a1\u7684\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u7ba1\u7406\u663e\u8457\u63d0\u5347\u89e3\u7801\u9636\u6bb5\u541e\u5410\u91cf\uff0c\u964d\u4f4e\u5b9e\u9645\u90e8\u7f72\u6210\u672c\u3002"}}
{"id": "2512.10493", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10493", "abs": "https://arxiv.org/abs/2512.10493", "authors": ["Binquan Zhang", "Li Zhang", "Haoyuan Zhang", "Fang Liu", "Song Wang", "Bo Shen", "An Fu", "Lin Shi"], "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild", "comment": null, "summary": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790LMSYS-Chat-1M\u548cWildChat\u6570\u636e\u96c6\uff0c\u5b9e\u8bc1\u7814\u7a76\u4e86\u4eba\u7c7b\u4e0eLLM\u5728\u7f16\u7a0b\u534f\u4f5c\u4e2d\u7684\u4ea4\u4e92\u673a\u5236\u3001\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u53d1\u73b0\u4efb\u52a1\u7c7b\u578b\u5f71\u54cd\u4ea4\u4e92\u6a21\u5f0f\uff0cbug\u4fee\u590d\u548c\u4ee3\u7801\u91cd\u6784\u5bf9LLM\u66f4\u5177\u6311\u6218\u6027\uff0c\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u7528\u6237\u6ee1\u610f\u5ea6\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u867d\u7136\u5df2\u6709LMSYS-Chat-1M\u548cWildChat\u7b49\u6570\u636e\u96c6\u8bb0\u5f55\u771f\u5b9e\u7528\u6237-LLM\u5bf9\u8bdd\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u7cfb\u7edf\u63a2\u7d22\u7f16\u7a0b\u573a\u666f\u4e2d\u4eba\u7c7b-LLM\u534f\u4f5c\u673a\u5236\u3002\u7528\u6237\u5728\u5b9e\u9645\u4ea4\u4e92\u4e2d\u7ecf\u5386\u4e86\u600e\u6837\u7684\u66f2\u6298\u8def\u5f84\uff1fLLM\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u5982\u4f55\uff1f\u7528\u6237\u6ee1\u610f\u5ea6\u5982\u4f55\uff1f\u8fd9\u4e9b\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528LMSYS-Chat-1M\u548cWildChat\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u8bc1\u5206\u6790\uff0c\u63a2\u7d22\u4eba\u7c7b-LLM\u534f\u4f5c\u673a\u5236\u3001LLM\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u548c\u4eba\u7c7b\u6ee1\u610f\u5ea6\u3002\u901a\u8fc7\u5206\u6790\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\uff0c\u8bc6\u522b\u4e0d\u540c\u7f16\u7a0b\u4efb\u52a1\u7c7b\u578b\u4e0b\u7684\u4ea4\u4e92\u6a21\u5f0f\u3002", "result": "1) \u4efb\u52a1\u7c7b\u578b\u5851\u9020\u4ea4\u4e92\u6a21\u5f0f\uff1a\u4ee3\u7801\u8d28\u91cf\u4f18\u5316\u504f\u597d\u7ebf\u6027\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u9a71\u52a8\u4efb\u52a1\u503e\u5411\u6811\u72b6\u7ed3\u6784\uff0c\u67e5\u8be2\u4efb\u52a1\u504f\u597d\u661f\u578b\u6a21\u5f0f\uff1b2) Bug\u4fee\u590d\u548c\u4ee3\u7801\u91cd\u6784\u5bf9LLM\u6307\u4ee4\u9075\u5faa\u66f4\u5177\u6311\u6218\u6027\uff0c\u4e0d\u9075\u5faa\u7387\u663e\u8457\u9ad8\u4e8e\u4fe1\u606f\u67e5\u8be2\uff1b3) \u4ee3\u7801\u8d28\u91cf\u4f18\u5316\u548c\u9700\u6c42\u9a71\u52a8\u5f00\u53d1\u4efb\u52a1\u7528\u6237\u6ee1\u610f\u5ea6\u8f83\u4f4e\uff0c\u800c\u7ed3\u6784\u5316\u77e5\u8bc6\u67e5\u8be2\u548c\u7b97\u6cd5\u8bbe\u8ba1\u6ee1\u610f\u5ea6\u8f83\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6539\u8fdbLLM\u754c\u9762\u548c\u63d0\u5347\u7f16\u7a0b\u534f\u4f5c\u4e2d\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u63d0\u4f9b\u4e86\u5efa\u8bae\uff0c\u540c\u65f6\u4e3a\u81ea\u9002\u5e94\u5bf9\u8bdd\u7cfb\u7edf\u7684\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002\u8fd9\u9879\u5de5\u4f5c\u62d3\u5bbd\u4e86\u5bf9\u4eba\u7c7b-LLM\u534f\u540c\u4f5c\u7528\u7684\u7406\u89e3\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684AI\u8f85\u52a9\u5f00\u53d1\u3002"}}
{"id": "2512.10618", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10618", "abs": "https://arxiv.org/abs/2512.10618", "authors": ["Georgia M. Kapitsaki", "Maria Papoutsoglou", "Christoph Treude", "Ioanna Theophilou"], "title": "Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories", "comment": "40 pages", "summary": "Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.", "AI": {"tldr": "\u5206\u6790GitHub\u4e0a32,820\u4e2a\u5f00\u6e90\u8f6f\u4ef6issue\uff0c\u7814\u7a76\u5f00\u53d1\u8005\u5982\u4f55\u8ba8\u8bba\u9690\u79c1\u6cd5\u89c4\uff08GDPR/CCPA\uff09\u5408\u89c4\u95ee\u9898\uff0c\u6784\u5efa\u5305\u542b6\u4e2a\u96c6\u7fa424\u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u6cd5", "motivation": "GDPR\u548cCCPA\u7b49\u9690\u79c1\u6cd5\u89c4\u6539\u53d8\u4e86\u8f6f\u4ef6\u5f00\u53d1\u65b9\u5f0f\uff0c\u4f46\u7f3a\u4e4f\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u8005\u5982\u4f55\u8ba8\u8bba\u5408\u89c4\u95ee\u9898\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002GitHub\u4e0a\u6709\u5927\u91cf\u5f00\u53d1\u8005issue\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u9690\u79c1\u6cd5\u89c4\u5408\u89c4\u8ba8\u8bba\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u4eceGitHub\u4ed3\u5e93\u6316\u639832,820\u4e2aissue\uff0c\u81ea\u52a8\u5206\u6790\u8bc6\u522b\u6cd5\u5f8b\u7528\u6237\u6743\u5229\u548c\u539f\u5219\uff0c\u624b\u52a8\u5206\u67901,186\u4e2aissue\u6837\u672c\uff0c\u6839\u636e\u5173\u6ce8\u95ee\u9898\u7c7b\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b6\u4e2a\u96c6\u7fa424\u4e2a\u7c7b\u522b\u7684\u5206\u7c7b\u6cd5\uff1a\u529f\u80fd/\u7f3a\u9677\u3001\u540c\u610f\u76f8\u5173\u3001\u6587\u6863\u3001\u6570\u636e\u5b58\u50a8/\u5171\u4eab\u3001\u9002\u5e94\u6027\u3001\u901a\u7528\u5408\u89c4\u3002\u53d1\u73b0\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u7528\u6237\u6743\u5229\uff08\u5220\u9664\u6743\u3001\u9009\u62e9\u9000\u51fa\u6743\u3001\u8bbf\u95ee\u6743\uff09\uff0c\u8ba8\u8bba\u96c6\u4e2d\u5728\u7528\u6237\u540c\u610f\u3001\u7528\u6237\u6743\u5229\u529f\u80fd\u3001\u7f3a\u9677\u548ccookie\u7ba1\u7406\u3002", "conclusion": "\u5206\u7c7b\u6cd5\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5148\u5904\u7406\u5408\u89c4\u95ee\u9898\uff0c\u6559\u80b2\u754c\u53ef\u8c03\u6574\u8bfe\u7a0b\u57f9\u517b\u9690\u79c1\u6cd5\u89c4\u610f\u8bc6\uff0c\u7814\u7a76\u754c\u53ef\u8bc6\u522b\u652f\u6301\u9690\u79c1\u6cd5\u89c4\u5408\u89c4\u7684\u6539\u8fdb\u9886\u57df\u3002"}}
{"id": "2512.10713", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10713", "abs": "https://arxiv.org/abs/2512.10713", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "comment": null, "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "AI": {"tldr": "PACIFIC\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u987a\u5e8f\u6307\u4ee4\u9075\u5faa\u548c\u4ee3\u7801\u5e72\u8fd0\u884c\u80fd\u529b\uff0c\u53ef\u63a7\u5236\u96be\u5ea6\u5e76\u907f\u514d\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5e38\u4f9d\u8d56\u5de5\u5177\u4f7f\u7528\u6216\u4ee3\u7406\u884c\u4e3a\uff0c\u96be\u4ee5\u8bc4\u4f30LLM\u5185\u5728\u7684\u4ee3\u7801\u63a8\u7406\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u9694\u79bb\u8bc4\u4f30LLM\u9010\u6b65\u63a8\u7406\u4ee3\u7801\u884c\u4e3a\uff08\u5e72\u8fd0\u884c\uff09\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u907f\u514d\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002", "method": "\u63d0\u51faPACIFIC\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u5177\u6709\u660e\u786e\u9884\u671f\u8f93\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u53d8\u4f53\uff0c\u901a\u8fc7\u7b80\u5355\u8f93\u51fa\u6bd4\u8f83\u8fdb\u884c\u53ef\u9760\u8bc4\u4f30\u3002\u6846\u67b6\u53ef\u63a7\u5236\u57fa\u51c6\u96be\u5ea6\uff0c\u8f7b\u677e\u751f\u6210\u65b0\u9896\u53d8\u4f53\u4ee5\u7f13\u89e3\u6570\u636e\u6c61\u67d3\u3002", "result": "\u9a8c\u8bc1\u6846\u67b6\u751f\u6210\u4e86\u6db5\u76d6\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u8bc4\u4f30\u591a\u4e2aSOTA LLM\u3002\u7ed3\u679c\u663e\u793aPACIFIC\u80fd\u4ea7\u751f\u8d8a\u6765\u8d8a\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u6709\u6548\u533a\u5206\u4e0d\u540c\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u548c\u5e72\u8fd0\u884c\u80fd\u529b\u3002", "conclusion": "PACIFIC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u6297\u6c61\u67d3\u7684\u65b9\u6cd5\u8bba\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u7279\u522b\u662f\u987a\u5e8f\u6307\u4ee4\u9075\u5faa\u548c\u4ee3\u7801\u5e72\u8fd0\u884c\u80fd\u529b\u3002"}}
{"id": "2512.10799", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10799", "abs": "https://arxiv.org/abs/2512.10799", "authors": ["Karolina Gorna", "Nicolas Iooss", "Yannick Seurin", "Rida Khatoun"], "title": "Zorya: Automated Concolic Execution of Single-Threaded Go Binaries", "comment": null, "summary": "Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.", "AI": {"tldr": "Zorya\u662f\u4e00\u4e2a\u9488\u5bf9Go\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u7b26\u53f7\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u5c06Go\u4e8c\u8fdb\u5236\u8f6c\u6362\u4e3aGhidra\u7684P-Code\u4e2d\u95f4\u8868\u793a\uff0c\u5e76\u6dfb\u52a0\u591a\u5c42\u8fc7\u6ee4\u673a\u5236\uff0c\u4e13\u6ce8\u4e8epanic\u76f8\u5173\u8def\u5f84\u7684\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6f0f\u6d1e\u68c0\u6d4b\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "Go\u8bed\u8a00\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u589e\u52a0\u4e86\u7cfb\u7edf\u5316\u6f0f\u6d1e\u68c0\u6d4b\u7684\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u7b26\u53f7\u6267\u884c\u5de5\u5177\u5728\u5904\u7406Go\u4e8c\u8fdb\u5236\u6587\u4ef6\u65f6\u9762\u4e34\u8fd0\u884c\u65f6\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u6f0f\u6d1e\u3002", "method": "\u57fa\u4e8eZorya\u6846\u67b6\uff0c\u5c06Go\u4e8c\u8fdb\u5236\u8f6c\u6362\u4e3aGhidra\u7684P-Code\u4e2d\u95f4\u8868\u793a\uff0c\u6dfb\u52a0\u4e86\u5177\u4f53\u672a\u6267\u884c\u8def\u5f84\u7684bug\u68c0\u6d4b\u548c\u591a\u5c42\u8fc7\u6ee4\u673a\u5236\uff0c\u4e13\u6ce8\u4e8epanic\u76f8\u5173\u8def\u5f84\u7684\u7b26\u53f7\u63a8\u7406\u3002\u91c7\u7528\u51fd\u6570\u6a21\u5f0f\u5206\u6790\uff0c\u4ece\u51fd\u6570\u7ea7\u522b\u800c\u975emain\u51fd\u6570\u5f00\u59cb\u5206\u6790\uff0c\u5927\u5e45\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2aGo\u6f0f\u6d1e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1apanic\u53ef\u8fbe\u6027\u95e8\u63a7\u5b9e\u73b0\u4e861.8-3.9\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u8fc7\u6ee4\u4e8633-70%\u7684\u5206\u652f\uff1bZorya\u68c0\u6d4b\u5230\u4e86\u6240\u6709panic\uff0c\u800c\u73b0\u6709\u5de5\u5177\u6700\u591a\u53ea\u80fd\u68c0\u6d4b\u4e24\u4e2a\uff1b\u51fd\u6570\u6a21\u5f0f\u5206\u6790\u4f7f\u590d\u6742\u7a0b\u5e8f\u7684\u5206\u6790\u901f\u5ea6\u6bd4\u4ecemain\u5f00\u59cb\u5feb\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u4e13\u95e8\u7684\u7b26\u53f7\u6267\u884c\u65b9\u6cd5\u53ef\u4ee5\u5728\u5177\u6709\u8fd0\u884c\u65f6\u5b89\u5168\u68c0\u67e5\u7684\u8bed\u8a00\u751f\u6001\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5b9e\u7528\u7684\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u4e3aGo\u7b49\u8bed\u8a00\u7684\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
