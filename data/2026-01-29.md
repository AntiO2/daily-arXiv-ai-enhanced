<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 21]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Achieving Productivity Gains with AI-based IDE features: A Journey at Google](https://arxiv.org/abs/2601.19964)
*Maxim Tabachnyk,Xu Shu,Alexander Frömmgen,Pavel Sychev,Vahid Meimand,Ilia Krets,Stanislav Pyatykh,Abner Araujo,Kristóf Molnár,Satish Chandra*

Main category: cs.SE

TL;DR: Google分享了开发AI代码助手（代码补全和自然语言代码转换）的经验，重点解决延迟、用户体验和质量问题，展示了如何在企业环境中通过多层面优化提升开发效率。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中开发AI驱动的IDE功能面临独特挑战，包括延迟、用户体验和代码建议质量等问题，需要系统性的优化方法来提供实际的生产力提升。

Method: 通过用户界面、后端和模型层的多层面优化，结合严格的实验验证，改进代码补全和自然语言代码转换功能。

Result: 成功开发出能够显著提升开发效率的AI IDE功能，通过系统性优化解决了延迟、用户体验和代码质量等关键问题。

Conclusion: Google的经验表明，通过跨层优化和严格实验，AI开发工具可以在企业环境中提供切实的生产力提升，为类似工具的开发提供了有价值的参考框架。

Abstract: We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.

</details>


### [2] [Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis](https://arxiv.org/abs/2601.20103)
*Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.SE

TL;DR: 该论文提出了TRACE基准测试，用于评估LLM在代码生成强化学习环境中检测奖励攻击的能力，发现在对比性异常检测设置中模型表现更好，GPT-5.2达到63%检测率。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成强化学习的发展，需要健壮的环境来防止奖励攻击。LLM越来越多地作为代码RL中的评估器，但其检测奖励攻击的能力尚未得到充分研究。

Method: 提出了涵盖54个类别的奖励攻击分类法，并创建了TRACE基准测试（包含517个测试轨迹）。在对比性异常检测设置中评估模型性能，而非传统的孤立分类场景。

Result: 模型在对比性设置中检测奖励攻击的效果更好，GPT-5.2在最高推理模式下达到63%检测率（孤立设置中为45%）。模型在语义上下文奖励攻击上的表现明显差于语法上下文攻击。

Conclusion: TRACE基准测试揭示了LLM检测奖励攻击的局限性，特别是在语义上下文攻击方面。对比性评估设置更接近实际场景，有助于改进代码生成RL系统的健壮性。

Abstract: Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.

</details>


### [3] [Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents](https://arxiv.org/abs/2601.20106)
*Shamse Tasnim Cynthia,Joy Krishan Das,Banani Roy*

Main category: cs.SE

TL;DR: 核心与外围开发者在使用自主编码代理时的行为差异研究：外围开发者更频繁使用代理但核心开发者代理PR合并率更高，两者在审查、修改和验证方面存在不同模式。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理改变软件开发方式，需要了解核心与外围开发者如何与这些代理协作。先前研究表明AI工具采用存在差异，但自主编码代理时代下这种动态关系尚不明确。

Method: 对9,427个代理生成的PR进行实证研究，采用定性与定量混合分析方法，考察核心与外围开发者如何使用、审查、修改和验证代理贡献。

Result: 1) 外围开发者更频繁使用代理，任务分布均匀；核心开发者更关注文档和测试，但其代理PR合并率更高。2) 核心开发者审查参与度略高，两者都关注可演化性问题。3) 代理PR修改概率较低，修改时都进行重构。4) 外围开发者更可能跳过CI检查合并，核心开发者更坚持验证通过。

Conclusion: 开发者经验显著影响与编码代理的协作方式，研究为两类开发者提供了有效协作的见解，强调核心开发者更注重质量保证而外围开发者更依赖代理功能。

Abstract: Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.

</details>


### [4] [Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests](https://arxiv.org/abs/2601.20109)
*Shamse Tasnim Cynthia,Al Muttakin,Banani Roy*

Main category: cs.SE

TL;DR: 分析1,210个AI生成的bug修复PR，发现合并成功不代表代码质量好，代码异味普遍存在，问题数量主要受PR大小影响而非AI代理差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理的广泛使用，大量AI生成的PR在很少或没有人工干预的情况下被合并。虽然这些PR承诺提高生产力，但其合并后的代码质量尚未得到充分研究，因为先前工作主要依赖基准测试和受控任务，缺乏大规模合并后分析。

Method: 使用AIDev数据集中的Python仓库，分析1,210个已合并的AI生成bug修复PR。通过SonarQube对基础提交和合并提交进行差异分析，识别PR变更新引入的代码质量问题。从问题频率、密度、严重性和规则级别五个维度比较五个AI代理。

Result: 不同AI代理的原始问题数量差异在按代码变更量标准化后基本消失，表明问题数量主要由PR大小驱动而非代理差异。所有代理中，代码异味占主导地位，尤其在关键和主要严重性级别；bug较少但通常很严重。

Conclusion: 合并成功不能可靠反映合并后的代码质量，强调了对AI生成bug修复PR进行系统性质量检查的必要性。

Abstract: The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.

</details>


### [5] [Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study](https://arxiv.org/abs/2601.20112)
*Maja Vukovic,Rangeet Pan,Tin Kam Ho,Rahul Krishna,Raju Pavuluri,Michele Merler*

Main category: cs.SE

TL;DR: 调查显示AI编程助手和CodeLLMs在现实项目中的使用体验、影响及需求


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，AI编程助手在软件工程任务中应用日益广泛，但它们在真实企业项目中的适用性、对现有开发流程的影响以及用户体验如何尚不明确，需要进行系统研究。

Method: 1. 调查了57名来自不同领域、具有不同技能水平的开发者对AI编程助手和CodeLLMs的使用体验；2. 回顾分析了35份关于专业人士和学生使用AI编程助手的用户调查报告。

Result: 通过调查和现有分析，获得了关于AI编程助手在实际使用中的体验、影响和用户期望的实证数据，为理解这些工具在真实项目中的表现提供了依据。

Conclusion: 基于研究发现和现有调查分析，讨论了AI驱动的编程助手需要满足的关键需求，为未来工具开发和改进提供了方向。

Abstract: The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.

</details>


### [6] [Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization](https://arxiv.org/abs/2601.20147)
*Saima Afrin,Zaiyu Cheng,Tushar Sharma,Alexander Serebrenik,Massimiliano Di Penta,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 本文首次系统评估了系统提示对指令调优语言模型在代码生成任务中性能的影响，发现模型规模越大受提示影响越明显，少样本提示能减少这种影响，且不同编程语言对提示的敏感度不同。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优语言模型在代码生成方面表现出色，但系统提示对其性能的影响尚未得到充分探索。本研究旨在填补这一空白，系统评估系统提示的详细程度、模型规模、提示策略和编程语言等因素对代码生成性能的影响。

Method: 建立了一个评估框架，涵盖120种模型配置，系统评估了不同详细程度的系统提示、模型规模（不同大小的模型）、提示策略（零样本vs少样本）以及编程语言（Python和Java）对指令调优语言模型和代码语言模型在代码生成任务中的影响。

Result: 研究发现：(1) 系统提示的影响随模型规模增大而增强；(2) 与零样本提示相比，少样本提示能减少系统提示的影响；(3) 编程语言很重要，Java比Python对系统提示的变化更敏感。

Conclusion: 系统提示对指令调优语言模型在代码生成任务中的性能有显著影响，且这种影响受模型规模、提示策略和编程语言等因素的调节。这为优化代码生成系统的提示设计提供了重要见解。

Abstract: Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.

</details>


### [7] [LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis](https://arxiv.org/abs/2601.20148)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: LogSieve：一种轻量级、RCA感知且语义保留的日志缩减技术，专门针对CI工作流中的非结构化、嘈杂日志，通过过滤低信息行来减少日志量，同时保留下游推理相关内容。


<details>
  <summary>Details</summary>
Motivation: CI日志对于理解持续集成行为至关重要，但日志量的增长和冗长性使得手动检查和自动化分析变得成本高昂、耗时且环境成本高。现有研究大多针对结构化系统日志，而非CI工作流中典型的非结构化、嘈杂、冗长日志。

Method: LogSieve采用基于嵌入的分类器自动检测相关性，过滤低信息行，保留与根本原因分析相关的语义内容。这是一种轻量级、RCA感知且语义保留的日志缩减技术。

Result: 在20个开源Android项目的GitHub Actions CI日志上评估，LogSieve平均减少42%的行数和40%的token数，语义损失最小。相比基于结构的基线方法（LogZip和随机行移除），LogSieve保持了更高的语义和分类保真度（余弦相似度0.93，GPTScore 0.93，80%精确匹配准确率）。

Conclusion: LogSieve通过嵌入分类器实现接近人类准确率（97%）的相关性检测，为CI工作流提供了可扩展且可持续的语义感知过滤方案，连接了日志管理和LLM推理，为更绿色、更可解释的CI自动化提供了实用路径。

Abstract: Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.
  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.
  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.

</details>


### [8] [Cascaded Vulnerability Attacks in Software Supply Chains](https://arxiv.org/abs/2601.20158)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: 提出基于SBOM的软件供应链安全分析方法，使用异构图注意力网络预测组件漏洞，并通过链接预测发现级联漏洞链


<details>
  <summary>Details</summary>
Motivation: 当前软件安全分析工具通常孤立评估漏洞，但复杂的软件供应链威胁往往来自跨依赖组件的级联漏洞链。此外，不同SBOM生成器和分析工具的下游漏洞发现结果差异很大。

Method: 将增强的SBOM建模为异构图，节点包括SBOM组件、依赖关系、已知漏洞和安全弱点。使用异构图注意力网络(HGAT)预测组件是否关联已知漏洞。将级联发现建模为CVE对的链接预测问题，使用多层感知机神经网络生成候选链接，可组合成多步路径。

Result: HGAT组件分类器达到91.03%的准确率和74.02%的F1分数，能够有效预测组件漏洞关联。

Conclusion: 该方法能够更全面地分析软件供应链安全，通过建模依赖关系和漏洞链，解决了传统孤立漏洞分析的局限性，为SBOM驱动的安全分析提供了新思路。

Abstract: Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.

</details>


### [9] [How do Agents Refactor: An Empirical Study](https://arxiv.org/abs/2601.20160)
*Lukas Ottenhof,Daniel Penner,Abram Hindle,Thibaud Lutellier*

Main category: cs.SE

TL;DR: 该研究首次分析了Java项目中AI代理（如Cursor、Claude Code等）与开发者的重构PR差异，发现代理重构主要集中于注解修改而非结构性改进，且只有Cursor模型显著增加了代码坏味道。


<details>
  <summary>Details</summary>
Motivation: 随着AI编程代理（Claude Code、GitHub Copilot等）在开发工作流中的广泛应用，现有研究主要关注代码补全和任务自动化能力，但缺乏对这些代理在实际Java重构中的表现、所做变更类型及其对代码质量影响的系统性分析。

Method: 研究比较了代理重构和开发者重构的PR，每组分析86个项目。使用RefactoringMiner识别重构类型，使用DesigniteJava 3.0检测重构提交前后的代码坏味道，进行统计分析。

Result: 代理重构主要由注解变更主导（前5种常见重构类型均为注解相关），与开发者多样化的结构性改进形成鲜明对比。尽管重构类型存在差异，但只有Cursor模型显示出统计上显著增加重构坏味道的现象。

Conclusion: AI代理在Java重构中的行为模式与开发者显著不同，主要关注注解层面的修改而非结构性改进。虽然大多数代理未显著降低代码质量，但Cursor模型的表现需要特别关注，这为AI编程工具的评估和改进提供了重要参考。

Abstract: Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.

</details>


### [10] [Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests](https://arxiv.org/abs/2601.20171)
*Kazuma Yamasaki,Joseph Ayobami Joshua,Tasha Settewong,Mahmoud Alfadel,Kazumasa Shimari,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: AI代理在软件文档任务中贡献了大量PR，但人类审查不足，引发文档质量担忧


<details>
  <summary>Details</summary>
Motivation: 随着SE3.0时代AI代理在软件开发中的广泛应用，需要了解其在文档任务中的贡献程度以及人类开发者如何审查和干预，以评估委托工作给AI代理的风险。虽然已有研究关注AI在代码生成等任务中的作用，但文档任务仍未被充分探索。

Method: 使用AIDev工具分析了1,997个文档相关的pull requests（PR），包括AI代理和人类开发者创建的文档PR（创建或修改项目文档）。比较了AI代理和人类在文档PR提交、审查和集成方面的差异。

Result: AI代理在研究的仓库中提交的文档相关PR显著多于人类。AI代理编写的文档编辑通常只需很少的人类后续修改就被集成，这引发了关于审查实践和AI生成文档可靠性的担忧。

Conclusion: 虽然AI代理已经在文档工作流中做出重要贡献，但研究结果表明SE3.0时代文档质量保证和人机协作面临新兴挑战，需要改进审查机制以确保文档可靠性。

Abstract: As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.
  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.

</details>


### [11] [Control Models for In-IDE Code Completion](https://arxiv.org/abs/2601.20223)
*Aral de Moor,Yana Hrynevich,Hleb Badzeika,Vladyslav Furda,Marko Kojic,Artem Savelev,Kostadin Cvejoski,Darya Rovdo,Ekaterina Garanina*

Main category: cs.SE

TL;DR: 该论文介绍了用于JetBrains IDE中LLM代码补全的控制模型，通过机器学习分类器触发推理并过滤建议，以提高用户对齐度和减少不必要请求。


<details>
  <summary>Details</summary>
Motivation: 在IDE中集成LLM驱动的代码补全功能时，需要更好地与用户意图对齐，减少不必要的建议请求，提高整体效率和用户体验。

Method: 使用基于boosting和transformer的架构，在包含98名用户的真实代码补全离线数据集上进行评估，并在多种语法多样化的语言上测试boosting方法的分类性能，最后在生产环境中进行A/B测试。

Result: 控制模型在离线评估中表现良好，在生产环境的A/B测试中提高了代码补全的效率和质量指标。

Conclusion: 辅助模型在IDE中智能集成LLM驱动功能方面具有潜力，为未来研究方向提供了启示，并指出了待解决的问题。

Abstract: We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.

</details>


### [12] [Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development](https://arxiv.org/abs/2601.20240)
*Anthony Peruma,Truman Choy,Gerald Lee,Italo De Oliveira Santos*

Main category: cs.SE

TL;DR: npm包开发者调查显示：开发者重视安全但认为包仅中等安全，主要担忧供应链攻击和依赖漏洞；仅40%满意现有安全工具，偏好自动化方法；改进建议包括更好检测工具、文档、账户保护和教育。


<details>
  <summary>Details</summary>
Motivation: npm生态系统在现代软件开发中至关重要，但第三方包漏洞导致严重安全漏洞。本研究旨在调查npm包开发者如何看待和处理安全问题，了解他们的安全风险认知、实践工具、障碍以及改进建议。

Method: 采用混合方法，对75名npm包开发者进行在线调查，分析他们对安全的理解、实践、工具使用、障碍和改进建议。

Result: 开发者优先考虑安全但认为包仅中等安全，主要担忧供应链攻击、依赖漏洞和恶意代码；仅40%满意现有安全工具（存在警报疲劳问题）；偏好自动化方法（双因素认证、npm audit）而非代码审查；常因废弃或漏洞而删除依赖；对漏洞快速发布补丁；主要障碍包括时间限制和高误报率。

Conclusion: 研究结果有助于npm贡献者和维护者了解普遍安全挑战，促进最佳实践讨论，以增强npm生态系统的安全性和可信度。改进方向包括更好的检测工具、清晰文档、强账户保护和更多教育计划。

Abstract: Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.

</details>


### [13] [How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective](https://arxiv.org/abs/2601.20382)
*Klara Borowa,Andrzej Zalewski,Lech Madeyski*

Main category: cs.SE

TL;DR: 波兰研究者通过ICSE FOSE社区调查分析，指出研究-产业鸿沟对小型经济体软件社区的负面影响，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 作为来自波兰（小型经济体、非英语国家）的研究者，作者希望从少数群体视角分析软件工程社区的关键问题，特别是研究-产业鸿沟对小型社区和本地公司的影响。

Method: 采用反思性主题分析法分析ICSE FOSE（软件工程未来）社区调查数据，结合波兰研究者的实际经验。

Result: 研究发现主要问题是日益扩大的研究-产业鸿沟，这对小型经济体的软件社区和本地公司造成特别大的影响。

Conclusion: 基于分析提出一系列改进建议，旨在增强小型经济体中软件工程研究与产业合作的联系。

Abstract: The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.

</details>


### [14] [Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments](https://arxiv.org/abs/2601.20394)
*Giovanna Broccia,Maurice H. ter Beek,Walter Cazzola,Luca Favalli,Francesco Bertolotti,Alessio Ferrari*

Main category: cs.SE

TL;DR: 该研究通过实验评估Neverlang语言工作台的元语言可理解性和用户接受度，发现用户能充分理解其语法并认可其有用性，但易用性仍是挑战，且可理解性与接受度无显著相关性。


<details>
  <summary>Details</summary>
Motivation: 当前文献在评估语言工作台时往往忽视用户中心方面，如可理解性和接受度。本文旨在填补这一空白，通过实验评估Neverlang这一模块化语言工作台，关注其元语言可理解性和用户接受度。

Method: 采用定制版方法评估模型（MEM），通过三个迭代实验评估Neverlang元语言和程序的可理解性，以及用户接受度（感知易用性、感知有用性和使用意图）。实验涉及学术界参与者。

Result: 用户对Neverlang元语言（特别是语法）表现出充分理解，对其有用性持积极态度，并表达使用意图。但易用性仍是挑战。感知易用性和有用性的变化影响使用意图。令人惊讶的是，可理解性与用户接受度无显著相关性。

Conclusion: 元语言可理解性高并不必然导致更高的接受度，表明理解与采纳之间存在复杂关系。虽然Neverlang在有用性和使用意图方面表现良好，但需要改进易用性以促进更广泛采用。

Abstract: Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.

</details>


### [15] [On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents](https://arxiv.org/abs/2601.20404)
*Jai Lal Lulla,Seyedmoein Mohsenimofidi,Matthias Galster,Jie M. Zhang,Sebastian Baltes,Christoph Treude*

Main category: cs.SE

TL;DR: AGENTS.md文件能显著降低AI编程代理的运行时间和token消耗，同时保持任务完成质量


<details>
  <summary>Details</summary>
Motivation: AI编程代理（如Codex、Claude Code）越来越多地用于自主贡献软件仓库，但人们对仓库级配置工件如何影响代理操作效率知之甚少

Method: 分析10个仓库和124个pull requests，在有无AGENTS.md文件的两种条件下执行代理，测量执行时间和token使用量

Result: AGENTS.md文件的存在与较低的中位数运行时间（减少28.64%）和减少的输出token消耗（减少16.58%）相关，同时保持可比较的任务完成行为

Conclusion: AGENTS.md文件能显著提高AI编程代理的效率，为实际配置和部署提供直接启示，并提出了关于仓库级指令在塑造AI编程代理行为、效率和集成方面的更广泛研究议程

Abstract: AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS.md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS.md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS.md is associated with a lower median runtime ($Δ28.64$%) and reduced output token consumption ($Δ16.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.

</details>


### [16] [An Empirical Evaluation of Modern MLOps Frameworks](https://arxiv.org/abs/2601.20415)
*Jon Marcos-Mercadé,Unai Lopez-Novoa,Mikel Egaña Aranguren*

Main category: cs.SE

TL;DR: 对MLflow、Metaflow、Apache Airflow和Kubeflow Pipelines四种MLOps工具进行实证评估，通过MNIST数字分类和IMDB情感分类两个场景，从安装易用性、配置灵活性、互操作性等六个维度比较，提供加权结果和实用建议。


<details>
  <summary>Details</summary>
Motivation: 随着AI解决方案在专业环境中的日益普及，开发者需要对当前工具生态有清晰的了解，以便为ML模型生命周期管理做出明智的工具选择决策。

Method: 通过实现两个常见ML场景（MNIST数字分类器和IMDB+BERT情感分类器），从六个评估标准（安装易用性、配置灵活性、互操作性、代码插桩复杂性、结果可解释性、文档质量）对四种MLOps工具进行实证评估。

Result: 提供了加权评估结果，明确了不同工具在不同场景下的适用性，为开发者选择最适合特定需求的MLOps工具提供了实用指导。

Conclusion: 不同MLOps工具各有优劣，适用于不同的使用场景和需求，开发者应根据具体项目需求（如易用性、灵活性、扩展性等）选择最合适的工具。

Abstract: Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.

</details>


### [17] [Challenges in Android Data Disclosure: An Empirical Study](https://arxiv.org/abs/2601.20459)
*Mugdha Khedkar,Michael Schlichtig,Mohamed Soliman,Eric Bodden*

Main category: cs.SE

TL;DR: 论文通过调查41名Android开发者和分析172个在线讨论，发现开发者在填写Google Play数据安全部分表单时面临挑战：手动分类隐私数据、依赖在线资源、对识别收集的数据有信心但缺乏转换为合规披露的信心。


<details>
  <summary>Details</summary>
Motivation: 当前法律框架要求Android开发者准确报告应用收集的数据，但大型代码库使得这一报告具有挑战性。本研究旨在了解开发者在填写Google Play数据安全部分(DSS)表单时的实际体验和挑战。

Method: 采用实证研究方法：1) 调查41名Android开发者，了解他们如何将隐私相关数据分类到DSS类别以及填写表单时的信心水平；2) 分析172个在线开发者讨论，包含642名开发者的观点。两种数据源共代表683名开发者的见解。

Result: 研究发现：开发者通常手动将应用收集的隐私数据分类到Google定义的类别中（有时完全省略分类），并严重依赖现有在线资源填写表单。开发者对识别应用收集的数据有信心，但缺乏将这些知识转化为DSS合规披露的信心。主要挑战包括：识别隐私相关数据、对表单理解有限、担心因与Google隐私要求不符而导致应用被拒。

Conclusion: 研究结果强调需要更清晰的指导和更易用的工具来支持开发者满足隐私意识报告义务。当前的DSS表单填写过程存在显著挑战，需要改进以帮助开发者更准确、更自信地完成隐私数据披露。

Abstract: Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.
  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.
  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.
  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.

</details>


### [18] [DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning](https://arxiv.org/abs/2601.20615)
*Yanlin Wang,Jiadong Wu,Tianyue Jiang,Mingwei Liu,Jiachi Chen,Chong Wang,Ensheng Shi,Xilin Liu,Yuchi Ma,Zibin Zheng*

Main category: cs.SE

TL;DR: DrainCode是一种针对RAG代码生成系统的对抗攻击，通过污染检索上下文迫使LLM生成更长输出，从而显著增加GPU延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码生成方面表现出色，但LLM推理的计算成本（延迟和能耗）在安全领域关注不足。现有研究主要关注功能安全，而计算效率安全被忽视。

Method: 采用基于突变的对抗攻击方法，策略性地污染检索上下文，迫使LLM生成显著更长的输出，从而增加GPU计算负载。

Result: 实验显示DrainCode能实现高达85%的延迟增加、49%的能耗增加和3倍以上的输出长度增加。攻击在不同提示策略下具有通用性，且能绕过多种防御。

Conclusion: DrainCode是首个针对RAG代码生成系统计算效率的对抗攻击，揭示了LLM在资源受限环境中的安全风险，为评估LLM安全性提供了新方法。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.

</details>


### [19] [Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model](https://arxiv.org/abs/2601.20662)
*Julien Malka,Arnout Engelen*

Main category: cs.SE

TL;DR: Lila是一个去中心化的可重现构建监控系统，针对功能包管理模型设计，通过分布式报告和聚合构建结果来解决大规模可重现构建监控的挑战。


<details>
  <summary>Details</summary>
Motivation: 软件构建完整性的重要性日益增加，但大规模采用可重现构建面临两大挑战：跨大型软件集合实现高可重现率，以及建立能够在大规模运行的可重现性监控基础设施。虽然Nix生态系统已展示超过90%的可重现率，但有效的可重现性监控问题仍未解决。

Method: 提出Lila系统，这是一个针对功能包管理模型的去中心化可重现性评估系统。Lila支持分布式报告构建结果，并将这些结果聚合到可重现性数据库中。

Result: Lila系统能够为实践者和未来的经验性构建可重现性研究提供支持，通过分布式监控机制解决大规模可重现构建的监控挑战。

Conclusion: Lila解决了可重现构建监控的关键挑战，为软件分发中的透明度和信任提供了基础设施支持，特别适用于功能包管理模型的大规模部署。

Abstract: Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.
  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.
  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.

</details>


### [20] [ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler](https://arxiv.org/abs/2601.20755)
*Bohua Zou,Debayan Roy,Dhimankumar Yogesh Airao,Weihao Xu,Binqi Sun,Yutao Liu,Haibo Chen*

Main category: cs.SE

TL;DR: 开发了一个基于eBPF的细粒度、非侵入式LLM推理引擎性能分析框架，能够在不修改源代码的情况下提供算子级可见性，帮助开发者理解推理过程中的性能瓶颈。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型从研究转向生产应用，理解推理引擎在实时环境中的行为变得至关重要但难以实现。现有的LLM推理系统缺乏算子级可见性，开发者无法了解时间和资源消耗的具体分布，甚至连基本问题（如工作负载是内存受限还是计算受限）都难以回答。

Method: 基于扩展的伯克利包过滤器（eBPF）技术，开发了一个非侵入式性能分析框架。该系统动态地将探针附加到运行时函数的多个层次，无需修改或重新编译源代码。收集的跟踪数据被转换为丰富的可视化图表，包括算子、计算图、时间线和硬件计数器趋势。

Result: 该框架能够揭示密集推理、混合专家路由和算子卸载在实际中的行为表现。系统运行时开销低于4%，具有高保真度的性能分析能力，使LLM推理变得透明且可诊断。

Conclusion: 该框架将性能分析转变为实用的优化工具，可用于调度和资源感知部署，解决了LLM推理引擎缺乏可见性的问题，为生产环境中的性能优化提供了有效手段。

Abstract: As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.

</details>


### [21] [Context-Augmented Code Generation Using Programming Knowledge Graphs](https://arxiv.org/abs/2601.20810)
*Shahd Seddik,Fahd Seddik,Iman Saberi,Fatemeh Fard,Minh Hieu Huynh,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 提出Programming Knowledge Graph (PKG)方法，通过语义表示和细粒度检索增强LLM代码生成能力，解决RAG中检索不精确和生成幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现出色，但在处理复杂问题时存在困难。检索增强生成(RAG)通过整合外部知识来缓解这一问题，但现有检索模型经常错过相关上下文，生成模型也会因无关数据产生幻觉

Method: 提出编程知识图(PKG)方法，将外部数据结构化为更细粒度的节点以提高检索粒度。通过树剪枝技术提升检索精度，并通过重排序机制整合非RAG解决方案来减少幻觉

Result: 在HumanEval和MBPP基准测试中，pass@1准确率提升高达20%，在MBPP上比基线方法提升34%。PKG方法有效处理复杂问题，同时对原本正确的非RAG解决方案影响最小

Conclusion: 提出的PKG方法结合重排序机制能有效解决复杂代码生成问题，提高检索精度并减少幻觉，同时保持对已有正确解决方案的最小负面影响

Abstract: Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [DBTuneSuite: An Extendible Experimental Suite to Test the Time Performance of Multi-layer Tuning Options on Database Management Systems](https://arxiv.org/abs/2601.20015)
*Amani Agrawal,Tianxin Wang,Dennis Shasha*

Main category: cs.DB

TL;DR: DBTuneSuite是一个针对四种广泛部署的免费数据库系统的实验套件，测试它们在各种查询/更新负载和调优选项下的性能，提供可扩展的测试脚本、系统选择建议和调优选项的定量分析。


<details>
  <summary>Details</summary>
Motivation: 为数据库系统工程师、高级用户和故障排除人员以及学生提供实用的性能评估工具，帮助理解不同数据库系统在各种工作负载下的表现差异，以及调优选项的实际效果。

Method: 开发了一个实验套件，包含数据生成脚本、安装和运行测试的脚本，支持扩展到其他测试和系统。对四种广泛部署的免费数据库系统进行性能测试，涵盖各种查询/更新负载和不同的调优选项。

Result: 提供了哪些系统最适合特定查询类型的建议，并给出了定量证据表明实践中广泛使用的调优选项在不同系统间表现差异很大。套件具有可扩展性，可用于更多测试和系统。

Conclusion: DBTuneSuite是一个实用的数据库性能评估工具，为数据库专业人员提供了系统选择依据和调优指导，揭示了不同数据库系统调优选项的差异性，对实际部署和故障排除有重要价值。

Abstract: DBTuneSuite is a suite of experiments on four widely deployed free database systems to test their performance under various query/upsert loads and under various tuning options. The suite provides: (i) scripts to generate data and to install and run tests, making it expandable to other tests and systems; (ii) suggestions of which systems work best for which query types; and (iii) quantitative evidence that tuning options widely used in practice can behave very differently across systems. This paper is most useful for database system engineers, advanced database users and troubleshooters, and students.

</details>


### [23] [Delta Fair Sharing: Performance Isolation for Multi-Tenant Storage Systems](https://arxiv.org/abs/2601.20030)
*Tyler Griggs,Soujanya Ponnapalli,Dev Bali,Wenjie Ma,James DeLoye,Audrey Cheng,Jaewan Hong,Natacha Crooks,Scott Shenker,Ion Stoica,Matei Zaharia*

Main category: cs.DB

TL;DR: Delta Fair Sharing算法族为具有高抢占延迟的存储系统提供性能隔离，通过δ-公平性和δ-帕累托效率保证客户端延迟有界且资源高利用率


<details>
  <summary>Details</summary>
Motivation: 现代云存储系统需要为多租户提供性能隔离，但传统公平共享方法因资源（如写缓冲区和读缓存）的高抢占延迟而失败，导致客户端尾部延迟出现不可接受的尖峰

Method: 提出Delta Fair Sharing算法族，满足δ-公平性（客户端获得公平份额的延迟不超过δ时间单位）和δ-帕累托效率（将未使用资源分配给需求未满足的客户端），并在FAIRDB（RocksDB扩展）中实现

Result: FAIRDB评估显示，相比现有最优方案，能更好地将行为良好的客户端与高需求工作负载隔离开来

Conclusion: Delta Fair Sharing算法通过端到端捕获资源获取延迟，将行为良好客户端的尾部延迟尖峰限制在δ时间单位内，同时确保高资源利用率，为具有高抢占延迟的存储系统提供了有效的性能隔离解决方案

Abstract: Modern storage systems, often deployed to support multiple tenants in the cloud, must provide performance isolation. Unfortunately, traditional approaches such as fair sharing do not provide performance isolation for storage systems, because their resources (e.g., write buffers and read caches) exhibit high preemption delays. These delays lead to unacceptable spikes in client tail latencies, as clients may be forced to wait arbitrarily long to receive their fair share of resources.
  We introduce Delta Fair Sharing, a family of algorithms for sharing resources with high preemption delays. These algorithms satisfy two key properties: $δ$-fairness, which bounds a client's delay in receiving its fair share of resources to $δ$ time units, and $δ$-Pareto-efficiency, which allocates unused resources to clients with unmet demand. Together, these properties capture resource-acquisition delays end-to-end, bound well-behaved clients' tail-latency spikes to $δ$ time units, and ensure high utilization. We implement such algorithms in FAIRDB, an extension of RocksDB. Our evaluation shows that FAIRDB isolates well-behaved clients from high-demand workloads better than state-of-the-art alternatives.

</details>


### [24] [ConStruM: A Structure-Guided LLM Framework for Context-Aware Schema Matching](https://arxiv.org/abs/2601.20482)
*Houming Chen,Zhe Zhang,H. V. Jagadish*

Main category: cs.DB

TL;DR: ConStruM是一个用于模式匹配中预算化证据打包的结构引导框架，通过构建轻量级可重用结构，在查询时组装强调最具区分性证据的小型上下文包，以提升LLM在模式匹配中的准确性。


<details>
  <summary>Details</summary>
Motivation: 在数据集成中，模式匹配是核心任务，列名和描述对此很有价值。LLM可以利用这些自然语言模式元数据，但许多数据集需要超出列本身的额外证据。由于向LLM提供捕获这些证据所需的整个模式元数据不切实际，核心挑战在于选择和组织最有用的上下文信息。

Method: ConStruM构建轻量级可重用结构，在查询时组装小型上下文包，强调最具区分性证据。作为附加组件，给定上游匹配器生成的候选目标短列表，它用结构化、查询特定的证据增强匹配器的最终LLM提示。开发了用于预算化多级上下文检索的上下文树和全局相似性超图，通过在线计算或离线预计算的组感知区分线索来总结高度相似列组。

Result: 在真实数据集上的实验表明，ConStruM通过提供和组织正确的上下文证据来改进模式匹配。

Conclusion: ConStruM是一个有效的结构引导框架，能够通过预算化证据打包和上下文组织，显著提升LLM在模式匹配任务中的性能，解决了传统方法中上下文信息过载的问题。

Abstract: Column matching is a central task in reconciling schemas for data integration. Column names and descriptions are valuable for this task. LLMs can leverage such natural-language schema metadata. However, in many datasets, correct matching requires additional evidence beyond the column itself. Because it is impractical to provide an LLM with the entire schema metadata needed to capture this evidence, the core challenge becomes to select and organize the most useful contextual information.
  We present ConStruM, a structure-guided framework for budgeted evidence packing in schema matching. ConStruM constructs a lightweight, reusable structure in which, at query time, it assembles a small context pack emphasizing the most discriminative evidence. ConStruM is designed as an add-on: given a shortlist of candidate targets produced by an upstream matcher, it augments the matcher's final LLM prompt with structured, query-specific evidence so that the final selection is better grounded. For this purpose, we develop a context tree for budgeted multi-level context retrieval and a global similarity hypergraph that surfaces groups of highly similar columns (on both the source and target sides), summarized via group-aware differentiation cues computed online or precomputed offline. Experiments on real datasets show that ConStruM improves matching by providing and organizing the right contextual evidence.

</details>


### [25] [ALER: An Active Learning Hybrid System for Efficient Entity Resolution](https://arxiv.org/abs/2601.20664)
*Dimitrios Karapiperis,Leonidas Akritidis,Panayiotis Bozanis,Vassilios Verykios*

Main category: cs.DB

TL;DR: ALER：一种新颖的半监督实体解析管道，通过冻结的双编码器架构和轻量级分类器解决了传统主动学习中的计算瓶颈问题，在保持高准确率的同时显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 当前监督式深度学习实体解析模型需要大量标注数据，而主动学习方法虽然能缓解标注稀缺问题，但存在严重的可扩展性瓶颈，包括重新训练复杂模型的高计算成本和NP-hard选择问题。

Method: ALER采用半监督管道：1）使用冻结的双编码器架构生成静态嵌入，避免重复训练；2）在嵌入上迭代训练轻量级分类器；3）通过K-Means对代表性数据样本进行语义分块，解决大规模候选池的内存瓶颈；4）提出结合"困惑"和"自信"对的混合查询策略，高效优化决策边界并纠正高置信度错误。

Result: 在DBLP大规模数据集上的评估显示，ALER训练循环加速1.3倍，解析延迟降低3.8倍，相比最快基线显著提升了效率。

Conclusion: ALER成功弥合了语义准确性和计算可扩展性之间的差距，为大规模实体解析应用提供了一种高效实用的解决方案。

Abstract: Entity Resolution (ER) is a critical task for data integration, yet state-of-the-art supervised deep learning models remain impractical for many real-world applications due to their need for massive, expensive-to-obtain labeled datasets. While Active Learning (AL) offers a potential solution to this "label scarcity" problem, existing approaches introduce severe scalability bottlenecks. Specifically, they achieve high accuracy but incur prohibitive computational costs by re-training complex models from scratch or solving NP-hard selection problems in every iteration. In this paper, we propose ALER, a novel, semi-supervised pipeline designed to bridge the gap between semantic accuracy and computational scalability. ALER eliminates the training bottleneck by using a frozen bi-encoder architecture to generate static embeddings once and then iteratively training a lightweight classifier on top. To address the memory bottleneck associated with large-scale candidate pools, we first select a representative sample of the data and then use K-Means to partition this sample into semantically coherent chunks, enabling an efficient AL loop. We further propose a hybrid query strategy that combines "confused" and "confident" pairs to efficiently refine the decision boundary while correcting high-confidence errors.Extensive evaluation demonstrates ALER's superior efficiency, particularly on the large-scale DBLP dataset: it accelerates the training loop by 1.3x while drastically reducing resolution latency by a factor of 3.8 compared to the fastest baseline.

</details>


### [26] [The Monotone Priority System: Foundations of Contract-Specific Sequencing](https://arxiv.org/abs/2601.20783)
*Naveen Durvasula*

Main category: cs.DB

TL;DR: 提出基于全局优先级的智能合约调用排序系统，通过五个公理证明其唯一性


<details>
  <summary>Details</summary>
Motivation: 现代区块链应用需要为交易设置排序约束，但需要在表达能力和区块生产的可处理性之间取得平衡

Method: 允许合约开发者为每个调用设置整数全局优先级，但调用的优先级不能高于其引用的任何调用的优先级。区块生产者只需按优先级从高到低排序交易

Result: 证明该系统是满足五个独立公理的唯一系统

Conclusion: 提出的优先级排序系统为智能合约调用提供了原则性且公理化的排序约束方法，平衡了表达能力和可处理性

Abstract: Modern blockchain applications benefit from the ability to specify sequencing constraints on the transactions that interact with them. This paper proposes a principled and axiomatically justified way of adding sequencing constraints on smart contract function calls that balances expressivity with the tractability of block production. Specifically, we propose a system in which contract developers are allowed to set an integer global priority for each of their calls, so long as that the call's chosen priority is no higher than the priority of any of its referenced calls. Block builders must then simply sequence transactions in priority order (from high to low priority), breaking ties however they would like. We show that this system is the unique system that satisfies five independent axioms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets](https://arxiv.org/abs/2601.20113)
*Arshan Khan,Rohit Deshmukh,Ben O'Neill*

Main category: cs.DC

TL;DR: 提出一种基于数据驱动的科学数据压缩方法Discontinuous DLS，通过局部时空子空间提升压缩效率，在保证误差边界的前提下显著减少存储需求。


<details>
  <summary>Details</summary>
Motivation: 科学模拟数据量快速增长给存储和传输带来巨大挑战，需要误差有界的有损压缩技术来减少数据规模，同时确保重建数据仍可用于科学分析。

Method: Discontinuous DLS方法利用数据驱动的局部时空子空间，基于底层数据结构构建局部化模型，在分布式计算环境中使用MPI实现，适用于流体动力学、环境模拟等高维时变数据集。

Result: 相比现有误差有界压缩方法，Discontinuous DLS在压缩比和重建精度方面表现优异，能显著降低存储需求而不损害关键数据保真度。

Conclusion: Discontinuous DLS是大规模科学数据压缩的有前景方法，为高性能计算环境中管理日益增长的科学模拟数据需求提供了稳健解决方案。

Abstract: The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.

</details>


### [28] [StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs](https://arxiv.org/abs/2601.20273)
*Jiacheng Yang,Jun Wu,Yaoyao Ding,Zhiying Xu,Yida Wang,Gennady Pekhimenko*

Main category: cs.DC

TL;DR: StreamFusion是一个针对Diffusion Transformers的高效服务引擎，通过拓扑感知的序列并行、Torus Attention和单边通信技术，解决了现有并行方法在通信模式、延迟和同步开销方面的限制，性能提升最高达1.77倍。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率图像和长视频生成需求的增长，单GPU推理效率低下。现有序列并行技术存在三个主要问题：1) 对现代GPU机器网络拓扑的通信模式不优化；2) 机器间all-to-all操作造成延迟瓶颈；3) 使用双边通信库带来的GPU发送-接收同步和计算开销。

Method: StreamFusion包含三个关键技术：1) 考虑机器间和机器内带宽差异的拓扑感知序列并行技术；2) Torus Attention，一种新的序列并行技术，允许机器间all-to-all操作与计算重叠；3) 最小化GPU发送-接收同步和计算开销的单边通信实现。

Result: 实验表明，StreamFusion在性能上平均优于现有最先进方法1.35倍，最高可达1.77倍。

Conclusion: StreamFusion通过解决现有序列并行技术的通信和同步瓶颈，为Diffusion Transformers提供了高效的服务引擎，显著提升了高分辨率图像和长视频生成的推理效率。

Abstract: Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\times$ (up to $1.77\times$).

</details>


### [29] [SuperInfer: SLO-Aware Rotary Scheduling and Memory Management for LLM Inference on Superchips](https://arxiv.org/abs/2601.20309)
*Jiahuan Yu,Mingtao Hu,Zichao Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: SuperInfer：基于Superchips的高性能LLM推理系统，通过RotaSched调度器和DuplexKV引擎解决KV缓存内存不足时的延迟SLO问题


<details>
  <summary>Details</summary>
Motivation: LLM服务面临严格的延迟SLO与有限的GPU内存容量之间的根本矛盾。当高请求率耗尽KV缓存预算时，现有系统会出现严重的队头阻塞问题。虽然先前工作探索了PCIe卸载方案，但这些方法无法在高请求率下保持响应性，难以满足严格的TTFT和TBT SLO要求。

Method: 1. 针对NVLink-C2C紧密耦合GPU-CPU架构的Superchips设计SuperInfer系统；2. 提出RotaSched：首个主动的、SLO感知的轮转调度器，通过轮转请求在Superchips上保持响应性；3. 开发DuplexKV：优化的轮转引擎，支持NVLink-C2C上的全双工传输。

Result: 在GH200上使用各种模型和数据集的评估显示，SuperInfer将TTFT SLO达成率提高了高达74.7%，同时保持了与最先进系统相当的TBT和吞吐量。

Conclusion: SLO感知调度和内存协同设计释放了Superchips在响应性LLM服务中的全部潜力，SuperInfer展示了在紧密耦合GPU-CPU架构上实现高性能LLM推理的有效途径。

Abstract: Large Language Model (LLM) serving faces a fundamental tension between stringent latency Service Level Objectives (SLOs) and limited GPU memory capacity. When high request rates exhaust the KV cache budget, existing LLM inference systems often suffer severe head-of-line (HOL) blocking. While prior work explored PCIe-based offloading, these approaches cannot sustain responsiveness under high request rates, often failing to meet tight Time-To-First-Token (TTFT) and Time-Between-Tokens (TBT) SLOs. We present SuperInfer, a high-performance LLM inference system designed for emerging Superchips (e.g., NVIDIA GH200) with tightly coupled GPU-CPU architecture via NVLink-C2C. SuperInfer introduces RotaSched, the first proactive, SLO-aware rotary scheduler that rotates requests to maintain responsiveness on Superchips, and DuplexKV, an optimized rotation engine that enables full-duplex transfer over NVLink-C2C. Evaluations on GH200 using various models and datasets show that SuperInfer improves TTFT SLO attainment rates by up to 74.7% while maintaining comparable TBT and throughput compared to state-of-the-art systems, demonstrating that SLO-aware scheduling and memory co-design unlocks the full potential of Superchips for responsive LLM serving.

</details>


### [30] [Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics](https://arxiv.org/abs/2601.20389)
*Xiao Yang,Yinan Ni,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: 提出一个统一的多任务竞争分类框架，通过表示变换、图结构建模和任务解耦机制，在高维系统环境中准确识别多种竞争类型。


<details>
  <summary>Details</summary>
Motivation: 在高维系统环境中准确识别多种任务竞争类型具有挑战性，需要处理复杂的跨维度动态特征和资源依赖关系，现有方法难以有效捕捉竞争传播模式和结构干扰。

Method: 1) 从高维指标序列构建系统状态表示，应用非线性变换提取跨维度动态特征；2) 引入基于图的建模机制捕捉指标间潜在依赖关系，学习竞争传播模式；3) 设计任务特定映射结构建模不同竞争类型差异；4) 采用自适应多任务损失权重策略平衡共享和特定特征学习。

Result: 在公开系统跟踪数据集上的实验显示，在准确率、召回率、精确率和F1分数方面具有优势。对批量大小、训练样本规模和指标维度的敏感性分析进一步证实了模型的稳定性和适用性。

Conclusion: 基于高维指标的结构化表示和多任务分类能显著改善竞争模式识别，为复杂计算环境中的性能管理提供了可靠的技术途径。

Abstract: This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.

</details>


### [31] [Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT](https://arxiv.org/abs/2601.20408)
*Nicholas Santavas,Kareem Eissa,Patrycja Cieplicka,Piotr Florek,Matteo Nulli,Stefan Vasilev,Seyyed Hadi Hashemi,Antonios Gasteratos,Shahram Khadivi*

Main category: cs.DC

TL;DR: OptiKIT是一个分布式LLM优化框架，通过自动化复杂优化工作流程，使非专业团队能够进行模型压缩和调优，在GPU受限环境下实现2倍以上的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 企业LLM部署面临可扩展性挑战：需要在有限计算预算内系统优化模型，但手动优化所需的专业知识稀缺且难以获取。特别是在异构基础设施上管理GPU利用率，同时让具有不同工作负载和有限LLM优化经验的团队高效部署模型。

Method: OptiKIT框架提供动态资源分配、分阶段管道执行（带自动清理）和无缝企业集成。它自动化了复杂的优化工作流程，包括资源分配算法、管道编排和集成模式，使非专家团队能够进行模型压缩和调优。

Result: 在生产环境中，OptiKIT实现了超过2倍的GPU吞吐量提升，同时使应用团队能够在没有深厚LLM优化专业知识的情况下获得一致的性能改进。该系统已开源以促进外部贡献和更广泛的可复现性。

Conclusion: OptiKIT通过自动化复杂优化流程，民主化了模型优化，解决了企业LLM部署中的可扩展性挑战。它使非专业团队能够在资源受限环境下高效部署模型，并开源系统以促进社区贡献和可复现性。

Abstract: Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.
  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.
  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.

</details>


### [32] [Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads](https://arxiv.org/abs/2601.20435)
*Aleix Roca,Vicenç Beltran*

Main category: cs.DC

TL;DR: 提出用户空间调度框架USF和SCHED_COOP调度策略，通过用户空间实现减少干扰的进程调度，在过载场景下提升性能达2.4倍


<details>
  <summary>Details</summary>
Motivation: HPC与AI融合导致复杂并行应用增多，多个并行运行时共存给传统OS调度器带来压力。过载时OS调度器的周期性抢占会引入干扰，降低性能

Method: 开发用户空间调度框架USF，完全在用户空间实现，无需特殊权限。实现SCHED_COOP协作调度策略，仅在阻塞时切换线程，减少干扰。通过扩展GNU C库和nOS-V运行时实现，支持多个运行时协调

Result: 在过载多进程场景下性能提升达2.4倍，包括嵌套BLAS工作负载、多进程PyTorch推理（LLaMA-3）和分子动力学模拟

Conclusion: USF框架和SCHED_COOP调度策略能有效减少调度干扰，解决LHP、LWP和可扩展性崩溃等问题，为复杂并行应用提供更好的调度支持

Abstract: The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.

</details>


### [33] [AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling](https://arxiv.org/abs/2601.20595)
*Xinwei Qiang,Yue Guan,Zhengding Hu,Yufei Ding,Adnan Aziz*

Main category: cs.DC

TL;DR: AutoOverlap：一个编译器与运行时系统，通过在单个融合内核内实现细粒度通信与计算重叠，解决大规模GPU工作负载中的通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有分布式编译器主要通过流级别重叠整个计算与通信内核来解决通信瓶颈，但这种粗粒度方法存在额外内核启动、设备级同步以及通信尾部空闲时间等问题。

Method: 引入通信块抽象，将通信粒度与内核结构和后端机制解耦；通过源到源编译器在Triton上实现，根据块调度对本地Triton内核进行转换，使计算与块可用性对齐。

Result: 在多GPU工作负载上实现平均1.3倍端到端加速，最高可达4.7倍加速。

Conclusion: AutoOverlap通过细粒度重叠显著提升多GPU工作负载性能，其通信块抽象和编译器转换方法为解决通信瓶颈提供了有效方案。

Abstract: Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\times$ and up to 4.7$\times$ on multi-GPU workloads.

</details>


### [34] [OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows](https://arxiv.org/abs/2601.20655)
*June Chen,Neal Xu,Gragas Huang,Bok Zhou,Stephen Liu*

Main category: cs.DC

TL;DR: OnePiece是一个基于RDMA优化的大规模分布式推理系统，专为多阶段AIGC工作流设计，通过微服务分解和单边RDMA通信显著降低延迟和CPU开销，提高GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 现有AIGC系统在处理并发工作负载时面临吞吐量、资源利用率和可扩展性方面的效率低下问题，需要更高效的分布式推理解决方案。

Method: 将流水线分解为细粒度微服务，利用单边RDMA通信减少节点间延迟和CPU开销；采用新颖的双环缓冲区设计解决RDMA感知内存访问的死锁问题；通过动态节点管理器根据实时负载弹性分配资源。

Result: 在Wan2.1图像到视频生成任务中，相比单体推理流水线，OnePiece将GPU资源消耗降低了16倍，提供了可扩展、容错且高效的AIGC生产环境解决方案。

Conclusion: OnePiece通过RDMA优化和分布式架构设计，为大规模AIGC工作流提供了显著的性能提升和资源效率，是生产环境中高效、可扩展的推理系统。

Abstract: The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.

</details>


### [35] [Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing](https://arxiv.org/abs/2601.20764)
*Saeed Akbar,Muhammad Waqas,Rahmat Ullah*

Main category: cs.DC

TL;DR: 提出Agentic Fog (AF)模型，将雾节点表示为基于策略的自主代理，通过共享内存和局部协调进行p2p通信，在异步更新和节点故障下保证收敛稳定，相比贪婪启发式和整数线性规划在动态条件下实现更低延迟和更好适应性。


<details>
  <summary>Details</summary>
Motivation: 雾计算和边缘计算需要能够处理部分可观测性、严格延迟要求和动态变化工作负载的自适应控制方案。现有基于大语言模型的Agentic AI工具由于计算成本高、随机性强和形式化分析能力差，不适用于基础设施级系统。

Method: 提出Agentic Fog (AF)通用模型：1) 雾节点表示为策略驱动的自主代理；2) 基于共享内存和局部协调进行p2p通信；3) 将系统目标分解为抽象策略指导；4) 将去中心化雾协调形式化为精确势博弈；5) 保证在异步更新、有界理性最佳响应动态和节点故障下的收敛稳定性。

Result: 仿真表明：1) AF系统相比贪婪启发式和整数线性规划实现更低的平均延迟；2) 在动态条件下更高效地适应变化需求；3) 敏感性分析显示在不同内存和协调条件下都能保持最优性能。

Conclusion: AF模型为雾计算提供了一种形式化可分析、计算高效且稳定的自主协调框架，解决了现有Agentic AI工具在基础设施系统中的适用性问题，在动态工作负载下表现出优越的性能和适应性。

Abstract: Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.

</details>
