{"id": "2602.15968", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.15968", "abs": "https://arxiv.org/abs/2602.15968", "authors": ["Pedro Reynolds-Cu\u00e9llar", "Marisol Wong-Villacres", "Adriana Alvarado Garcia", "Heila Precel"], "title": "From Reflection to Repair: A Scoping Review of Dataset Documentation Tools", "comment": "to be published at the CHI conference on Human Factors in Computing Systems", "summary": "Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.", "AI": {"tldr": "\u5bf959\u7bc7\u6570\u636e\u96c6\u6587\u6863\u5de5\u5177\u6587\u732e\u7684\u7cfb\u7edf\u7efc\u8ff0\u53d1\u73b0\uff0c\u6587\u6863\u5de5\u5177\u8bbe\u8ba1\u5b58\u5728\u56db\u5927\u969c\u788d\uff1a\u4ef7\u503c\u5b9a\u4e49\u6a21\u7cca\u3001\u8bbe\u8ba1\u8131\u79bb\u4e0a\u4e0b\u6587\u3001\u5ffd\u89c6\u52b3\u52a8\u9700\u6c42\u3001\u96c6\u6210\u88ab\u89c6\u4e3a\u672a\u6765\u5de5\u4f5c\uff0c\u5efa\u8bae\u8f6c\u5411\u673a\u6784\u5316\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u96c6\u6587\u6863\u5bf9\u8d1f\u8d23\u4efbAI\u5f00\u53d1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6587\u6863\u5de5\u5177\u7684\u8bbe\u8ba1\u52a8\u673a\u3001\u91c7\u7528\u969c\u788d\u4ee5\u53ca\u5982\u4f55\u4e0e\u73b0\u6709\u7cfb\u7edf\u3001\u6cd5\u89c4\u548c\u6587\u5316\u89c4\u8303\u8fde\u63a5\u7b49\u65b9\u9762\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5206\u6790\uff0c\u5bf959\u7bc7\u6570\u636e\u96c6\u6587\u6863\u51fa\u7248\u7269\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5206\u6790\u6587\u6863\u5de5\u5177\u7684\u8bbe\u8ba1\u52a8\u673a\u3001\u6587\u6863\u5b9e\u8df5\u6982\u5ff5\u5316\u65b9\u5f0f\uff0c\u4ee5\u53ca\u5de5\u5177\u4e0e\u73b0\u6709\u7cfb\u7edf\u7684\u8fde\u63a5\u60c5\u51b5\u3002", "result": "\u53d1\u73b0\u6570\u636e\u96c6\u6587\u6863\u6982\u5ff5\u5316\u5b58\u5728\u56db\u5927\u6301\u7eed\u6a21\u5f0f\uff1a\u6587\u6863\u4ef7\u503c\u64cd\u4f5c\u5316\u4e0d\u660e\u786e\u3001\u8bbe\u8ba1\u8131\u79bb\u4e0a\u4e0b\u6587\u3001\u672a\u89e3\u51b3\u7684\u52b3\u52a8\u9700\u6c42\u3001\u503e\u5411\u4e8e\u5c06\u96c6\u6210\u89c6\u4e3a\u672a\u6765\u5de5\u4f5c\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u963b\u788d\u4e86\u5de5\u5177\u7684\u91c7\u7528\u548c\u6807\u51c6\u5316\u3002", "conclusion": "\u5efa\u8bae\u8d1f\u8d23\u4efbAI\u5de5\u5177\u8bbe\u8ba1\u5e94\u4ece\u4e2a\u4f53\u89e3\u51b3\u65b9\u6848\u8f6c\u5411\u673a\u6784\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u6982\u8ff0\u4e86HCI\u793e\u533a\u53ef\u4ee5\u91c7\u53d6\u7684\u884c\u52a8\uff0c\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u7684\u6587\u6863\u5b9e\u8df5\u3002"}}
{"id": "2602.15983", "categories": ["cs.SE", "cs.AI", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.15983", "abs": "https://arxiv.org/abs/2602.15983", "authors": ["Junbo Jacob Lian", "Yujun Sun", "Huiling Chen", "Chaoyu Zhang", "Chung-Piaw Teo"], "title": "ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization", "comment": "Code and benchmark: \\url{https://github.com/junbolian/ReLoop}", "summary": "Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.", "AI": {"tldr": "ReLoop\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u548c\u884c\u4e3a\u9a8c\u8bc1\u89e3\u51b3LLM\u751f\u6210\u4f18\u5316\u4ee3\u7801\u65f6\u7684\u9759\u9ed8\u5931\u8d25\u95ee\u9898\uff0c\u5c06\u6b63\u786e\u7387\u4ece22.6%\u63d0\u5347\u523031.1%\uff0c\u6267\u884c\u7387\u4ece72.1%\u63d0\u5347\u5230100%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u4f18\u5316\u4ee3\u7801\uff0c\u4f46\u5b58\u5728\u9759\u9ed8\u5931\u8d25\u98ce\u9669\uff1a\u4ee3\u7801\u53ef\u4ee5\u6267\u884c\u5e76\u8fd4\u56de\u6c42\u89e3\u5668\u53ef\u884c\u89e3\uff0c\u4f46\u53ef\u80fd\u7f16\u7801\u4e86\u8bed\u4e49\u9519\u8bef\u7684\u516c\u5f0f\uff0c\u5728\u7ec4\u5408\u95ee\u9898\u4e0a\u53ef\u884c\u6027-\u6b63\u786e\u6027\u5dee\u8ddd\u9ad8\u8fbe90\u4e2a\u767e\u5206\u70b9\u3002", "method": "ReLoop\u4ece\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\u89e3\u51b3\u9759\u9ed8\u5931\u8d25\uff1a1) \u7ed3\u6784\u5316\u751f\u6210\u5c06\u4ee3\u7801\u751f\u6210\u5206\u89e3\u4e3a\u56db\u9636\u6bb5\u63a8\u7406\u94fe\uff08\u7406\u89e3\u3001\u5f62\u5f0f\u5316\u3001\u5408\u6210\u3001\u9a8c\u8bc1\uff09\uff0c\u6a21\u62df\u4e13\u5bb6\u5efa\u6a21\u5b9e\u8df5\uff1b2) \u884c\u4e3a\u9a8c\u8bc1\u901a\u8fc7\u57fa\u4e8e\u6c42\u89e3\u5668\u7684\u53c2\u6570\u6270\u52a8\u6d4b\u8bd5\u68c0\u6d4b\u751f\u6210\u540e\u9519\u8bef\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\u3002", "result": "\u5728\u6700\u5f3a\u6a21\u578b\u4e0a\uff0cReLoop\u5c06\u6b63\u786e\u7387\u4ece22.6%\u63d0\u5347\u523031.1%\uff0c\u6267\u884c\u7387\u4ece72.1%\u63d0\u5347\u5230100%\u3002\u5728\u4e94\u4e2a\u6a21\u578b\uff08\u57fa\u7840\u3001SFT\u3001RL\uff09\u548c\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\u3002\u540c\u65f6\u53d1\u5e03\u4e86RetailOpt-190\u6570\u636e\u96c6\u3002", "conclusion": "\u7ed3\u6784\u5316\u751f\u6210\u5728\u590d\u6742\u7ec4\u5408\u95ee\u9898\u4e0a\u5360\u4e3b\u5bfc\uff0c\u884c\u4e3a\u9a8c\u8bc1\u5728\u5c40\u90e8\u516c\u5f0f\u7f3a\u9677\u95ee\u9898\u4e0a\u8d21\u732e\u6700\u5927\u3002\u4e24\u79cd\u673a\u5236\u4e92\u8865\uff0c\u7ed3\u5408IIS\u589e\u5f3a\u7684\u8bca\u65ad\u6267\u884c\u6062\u590d\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u751f\u6210\u4f18\u5316\u4ee3\u7801\u7684\u53ef\u9760\u6027\u548c\u6b63\u786e\u6027\u3002"}}
{"id": "2602.16069", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.16069", "abs": "https://arxiv.org/abs/2602.16069", "authors": ["Ravi Raju", "Mengmeng Ji", "Shubhangi Upasani", "Bo Li", "Urmish Thakker"], "title": "The Limits of Long-Context Reasoning in Automated Bug Fixing", "comment": "4 pages, under review", "summary": "Rapidly increasing context lengths have led to the assumption that large language models (LLMs) can directly reason over entire codebases. Concurrently, recent advances in LLMs have enabled strong performance on software engineering benchmarks, particularly when paired with agentic workflows. In this work, we systematically evaluate whether current LLMs can reliably perform long-context code debugging and patch generation. Using SWE-bench Verified as a controlled experimental setting, we first evaluate state-of-the-art models within an agentic harness (mini-SWE-agent), where performance improves substantially: GPT-5-nano achieves up to a 31\\% resolve rate on 100 samples, and open-source models such as Deepseek-R1-0528 obtain competitive results. However, token-level analysis shows that successful agentic trajectories typically remain under 20k tokens, and that longer accumulated contexts correlate with lower success rates, indicating that agentic success primarily arises from task decomposition into short-context steps rather than effective long-context reasoning. To directly test long-context capability, we construct a data pipeline where we artificially inflate the context length of the input by placing the relevant files into the context (ensuring perfect retrieval recall); we then study single-shot patch generation under genuinely long contexts (64k-128k tokens). Despite this setup, performance degrades sharply: Qwen3-Coder-30B-A3B achieves only a 7\\% resolve rate at 64k context, while GPT-5-nano solves none of the tasks. Qualitative analysis reveals systematic failure modes, including hallucinated diffs, incorrect file targets, and malformed patch headers. Overall, our findings highlight a significant gap between nominal context length and usable context capacity in current LLMs, and suggest that existing agentic coding benchmarks do not meaningfully evaluate long-context reasoning.", "AI": {"tldr": "\u5f53\u524dLLMs\u5728\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u8c03\u8bd5\u548c\u8865\u4e01\u751f\u6210\u65b9\u9762\u5b58\u5728\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff0c\u5c3d\u7ba1\u4ee3\u7406\u5de5\u4f5c\u6d41\u5728\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u771f\u6b63\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u5feb\u901f\u589e\u52a0\uff0c\u4eba\u4eec\u666e\u904d\u5047\u8bbe\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u5bf9\u6574\u4e2a\u4ee3\u7801\u5e93\u8fdb\u884c\u63a8\u7406\u3002\u540c\u65f6\uff0cLLMs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u5f53\u524dLLMs\u662f\u5426\u80fd\u591f\u53ef\u9760\u5730\u6267\u884c\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u8c03\u8bd5\u548c\u8865\u4e01\u751f\u6210\u3002", "method": "\u4f7f\u7528SWE-bench Verified\u4f5c\u4e3a\u53d7\u63a7\u5b9e\u9a8c\u8bbe\u7f6e\uff1a1\uff09\u5728\u4ee3\u7406\u6846\u67b6\uff08mini-SWE-agent\uff09\u4e2d\u8bc4\u4f30\u6700\u5148\u8fdb\u6a21\u578b\uff1b2\uff09\u6784\u5efa\u6570\u636e\u7ba1\u9053\uff0c\u901a\u8fc7\u5c06\u76f8\u5173\u6587\u4ef6\u653e\u5165\u4e0a\u4e0b\u6587\u6765\u4eba\u4e3a\u589e\u52a0\u8f93\u5165\u957f\u5ea6\uff0c\u7814\u7a76\u771f\u6b63\u957f\u4e0a\u4e0b\u6587\uff0864k-128k tokens\uff09\u4e0b\u7684\u5355\u6b21\u8865\u4e01\u751f\u6210\u3002", "result": "\u4ee3\u7406\u5de5\u4f5c\u6d41\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08GPT-5-nano\u5728100\u4e2a\u6837\u672c\u4e0a\u8fbe\u523031%\u89e3\u51b3\u7387\uff09\uff0c\u4f46\u6210\u529f\u8f68\u8ff9\u901a\u5e38\u4fdd\u6301\u572820k tokens\u4ee5\u4e0b\uff0c\u4e14\u66f4\u957f\u7684\u7d2f\u79ef\u4e0a\u4e0b\u6587\u4e0e\u8f83\u4f4e\u6210\u529f\u7387\u76f8\u5173\u3002\u5728\u771f\u6b63\u957f\u4e0a\u4e0b\u6587\u8bbe\u7f6e\u4e2d\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff1aQwen3-Coder-30B-A3B\u572864k\u4e0a\u4e0b\u6587\u4e2d\u4ec5\u8fbe\u52307%\u89e3\u51b3\u7387\uff0cGPT-5-nano\u5219\u65e0\u6cd5\u89e3\u51b3\u4efb\u4f55\u4efb\u52a1\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u7cfb\u7edf\u6027\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u5f53\u524dLLMs\u7684\u540d\u4e49\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0e\u5b9e\u9645\u53ef\u7528\u4e0a\u4e0b\u6587\u5bb9\u91cf\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u73b0\u6709\u7684\u4ee3\u7406\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u5e76\u672a\u771f\u6b63\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002\u6210\u529f\u4e3b\u8981\u6765\u81ea\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u77ed\u4e0a\u4e0b\u6587\u6b65\u9aa4\uff0c\u800c\u975e\u6709\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u3002"}}
{"id": "2602.16091", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16091", "abs": "https://arxiv.org/abs/2602.16091", "authors": ["Amirali Rayegan", "Tim Menzies"], "title": "Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?", "comment": "Submitted to MSR'26 in Registered Report track", "summary": "Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5728\u7b26\u53f7\u6a21\u578b\uff08\u51b3\u7b56\u6811\uff09\u4e2d\u5f15\u5165\u56e0\u679c\u611f\u77e5\u5206\u88c2\u51c6\u5219\u662f\u5426\u80fd\u63d0\u5347\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u8bc4\u4f30\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4eba\u7c7b\u4e13\u5bb6\u4e0e\u81ea\u52a8\u5316\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u7b26\u53f7\u6a21\u578b\uff08\u5982\u51b3\u7b56\u6811\uff09\u4e3b\u8981\u57fa\u4e8e\u76f8\u5173\u6027\u5206\u88c2\u51c6\u5219\uff08\u65b9\u5dee\u51cf\u5c11\u3001\u4fe1\u606f\u589e\u76ca\u7b49\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u6df7\u6dc6\u4e86\u5173\u8054\u4e0e\u56e0\u679c\u5173\u7cfb\u3002\u540c\u65f6\uff0c\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u56e0NP\u96be\u95ee\u9898\u800c\u4f9d\u8d56\u542f\u53d1\u5f0f\u8fd1\u4f3c\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3002\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u4e86SE\u4efb\u52a1\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3001\u53ef\u590d\u73b0\u6027\u548c\u89e3\u91ca\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528MOOT\u4ed3\u5e93\u4e2d\u7684120\u591a\u4e2a\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u9884\u6ce8\u518c\u7684\u81ea\u4e3e\u96c6\u6210\u534f\u8bae\u8bc4\u4f30\u7a33\u5b9a\u6027\uff0c\u91c7\u7528\u8d62\u5206\u5206\u914d\u65b9\u6cd5\u6d4b\u91cf\u65b9\u5dee\u3002\u6bd4\u8f83\u4eba\u7c7b\u56e0\u679c\u8bc4\u4f30\u3001\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u51b3\u7b56\u6811\uff08EZR\uff09\u4ee5\u53ca\u56e0\u679c\u611f\u77e5\u6811\uff08\u5229\u7528\u6761\u4ef6\u71b5\u5206\u88c2\u51c6\u5219\u548c\u6df7\u6742\u56e0\u7d20\u8fc7\u6ee4\uff09\u3002\u4f7f\u7528\u7edf\u8ba1\u65b9\u6cd5\uff08\u65b9\u5dee\u3001\u57fa\u5c3c\u4e0d\u7eaf\u5ea6\u3001KS\u68c0\u9a8c\u3001Cliff's delta\uff09\u5206\u6790\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u8bba\u6587\u672a\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\u6570\u636e\uff0c\u4f46\u7814\u7a76\u6846\u67b6\u5df2\u5efa\u7acb\uff1a\u901a\u8fc7\u7cfb\u7edf\u6bd4\u8f83\u4eba\u7c7b\u4e13\u5bb6\u3001\u4f20\u7edf\u76f8\u5173\u6027\u51b3\u7b56\u6811\u548c\u56e0\u679c\u611f\u77e5\u6811\u5728\u7a33\u5b9a\u6027\u3001\u9884\u6d4b\u6027\u80fd\u548c\u4f18\u5316\u6027\u80fd\u65b9\u9762\u7684\u8868\u73b0\uff0c\u65e8\u5728\u9a8c\u8bc1\u56e0\u679c\u611f\u77e5\u65b9\u6cd5\u662f\u5426\u80fd\u63d0\u5347\u7a33\u5b9a\u6027\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u65e8\u5728\u8bc1\u660e\u5728\u7b26\u53f7\u6a21\u578b\u4e2d\u5f15\u5165\u56e0\u679c\u611f\u77e5\u5206\u88c2\u51c6\u5219\u53ef\u4ee5\u6539\u5584\u6a21\u578b\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u9884\u6d4b\u548c\u4f18\u5316\u6027\u80fd\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u53ef\u89e3\u91ca\u5206\u6790\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2602.16551", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.16551", "abs": "https://arxiv.org/abs/2602.16551", "authors": ["Rui Hu", "Yue Wu", "Tianhao Su", "Yin Wang", "Shunbo Hu", "Jizhong Huang"], "title": "Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation", "comment": null, "summary": "The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and \"Digital Twin\" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a \"Data Silo\" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient \"Gatekeeper\" agent for relevance filtering and a high-capability \"Analyst\" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\\%, establishing a highly efficient \"Human-in-the-loop\" workflow that reduces manual data curation time by approximately 90\\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the \"Digital Material Twin\" of built heritage.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff0c\u4ecePDF\u6587\u732e\u4e2d\u81ea\u52a8\u63d0\u53d6\u529b\u5b66\u672c\u6784\u65b9\u7a0b\u3001\u53c2\u6570\u548c\u5143\u6570\u636e\uff0c\u89e3\u51b3\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u4e2d\u6570\u636e\u788e\u7247\u5316\u95ee\u9898", "motivation": "\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u6b63\u5411\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\"\u6570\u5b57\u5b6a\u751f\"\u6784\u5efa\u53d1\u5c55\uff0c\u4f46\u6240\u9700\u7684\u529b\u5b66\u672c\u6784\u6a21\u578b\u5206\u6563\u5728\u6570\u5341\u5e74\u7684\u975e\u7ed3\u6784\u5316\u79d1\u5b66\u6587\u732e\u4e2d\uff0c\u5f62\u6210\"\u6570\u636e\u5b64\u5c9b\"\uff0c\u963b\u788d\u4e86\u4fdd\u62a4\u5de5\u7a0b\u7684\u53d1\u5c55", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u667a\u80fd\u4ee3\u7406\u6846\u67b6\uff1a1) \u8d44\u6e90\u9ad8\u6548\u7684\"\u5b88\u95e8\u5458\"\u4ee3\u7406\u8fdb\u884c\u76f8\u5173\u6027\u7b5b\u9009\uff1b2) \u9ad8\u80fd\u529b\"\u5206\u6790\u5e08\"\u4ee3\u7406\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63d0\u53d6\uff0c\u5305\u542b\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7b26\u53f7\u63a5\u5730\u673a\u5236\u89e3\u51b3\u6570\u5b66\u6b67\u4e49", "result": "\u5e94\u7528\u4e8e2000\u591a\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u6210\u529f\u7b5b\u9009\u51fa113\u7bc7\u6838\u5fc3\u6587\u6863\uff0c\u6784\u5efa\u4e86\u5305\u542b185\u4e2a\u672c\u6784\u6a21\u578b\u5b9e\u4f8b\u548c450\u591a\u4e2a\u6821\u51c6\u53c2\u6570\u7684\u7ed3\u6784\u5316\u6570\u636e\u5e93\uff0c\u63d0\u53d6\u7cbe\u5ea6\u8fbe80.4%\uff0c\u51cf\u5c11\u4eba\u5de5\u6570\u636e\u6574\u7406\u65f6\u95f4\u7ea690%", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u5206\u6563\u7684\u6587\u732e\u8f6c\u5316\u4e3a\u53ef\u67e5\u8be2\u7684\u6570\u5b57\u8d44\u4ea7\uff0c\u4e3a\u5efa\u7b51\u9057\u4ea7\u7684\"\u6570\u5b57\u6750\u6599\u5b6a\u751f\"\u5960\u5b9a\u6570\u636e\u57fa\u7840\uff0c\u901a\u8fc7\u57fa\u4e8e\u7f51\u7edc\u7684\u77e5\u8bc6\u68c0\u7d22\u5e73\u53f0\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u5b9e\u7528\u6027"}}
{"id": "2602.15995", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.15995", "abs": "https://arxiv.org/abs/2602.15995", "authors": ["Xiang Fu", "Shiman Meng", "Weiping Zhang", "Luanzheng Guo", "Kento Sato", "Dong H. Ahn", "Ignacio Laguna", "Gregory L. Lee", "Martin Schulz"], "title": "Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs", "comment": "IEEE Cluster 2024", "summary": "After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \\toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u6280\u672f\uff08\u5206\u5e03\u5f0f\u65f6\u949fDC\u548c\u5206\u5e03\u5f0f\u7eaa\u5143DE\uff09\u6765\u9ad8\u6548\u8bb0\u5f55\u548c\u91cd\u653eOpenMP\u7a0b\u5e8f\uff0c\u89e3\u51b3\u4e86OpenMP\u7a0b\u5e8f\u53ef\u6269\u5c55\u91cd\u653e\u7684\u96be\u9898\uff0c\u6027\u80fd\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u53472-5\u500d\u3002", "motivation": "OpenMP\u867d\u7136\u662f\u6700\u6d41\u884c\u7684\u5171\u4eab\u5185\u5b58\u7f16\u7a0b\u6846\u67b6\uff0c\u4f46\u5176\u975e\u786e\u5b9a\u6027\u6267\u884c\u7279\u6027\u4f7f\u5f97\u8c03\u8bd5\u548c\u6d4b\u8bd5\u53d8\u5f97\u56f0\u96be\u3002\u8bb0\u5f55\u548c\u786e\u5b9a\u6027\u91cd\u653e\u7a0b\u5e8f\u6267\u884c\u662f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u7684\u5173\u952e\uff0c\u4f46\u76ee\u524d\u53ef\u6269\u5c55\u5730\u91cd\u653eOpenMP\u7a0b\u5e8f\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u9896\u6280\u672f\uff1a\u5206\u5e03\u5f0f\u65f6\u949f\uff08DC\uff09\u548c\u5206\u5e03\u5f0f\u7eaa\u5143\uff08DE\uff09\u8bb0\u5f55\u65b9\u6848\uff0c\u7528\u4e8e\u6d88\u9664OpenMP\u8bb0\u5f55\u548c\u91cd\u653e\u4e2d\u8fc7\u591a\u7684\u7ebf\u7a0b\u540c\u6b65\u3002\u901a\u8fc7ReOMP\u5de5\u5177\u5b9e\u73b0DC\u548cDE\u8bb0\u5f55\uff0c\u5e76\u4e0eMPI\u7ea7\u91cd\u653e\u5de5\u5177ReMPI\u96c6\u6210\uff0c\u652f\u6301MPI+OpenMP\u5e94\u7528\u7a0b\u5e8f\u7684\u91cd\u653e\u3002", "result": "\u5728\u4ee3\u8868\u6027HPC\u5e94\u7528\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5728\u6bcf\u4e2a\u5171\u4eab\u5185\u5b58\u8bbf\u95ee\u65f6\u540c\u6b65\u7684\u65b9\u6cd5\u6548\u7387\u9ad82-5\u500d\u3002\u4e0eReMPI\u96c6\u6210\u540e\uff0c\u4ec5\u4ea7\u751f\u5c11\u91cf\u4e0eMPI\u89c4\u6a21\u65e0\u5173\u7684\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u80fd\u591f\u91cd\u653e\u590d\u6742\u7684MPI+OpenMP\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684DC\u548cDE\u8bb0\u5f55\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86OpenMP\u7a0b\u5e8f\u53ef\u6269\u5c55\u91cd\u653e\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bb0\u5f55\u548c\u91cd\u653e\u7684\u6548\u7387\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709MPI\u91cd\u653e\u5de5\u5177\u65e0\u7f1d\u96c6\u6210\uff0c\u4e3a\u8c03\u8bd5\u548c\u6d4b\u8bd5OpenMP\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.16106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16106", "abs": "https://arxiv.org/abs/2602.16106", "authors": ["Shahriar Rumi Dipto", "Saikat Mondal", "Chanchal K. Roy"], "title": "Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs", "comment": "Accepted at 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026)", "summary": "Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.", "AI": {"tldr": "\u7b97\u6cd5\u9a71\u52a8\u7684\u4ee3\u7801\u7ffb\u8bd1\u7ba1\u9053\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u4e2d\u7acb\u7684\u4e2d\u95f4\u89c4\u8303\uff0c\u5c06\u7ffb\u8bd1\u51c6\u786e\u7387\u4ece67.7%\u63d0\u5347\u81f378.5%\uff0c\u663e\u8457\u51cf\u5c11\u5404\u7c7b\u7f16\u8bd1\u548c\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "motivation": "\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4ee3\u7801\u7ffb\u8bd1\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u7a0b\u5e8f\u610f\u56fe\uff0c\u5bfc\u81f4\u63a7\u5236\u6d41\u3001\u7c7b\u578b\u5904\u7406\u548cI/O\u884c\u4e3a\u9519\u8bef\u3002\u9700\u8981\u66f4\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u548c\u610f\u56fe\u4fdd\u6301\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7b97\u6cd5\u7684\u7ffb\u8bd1\u7ba1\u9053\uff0c\u5f15\u5165\u8bed\u8a00\u4e2d\u7acb\u7684\u4e2d\u95f4\u89c4\u8303\u6765\u6355\u83b7\u7a0b\u5e8f\u7ec6\u8282\u540e\u518d\u751f\u6210\u4ee3\u7801\u3002\u5728Avatar\u548cCodeNet\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u4e94\u79cd\u4e3b\u6d41LLM\u8fdb\u884cPython\u548cJava\u4e4b\u95f4\u7684\u7ffb\u8bd1\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u7f16\u8bd1\u7ed3\u679c\u3001\u8fd0\u884c\u65f6\u884c\u4e3a\u548c\u6d4b\u8bd5\u901a\u8fc7\u7387\u3002", "result": "\u7b97\u6cd5\u65b9\u6cd5\u5c06\u5e73\u5747\u51c6\u786e\u7387\u4ece67.7%\u63d0\u5347\u81f378.5%\uff08\u589e\u52a010.8%\uff09\uff0c\u5b8c\u5168\u6d88\u9664\u8bcd\u6cd5\u548c\u6807\u8bb0\u9519\u8bef\uff0c\u51cf\u5c1172.7%\u7684\u4e0d\u5b8c\u6574\u6784\u9020\uff0c\u964d\u4f4e61.1%\u7684\u7ed3\u6784\u548c\u58f0\u660e\u95ee\u9898\uff0c\u51cf\u5c1178.4%\u7684\u8fd0\u884c\u65f6\u4f9d\u8d56\u548c\u5165\u53e3\u70b9\u5931\u8d25\u3002", "conclusion": "\u57fa\u4e8e\u7b97\u6cd5\u7684\u7ba1\u9053\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u3001\u610f\u56fe\u4fdd\u6301\u7684\u4ee3\u7801\u7ffb\u8bd1\uff0c\u4e3a\u5065\u58ee\u7684\u591a\u8bed\u8a00\u7f16\u7a0b\u52a9\u624b\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.16585", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16585", "abs": "https://arxiv.org/abs/2602.16585", "authors": ["Dimitri Yatsenko", "Thinh T. Nguyen"], "title": "DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows", "comment": "20 pages, 2 figures, 1 table", "summary": "Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.", "AI": {"tldr": "DataJoint 2.0\u63d0\u51fa\u5173\u7cfb\u578b\u5de5\u4f5c\u6d41\u6a21\u578b\uff0c\u901a\u8fc7\u56db\u4e2a\u6280\u672f\u521b\u65b0\u5b9e\u73b0\u79d1\u5b66\u6570\u636e\u7ba1\u9053\u7684SciOps\uff08\u79d1\u5b66\u8fd0\u7ef4\uff09\uff0c\u7edf\u4e00\u6570\u636e\u3001\u7ed3\u6784\u548c\u8ba1\u7b97\u8f6c\u6362\u3002", "motivation": "\u79d1\u5b66\u6570\u636e\u7ba1\u9053\u9700\u8981\u7c7b\u4f3cDevOps\u7684SciOps\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5c06\u6570\u636e\u6eaf\u6e90\u5206\u6563\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\uff0c\u7f3a\u4e4f\u4e8b\u52a1\u4fdd\u8bc1\uff0c\u5bfc\u81f4\u4eba\u673a\u534f\u4f5c\u5bb9\u6613\u5931\u8d25\u3002", "method": "\u91c7\u7528\u5173\u7cfb\u578b\u5de5\u4f5c\u6d41\u6a21\u578b\uff1a\u7528\u8868\u8868\u793a\u5de5\u4f5c\u6d41\u6b65\u9aa4\uff0c\u884c\u8868\u793a\u6570\u636e\u4ea7\u7269\uff0c\u5916\u952e\u89c4\u5b9a\u6267\u884c\u987a\u5e8f\u3002\u901a\u8fc7\u56db\u4e2a\u6280\u672f\u521b\u65b0\u6269\u5c55\uff1a\u5bf9\u8c61\u589e\u5f3a\u6a21\u5f0f\u3001\u8bed\u4e49\u5339\u914d\u3001\u53ef\u6269\u5c55\u7c7b\u578b\u7cfb\u7edf\u548c\u5206\u5e03\u5f0f\u4f5c\u4e1a\u534f\u8c03\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6570\u636e\u7ed3\u6784\u3001\u6570\u636e\u548c\u8ba1\u7b97\u8f6c\u6362\u7684\u5e73\u53f0\uff0c\u4f7f\u4ee3\u7406\u80fd\u591f\u53c2\u4e0e\u79d1\u5b66\u5de5\u4f5c\u6d41\u800c\u4e0d\u4f1a\u5bfc\u81f4\u6570\u636e\u635f\u574f\uff0c\u5b9e\u73b0\u4e86SciOps\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "DataJoint 2.0\u901a\u8fc7\u5173\u7cfb\u578b\u5de5\u4f5c\u6d41\u6a21\u578b\u548c\u56db\u4e2a\u6280\u672f\u521b\u65b0\uff0c\u4e3a\u79d1\u5b66\u6570\u636e\u7ba1\u9053\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684SciOps\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6eaf\u6e90\u5206\u6563\u548c\u7f3a\u4e4f\u4e8b\u52a1\u4fdd\u8bc1\u7684\u95ee\u9898\u3002"}}
{"id": "2602.16010", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16010", "abs": "https://arxiv.org/abs/2602.16010", "authors": ["Xin Huang", "Weiping Zhang", "Shiman Meng", "Wubiao Xu", "Xiang Fu", "Luanzheng Guo", "Kento Sato"], "title": "Scrutinizing Variables for Checkpoint Using Automatic Differentiation", "comment": "The Second Workshop on Enabling Predictive Science with Optimization and Uncertainty Quantification in HPC (EPSOUQ-HPC) in conjunction with SC24", "summary": "Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.", "AI": {"tldr": "\u5229\u7528\u81ea\u52a8\u5fae\u5206\u6280\u672f\u5206\u6790HPC\u5e94\u7528\u4e2d\u53d8\u91cf\u5143\u7d20\u7684\"\u5173\u952e\u6027\"\uff0c\u4ec5\u5bf9\u5f71\u54cd\u7a0b\u5e8f\u8f93\u51fa\u7684\u5173\u952e\u5143\u7d20\u8fdb\u884ccheckpoint\uff0c\u53ef\u8282\u7701\u9ad8\u8fbe20%\u7684\u5b58\u50a8\u7a7a\u95f4", "motivation": "\u4f20\u7edfcheckpoint/restart\u673a\u5236\u4fdd\u5b58\u6574\u4e2a\u7a0b\u5e8f\u72b6\u6001\uff0c\u6d88\u8017\u5927\u91cf\u7cfb\u7edf\u8d44\u6e90\u3002\u4f46HPC\u5e94\u7528\u4e2d\u5e76\u975e\u6240\u6709\u6570\u636e\u90fd\u53c2\u4e0e\u8ba1\u7b97\uff0c\u672a\u4f7f\u7528\u7684\u6570\u636e\u5e94\u4ececheckpoint\u4e2d\u6392\u9664\u4ee5\u63d0\u9ad8\u5b58\u50a8\u548c\u8ba1\u7b97\u6548\u7387", "method": "\u63d0\u51fa\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u81ea\u52a8\u5fae\u5206\u5de5\u5177\u68c0\u67e5\u53d8\u91cf\uff08\u5982\u6570\u7ec4\uff09\u4e2d\u6bcf\u4e2a\u5143\u7d20\u5bf9checkpoint\u7684\u5fc5\u8981\u6027\u3002\u901a\u8fc7AD\u5206\u6790\u6bcf\u4e2a\u5143\u7d20\u662f\u5426\u5f71\u54cd\u7a0b\u5e8f\u8f93\u51fa\uff0c\u8bc6\u522b\u5173\u952e/\u975e\u5173\u952e\u5143\u7d20\uff0c\u4ec5\u5bf9\u5173\u952e\u5143\u7d20\u8fdb\u884ccheckpoint", "result": "\u5728NAS Parallel Benchmark\u76848\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u53ef\u89c6\u5316\u53d8\u91cf\u5185\u5173\u952e/\u975e\u5173\u952e\u5143\u7d20\u533a\u57df\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u5f0f\u5206\u5e03\u4e0e\u7b97\u6cd5\u7684\u7269\u7406\u516c\u5f0f/\u903b\u8f91\u76f8\u7b26\u3002\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u53ef\u8282\u7701\u9ad8\u8fbe20%\u7684checkpoint\u5b58\u50a8\u7a7a\u95f4", "conclusion": "\u57fa\u4e8e\u81ea\u52a8\u5fae\u5206\u7684\u5143\u7d20\u7ea7\u5173\u952e\u6027\u5206\u6790\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522bHPC\u5e94\u7528\u4e2d\u53ef\u6392\u9664checkpoint\u7684\u6570\u636e\uff0c\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u7a0b\u5e8f\u6b63\u786e\u6027"}}
{"id": "2602.16499", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.16499", "abs": "https://arxiv.org/abs/2602.16499", "authors": ["Carsten Ellwein", "David Dietrich", "Jessica Roth", "Rozana Cvitkovic", "Andreas Wortmann"], "title": "Software-heavy Asset Administration Shells: Classification and Use Cases", "comment": null, "summary": "The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5c06\u8f6f\u4ef6\u670d\u52a1\u76f4\u63a5\u96c6\u6210\u5230\u8d44\u4ea7\u7ba1\u7406\u58f3(AAS)\u4e2d\u7684\u8f6f\u4ef6\u67b6\u6784\uff0c\u57fa\u4e8e\u8f6f\u4ef6\u8d28\u91cf\u6807\u51c6\u548c\u5178\u578b\u5236\u9020\u7528\u4f8b\u8fdb\u884c\u5206\u7c7b\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u5b6a\u751f\u548c\u4eba\u5de5\u667a\u80fd\u5728\u5236\u9020\u4e1a\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u5c06\u8f6f\u4ef6\u670d\u52a1\u76f4\u63a5\u96c6\u6210\u5230AAS\u4e2d\u3002\u73b0\u6709\u6587\u732e\u53ea\u6709\u96f6\u6563\u89e3\u51b3\u65b9\u6848\uff0c\u7f3a\u4e4f\u5bf9\u76f8\u5173\u8f6f\u4ef6\u67b6\u6784\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u8f6f\u4ef6\u8d28\u91cf\u6807\u51c6\u548c\u5178\u578b\u5236\u9020\u7528\u4f8b\uff0c\u5bf9\u5c06\u8f6f\u4ef6\u670d\u52a1\u76f4\u63a5\u96c6\u6210\u5230AAS\u7684\u8f6f\u4ef6\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u548c\u5206\u7c7b\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u8f6f\u4ef6\u5bc6\u96c6\u578bAAS\u7684\u67b6\u6784\u5206\u7c7b\u6846\u67b6\uff0c\u4e3a\u4e0d\u540c\u5236\u9020\u573a\u666f\u4e0b\u7684\u8f6f\u4ef6\u670d\u52a1\u96c6\u6210\u63d0\u4f9b\u7cfb\u7edf\u5316\u7684\u67b6\u6784\u9009\u62e9\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u8f6f\u4ef6\u5bc6\u96c6\u578bAAS\u67b6\u6784\u7cfb\u7edf\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u67b6\u6784\u9009\u62e9\u6307\u5bfc\u6846\u67b6\u3002"}}
{"id": "2602.16100", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16100", "abs": "https://arxiv.org/abs/2602.16100", "authors": ["Zijie Su", "Muhammed Tawfiqul Islam", "Mohammad Goudarzi", "Adel N. Toosi"], "title": "LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum", "comment": null, "summary": "With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u914d\u7f6e\u65b9\u6cd5\uff0c\u5728\u5f02\u6784GPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0LLM\u63a8\u7406\u670d\u52a1\u7684\u5728\u7ebf\u914d\u7f6e\u8c03\u6574\uff0c\u6700\u5c0f\u5316\u670d\u52a1\u4e2d\u65ad\u65f6\u95f4", "motivation": "\u968f\u7740LLM\u5feb\u901f\u53d1\u5c55\uff0c\u5728\u6709\u9650GPU\u8d44\u6e90\u4e0b\u9ad8\u6548\u670d\u52a1LLM\u63a8\u7406\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u63a2\u7d22\u5c06\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u8303\u5f0f\u5e94\u7528\u4e8eLLM\u670d\u52a1\u4ee5\u6700\u5927\u5316\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46LLM\u63a8\u7406\u5de5\u4f5c\u8d1f\u8f7d\u9ad8\u5ea6\u591a\u6837\u5316\uff0c\u73b0\u4ee3GPU\u96c6\u7fa4\u672c\u8d28\u4e0a\u662f\u5f02\u6784\u7684\uff0c\u9700\u8981\u52a8\u6001\u8c03\u6574\u90e8\u7f72\u914d\u7f6e\u4ee5\u9002\u5e94\u65e0\u670d\u52a1\u5668\u73af\u5883\u7684\u5f39\u6027\u548c\u52a8\u6001\u7279\u6027\u3002\u540c\u65f6\uff0c\u7531\u4e8eLLM\u63a8\u7406\u7684\u6709\u72b6\u6001\u7279\u6027\u548c\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u5de8\u5927\uff0c\u5b9e\u73b0\u5728\u7ebf\u91cd\u914d\u7f6e\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u914d\u7f6e\u65b9\u6cd5\uff0c\u652f\u6301\u5728\u7ebf\u8c03\u6574\u6d41\u6c34\u7ebf\u914d\u7f6e\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u670d\u52a1\u505c\u673a\u65f6\u95f4\u548c\u6027\u80fd\u4e0b\u964d\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u7cfb\u7edf\u6839\u636e\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u6700\u4f18\u7684\u6d41\u6c34\u7ebf\u914d\u7f6e\u3002", "result": "\u5728\u5305\u62ecNVIDIA A100\u548cL40s\u7684\u5f02\u6784GPU\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8fc1\u79fb\u673a\u5236\u5bfc\u81f4\u7684\u670d\u52a1\u505c\u673a\u65f6\u95f4\u5c11\u4e8e50\u6beb\u79d2\uff0c\u540c\u65f6\u5bf9\u9996\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09\u548c\u6bcf\u8f93\u51fa\u4ee4\u724c\u65f6\u95f4\uff08TPOT\uff09\u5f15\u5165\u7684\u989d\u5916\u5f00\u9500\u5747\u4f4e\u4e8e10%\u3002", "conclusion": "\u63d0\u51fa\u7684\u52a8\u6001\u6d41\u6c34\u7ebf\u91cd\u914d\u7f6e\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9LLM\u63a8\u7406\u670d\u52a1\u5728\u5f02\u6784\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5728\u7ebf\u914d\u7f6e\u8c03\u6574\uff0c\u4e3a\u5f39\u6027LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.16671", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16671", "abs": "https://arxiv.org/abs/2602.16671", "authors": ["Jaid Monwar Chowdhury", "Chi-An Fu", "Reyhaneh Jabbarvand"], "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation", "comment": "9 pages, 6 figures, 4 tables", "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.", "AI": {"tldr": "SPARC\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u3001\u57fa\u4e8e\u573a\u666f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u63a7\u5236\u6d41\u56fe\u5206\u6790\u3001\u64cd\u4f5c\u6620\u5c04\u3001\u8def\u5f84\u5b9a\u5411\u6d4b\u8bd5\u5408\u6210\u548c\u8fed\u4ee3\u81ea\u6821\u6b63\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347C\u8bed\u8a00\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u7684\u8d28\u91cf\u548c\u8986\u76d6\u7387\u3002", "motivation": "C\u8bed\u8a00\u7684\u81ea\u52a8\u5316\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u9ad8\u7ea7\u7a0b\u5e8f\u610f\u56fe\u4e0e\u6307\u9488\u8fd0\u7b97\u548c\u624b\u52a8\u5185\u5b58\u7ba1\u7406\u7684\u4e25\u683c\u8bed\u6cd5\u7ea6\u675f\u4e4b\u95f4\u5b58\u5728\u8bed\u4e49\u9e3f\u6c9f\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u751f\u6210\u80fd\u529b\uff0c\u4f46\u76f4\u63a5\u610f\u56fe\u5230\u4ee3\u7801\u7684\u5408\u6210\u7ecf\u5e38\u51fa\u73b0\"\u8df3\u8dc3\u5230\u4ee3\u7801\"\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u65e9\u751f\u6210\u4ee3\u7801\u800c\u7f3a\u4e4f\u5bf9\u7a0b\u5e8f\u7ed3\u6784\u3001\u7ea6\u675f\u548c\u8bed\u4e49\u7684\u57fa\u7840\u7406\u89e3\u3002", "method": "SPARC\u91c7\u7528\u56db\u9636\u6bb5\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff1a1) \u63a7\u5236\u6d41\u56fe\u5206\u6790\uff1b2) \u64cd\u4f5c\u6620\u5c04\uff0c\u5c06LLM\u63a8\u7406\u57fa\u4e8e\u5df2\u9a8c\u8bc1\u7684\u5b9e\u7528\u8f85\u52a9\u51fd\u6570\uff1b3) \u8def\u5f84\u5b9a\u5411\u6d4b\u8bd5\u5408\u6210\uff1b4) \u4f7f\u7528\u7f16\u8bd1\u5668\u548c\u8fd0\u884c\u65f6\u53cd\u9988\u7684\u8fed\u4ee3\u81ea\u6821\u6b63\u9a8c\u8bc1\u5faa\u73af\u3002", "result": "\u572859\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u7b97\u6cd5\u4e3b\u9898\u4e0a\u8bc4\u4f30\uff0cSPARC\u76f8\u6bd4\u57fa\u7ebf\u63d0\u793a\u751f\u6210\u5728\u7ebf\u8986\u76d6\u7387\u4e0a\u63d0\u534731.36%\uff0c\u5206\u652f\u8986\u76d6\u7387\u63d0\u534726.01%\uff0c\u53d8\u5f02\u5206\u6570\u63d0\u534720.78%\uff0c\u5728\u590d\u6742\u4e3b\u9898\u4e0a\u5339\u914d\u6216\u8d85\u8fc7\u7b26\u53f7\u6267\u884c\u5de5\u5177KLEE\u3002\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u4fdd\u7559\u4e8694.3%\u7684\u6d4b\u8bd5\uff0c\u751f\u6210\u7684\u4ee3\u7801\u5728\u5f00\u53d1\u8005\u8bc4\u5b9a\u7684\u53ef\u8bfb\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u65b9\u9762\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u63a8\u7406\u4e0e\u7a0b\u5e8f\u7ed3\u6784\u5bf9\u9f50\uff0cSPARC\u4e3a\u5de5\u4e1a\u7ea7\u9057\u7559C\u4ee3\u7801\u5e93\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u751f\u6210\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2602.16222", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2602.16222", "abs": "https://arxiv.org/abs/2602.16222", "authors": ["Joel Rybicki", "Jakob Solnerzik", "Robin Vacus"], "title": "Near-optimal population protocols on bounded-degree trees", "comment": "37 pages, 7 figures", "summary": "We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.\n  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.\n  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \\log n)$ steps on directed trees.", "AI": {"tldr": "\u5728\u7a00\u758f\u4ea4\u4e92\u56fe\uff08\u6709\u754c\u5ea6\u6811\uff09\u4e0a\uff0c\u7fa4\u4f53\u534f\u8bae\u5bf9\u4e8e\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u4e0d\u5b58\u5728\u663e\u8457\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\uff0c\u8fd9\u4e0e\u5b8c\u5168\u56fe\u4e0d\u540c\u3002\u4f5c\u8005\u63d0\u51fa\u4e86\u5e38\u6570\u7a7a\u95f4\u534f\u8bae\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6700\u574f\u60c5\u51b5\u7a33\u5b9a\u65f6\u95f4\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5df2\u77e5\u5728\u5b8c\u5168\u4ea4\u4e92\u56fe\u4e2d\uff0c\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u5b58\u5728\u6700\u4f18\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\u3002\u4f46\u73b0\u6709\u4e0b\u754c\u6280\u672f\u65e0\u6cd5\u6269\u5c55\u5230\u9ad8\u5bc6\u5ea6\u56fe\u4e4b\u5916\uff0c\u56e0\u6b64\u4e0d\u6e05\u695a\u5176\u4ed6\u56fe\u65cf\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u7684\u7a7a\u95f4-\u65f6\u95f4\u590d\u6742\u5ea6\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u534f\u8bae\uff1a1\uff09\u9002\u7528\u4e8e\u4e00\u822c\u4ea4\u4e92\u56fe\u7684\u5feb\u901f\u81ea\u7a33\u5b9a2\u8df3\u7740\u8272\u534f\u8bae\uff0c\u4f7f\u7528\u968f\u673a\u6f02\u79fb\u8bba\u8bc1\u6765\u754c\u5b9a\u7a33\u5b9a\u65f6\u95f4\uff1b2\uff09\u5728\u4efb\u4f55\u6811\u4e0a\u4ee5\u6700\u4f18\u65f6\u95f4\u6784\u5efa\u6709\u6839\u6811\u7684\u81ea\u7a33\u5b9a\u6811\u5b9a\u5411\u7b97\u6cd5\u3002\u5229\u7528\u8fd9\u4e9b\u534f\u8bae\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e3a\u6709\u5411\u6811\u8bbe\u8ba1\u7684\u7b80\u5355\u5e38\u6570\u72b6\u6001\u534f\u8bae\u6765\u5feb\u901f\u89e3\u51b3\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u3002", "result": "\u5728\u6709\u754c\u5ea6\u6811\u4e0a\uff0c\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u53ef\u4ee5\u5b9e\u73b0\u5e38\u6570\u7a7a\u95f4\u534f\u8bae\uff0c\u4e14\u5177\u6709\u63a5\u8fd1\u6700\u4f18\u7684\u6700\u574f\u60c5\u51b5\u671f\u671b\u7a33\u5b9a\u65f6\u95f4\u3002\u65b0\u534f\u8bae\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5b9e\u73b0\u4e86\u7ebf\u6027\u52a0\u901f\u3002\u4f8b\u5982\uff0c\"\u6709\u5411\"\u6e6e\u706d\u52a8\u529b\u5b66\u53ef\u4ee5\u5728\u6709\u5411\u6811\u4e0a\u4ee5O(n\u00b2 log n)\u6b65\u89e3\u51b3\u7cbe\u786e\u591a\u6570\u95ee\u9898\u3002", "conclusion": "\u4e0e\u5b8c\u5168\u56fe\u4e0d\u540c\uff0c\u6709\u754c\u5ea6\u6811\u4e0a\u7684\u7fa4\u4f53\u534f\u8bae\u5bf9\u4e8e\u9886\u5bfc\u8005\u9009\u4e3e\u548c\u7cbe\u786e\u591a\u6570\u95ee\u9898\u4e0d\u5b58\u5728\u663e\u8457\u7684\u7a7a\u95f4-\u65f6\u95f4\u6743\u8861\u3002\u63d0\u51fa\u7684\u65b0\u534f\u8bae\u548c\u7b97\u6cd5\u4e3a\u7a00\u758f\u4ea4\u4e92\u56fe\u4e0a\u7684\u7fa4\u4f53\u534f\u8bae\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e9b\u534f\u8bae\u5177\u6709\u72ec\u7acb\u7684\u7406\u8bba\u4ef7\u503c\u3002"}}
{"id": "2602.16233", "categories": ["cs.DC", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.16233", "abs": "https://arxiv.org/abs/2602.16233", "authors": ["Prabhjot Singh", "Adel N. Toosi", "Rajkumar Buyya"], "title": "DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting", "comment": null, "summary": "Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.", "AI": {"tldr": "\u7535\u8def\u5207\u5272\u5c06\u5927\u91cf\u5b50\u7535\u8def\u5206\u89e3\u4e3a\u5c0f\u7535\u8def\uff0c\u901a\u8fc7\u7ecf\u5178\u91cd\u6784\u6062\u590d\u671f\u671b\u503c\u3002\u672c\u6587\u63d0\u51fa\u5207\u5272\u611f\u77e5\u7684\u4f30\u8ba1\u5668\u6267\u884c\u6d41\u7a0b\uff0c\u91cf\u5316\u5207\u5272\u5728\u8bad\u7ec3\u4e2d\u7684\u7aef\u5230\u7aef\u5f00\u9500\uff0c\u53d1\u73b0\u91cd\u6784\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u4f46\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5f97\u4ee5\u4fdd\u6301\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7535\u8def\u5207\u5272\u7684\u5b50\u7535\u8def\u6570\u91cf\u548c\u91c7\u6837\u590d\u6742\u5ea6\uff0c\u4f46\u5bf9\u7aef\u5230\u7aef\u8bad\u7ec3\u6d41\u7a0b\u7684\u7cfb\u7edf\u5f71\u54cd\u7f3a\u4e4f\u6d4b\u91cf\u3002\u9700\u8981\u91cf\u5316\u5207\u5272\u5728\u8fed\u4ee3\u8bad\u7ec3\u4e2d\u7684\u5b9e\u9645\u5f00\u9500\u3001\u6269\u5c55\u9650\u5236\u4ee5\u53ca\u5bf9\u5b66\u4e60\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u5207\u5272\u611f\u77e5\u7684\u4f30\u8ba1\u5668\u6267\u884c\u6d41\u7a0b\uff0c\u5c06\u7535\u8def\u5207\u5272\u89c6\u4e3a\u5206\u9636\u6bb5\u5206\u5e03\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5c06\u6bcf\u4e2a\u4f30\u8ba1\u5668\u67e5\u8be2\u5206\u89e3\u4e3a\uff1a\u5206\u533a\u3001\u5b50\u5b9e\u9a8c\u751f\u6210\u3001\u5e76\u884c\u6267\u884c\u548c\u7ecf\u5178\u91cd\u6784\u56db\u4e2a\u9636\u6bb5\u3002\u4f7f\u7528Iris\u548cMNIST\u4e8c\u5206\u7c7b\u4efb\u52a1\u7684\u8fd0\u884c\u65f6\u8ddf\u8e2a\u548c\u5b66\u4e60\u7ed3\u679c\u8fdb\u884c\u6d4b\u91cf\u3002", "result": "\u5207\u5272\u5f15\u5165\u663e\u8457\u7684\u7aef\u5230\u7aef\u5f00\u9500\uff0c\u968f\u5207\u5272\u6570\u91cf\u589e\u52a0\uff1b\u91cd\u6784\u5360\u6bcf\u67e5\u8be2\u65f6\u95f4\u7684\u4e3b\u5bfc\u90e8\u5206\uff0c\u9650\u5236\u4e86\u5e76\u884c\u5316\u5e26\u6765\u7684\u52a0\u901f\u3002\u5728\u6d4b\u8bd5\u7684\u914d\u7f6e\u4e2d\uff0c\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u5f97\u4ee5\u4fdd\u6301\uff0c\u67d0\u4e9b\u5207\u5272\u8bbe\u7f6e\u4e0b\u751a\u81f3\u89c2\u5bdf\u5230\u6539\u8fdb\u3002", "conclusion": "\u7535\u8def\u5207\u5272\u5728\u91cf\u5b50\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u5b9e\u9645\u6269\u5c55\u53d6\u51b3\u4e8e\u51cf\u5c11\u548c\u91cd\u53e0\u91cd\u6784\u64cd\u4f5c\uff0c\u4ee5\u53ca\u8003\u8651\u5c4f\u969c\u4e3b\u5bfc\u5173\u952e\u8def\u5f84\u7684\u8c03\u5ea6\u7b56\u7565\u3002\u867d\u7136\u7cfb\u7edf\u5f00\u9500\u663e\u8457\uff0c\u4f46\u5b66\u4e60\u8d28\u91cf\u53ef\u4ee5\u4fdd\u6301\u3002"}}
{"id": "2602.16338", "categories": ["cs.DC", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.16338", "abs": "https://arxiv.org/abs/2602.16338", "authors": ["Mohsen Ahmadvand", "Rok Pajni\u010d", "Ching-Lun Chiu"], "title": "push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation", "comment": null, "summary": "Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks.", "AI": {"tldr": "push0\u662f\u4e00\u4e2a\u4e91\u539f\u751f\u7684\u96f6\u77e5\u8bc6\u8bc1\u660e\u7f16\u6392\u7cfb\u7edf\uff0c\u901a\u8fc7\u89e3\u8026\u8bc1\u660e\u5668\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0e\u8c03\u5ea6\u57fa\u7840\u8bbe\u65bd\uff0c\u5b9e\u73b0\u533a\u5757\u94fe\u7cfb\u7edf\u4e2d\u4e25\u683c\u6709\u5e8f\u3001\u4f4e\u5ef6\u8fdf\u3001\u5bb9\u9519\u7684\u8bc1\u660e\u751f\u6210\u7f16\u6392\u3002", "motivation": "\u96f6\u77e5\u8bc6\u8bc1\u660e\u751f\u6210\u5bf9\u533a\u5757\u94fe\u7cfb\u7edf\u6709\u4e25\u683c\u7684\u65f6\u5e8f\u548c\u53ef\u9760\u6027\u8981\u6c42\u3002ZK-rollups\u4e2d\u5ef6\u8fdf\u8bc1\u660e\u4f1a\u5bfc\u81f4\u6700\u7ec8\u6027\u5ef6\u8fdf\u548c\u7ecf\u6d4e\u635f\u5931\uff1b\u4ee5\u592a\u574aL1 zkEVM\u9700\u8981\u572812\u79d2\u65f6\u9699\u7a97\u53e3\u5185\u5b8c\u6210\u8bc1\u660e\u4ee5\u5b9e\u73b0\u65e0\u72b6\u6001\u9a8c\u8bc1\u3002\u5f53\u524d\u7f3a\u4e4f\u89e3\u51b3\u4e25\u683c\u94fe\u5934\u6392\u5e8f\u3001\u5b50\u65f6\u9699\u5ef6\u8fdf\u9650\u5236\u3001\u5bb9\u9519\u4efb\u52a1\u91cd\u65b0\u5206\u914d\u548c\u8bc1\u660e\u5668\u65e0\u5173\u5de5\u4f5c\u6d41\u7ec4\u5408\u7b49\u8054\u5408\u6311\u6218\u7684\u539f\u5219\u6027\u7f16\u6392\u6846\u67b6\u3002", "method": "push0\u91c7\u7528\u57fa\u4e8e\u6301\u4e45\u4f18\u5148\u7ea7\u961f\u5217\u7684\u4e8b\u4ef6\u9a71\u52a8\u5206\u53d1\u5668-\u6536\u96c6\u5668\u67b6\u6784\uff0c\u5c06\u8bc1\u660e\u5668\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e0e\u8c03\u5ea6\u57fa\u7840\u8bbe\u65bd\u89e3\u8026\u3002\u8be5\u7cfb\u7edf\u5f3a\u5236\u6267\u884c\u5757\u987a\u5e8f\u8bc1\u660e\uff0c\u540c\u65f6\u5229\u7528\u5757\u5185\u5e76\u884c\u6027\uff0c\u652f\u6301\u5f02\u6784zkVMs\u7684\u65e0\u7f1d\u96c6\u6210\u548c\u901a\u8fc7\u6d88\u606f\u6301\u4e45\u5316\u5b9e\u73b0\u81ea\u52a8\u4efb\u52a1\u6062\u590d\u3002", "result": "\u5728\u751f\u4ea7Kubernetes\u96c6\u7fa4\u5b9e\u9a8c\u4e2d\uff0cpush0\u5b9e\u73b0\u4e865\u6beb\u79d2\u4e2d\u4f4d\u6570\u7f16\u6392\u5f00\u9500\uff0c\u572832\u4e2a\u5206\u53d1\u5668\u4e0b\u8fbe\u523099-100%\u7684\u6269\u5c55\u6548\u7387\uff0c\u76f8\u5bf9\u4e8e\u5178\u578b\u76847+\u79d2\u8bc1\u660e\u8ba1\u7b97\u65f6\u95f4\uff0c\u5f00\u9500\u53ef\u5ffd\u7565\uff08\u5c0f\u4e8e0.1%\uff09\u3002\u5728\u53d7\u63a7Docker\u5b9e\u9a8c\u4e2d\u663e\u793a3-10\u6beb\u79d2P50\u6027\u80fd\u3002\u5df2\u5728Zircuit zkrollup\u751f\u4ea7\u90e8\u7f72\uff08\u81ea2025\u5e743\u6708\u4ee5\u6765\u5904\u74061400\u591a\u4e07\u4e2a\u4e3b\u7f51\u533a\u5757\uff09\u3002", "conclusion": "push0\u4e3a\u96c6\u4e2d\u5f0frollup\u8fd0\u8425\u5546\u548c\u53bb\u4e2d\u5fc3\u5316\u591a\u8bc1\u660e\u5668\u7f51\u7edc\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u8c03\u5ea6\u539f\u8bed\uff0c\u80fd\u591f\u6ee1\u8db3\u5b9e\u65f6\u8bc1\u660e\u751f\u6210\u7684\u8981\u6c42\uff0c\u89e3\u51b3\u4e86\u96f6\u77e5\u8bc6\u8bc1\u660e\u7f16\u6392\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2602.16347", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.16347", "abs": "https://arxiv.org/abs/2602.16347", "authors": ["Jon Vehovar", "Miha Rot", "Matja\u017e Depolli", "Gregor Kosec"], "title": "Load Balanced Parallel Node Generation for Meshless Numerical Methods", "comment": null, "summary": "Meshless methods are used to solve partial differential equations by approximating differential operators at a node as a weighted sum of values at its neighbours. One of the algorithms for generating nodes suitable for meshless numerical analysis is an n-dimensional Poisson disc sampling based method. It can handle complex geometries and supports variable node density, a crucial feature for adaptive analysis. We modify this method for parallel execution using coupled spatial indexing and work distribution hypertrees. The latter is prebuilt according to the node density function, ensuring that each leaf represents a balanced work unit. Threads advance separate fronts and claim work hypertree leaves as needed while avoiding leaves neighbouring those claimed by other threads. Node placement constraints and the partially prebuilt spatial hypertree are combined to eliminate the need to lock the tree while it is being modified. Thread collision handling is managed by the work hypertree at the leaf level, drastically reducing the number of required mutex acquisitions for point insertion collision checks. We explore the behaviour of the proposed algorithm and compare the performance with existing attempts at parallelisation and consider the requirements for adapting the developed algorithm to distributed systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e76\u884c\u5316\u7684n\u7ef4\u6cca\u677e\u5706\u76d8\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u65e0\u7f51\u683c\u65b9\u6cd5\u8282\u70b9\uff0c\u901a\u8fc7\u8026\u5408\u7a7a\u95f4\u7d22\u5f15\u548c\u5de5\u4f5c\u5206\u5e03\u8d85\u6811\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\uff0c\u51cf\u5c11\u9501\u7ade\u4e89", "motivation": "\u65e0\u7f51\u683c\u65b9\u6cd5\u9700\u8981\u5408\u9002\u7684\u8282\u70b9\u5206\u5e03\uff0c\u6cca\u677e\u5706\u76d8\u91c7\u6837\u80fd\u5904\u7406\u590d\u6742\u51e0\u4f55\u548c\u53ef\u53d8\u8282\u70b9\u5bc6\u5ea6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e76\u884c\u6548\u7387\u4f4e\uff0c\u9700\u8981\u6539\u8fdb\u5e76\u884c\u5316\u7b56\u7565", "method": "\u4fee\u6539n\u7ef4\u6cca\u677e\u5706\u76d8\u91c7\u6837\u65b9\u6cd5\uff0c\u91c7\u7528\u8026\u5408\u7a7a\u95f4\u7d22\u5f15\u548c\u5de5\u4f5c\u5206\u5e03\u8d85\u6811\uff0c\u9884\u5efa\u5de5\u4f5c\u8d85\u6811\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u7ebf\u7a0b\u72ec\u7acb\u63a8\u8fdb\u5e76\u907f\u514d\u51b2\u7a81\uff0c\u51cf\u5c11\u9501\u9700\u6c42", "result": "\u63d0\u51fa\u7684\u7b97\u6cd5\u51cf\u5c11\u4e86\u7ebf\u7a0b\u78b0\u649e\u5904\u7406\u7684\u4e92\u65a5\u9501\u83b7\u53d6\u6b21\u6570\uff0c\u63d0\u9ad8\u4e86\u5e76\u884c\u6548\u7387\uff0c\u5e76\u63a2\u8ba8\u4e86\u9002\u5e94\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u8981\u6c42", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u6cca\u677e\u5706\u76d8\u91c7\u6837\u7684\u9ad8\u6548\u5e76\u884c\u5316\uff0c\u4e3a\u65e0\u7f51\u683c\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8282\u70b9\u751f\u6210\u65b9\u6848\uff0c\u5e76\u5177\u5907\u6269\u5c55\u5230\u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u6f5c\u529b"}}
{"id": "2602.16362", "categories": ["cs.DC", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.16362", "abs": "https://arxiv.org/abs/2602.16362", "authors": ["MHD Saria Allahham", "Hossam S. Hassanein"], "title": "How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability", "comment": null, "summary": "Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5206\u6790\u6846\u67b6\u6765\u91cf\u5316\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u8ba1\u7b97\u53ef\u9760\u6027\uff0c\u5b9a\u4e49\u8bbe\u5907\u6216\u8bbe\u5907\u96c6\u5408\u5728\u7279\u5b9aQoS\u9608\u503c\u4e0b\u6ee1\u8db3\u5904\u7406\u9700\u6c42\u7684\u6982\u7387\uff0c\u5e76\u63a8\u5bfc\u51fa\u95ed\u5f0f\u53ef\u9760\u6027\u8868\u8fbe\u5f0f\u548c\u6700\u4f18\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u89c4\u5219\u3002", "motivation": "\u6781\u7aef\u8fb9\u7f18\u8ba1\u7b97\uff08XEC\uff09\u5229\u7528\u6d88\u8d39\u8005\u8bbe\u5907\u5904\u7406\u6d41\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4f46\u8fd9\u4e9b\u8bbe\u5907\u7531\u4e8e\u7ade\u4e89\u5e94\u7528\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u4f7f\u7528\u6a21\u5f0f\u8868\u73b0\u51fa\u8ba1\u7b97\u53ef\u7528\u6027\u7684\u6ce2\u52a8\u6027\u3002\u8fd9\u79cd\u6ce2\u52a8\u6027\u5e26\u6765\u4e86\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u5982\u4f55\u91cf\u5316\u8bbe\u5907\u6216\u8bbe\u5907\u96c6\u5408\u80fd\u591f\u7ef4\u6301\u6d41\u5f0f\u670d\u52a1\u6240\u9700\u5904\u7406\u901f\u7387\u7684\u6982\u7387\uff1f", "method": "\u63d0\u51fa\u4e00\u4e2a\u8ba1\u7b97\u53ef\u9760\u6027\u5206\u6790\u6846\u67b6\uff0c\u5728\u4e24\u79cd\u4fe1\u606f\u673a\u5236\u4e0b\u63a8\u5bfc\u95ed\u5f0f\u53ef\u9760\u6027\u8868\u8fbe\u5f0f\uff1a\u6700\u5c0f\u4fe1\u606f\uff08\u4ec5\u9700\u58f0\u660e\u7684\u64cd\u4f5c\u8fb9\u754c\uff09\u548c\u5386\u53f2\u6570\u636e\uff08\u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u4ece\u8fc7\u53bb\u89c2\u6d4b\u4e2d\u7cbe\u70bc\u4f30\u8ba1\uff09\u3002\u6846\u67b6\u6269\u5c55\u5230\u591a\u8bbe\u5907\u90e8\u7f72\uff0c\u4e3a\u4e32\u884c\u3001\u5e76\u884c\u548c\u5206\u533a\u5de5\u4f5c\u8d1f\u8f7d\u914d\u7f6e\u63d0\u4f9b\u53ef\u9760\u6027\u8868\u8fbe\u5f0f\uff0c\u63a8\u5bfc\u6700\u4f18\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u89c4\u5219\u548c\u8bbe\u5907\u9009\u62e9\u7684\u89e3\u6790\u8fb9\u754c\u3002", "result": "\u4f7f\u7528YOLO11m\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u4f5c\u4e3a\u4ee3\u8868\u6027\u5206\u5e03\u5f0f\u63a8\u7406\u6d41\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5728\u6a21\u62dfXED\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5206\u6790\u9884\u6d4b\u3001\u8499\u7279\u5361\u6d1b\u91c7\u6837\u548c\u5b9e\u9645\u6d4b\u91cf\u5728\u4e0d\u540c\u5bb9\u91cf\u548c\u9700\u6c42\u914d\u7f6e\u4e0b\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u5206\u6790\u6846\u67b6\u4e3a\u7f16\u6392\u5668\u63d0\u4f9b\u4e86\u53ef\u5904\u7406\u7684\u5de5\u5177\u6765\u8bc4\u4f30\u90e8\u7f72\u53ef\u884c\u6027\u548c\u914d\u7f6e\u5206\u5e03\u5f0f\u6d41\u5f0f\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u8bbe\u5907\u8ba1\u7b97\u53ef\u7528\u6027\u6ce2\u52a8\u5e26\u6765\u7684\u53ef\u9760\u6027\u6311\u6218\u3002"}}
{"id": "2602.16603", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.16603", "abs": "https://arxiv.org/abs/2602.16603", "authors": ["Chia-chi Hsieh", "Zan Zong", "Xinyang Chen", "Jianjiang Li", "Jidong Zhai", "Lijie Wen"], "title": "FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving", "comment": "13 pages", "summary": "The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.\n  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.", "AI": {"tldr": "FlowPrefill\u901a\u8fc7\u89e3\u8026\u62a2\u5360\u7c92\u5ea6\u4e0e\u8c03\u5ea6\u9891\u7387\uff0c\u63d0\u51fa\u7b97\u5b50\u7ea7\u62a2\u5360\u548c\u4e8b\u4ef6\u9a71\u52a8\u8c03\u5ea6\uff0c\u5728\u4fdd\u8bc1TTFT SLO\u7684\u540c\u65f6\u63d0\u5347\u541e\u5410\u91cf5.6\u500d", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7cfb\u7edf\u4e2d\uff0c\u8ba1\u7b97\u5bc6\u96c6\u7684prefill\u9636\u6bb5\u5b58\u5728\u961f\u5934\u963b\u585e\u95ee\u9898\uff0c\u957f\u8bf7\u6c42\u72ec\u5360\u8d44\u6e90\u5bfc\u81f4\u9ad8\u4f18\u5148\u7ea7\u8bf7\u6c42\u5ef6\u8fdf\uff0c\u9020\u6210TTFT SLO\u8fdd\u89c4\u3002\u73b0\u6709\u5206\u5757prefill\u65b9\u6cd5\u5b58\u5728\u54cd\u5e94\u6027\u4e0e\u541e\u5410\u91cf\u7684\u56fa\u6709\u6743\u8861\uff0c\u9700\u8981\u81ea\u9002\u5e94\u62a2\u5360\u673a\u5236\u3002", "method": "1) \u7b97\u5b50\u7ea7\u62a2\u5360\uff1a\u5229\u7528\u7b97\u5b50\u8fb9\u754c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6267\u884c\u4e2d\u65ad\uff0c\u907f\u514d\u56fa\u5b9a\u5c0f\u5206\u5757\u5e26\u6765\u7684\u6548\u7387\u635f\u5931\uff1b2) \u4e8b\u4ef6\u9a71\u52a8\u8c03\u5ea6\uff1a\u4ec5\u5728\u8bf7\u6c42\u5230\u8fbe\u6216\u5b8c\u6210\u65f6\u89e6\u53d1\u8c03\u5ea6\u51b3\u7b56\uff0c\u652f\u6301\u9ad8\u6548\u62a2\u5360\u54cd\u5e94\u6027\u540c\u65f6\u6700\u5c0f\u5316\u63a7\u5236\u5e73\u9762\u5f00\u9500\u3002", "result": "\u5728\u771f\u5b9e\u751f\u4ea7trace\u8bc4\u4f30\u4e2d\uff0cFlowPrefill\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5c06\u6700\u5927\u6709\u6548\u541e\u5410\u91cf\u63d0\u5347\u8fbe5.6\u500d\uff0c\u540c\u65f6\u6ee1\u8db3\u5f02\u6784SLO\u8981\u6c42\u3002", "conclusion": "FlowPrefill\u901a\u8fc7\u89e3\u8026\u62a2\u5360\u7c92\u5ea6\u4e0e\u8c03\u5ea6\u9891\u7387\uff0c\u89e3\u51b3\u4e86LLM\u670d\u52a1\u7cfb\u7edf\u4e2dprefill\u9636\u6bb5\u7684\u54cd\u5e94\u6027\u4e0e\u541e\u5410\u91cf\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86TTFT\u4e0e\u6709\u6548\u541e\u5410\u91cf\u7684\u53cc\u91cd\u4f18\u5316\u3002"}}
