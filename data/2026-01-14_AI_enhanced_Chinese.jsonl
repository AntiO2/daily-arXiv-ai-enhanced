{"id": "2601.08025", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08025", "abs": "https://arxiv.org/abs/2601.08025", "authors": ["Adiba Masud", "Nicholas Foley", "Pragathi Durga Rajarajan", "Palden Lama"], "title": "Where to Split? A Pareto-Front Analysis of DNN Partitioning for Edge Inference", "comment": null, "summary": "The deployment of deep neural networks (DNNs) on resource-constrained edge devices is frequently hindered by their significant computational and memory requirements. While partitioning and distributing a DNN across multiple devices is a well-established strategy to mitigate this challenge, prior research has largely focused on single-objective optimization, such as minimizing latency or maximizing throughput. This paper challenges that view by reframing DNN partitioning as a multi-objective optimization problem. We argue that in real-world scenarios, a complex trade-off between latency and throughput exists, which is further complicated by network variability. To address this, we introduce ParetoPipe, an open-source framework that leverages Pareto front analysis to systematically identify optimal partitioning strategies that balance these competing objectives.\n  Our contributions are threefold: we benchmark pipeline partitioned inference on a heterogeneous testbed of Raspberry Pis and a GPU-equipped edge server; we identify Pareto-optimal points to analyze the latency-throughput trade-off under varying network conditions; and we release a flexible, open-source framework to facilitate distributed inference and benchmarking. This toolchain features dual communication backends, PyTorch RPC and a custom lightweight implementation, to minimize overhead and support broad experimentation.", "AI": {"tldr": "ParetoPipe\uff1a\u4e00\u4e2a\u57fa\u4e8e\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5e73\u8861DNN\u5206\u533a\u63a8\u7406\u7684\u5ef6\u8fdf\u4e0e\u541e\u5410\u91cf\u6743\u8861\u3002", "motivation": "\u73b0\u6709DNN\u5206\u533a\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u76ee\u6807\u4f18\u5316\uff08\u5982\u6700\u5c0f\u5316\u5ef6\u8fdf\u6216\u6700\u5927\u5316\u541e\u5410\u91cf\uff09\uff0c\u4f46\u5728\u5b9e\u9645\u8fb9\u7f18\u90e8\u7f72\u573a\u666f\u4e2d\uff0c\u9700\u8981\u5728\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u8fdb\u884c\u590d\u6742\u6743\u8861\uff0c\u4e14\u7f51\u7edc\u6761\u4ef6\u53d8\u5316\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faParetoPipe\u6846\u67b6\uff0c\u91c7\u7528\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\u6765\u7cfb\u7edf\u8bc6\u522b\u5e73\u8861\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u7684\u6700\u4f18\u5206\u533a\u7b56\u7565\u3002\u6846\u67b6\u5305\u542b\u53cc\u901a\u4fe1\u540e\u7aef\uff08PyTorch RPC\u548c\u81ea\u5b9a\u4e49\u8f7b\u91cf\u7ea7\u5b9e\u73b0\uff09\uff0c\u5e76\u5728\u6811\u8393\u6d3e\u548cGPU\u8fb9\u7f18\u670d\u52a1\u5668\u7ec4\u6210\u7684\u5f02\u6784\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u5e15\u7d2f\u6258\u6700\u4f18\u70b9\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u7f51\u7edc\u6761\u4ef6\u4e0b\u7684\u5ef6\u8fdf-\u541e\u5410\u91cf\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u5f00\u6e90\u5de5\u5177\u94fe\u652f\u6301\u5206\u5e03\u5f0f\u63a8\u7406\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u5c06DNN\u5206\u533a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\u80fd\u591f\u7cfb\u7edf\u5e73\u8861\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u7684\u7ade\u4e89\u76ee\u6807\uff0c\u4e3a\u5b9e\u9645\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08082", "categories": ["cs.DC", "cs.ET", "cs.MS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.08082", "abs": "https://arxiv.org/abs/2601.08082", "authors": ["Vicki Carrica", "Rabab Alomairy", "Evelyne Ringoot", "Alan Edelman"], "title": "Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs", "comment": "10 pages, 11 figures", "summary": "Symmetric linear solves are fundamental to a wide range of scientific and engineering applications, from climate modeling and structural analysis to machine learning and optimization. These workloads often rely on Cholesky (POTRF) decomposition and its supporting operations, triangular solves (TRSM) and symmetric rank-k updates (SYRK), which together form the computational core for solving symmetric positive-definite systems. To accelerate these kernels, we present a portable, mixed-precision solver designed for Matrix Processing Units (MXUs), including NVIDIA Tensor Cores (H200) and AMD Matrix Cores (MI300X). Our algorithm builds on a nested recursive formulation in which Cholesky exposes parallelism through recursive decomposition of its TRSM and SYRK sub-problems. This structure yields a hierarchical recursion that maximizes GEMM throughput while enabling fine-grained control over numerical precision. We introduce a custom recursive data structure that assigns low-precision FP16 arithmetic to large off-diagonal blocks, while preserving high precision on diagonal blocks to ensure numerical stability. The solver is implemented in Julia, leveraging array programming, multiple dispatch, and dynamic type inference to enable seamless expression of mixed-precision computation. This design provides a high-level, hardware-agnostic interface while efficiently interfacing with low-level vendor libraries for backend portability. On H200, our recursive FP64 SYRK achieves a 14x speedup over cuBLAS, while mixed-precision delivers up to 27x speedup in SYRK and 5x in TRSM over full-precision baselines. This results in a 5x overall speedup for Cholesky versus cuSOLVER FP64, with 100x better accuracy than pure FP16 while retaining 88% of its peak speedup. Comparable performance and accuracy trends are observed on MI300X, demonstrating broad applicability across GPUs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u77e9\u9635\u5904\u7406\u5355\u5143\uff08MXU\uff09\u7684\u4fbf\u643a\u5f0f\u6df7\u5408\u7cbe\u5ea6\u5bf9\u79f0\u7ebf\u6027\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u548c\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\uff0c\u5728NVIDIA H200\u548cAMD MI300X\u4e0a\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u5bf9\u79f0\u7ebf\u6027\u6c42\u89e3\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edfCholesky\u5206\u89e3\u53ca\u5176\u76f8\u5173\u64cd\u4f5c\uff08TRSM\u3001SYRK\uff09\u5728\u77e9\u9635\u5904\u7406\u5355\u5143\u4e0a\u7684\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u9700\u8981\u517c\u987e\u8ba1\u7b97\u6548\u7387\u548c\u6570\u503c\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u5d4c\u5957\u9012\u5f52\u7b97\u6cd5\uff0c\u5c06Cholesky\u5206\u89e3\u9012\u5f52\u5206\u89e3\u4e3aTRSM\u548cSYRK\u5b50\u95ee\u9898\uff1b\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u9012\u5f52\u6570\u636e\u7ed3\u6784\uff0c\u5bf9\u5927\u578b\u975e\u5bf9\u89d2\u5757\u4f7f\u7528FP16\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\uff0c\u5bf9\u89d2\u5757\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u4ee5\u786e\u4fdd\u6570\u503c\u7a33\u5b9a\u6027\uff1b\u5728Julia\u4e2d\u5b9e\u73b0\uff0c\u5229\u7528\u6570\u7ec4\u7f16\u7a0b\u548c\u591a\u5206\u6d3e\u7279\u6027\u3002", "result": "\u5728NVIDIA H200\u4e0a\uff1a\u9012\u5f52FP64 SYRK\u6bd4cuBLAS\u5feb14\u500d\uff1b\u6df7\u5408\u7cbe\u5ea6SYRK\u6bd4\u5168\u7cbe\u5ea6\u57fa\u7ebf\u5feb27\u500d\uff0cTRSM\u5feb5\u500d\uff1bCholesky\u6574\u4f53\u6bd4cuSOLVER FP64\u5feb5\u500d\uff0c\u7cbe\u5ea6\u6bd4\u7eafFP16\u9ad8100\u500d\uff0c\u540c\u65f6\u4fdd\u630188%\u7684\u5cf0\u503c\u52a0\u901f\u3002\u5728AMD MI300X\u4e0a\u89c2\u5bdf\u5230\u7c7b\u4f3c\u7684\u6027\u80fd\u548c\u7cbe\u5ea6\u8d8b\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8de8GPU\u5e73\u53f0\u7684\u4fbf\u643a\u5f0f\u9ad8\u6027\u80fd\u5bf9\u79f0\u7ebf\u6027\u6c42\u89e3\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\u5728\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08277", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08277", "abs": "https://arxiv.org/abs/2601.08277", "authors": ["Yizhuo Rao", "Xingjian Cui", "Jiabin Xie", "Shangzhi Pang", "Guangnan Feng", "Jinhui Wei", "Zhiguang Chen", "Yutong Lu"], "title": "Matrix-PIC: Harnessing Matrix Outer-product for High-Performance Particle-in-Cell Simulations", "comment": "Accepted for publication at EuroSys 2026", "summary": "Particle-in-Cell (PIC) simulations spend most of their execution time on particle--grid interactions, where fine-grained atomic updates become a major bottleneck on traditional many-core CPUs. Recent CPU architectures integrate specialized Matrix Processing Units (MPUs) that efficiently support matrix outer-product operations, offering new opportunities to overcome this limitation. Leveraging this architectural shift, this work focuses on redesigning the current deposition step of PIC simulations under a matrix-centric execution model.\n  We present MatrixPIC, the first holistic co-design of the deposition kernel, data layout, and incremental particle sorting tailored to the hybrid MPU--VPU SIMD model on modern CPUs. MatrixPIC introduces: (i)~a block-matrix formulation of the current deposition algorithm that maps naturally to MPU outer-product primitives; (ii)~a hybrid execution pipeline that combines MPU-based high-density accumulation with VPU-based data preparation and control flow; and (iii)~an $O(1)$-amortized incremental sorter based on a gapped packed-memory array to preserve data locality for efficient MPU execution.\n  Evaluated on a next-generation HPC platform, MatrixPIC achieves significant performance gains. In Laser-Wakefield Acceleration (LWFA) simulations, it delivers up to $2.63\\times$ speedup in total runtime. For third-order deposition, the core kernel is accelerated by $8.7\\times$ over the baseline and $2.0\\times$ over the best hand-optimized VPU implementation. Moreover, MatrixPIC reaches $83.08\\%$ of theoretical CPU peak performance, nearly $2.8\\times$ higher than a highly optimized CUDA kernel on a data center GPU. These results demonstrate the effectiveness of matrix-oriented co-design for accelerating PIC simulations on emerging CPU architectures.", "AI": {"tldr": "MatrixPIC\uff1a\u9996\u4e2a\u9488\u5bf9\u73b0\u4ee3CPU\u4e2dMPU-VPU\u6df7\u5408\u67b6\u6784\u7684PIC\u6a21\u62df\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u77e9\u9635\u5316\u6c89\u79ef\u7b97\u6cd5\u3001\u6df7\u5408\u6267\u884c\u6d41\u6c34\u7ebf\u548c\u589e\u91cf\u6392\u5e8f\uff0c\u663e\u8457\u63d0\u5347\u7c92\u5b50-\u7f51\u683c\u4ea4\u4e92\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u591a\u6838CPU\u4e0aPIC\u6a21\u62df\u7684\u7c92\u5b50-\u7f51\u683c\u4ea4\u4e92\u4e2d\uff0c\u7ec6\u7c92\u5ea6\u539f\u5b50\u66f4\u65b0\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u4ee3CPU\u96c6\u6210\u4e86\u4e13\u95e8\u652f\u6301\u77e9\u9635\u5916\u79ef\u8fd0\u7b97\u7684MPU\uff0c\u4e3a\u514b\u670d\u8fd9\u4e00\u9650\u5236\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "1) \u63d0\u51fa\u5757\u77e9\u9635\u5f62\u5f0f\u7684\u7535\u6d41\u6c89\u79ef\u7b97\u6cd5\uff0c\u81ea\u7136\u6620\u5c04\u5230MPU\u5916\u79ef\u539f\u8bed\uff1b2) \u8bbe\u8ba1\u6df7\u5408\u6267\u884c\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408MPU\u9ad8\u5bc6\u5ea6\u7d2f\u79ef\u4e0eVPU\u6570\u636e\u51c6\u5907\u548c\u63a7\u5236\u6d41\uff1b3) \u57fa\u4e8e\u95f4\u9699\u586b\u5145\u5185\u5b58\u6570\u7ec4\u7684O(1)\u644a\u9500\u589e\u91cf\u6392\u5e8f\u5668\uff0c\u4fdd\u6301\u6570\u636e\u5c40\u90e8\u6027\u3002", "result": "\u5728\u4e0b\u4e00\u4ee3HPC\u5e73\u53f0\u4e0a\uff0cMatrixPIC\u5728\u6fc0\u5149\u5c3e\u6ce2\u573a\u52a0\u901f\u6a21\u62df\u4e2d\u5b9e\u73b0\u603b\u8fd0\u884c\u65f6\u95f42.63\u500d\u52a0\u901f\uff0c\u4e09\u9636\u6c89\u79ef\u6838\u5fc3\u5185\u6838\u6bd4\u57fa\u7ebf\u52a0\u901f8.7\u500d\uff0c\u6bd4\u6700\u4f73\u624b\u52a8\u4f18\u5316VPU\u5b9e\u73b0\u5feb2.0\u500d\uff0c\u8fbe\u5230\u7406\u8bbaCPU\u5cf0\u503c\u6027\u80fd\u768483.08%\uff0c\u6bd4\u6570\u636e\u4e2d\u5fc3GPU\u4e0a\u9ad8\u5ea6\u4f18\u5316\u7684CUDA\u5185\u6838\u9ad8\u8fd12.8\u500d\u3002", "conclusion": "MatrixPIC\u8bc1\u660e\u4e86\u9762\u5411\u77e9\u9635\u7684\u534f\u540c\u8bbe\u8ba1\u5728\u52a0\u901f\u65b0\u5174CPU\u67b6\u6784\u4e0aPIC\u6a21\u62df\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5229\u7528\u73b0\u4ee3CPU\u4e2d\u4e13\u7528\u77e9\u9635\u5904\u7406\u5355\u5143\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.08374", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2601.08374", "abs": "https://arxiv.org/abs/2601.08374", "authors": ["Dali Chang", "Chong Zhang", "Kaiqi Zhang", "Mingguan Yang", "Huiyuan Li", "Weiqiang Kong"], "title": "Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity", "comment": null, "summary": "In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product elements, causing their performance \"sweet spot\" to anomalously remain at the low order of $p \\approx 2$, which severely limits the potential of high-order methods. To address this challenge, we design and implement a highly optimized PA operator within the MFEM framework, deeply integrated with a Geometric Multigrid (GMG) preconditioner. Our multi-level optimization strategy includes replacing the original $O(p^6)$ generic algorithm with an efficient $O(p^4)$ one based on tensor factorization, exploiting Voigt symmetry to reduce redundant computations for the elasticity problem, and employing macro-kernel fusion to enhance data locality and break the memory bandwidth bottleneck. Extensive experiments on mainstream x86 and ARM architectures demonstrate that our method successfully shifts the performance \"sweet spot\" to the higher-order region of $p \\ge 6$. Compared to the MFEM baseline, the optimized core operator (kernel) achieves speedups of 7x to 83x, which translates to a 3.6x to 16.8x end-to-end performance improvement in the complete solution process. This paper provides a validated and efficient practical path for conducting large-scale, high-order elasticity simulations on mainstream CPU hardware.", "AI": {"tldr": "\u9488\u5bf9\u5f39\u6027\u529b\u5b66\u9ad8\u9636\u6709\u9650\u5143\u5206\u6790\uff0c\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u9ad8\u5ea6\u4f18\u5316\u7684\u77e9\u9635\u81ea\u7531\u7b97\u5b50\uff0c\u901a\u8fc7\u591a\u7ea7\u4f18\u5316\u7b56\u7565\u5c06\u6027\u80fd\u6700\u4f73\u70b9\u4ece\u4f4e\u9636p\u22482\u63d0\u5347\u5230\u9ad8\u9636p\u22656\u533a\u57df\uff0c\u5728\u4e3b\u6d41CPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u77e9\u9635\u81ea\u7531\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3CPU\u67b6\u6784\u548c\u5f20\u91cf\u79ef\u5355\u5143\u7684\u7279\u6b8a\u7ed3\u6784\uff0c\u5bfc\u81f4\u5176\u6027\u80fd\u6700\u4f73\u70b9\u5f02\u5e38\u5730\u505c\u7559\u5728\u4f4e\u9636p\u22482\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u9ad8\u9636\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "method": "\u5728MFEM\u6846\u67b6\u5185\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u9ad8\u5ea6\u4f18\u5316\u7684\u77e9\u9635\u81ea\u7531\u7b97\u5b50\uff0c\u4e0e\u51e0\u4f55\u591a\u91cd\u7f51\u683c\u9884\u5904\u7406\u5668\u6df1\u5ea6\u96c6\u6210\u3002\u91c7\u7528\u591a\u7ea7\u4f18\u5316\u7b56\u7565\uff1a1\uff09\u7528\u57fa\u4e8e\u5f20\u91cf\u5206\u89e3\u7684O(p^4)\u7b97\u6cd5\u66ff\u6362\u539f\u59cbO(p^6)\u901a\u7528\u7b97\u6cd5\uff1b2\uff09\u5229\u7528Voigt\u5bf9\u79f0\u6027\u51cf\u5c11\u5f39\u6027\u95ee\u9898\u7684\u5197\u4f59\u8ba1\u7b97\uff1b3\uff09\u91c7\u7528\u5b8f\u6838\u878d\u5408\u589e\u5f3a\u6570\u636e\u5c40\u90e8\u6027\u5e76\u7a81\u7834\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\u3002", "result": "\u5728\u4e3b\u6d41x86\u548cARM\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u6027\u80fd\u6700\u4f73\u70b9\u8f6c\u79fb\u5230\u9ad8\u9636\u533a\u57dfp\u22656\u3002\u4e0eMFEM\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4f18\u5316\u540e\u7684\u6838\u5fc3\u7b97\u5b50\u5b9e\u73b0\u4e867x\u523083x\u7684\u52a0\u901f\uff0c\u5728\u5b8c\u6574\u6c42\u89e3\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e863.6x\u523016.8x\u7684\u7aef\u5230\u7aef\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u4e3a\u5728\u4e3b\u6d41CPU\u786c\u4ef6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u9ad8\u9636\u5f39\u6027\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u6761\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u9ad8\u6548\u5b9e\u8df5\u8def\u5f84\uff0c\u901a\u8fc7\u6df1\u5ea6\u4f18\u5316\u6210\u529f\u89e3\u51b3\u4e86\u77e9\u9635\u81ea\u7531\u65b9\u6cd5\u5728\u9ad8\u9636\u533a\u57df\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002"}}
{"id": "2601.07939", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.07939", "abs": "https://arxiv.org/abs/2601.07939", "authors": ["Shireesh Reddy Pyreddy", "Khaja Valli Pathan", "Hasan Masum", "Tarannum Shaila Zaman"], "title": "SECite: Analyzing and Summarizing Citations in Software Engineering Literature", "comment": "Accepted at IEEE CCWC 2026", "summary": "Identifying the strengths and limitations of a research paper is a core component of any literature review. However, traditional summaries reflect only the authors' self-presented perspective. Analyzing how other researchers discuss and cite the paper can offer a deeper, more practical understanding of its contributions and shortcomings. In this research, we introduce SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. We develop a semi-automated pipeline to extract citations referencing nine research papers and apply advanced natural language processing (NLP) techniques with unsupervised machine learning to classify these citation statements as positive or negative. Beyond sentiment classification, we use generative AI to produce sentiment-specific summaries that capture the strengths and limitations of each target paper, derived both from clustered citation groups and from the full text. Our findings reveal meaningful patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentation. By integrating citation sentiment analysis with LLM-based summarization, this study provides a comprehensive framework for assessing scholarly contributions.", "AI": {"tldr": "SECite\uff1a\u901a\u8fc7\u5f15\u6587\u60c5\u611f\u5206\u6790\u8bc4\u4f30\u5b66\u672f\u5f71\u54cd\u529b\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408NLP\u548c\u751f\u6210\u5f0fAI\u81ea\u52a8\u5206\u6790\u5f15\u6587\u60c5\u611f\u5e76\u751f\u6210\u603b\u7ed3", "motivation": "\u4f20\u7edf\u6587\u732e\u7efc\u8ff0\u4ec5\u53cd\u6620\u4f5c\u8005\u81ea\u6211\u9648\u8ff0\u7684\u89c6\u89d2\uff0c\u65e0\u6cd5\u5168\u9762\u4e86\u89e3\u8bba\u6587\u7684\u5b9e\u9645\u8d21\u732e\u548c\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5206\u6790\u5176\u4ed6\u7814\u7a76\u8005\u5982\u4f55\u8ba8\u8bba\u548c\u5f15\u7528\u8bba\u6587\uff0c\u53ef\u4ee5\u83b7\u5f97\u66f4\u6df1\u5165\u3001\u66f4\u5b9e\u7528\u7684\u7406\u89e3\u3002", "method": "\u5f00\u53d1\u534a\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u53d69\u7bc7\u7814\u7a76\u8bba\u6587\u7684\u5f15\u6587\uff0c\u5e94\u7528\u5148\u8fdb\u7684NLP\u6280\u672f\u548c\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5bf9\u5f15\u6587\u8bed\u53e5\u8fdb\u884c\u6b63\u9762/\u8d1f\u9762\u60c5\u611f\u5206\u7c7b\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u5f0fAI\u57fa\u4e8e\u805a\u7c7b\u5f15\u6587\u7ec4\u548c\u5168\u6587\u751f\u6210\u60c5\u611f\u7279\u5b9a\u7684\u603b\u7ed3\u3002", "result": "\u63ed\u793a\u4e86\u5b66\u672f\u793e\u533a\u5bf9\u8fd9\u4e9b\u5de5\u4f5c\u7684\u611f\u77e5\u6a21\u5f0f\uff0c\u7a81\u51fa\u4e86\u5916\u90e8\u5f15\u6587\u53cd\u9988\u4e0e\u4f5c\u8005\u81ea\u6211\u9648\u8ff0\u4e4b\u95f4\u7684\u5951\u5408\u4e0e\u5206\u6b67\u70b9\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u5f15\u6587\u60c5\u611f\u5206\u6790\u548c\u57fa\u4e8eLLM\u7684\u603b\u7ed3\uff0c\u672c\u7814\u7a76\u4e3a\u8bc4\u4f30\u5b66\u672f\u8d21\u732e\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u3002"}}
{"id": "2601.08109", "categories": ["cs.DB", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08109", "abs": "https://arxiv.org/abs/2601.08109", "authors": ["Sridhar Mahadevan"], "title": "CSQL: Mapping Documents into Causal Databases", "comment": "26 pages", "summary": "We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer \"why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: \"What are the strongest causal influences on bipedalism?'' or \"Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.", "AI": {"tldr": "CSQL\u7cfb\u7edf\u5c06\u975e\u7ed3\u6784\u5316\u6587\u672c\u6587\u6863\u81ea\u52a8\u8f6c\u6362\u4e3a\u53efSQL\u67e5\u8be2\u7684\u56e0\u679c\u6570\u636e\u5e93\uff0c\u652f\u6301\u56e0\u679c\u5e72\u9884\u548c\u7ed3\u6784\u5316\u56e0\u679c\u67e5\u8be2\uff0c\u8d85\u8d8a\u4f20\u7edf\u5173\u8054\u68c0\u7d22\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\uff08\u5982RAG\u6216\u77e5\u8bc6\u56fe\u8c31\uff09\u4e3b\u8981\u652f\u6301\u5173\u8054\u68c0\u7d22\uff0c\u65e0\u6cd5\u56de\u7b54\"\u4e3a\u4ec0\u4e48\"\u8fd9\u7c7b\u56e0\u679c\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u6587\u6863\u96c6\u5408\u4e2d\u63d0\u53d6\u56e0\u679c\u4fe1\u606f\u5e76\u8fdb\u884c\u56e0\u679c\u5206\u6790\u7684\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8eDEMOCRITUS\u7cfb\u7edf\uff0c\u4ece\u6587\u6863\u4e2d\u63d0\u53d6\u6570\u5343\u4e2a\u5c40\u90e8\u56e0\u679c\u6a21\u578b\uff0c\u6784\u5efa\u56e0\u679c\u6570\u636e\u5e93\u3002\u652f\u6301\u5c06RAG/IE\u7f16\u8bd1\u7684\u56e0\u679c\u8bed\u6599\u5e93\u5927\u89c4\u6a21\u6574\u5408\uff0c\u5982TCC\u7ecf\u6d4e\u5b66\u8bba\u6587\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u5305\u542b265,656\u4e2a\u56e0\u679c\u58f0\u660e\u5b9e\u4f8b\u7684\u56e0\u679c\u6570\u636e\u5e93\uff0c\u6db5\u76d645,319\u7bc7\u8bba\u6587\u300144\u5e74\u6570\u636e\u548c1,575\u4e2a\u62a5\u544a\u65b9\u6cd5\u5b57\u7b26\u4e32\u3002\u652f\u6301\u6587\u6863\u7ea7\u548c\u8bed\u6599\u5e93\u7ea7\u7684\u56e0\u679c\u67e5\u8be2\u4e0e\u7eb5\u5411\u5206\u6790\u3002", "conclusion": "CSQL\u4f5c\u4e3a\u4ece\u975e\u7ed3\u6784\u5316\u6587\u6863\u5230\u56e0\u679c\u6570\u636e\u5e93\u7684\u7f16\u8bd1\u5668\uff0c\u914d\u5907\u6709\u539f\u5219\u7684\u67e5\u8be2\u4ee3\u6570\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5546\u4e1a\u3001\u4eba\u6587\u548c\u79d1\u5b66\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u56e0\u679c\u5206\u6790\u800c\u4e0d\u4ec5\u4ec5\u662f\u5173\u8054\u68c0\u7d22\u3002"}}
{"id": "2601.08800", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.08800", "abs": "https://arxiv.org/abs/2601.08800", "authors": ["Bowen Zhou", "Jinrui Jia", "Wenhao He", "Yong Zhang", "Fang Dong"], "title": "MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm", "comment": "Submitted to ICDCS 2026", "summary": "The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.\n  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.", "AI": {"tldr": "MixServe\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u90e8\u7f72MoE\u6a21\u578b\u7684\u81ea\u52a8\u5206\u5e03\u5f0f\u670d\u52a1\u7cfb\u7edf\uff0c\u91c7\u7528\u57fa\u4e8e\u878d\u5408AR-A2A\u901a\u4fe1\u7b97\u6cd5\u7684TP-EP\u6df7\u5408\u5e76\u884c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "MoE\u6a21\u578b\u53c2\u6570\u5e9e\u5927\uff0c\u53ea\u80fd\u5728\u591aGPU/\u591a\u8282\u70b9\u7cfb\u7edf\u4e2d\u90e8\u7f72\uff0c\u901a\u4fe1\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709TP\u65b9\u6cd5\u8282\u70b9\u95f4\u6548\u7387\u4f4e\uff0cEP\u65b9\u6cd5\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u9700\u8981\u66f4\u597d\u7684\u5e76\u884c\u7b56\u7565\u3002", "method": "\u63d0\u51faMixServe\u7cfb\u7edf\uff1a1) \u8bc4\u4f30\u4e0d\u540c\u5e76\u884c\u7b56\u7565\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u878d\u5408AR-A2A\u901a\u4fe1\u7b97\u6cd5\u7684TP-EP\u6df7\u5408\u5e76\u884c\uff0c\u91cd\u53e0\u8282\u70b9\u5185AR\u901a\u4fe1\u548c\u8282\u70b9\u95f4A2A\u901a\u4fe1\u3002", "result": "\u5728DeepSeek-R1\u548cQwen3\u6a21\u578b\u4e0a\u5b9e\u9a8c\u663e\u793a\uff1a\u9996token\u65f6\u95f4\u52a0\u901f1.08-3.80\u500d\uff0ctoken\u95f4\u5ef6\u8fdf\u52a0\u901f1.03-1.66\u500d\uff0c\u541e\u5410\u91cf\u63d0\u53475.2%-50.3%\u3002", "conclusion": "MixServe\u901a\u8fc7\u81ea\u52a8\u9009\u62e9\u5e76\u884c\u7b56\u7565\u548c\u878d\u5408\u901a\u4fe1\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5206\u5e03\u5f0f\u670d\u52a1\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2601.08012", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08012", "abs": "https://arxiv.org/abs/2601.08012", "authors": ["Aarya Doshi", "Yining Hong", "Congying Xu", "Eunsuk Kang", "Alexandros Kapravelos", "Christian K\u00e4stner"], "title": "Towards Verifiably Safe Tool Use for LLM Agents", "comment": "4 pages, 1 figure; accepted to ICSE NIER 2026", "summary": "Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408STPA\u5b89\u5168\u5206\u6790\u548c\u589e\u5f3aMCP\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\uff0c\u4ece\u4e34\u65f6\u53ef\u9760\u6027\u4fee\u590d\u8f6c\u5411\u4e3b\u52a8\u9632\u62a4", "motivation": "LLM\u667a\u80fd\u4f53\u901a\u8fc7\u5de5\u5177\u8c03\u7528\u6269\u5c55\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u5f15\u53d1\u654f\u611f\u6570\u636e\u6cc4\u9732\u3001\u5173\u952e\u8bb0\u5f55\u8986\u76d6\u7b49\u98ce\u9669\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u7cfb\u7edf\u5b89\u5168\uff0c\u9700\u8981\u5f62\u5f0f\u5316\u4fdd\u969c\u673a\u5236", "method": "1) \u5e94\u7528\u7cfb\u7edf\u7406\u8bba\u8fc7\u7a0b\u5206\u6790(STPA)\u8bc6\u522b\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u5371\u9669\uff0c\u63a8\u5bfc\u5b89\u5168\u8981\u6c42\uff0c\u5f62\u5f0f\u5316\u4e3a\u6570\u636e\u6d41\u548c\u5de5\u5177\u5e8f\u5217\u7684\u53ef\u6267\u884c\u89c4\u8303\uff1b2) \u5f15\u5165\u80fd\u529b\u589e\u5f3a\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u6846\u67b6\uff0c\u8981\u6c42\u5bf9\u80fd\u529b\u3001\u673a\u5bc6\u6027\u548c\u4fe1\u4efb\u7ea7\u522b\u8fdb\u884c\u7ed3\u6784\u5316\u6807\u6ce8", "result": "\u8be5\u65b9\u6cd5\u65e8\u5728\u5c06LLM\u667a\u80fd\u4f53\u5b89\u5168\u4ece\u4e34\u65f6\u53ef\u9760\u6027\u4fee\u590d\u8f6c\u5411\u5177\u6709\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7684\u4e3b\u52a8\u9632\u62a4\uff0c\u51cf\u5c11\u5bf9\u7528\u6237\u786e\u8ba4\u7684\u4f9d\u8d56\uff0c\u4f7f\u81ea\u4e3b\u6027\u6210\u4e3a\u6709\u610f\u8bc6\u7684\u8bbe\u8ba1\u9009\u62e9", "conclusion": "\u901a\u8fc7STPA\u5b89\u5168\u5206\u6790\u548c\u589e\u5f3aMCP\u6846\u67b6\u7684\u7ed3\u5408\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\uff0c\u5b9e\u73b0\u4ece\u53ef\u9760\u6027\u5230\u5b89\u5168\u6027\u7684\u8f6c\u53d8\uff0c\u652f\u6301\u4f01\u4e1a\u7ea7\u5e94\u7528"}}
{"id": "2601.08528", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.08528", "abs": "https://arxiv.org/abs/2601.08528", "authors": ["Yuchen Peng", "Dingyu Yang", "Zhongle Xie", "Ji Sun", "Lidan Shou", "Ke Chen", "Gang Chen"], "title": "SVFusion: A CPU-GPU Co-Processing Architecture for Large-Scale Real-Time Vector Search", "comment": "This paper has been accepted for publication in PVLDB Volume 19(VLDB 2026)", "summary": "Approximate Nearest Neighbor Search (ANNS) underpins modern applications such as information retrieval and recommendation. With the rapid growth of vector data, efficient indexing for real-time vector search has become rudimentary. Existing CPU-based solutions support updates but suffer from low throughput, while GPU-accelerated systems deliver high performance but face challenges with dynamic updates and limited GPU memory, resulting in a critical performance gap for continuous, large-scale vector search requiring both accuracy and speed. In this paper, we present SVFusion, a GPU-CPU-disk collaborative framework for real-time vector search that bridges sophisticated GPU computation with online updates. SVFusion leverages a hierarchical vector index architecture that employs CPU-GPU co-processing, along with a workload-aware vector caching mechanism to maximize the efficiency of limited GPU memory. It further enhances performance through real-time coordination with CUDA multi-stream optimization and adaptive resource management, along with concurrency control that ensures data consistency under interleaved queries and updates. Empirical results demonstrate that SVFusion achieves significant improvements in query latency and throughput, exhibiting a 20.9x higher throughput on average and 1.3x to 50.7x lower latency compared to baseline methods, while maintaining high recall for large-scale datasets under various streaming workloads.", "AI": {"tldr": "SVFusion\u662f\u4e00\u4e2aGPU-CPU-\u78c1\u76d8\u534f\u540c\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u5411\u91cf\u641c\u7d22\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u7d22\u5f15\u67b6\u6784\u548c\u8d1f\u8f7d\u611f\u77e5\u7f13\u5b58\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u9ad8\u53ec\u56de\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u67e5\u8be2\u541e\u5410\u91cf\u548c\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709CPU\u65b9\u6848\u652f\u6301\u66f4\u65b0\u4f46\u541e\u5410\u91cf\u4f4e\uff0cGPU\u52a0\u901f\u7cfb\u7edf\u6027\u80fd\u9ad8\u4f46\u9762\u4e34\u52a8\u6001\u66f4\u65b0\u548cGPU\u5185\u5b58\u9650\u5236\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5927\u89c4\u6a21\u5b9e\u65f6\u5411\u91cf\u641c\u7d22\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u91c7\u7528GPU-CPU-\u78c1\u76d8\u534f\u540c\u6846\u67b6\uff0c\u5305\u542b\u5c42\u6b21\u5316\u5411\u91cf\u7d22\u5f15\u67b6\u6784\u3001CPU-GPU\u534f\u540c\u5904\u7406\u3001\u8d1f\u8f7d\u611f\u77e5\u5411\u91cf\u7f13\u5b58\u673a\u5236\u3001CUDA\u591a\u6d41\u4f18\u5316\u3001\u81ea\u9002\u5e94\u8d44\u6e90\u7ba1\u7406\u548c\u5e76\u53d1\u63a7\u5236\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u541e\u5410\u91cf\u63d0\u534720.9\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e1.3-50.7\u500d\uff0c\u540c\u65f6\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5404\u79cd\u6d41\u5f0f\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u4fdd\u6301\u9ad8\u53ec\u56de\u7387\u3002", "conclusion": "SVFusion\u6210\u529f\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5b9e\u65f6\u5411\u91cf\u641c\u7d22\u4e2d\u6027\u80fd\u4e0e\u66f4\u65b0\u80fd\u529b\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08036", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08036", "abs": "https://arxiv.org/abs/2601.08036", "authors": ["Bonan Kou", "Zijie Zhou", "Muhao Chen", "Tianyi Zhang"], "title": "Automating API Documentation from Crowdsourced Knowledge", "comment": "13 pages, 2 figures, Accepted to ICSE 2026", "summary": "API documentation is crucial for developers to learn and use APIs. However, it is known that many official API documents are obsolete and incomplete. To address this challenge, we propose a new approach called AutoDoc that generates API documents with API knowledge extracted from online discussions on Stack Overflow (SO). AutoDoc leverages a fine-tuned dense retrieval model to identify seven types of API knowledge from SO posts. Then, it uses GPT-4o to summarize the API knowledge in these posts into concise text. Meanwhile, we designed two specific components to handle LLM hallucination and redundancy in generated content. We evaluated AutoDoc against five comparison baselines on 48 APIs of different popularity levels. Our results indicate that the API documents generated by AutoDoc are up to 77.7% more accurate, 9.5% less duplicated, and contain 34.4% knowledge uncovered by the official documents. We also measured the sensitivity of AutoDoc to the choice of different LLMs. We found that while larger LLMs produce higher-quality API documents, AutoDoc enables smaller open-source models (e.g., Mistral-7B-v0.3) to achieve comparable results. Finally, we conducted a user study to evaluate the usefulness of the API documents generated by AutoDoc. All participants found API documents generated by AutoDoc to be more comprehensive, concise, and helpful than the comparison baselines. This highlights the feasibility of utilizing LLMs for API documentation with careful design to counter LLM hallucination and information redundancy.", "AI": {"tldr": "AutoDoc\uff1a\u5229\u7528Stack Overflow\u8ba8\u8bba\u548cGPT-4o\u81ea\u52a8\u751f\u6210\u66f4\u51c6\u786e\u3001\u66f4\u5168\u9762\u7684API\u6587\u6863\uff0c\u901a\u8fc7\u4e13\u95e8\u7ec4\u4ef6\u5904\u7406LLM\u5e7b\u89c9\u548c\u5197\u4f59\u95ee\u9898", "motivation": "\u5b98\u65b9API\u6587\u6863\u7ecf\u5e38\u5b58\u5728\u8fc7\u65f6\u548c\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u9700\u8981\u4ece\u5f00\u53d1\u8005\u793e\u533a\uff08\u5982Stack Overflow\uff09\u4e2d\u63d0\u53d6\u77e5\u8bc6\u6765\u8865\u5145\u548c\u5b8c\u5584\u6587\u6863", "method": "1. \u4f7f\u7528\u5fae\u8c03\u7684\u5bc6\u96c6\u68c0\u7d22\u6a21\u578b\u4eceStack Overflow\u5e16\u5b50\u4e2d\u8bc6\u522b7\u7c7bAPI\u77e5\u8bc6\uff1b2. \u7528GPT-4o\u603b\u7ed3\u77e5\u8bc6\u4e3a\u7b80\u6d01\u6587\u672c\uff1b3. \u8bbe\u8ba1\u4e13\u95e8\u7ec4\u4ef6\u5904\u7406LLM\u5e7b\u89c9\u548c\u5185\u5bb9\u5197\u4f59", "result": "\u572848\u4e2a\u4e0d\u540c\u6d41\u884c\u5ea6\u7684API\u4e0a\u8bc4\u4f30\uff0cAutoDoc\u751f\u6210\u7684\u6587\u6863\u6bd4\u57fa\u7ebf\u51c6\u786e\u5ea6\u9ad877.7%\uff0c\u91cd\u590d\u7387\u4f4e9.5%\uff0c\u5305\u542b34.4%\u5b98\u65b9\u6587\u6863\u672a\u8986\u76d6\u7684\u77e5\u8bc6\uff1b\u7528\u6237\u7814\u7a76\u663e\u793a\u6240\u6709\u53c2\u4e0e\u8005\u90fd\u8ba4\u4e3aAutoDoc\u6587\u6863\u66f4\u5168\u9762\u3001\u7b80\u6d01\u3001\u6709\u5e2e\u52a9", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u5bf9\u6297LLM\u5e7b\u89c9\u548c\u4fe1\u606f\u5197\u4f59\uff0c\u5229\u7528LLMs\u751f\u6210API\u6587\u6863\u662f\u53ef\u884c\u7684\uff0c\u5373\u4f7f\u8f83\u5c0f\u7684\u5f00\u6e90\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u53ef\u6bd4\u6548\u679c"}}
{"id": "2601.08045", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.08045", "abs": "https://arxiv.org/abs/2601.08045", "authors": ["Xinyi Zhou", "Zeinadsadat Saghi", "Sadra Sabouri", "Rahul Pandita", "Mollie McGuire", "Souti Chattopadhyay"], "title": "Cognitive Biases in LLM-Assisted Software Development", "comment": "13 pages, 6 figures, 7 tables", "summary": "The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8c03\u67e5\u4e86LLM\u8f85\u52a9\u5f00\u53d1\u4e2d\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u53d1\u73b048.8%\u7684\u7a0b\u5e8f\u5458\u884c\u4e3a\u5b58\u5728\u504f\u89c1\uff0c\u5176\u4e2d56.4%\u4e0e\u5f00\u53d1\u8005-LLM\u4ea4\u4e92\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u4e8615\u4e2a\u504f\u89c1\u5206\u7c7b\u548c\u7f13\u89e3\u5efa\u8bae\u3002", "motivation": "LLM\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6b63\u5728\u5c06\u7f16\u7a0b\u4ece\u89e3\u51b3\u65b9\u6848\u751f\u6210\u6d3b\u52a8\u8f6c\u53d8\u4e3a\u89e3\u51b3\u65b9\u6848\u8bc4\u4f30\u6d3b\u52a8\uff0c\u8fd9\u79cd\u8f6c\u53d8\u53ef\u80fd\u653e\u5927\u73b0\u6709\u51b3\u7b56\u504f\u89c1\u6216\u521b\u9020\u5168\u65b0\u504f\u89c1\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u8ba4\u77e5\u504f\u89c1\u5982\u4f55\u5728\u65b0\u5174AI\u534f\u4f5c\u5f00\u53d1\u4e2d\u663e\u73b0\u5e76\u5f71\u54cd\u51b3\u7b56\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff1a\u9996\u5148\u5bf914\u540d\u5b66\u751f\u548c\u4e13\u4e1a\u5f00\u53d1\u8005\u8fdb\u884c\u89c2\u5bdf\u7814\u7a76\uff0c\u7136\u540e\u5bf9\u53e6\u591622\u540d\u5f00\u53d1\u8005\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\u3002\u901a\u8fc7\u5b9a\u6027\u6bd4\u8f83\u4f20\u7edf\u975eLLM\u5de5\u4f5c\u6d41\u4e2d\u5f71\u54cd\u5f00\u53d1\u8005\u7684\u504f\u89c1\u7c7b\u522b\uff0c\u7cfb\u7edf\u5206\u6790\u4e8690\u4e2a\u7279\u5b9a\u4e8e\u5f00\u53d1\u8005-LLM\u4ea4\u4e92\u7684\u8ba4\u77e5\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b048.8%\u7684\u7a0b\u5e8f\u5458\u884c\u4e3a\u5b58\u5728\u504f\u89c1\uff0c\u5176\u4e2d56.4%\u7684\u504f\u89c1\u884c\u4e3a\u4e0e\u5f00\u53d1\u8005-LLM\u4ea4\u4e92\u76f8\u5173\u3002LLM\u76f8\u5173\u884c\u4e3a\u66f4\u53ef\u80fd\u4e0e\u65b0\u578b\u504f\u89c1\u76f8\u5173\u3002\u7814\u7a76\u5f00\u53d1\u4e86\u7531\u8ba4\u77e5\u5fc3\u7406\u5b66\u5bb6\u9a8c\u8bc1\u768415\u4e2a\u504f\u89c1\u5206\u7c7b\u6cd5\u3002", "conclusion": "LLM\u8f85\u52a9\u5f00\u53d1\u4e2d\u5b58\u5728\u663e\u8457\u7684\u8ba4\u77e5\u504f\u89c1\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u5de5\u5177\u548c\u5b9e\u8df5\u6765\u7f13\u89e3\u3002\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u548cLLM\u5de5\u5177\u6784\u5efa\u8005\u63d0\u4f9b\u4e86\u5177\u4f53\u5efa\u8bae\uff0c\u4ee5\u6539\u5584\u4eba\u673a\u534f\u4f5c\u7f16\u7a0b\u4e2d\u7684\u51b3\u7b56\u8d28\u91cf\u3002"}}
{"id": "2601.08609", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08609", "abs": "https://arxiv.org/abs/2601.08609", "authors": ["Qurban Ali", "Andrea Stocco", "Leonardo Mariani", "Oliviero Riganelli"], "title": "Coverage-Guided Road Selection and Prioritization for Efficient Testing in Autonomous Driving Systems", "comment": "The IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER) 2026", "summary": "Autonomous Driving Assistance Systems (ADAS) rely on extensive testing to ensure safety and reliability, yet road scenario datasets often contain redundant cases that slow down the testing process without improving fault detection. To address this issue, we present a novel test prioritization framework that reduces redundancy while preserving geometric and behavioral diversity. Road scenarios are clustered based on geometric and dynamic features of the ADAS driving behavior, from which representative cases are selected to guarantee coverage. Roads are finally prioritized based on geometric complexity, driving difficulty, and historical failures, ensuring that the most critical and challenging tests are executed first. We evaluate our framework on the OPENCAT dataset and the Udacity self-driving car simulator using two ADAS models. On average, our approach achieves an 89% reduction in test suite size while retaining an average of 79% of failed road scenarios. The prioritization strategy improves early failure detection by up to 95x compared to random baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u548c\u884c\u4e3a\u591a\u6837\u6027\u7684ADAS\u6d4b\u8bd5\u4f18\u5148\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u4ee3\u8868\u6027\u9009\u62e9\u51cf\u5c11\u5197\u4f59\u6d4b\u8bd5\uff0c\u5728\u4fdd\u630179%\u5931\u8d25\u573a\u666f\u7684\u540c\u65f6\u51cf\u5c1189%\u6d4b\u8bd5\u89c4\u6a21\uff0c\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u63d0\u534795\u500d\u3002", "motivation": "ADAS\u9700\u8981\u5927\u91cf\u6d4b\u8bd5\u786e\u4fdd\u5b89\u5168\uff0c\u4f46\u73b0\u6709\u9053\u8def\u573a\u666f\u6570\u636e\u96c6\u5b58\u5728\u5927\u91cf\u5197\u4f59\u6848\u4f8b\uff0c\u8fd9\u4e9b\u5197\u4f59\u4f1a\u51cf\u6162\u6d4b\u8bd5\u8fc7\u7a0b\u800c\u4e0d\u63d0\u9ad8\u6545\u969c\u68c0\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u51cf\u5c11\u5197\u4f59\u540c\u65f6\u4fdd\u6301\u6d4b\u8bd5\u591a\u6837\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u6d4b\u8bd5\u4f18\u5148\u7ea7\u6846\u67b6\uff1a1) \u57fa\u4e8eADAS\u9a7e\u9a76\u884c\u4e3a\u7684\u51e0\u4f55\u548c\u52a8\u6001\u7279\u5f81\u5bf9\u9053\u8def\u573a\u666f\u8fdb\u884c\u805a\u7c7b\uff1b2) \u4ece\u6bcf\u4e2a\u805a\u7c7b\u4e2d\u9009\u62e9\u4ee3\u8868\u6027\u6848\u4f8b\u4ee5\u4fdd\u8bc1\u8986\u76d6\uff1b3) \u57fa\u4e8e\u51e0\u4f55\u590d\u6742\u5ea6\u3001\u9a7e\u9a76\u96be\u5ea6\u548c\u5386\u53f2\u5931\u8d25\u7387\u5bf9\u9053\u8def\u8fdb\u884c\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u786e\u4fdd\u6700\u5173\u952e\u548c\u6700\u5177\u6311\u6218\u6027\u7684\u6d4b\u8bd5\u4f18\u5148\u6267\u884c\u3002", "result": "\u5728OPENCAT\u6570\u636e\u96c6\u548cUdacity\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u5668\u4e0a\u4f7f\u7528\u4e24\u4e2aADAS\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff1a\u5e73\u5747\u51cf\u5c1189%\u6d4b\u8bd5\u5957\u4ef6\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u755979%\u7684\u5931\u8d25\u9053\u8def\u573a\u666f\uff1b\u4f18\u5148\u7ea7\u7b56\u7565\u76f8\u6bd4\u968f\u673a\u57fa\u7ebf\u5c06\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u63d0\u5347\u9ad8\u8fbe95\u500d\u3002", "conclusion": "\u8be5\u6d4b\u8bd5\u4f18\u5148\u7ea7\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11ADAS\u6d4b\u8bd5\u5197\u4f59\uff0c\u663e\u8457\u63d0\u9ad8\u6d4b\u8bd5\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6d4b\u8bd5\u6210\u672c\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.08691", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08691", "abs": "https://arxiv.org/abs/2601.08691", "authors": ["Shaznin Sultana", "Sadia Afreen", "Nasir U. Eisty"], "title": "LLMs in Code Vulnerability Analysis: A Proof of Concept", "comment": "Accepted for publication at the Fourth International Workshop on Software Vulnerability Management (SVM 2026) co-located with Intenational Conference in Software Engineering (ICSE 2026)", "summary": "Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e865\u5bf9\u4ee3\u7801\u4e13\u7528\u548c\u901a\u7528LLM\u5728C/C++\u6f0f\u6d1e\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5fae\u8c03\u4f18\u4e8e\u96f6/\u5c11\u6837\u672c\u65b9\u6cd5\uff0c\u4ee3\u7801\u4e13\u7528\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u5b89\u5168\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u4ee3\u7801\u5e93\u7684\u89c4\u6a21\u548c\u590d\u6742\u6027\uff0c\u9700\u8981\u667a\u80fd\u81ea\u52a8\u5316\u6765\u66f4\u9ad8\u6548\u51c6\u786e\u5730\u68c0\u6d4b\u3001\u8bc4\u4f30\u548c\u4fee\u590d\u6f0f\u6d1e\u3002\u672c\u7814\u7a76\u63a2\u7d22\u5229\u7528\u4ee3\u7801\u4e13\u7528\u548c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6765\u81ea\u52a8\u5316\u5173\u952e\u8f6f\u4ef6\u5b89\u5168\u4efb\u52a1\u3002", "method": "\u8bc4\u4f30\u4e865\u5bf9\u8fd1\u671f\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u4ee3\u7801\u4e13\u7528\u548c\u901a\u7528\u5f00\u6e90\u6a21\u578b\uff09\uff0c\u5728Big-Vul\u548cVul-Repair\u4e24\u4e2aC/C++\u6f0f\u6d1e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002\u6bd4\u8f83\u4e86\u5fae\u8c03\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u3002", "result": "\u5fae\u8c03\u5728\u6240\u6709\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u90fd\u4f18\u4e8e\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u65b9\u6cd5\u3002\u4ee3\u7801\u4e13\u7528\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u7684\u96f6/\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u66f4\u4f73\uff0c\u800c\u901a\u7528\u6a21\u578b\u6548\u679c\u76f8\u8fd1\u3002\u73b0\u6709\u8bc4\u4f30\u6307\u6807\uff08CodeBLEU\u3001CodeBERTScore\u3001BLEU\u3001ChrF\uff09\u5728\u8861\u91cf\u4fee\u590d\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63a2\u7d22\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6539\u8fdb\u6f0f\u6d1e\u5206\u6790\u548c\u4fee\u590d\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u8f6f\u4ef6\u5b89\u5168\u793e\u533a\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2601.08706", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08706", "abs": "https://arxiv.org/abs/2601.08706", "authors": ["Maria Teresa Rossi", "Leonardo Mariani", "Oliviero Riganelli", "Giuseppe Filomento", "Danilo Giannone", "Paolo Gavazzo"], "title": "\"Where is My Troubleshooting Procedure?\": Studying the Potential of RAG in Assisting Failure Resolution of Large Cyber-Physical System", "comment": "This paper has been accepted at the Software Engineering in Practice track of the 48th International Conference on Software Engineering (ICSE 2026)", "summary": "In today's complex industrial environments, operators must often navigate through extensive technical manuals to identify troubleshooting procedures that may help react to some observed failure symptoms. These manuals, written in natural language, describe many steps in detail. Unfortunately, the number, magnitude, and articulation of these descriptions can significantly slow down and complicate the retrieval of the correct procedure during critical incidents. Interestingly, Retrieval Augmented Generation (RAG) enables the development of tools based on conversational interfaces that can assist operators in their retrieval tasks, improving their capability to respond to incidents. This paper presents the results of a set of experiments that derive from the analysis of the troubleshooting procedures available in Fincantieri, a large international company developing complex naval cyber-physical systems. Results show that RAG can assist operators in reacting promptly to failure symptoms, although specific measures have to be taken into consideration to cross-validate recommendations before actuating them.", "AI": {"tldr": "RAG\u7cfb\u7edf\u80fd\u5e2e\u52a9\u64cd\u4f5c\u5458\u5728\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e2d\u5feb\u901f\u68c0\u7d22\u6545\u969c\u6392\u9664\u7a0b\u5e8f\uff0c\u4f46\u9700\u8981\u4ea4\u53c9\u9a8c\u8bc1\u5efa\u8bae\u540e\u518d\u6267\u884c", "motivation": "\u5de5\u4e1a\u73af\u5883\u4e2d\u64cd\u4f5c\u5458\u9700\u8981\u4ece\u5927\u91cf\u6280\u672f\u624b\u518c\u4e2d\u68c0\u7d22\u6545\u969c\u6392\u9664\u7a0b\u5e8f\uff0c\u4f46\u624b\u518c\u7684\u590d\u6742\u6027\u548c\u89c4\u6a21\u4f1a\u663e\u8457\u51cf\u6162\u5173\u952e\u4e8b\u6545\u671f\u95f4\u7684\u68c0\u7d22\u901f\u5ea6\uff0c\u5f71\u54cd\u5e94\u6025\u54cd\u5e94", "method": "\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5f00\u53d1\u5bf9\u8bdd\u754c\u9762\u5de5\u5177\uff0c\u5206\u6790Fincantieri\u516c\u53f8\u7684\u6d77\u519b\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u6545\u969c\u6392\u9664\u7a0b\u5e8f\uff0c\u8fdb\u884c\u4e00\u7cfb\u5217\u5b9e\u9a8c\u8bc4\u4f30", "result": "RAG\u80fd\u591f\u5e2e\u52a9\u64cd\u4f5c\u5458\u5feb\u901f\u54cd\u5e94\u6545\u969c\u75c7\u72b6\uff0c\u4f46\u9700\u8981\u5728\u6267\u884c\u5efa\u8bae\u524d\u91c7\u53d6\u7279\u5b9a\u63aa\u65bd\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1", "conclusion": "RAG\u6280\u672f\u53ef\u4ee5\u6709\u6548\u8f85\u52a9\u5de5\u4e1a\u64cd\u4f5c\u5458\u68c0\u7d22\u6545\u969c\u6392\u9664\u7a0b\u5e8f\uff0c\u63d0\u9ad8\u5e94\u6025\u54cd\u5e94\u80fd\u529b\uff0c\u4f46\u9700\u8981\u5efa\u7acb\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u5efa\u8bae\u7684\u53ef\u9760\u6027"}}
{"id": "2601.08729", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.08729", "abs": "https://arxiv.org/abs/2601.08729", "authors": ["Jinhan Kim", "Nargiz Humbatova", "Gunel Jahangirova", "Shin Yoo", "Paolo Tonella"], "title": "Revisiting \"Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion\": A Critical Review and Implications on DNN Coverage Testing", "comment": "ICSE 2026", "summary": "We present a critical review of Neural Coverage (NLC), a state-of-the-art DNN coverage criterion by Yuan et al. at ICSE 2023. While NLC proposes to satisfy eight design requirements and demonstrates strong empirical performance, we question some of their theoretical and empirical assumptions. We observe that NLC deviates from core principles of coverage criteria, such as monotonicity and test suite order independence, and could more fully account for key properties of the covariance matrix. Additionally, we note threats to the validity of the empirical study, related to the ground truth ordering of test suites. Through our empirical validation, we substantiate our claims and propose improvements for future DNN coverage metrics. Finally, we conclude by discussing the implications of these insights.", "AI": {"tldr": "\u5bf9ICSE 2023\u63d0\u51fa\u7684Neural Coverage (NLC)\u8fdb\u884c\u6279\u5224\u6027\u8bc4\u5ba1\uff0c\u6307\u51fa\u5176\u5728\u7406\u8bba\u5047\u8bbe\u3001\u8bbe\u8ba1\u539f\u5219\u548c\u5b9e\u8bc1\u6709\u6548\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u867d\u7136NLC\u58f0\u79f0\u6ee1\u8db3\u516b\u4e2a\u8bbe\u8ba1\u9700\u6c42\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5b9e\u8bc1\u6027\u80fd\uff0c\u4f46\u4f5c\u8005\u8d28\u7591\u5176\u7406\u8bba\u548c\u5b9e\u8bc1\u5047\u8bbe\uff0c\u8ba4\u4e3aNLC\u504f\u79bb\u4e86\u8986\u76d6\u7387\u51c6\u5219\u7684\u6838\u5fc3\u539f\u5219\uff0c\u4e14\u5b9e\u8bc1\u7814\u7a76\u5b58\u5728\u6709\u6548\u6027\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff1a1) \u5206\u6790NLC\u662f\u5426\u6ee1\u8db3\u8986\u76d6\u7387\u51c6\u5219\u7684\u6838\u5fc3\u539f\u5219\uff08\u5982\u5355\u8c03\u6027\u548c\u6d4b\u8bd5\u5957\u4ef6\u987a\u5e8f\u72ec\u7acb\u6027\uff09\uff1b2) \u8bc4\u4f30NLC\u5bf9\u534f\u65b9\u5dee\u77e9\u9635\u5173\u952e\u5c5e\u6027\u7684\u5904\u7406\uff1b3) \u8bc6\u522b\u5b9e\u8bc1\u7814\u7a76\u4e2d\u7684\u6709\u6548\u6027\u5a01\u80c1\uff1b4) \u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u652f\u6301\u8bba\u70b9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) NLC\u504f\u79bb\u4e86\u8986\u76d6\u7387\u51c6\u5219\u7684\u6838\u5fc3\u539f\u5219\uff1b2) \u672a\u80fd\u5145\u5206\u8003\u8651\u534f\u65b9\u5dee\u77e9\u9635\u7684\u5173\u952e\u5c5e\u6027\uff1b3) \u5b9e\u8bc1\u7814\u7a76\u5b58\u5728\u6709\u6548\u6027\u5a01\u80c1\uff08\u7279\u522b\u662f\u5173\u4e8e\u6d4b\u8bd5\u5957\u4ef6\u771f\u5b9e\u6392\u5e8f\u7684\u95ee\u9898\uff09\uff1b4) \u5b9e\u8bc1\u9a8c\u8bc1\u652f\u6301\u4e86\u8fd9\u4e9b\u8bba\u70b9\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u5bf9NLC\u7684\u6279\u5224\u6027\u89c1\u89e3\uff0c\u4e3a\u672a\u6765DNN\u8986\u76d6\u7387\u6307\u6807\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u5efa\u8bae\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u53d1\u73b0\u5bf9DNN\u6d4b\u8bd5\u9886\u57df\u7684\u5f71\u54cd\u3002"}}
{"id": "2601.08734", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08734", "abs": "https://arxiv.org/abs/2601.08734", "authors": ["Prithwish Jana", "Sam Davidson", "Bhavana Bhasker", "Andrey Kan", "Anoop Deoras", "Laurent Callot"], "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback", "comment": "The paper has been published at the 2026 IEEE/ACM 48th International Conference on Software Engineering (ICSE 2026), Rio de Janeiro, Brazil, April 12-18, 2026", "summary": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.", "AI": {"tldr": "TerraFormer\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8eIaC\u751f\u6210\u548c\u53d8\u5f02\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6b63\u786e\u6027\u5e76\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002", "motivation": "\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801(IaC)\u5177\u6709\u6311\u6218\u6027\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u914d\u7f6e\u65f6\u7ecf\u5e38\u51fa\u9519\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faTerraFormer\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u4e0e\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5de5\u5177\u63d0\u4f9b\u8bed\u6cd5\u3001\u53ef\u90e8\u7f72\u6027\u548c\u7b56\u7565\u5408\u89c4\u6027\u53cd\u9988\u3002\u901a\u8fc7\u591a\u9636\u6bb5\u9a8c\u8bc1\u548c\u8fed\u4ee3LLM\u81ea\u6821\u6b63\u6784\u5efa\u4e86\u4e24\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002", "result": "\u5728IaC-Eval\u4e0a\u6bd4\u57fa\u7840LLM\u63d0\u534715.94%\u6b63\u786e\u6027\uff0c\u5728TF-Gen\u548cTF-Mutn\u6d4b\u8bd5\u96c6\u4e0a\u5206\u522b\u63d0\u534711.65%\u548c19.60%\u3002\u8d85\u8d8a\u5305\u62ecSonnet 3.7\u3001DeepSeek-R1\u3001GPT-4.1\u7b49\u7ea650\u500d\u5927\u7684\u6a21\u578b\uff0c\u5728IaC-Eval\u6392\u540d\u7b2c\u4e09\uff0c\u5728\u6700\u4f73\u5b9e\u8df5\u548c\u5b89\u5168\u5408\u89c4\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "TerraFormer\u6846\u67b6\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86IaC\u81ea\u52a8\u5316\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u5927\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u9a8c\u8bc1\u5668\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.08773", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.08773", "abs": "https://arxiv.org/abs/2601.08773", "authors": ["Manideep Reddy Chinthareddy"], "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs", "comment": "46 pages, 2 figures", "summary": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.\n  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cdRAG\u68c0\u7d22\u7ba1\u9053\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\uff1a\u5411\u91cf\u68c0\u7d22\u3001LLM\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u548cAST\u786e\u5b9a\u6027\u77e5\u8bc6\u56fe\u8c31\uff0c\u53d1\u73b0AST\u65b9\u6cd5\u5728\u6210\u672c\u3001\u8986\u76d6\u7387\u548c\u6b63\u786e\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u7684\u5411\u91cf\u76f8\u4f3c\u6027\u68c0\u7d22\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u80fd\u6355\u6349\u4e3b\u9898\u76f8\u4f3c\u6027\uff0c\u4f46\u5728\u591a\u8df3\u67b6\u6784\u63a8\u7406\uff08\u5982\u63a7\u5236\u5668-\u670d\u52a1-\u4ed3\u5e93\u94fe\u3001\u63a5\u53e3\u9a71\u52a8\u8fde\u63a5\u3001\u7ee7\u627f\u5173\u7cfb\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "\u5728Java\u4ee3\u7801\u5e93\uff08Shopizer\u3001ThingsBoard\u3001OpenMRS Core\uff09\u4e0a\u57fa\u51c6\u6d4b\u8bd5\u4e09\u79cd\u68c0\u7d22\u7ba1\u9053\uff1aA) \u7eaf\u5411\u91cf\u68c0\u7d22\uff08No-Graph RAG\uff09\uff1bB) LLM\u751f\u6210\u77e5\u8bc6\u56fe\u8c31RAG\uff08LLM-KB\uff09\uff1bC) \u786e\u5b9a\u6027AST\u6d3e\u751f\u77e5\u8bc6\u56fe\u8c31RAG\uff08DKB\uff09\uff0c\u4f7f\u7528Tree-sitter\u548c\u53cc\u5411\u904d\u5386\u6784\u5efa\u3002\u6bcf\u4e2a\u4ed3\u5e93\u4f7f\u752815\u4e2a\u67b6\u6784\u548c\u4ee3\u7801\u8ffd\u8e2a\u67e5\u8be2\uff0c\u6d4b\u91cf\u7d22\u5f15\u65f6\u95f4\u3001\u67e5\u8be2\u5ef6\u8fdf\u3001\u8bed\u6599\u8986\u76d6\u7387\u3001\u6210\u672c\u548c\u7b54\u6848\u6b63\u786e\u6027\u3002", "result": "DKB\u5728\u51e0\u79d2\u5185\u6784\u5efa\u56fe\u8c31\uff0c\u800cLLM-KB\u9700\u8981\u66f4\u957f\u65f6\u95f4\uff1bLLM-KB\u5b58\u5728\u7d22\u5f15\u4e0d\u5b8c\u6574\u95ee\u9898\uff08Shopizer\u4e2d377\u4e2a\u6587\u4ef6\u88ab\u8df3\u8fc7\uff09\uff1bDKB\u7aef\u5230\u7aef\u6210\u672c\u76f8\u5bf9\u8f83\u4f4e\uff0cLLM-KB\u6210\u672c\u968f\u4ed3\u5e93\u89c4\u6a21\u589e\u52a0\u800c\u663e\u8457\u4e0a\u5347\uff1b\u67e5\u8be2\u5ef6\u8fdf\u65b9\u9762\uff0cNo-Graph\u548cDKB\u76f8\u4f3c\uff0cLLM-KB\u8f83\u6162\u4e14\u4e0d\u7a33\u5b9a\uff1b\u5728Shopizer\u95ee\u9898\u96c6\u4e0a\uff0cDKB\u6b63\u786e\u7387\u6700\u9ad8\uff0cLLM-KB\u6b21\u4e4b\uff0c\u7eaf\u5411\u91cf\u68c0\u7d22\u5728\u67b6\u6784\u67e5\u8be2\u4e0a\u8868\u73b0\u6700\u5dee\u4e14\u5e7b\u89c9\u98ce\u9669\u6700\u9ad8\u3002", "conclusion": "\u786e\u5b9a\u6027AST\u6d3e\u751f\u56fe\u8c31\u76f8\u6bd4LLM\u63d0\u53d6\u56fe\u8c31\uff0c\u5728\u663e\u8457\u964d\u4f4e\u7d22\u5f15\u6210\u672c\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8986\u76d6\u7387\u548c\u591a\u8df3\u63a8\u7406\u57fa\u7840\uff0c\u662f\u8f6f\u4ef6\u5de5\u7a0bRAG\u4e2d\u66f4\u6709\u6548\u7684\u68c0\u7d22\u65b9\u6cd5\u3002"}}
{"id": "2601.08806", "categories": ["cs.SE", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.08806", "abs": "https://arxiv.org/abs/2601.08806", "authors": ["Abhi Kottamasu", "Akul Datta", "Aakash Barthwal", "Chirag Mahapatra", "Ajay Arun", "Adarsh Hiremath", "Brendan Foody", "Bertie Vidgen"], "title": "APEX-SWE", "comment": null, "summary": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).", "AI": {"tldr": "APEX-SWE\u662f\u4e00\u4e2a\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u80fd\u5426\u6267\u884c\u5177\u6709\u7ecf\u6d4e\u4ef7\u503c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u7684\u57fa\u51c6\uff0c\u5305\u542b\u96c6\u6210\u4efb\u52a1\u548c\u53ef\u89c2\u6d4b\u6027\u4efb\u52a1\u4e24\u79cd\u65b0\u4efb\u52a1\u7c7b\u578b\uff0cGemini 3 Pro\u8868\u73b0\u6700\u4f73\uff0825% Pass@1\uff09\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u72ed\u7a84\u3001\u5b9a\u4e49\u660e\u786e\u7684\u4efb\u52a1\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u3002\u9700\u8981\u521b\u5efa\u80fd\u591f\u8bc4\u4f30AI\u6a21\u578b\u5728\u5b9e\u9645\u8f6f\u4ef6\u5de5\u7a0b\u73af\u5883\u4e2d\u6267\u884c\u7ecf\u6d4e\u4ef7\u503c\u5de5\u4f5c\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faAPEX-SWE\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u79cd\u4efb\u52a1\u7c7b\u578b\uff1a1) \u96c6\u6210\u4efb\u52a1\uff08100\u4e2a\uff09\uff0c\u8981\u6c42\u8de8\u5f02\u6784\u4e91\u539f\u8bed\u3001\u4e1a\u52a1\u5e94\u7528\u548c\u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801\u670d\u52a1\u6784\u5efa\u7aef\u5230\u7aef\u7cfb\u7edf\uff1b2) \u53ef\u89c2\u6d4b\u6027\u4efb\u52a1\uff08100\u4e2a\uff09\uff0c\u8981\u6c42\u4f7f\u7528\u65e5\u5fd7\u3001\u4eea\u8868\u677f\u7b49\u9065\u6d4b\u4fe1\u53f7\u4ee5\u53ca\u975e\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u8c03\u8bd5\u751f\u4ea7\u6545\u969c\u3002\u8bc4\u4f30\u4e868\u4e2a\u524d\u6cbf\u6a21\u578b\u3002", "result": "Gemini 3 Pro\uff08Thinking = High\uff09\u8868\u73b0\u6700\u4f73\uff0cPass@1\u5f97\u5206\u4e3a25%\u3002\u5206\u6790\u8868\u660e\uff0c\u5f3a\u6027\u80fd\u4e3b\u8981\u7531\u8ba4\u77e5\u63a8\u7406\u80fd\u529b\u9a71\u52a8\uff0c\u5373\u533a\u5206\u5047\u8bbe\u4e0e\u5df2\u9a8c\u8bc1\u4e8b\u5b9e\u7684\u80fd\u529b\uff0c\u7ed3\u5408\u5728\u884c\u52a8\u524d\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u7684\u4e3b\u52a8\u6027\u3002", "conclusion": "APEX-SWE\u586b\u8865\u4e86\u73b0\u6709AI\u8bc4\u4f30\u5728\u771f\u5b9e\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u4f5c\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u5f00\u6e90\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u5f00\u53d1\u96c6\uff0850\u4e2a\u4efb\u52a1\uff09\uff0c\u8ba4\u77e5\u63a8\u7406\u80fd\u529b\u662fAI\u6a21\u578b\u5728\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u6210\u529f\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
