<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 15]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.SE](#cs.SE) [Total: 22]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 提出一个结合高性能集群计算、智能算法和区块链的新框架，通过改进的PoW共识、动态信任评级和统计抽签系统，实现高效、环保、包容的智能算法开发平台。


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算和智能算法需要大量计算资源，导致高能耗，且排除了计算能力较弱的系统。需要一种更包容、可扩展且环保的智能算法开发和实施方法。

Method: 1. 提出新框架：融合高性能集群计算、智能算法和区块链基础设施；2. 改进的PoW共识机制：将计算努力直接与区块生产奖励挂钩；3. 动态信任评级：基于准确区块验证记录调整，决定节点被选为区块生成者的概率；4. 统计抽签系统：为计算能力较弱的节点提供参与区块创建的机会。

Result: 该框架实现了高效资源利用和广泛参与，创建了基于贡献质量的奖励系统，使不同计算能力的节点都能参与，促进了包容性和可持续性。

Conclusion: 提出的框架为智能算法开发提供了高效、环保、包容的解决方案，通过区块链技术整合高性能计算资源，改进了共识机制和参与机制，平衡了性能与可持续性需求。

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [2] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 论文研究了异步消息传递模型(AMP_f)与Heard-Of模型(HO_f)在分布式计算中的等价关系，发现对于无颜色任务在n>2f时等价，对于颜色任务仅在f=1时等价，揭示了基于轮次的抽象在异步计算中的表达能力界限。


<details>
  <summary>Details</summary>
Motivation: 重新审视两个基础分布式计算模型之间的关系：具有最多f个崩溃故障的异步消息传递模型(AMP_f)和具有最多f个消息遗漏的Heard-Of模型(HO_f)。理解这两个模型在任务可解性方面的等价性，以及基于轮次的抽象在异步计算中的表达能力。

Method: 通过双向模拟方法，在AMP_f和HO_f之间建立等价关系证明，使用一个中间模型来捕捉"静默进程"的概念。对于n>2f的情况，证明两个模型在无颜色任务上的等价性；对于颜色任务，证明仅在f=1时等价。方法扩展到针对非自适应对手的随机化协议。

Result: 1. 对于n>2f，AMP_f和HO_f在无颜色任务的可解性方面是等价的；2. 对于颜色任务，等价性仅在f=1时成立（且n>2）；3. 对于更大的f，由于HO_f中存在静默进程可能导致决策不一致，两个模型分离；4. 结果扩展到随机化协议，表明规范轮次的表达能力限制是结构性的而非概率性的。

Conclusion: 研究精确界定了基于轮次的抽象在何处能够捕捉异步计算，以及在何处不能。对于无颜色任务，当n>2f时，异步消息传递模型与Heard-Of模型等价；对于颜色任务，仅在f=1时等价。这表明规范轮次的表达能力限制是结构性的，为分布式计算模型的理论理解提供了重要洞见。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [3] [DisCEdge: Distributed Context Management for Large Language Models at the Edge](https://arxiv.org/abs/2511.22599)
*Mohammadreza Malekabbasi,Minghe Wang,David Bermbach*

Main category: cs.DC

TL;DR: DisCEdge是一个分布式上下文管理系统，将用户上下文以token形式存储和复制在边缘节点，相比原始文本系统提升响应时间14.46%，相比客户端管理减少请求大小90%


<details>
  <summary>Details</summary>
Motivation: 在边缘部署LLM服务有利于延迟敏感和隐私保护应用，但LLM的无状态特性使得跨地理分布式边缘节点管理用户上下文（如会话、偏好）具有挑战性。现有解决方案（如客户端上下文存储）通常引入网络延迟和带宽开销，削弱了边缘部署的优势。

Method: 提出DisCEdge分布式上下文管理系统，以token化形式而非原始文本存储和复制用户上下文。通过将上下文维护为token序列，避免冗余计算并实现高效数据复制。在真实边缘环境中使用商用硬件实现和评估开源原型。

Result: DisCEdge相比原始文本系统将中位响应时间提升高达14.46%，中位节点间同步开销降低高达15%。相比客户端上下文管理，将客户端请求大小中位减少90%，同时保证数据一致性。

Conclusion: DisCEdge通过token化上下文管理有效解决了边缘LLM服务中的上下文管理挑战，显著提升了性能并减少了开销，为延迟敏感和隐私保护的边缘LLM应用提供了实用解决方案。

Abstract: Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.
  We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.

</details>


### [4] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出了一种基于延迟约束的解耦架构，将集群资源分为延迟严格和延迟宽松两个池，通过瓶颈调度器和快速抢占机制，在保证在线服务SLO的同时，将离线吞吐量提升高达3倍。


<details>
  <summary>Details</summary>
Motivation: LLM部署中在线服务（延迟敏感）和离线任务（成本敏感）共置可提高资源利用率，但在Prefill/Decode解耦系统中，请求混合的波动会导致严重的负载不平衡，现有动态调整技术无法应对在线服务的突发流量模式。

Method: 1) 延迟约束解耦架构：根据任务延迟要求将集群资源分为延迟严格池和延迟宽松池，灵活放置离线解码任务；2) 基于瓶颈的调度器：使用Roofline性能模型指导的性能瓶颈调度；3) 快速抢占机制：严格强制执行在线请求的服务级别目标(SLO)。

Result: 在真实世界跟踪上的实验表明，与现有离线系统方法相比，该方法在保持在线请求SLO的同时，将离线吞吐量提升高达3倍。

Conclusion: 提出的延迟约束解耦架构和配套调度机制有效解决了Prefill/Decode解耦系统中的负载不平衡问题，实现了在线服务性能保证和离线任务吞吐量的显著提升。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [5] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: Clock2Q+是针对元数据缓存设计的缓存替换算法，通过在小FIFO队列中引入相关窗口来避免将相关引用误判为热点块，显著提升缓存性能。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存存在固有的相关引用特性，即使对应的数据访问不包含相关引用。这些相关引用会被现有缓存替换算法误判为热点块，从而降低缓存效率。

Method: Clock2Q+采用类似S3-FIFO的三队列结构，但在小FIFO队列中引入相关窗口，该窗口内的块不设置引用位，从而避免将相关引用误分类为热点块。

Result: 在元数据跟踪中，Clock2Q+相比性能第二的S3-FIFO算法，未命中率降低高达28.5%。在数据跟踪中也优于现有最先进的缓存替换算法。

Conclusion: Clock2Q+是针对元数据缓存的高效缓存替换算法，具有低CPU开销、低内存开销、多CPU扩展性好、易于调优和实现等特性，已成功应用于VMware by Broadcom的vSAN和VDFS存储产品。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [6] [ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast](https://arxiv.org/abs/2511.21969)
*Matteo Bjornsson,Taylor Hardin,Taylor Heinecke,Marcin Furtak,David L. Millman,Mike P. Wittie*

Main category: cs.DC

TL;DR: ZipperChain是一种无需分布式共识的区块链技术，通过专用服务流水线在少数节点上构建区块，实现接近网络线速的交易吞吐量和500毫秒的最终确定性。


<details>
  <summary>Details</summary>
Motivation: 传统分布式账本技术依赖分布式共识机制，存在网络通信性能限制。ZipperChain旨在解决这一性能瓶颈，同时保证交易数据的不可篡改性、一致性和可用性。

Method: ZipperChain不依赖分布式共识，而是将信任从广泛使用的第三方服务转移到ZipperChain的正确性保证上。通过部署在少数节点上的专用服务流水线构建区块，这些节点通过快速数据中心网络连接。

Result: ZipperChain交易吞吐量接近网络线速，区块最终确定性约为500毫秒。由于基础设施集中创建区块，无需原生代币激励验证者社区。

Conclusion: ZipperChain提供了一种替代传统分布式共识的区块链架构，通过集中式区块构建实现了高性能交易处理，同时保持了区块链的核心特性（不可篡改性、一致性和可用性）。

Abstract: Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participating nodes. BLOCKY ZipperChain guarantees immutability, agreement, and availability of transaction data, but without relying on distributed consensus. Instead, its construction process transfers trust from widely-used, third-party services onto ZipperChains's correctness guarantees. ZipperChain blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by a fast data center network. As a result, ZipperChain transaction throughput approaches network line speeds and block finality is on the order of 500 ms. Finally, ZipperChain infrastructure creates blocks centrally and so does not need a native token to incentivize a community of verifiers.

</details>


### [7] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 该研究比较了多语言复制数据系统中RDL集成的两种策略：FFI和CDF，发现CDF在软件质量、延迟、内存消耗和吞吐量方面具有优势，并开发了可扩展的CDF-based RDL。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统需要在多个执行站点复制数据，业务需求和资源限制常导致不同语言混合使用。现有复制数据库(RDLs)通常只支持单一语言，在多语言环境中集成RDLs需要特殊代码，但其软件质量和性能特性缺乏研究。

Method: 通过实证研究比较两种RDL集成策略：外部函数接口(FFI)和通用数据格式(CDF)，测量并比较它们的软件指标和性能，评估其适用性。

Result: 研究结果显示，采用CDF进行跨语言交互在软件质量、延迟、内存消耗和吞吐量方面具有优势。通过创建支持编译、解释和管理语言的CDF-based RDL，并增强其插件扩展性，验证了这些发现。

Conclusion: 随着现代分布式系统使用多种语言，本研究为多语言复制数据系统中RDL的设计提供了新的见解，CDF策略展现出更好的综合性能。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [8] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: PAT提出了一种前缀感知的注意力核实现，通过打包-前向-合并范式，利用请求间的共享前缀减少KV缓存重复加载，显著提升LLM解码注意力效率。


<details>
  <summary>Details</summary>
Motivation: 现有注意力实现无法充分利用实际工作负载中请求间的共享前缀（如系统提示、工具模板、RAG），导致重复加载KV缓存，加剧内存带宽压力和解码注意力停滞。

Method: 采用打包-前向-合并范式：1) 按共享前缀打包查询以减少内存访问；2) 运行定制化多瓦片核实现高资源效率；3) 应用多流前向和KV分割减少资源气泡；4) 在线softmax合并。

Result: 在真实和合成工作负载上，PAT平均减少注意力延迟67.4%，在相同配置下TPOT降低13.6-83.4%，优于最先进的注意力核。

Conclusion: PAT通过前缀感知的注意力核实现，有效利用请求间的共享前缀，显著提升LLM解码效率，可作为vLLM的即插即用插件。

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [9] [Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)](https://arxiv.org/abs/2511.22380)
*Kaya Alpturer,Ron van der Meyden,Sushmita Ruj,Godfrey Wong*

Main category: cs.DC

TL;DR: 本文研究崩溃故障模型下的同时一致性协议，针对多种有限信息交换机制，推导出各机制下的最优协议。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识逻辑的最优容错一致性协议研究主要关注"全信息"交换方式，但这种方式消息开销大。本文旨在研究在有限信息交换条件下的最优协议设计。

Method: 分析多种文献中的有限信息交换机制：FloodSet协议、带故障计数的变体、带代理值关联的变体，并引入新的信息交换机制。通过实现基于知识的程序，推导出各信息交换机制下的最优协议。

Result: 为每种信息交换机制推导出最优协议，新提出的信息交换机制虽然比Dwork和Moses的最优协议晚一轮决策，但计算成本和空间需求更低。

Conclusion: 在崩溃故障模型下，针对不同的有限信息交换机制，可以设计出各自最优的同时一致性协议，新提出的机制在性能和开销之间取得了良好平衡。

Abstract: Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the "full information" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Castañeda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.

</details>


### [10] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer是一个统一的系统级加速框架，通过专家放置、缓存压缩和调度优化来最大化LLM服务效率，在DeepSeek-R1上实现616 QPM，TPOT减少36%，TTFT减少38%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在计算密集、延迟严格和吞吐瓶颈方面给大规模服务系统带来重大挑战，需要系统级优化来提升端到端服务效率。

Method: OmniInfer包含三个组件：OmniPlacement用于负载感知的MoE调度，OmniAttn用于稀疏注意力加速，OmniProxy用于解耦感知的请求调度。基于vLLM构建，通过自适应资源解耦、高效稀疏利用和全局协调实现系统级优化。

Result: 在10节点Ascend 910C集群上评估DeepSeek-R1，OmniInfer达到616 QPM，统一框架减少TPOT 36%，OmniProxy叠加进一步减少TTFT 38%。

Conclusion: OmniInfer通过系统级优化有效解决了LLM服务中的计算密集、延迟和吞吐瓶颈问题，显著提升了服务效率，并已开源。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [11] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: RT-MMC利用GPU的RT-cores硬件加速光线追踪，显著提升基于网格的蒙特卡罗模拟速度，简化工作流程


<details>
  <summary>Details</summary>
Motivation: 传统的基于网格的蒙特卡罗方法虽然精度高，但频繁的光线-边界相交测试计算成本高，限制了性能提升，需要更高效的加速方案

Method: 使用NVIDIA OptiX平台，利用现代GPU的RT-cores硬件加速光线追踪能力，将图形光线追踪管道扩展到浑浊介质中的体积光线追踪，无需复杂的四面体网格生成

Result: RT-MMC与传统软件光线追踪MMC算法结果高度一致，在不同GPU架构上实现1.5倍到4.5倍的速度提升，显著增强了MMC的实用性

Conclusion: 从软件转向硬件光线追踪不仅大大简化了MMC模拟工作流程，还带来了显著的性能提升，随着光线追踪硬件的普及，这种优势将进一步增强，有利于广泛的生物光子学应用

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [12] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe是一个动态适配器放置和路由框架，专门解决LoRA服务中不同适配器大小导致的性能倾斜问题，通过动态重新平衡适配器分布和使用GPU Direct RDMA远程访问，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA服务系统在处理多租户环境中不同大小的适配器时存在严重性能倾斜问题，导致GPU资源利用不足，需要更多GPU来满足服务级别目标。

Method: 提出LoRAServe框架，采用工作负载感知的动态适配器放置和路由策略，通过动态重新平衡GPU间的适配器分布，并利用GPU Direct RDMA实现远程访问。

Result: 在真实生产环境评估中，相比现有最优系统，LoRAServe实现高达2倍的吞吐量提升，高达9倍的首令牌时间降低，在满足SLO约束下减少50%的GPU使用量。

Conclusion: LoRAServe通过有效管理LoRA服务中的适配器大小多样性，显著提升了多租户环境下的GPU资源利用效率和服务性能。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


### [13] [Areon: Latency-Friendly and Resilient Multi-Proposer Consensus](https://arxiv.org/abs/2511.23025)
*Álvaro Castro-Castilla,Marcin Pawlowski,Hong-Sheng Zhou*

Main category: cs.DC

TL;DR: Areon是一个基于DAG的多提议者PoS共识协议，通过滑动窗口引用和CCA-local分叉选择实现低延迟确定性，相比传统链式协议具有更低的重组频率和深度。


<details>
  <summary>Details</summary>
Motivation: 解决传统链式PoS共识协议在高延迟网络环境下的性能问题，通过多提议者和DAG结构提高鲁棒性，实现低延迟确定性。

Method: 提出Areon协议家族：1) 每个时隙允许多个提议者；2) 将区块组织成DAG结构；3) 使用滑动窗口引用机制；4) 采用CCA-local、窗口过滤的分叉选择规则；5) 引入Tip-Boundedness结构不变性保证边界宽度。

Result: 1) 形式化证明协议安全性（公共前缀、链增长、链质量）；2) 提出(k,ε)-确定性定理；3) 仿真显示在相同区块到达率下，相比Ouroboros Praos，Areon-Base在广泛对抗条件和网络延迟下实现有界延迟确定性，且重组频率和深度更低。

Conclusion: Areon通过DAG结构和多提议者设计，在部分同步网络下实现了鲁棒的低延迟确定性共识，相比传统链式协议具有显著性能优势，为未来更丰富的交易选择和冗余策略提供了基础框架。

Abstract: We present Areon, a family of latency-friendly, stake-weighted, multi-proposer proof-of-stake consensus protocols. By allowing multiple proposers per slot and organizing blocks into a directed acyclic graph (DAG), Areon achieves robustness under partial synchrony. Blocks reference each other within a sliding window, forming maximal antichains that represent parallel ``votes'' on history. Conflicting subDAGs are resolved by a closest common ancestor (CCA)-local, window-filtered fork choice that compares the weight of each subDAG -- the number of recent short references -- and prefers the heavier one. Combined with a structural invariant we call Tip-Boundedness (TB), this yields a bounded-width frontier and allows honest work to aggregate quickly.
  We formalize an idealized protocol (Areon-Ideal) that abstracts away network delay and reference bounds, and a practical protocol (Areon-Base) that adds VRF-based eligibility, bounded short and long references, and application-level validity and conflict checks at the block level. On top of DAG analogues of the classical common-prefix, chain-growth, and chain-quality properties, we prove a backbone-style $(k,\varepsilon)$-finality theorem that calibrates confirmation depth as a function of the window length and target tail probability. We focus on consensus at the level of blocks; extending the framework to richer transaction selection, sampling, and redundancy policies is left to future work.
  Finally, we build a discrete-event simulator and compare Areon-Base against a chain-based baseline (Ouroboros Praos) under matched block-arrival rates. Across a wide range of adversarial stakes and network delays, Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth.

</details>


### [14] [Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks](https://arxiv.org/abs/2511.23167)
*Chenyu Liu,Zhaoyang Zhang,Zirui Chen,Zhaohui Yang*

Main category: cs.DC

TL;DR: 本文提出C²P²SL方法，将分布式训练中的流水线并行技术应用于无线网络中的分割学习，通过通信-计算流水线并行显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统分割学习虽然将主要计算任务从资源受限的用户设备卸载到基站并保护数据隐私，但其计算和通信过程是顺序执行的，导致系统效率有限。

Method: 提出通信-计算流水线并行分割学习(C²P²SL)，将用户设备和基站的通信与计算过程视为整体流水线，在不同微批次间实现流水线并行，重叠通信与计算时间。同时提出基于交替优化的任务分割和资源分配联合优化方案。

Result: 实验结果表明，C²P²SL在不同通信条件下能保持收敛精度的同时，显著减少系统训练时间超过38%。

Conclusion: C²P²SL通过流水线并行技术有效解决了传统分割学习的效率瓶颈问题，为无线网络中的分布式学习提供了高效解决方案。

Abstract: Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\% while maintaining convergence accuracy under different communication conditions.

</details>


### [15] [Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election](https://arxiv.org/abs/2511.23297)
*Yi-Jun Chang,Lyuting Chen,Haoran Zhou*

Main category: cs.DC

TL;DR: 在内容无关通信模型下，利用网络拓扑知识可实现领导者选举，但需要精确的拓扑信息，且在某些对称图中不可能实现。


<details>
  <summary>Details</summary>
Motivation: 先前研究显示内容无关通信模型下无法计算非恒定函数，这似乎排除了许多自然图问题的可能性。本文旨在探索在该模型下，利用拓扑知识能否实现领导者选举。

Method: 1. 证明边对称图不可能实现随机终止的领导者选举；2. 为非边对称树设计静默终止的领导者选举算法；3. 分析拓扑知识必要性，通过路径图族展示精确拓扑信息的重要性。

Result: 1. 边对称图不可能实现领导者选举；2. 非边对称树可实现领导者选举（O(n²)消息复杂度）；3. 偶直径树仅需直径知识即可实现（O(nr)消息复杂度）；4. 精确拓扑知识对终止选举是必要的。

Conclusion: 在内容无关通信模型中，网络拓扑知识是实现领导者选举的关键因素，但需要精确的拓扑信息，且在某些对称图中不可能实现。

Abstract: The content-oblivious model, introduced by Censor-Hillel, Cohen, Gelles, and Sel (PODC 2022; Distributed Computing 2023), captures an extremely weak form of communication where nodes can only send asynchronous, content-less pulses. Censor-Hillel, Cohen, Gelles, and Sel showed that no non-constant function $f(x,y)$ can be computed correctly by two parties using content-oblivious communication over a single edge, where one party holds $x$ and the other holds $y$. This seemingly ruled out many natural graph problems on non-2-edge-connected graphs.
  In this work, we show that, with the knowledge of network topology $G$, leader election is possible in a wide range of graphs.
  Impossibility: Graphs symmetric about an edge admit no randomized terminating leader election algorithm, even when nodes have unique identifiers and full knowledge of $G$.
  Leader election algorithms: Trees that are not symmetric about any edge admit a quiescently terminating leader election algorithm with topology knowledge, even in anonymous networks, using $O(n^2)$ messages, where $n$ is the number of nodes. Moreover, even-diameter trees admit a terminating leader election given only the knowledge of the network diameter $D = 2r$, with message complexity $O(nr)$.
  Necessity of topology knowledge: In the family of graphs $\mathcal{G} = \{P_3, P_5\}$, both the 3-path $P_3$ and the 5-path $P_5$ admit a quiescently terminating leader election if nodes know the topology exactly. However, if nodes only know that the underlying topology belongs to $\mathcal{G}$, then terminating leader election is impossible.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [A Conceptual Model for Context Awareness in Ethical Data Management](https://arxiv.org/abs/2511.21942)
*Elisa Quintarelli,Fabio Alberto Schreiber,Kostas Stefanidis,Letizia Tanca,Barbara Oliboni*

Main category: cs.DB

TL;DR: 本文提出一个双部分概念模型（CDT和ERT），用于根据上下文调整数据集以满足伦理要求，确保数据分析和学习系统在不同情境下遵循伦理规则。


<details>
  <summary>Details</summary>
Motivation: 信息管理社区日益关注伦理问题，算法和数据需要满足伦理规则以避免产生不道德行为。然而，伦理规则可能因应用程序运行的具体情境（上下文）而异，需要一种系统方法来处理这种变化性。

Method: 提出一个双部分概念模型：1) 上下文维度树（CDT），描述可能的各种上下文；2) 伦理要求树（ERT），表示在不同上下文中为数据分析和学习系统准备数据集所需的伦理规则。通过这两个工具来调整和预处理数据集。

Result: 提供了概念模型的具体示例和使用建议，展示了如何在不同上下文中应用伦理要求来调整数据集，确保数据分析和学习系统符合伦理规范。

Conclusion: 提出的CDT和ERT概念模型为处理数据管理中的伦理问题提供了系统框架，能够根据具体上下文调整伦理要求，确保算法和数据使用符合道德标准。

Abstract: Ethics has become a major concern to the information management community, as both algorithms and data should satisfy ethical rules that guarantee not to generate dishonourable behaviours when they are used. However, these ethical rules may vary according to the situation-the context-in which the application programs must work. In this paper, after reviewing the basic ethical concepts and their possible influence on data management, we propose a bipartite conceptual model, composed of the Context Dimensions Tree (CDT), which describes the possible contexts, and the Ethical Requirements Tree (ERT), representing the ethical rules necessary to tailor and preprocess the datasets that should be fed to Data Analysis and Learning Systems in each possible context. We provide some examples and suggestions on how these conceptual tools can be used.

</details>


### [17] [Relation-Stratified Sampling for Shapley Values Estimation in Relational Databases](https://arxiv.org/abs/2511.22035)
*Amirhossein Alizad,Mostafa Milani*

Main category: cs.DB

TL;DR: 提出RSS和ARSS采样方法，用于高效计算关系查询中元组的Shapley值，相比传统方法显著降低误差和方差


<details>
  <summary>Details</summary>
Motivation: Shapley值等指标能量化元组对查询结果的贡献，但精确计算需要指数级排列组合，计算不可行。现有采样方法未充分利用关系数据库的结构特性

Method: 提出关系分层采样(RSS)：按关系计数向量而非仅按大小进行分层，聚焦于结构有效且信息丰富的联盟；进一步提出自适应变体ARSS：根据方差估计动态调整预算分配；实现重用编译视图降低单样本查询成本

Result: 在TPCH工作负载的多关系连接和聚合查询中，RSS和ARSS一致优于经典蒙特卡洛采样和基于大小的分层采样，用更少样本获得更低误差和方差

Conclusion: 关系感知分层和自适应分配提供互补优势，使ARSS成为数据库中心Shapley归因的简单、有效、随时可用的估计器

Abstract: Shapley-like values, including the Shapley and Banzhaf values, provide a principled way to quantify how individual tuples contribute to a query result. Their exact computation, however, is intractable because it requires aggregating marginal contributions over exponentially many permutations or subsets. While sampling-based estimators have been studied in cooperative game theory, their direct use for relational query answering remains underexplored and often ignores the structure of schemas and joins.
  We study tuple-level attribution for relational queries through sampling and introduce Relation-Stratified Sampling (RSS). Instead of stratifying coalitions only by size, RSS partitions the sample space by a relation-wise count vector that records how many tuples are drawn from each relation. This join-aware stratification concentrates samples on structurally valid and informative coalitions and avoids strata that cannot satisfy query conditions. We further develop an adaptive variant, ARSS, that reallocates budget across strata using variance estimates obtained during sampling, improving estimator efficiency without increasing the total number of samples. We analyze these estimators, describe a practical implementation that reuses compiled views to reduce per-sample query cost, and evaluate them on TPCH workloads.
  Across diverse queries with multi-relation joins and aggregates, RSS and ARSS consistently outperform classical Monte Carlo (MCS) and size-based Stratified Sampling (SS), yielding lower error and variance with fewer samples. An ablation shows that relation-aware stratification and adaptive allocation contribute complementary gains, making ARSS a simple, effective, and anytime estimator for database-centric Shapley attribution.

</details>


### [18] [Performant Synchronization in Geo-Distributed Databases](https://arxiv.org/abs/2511.22444)
*Duling Xu,Tong Li,Zegang Sun,Zheng Chen,Weixing Zhou,Yanfeng Zhang,Wei Lu,Xiaoyong Du*

Main category: cs.DB

TL;DR: GeoCoCo框架通过分组重调度、数据过滤和一致性传输优化，将跨区域分布式数据库的同步成本降低40.3%，吞吐量提升14.1%


<details>
  <summary>Details</summary>
Motivation: 分布式数据库在广域网中面临高延迟问题，主要瓶颈是跨节点数据一致性所需的同步成本。研究发现网络聚类现象、传输三角不等式违规和冗余数据传输为优化提供了机会。

Method: 提出GeoCoCo同步加速框架：1) 基于实时网络状况的分组重调度策略最大化WAN传输效率；2) 任务保留数据过滤方法减少WAN传输数据量；3) 整合分组和剪枝的一致性保证传输框架。

Result: 在轨迹驱动仿真和实际部署中的广泛评估表明，GeoCoCo将同步成本（主要通过降低WAN带宽使用）降低高达40.3%，在GeoGauss中将系统吞吐量提升高达14.1%。

Conclusion: GeoCoCo通过优化同步成本有效解决了跨区域分布式数据库的性能瓶颈，显著提升了广域网环境下的数据库性能。

Abstract: The deployment of databases across geographically distributed regions has become increasingly critical for ensuring data reliability and scalability. Recent studies indicate that distributed databases exhibit significantly higher latency than single-node databases, primarily due to consensus protocols maintaining data consistency across multiple nodes. We argue that synchronization cost constitutes the primary bottleneck for distributed databases, which is particularly pronounced in wide-area networks (WAN). Fortunately, we identify opportunities to optimize synchronization costs in real production environments: (1) network clustering phenomena, (2) triangle inequality violations in transmission, and (3) redundant data transfers. Based on these observations, we propose GeoCoCo, a synchronization acceleration framework for cross-region distributed databases. First, GeoCoCo presents a group rescheduling strategy that adapts to real-time network conditions to maximize WAN transmission efficiency. Second, GeoCoCo introduces a task-preserving data filtering method that reduces data volume transmitted over the WAN. Finally, GeoCoCo develops a consistency-guaranteed transmission framework integrating grouping and pruning. Extensive evaluations in both trace-driven simulations and real-world deployments demonstrate that GeoCoCo reduces synchronization cost-primarily by lowering WAN bandwidth usage-by up to 40.3%, and increases system throughput by up to 14.1% in GeoGauss.

</details>


### [19] [Structured Multi-Step Reasoning for Entity Matching Using Large Language Model](https://arxiv.org/abs/2511.22832)
*Rohan Bopardikar,Jin Wang,Jia Zou*

Main category: cs.DB

TL;DR: 提出一个基于LLM的三步推理框架用于实体匹配，通过分解匹配过程为多个明确推理阶段来提升性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的实体匹配方法大多依赖单步提示，缺乏对结构化推理策略的深入探索，需要更系统的推理方法来提升匹配准确性和鲁棒性

Method: 提出三步推理框架：1) 识别两个记录间的匹配和不匹配标记；2) 确定对匹配决策最有影响的属性；3) 预测记录是否指向同一真实世界实体。同时探索基于辩论的策略，对比支持和反对论点以提高决策鲁棒性

Result: 在多个真实世界实体匹配基准数据集上的实验表明，结构化多步推理在多个案例中能提升匹配性能，同时也揭示了推理引导的LLM方法面临的挑战和改进机会

Conclusion: 结构化多步推理能有效提升基于LLM的实体匹配性能，但仍有挑战需要进一步优化推理引导的LLM方法

Abstract: Entity matching is a fundamental task in data cleaning and data integration. With the rapid adoption of large language models (LLMs), recent studies have explored zero-shot and few-shot prompting to improve entity matching accuracy. However, most existing approaches rely on single-step prompting and offer limited investigation into structured reasoning strategies. In this work, we investigate how to enhance LLM-based entity matching by decomposing the matching process into multiple explicit reasoning stages. We propose a three-step framework that first identifies matched and unmatched tokens between two records, then determines the attributes most influential to the matching decision, and finally predicts whether the records refer to the same real-world entity. In addition, we explore a debate-based strategy that contrasts supporting and opposing arguments to improve decision robustness. We evaluate our approaches against multiple existing baselines on several real-world entity matching benchmark datasets. Experimental results demonstrate that structured multi-step reasoning can improve matching performance in several cases, while also highlighting remaining challenges and opportunities for further refinement of reasoning-guided LLM approaches.

</details>


### [20] [Extended Serial Safety Net: A Refined Serializability Criterion for Multiversion Concurrency Control](https://arxiv.org/abs/2511.22956)
*Atsushi Kitazawa,Chihaya Ito,Yuta Yoshida,Takamitsu Shioi*

Main category: cs.DB

TL;DR: ESSN是SSN的改进版本，通过放宽排除条件允许更多事务安全提交，保持多版本可串行化，并严格包含SSN。它基于MVSG标准，引入已知总序进行推理，通过单次提交检查消除链遍历，工作复杂度与读写数量线性相关。


<details>
  <summary>Details</summary>
Motivation: 传统的并发控制协议基于单一串行化点（开始或提交）论证正确性，这与快照隔离不兼容。SSN虽然提供了轻量级提交时测试，但过于保守且仅以提交时间为唯一锚点。需要一种更通用的方法，既能保持正确性，又能提高事务提交率。

Method: ESSN通过放宽SSN的排除条件，允许更多事务安全提交。它基于MVSG标准，引入已知总序（如开始排序或提交排序）进行可串行化推理。采用基于不变量的语义进行单次提交检查，保持版本链单调性，消除链遍历。协议基于直接串行化图，提交时工作与读写数量线性相关。

Result: ESSN严格包含SSN，保持多版本可串行化。在提交排序的已知总序下，使用开始快照读取可将长事务中止率降低约0.25绝对值（约50%相对值）。协议工作复杂度与SSN相同，每版本足迹匹配。

Conclusion: ESSN是SSN的原则性泛化，通过放宽排除条件提高事务提交率，同时保持正确性。它提供了更灵活的并发控制方法，特别适用于混合工作负载，能显著减少长事务中止率。

Abstract: A long line of concurrency-control (CC) protocols argues correctness via a single serialization point (begin or commit), an assumption that is incompatible with snapshot isolation (SI), where read-write anti-dependencies arise. Serial Safety Net (SSN) offers a lightweight commit-time test but is conservative and effectively anchored on commit time as the sole point. We present ESSN, a principled generalization of SSN that relaxes the exclusion condition to allow more transactions to commit safely, and we prove that this preserves multiversion serializability (MVSR) and that it strictly subsumes SSN. ESSN states an MVSG (Multiversion Serialization Graph)-based criterion and introduces a known total order over transactions (KTO; e.g., begin-ordered or commit-ordered) for reasoning about the graph's serializability. With a single commit-time check under invariant-based semantics, ESSN's exclusion condition preserves monotonicity along per-item version chains, and eliminates chain traversal. The protocol is Direct Serialization Graph (DSG)-based with commit-time work linear in the number of reads and writes, matching SSN's per-version footprint. We also make mixed workloads explicit by defining a Long transaction via strict interval containment of Short transactions, and we evaluate ESSN on reproducible workloads. Under a commit-ordered KTO, using begin-snapshot reads reduces the long-transaction abort rate by up to approximately 0.25 absolute (about 50% relative) compared with SSN.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [21] [Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem](https://arxiv.org/abs/2511.21769)
*Royer David Estrada-Esponda,Gerardo Matturro,Jose Reinaldo Sabogal-Pinilla*

Main category: cs.SE

TL;DR: 研究哥伦比亚软件创业生态系统中，创业团队在不同发展阶段最重视的技术知识和软技能，发现需求工程、软件测试、敏捷方法等技术知识，以及沟通、领导力、团队合作等软技能最为重要。


<details>
  <summary>Details</summary>
Motivation: 创业团队成员的技术知识和软技能对软件初创企业的早期阶段有显著影响，创业成功与否很大程度上取决于创始团队的质量。然而，需要了解创业团队最重视哪些具体的技术知识和软技能，以及这些需求如何随着企业成长而变化。

Method: 在哥伦比亚创业生态系统内进行调查研究，对软件初创企业代表进行问卷调查，分析创业团队在不同发展阶段对技术知识和软技能的需求变化。

Result: 调查显示，最受重视的技术知识包括：需求工程、软件测试、项目规划与管理、敏捷方法、市场营销、商业模式定义和预算编制。最受重视的软技能包括：沟通能力、领导力和团队合作能力。

Conclusion: 这项研究的结果对软件创业者、孵化器和研究人员具有重要参考价值，帮助他们了解创业团队在不同发展阶段需要重点关注的技术知识和软技能，从而提高创业成功率。

Abstract: The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.

</details>


### [22] [Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings](https://arxiv.org/abs/2511.21788)
*Md. Raihan Tapader,Md. Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe*

Main category: cs.SE

TL;DR: 提出基于LLM的多语言代码重构框架，通过提示工程和微调提升重构效果，实验显示Java在正确性和可编译性上表现最佳，Python在结构改动最小化方面最优。


<details>
  <summary>Details</summary>
Motivation: 现有代码重构方法依赖手工规则，难以泛化到多种编程语言和编码风格，需要开发能够跨语言进行准确高效代码重构的自动化框架。

Method: 提出基于提示工程的微调模型，结合少样本学习进行多语言代码重构，研究温度参数、不同样本算法等提示工程技术对重构效果的影响。

Result: Java在10-shot设置下达到99.99%最高正确性，平均可编译性94.78%，保持53-54%相似度；Python在所有样本设置下结构距离最小（277-294），相似度44-48%，表明重构改动最小且一致。

Conclusion: 基于LLM的代码重构框架能够有效进行多语言代码重构，不同语言在重构效果上呈现不同特点，提示工程和微调对提升重构质量有显著影响。

Abstract: In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.

</details>


### [23] [LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems](https://arxiv.org/abs/2511.21877)
*Nenad Petrovic,Norbert Kroth,Axel Torschmied,Yinglei Song,Fengjunjie Pan,Vahid Zolfaghari,Nils Purschke,Sven Kirchner,Chengdong Wu,Andre Schamschurko,Yi Zhang,Alois Knoll*

Main category: cs.SE

TL;DR: 提出基于事件链和LLM的工作流，从自然语言需求生成经过验证的汽车代码，无需LLM重训练


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言需求生成汽车代码时的幻觉问题、架构正确性问题和实时可行性问题

Method: 使用RAG检索VSS信号作为上下文，映射验证后转换为事件链，用事件链约束LLM代码生成

Result: 在紧急制动案例研究中实现了有效的信号使用和一致的代码生成

Conclusion: 提出的工作流能够生成经过验证的汽车代码，确保行为一致性和实时可行性，无需LLM重训练

Abstract: This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.

</details>


### [24] [Advancing Automated In-Isolation Validation in Repository-Level Code Translation](https://arxiv.org/abs/2511.21878)
*Kaiyao Ke,Ali Reza Ibrahimzada,Rangeet Pan,Saurabh Sinha,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: TRAM：结合上下文感知类型解析与基于模拟的隔离验证，实现高质量的跨语言代码库翻译


<details>
  <summary>Details</summary>
Motivation: 尽管代码库级跨语言翻译技术有所进步，但验证翻译结果仍然具有挑战性。现有方法要么依赖语言互操作性需要大量人工工作，要么使用代理进行验证成本高昂。

Method: 1. 翻译前检索API文档和上下文代码信息；2. 使用LLM结合检索到的上下文信息解析跨语言类型映射；3. 通过自定义序列化/反序列化工作流自动构建目标语言中的等效模拟对象；4. 在隔离环境中验证每个方法片段。

Result: TRAM在Java到Python翻译中展示了最先进的性能，证明了其基于RAG的类型解析与可靠隔离验证集成的有效性。

Conclusion: TRAM通过结合上下文感知类型解析和基于模拟的隔离验证，实现了高质量的跨语言代码库翻译，解决了现有方法在验证方面的挑战。

Abstract: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.

</details>


### [25] [Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code](https://arxiv.org/abs/2511.21920)
*Apu Kumar Chakroborti,Yi Ding,Lipeng Wan*

Main category: cs.SE

TL;DR: 研究评估开源大语言模型在科学数据分析和可视化Python代码生成中的可信度，发现无人工干预时可靠性有限，提出了三种改进策略并创建了可重用基准。


<details>
  <summary>Details</summary>
Motivation: 现代科学日益数据密集型，但许多领域科学家缺乏编程专业知识，难以开发自定义数据分析工作流。大语言模型通过自然语言生成可执行代码提供了有前景的解决方案，但其在科学领域的可信度需要评估。

Method: 构建反映真实研究任务的领域启发式提示基准套件，系统评估生成代码的可执行性和正确性。设计并评估三种互补策略：数据感知提示消歧、检索增强提示改进和迭代错误修复。

Result: 研究发现无人工干预时，LLM生成代码的可靠性有限，失败常由模糊提示和模型对领域特定上下文理解不足导致。三种改进策略显著提高了执行成功率和输出质量，但仍需进一步改进。

Conclusion: 这项工作突出了LLM驱动科学工作流自动化的前景和当前局限性，引入了可操作技术和可重用基准，用于构建更具包容性、可访问性和可信度的AI辅助研究工具。

Abstract: As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.

</details>


### [26] [Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications](https://arxiv.org/abs/2511.21956)
*M. Polzin,M. Guzman*

Main category: cs.SE

TL;DR: 本文探讨如何现代化遗留应用程序，强调不应简单复制旧系统，而应通过用户参与设计更直观、支持任务效率的新应用，利用现有应用作为设计参考而非限制。


<details>
  <summary>Details</summary>
Motivation: 现代化遗留应用时，人们容易陷入简单复制旧系统的陷阱，但这会延续原有痛点。现有应用虽然专家用户熟悉，但可能包含低效流程。需要平衡专家用户习惯与解决现有问题，创造真正有价值的现代化应用。

Method: 通过用户参与式设计方法，将现有遗留应用作为设计参考工具，分析用户任务流程，识别痛点，在保持专家用户熟悉度的同时引入新的GUI设计理念，创造更直观的用户体验。

Result: 用户参与能有效弥合简单复制与全新设计之间的差距，利用现有应用提供的洞察（这是全新应用不具备的），开发出既满足专家用户需求又解决原有问题、更易开发的现代化应用。

Conclusion: 现代化遗留应用不应简单复制，而应利用现有应用作为设计参考，通过用户参与平衡熟悉度与创新，创造真正支持用户任务效率的现代化应用，使开发过程更顺利。

Abstract: When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.

</details>


### [27] [DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction](https://arxiv.org/abs/2511.21964)
*Ali Sayedsalehi,Peter C. Rigby,Audris Mockus*

Main category: cs.SE

TL;DR: DRS-OSS是一个开源系统，使用微调的Llama 3.1 8B模型评估代码变更引入缺陷的风险，帮助大型开源项目优先审查高风险提交，防止回归问题。


<details>
  <summary>Details</summary>
Motivation: 大型开源项目每天有数百个拉取请求，每个都可能引入回归缺陷。需要一种自动化的方法来评估代码变更的风险，以便更好地进行审查优先级排序、测试规划和CI/CD门控。

Method: 使用微调的Llama 3.1 8B序列分类器，结合提交信息、结构化差异和变更指标的长上下文表示。通过参数高效适配、4位QLoRA和DeepSpeed ZeRO-3 CPU卸载技术，在单个20GB GPU上训练22k令牌的上下文。

Result: 在ApacheJIT基准测试中达到最先进性能（F1=0.64，ROC-AUC=0.89）。模拟显示，仅门控风险最高的30%提交就能防止高达86.4%的缺陷引入变更。

Conclusion: DRS-OSS提供了一个完整的开源解决方案，包括API、Web UI和GitHub插件，能够有效识别高风险代码变更，帮助开发团队优先处理潜在问题，减少回归缺陷。

Abstract: In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.

</details>


### [28] [Statistical Independence Aware Caching for LLM Workflows](https://arxiv.org/abs/2511.22118)
*Yihan Dai,Dimitrios Stamatios Bouras,Haoxiang Jia,Sergey Mechtaev*

Main category: cs.SE

TL;DR: Mnimi是一个LLM缓存设计模式，通过封装统计约束来确保组件级统计独立性，解决缓存重用破坏统计独立性的问题


<details>
  <summary>Details</summary>
Motivation: LLM推理成本高且慢，本地缓存可降低成本延迟，但简单重用缓存会破坏统计独立性，而现有缓存系统缺乏统计独立性约束机制

Method: 将统计约束封装在LLM引用类型中，用户可根据算法范围和需求管理和转换这些类型，使用装饰器和无限序列迭代器在Python中实现

Result: 在SpecFix程序规范修复系统案例中，Mnimi提高了可重现性、调试便利性、时间和成本效率，同时保持统计正确性

Conclusion: Mnimi设计模式支持模块化LLM工作流，确保组件级统计完整性，解决了缓存重用与统计独立性之间的冲突

Abstract: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

</details>


### [29] [Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem](https://arxiv.org/abs/2511.22186)
*Chayanid Termphaiboon,Raula Gaikovina Kula,Youmei Fan,Morakot Choetkiertikul,Chaiyong Ragkhitwetsagul,Thanwadee Sunetnanta,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 研究探讨了安全政策（SECURITY.md文件）对PyPI项目依赖管理的影响，发现拥有安全政策的项目依赖更广泛但深度相似，且依赖更新更频繁。


<details>
  <summary>Details</summary>
Motivation: 尽管安全政策在开源项目中越来越普遍，但它们如何影响软件依赖的结构和演化仍不清楚。软件依赖的互联性影响功能和安全性，需要研究安全政策与依赖管理之间的关系。

Method: 分析PyPI项目中拥有和没有SECURITY.md文件的项目，通过检查依赖树和跟踪依赖随时间的变化来研究安全政策与依赖管理的关系。

Result: 拥有安全政策的项目倾向于依赖更广泛的直接依赖，但整体深度和传递依赖相似。在SECURITY.md引入后创建的项目（特别是后期采用者）显示更频繁的依赖更新。

Conclusion: 安全政策与更模块化和功能丰富的项目相关，SECURITY.md在促进主动依赖管理和降低软件供应链风险方面发挥重要作用。

Abstract: Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.

</details>


### [30] [UniBOM -- A Unified SBOM Analysis and Visualisation Tool for IoT Systems and Beyond](https://arxiv.org/abs/2511.22359)
*Vadim Safronov,Ionut Bostan,Nicholas Allott,Andrew Martin*

Main category: cs.SE

TL;DR: UniBOM是一个先进的SBOM生成、分析和可视化工具，通过集成二进制、文件系统和源代码分析，提升网络系统的安全可追溯性，特别针对C/C++等非包管理语言提供精细化的漏洞检测。


<details>
  <summary>Details</summary>
Motivation: 现代网络系统依赖复杂的软件栈，其中隐藏着由复杂依赖关系产生的漏洞。现有SBOM解决方案在二进制分析和C/C++等非包管理语言方面缺乏精确性，需要更先进的工具来增强网络系统的安全可追溯性。

Method: UniBOM集成了二进制分析、文件系统分析和源代码分析，支持历史CPE跟踪、基于AI的漏洞严重性和内存安全分类，特别针对非包管理的C/C++依赖关系提供支持。

Result: 通过对258个无线路由器固件二进制文件和4个流行物联网操作系统源代码的比较漏洞分析，UniBOM展示了比其他广泛使用的SBOM工具更优越的检测能力。

Conclusion: UniBOM作为开源工具，提供了端到端的统一分析和可视化解决方案，推动了SBOM驱动的安全管理，为可靠的网络系统和更广泛的软件安全提供了先进支持。

Abstract: Modern networked systems rely on complex software stacks, which often conceal vulnerabilities arising from intricate interdependencies. A Software Bill of Materials (SBOM) is effective for identifying dependencies and mitigating security risks. However, existing SBOM solutions lack precision, particularly in binary analysis and non-package-managed languages like C/C++.
  This paper introduces UniBOM, an advanced tool for SBOM generation, analysis, and visualisation, designed to enhance the security accountability of networked systems. UniBOM integrates binary, filesystem, and source code analysis, enabling fine-grained vulnerability detection and risk management. Key features include historical CPE tracking, AI-based vulnerability classification by severity and memory safety, and support for non-package-managed C/C++ dependencies.
  UniBOM's effectiveness is demonstrated through a comparative vulnerability analysis of 258 wireless router firmware binaries and the source code of four popular IoT operating systems, highlighting its superior detection capabilities compared to other widely used SBOM generation and analysis tools. Packaged for open-source distribution, UniBOM offers an end-to-end unified analysis and visualisation solution, advancing SBOM-driven security management for dependable networked systems and broader software.

</details>


### [31] [NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements](https://arxiv.org/abs/2511.22409)
*Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.SE

TL;DR: NOMAD是一个认知启发的多智能体框架，用于将LLM生成UML图分解为多个专门子任务，在性能上超越基线方法，并首次系统分类了LLM生成UML图的错误类型。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在软件工程中应用日益广泛，但其生成UML图等结构化制品的能力尚未充分探索。现有方法缺乏对LLM生成UML图错误的系统理解和验证策略。

Method: 提出NOMAD框架，采用认知启发的模块化多智能体设计，将UML生成分解为实体提取、关系分类、图合成等专门子任务。每个智能体处理不同的建模活动，模仿工程师的目标导向推理过程。

Result: NOMAD在所有选定基线上表现更优。通过Northwind案例研究和人工编写的UML练习进行评估，揭示了细粒度属性提取方面的持续挑战。首次提出了LLM生成UML图的系统错误分类法，包括结构、关系和语义/逻辑错误。

Conclusion: NOMAD既是UML类图生成的有效框架，也为可靠的语言到模型工作流研究提供了视角。验证作为设计探针显示出混合效果，自适应策略是有前景的研究方向。

Abstract: Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.

</details>


### [32] [Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X](https://arxiv.org/abs/2511.22513)
*Jérôme Pfeiffer,Nicolai Maisch,Sebastian Friedl,Matthias Milan Strljic,Armin Lechler,Oliver Riedel,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文提出一种基于领域特定语言（DSL）的方法，让领域专家能够以声明式、人类可读且机器可执行的方式定义数据使用策略，解决工业数据空间中策略描述和执行的实际挑战。


<details>
  <summary>Details</summary>
Motivation: 随着GAIA-X和国际数据空间（IDS）等联邦数据空间的采用，工业4.0中跨组织边界的安全主权数据共享成为可能。然而，现有技术框架（如AAS、EDC、ID-Link、OPC UA）面临一个主要挑战：如何让没有软件工程背景的领域专家实际描述和执行上下文相关的数据使用策略。

Method: 提出利用领域特定语言（DSL）的方法，通过声明式、人类可读且机器可执行的策略定义，实现主权数据共享。DSL使领域专家能够指定细粒度的数据治理要求，而无需编写命令式代码。

Result: 该方法使领域专家能够定义如限制特定生产批次数据访问、在定义保留期后自动删除等细粒度数据治理策略，解决了数据空间连接器中策略定义的实际问题。

Conclusion: 通过DSL方法，实现了让领域专家直接参与数据治理策略定义的目标，填补了工业数据空间中策略描述和执行的技术空白，促进了更广泛的数据空间采用。

Abstract: The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.

</details>


### [33] [The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods](https://arxiv.org/abs/2511.22726)
*Ethan Friesen,Sasha Morton-Salmon,Md Nahidul Islam Opu,Shahidul Islam,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 该研究首次大规模分析了极端易错方法（多次涉及bug修复的方法），发现它们虽然只占所有方法的极小部分，却导致了不成比例的bug数量。这些方法在创建时就比单次bug方法和无bug方法更大、更复杂、可读性更差，但机器学习模型难以早期预测，需要更丰富的演化感知代码表示。


<details>
  <summary>Details</summary>
Motivation: 识别那些反复吸引bug的源代码子集对于减少长期维护成本至关重要。研究极端易错方法（多次涉及bug修复的方法）的普遍性、特征和可预测性，有助于开发人员早期识别高风险代码。

Method: 使用98个开源Java项目的125万多个方法数据集，分析极端易错方法的特征。评估五种机器学习模型的预测能力，并对265个极端易错方法进行主题分析，识别视觉问题、上下文角色和常见缺陷模式。

Result: 极端易错方法只占所有方法的极小部分，却导致了不成比例的bug数量。这些方法在创建时就显著更大、更复杂、可读性更差、可维护性更低。机器学习模型早期预测不可靠，主要受数据不平衡、项目异质性和bug多来自后续演化而非初始实现的影响。

Conclusion: 需要更丰富的演化感知代码表示来改进预测。研究提供了可操作的见解，帮助开发人员早期优先处理高风险方法，识别了极端易错方法的视觉问题、上下文角色和常见缺陷模式。

Abstract: Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.

</details>


### [34] [MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement](https://arxiv.org/abs/2511.22921)
*Hengyuan Liu,Xia Song,Yong Liu,Zheng Li*

Main category: cs.SE

TL;DR: 提出DKMR方法，通过信号处理技术对MBFL中的kill矩阵进行去噪，提升故障定位效果


<details>
  <summary>Details</summary>
Motivation: MBFL中的噪声现象（特别是突变体与测试之间的虚假kill关系）严重降低了定位效果，现有方法只修正最终结果而不直接处理底层噪声

Method: 提出DKMR方法：1) 通过混合矩阵构建进行信号增强，提高信噪比；2) 通过频域滤波进行信号去噪，抑制噪声同时保留故障相关模式。基于此开发MBFL-DKMR框架，使用精炼后的模糊值矩阵计算可疑度

Result: 在Defects4J v2.0.0上评估，MBFL-DKMR有效缓解噪声，优于最先进的MBFL技术：Top-1定位129个故障（BLMu为85，Delta4Ms为103），计算开销极小（0.11秒，占总时间0.001%）

Conclusion: 信号处理技术通过精炼kill矩阵有潜力提升MBFL的有效性，DKMR方法显著改善了故障定位性能

Abstract: Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.

</details>


### [35] [A transfer learning approach for automatic conflicts detection in software requirement sentence pairs based on dual encoders](https://arxiv.org/abs/2511.23007)
*Yizheng Wang,Tao Jiang,Jinyan Bai,Zhengbin Zou,Tiancheng Xue,Nan Zhang,Jie Luan*

Main category: cs.SE

TL;DR: 提出基于SBERT和SimCSE的可迁移软件需求冲突检测框架TSRCDF-SS，通过双编码器、六元素拼接和混合损失优化，在跨域场景中显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 软件需求文档通常包含数万个需求，确保需求间一致性对项目成功至关重要。现有自动化检测方法面临数据不平衡导致检测精度低、单一编码器语义提取有限、跨域迁移学习性能不佳等挑战。

Method: 1) 使用SBERT和SimCSE双独立编码器为需求对生成句子嵌入，采用六元素拼接策略；2) 通过两层全连接前馈神经网络分类器，结合Focal Loss变体、领域特定约束和置信度惩罚项的混合损失优化策略；3) 协同整合顺序和跨域迁移学习。

Result: 实验结果表明，该框架在域内设置中macro-F1和weighted-F1分数均提升10.4%，在跨域场景中macro-F1提升11.4%。

Conclusion: TSRCDF-SS框架有效解决了软件需求冲突检测中的数据不平衡、语义提取有限和跨域迁移性能不足等问题，显著提升了检测准确性和可迁移性。

Abstract: Software Requirement Document (RD) typically contain tens of thousands of individual requirements, and ensuring consistency among these requirements is critical for the success of software engineering projects. Automated detection methods can significantly enhance efficiency and reduce costs; however, existing approaches still face several challenges, including low detection accuracy on imbalanced data, limited semantic extraction due to the use of a single encoder, and suboptimal performance in cross-domain transfer learning. To address these issues, this paper proposes a Transferable Software Requirement Conflict Detection Framework based on SBERT and SimCSE, termed TSRCDF-SS. First, the framework employs two independent encoders, Sentence-BERT (SBERT) and Simple Contrastive Sentence Embedding (SimCSE), to generate sentence embeddings for requirement pairs, followed by a six-element concatenation strategy. Furthermore, the classifier is enhanced by a two-layer fully connected feedforward neural network (FFNN) with a hybrid loss optimization strategy that integrates a variant of Focal Loss, domain-specific constraints, and a confidence-based penalty term. Finally, the framework synergistically integrates sequential and cross-domain transfer learning. Experimental results demonstrate that the proposed framework achieves a 10.4% improvement in both macro-F1 and weighted-F1 scores in in-domain settings, and an 11.4% increase in macro-F1 in cross-domain scenarios.

</details>


### [36] [APDT: A Digital Twin for Assessing Access Point Characteristics in a Network](https://arxiv.org/abs/2511.23009)
*D. Sree Yashaswinee,Gargie Tambe,Y. Raghu Reddy,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 提出名为APDT的接入点数字孪生系统，用于实时监控网络状态、模拟分析并预测流量拥塞，通过主动卸载客户端来优化网络性能


<details>
  <summary>Details</summary>
Motivation: 数字孪生技术在网络领域的应用尚未充分探索，当前网络面临客户端密度增加和流量拥塞等问题，需要更好的监控和预测工具来提升网络性能和服务质量

Method: 开发APDT系统，通过Ruckus SmartZone API从三个接入点收集实时数据，结合NS-3仿真平台模拟网络状态，建立预测模型分析流量模式并建议客户端卸载策略

Result: 初步结果表明APDT能成功预测短期流量激增，改善服务质量参数（如延迟、抖动），减少网络拥塞，为网络管理和性能优化提供有效工具

Conclusion: APDT数字孪生系统为网络管理提供了创新的监控和预测能力，能够通过模拟分析和主动干预来优化网络性能，在数字孪生技术应用于网络领域方面具有重要价值

Abstract: Digital twins (DT) have emerged as a transformative technology, enabling real-time monitoring, simulations, and predictive maintenance across various domains, though their Application in the networking domain remains underexplored. This paper focuses on issues such as increasing client density and traffic congestion by proposing a digital twin for computer networks. Our Digital Twin, named Access Point Digital Twin (APDT) is used for tracking user behavior and changing bandwidth demands, directly impacting network performance and Quality of Service (QoS) parameters like latency, jitter, etc. APDT captures the real-time state of networks with data from access points (APs), enabling simulation-based analyses and predictive modelling. APDT facilitates the simulation of various what-if scenarios thereby providing a better understanding of various aspects of the network characteristics. We tested APDT on our University network. APDT uses data collected from three access points via the Ruckus SmartZone API and incorporates NS-3 based simulations. The simulation replicates a real-time snapshot from a Ruckus access point and models metrics such as latency and inter-packet transfer time. Additionally, a forecasting model predicts traffic congestion and suggests proactive client offloading, enhancing network management and performance optimization. Preliminary results indicate that APDT can successfully predict short-term traffic surges, leading to improved QoS and reduced traffic congestion.

</details>


### [37] [Software for Studying CASCADE Error Correction Protocols in Quantum Communications](https://arxiv.org/abs/2511.23050)
*Nikita Repnkiov,Vladimir Faerman*

Main category: cs.SE

TL;DR: 开发基于CASCADE协议的量子密钥协调软件原型，采用并行纠错算法提升效率，实验验证核心算法正确性，提出架构改进方案


<details>
  <summary>Details</summary>
Motivation: 应对量子计算威胁，发展量子通信技术，重点解决量子密钥协调中的效率问题，为研究和教育提供软件工具

Method: 基于CASCADE协议设计软件原型，实现基于actor模型的并行纠错算法，减少数据交换量，进行实验评估

Result: 成功实现核心CASCADE算法，但发现消息传递计算成本高、错误处理复杂、代码冗余等问题，并行算法提高了密钥协调效率

Conclusion: 原型验证了核心算法可行性，提出架构重构、数据导出接口、通信组件分离、系统验证工具扩展等改进方向，为盲密钥协调方法研究奠定基础

Abstract: This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.

</details>


### [38] [Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning](https://arxiv.org/abs/2511.23157)
*Hana Kataoka,Jialong Li,Yutaka Matsuno*

Main category: cs.SE

TL;DR: 最新LLM在PBL中扮演双重角色：既是"均衡器"提升平均表现，也是"放大器"扩大绝对差距


<details>
  <summary>Details</summary>
Motivation: 随着LLM重塑软件开发，将其融入SE教育变得迫切。现有研究主要关注入门编程或孤立SE任务，但在更开放的项目式学习(PBL)中的影响尚未探索。

Method: 进行为期两年的纵向研究，比较2024年（使用早期免费LLM，n=48）和2025年（使用最新付费LLM，n=46）两个队列。

Result: 最新强大LLM具有双重作用：作为"均衡器"提升平均表现（包括编程能力弱的学生），提供更真实的SE实践机会；同时作为"放大器"显著扩大绝对表现差距。

Conclusion: LLM在PBL中创造了新的教学挑战，需要解决教育不平等问题，同时利用其提供更真实SE实践机会的潜力。

Abstract: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

</details>


### [39] [AI for software engineering: from probable to provable](https://arxiv.org/abs/2511.23159)
*Bertrand Meyer*

Main category: cs.SE

TL;DR: 论文提出将AI编程的创造力与形式化规范方法和程序验证相结合，以解决AI编程中的目标指定困难和幻觉问题


<details>
  <summary>Details</summary>
Motivation: AI编程面临两大障碍：目标指定困难（提示工程本质上是需求工程，是软件工程中最困难的领域之一）和幻觉现象。程序只有在正确或接近正确时才有用。

Method: 将人工智能的创造力与形式化规范方法和形式化程序验证相结合，并借助现代证明工具的支持。

Result: 未在摘要中明确说明具体结果，但提出了解决AI编程问题的综合方法框架。

Conclusion: 通过结合AI创造力、形式化规范和程序验证，可以克服AI编程中的目标指定和幻觉问题，确保生成程序的正确性。

Abstract: Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals ("prompt engineering" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.
  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.

</details>


### [40] [GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis](https://arxiv.org/abs/2511.23213)
*Samuele Doria,Eleonora Losiouk*

Main category: cs.SE

TL;DR: GAPS是一个集成静态和动态分析的Android方法可达性系统，通过图遍历和数据流分析重建到达目标方法的路径，并在运行时指导应用探索，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: Android应用中动态解析方法可达性是一个关键但未解决的问题。现有工具在驱动执行到达特定目标方法（尤其是非图形组件中的方法）方面能力不足，这对于漏洞验证、调试和行为分析等任务至关重要。

Method: GAPS集成了静态、方法引导的调用图分析与动态、交互驱动的执行。系统执行轻量级的调用图后向遍历，通过数据流分析指导，重建到达目标方法的路径，然后将这些路径转换为指导运行时应用探索的指令。

Result: 在AndroTest基准测试中，GAPS静态识别到达88.24%目标方法的路径（平均4.27秒/应用），动态到达57.44%。相比之下，最佳动态工具APE仅达到12.82%，静态工具FlowDroid和DroidReach分别达到58.81%和9.48%。在50个最受欢迎的真实应用中，GAPS静态重建62.03%目标方法的路径（平均278.9秒），动态到达59.86%。

Conclusion: GAPS是第一个集成静态和动态分析来解决Android方法可达性问题的系统，在静态路径识别和动态方法到达方面显著优于现有工具，展示了在实际安全关键代码分析中的实用价值。

Abstract: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.
  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.
  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.
  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

</details>


### [41] [FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation](https://arxiv.org/abs/2511.23302)
*Hengyuan Liu,Zheng Li,Donghua Wang,Yankai Wu,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 提出MBFL-FLIM框架，通过LLM语义分析识别和缓解干扰突变体，显著提升基于突变的故障定位效果


<details>
  <summary>Details</summary>
Motivation: 传统基于突变的故障定位(MBFL)面临干扰突变体的挑战，这些来自非故障代码的突变体会被失败测试杀死，模仿真实故障行为，从而削弱故障定位效果

Method: 1) 提出故障定位干扰突变体(FLIMs)概念并进行RIPR模型理论分析；2) 使用基于LLM的语义分析识别FLIMs，通过微调和置信度估计解决LLM输出不稳定问题；3) 通过精炼可疑度分数来缓解FLIMs影响；4) 集成到MBFL工作流中形成MBFL-FLIM框架

Result: 在Defects4J基准测试的395个程序版本上，使用8个LLM进行实验，MBFL-FLIM在Top-1指标上平均提升44个故障，优于传统SBFL、MBFL方法、动态特征方法和近期LLM故障定位技术，在多故障场景中也表现稳健

Conclusion: MBFL-FLIM框架通过LLM语义分析有效识别和缓解干扰突变体，显著提升基于突变的故障定位效果，为自动化软件调试提供了更可靠的方法

Abstract: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

</details>


### [42] [Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing](https://arxiv.org/abs/2511.23321)
*Yifei Wang,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Yuchen Cao*

Main category: cs.SE

TL;DR: C2C-MoLA是一个结合MoE和LoRA的多模态框架，用于图表到代码生成，通过复杂度感知路由和参数高效调优，在准确率、内存效率和收敛速度上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有图表到代码生成方法在跨类型泛化、内存效率和模块化设计方面存在不足，需要更高效的多模态框架来解决这些挑战。

Method: 提出C2C-MoLA框架，结合混合专家(MoE)和低秩适应(LoRA)。MoE使用复杂度感知路由机制，包含领域专家和负载均衡稀疏门控；LoRA实现参数高效更新；采用定制训练策略对齐路由稳定性和语义准确性。

Result: 在Chart2Code-160k数据集上，相比标准微调和仅LoRA基线，生成准确率提升高达17%，GPU峰值内存降低18%，收敛速度加快20%，尤其在复杂图表上表现突出。

Conclusion: C2C-MoLA通过MoE和LoRA的协同作用，有效解决了图表到代码生成中的泛化、效率和模块化问题，为实际多模态代码生成应用提供了可扩展的解决方案。

Abstract: Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.

</details>
