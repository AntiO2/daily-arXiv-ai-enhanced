{"id": "2512.21347", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21347", "abs": "https://arxiv.org/abs/2512.21347", "authors": ["V\u00edtor Mateus de Brito", "Kleinner Farias"], "title": "Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey", "comment": "4 Figures, 8 Tables, Text in Portuguese", "summary": "The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8c03\u67e546\u540d\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\uff0c\u5b9e\u8bc1\u5206\u6790\u4e86LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91c7\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u5f00\u53d1\u8005\u5bf9LLM\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u4f46\u540c\u65f6\u4e5f\u62c5\u5fe7\u8ba4\u77e5\u4f9d\u8d56\u3001\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u81ea\u4e3b\u6027\u4fb5\u8680\u7b49\u95ee\u9898\u3002", "motivation": "\u968f\u7740LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5feb\u901f\u53d1\u5c55\u548c\u6df1\u5ea6\u878d\u5165\u5f00\u53d1\u8005\u65e5\u5e38\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7406\u89e3\u5176\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u5b66\u672f\u8ba8\u8bba\u4e0e\u771f\u5b9e\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u91c7\u7528\u5b9e\u8bc1\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf946\u540d\u5177\u6709\u4e0d\u540c\u6559\u80b2\u80cc\u666f\u548c\u7ecf\u9a8c\u6c34\u5e73\u7684\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u6536\u96c6\u5173\u4e8eLLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u91c7\u7528\u60c5\u51b5\u7684\u6570\u636e\u3002", "result": "\u8c03\u67e5\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5f00\u53d1\u8005\u5bf9LLM\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5feb\u901f\u89e3\u51b3\u6280\u672f\u95ee\u9898\u3001\u6539\u8fdb\u6587\u6863\u652f\u6301\u548c\u589e\u5f3a\u6e90\u4ee3\u7801\u6807\u51c6\u5316\u65b9\u9762\uff1b2\uff09\u540c\u65f6\u5b58\u5728\u5bf9\u8ba4\u77e5\u4f9d\u8d56\u3001\u5b89\u5168\u98ce\u9669\u548c\u6280\u672f\u81ea\u4e3b\u6027\u4fb5\u8680\u7684\u62c5\u5fe7\uff1b3\uff09\u9700\u8981\u6279\u5224\u6027\u548c\u76d1\u7763\u6027\u5730\u4f7f\u7528LLM\u5de5\u5177\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u4ee5\u6279\u5224\u548c\u76d1\u7763\u7684\u65b9\u5f0f\u4f7f\u7528LLM\u5de5\u5177\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u540c\u65f6\u63a8\u52a8\u4e86\u5173\u4e8eLLM\u8ba4\u77e5\u3001\u4f26\u7406\u548c\u7ec4\u7ec7\u5f71\u54cd\u7684\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2512.21348", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21348", "abs": "https://arxiv.org/abs/2512.21348", "authors": ["Ying Xiao", "Shangwen Wang", "Sicen Liu", "Dingyuan Xue", "Xian Zhan", "Yepang Liu", "Jie M. Zhang"], "title": "Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software", "comment": "Accepted to the 48th International Conference on Software Engineering (ICSE 2026) Research Track", "summary": "Traditional software fairness research typically emphasizes ethical and social imperatives, neglecting that fairness fundamentally represents a core software quality issue arising directly from performance disparities across sensitive user groups. Recognizing fairness explicitly as a software quality dimension yields practical benefits beyond ethical considerations, notably improved predictive performance for unprivileged groups, enhanced out-of-distribution generalization, and increased geographic transferability in real-world deployments. Nevertheless, existing bias mitigation methods face a critical dilemma: while pre-processing methods offer broad applicability across model types, they generally fall short in effectiveness compared to post-processing techniques. To overcome this challenge, we propose Correlation Tuning (CoT), a novel pre-processing approach designed to mitigate bias by adjusting data correlations. Specifically, CoT introduces the Phi-coefficient, an intuitive correlation measure, to systematically quantify correlation between sensitive attributes and labels, and employs multi-objective optimization to address the proxy biases. Extensive evaluations demonstrate that CoT increases the true positive rate of unprivileged groups by an average of 17.5% and reduces three key bias metrics, including statistical parity difference (SPD), average odds difference (AOD), and equal opportunity difference (EOD), by more than 50% on average. CoT outperforms state-of-the-art methods by three and ten percentage points in single attribute and multiple attributes scenarios, respectively. We will publicly release our experimental results and source code to facilitate future research.", "AI": {"tldr": "CoT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6570\u636e\u76f8\u5173\u6027\u6765\u7f13\u89e3\u504f\u89c1\uff0c\u5728\u5355\u5c5e\u6027\u548c\u591a\u5c5e\u6027\u573a\u666f\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f31\u52bf\u7fa4\u4f53\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u516c\u5e73\u6027\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f26\u7406\u548c\u793e\u4f1a\u8d23\u4efb\uff0c\u5ffd\u89c6\u4e86\u516c\u5e73\u6027\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u8f6f\u4ef6\u8d28\u91cf\u95ee\u9898\uff0c\u6e90\u4e8e\u4e0d\u540c\u654f\u611f\u7528\u6237\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002\u5c06\u516c\u5e73\u6027\u660e\u786e\u4f5c\u4e3a\u8f6f\u4ef6\u8d28\u91cf\u7ef4\u5ea6\uff0c\u9664\u4e86\u4f26\u7406\u8003\u91cf\u5916\uff0c\u8fd8\u80fd\u5e26\u6765\u5b9e\u9645\u6548\u76ca\uff0c\u5982\u63d0\u9ad8\u5f31\u52bf\u7fa4\u4f53\u7684\u9884\u6d4b\u6027\u80fd\u3001\u589e\u5f3a\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u548c\u5730\u7406\u53ef\u8fc1\u79fb\u6027\u3002\u73b0\u6709\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u9762\u4e34\u56f0\u5883\uff1a\u9884\u5904\u7406\u65b9\u6cd5\u9002\u7528\u6027\u5e7f\u4f46\u6548\u679c\u4e0d\u5982\u540e\u5904\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCorrelation Tuning (CoT)\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u6570\u636e\u76f8\u5173\u6027\u6765\u7f13\u89e3\u504f\u89c1\u3002\u5177\u4f53\u5305\u62ec\uff1a1) \u5f15\u5165Phi\u7cfb\u6570\u4f5c\u4e3a\u76f4\u89c2\u7684\u76f8\u5173\u6027\u5ea6\u91cf\uff0c\u7cfb\u7edf\u91cf\u5316\u654f\u611f\u5c5e\u6027\u4e0e\u6807\u7b7e\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff1b2) \u4f7f\u7528\u591a\u76ee\u6807\u4f18\u5316\u6765\u89e3\u51b3\u4ee3\u7406\u504f\u89c1\u95ee\u9898\u3002", "result": "CoT\u5c06\u5f31\u52bf\u7fa4\u4f53\u7684\u771f\u9633\u6027\u7387\u5e73\u5747\u63d0\u9ad817.5%\uff0c\u5e76\u5c06\u4e09\u4e2a\u5173\u952e\u504f\u89c1\u6307\u6807\uff08\u7edf\u8ba1\u5947\u5076\u5dee\u5f02SPD\u3001\u5e73\u5747\u51e0\u7387\u5dee\u5f02AOD\u3001\u5e73\u7b49\u673a\u4f1a\u5dee\u5f02EOD\uff09\u5e73\u5747\u964d\u4f4e50%\u4ee5\u4e0a\u3002\u5728\u5355\u5c5e\u6027\u548c\u591a\u5c5e\u6027\u573a\u666f\u4e0b\uff0cCoT\u5206\u522b\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u51fa3\u4e2a\u548c10\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CoT\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u7684\u56f0\u5883\uff0c\u5728\u4fdd\u6301\u5e7f\u6cdb\u9002\u7528\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u5c06\u516c\u5e73\u6027\u4f5c\u4e3a\u8f6f\u4ef6\u8d28\u91cf\u7ef4\u5ea6\u6765\u5904\u7406\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u516c\u5e73\u6027\u6539\u8fdb\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21351", "categories": ["cs.SE", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.21351", "abs": "https://arxiv.org/abs/2512.21351", "authors": ["Santhosh Kumar Ravindran"], "title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation", "comment": "10 pages, 2 figures; Code for Simulation", "summary": "Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.", "AI": {"tldr": "CosmoCore-Evo \u6269\u5c55\u4e86 CosmoCore \u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8fdb\u5316\u7b97\u6cd5\u589e\u5f3a\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\uff0c\u5c06RL\u8f68\u8ff9\u89c6\u4e3a\"\u57fa\u56e0\u7ec4\"\u8fdb\u884c\u7a81\u53d8\u548c\u9009\u62e9\uff0c\u5728\u5206\u5e03\u504f\u79fb\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5bb9\u6613\u9677\u5165\u8bad\u7ec3\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u5bf9\u5206\u5e03\u504f\u79fb\u73af\u5883\uff08\u5982API\u53d8\u66f4\u3001\u65b0\u5e93\u5f15\u5165\uff09\u7684\u9002\u5e94\u80fd\u529b\uff0c\u9700\u8981\u589e\u5f3a\u65b0\u9896\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u57fa\u4e8eCosmoCore\u7684affective dream-replay\u6846\u67b6\uff0c\u5f15\u5165\u8fdb\u5316\u7b97\u6cd5\uff1a\u5c06RL\u8f68\u8ff9\u89c6\u4e3a\"\u57fa\u56e0\u7ec4\"\uff0c\u5728\u591c\u95f4\u56de\u653e\u9636\u6bb5\u8fdb\u884c\u7a81\u53d8\u548c\u9009\u62e9\uff1b\u589e\u5f3aDream Queue\uff0c\u5305\u542b\u9ad8\u9002\u5e94\u5ea6\u8f68\u8ff9\u7684\u7a81\u53d8\u548c\u4f01\u4e1a\u8c03\u4f18\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff08\u6548\u7387\u3001\u5408\u89c4\u6027\u3001\u53ef\u6269\u5c55\u6027\uff09\u3002", "result": "\u5728HumanEval\u53d8\u4f53\u3001BigCodeBench\u548c\u81ea\u5b9a\u4e49PySpark\u7ba1\u9053\u6a21\u62df\u7b49\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCosmoCore-Evo\u76f8\u6bd4\u539f\u59cbCosmoCore\u548cPPO\u3001REAMER\u7b49\u57fa\u7ebf\uff0c\u89e3\u51b3\u65b9\u6848\u65b0\u9896\u6027\u63d0\u9ad835%\uff0c\u9002\u5e94\u901f\u5ea6\u52a0\u5feb25%\u3002", "conclusion": "\u8fdb\u5316\u7ec4\u4ef6\u5728\u5f25\u5408LLM\u4ee3\u7406\u7684\u611f\u77e5\u5dee\u8ddd\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\uff0cCosmoCore-Evo\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u6709\u6548\u589e\u5f3a\u4e86\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u65b0\u9896\u6027\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u504f\u79fb\u73af\u5883\u4e2d\u3002"}}
{"id": "2512.21352", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21352", "abs": "https://arxiv.org/abs/2512.21352", "authors": ["Sumanth Bharadwaj Hachalli Karanam", "Dhiwahar Adhithya Kennady"], "title": "Multi-Agent LLM Committees for Autonomous Software Beta Testing", "comment": null, "summary": "Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u589e\u5f3a\u7684LLM\u534f\u4f5c\u8fdb\u884c\u8f6f\u4ef6\u6d4b\u8bd5\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u624b\u52a8\u8f6f\u4ef6\u6d4b\u8bd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u5355\u667a\u80fd\u4f53LLM\u65b9\u6cd5\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u884c\u4e3a\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u65b9\u6848", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\uff0c\u5305\u542b\u591a\u6837\u5316\u89c6\u89c9\u589e\u5f3aLLM\uff0c\u901a\u8fc7\u4e09\u8f6e\u6295\u7968\u534f\u8bae\u8fbe\u6210\u5171\u8bc6\uff0c\u7ed3\u5408\u6a21\u578b\u591a\u6837\u6027\u3001\u89d2\u8272\u9a71\u52a8\u884c\u4e3a\u53d8\u5316\u548c\u89c6\u89c9\u754c\u9762\u7406\u89e3", "result": "\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u8fbe\u523089.5%\u603b\u4f53\u4efb\u52a1\u6210\u529f\u7387\uff0c2-4\u667a\u80fd\u4f53\u914d\u7f6e\u8fbe\u523091.7-100%\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u534713.7-22.0\u4e2a\u767e\u5206\u70b9\uff1b\u5728WebShop\u4e0a\u8fbe\u523074.7%\u6210\u529f\u7387\uff0cOWASP\u5b89\u5168\u6d4b\u8bd5\u8fbe\u523082.0%\u6210\u529f\u7387\uff0c\u8986\u76d68/10 OWASP Top 10\u6f0f\u6d1e\u7c7b\u522b", "conclusion": "\u591a\u667a\u80fd\u4f53\u59d4\u5458\u4f1a\u6846\u67b6\u663e\u8457\u63d0\u5347\u8f6f\u4ef6\u6d4b\u8bd5\u6548\u679c\uff0c\u652f\u6301\u5b9e\u65f6\u6301\u7eed\u96c6\u6210\u6d4b\u8bd5\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u548c\u5b9e\u9645\u90e8\u7f72"}}
{"id": "2512.21345", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.21345", "abs": "https://arxiv.org/abs/2512.21345", "authors": ["Jasmin Saxer", "Isabella Maria Aigner", "Luise Linzmeier", "Andreas Weiler", "Kurt Stockinger"], "title": "Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks", "comment": "Accepted to the HC@AIxIA + HYDRA 2025", "summary": "Text-to-SQL systems allow non-SQL experts to interact with relational databases using natural language. However, their tendency to generate executable SQL for ambiguous, out-of-scope, or unanswerable queries introduces a hidden risk, as outputs may be misinterpreted as correct. This risk is especially serious in biomedical contexts, where precision is critical. We therefore present Query Carefully, a pipeline that integrates LLM-based SQL generation with explicit detection and handling of unanswerable inputs. Building on the OncoMX component of ScienceBenchmark, we construct OncoMX-NAQ (No-Answer Questions), a set of 80 no-answer questions spanning 8 categories (non-SQL, out-of-schema/domain, and multiple ambiguity types). Our approach employs llama3.3:70b with schema-aware prompts, explicit No-Answer Rules (NAR), and few-shot examples drawn from both answerable and unanswerable questions. We evaluate SQL exact match, result accuracy, and unanswerable-detection accuracy. On the OncoMX dev split, few-shot prompting with answerable examples increases result accuracy, and adding unanswerable examples does not degrade performance. On OncoMX-NAQ, balanced prompting achieves the highest unanswerable-detection accuracy (0.8), with near-perfect results for structurally defined categories (non-SQL, missing columns, out-of-domain) but persistent challenges for missing-value queries (0.5) and column ambiguity (0.3). A lightweight user interface surfaces interim SQL, execution results, and abstentions, supporting transparent and reliable text-to-SQL in biomedical applications.", "AI": {"tldr": "\u63d0\u51faQuery Carefully\u7ba1\u9053\uff0c\u6574\u5408LLM-based SQL\u751f\u6210\u4e0e\u4e0d\u53ef\u56de\u7b54\u8f93\u5165\u7684\u663e\u5f0f\u68c0\u6d4b\u5904\u7406\uff0c\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u6784\u5efaOncoMX-NAQ\u6570\u636e\u96c6\u8bc4\u4f30\u4e0d\u53ef\u56de\u7b54\u68c0\u6d4b\u80fd\u529b", "motivation": "\u73b0\u6709Text-to-SQL\u7cfb\u7edf\u5bf9\u6a21\u7cca\u3001\u8d85\u51fa\u8303\u56f4\u6216\u4e0d\u53ef\u56de\u7b54\u7684\u67e5\u8be2\u4ecd\u4f1a\u751f\u6210\u53ef\u6267\u884cSQL\uff0c\u5b58\u5728\u88ab\u8bef\u8ba4\u4e3a\u6b63\u786e\u7684\u98ce\u9669\uff0c\u8fd9\u5728\u9700\u8981\u7cbe\u786e\u6027\u7684\u751f\u7269\u533b\u5b66\u9886\u57df\u5c24\u4e3a\u4e25\u91cd", "method": "\u4f7f\u7528llama3.3:70b\u6a21\u578b\uff0c\u7ed3\u5408schema-aware\u63d0\u793a\u3001\u663e\u5f0fNo-Answer Rules\u548c\u5305\u542b\u53ef\u56de\u7b54\u4e0e\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\u7684few-shot\u793a\u4f8b\uff0c\u6784\u5efaOncoMX-NAQ\u6570\u636e\u96c6\uff0880\u4e2a\u4e0d\u53ef\u56de\u7b54\u95ee\u9898\uff0c8\u4e2a\u7c7b\u522b\uff09", "result": "\u5728OncoMX\u5f00\u53d1\u96c6\u4e0a\uff0cfew-shot\u63d0\u793a\u63d0\u9ad8\u7ed3\u679c\u51c6\u786e\u7387\uff1b\u5728OncoMX-NAQ\u4e0a\uff0c\u5e73\u8861\u63d0\u793a\u8fbe\u5230\u6700\u9ad8\u4e0d\u53ef\u56de\u7b54\u68c0\u6d4b\u51c6\u786e\u7387\uff080.8\uff09\uff0c\u7ed3\u6784\u5b9a\u4e49\u7c7b\u522b\u8868\u73b0\u4f18\u5f02\u4f46\u7f3a\u5931\u503c\u67e5\u8be2\uff080.5\uff09\u548c\u5217\u6a21\u7cca\uff080.3\uff09\u4ecd\u6709\u6311\u6218", "conclusion": "Query Carefully\u7ba1\u9053\u901a\u8fc7\u663e\u5f0f\u68c0\u6d4b\u548c\u5904\u7406\u4e0d\u53ef\u56de\u7b54\u67e5\u8be2\uff0c\u652f\u6301\u900f\u660e\u53ef\u9760\u7684\u751f\u7269\u533b\u5b66Text-to-SQL\u5e94\u7528\uff0c\u8f7b\u91cf\u7ea7\u7528\u6237\u754c\u9762\u5c55\u793a\u4e2d\u95f4SQL\u3001\u6267\u884c\u7ed3\u679c\u548c\u5f03\u6743\u4fe1\u606f"}}
{"id": "2512.21340", "categories": ["cs.DC", "cs.DB", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21340", "abs": "https://arxiv.org/abs/2512.21340", "authors": ["Dimitrios Amaxilatis", "Themistoklis Sarantakos", "Nikolaos Tsironis", "Souvik Sengupta", "Kostas Ramantas", "Jhofre Ojeda"], "title": "Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum", "comment": null, "summary": "Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.", "AI": {"tldr": "\u667a\u6167\u57ce\u5e02\u91c7\u7528\u6570\u636e\u9a71\u52a8\u67b6\u6784\u63d0\u5347\u57ce\u5e02\u670d\u52a1\u6548\u7387\u3001\u53ef\u6301\u7eed\u6027\u548c\u97e7\u6027", "motivation": "\u667a\u6167\u57ce\u5e02\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u548c\u97e7\u6027\u7684\u57ce\u5e02\u670d\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u57ce\u5e02\u9700\u6c42", "method": "\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u6574\u5408\u57ce\u5e02\u5404\u7cfb\u7edf\u6570\u636e", "result": "\u57ce\u5e02\u670d\u52a1\u6548\u7387\u3001\u53ef\u6301\u7eed\u6027\u548c\u97e7\u6027\u5f97\u5230\u63d0\u5347", "conclusion": "\u6570\u636e\u9a71\u52a8\u67b6\u6784\u662f\u667a\u6167\u57ce\u5e02\u53d1\u5c55\u7684\u5173\u952e\u65b9\u5411"}}
{"id": "2512.21373", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.21373", "abs": "https://arxiv.org/abs/2512.21373", "authors": ["Titouan Duston", "Shuo Xin", "Yang Sun", "Daoguang Zan", "Aoyan Li", "Shulin Xin", "Kai Shen", "Yixiao Chen", "Qiming Sun", "Ge Zhang", "Jiashuo Liu", "Huan Zhou", "Jingkai Liu", "Zhichen Pu", "Yuanheng Wang", "Bo-Xuan Ge", "Xin Tong", "Fei Ye", "Zhi-Chao Zhao", "Wen-Biao Han", "Zhoujian Cao", "Yueran Zhao", "Weiluo Ren", "Qingshen Long", "Yuxiao Liu", "Anni Huang", "Yidi Du", "Yuanyuan Rong", "Jiahao Peng"], "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories", "comment": null, "summary": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.", "AI": {"tldr": "AInsteinBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u771f\u5b9e\u79d1\u7814\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u4f5c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u5f00\u53d1\u4ee3\u7406\u80fd\u529b\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u751f\u4ea7\u7ea7\u79d1\u5b66\u4ee3\u7801\u5e93\u7684\u5b9e\u9645\u7ef4\u62a4\u8005PR\u4efb\u52a1", "motivation": "\u73b0\u6709\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u4fa7\u91cd\u4e8e\u6982\u5ff5\u77e5\u8bc6\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u5f3a\u8c03\u901a\u7528\u529f\u80fd\u5b9e\u73b0\u548c\u95ee\u9898\u89e3\u51b3\uff0c\u7f3a\u4e4f\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u79d1\u7814\u5f00\u53d1\u73af\u5883\u4e2d\u7aef\u5230\u7aef\u80fd\u529b\u7684\u57fa\u51c6", "method": "\u4ece\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u79d1\u5b66\u4ee3\u7801\u5e93\uff08\u91cf\u5b50\u5316\u5b66\u3001\u91cf\u5b50\u8ba1\u7b97\u3001\u5206\u5b50\u52a8\u529b\u5b66\u3001\u6570\u503c\u76f8\u5bf9\u8bba\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u5316\u5b66\u4fe1\u606f\u5b66\uff09\u4e2d\u63d0\u53d6\u7ef4\u62a4\u8005\u7f16\u5199\u7684PR\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u548c\u4e13\u5bb6\u8bc4\u5ba1\u786e\u4fdd\u79d1\u5b66\u6311\u6218\u6027\u3001\u5145\u5206\u6d4b\u8bd5\u8986\u76d6\u548c\u6821\u51c6\u96be\u5ea6", "result": "\u901a\u8fc7\u53ef\u6267\u884c\u73af\u5883\u8bc4\u4f30\u3001\u79d1\u5b66\u6709\u610f\u4e49\u7684\u5931\u8d25\u6a21\u5f0f\u548c\u6d4b\u8bd5\u9a71\u52a8\u9a8c\u8bc1\uff0c\u57fa\u51c6\u80fd\u591f\u6d4b\u91cf\u6a21\u578b\u8d85\u8d8a\u8868\u9762\u4ee3\u7801\u751f\u6210\u3001\u638c\u63e1\u8ba1\u7b97\u79d1\u5b66\u7814\u7a76\u6838\u5fc3\u80fd\u529b\u7684\u80fd\u529b", "conclusion": "AInsteinBench\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u79d1\u7814\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u79d1\u5b66\u8ba1\u7b97\u5f00\u53d1\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177"}}
{"id": "2512.21473", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.21473", "abs": "https://arxiv.org/abs/2512.21473", "authors": ["Chencheng Deng", "Weiling Yang", "Jianbin Fang", "Dezun Dong"], "title": "Demystifying ARM SME to Optimize General Matrix Multiplications", "comment": null, "summary": "General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.", "AI": {"tldr": "MpGEMM\u662f\u4e00\u4e2a\u9488\u5bf9ARM SME\u67b6\u6784\u4f18\u5316\u7684\u5f00\u6e90GEMM\u5e93\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u67b6\u6784\u7279\u6027\u5206\u6790\uff0c\u91c7\u7528\u7f13\u5b58\u611f\u77e5\u5206\u533a\u3001\u9ad8\u6548\u6570\u636e\u6253\u5305\u548c\u4e13\u7528\u5fae\u5185\u6838\u7b49\u6280\u672f\uff0c\u5728Apple M4 Pro\u4e0a\u76f8\u6bd4Apple Accelerate\u5e93\u5e73\u5747\u52a0\u901f1.23\u500d\u3002", "motivation": "\u73b0\u4ee3\u67b6\u6784\u5982ARM SME\u5f15\u5165\u4e86\u4e13\u95e8\u7684\u77e9\u9635\u8fd0\u7b97\u786c\u4ef6\uff0c\u4f46\u73b0\u6709\u7ebf\u6027\u4ee3\u6570\u5e93\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u77e9\u9635\u65f6\u3002\u9700\u8981\u5f00\u53d1\u80fd\u5145\u5206\u5229\u7528SME\u67b6\u6784\u7279\u6027\u7684GEMM\u5e93\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "1. \u5bf9SME\u67b6\u6784\u8fdb\u884c\u7cfb\u7edf\u5316\u7279\u6027\u5206\u6790\uff0c\u63a8\u5bfc\u4f18\u5316\u6307\u5bfc\u539f\u5219\uff1b2. \u91c7\u7528\u7f13\u5b58\u611f\u77e5\u7684\u5206\u533a\u7b56\u7565\uff1b3. \u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u6253\u5305\u548c\u5b9e\u65f6\u8f6c\u7f6e\uff1b4. \u5f00\u53d1\u4e13\u7528\u5fae\u5185\u6838\uff0c\u5229\u7528\u591a\u5411\u91cf\u52a0\u8f7d\u548c\u6240\u6709\u53ef\u7528\u74e6\u7247\u5bc4\u5b58\u5668\u3002", "result": "\u5728Apple M4 Pro\u4e0a\u4f7f\u7528DeepSeek\u548cLLaMA\u7684\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u8bc4\u4f30\uff0cMpGEMM\u76f8\u6bd4\u5382\u5546\u4f18\u5316\u7684Apple Accelerate\u5e93\u5e73\u5747\u52a0\u901f1.23\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "MpGEMM\u901a\u8fc7\u5145\u5206\u5229\u7528ARM SME\u67b6\u6784\u7684\u5173\u952e\u7279\u6027\uff0c\u6210\u529f\u4f18\u5316\u4e86GEMM\u6027\u80fd\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21426", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21426", "abs": "https://arxiv.org/abs/2512.21426", "authors": ["Mohammed Sayagh"], "title": "What Makes a GitHub Issue Ready for Copilot?", "comment": null, "summary": "AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86GitHub issue\u8d28\u91cf\u5bf9AI\u4ee3\u7406\uff08\u5982Copilot\uff09\u5b9e\u73b0\u6210\u529f\u7387\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e8632\u6761\u8be6\u7ec6\u6807\u51c6\u8bc4\u4f30issue\u8d28\u91cf\uff0c\u5e76\u6784\u5efa\u4e86\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4bissue\u80fd\u5426\u4ea7\u751f\u88ab\u5408\u5e76\u7684PR\u3002", "motivation": "AI\u4ee3\u7406\uff08\u5982Copilot\uff09\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u8868\u73b0\u4f9d\u8d56\u4e8e\u8f93\u5165\u8d28\u91cf\uff0c\u4f46\u73b0\u6709GitHub Copilot\u7684\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\u6709\u9650\u4e14\u62bd\u8c61\u3002\u4e0d\u6e05\u695a\u6216\u8303\u56f4\u4e0d\u660e\u786e\u7684issue\u53ef\u80fd\u5bfc\u81f4\u5b9e\u73b0\u5931\u8d25\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u65b9\u6cd5\u8bc4\u4f30issue\u8d28\u91cf\u4ee5\u63d0\u9ad8AI\u4ee3\u7406\u6210\u529f\u7387\u3002", "method": "1. \u6784\u5efa32\u6761\u8be6\u7ec6\u6807\u51c6\u8bc4\u4f30GitHub issue\u8d28\u91cf\uff1b2. \u6bd4\u8f83\u5bfc\u81f4\u5408\u5e76PR\u4e0e\u5173\u95edPR\u7684issue\u5dee\u5f02\uff1b3. \u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4bissue\u4ea7\u751f\u5408\u5e76PR\u7684\u53ef\u80fd\u6027\u3002", "result": "\u6210\u529f\u5408\u5e76\u7684PR\u901a\u5e38\u6765\u81ea\u66f4\u7b80\u77ed\u3001\u8303\u56f4\u660e\u786e\u3001\u63d0\u4f9b\u6e05\u6670\u6307\u5bfc\u548c\u5b9e\u73b0\u63d0\u793a\u7684issue\u3002\u5305\u542b\u5916\u90e8\u5f15\u7528\uff08\u914d\u7f6e\u3001\u73af\u5883\u8bbe\u7f6e\u3001\u4f9d\u8d56\u6216\u5916\u90e8API\uff09\u7684issue\u5408\u5e76\u7387\u8f83\u4f4e\u3002\u6784\u5efa\u7684\u6a21\u578bAUC\u4e2d\u4f4d\u6570\u4e3a72%\uff0c\u80fd\u5e2e\u52a9\u7528\u6237\u6539\u8fdbissue\u8d28\u91cf\u3002", "conclusion": "\u5728AI\u534f\u4f5c\u65f6\u4ee3\uff0c\u7f16\u5199\u9ad8\u8d28\u91cf\u7684GitHub issue\u5e94\u6210\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u7684\u4e00\u7b49\u516c\u6c11\u6d3b\u52a8\u3002\u7814\u7a76\u63ed\u793a\u4e86issue\u8d28\u91cf\u7684\u5173\u952e\u6307\u6807\uff0c\u4e3a\u672a\u6765\u8fdb\u4e00\u6b65\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5316issue\u4ee5\u63d0\u9ad8AI\u4ee3\u7406\u6210\u529f\u7387\u3002"}}
{"id": "2512.21487", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21487", "abs": "https://arxiv.org/abs/2512.21487", "authors": ["Xinglin Pan", "Shaohuai Shi", "Wenxiang Lin", "Yuxin Wang", "Zhenheng Tang", "Wei Wang", "Xiaowen Chu"], "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism", "comment": null, "summary": "The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.\n  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.\n  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.", "AI": {"tldr": "FinDEP\uff1a\u9488\u5bf9MoE\u6a21\u578b\u63a8\u7406\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97/\u901a\u4fe1\u4efb\u52a1\u5212\u5206\u548c\u8c03\u5ea6\uff0c\u5728DEP\u67b6\u6784\u4e0a\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf", "motivation": "MoE\u67b6\u6784\u867d\u7136\u80fd\u4ee5\u4e9a\u7ebf\u6027\u8ba1\u7b97\u589e\u957f\u6269\u5c55\u6a21\u578b\u89c4\u6a21\uff0c\u4f46\u63a8\u7406\u65f6\u7531\u4e8eKV\u7f13\u5b58\u548c\u7a00\u758f\u4e13\u5bb6\u6fc0\u6d3b\u5bfc\u81f4\u5185\u5b58\u5bc6\u96c6\u3002\u73b0\u6709\u7684DEP\u65b9\u6cd5\u867d\u7136\u5c06\u6ce8\u610f\u529b\u673a\u5236\u548c\u4e13\u5bb6\u5206\u914d\u5230\u4e13\u7528GPU\u7ec4\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5171\u4eab\u4e13\u5bb6\u7684\u652f\u6301\u4e14\u4efb\u52a1\u8c03\u5ea6\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "1) \u5c06\u8ba1\u7b97\u548c\u901a\u4fe1\u5212\u5206\u4e3a\u66f4\u5c0f\u7684\u4efb\u52a1\u4ee5\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\uff1b2) \u5236\u5b9a\u652f\u6301\u53ef\u53d8\u7c92\u5ea6\u548c\u987a\u5e8f\u7684\u8c03\u5ea6\u4f18\u5316\u95ee\u9898\uff1b3) \u5f00\u53d1\u9488\u5bf9\u8fd9\u4e2a\u5927\u578b\u641c\u7d22\u7a7a\u95f4\u7684\u9ad8\u6548\u6c42\u89e3\u5668", "result": "\u5728\u56db\u4e2aGPU\u7cfb\u7edf\u4e0a\u4f7f\u7528DeepSeek-V2\u548cQwen3-MoE\u8fdb\u884c\u5b9e\u9a8c\uff0cFinDEP\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c06\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad8\u8fbe1.61\u500d\uff0c\u572832-GPU\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u6700\u9ad81.24\u500d\u7684\u52a0\u901f", "conclusion": "FinDEP\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4efb\u52a1\u8c03\u5ea6\u6709\u6548\u89e3\u51b3\u4e86MoE\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u548c\u8c03\u5ea6\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86DEP\u67b6\u6784\u4e0a\u7684\u63a8\u7406\u6027\u80fd"}}
{"id": "2512.21431", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21431", "abs": "https://arxiv.org/abs/2512.21431", "authors": ["Hridya Dhulipala", "Xiaokai Rong", "Tien N. Nguyen"], "title": "Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors", "comment": null, "summary": "In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.", "AI": {"tldr": "Cerberus\u662f\u4e00\u4e2a\u65e0\u9700\u6267\u884c\u7684\u9884\u6d4b\u6027\u8986\u76d6\u5f15\u5bfc\u6d4b\u8bd5\u6846\u67b6\uff0c\u4f7f\u7528LLM\u751f\u6210\u89e6\u53d1\u8fd0\u884c\u65f6\u9519\u8bef\u7684\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u53cd\u9988\u5faa\u73af\u63d0\u9ad8\u4ee3\u7801\u8986\u76d6\u7387\u548c\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\uff0c\u9700\u8981\u5728\u65e0\u9700\u5b9e\u9645\u6267\u884c\u7684\u60c5\u51b5\u4e0b\u68c0\u6d4b\u4ee3\u7801\u7247\u6bb5\u4e2d\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u548c\u5f02\u5e38\uff0c\u7279\u522b\u662f\u5728\u5c06\u5728\u7ebf\u4ee3\u7801\u7247\u6bb5\u96c6\u6210\u5230\u4ee3\u7801\u5e93\u4e4b\u524d\u8fdb\u884c\u68c0\u6d4b\u3002", "method": "Cerberus\u4f7f\u7528LLM\u751f\u6210\u89e6\u53d1\u8fd0\u884c\u65f6\u9519\u8bef\u7684\u8f93\u5165\uff0c\u5e76\u8fdb\u884c\u4ee3\u7801\u8986\u76d6\u9884\u6d4b\u548c\u9519\u8bef\u68c0\u6d4b\uff0c\u65e0\u9700\u4ee3\u7801\u6267\u884c\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u53cd\u9988\u5faa\u73af\uff1a\u7b2c\u4e00\u9636\u6bb5\u540c\u65f6\u63d0\u9ad8\u4ee3\u7801\u8986\u76d6\u7387\u548c\u68c0\u6d4b\u8fd0\u884c\u65f6\u9519\u8bef\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f53\u8986\u76d6\u7387\u8fbe\u5230100%\u6216\u6700\u5927\u503c\u65f6\uff0c\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "result": "\u7ecf\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cCerberus\u5728\uff08\u4e0d\uff09\u5b8c\u6574\u4ee3\u7801\u7247\u6bb5\u4e0a\u6bd4\u4f20\u7edf\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u6d4b\u8bd5\u6846\u67b6\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u66f4\u9ad8\u6548\u5730\u751f\u6210\u9ad8\u8986\u76d6\u7387\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4ece\u800c\u53d1\u73b0\u66f4\u591a\u8fd0\u884c\u65f6\u9519\u8bef\u3002", "conclusion": "Cerberus\u901a\u8fc7LLM\u9a71\u52a8\u7684\u65e0\u6267\u884c\u6d4b\u8bd5\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u9700\u5b9e\u9645\u6267\u884c\u5373\u53ef\u68c0\u6d4b\u8fd0\u884c\u65f6\u9519\u8bef\u7684\u95ee\u9898\uff0c\u5728\u4ee3\u7801\u8986\u76d6\u7387\u548c\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.21571", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21571", "abs": "https://arxiv.org/abs/2512.21571", "authors": ["Hui Guo", "Qihang Zheng", "Chenghai Huo", "Dongliang Guo", "Haoqi Yang", "Yang Zhang"], "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures", "comment": null, "summary": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.", "AI": {"tldr": "nncase\u662f\u4e00\u4e2a\u5f00\u6e90\u7aef\u5230\u7aef\u7f16\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8ee-graph\u7684\u9879\u91cd\u5199\u5f15\u64ce\u89e3\u51b3\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\u95ee\u9898\uff0c\u7edf\u4e00\u4f18\u5316\u8de8\u4e0d\u540c\u76ee\u6807\u5e73\u53f0\uff0c\u5728Qwen3\u7cfb\u5217\u6a21\u578b\u4e0a\u8d85\u8d8a\u4e3b\u6d41\u6846\u67b6\uff0cCPU\u6027\u80fd\u63a5\u8fd1\u624b\u5de5\u4f18\u5316\u7684llama.cpp\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u53d7\u5230\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\u7684\u963b\u788d\uff0c\u4f20\u7edf\u7f16\u8bd1\u5668\u5b58\u5728\u5de5\u4f5c\u6d41\u788e\u7247\u5316\u548c\u9ad8\u9002\u914d\u6210\u672c\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fanncase\u7f16\u8bd1\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u57fa\u4e8ee-graph\u7684\u9879\u91cd\u5199\u5f15\u64ce\u89e3\u51b3\u9636\u6bb5\u6392\u5e8f\u95ee\u9898\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1aAuto Vectorize\u9002\u914d\u5f02\u6784\u8ba1\u7b97\u5355\u5143\uff0cAuto Distribution\u641c\u7d22\u5e76\u884c\u7b56\u7565\u5e76\u4f18\u5316\u901a\u4fe1\u6210\u672c\uff0cAuto Schedule\u6700\u5927\u5316\u7247\u4e0a\u7f13\u5b58\u5c40\u90e8\u6027\uff0c\u4ee5\u53ca\u7f13\u51b2\u533a\u611f\u77e5\u7684\u4ee3\u7801\u751f\u6210\u9636\u6bb5\u3002", "result": "\u5728Qwen3\u7cfb\u5217\u6a21\u578b\u4e0a\u8d85\u8d8aMLC LLM\u548cIntel IPEX\u7b49\u4e3b\u6d41\u6846\u67b6\uff0c\u5728CPU\u4e0a\u8fbe\u5230\u4e0e\u624b\u5de5\u4f18\u5316\u7684llama.cpp\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "nncase\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u7f16\u8bd1\u5728\u9ad8\u6027\u80fdLLM\u90e8\u7f72\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f02\u6784\u5185\u5b58\u67b6\u6784\u4e0b\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21440", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21440", "abs": "https://arxiv.org/abs/2512.21440", "authors": ["Hridya Dhulipala", "Xiaokai Rong", "Aashish Yadavally", "Tien N. Nguyen"], "title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing", "comment": null, "summary": "In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.", "AI": {"tldr": "FuzzWise\uff1a\u57fa\u4e8eLLM\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u7070\u76d2\u6a21\u7cca\u6d4b\u8bd5\u521d\u59cb\u79cd\u5b50\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u4ee3\u7801\u8986\u76d6\u8bc4\u4f30\uff0c\u5728\u5355\u6d41\u7a0b\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u521d\u59cb\u79cd\u5b50\u96c6", "motivation": "\u4f20\u7edf\u6a21\u7cca\u6d4b\u8bd5\u4e2d\uff0c\u751f\u6210\u5927\u89c4\u6a21\u521d\u59cb\u79cd\u5b50\u96c6\u7136\u540e\u8fdb\u884c\u6700\u5c0f\u5316\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u521d\u59cb\u79cd\u5b50\u96c6\u7684\u65b9\u6cd5\uff0c\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\uff0c\u7279\u522b\u662f\u5728\u7a0b\u5e8f\u6267\u884c\u4e0d\u53ef\u884c\u6216\u4e0d\u5e0c\u671b\u6267\u884c\u7684\u60c5\u51b5\u4e0b\u3002", "method": "FuzzWise\u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a\u7b2c\u4e00\u4e2aLLM\u667a\u80fd\u4f53\u4e3a\u76ee\u6807\u7a0b\u5e8f\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\uff1b\u7b2c\u4e8c\u4e2aLLM\u667a\u80fd\u4f53\u4f5c\u4e3a\u9884\u6d4b\u6027\u4ee3\u7801\u8986\u76d6\u6a21\u5757\uff0c\u8bc4\u4f30\u6bcf\u4e2a\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u662f\u5426\u80fd\u63d0\u5347\u5f53\u524d\u79cd\u5b50\u96c6\u7684\u6574\u4f53\u8986\u76d6\u7387\u3002\u8fd9\u79cd\u6d41\u7ebf\u5316\u6d41\u7a0b\u5141\u8bb8\u6bcf\u4e2a\u65b0\u751f\u6210\u7684\u6d4b\u8bd5\u79cd\u5b50\u7acb\u5373\u88ab\u8bc4\u4f30\u5176\u5bf9\u6574\u4f53\u8986\u76d6\u7387\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cFuzzWise\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u663e\u8457\u5c11\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u5c3d\u7ba1\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u8f83\u5c11\uff0c\u4f46FuzzWise\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4ee3\u7801\u8986\u76d6\u7387\uff0c\u5e76\u89e6\u53d1\u4e86\u66f4\u591a\u7684\u8fd0\u884c\u65f6\u9519\u8bef\u3002\u6b64\u5916\uff0c\u5728\u751f\u6210\u521d\u59cb\u79cd\u5b50\u96c6\u65b9\u9762\uff0cFuzzWise\u66f4\u52a0\u65f6\u95f4\u9ad8\u6548\u548c\u8986\u76d6\u9ad8\u6548\uff0c\u80fd\u591f\u6355\u83b7\u66f4\u591a\u9519\u8bef\u3002", "conclusion": "FuzzWise\u901a\u8fc7\u6574\u5408\u79cd\u5b50\u751f\u6210\u548c\u6700\u5c0f\u5316\u8fc7\u7a0b\uff0c\u5229\u7528LLM\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u5b9e\u9645\u7a0b\u5e8f\u6267\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u521d\u59cb\u79cd\u5b50\u96c6\uff0c\u4e3a\u6a21\u7cca\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u521d\u59cb\u79cd\u5b50\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2512.21615", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.21615", "abs": "https://arxiv.org/abs/2512.21615", "authors": ["Guopeng Li", "Haisheng Tan", "Chi Zhang", "Hongqiu Ni", "Zilong Wang", "Xinyue Zhang", "Yang Xu", "Han Tian"], "title": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments", "comment": "This paper is an English version of Samples Dispatching Mechanism for Accelerating Recommendation Model Training in Edge Intelligent Computing System published in 2025 in the Journal of Computer Research and Development", "summary": "Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.", "AI": {"tldr": "ESD\u673a\u5236\u901a\u8fc7\u4f18\u5316\u8fb9\u7f18\u5de5\u4f5c\u8005\u4e4b\u95f4\u7684\u5d4c\u5165\u6837\u672c\u5206\u53d1\uff0c\u51cf\u5c11\u5d4c\u5165\u4f20\u8f93\u6210\u672c\uff0c\u52a0\u901fDLRM\u8bad\u7ec3", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bad\u7ec3\u6df1\u5ea6\u63a8\u8350\u6a21\u578b\uff08DLRM\uff09\u80fd\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3001\u964d\u4f4e\u5ef6\u8fdf\u5e76\u5b9e\u73b0\u4e2a\u6027\u5316\uff0c\u4f46\u5de8\u5927\u7684\u5d4c\u5165\u8868\u9700\u8981\u53c2\u6570\u670d\u52a1\u5668\u7ef4\u62a4\uff0c\u5bfc\u81f4\u8fb9\u7f18\u5de5\u4f5c\u8005\u4e0e\u670d\u52a1\u5668\u95f4\u5d4c\u5165\u4f20\u8f93\u6210\u672c\u9ad8\u6602\uff0c\u6210\u4e3a\u8bad\u7ec3\u74f6\u9888", "method": "\u63d0\u51faESD\u673a\u5236\uff0c\u901a\u8fc7HybridDis\u5206\u53d1\u51b3\u7b56\u65b9\u6cd5\u4f18\u5316\u8f93\u5165\u5d4c\u5165\u6837\u672c\u5230\u8fb9\u7f18\u5de5\u4f5c\u8005\u7684\u5206\u53d1\uff0cHybridDis\u7ed3\u5408\u8d44\u6e90\u5bc6\u96c6\u578b\u6700\u4f18\u7b97\u6cd5\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5e73\u8861\u51b3\u7b56\u8d28\u91cf\u4e0e\u8d44\u6e90\u6d88\u8017", "result": "\u5b9e\u9a8c\u8868\u660eESD\u51cf\u5c11\u5d4c\u5165\u4f20\u8f93\u6210\u672c\u9ad8\u8fbe36.76%\uff0c\u7aef\u5230\u7aefDLRM\u8bad\u7ec3\u52a0\u901f\u8fbe1.74\u500d", "conclusion": "ESD\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18DLRM\u8bad\u7ec3\u4e2d\u7684\u5d4c\u5165\u4f20\u8f93\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u6837\u672c\u5206\u53d1\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387"}}
{"id": "2512.21511", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21511", "abs": "https://arxiv.org/abs/2512.21511", "authors": ["Takuto Kawamoto", "Yoshiki Higo"], "title": "Code Clone Refactoring in C# with Lambda Expressions", "comment": "8 pages, World Symposium on Software Engineering (WSSE 2025)", "summary": "\"Extract Method\" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.\n  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal \"Extract Method\" approach may vary depending on the programming language.\n  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.\n  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9C#\u8bed\u8a00\u7684\u4ee3\u7801\u514b\u9686\u91cd\u6784\u65b9\u6cd5\uff0c\u4f7f\u7528lambda\u8868\u8fbe\u5f0f\u53c2\u6570\u5316\u884c\u4e3a\u5dee\u5f02\uff0c\u8bc4\u4f30\u663e\u793a35%\u7684\u514b\u9686\u5bf9\u9002\u5408\u91cd\u6784\uff0c\u5176\u4e2d28.9%\u6210\u529f\u91cd\u6784", "motivation": "\u73b0\u6709\"\u63d0\u53d6\u65b9\u6cd5\"\u91cd\u6784\u6280\u672f\u4e3b\u8981\u9488\u5bf9Java\u7a0b\u5e8f\uff0c\u4f7f\u7528\u53c2\u6570\u5316\u65b9\u6cd5\u5904\u7406\u4ee3\u7801\u514b\u9686\u4e2d\u7684\u5dee\u5f02\uff0c\u4f46\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u7279\u6027\uff08\u7279\u522b\u662flambda\u8868\u8fbe\u5f0f\u89c4\u8303\uff09\u4f1a\u5f71\u54cd\u8be5\u6280\u672f\u7684\u9002\u7528\u6027\uff0c\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\uff08\u5982C#\uff09\u5f00\u53d1\u4f18\u5316\u7684\u91cd\u6784\u65b9\u6cd5", "method": "\u63d0\u51faC#\u7279\u5b9a\u7684\u91cd\u6784\u6280\u672f\uff0c\u4f7f\u7528lambda\u8868\u8fbe\u5f0f\u5206\u6790\u548c\u5408\u5e76\u4ee3\u7801\u514b\u9686\u3002\u9996\u5148\u4f7f\u7528NiCad\u514b\u9686\u68c0\u6d4b\u5668\u68c0\u6d4b\u4ee3\u7801\u514b\u9686\uff0c\u7136\u540e\u5e94\u7528\u63d0\u51fa\u7684\u65b9\u6cd5\u5206\u6790\u514b\u9686\u5bf9\uff0c\u5224\u65ad\u662f\u5426\u9002\u5408\u91cd\u6784\uff0c\u5e76\u5bf9\u53ef\u91cd\u6784\u7684\u514b\u9686\u5bf9\u5b9e\u9645\u6267\u884c\u91cd\u6784\u64cd\u4f5c", "result": "\u8bc4\u4f30\u5305\u542b22\u4e2a\u9879\u76ee\u76842,217\u4e2a\u514b\u9686\u5bf9\uff1a35.0%\u7684\u514b\u9686\u5bf9\u88ab\u5224\u5b9a\u9002\u5408\u91cd\u6784\uff0c\u5176\u4e2d28.9%\u6210\u529f\u5b8c\u6210\u91cd\u6784\u3002\u8868\u660e\u63d0\u51fa\u7684C#\u7279\u5b9a\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u4ee3\u7801\u514b\u9686\u91cd\u6784\u95ee\u9898", "conclusion": "\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u9700\u8981\u9488\u5bf9\u6027\u7684\u91cd\u6784\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684C#\u7279\u5b9a\u6280\u672f\u80fd\u6709\u6548\u5229\u7528lambda\u8868\u8fbe\u5f0f\u53c2\u6570\u5316\u884c\u4e3a\u5dee\u5f02\uff0c\u6210\u529f\u91cd\u6784\u76f8\u5f53\u6bd4\u4f8b\u7684\u4ee3\u7801\u514b\u9686\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u7279\u6027\u5bf9\u91cd\u6784\u6280\u672f\u9002\u7528\u6027\u7684\u91cd\u8981\u5f71\u54cd"}}
{"id": "2512.21730", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.21730", "abs": "https://arxiv.org/abs/2512.21730", "authors": ["Linyi Jiang", "Yifei Zhu", "Hao Yin", "Bo Li"], "title": "Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference", "comment": "Accepted for publication in IEEE INFOCOM 2026", "summary": "Recent advancements in array-camera videography enable real-time capturing of ultra-high-definition (Ultra-HD) videos, providing rich visual information in a large field of view. However, promptly processing such data using state-of-the-art transformer-based vision foundation models faces significant computational overhead in on-device computing or transmission overhead in cloud computing. In this paper, we present Hyperion, the first cloud-device collaborative framework that enables low-latency inference on Ultra-HD vision data using off-the-shelf vision transformers over dynamic networks. Hyperion addresses the computational and transmission bottleneck of Ultra-HD vision transformers by exploiting the intrinsic property in vision Transformer models. Specifically, Hyperion integrates a collaboration-aware importance scorer that identifies critical regions at the patch level, a dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under dynamic network conditions, and a weighted ensembler that fuses edge and cloud results to improve accuracy. Experimental results demonstrate that Hyperion enhances frame processing rate by up to 1.61 times and improves the accuracy by up to 20.2% when compared with state-of-the-art baselines under various network environments.", "AI": {"tldr": "Hyperion\u662f\u4e00\u4e2a\u4e91-\u7aef\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u533a\u57df\u3001\u52a8\u6001\u8c03\u6574\u4f20\u8f93\u8d28\u91cf\u548c\u52a0\u6743\u878d\u5408\u7ed3\u679c\uff0c\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\u4e0b\u5bf9\u8d85\u9ad8\u6e05\u89c6\u9891\u7684\u4f4e\u5ef6\u8fdfTransformer\u63a8\u7406\u3002", "motivation": "\u9635\u5217\u76f8\u673a\u89c6\u9891\u6280\u672f\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u8d85\u9ad8\u6e05\u89c6\u9891\uff0c\u4f46\u4f7f\u7528Transformer\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u9762\u4e34\u8ba1\u7b97\u6216\u4f20\u8f93\u5f00\u9500\u5927\u7684\u6311\u6218\u3002\u5728\u8bbe\u5907\u7aef\u8ba1\u7b97\u6709\u8ba1\u7b97\u74f6\u9888\uff0c\u5728\u4e91\u7aef\u8ba1\u7b97\u6709\u4f20\u8f93\u74f6\u9888\u3002", "method": "Hyperion\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u534f\u4f5c\u611f\u77e5\u91cd\u8981\u6027\u8bc4\u5206\u5668\uff0c\u5728\u8865\u4e01\u7ea7\u522b\u8bc6\u522b\u5173\u952e\u533a\u57df\uff1b2) \u52a8\u6001\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u7f51\u7edc\u6761\u4ef6\u81ea\u9002\u5e94\u8c03\u6574\u8865\u4e01\u4f20\u8f93\u8d28\u91cf\u4ee5\u5e73\u8861\u5ef6\u8fdf\u548c\u7cbe\u5ea6\uff1b3) \u52a0\u6743\u96c6\u6210\u5668\uff0c\u878d\u5408\u8fb9\u7f18\u548c\u4e91\u7aef\u7ed3\u679c\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHyperion\u5728\u5404\u79cd\u7f51\u7edc\u73af\u5883\u4e0b\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5e27\u5904\u7406\u7387\u6700\u9ad8\u63d0\u53471.61\u500d\uff0c\u7cbe\u5ea6\u6700\u9ad8\u63d0\u534720.2%\u3002", "conclusion": "Hyperion\u901a\u8fc7\u5229\u7528\u89c6\u89c9Transformer\u6a21\u578b\u7684\u56fa\u6709\u7279\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u9ad8\u6e05\u89c6\u89c9Transformer\u7684\u8ba1\u7b97\u548c\u4f20\u8f93\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u4e91-\u7aef\u534f\u540c\u7684\u4f4e\u5ef6\u8fdf\u63a8\u7406\u6846\u67b6\u3002"}}
{"id": "2512.21555", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21555", "abs": "https://arxiv.org/abs/2512.21555", "authors": ["Qi Hu", "Jiangchao Liu", "Xin Yu", "Lin Zhang", "Edward Jiang"], "title": "XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production", "comment": null, "summary": "As the complexity of mobile applications grows exponentially and the fragmentation of user device environments intensifies, ensuring online application stability faces unprecedented challenges. Traditional methods, such as static logging and post-crash analysis, lack real-time contextual information, rendering them ineffective against \"ghost bugs\" that only manifest in specific scenarios. This highlights an urgent need for dynamic runtime observability: intercepting and tracing arbitrary methods in production without requiring an app release. We propose XTrace, a novel dynamic tracing framework. XTrace introduces a new paradigm of non-invasive proxying, which avoids direct modification of the virtual machine's underlying data structures. It achieves high-performance method interception by leveraging and optimizing the highly stable, built-in instrumentation mechanism of the Android ART virtual machine. Evaluated in a ByteDance application with hundreds of millions of daily active users, XTrace demonstrated production-grade stability and performance. Large-scale online A/B experiments confirmed its stability, showing no statistically significant impact (p > 0.05) on Crash User Rate or ANR rate, while maintaining minimal overhead (<7 ms startup latency, <0.01 ms per-method call) and broad compatibility (Android 5.0-15+). Critically, XTrace diagnosed over 11 severe online crashes and multiple performance bottlenecks, improving root-cause localization efficiency by over 90%. This confirms XTrace provides a production-grade solution that reconciles the long-standing conflict between stability and comprehensive coverage in Android dynamic tracing.", "AI": {"tldr": "XTrace\u662f\u4e00\u4e2aAndroid\u52a8\u6001\u8ffd\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u4fb5\u5165\u5f0f\u4ee3\u7406\u548c\u4f18\u5316ART\u865a\u62df\u673a\u7684\u5185\u7f6e\u63d2\u6869\u673a\u5236\uff0c\u5b9e\u73b0\u751f\u4ea7\u73af\u5883\u4e2d\u4efb\u610f\u65b9\u6cd5\u7684\u9ad8\u6027\u80fd\u62e6\u622a\u548c\u8ffd\u8e2a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6355\u83b7\"\u5e7d\u7075bug\"\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u5e94\u7528\u590d\u6742\u6027\u6307\u6570\u7ea7\u589e\u957f\u548c\u8bbe\u5907\u73af\u5883\u788e\u7247\u5316\u52a0\u5267\uff0c\u786e\u4fdd\u5728\u7ebf\u5e94\u7528\u7a33\u5b9a\u6027\u9762\u4e34\u524d\u6240\u672a\u6709\u7684\u6311\u6218\u3002\u4f20\u7edf\u9759\u6001\u65e5\u5fd7\u8bb0\u5f55\u548c\u5d29\u6e83\u540e\u5206\u6790\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u65f6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u53ea\u5728\u7279\u5b9a\u573a\u666f\u51fa\u73b0\u7684\"\u5e7d\u7075bug\"\uff0c\u8feb\u5207\u9700\u8981\u52a8\u6001\u8fd0\u884c\u65f6\u53ef\u89c2\u6d4b\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "XTrace\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u4fb5\u5165\u5f0f\u4ee3\u7406\u8303\u5f0f\uff0c\u907f\u514d\u76f4\u63a5\u4fee\u6539\u865a\u62df\u673a\u5e95\u5c42\u6570\u636e\u7ed3\u6784\u3002\u5b83\u901a\u8fc7\u5229\u7528\u548c\u4f18\u5316Android ART\u865a\u62df\u673a\u5185\u7f6e\u7684\u9ad8\u5ea6\u7a33\u5b9a\u7684\u63d2\u6869\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u65b9\u6cd5\u62e6\u622a\u3002\u8be5\u6846\u67b6\u652f\u6301\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u62e6\u622a\u548c\u8ffd\u8e2a\u4efb\u610f\u65b9\u6cd5\uff0c\u65e0\u9700\u53d1\u5e03\u65b0\u7248\u672c\u5e94\u7528\u3002", "result": "\u5728\u62e5\u6709\u6570\u4ebf\u65e5\u6d3b\u7528\u6237\u7684\u5b57\u8282\u8df3\u52a8\u5e94\u7528\u4e2d\u8bc4\u4f30\u663e\u793a\uff0cXTrace\u5177\u5907\u751f\u4ea7\u7ea7\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002\u5927\u89c4\u6a21\u5728\u7ebfA/B\u5b9e\u9a8c\u8bc1\u5b9e\u5176\u7a33\u5b9a\u6027\uff0c\u5bf9\u5d29\u6e83\u7528\u6237\u7387\u548cANR\u7387\u65e0\u663e\u8457\u5f71\u54cd(p>0.05)\uff0c\u540c\u65f6\u4fdd\u6301\u6781\u4f4e\u5f00\u9500(<7ms\u542f\u52a8\u5ef6\u8fdf\uff0c<0.01ms\u6bcf\u6b21\u65b9\u6cd5\u8c03\u7528)\u548c\u5e7f\u6cdb\u517c\u5bb9\u6027(Android 5.0-15+)\u3002\u6210\u529f\u8bca\u65ad\u4e8611\u4e2a\u4ee5\u4e0a\u4e25\u91cd\u5728\u7ebf\u5d29\u6e83\u548c\u591a\u4e2a\u6027\u80fd\u74f6\u9888\uff0c\u5c06\u6839\u56e0\u5b9a\u4f4d\u6548\u7387\u63d0\u5347\u8d85\u8fc790%\u3002", "conclusion": "XTrace\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u4ea7\u7ea7\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u8c03\u548c\u4e86Android\u52a8\u6001\u8ffd\u8e2a\u4e2d\u957f\u671f\u5b58\u5728\u7684\u7a33\u5b9a\u6027\u4e0e\u5168\u9762\u8986\u76d6\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4e3a\u79fb\u52a8\u5e94\u7528\u7a33\u5b9a\u6027\u4fdd\u969c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a8\u6001\u8fd0\u884c\u65f6\u53ef\u89c2\u6d4b\u6027\u5de5\u5177\u3002"}}
{"id": "2512.21835", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.21835", "abs": "https://arxiv.org/abs/2512.21835", "authors": ["Mingyu Sun", "Xiao Zhang", "Shen Qu", "Yan Li", "Mengbai Xiao", "Yuan Yuan", "Dongxiao Yu"], "title": "LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices", "comment": "15 pages, 18 figures", "summary": "Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\\times$ and 3.7$\\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.", "AI": {"tldr": "LIME\u662f\u4e00\u4e2a\u652f\u6301\u591a\u5185\u5b58\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u65e0\u635f\u5927\u6a21\u578b\u63a8\u7406\u7684\u534f\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u9519\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u6a21\u578b\u5378\u8f7d\u6280\u672f\uff0c\u5728\u6709\u9650\u7f51\u7edc\u5e26\u5bbd\u4e0b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9762\u4e34\u5de8\u5927\u6311\u6218\uff1a\u53c2\u6570\u89c4\u6a21\u5927\u3001\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u800c\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u548c\u5185\u5b58\u6709\u9650\uff0c\u7f51\u7edc\u5e26\u5bbd\u74f6\u9888\u9650\u5236\u4e86\u5206\u5e03\u5f0f\u90e8\u7f72\u548c\u5b9e\u65f6\u54cd\u5e94\u3002\u73b0\u6709\u8f7b\u91cf\u5316\u4f18\u5316\u6280\u672f\u5f80\u5f80\u5bfc\u81f4\u6a21\u578b\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51faLIME\u7cfb\u7edf\uff0c\u91c7\u7528\u4ea4\u9519\u6d41\u6c34\u7ebf\u5e76\u884c\u7ed3\u5408\u6a21\u578b\u5378\u8f7d\u6280\u672f\uff0c\u52a8\u6001\u5e73\u8861\u8ba1\u7b97\u548c\u901a\u4fe1\u3002\u5f15\u5165\u7ec6\u7c92\u5ea6\u79bb\u7ebf\u5206\u914d\u8c03\u5ea6\u5668\u548c\u5728\u7ebf\u5185\u5b58\u9002\u5e94\u7b56\u7565\uff0c\u4f18\u5316\u8bbe\u5907\u8ba1\u7b97\u548c\u5b58\u50a8\u8d44\u6e90\uff0c\u6700\u5c0f\u5316\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u56db\u4e2a\u5f02\u6784Nvidia Jetson\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72LLaMA3.3-70B-Instruct\u6a21\u578b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5728\u96f6\u661f\u548c\u7a81\u53d1\u8bf7\u6c42\u6a21\u5f0f\u4e0b\u5206\u522b\u5b9e\u73b01.7\u500d\u548c3.7\u500d\u7684\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5931\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "LIME\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5927\u6a21\u578b\u63a8\u7406\u7684\u8d44\u6e90\u7ea6\u675f\u95ee\u9898\uff0c\u901a\u8fc7\u534f\u4f5c\u5f0f\u67b6\u6784\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.21591", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21591", "abs": "https://arxiv.org/abs/2512.21591", "authors": ["Shuo Sun", "Shixin Zhang", "Jiwei Yan", "Jun Yan", "Jian Zhang"], "title": "Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code", "comment": "Accepted by FSE 2026", "summary": "Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \\methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \\methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \\methodName~significantly outperformed prior works, achieving a \\textit{TypeSim} score of 0.89 and a \\textit{TypeExact} score of 0.84, representing a 27\\% and 40\\% relative improvement over the strongest baseline. More importantly, \\methodName~removed new type errors introduced by the tool by 92.7\\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u4ed3\u5e93\u7ea7Python\u7c7b\u578b\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u578b\u4e0e\u4f9d\u8d56\u7684\u534f\u540c\u6f14\u5316\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u63a8\u65ad", "motivation": "Python\u52a8\u6001\u7c7b\u578b\u673a\u5236\u5bfc\u81f4\u8fd0\u884c\u65f6\u7c7b\u578b\u9519\u8bef\uff0c\u73b0\u6709\u5de5\u5177\u96be\u4ee5\u5904\u7406\u4ed3\u5e93\u7ea7\u522b\u7684\u590d\u6742\u8de8\u8fc7\u7a0b\u4f9d\u8d56\u5173\u7cfb", "method": "\u6784\u5efa\u5b9e\u4f53\u4f9d\u8d56\u56fe(EDG)\u5efa\u6a21\u4ed3\u5e93\u7ea7\u7c7b\u578b\u4f9d\u8d56\uff0c\u91c7\u7528\u8fed\u4ee3\u5f0f\u7c7b\u578b\u63a8\u65ad\uff0c\u7c7b\u578b\u4e0e\u4f9d\u8d56\u534f\u540c\u6f14\u5316\uff0c\u7ed3\u5408\u7c7b\u578b\u68c0\u67e5\u5668\u5b9e\u65f6\u9a8c\u8bc1", "result": "\u572812\u4e2a\u590d\u6742Python\u4ed3\u5e93\u4e0a\uff0cTypeSim\u5f97\u52060.89\uff0cTypeExact\u5f97\u52060.84\uff0c\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u5206\u522b\u63d0\u534727%\u548c40%\uff0c\u51cf\u5c1192.7%\u7684\u5de5\u5177\u5f15\u5165\u65b0\u7c7b\u578b\u9519\u8bef", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63a8\u8fdb\u4e86\u771f\u5b9e\u4e16\u754cPython\u5f00\u53d1\u7684\u81ea\u52a8\u5316\u3001\u53ef\u9760\u7c7b\u578b\u6807\u6ce8"}}
{"id": "2512.21884", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.21884", "abs": "https://arxiv.org/abs/2512.21884", "authors": ["Tingyang Sun", "Ting He", "Bo Ji", "Parimal Parag"], "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models", "comment": null, "summary": "Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5206\u5e03\u5f0fLLM\u63a8\u7406\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6027\u80fd\u9884\u6d4b\u6a21\u578b\u3001\u4f18\u5316\u7b97\u6cd5\u548c\u5728\u7ebf\u9002\u5e94\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9700\u8981\u9ad8\u7aefGPU\uff0c\u6210\u672c\u9ad8\u6602\u3002PETALS\u7b49\u5206\u5e03\u5f0f\u7cfb\u7edf\u901a\u8fc7\u8de8\u591a\u4e2a\u4f4e\u7aefGPU\u670d\u52a1\u5668\u5206\u5272\u6a21\u578b\u5757\u6765\u964d\u4f4e\u90e8\u7f72\u95e8\u69db\uff0c\u4f46\u5176\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u8d44\u6e90\u5206\u914d\uff0c\u800c\u5982\u4f55\u6700\u4f18\u5206\u914d\u4ecd\u672a\u77e5\u3002", "method": "1) \u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6027\u80fd\u6a21\u578b\u9884\u6d4b\u7ed9\u5b9a\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\u51b3\u7b56\u4e0b\u7684\u63a8\u7406\u6027\u80fd\uff1b2) \u5c06\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\u79bb\u7ebf\u4f18\u5316\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u8bc1\u660e\u5176NP-hard\u5e76\u8bbe\u8ba1\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u7b97\u6cd5\uff1b3) \u5c06\u79bb\u7ebf\u7b97\u6cd5\u9002\u5e94\u5728\u7ebf\u8bbe\u7f6e\uff0c\u5728\u8d1f\u8f7d\u6709\u754c\u65f6\u4fdd\u6301\u76f8\u540c\u6027\u80fd\u4fdd\u8bc1\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u4eff\u771f\u8868\u660e\uff0c\u6240\u63d0\u89e3\u51b3\u65b9\u6848\u5728\u591a\u79cd\u5730\u7406\u5206\u5e03\u5f0f\u670d\u52a1\u5668\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6848\u80fd\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002\u8fd8\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7CPU-only\u6a21\u62df\u5668\uff0c\u53ef\u5728\u6709\u9650GPU\u8bbf\u95ee\u4e0b\u8bc4\u4f30\u5927\u89c4\u6a21\u90e8\u7f72\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5206\u5e03\u5f0fLLM\u63a8\u7406\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u4e3a\u9ad8\u6548\u5206\u5e03\u5f0fLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5668\u964d\u4f4e\u4e86\u672a\u6765\u7814\u7a76\u7684\u95e8\u69db\u3002"}}
{"id": "2512.21757", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21757", "abs": "https://arxiv.org/abs/2512.21757", "authors": ["Huiyun Peng", "Antonio Zhong", "Ricardo Andr\u00e9s Calvo M\u00e9ndez", "Kelechi G. Kalu", "James C. Davis"], "title": "How Do Agents Perform Code Optimization? An Empirical Study", "comment": null, "summary": "Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\\% vs. 63.6\\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.", "AI": {"tldr": "\u9996\u4e2a\u6bd4\u8f83AI\u4ee3\u7406\u4e0e\u4eba\u7c7b\u5728\u6027\u80fd\u4f18\u5316\u63d0\u4ea4\u65b9\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u53d1\u73b0AI\u751f\u6210\u7684PR\u5728\u6027\u80fd\u9a8c\u8bc1\u65b9\u9762\u4e0d\u5982\u4eba\u7c7b\u5145\u5206\uff0c\u4f46\u4f18\u5316\u6a21\u5f0f\u76f8\u4f3c\u3002", "motivation": "\u5c3d\u7ba1AI\u7f16\u7801\u4ee3\u7406\u5728\u4ee3\u7801\u751f\u6210\u548c\u9519\u8bef\u4fee\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u5b9e\u8bc1\u7814\u7a76\u6765\u8bc4\u4f30AI\u4ee3\u7406\u5728\u6027\u80fd\u4f18\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86324\u4e2aAI\u751f\u6210\u7684PR\u548c83\u4e2a\u4eba\u7c7b\u7f16\u5199\u7684PR\uff0c\u4ece\u91c7\u7eb3\u7387\u3001\u53ef\u7ef4\u62a4\u6027\u3001\u4f18\u5316\u6a21\u5f0f\u548c\u9a8c\u8bc1\u5b9e\u8df5\u56db\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\u3002", "result": "AI\u751f\u6210\u7684\u6027\u80fd\u4f18\u5316PR\u5305\u542b\u660e\u786e\u6027\u80fd\u9a8c\u8bc1\u7684\u6bd4\u4f8b\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0845.7% vs. 63.6%\uff0cp=0.007\uff09\uff0c\u4f46\u4e24\u8005\u4f7f\u7528\u7684\u4f18\u5316\u6a21\u5f0f\u57fa\u672c\u76f8\u540c\u3002", "conclusion": "AI\u4ee3\u7406\u5728\u6027\u80fd\u4f18\u5316\u65b9\u9762\u4e0e\u4eba\u7c7b\u6709\u76f8\u4f3c\u7684\u4f18\u5316\u6a21\u5f0f\uff0c\u4f46\u5728\u6027\u80fd\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u4e3a\u6539\u8fdbAI\u4ee3\u7801\u4f18\u5316\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b9\u5411\u548c\u673a\u4f1a\u3002"}}
{"id": "2512.21967", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.21967", "abs": "https://arxiv.org/abs/2512.21967", "authors": ["Deniz Elbek", "Kamer Kaya"], "title": "BLEST: Blazingly Efficient BFS using Tensor Cores", "comment": "13 pages, 3 figures, 4 tables, 3 algorithms, 46 references", "summary": "Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\\times$, $4.64\\times$ and $4.9\\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.", "AI": {"tldr": "BLEST\u662f\u4e00\u4e2a\u5229\u7528GPU Tensor Core\u52a0\u901fBFS\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f4d\u56fe\u7ed3\u6784\u3001\u8d1f\u8f7d\u5747\u8861\u3001\u56fe\u91cd\u6392\u5e8f\u548c\u6279\u5904\u7406SpMSpV\u7b49\u6280\u672f\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u56fe\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3GPU\u7684Tensor Core\u867d\u7136\u8ba1\u7b97\u541e\u5410\u91cf\u6781\u9ad8\uff0c\u4f46\u4e13\u4e3a\u5bc6\u96c6\u77e9\u9635\u8fd0\u7b97\u8bbe\u8ba1\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8eBFS\u7b49\u4e0d\u89c4\u5219\u3001\u975e\u7ed3\u6784\u5316\u7684\u56fe\u8ba1\u7b97\u3002\u9700\u8981\u89e3\u51b3\u8d1f\u8f7d\u4e0d\u5747\u8861\u3001\u5197\u4f59\u8ba1\u7b97\u548c\u540c\u6b65\u5f00\u9500\u7b49\u95ee\u9898\u3002", "method": "1. \u57fa\u4e8e\u4f4d\u56fe\u7684\u7ed3\u6784\u548c\u6267\u884c\u5e03\u5c40\u91cd\u6784pull-based BFS\u6d41\u6c34\u7ebf\uff1b2. \u5f15\u5165\u4e8c\u503c\u5316\u865a\u62df\u5207\u7247\u96c6(BVSS)\u5b9e\u73b0warp\u7ea7\u8d1f\u8f7d\u5747\u8861\uff1b3. \u4e24\u79cd\u56fe\u91cd\u6392\u5e8f\u7b56\u7565\uff1a\u793e\u4ea4\u56fe\u7528\u538b\u7f29\u5bfc\u5411\u6392\u5e8f\uff0c\u975e\u793e\u4ea4\u56fe\u7528\u5e26\u5bbd\u51cf\u5c11\u6392\u5e8f\uff1b4. \u6279\u5904\u7406SpMSpV\u4e58\u6cd5\u6a21\u5f0f\u5229\u7528\u4f4d\u8fd0\u7b97TC\u74e6\u7247\uff1b5. \u5185\u6838\u878d\u5408\u548c\u60f0\u6027\u9876\u70b9\u66f4\u65b0\u51cf\u5c11\u540c\u6b65\u548c\u539f\u5b50\u5f00\u9500\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u56fe\u4e0a\uff0cBLEST\u5e73\u5747\u6bd4BerryBees\u5feb3.58\u500d\uff0c\u6bd4Gunrock\u5feb4.64\u500d\uff0c\u6bd4GSWITCH\u5feb4.9\u500d\u3002", "conclusion": "BLEST\u6210\u529f\u5c06GPU Tensor Core\u5e94\u7528\u4e8eBFS\u8ba1\u7b97\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8d1f\u8f7d\u5747\u8861\u3001\u56fe\u91cd\u6392\u5e8f\u548c\u8ba1\u7b97\u6a21\u5f0f\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u89c4\u5219\u56fe\u8ba1\u7b97\u7684\u6027\u80fd\u3002"}}
{"id": "2512.21781", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21781", "abs": "https://arxiv.org/abs/2512.21781", "authors": ["Abdul Ali Bangash", "Tongxu Ge", "Zhimin Zhao", "Arshdeep Singh", "Zitao Wang", "Bram Adams"], "title": "The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX", "comment": null, "summary": "A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9SBOM\uff08\u8f6f\u4ef6\u7269\u6599\u6e05\u5355\uff09\u7684\u4e24\u79cd\u4e3b\u6d41\u683c\u5f0fSPDX\u548cCycloneDX\u7684\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u8fdb\u884c\u4e86\u91cf\u5316\u6bd4\u8f83\uff0c\u5206\u6790\u4e86170\u4e2a\u516c\u5f00\u5de5\u5177\u300136,990\u4e2a\u95ee\u9898\u62a5\u544a\u4ee5\u53ca\u9876\u7ea7\u5f00\u6e90\u9879\u76ee\uff0c\u63ed\u793a\u4e86\u4e24\u4e2a\u751f\u6001\u7cfb\u7edf\u7684\u4e0d\u540c\u7279\u70b9\u548c\u4e92\u8865\u4f18\u52bf\u3002", "motivation": "SBOM\u7684\u91c7\u7528\u4f9d\u8d56\u4e8e\u5de5\u5177\u751f\u6001\u7cfb\u7edf\uff0c\u4f46\u76ee\u524dSPDX\u548cCycloneDX\u4e24\u79cd\u4e3b\u6d41\u683c\u5f0f\u7684\u751f\u6001\u7cfb\u7edf\u5728\u6210\u719f\u5ea6\u3001\u5de5\u5177\u652f\u6301\u548c\u793e\u533a\u53c2\u4e0e\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6bd4\u8f83\u7814\u7a76\u6765\u6307\u5bfc\u5f00\u53d1\u8005\u548c\u5b9e\u8df5\u8005\u3002", "method": "1. \u5bf9170\u4e2a\u516c\u5f00\u5ba3\u4f20\u7684SBOM\u5de5\u5177\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\uff1b2. \u5206\u6790171\u4e2aCycloneDX\u5de5\u5177\u548c470\u4e2aSPDX\u5de5\u5177\u7684\u5065\u5eb7\u6307\u6807\uff1b3. \u6bd4\u8f8336,990\u4e2a\u5f00\u6e90\u5de5\u5177\u7684\u95ee\u9898\u62a5\u544a\uff1b4. \u8c03\u67e5\u6bcf\u4e2a\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u524d250\u4e2a\u5f00\u6e90\u9879\u76ee\u5e76\u6bd4\u8f83\u5176\u5065\u5eb7\u6307\u6807\u3002", "result": "CycloneDX\u5de5\u5177\u9879\u76ee\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u5f00\u53d1\u8005\u53c2\u4e0e\u5ea6\u548c\u67d0\u4e9b\u5065\u5eb7\u6307\u6807\uff0c\u800cSPDX\u5de5\u5177\u5219\u53d7\u76ca\u4e8e\u66f4\u6210\u719f\u7684\u751f\u6001\u7cfb\u7edf\u3001\u66f4\u5e7f\u6cdb\u7684\u5de5\u5177\u53ef\u7528\u6027\u548c\u5df2\u5efa\u7acb\u7684\u884c\u4e1a\u91c7\u7528\u3002\u4e24\u4e2a\u751f\u6001\u7cfb\u7edf\u5404\u6709\u4f18\u52bf\uff0c\u5b58\u5728\u4e92\u8865\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u8005\u3001\u8d21\u732e\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5173\u4e8e\u8fd9\u4e24\u4e2a\u751f\u6001\u7cfb\u7edf\u4e92\u8865\u4f18\u52bf\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u76f8\u4e92\u589e\u5f3a\u7684\u673a\u4f1a\u3002CycloneDX\u5728\u5f00\u53d1\u8005\u53c2\u4e0e\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800cSPDX\u5728\u751f\u6001\u7cfb\u7edf\u6210\u719f\u5ea6\u548c\u884c\u4e1a\u91c7\u7528\u65b9\u9762\u66f4\u6709\u4f18\u52bf\u3002"}}
{"id": "2512.22035", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22035", "abs": "https://arxiv.org/abs/2512.22035", "authors": ["Yanmeng Wang", "Zhiwen Dai", "Shuai Wang", "Jian Zhou", "Fu Xiao", "Tony Q. S. Quek", "Tsung-Hui Chang"], "title": "Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View", "comment": null, "summary": "Federated Fine-Tuning (FFT) has attracted growing interest as it leverages both server- and client-side data to enhance global model generalization while preserving privacy, and significantly reduces the computational burden on edge devices by avoiding training from scratch. Despite these advantages, FFT performance is often degraded by unreliable server-client connections and heterogeneous client data distributions. Most existing methods assume homogeneous network conditions or require prior knowledge of connection failures. However, these assumptions are impractical in real-world networks characterized by diverse communication standards (e.g., wired, Wi-Fi, 4G, and 5G) and heterogeneous failure patterns. To address these limitations, we propose FedAuto, a novel FFT framework that mitigates the combined effects of connection failures and data heterogeneity via adaptive aggregation. FedAuto operates without prior knowledge of network conditions or modifications to existing infrastructure, enabling seamless plug-and-play deployment. Moreover, we establish a rigorous convergence guarantee for FedAuto. By adopting a novel per-round aggregation perspective, our analysis removes the need for assumptions on connection failures probabilities or client selection strategies commonly imposed in prior work, and guarantees convergence of FedAuto for each individual realization, providing a stronger theoretical assurance. Extensive experiments demonstrate that FedAuto consistently outperforms state-of-the-art baselines under diverse connection failure scenarios for both full-parameter and partial-parameter fine-tuning (e.g., LoRA), and even surpasses strategies that rely on complex communication resource optimization.", "AI": {"tldr": "FedAuto\uff1a\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u7f51\u7edc\u77e5\u8bc6\u7684\u81ea\u9002\u5e94\u805a\u5408\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u6709\u6548\u5e94\u5bf9\u8fde\u63a5\u5931\u8d25\u548c\u6570\u636e\u5f02\u6784\u95ee\u9898\uff0c\u5177\u6709\u4e25\u683c\u7684\u6536\u655b\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u8054\u90a6\u5fae\u8c03\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u540c\u8d28\u7f51\u7edc\u6761\u4ef6\u6216\u9700\u8981\u8fde\u63a5\u5931\u8d25\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f46\u5728\u73b0\u5b9e\u7f51\u7edc\u4e2d\uff08\u6709\u7ebf\u3001Wi-Fi\u30014G\u30015G\u6df7\u5408\uff09\u8fd9\u4e9b\u5047\u8bbe\u4e0d\u5207\u5b9e\u9645\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d", "method": "\u63d0\u51faFedAuto\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u5408\u7f13\u89e3\u8fde\u63a5\u5931\u8d25\u548c\u6570\u636e\u5f02\u6784\u7684\u8054\u5408\u5f71\u54cd\uff0c\u65e0\u9700\u7f51\u7edc\u6761\u4ef6\u5148\u9a8c\u77e5\u8bc6\u6216\u57fa\u7840\u8bbe\u65bd\u4fee\u6539\uff0c\u652f\u6301\u5373\u63d2\u5373\u7528\u90e8\u7f72", "result": "FedAuto\u5728\u5404\u79cd\u8fde\u63a5\u5931\u8d25\u573a\u666f\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u57fa\u7ebf\uff08\u5305\u62ec\u5168\u53c2\u6570\u548c\u90e8\u5206\u53c2\u6570\u5fae\u8c03\u5982LoRA\uff09\uff0c\u751a\u81f3\u8d85\u8d8a\u4f9d\u8d56\u590d\u6742\u901a\u4fe1\u8d44\u6e90\u4f18\u5316\u7684\u7b56\u7565", "conclusion": "FedAuto\u4e3a\u8054\u90a6\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u5408\u548c\u4e25\u683c\u7684\u6bcf\u8f6e\u6536\u655b\u4fdd\u8bc1\uff0c\u89e3\u51b3\u4e86\u73b0\u5b9e\u7f51\u7edc\u4e2d\u7684\u8fde\u63a5\u5931\u8d25\u548c\u6570\u636e\u5f02\u6784\u95ee\u9898"}}
{"id": "2512.21811", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.21811", "abs": "https://arxiv.org/abs/2512.21811", "authors": ["Qiaolin Qin", "Jianchen Zhao", "Heng Li", "Weiyi Shang", "Ettore Merlo"], "title": "A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation", "comment": "Accepted at the 33rd IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER), Research Papers track, 2026", "summary": "Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.", "AI": {"tldr": "\u63d0\u51faPMSS\uff0c\u4e00\u79cd\u65e0\u9700\u6807\u7b7e\u7684\u65e5\u5fd7\u89e3\u6790\u5668\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u805a\u7c7b\u8d28\u91cf\u5206\u6790\u66ff\u4ee3\u4f20\u7edf\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u7684\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65e5\u5fd7\u89e3\u6790\u5668\u8bc4\u4f30\u6307\u6807\u4e25\u91cd\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u8bc4\u4f30\u8303\u56f4\uff0c\u4e14\u4e0d\u540c\u7248\u672c\u7684\u771f\u5b9e\u6807\u7b7e\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u7684\u8bc4\u4f30\u7ed3\u8bba", "method": "\u63d0\u51faPMSS\u6307\u6807\uff0c\u4f7f\u7528\u4e2d\u5fc3\u70b9\u8f6e\u5ed3\u5206\u6790\u548cLevenshtein\u8ddd\u79bb\u8bc4\u4f30\u89e3\u6790\u5668\u7684\u5206\u7ec4\u8d28\u91cf\u548c\u6a21\u677f\u8d28\u91cf\uff0c\u5177\u6709\u8fd1\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6", "result": "PMSS\u4e0e\u6807\u7b7e\u6307\u6807FGA/FTA\u663e\u8457\u6b63\u76f8\u5173\uff0cPMSS\u6700\u4f73\u89e3\u6790\u5668\u4e0eFGA\u6700\u4f73\u89e3\u6790\u5668\u6027\u80fd\u5dee\u5f02\u4ec52.1%\uff0c\u63d0\u4f9b\u4e86\u65e0\u6807\u7b7e\u8bc4\u4f30\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848", "conclusion": "PMSS\u4e3a\u65e5\u5fd7\u89e3\u6790\u5668\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u6307\u6807\uff0c\u7279\u522b\u9002\u7528\u4e8e\u771f\u5b9e\u6807\u7b7e\u4e0d\u4e00\u81f4\u6216\u4e0d\u53ef\u7528\u7684\u60c5\u51b5"}}
{"id": "2512.22036", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22036", "abs": "https://arxiv.org/abs/2512.22036", "authors": ["Zhuoran Zhu", "Chunyang Zhu", "Hao Lin", "Xu Fu", "Yiming Zhou", "Quanlu Zhang", "Zhenhua Li", "Feng Qian", "Chao Yu", "Boxun Li", "Guohao Dai", "Yu Wang"], "title": "FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion", "comment": null, "summary": "Large-scale Mixture-of-Experts (MoE) models rely on \\emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\\times$ and 2.01$\\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\\times$ and 1.10-1.19$\\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\\times$ and 1.06-1.16$\\times$.", "AI": {"tldr": "FUSCO\u662f\u4e00\u4e2a\u4e13\u4e3aMoE\u6a21\u578b\u8bbe\u8ba1\u7684\u901a\u4fe1\u5e93\uff0c\u901a\u8fc7\u878d\u5408\u6570\u636e\u8f6c\u6362\u548c\u901a\u4fe1\u6765\u89e3\u51b3\u4e13\u5bb6\u5e76\u884c\u4e2d\u7684\u6570\u636e\u4f20\u8f93\u74f6\u9888\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21MoE\u6a21\u578b\u4f9d\u8d56\u4e13\u5bb6\u5e76\u884c\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u901a\u4fe1\u5e93\u5904\u7406\u6570\u636e\u6df7\u6d17\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5176\u5f00\u9500\u53ef\u5360\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u7684\u4e00\u534a\u4ee5\u4e0a\u3002MoE\u7684\u4e13\u5bb6\u4e3b\u6570\u636e\u5e03\u5c40\u4e0e\u901a\u4fe1\u64cd\u4f5c\u671f\u671b\u7684\u8bbe\u5907\u4e3b\u5e03\u5c40\u5b58\u5728\u51b2\u7a81\uff0c\u5bfc\u81f4\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u3002", "method": "FUSCO\u901a\u8fc7\u878d\u5408\u6570\u636e\u8f6c\u6362\u548c\u901a\u4fe1\u5b9e\u73b0\u9ad8\u6548\u8f7b\u91cf\u7684\u6570\u636e\u6df7\u6d17\u3002\u5b83\u6355\u83b7\u7ec6\u7c92\u5ea6\u6570\u636e\u5e03\u5c40\uff0c\u7531\u6d41\u6c34\u7ebf\u901a\u4fe1\u5f15\u64ce\u5728\u901a\u4fe1\u8def\u5f84\u4e0a\u9ad8\u6548\u6267\u884c\u6df7\u6d17\u64cd\u4f5c\u3002\u8f7b\u91cf\u7ea7\u89c4\u5212\u548c\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u6d88\u9664\u5197\u4f59\u901a\u4fe1\u5e76\u5206\u6563\u6d41\u91cf\u3002", "result": "\u5728\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFUSCO\u76f8\u6bd4NCCL\u548cDeepEP\u5206\u522b\u5b9e\u73b0\u6700\u9ad83.84\u500d\u548c2.01\u500d\u7684\u52a0\u901f\u3002\u5728\u7aef\u5230\u7aefMoE\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4NCCL\u548cDeepEP\uff0cFUSCO\u5206\u522b\u964d\u4f4e\u8bad\u7ec3\u5ef6\u8fdf1.17-1.39\u500d\u548c1.10-1.19\u500d\uff0c\u964d\u4f4e\u63a8\u7406\u4e2d\u9996\u4ee4\u724c\u751f\u6210\u5ef6\u8fdf1.09-1.25\u500d\u548c1.06-1.16\u500d\u3002", "conclusion": "FUSCO\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9MoE\u901a\u4fe1\u6a21\u5f0f\u4f18\u5316\u7684\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u901a\u4fe1\u5e93\u5728\u5904\u7406\u4e13\u5bb6\u5e76\u884c\u6570\u636e\u6df7\u6d17\u65f6\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2512.21818", "categories": ["cs.SE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.21818", "abs": "https://arxiv.org/abs/2512.21818", "authors": ["Brian Bowers", "Smita Khapre", "Jugal Kalita"], "title": "Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development", "comment": null, "summary": "Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u53d1\u73b0coder-reviewer-tester\u67b6\u6784\u6bd4coder\u548ccoder-tester\u67b6\u6784\u66f4\u5b89\u5168\u4f46\u6548\u7387\u8f83\u4f4e\uff0c\u901a\u8fc7\u6dfb\u52a0\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u53ef\u63d0\u9ad8\u6548\u7387\u540c\u65f6\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u4f46\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u672c\u8eab\u4ecd\u6613\u53d7\u9ad8\u7ea7\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u4f53AI\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5373\u5c06\u4e3b\u5bfc\u4ea7\u4e1a\u548c\u793e\u4f1a\uff0c\u8fd9\u4e9b\u7531\u76ee\u6807\u9a71\u52a8\u81ea\u4e3b\u6027\u7684\u7cfb\u7edf\u4ee3\u8868\u4e86\u751f\u6210\u5f0fAI\u7684\u5f3a\u5927\u5f62\u5f0f\uff0c\u4ece\u88ab\u52a8\u5185\u5bb9\u751f\u6210\u8f6c\u5411\u4e3b\u52a8\u591a\u4efb\u52a1\u80fd\u529b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u81ea\u4e3b\u8bbe\u8ba1\u548c\u7f3a\u4e4f\u4eba\u5de5\u76d1\u7763\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u65e0\u6cd5\u81ea\u884c\u8bc6\u522b\u548c\u5e94\u5bf9\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u5206\u6790\u5176\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u73b0\u9636\u6bb5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u67b6\u6784\uff0c\u5e76\u5efa\u7acb\u5168\u9762\u7684\u5a01\u80c1\u6a21\u578b\u3002\u5206\u6790\u4e0d\u540c\u67b6\u6784\uff08coder\u3001coder-tester\u3001coder-reviewer-tester\uff09\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u901a\u8fc7\u6dfb\u52a0\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u6765\u6539\u8fdb\u7cfb\u7edf\u3002\u6f14\u793a\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u5bf9\u9ad8\u7ea7\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "result": "coder-reviewer-tester\u67b6\u6784\u6bd4coder\u548ccoder-tester\u67b6\u6784\u66f4\u5177\u5f39\u6027\u4f46\u7f16\u7801\u6548\u7387\u8f83\u4f4e\u3002\u6dfb\u52a0\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u53ef\u7f13\u89e3\u6548\u7387\u635f\u5931\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u5f39\u6027\u3002\u7136\u800c\uff0c\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u672c\u8eab\u6613\u53d7\u9ad8\u7ea7\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\uff0c\u5728\u6ce8\u5165\u4ee3\u7801\u4e2d\u5d4c\u5165\u6709\u6bd2few-shot\u793a\u4f8b\u53ef\u5c06\u653b\u51fb\u6210\u529f\u7387\u4ece0%\u63d0\u9ad8\u523071.95%\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u7136\u80fd\u51c6\u786e\u751f\u6210\u4ee3\u7801\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u7279\u522b\u662f\u4ee3\u7801\u6ce8\u5165\u653b\u51fb\u3002coder-reviewer-tester\u67b6\u6784\u66f4\u5b89\u5168\u4f46\u6548\u7387\u8f83\u4f4e\uff0c\u5b89\u5168\u5206\u6790\u667a\u80fd\u4f53\u80fd\u6539\u5584\u8fd9\u4e00\u5e73\u8861\u4f46\u4ecd\u5b58\u5728\u6f0f\u6d1e\u3002\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4fdd\u62a4\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u514d\u53d7\u9ad8\u7ea7\u653b\u51fb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.22113", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22113", "abs": "https://arxiv.org/abs/2512.22113", "authors": ["Shengkun Cui", "Rahul Krishna", "Saurabh Jha", "Ravishankar K. Iyer"], "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications", "comment": null, "summary": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.", "AI": {"tldr": "PRAXIS\u662f\u4e00\u4e2a\u7528\u4e8e\u8bca\u65ad\u4e91\u4e8b\u6545\u7684\u667a\u80fd\u7f16\u6392\u5668\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u56fe\u904d\u5386\u65b9\u6cd5\u5206\u6790\u670d\u52a1\u4f9d\u8d56\u548c\u4ee3\u7801\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u6839\u56e0\u5206\u6790\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4e91\u4e8b\u6545\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\uff08\u5e73\u5747\u6bcf\u5c0f\u65f6\u8d85\u8fc7200\u4e07\u7f8e\u5143\uff09\uff0c\u800c\u4ee3\u7801\u548c\u914d\u7f6e\u95ee\u9898\u662f\u4e91\u4e8b\u6545\u7684\u4e3b\u8981\u6839\u56e0\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bca\u65ad\u8fd9\u7c7b\u4e8b\u6545\u65f6\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u52a8\u5316\u8bca\u65ad\u5de5\u5177\u3002", "method": "PRAXIS\u91c7\u7528LLM\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u56fe\u904d\u5386\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u79cd\u4f9d\u8d56\u56fe\uff1a1\uff09\u670d\u52a1\u4f9d\u8d56\u56fe\uff08SDG\uff09\u6355\u83b7\u5fae\u670d\u52a1\u7ea7\u4f9d\u8d56\uff1b2\uff09\u7a0b\u5e8f\u4f9d\u8d56\u56fe\uff08PDG\uff09\u6355\u83b7\u4ee3\u7801\u7ea7\u4f9d\u8d56\u3002LLM\u4f5c\u4e3a\u904d\u5386\u7b56\u7565\u5728\u8fd9\u4e9b\u56fe\u4e0a\u79fb\u52a8\uff0c\u5b9a\u4f4d\u548c\u89e3\u91ca\u6545\u969c\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684ReAct\u57fa\u7ebf\u65b9\u6cd5\uff0cPRAXIS\u5c06\u6839\u56e0\u5206\u6790\u51c6\u786e\u6027\u63d0\u5347\u6700\u9ad8\u8fbe3.1\u500d\uff0c\u540c\u65f6\u5c06token\u6d88\u8017\u964d\u4f4e3.8\u500d\u3002\u572830\u4e2a\u771f\u5b9e\u4e16\u754c\u4e91\u4e8b\u6545\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8fd9\u4e9b\u6570\u636e\u6b63\u5728\u88ab\u7f16\u8bd1\u4e3aRCA\u57fa\u51c6\u3002", "conclusion": "PRAXIS\u901a\u8fc7\u667a\u80fd\u7f16\u6392LLM\u9a71\u52a8\u7684\u56fe\u904d\u5386\uff0c\u6709\u6548\u8bca\u65ad\u4ee3\u7801\u548c\u914d\u7f6e\u5f15\u8d77\u7684\u4e91\u4e8b\u6545\uff0c\u663e\u8457\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3a\u4e91\u4e8b\u6545\u6839\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.22043", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22043", "abs": "https://arxiv.org/abs/2512.22043", "authors": ["Zhangbo Long", "Letian Sha", "Jiaye Pan", "Dongpeng Xu", "Yifei Huang", "Fu Xiao"], "title": "HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules", "comment": null, "summary": "Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u4e8c\u8fdb\u5236\u7a0b\u5e8f\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u6838\u6a21\u5757\u6269\u5c55\u4f20\u7edf\u52a8\u6001\u4e8c\u8fdb\u5236\u63d2\u6869\u80fd\u529b\uff0c\u91c7\u7528\u8fdb\u7a0b\u7a7a\u6d1e\u6280\u672f\u548c\u5bb9\u5668\u73af\u5883\u5b9e\u73b0\u89e3\u8026\u5206\u6790\uff0c\u63d0\u9ad8\u7ec6\u7c92\u5ea6\u5206\u6790\u7684\u53ef\u7528\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u4e8c\u8fdb\u5236\u7a0b\u5e8f\u5206\u6790\u5728\u7cfb\u7edf\u5b89\u5168\u4e2d\u4ecd\u7136\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u7ec6\u7c92\u5ea6\u5206\u6790\uff08\u5982\u52a8\u6001\u6c61\u70b9\u5206\u6790\uff09\u5b58\u5728\u90e8\u7f72\u56f0\u96be\u3001\u5185\u5b58\u4f7f\u7528\u9ad8\u3001\u6027\u80fd\u5f00\u9500\u5927\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u7684\u5206\u6790\u573a\u666f\uff08\u5982\u5185\u5b58\u7834\u574f\u5229\u7528\u548c\u6c99\u7bb1\u9003\u9038\u6076\u610f\u8f6f\u4ef6\uff09\u3002", "method": "1. \u4f7f\u7528\u5185\u6838\u6a21\u5757\u6269\u5c55\u4f20\u7edf\u52a8\u6001\u4e8c\u8fdb\u5236\u63d2\u6869\u7684\u5206\u6790\u80fd\u529b\uff1b2. \u57fa\u4e8e\u89e3\u8026\u5206\u6790\u601d\u60f3\uff0c\u901a\u8fc7\u8fdb\u7a0b\u7a7a\u6d1e\u6280\u672f\u5728\u5bb9\u5668\u8fdb\u7a0b\u4e2d\u6784\u5efa\u5206\u6790\u73af\u5883\uff1b3. \u5728Windows\u5e73\u53f0\u4e0a\u5b9e\u73b0\u539f\u578b\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u7a0b\u5e8f\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u5b9e\u9645\u5229\u7528\u7a0b\u5e8f\u548c\u6076\u610f\u4ee3\u7801\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u590d\u7528\u73b0\u6709\u52a8\u6001\u4e8c\u8fdb\u5236\u63d2\u6869\u5e73\u53f0\u7684\u529f\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u76ee\u6807\u7a0b\u5e8f\u6267\u884c\u7684\u5f71\u54cd\uff0c\u63d0\u9ad8\u4e86\u7ec6\u7c92\u5ea6\u4e8c\u8fdb\u5236\u7a0b\u5e8f\u5206\u6790\u7684\u53ef\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2512.22054", "categories": ["cs.SE", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22054", "abs": "https://arxiv.org/abs/2512.22054", "authors": ["Giuseppe De Palma", "Saverio Giallorenzo"], "title": "Proceedings First Workshop on Adaptable Cloud Architectures", "comment": null, "summary": "This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.", "AI": {"tldr": "WACA 2025\u7814\u8ba8\u4f1a\u8bba\u6587\u96c6\uff0c\u805a\u7126\u53ef\u9002\u5e94\u4e91\u67b6\u6784\uff0c\u4e0eDisCoTec 2025\u8054\u5408\u4e3e\u529e", "motivation": "\u968f\u7740\u4e91\u8ba1\u7b97\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u63a2\u7d22\u80fd\u591f\u9002\u5e94\u52a8\u6001\u53d8\u5316\u9700\u6c42\u3001\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u4e91\u67b6\u6784\u8bbe\u8ba1\u65b9\u6cd5", "method": "\u901a\u8fc7\u7814\u8ba8\u4f1a\u5f62\u5f0f\u6c47\u96c6\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u4e13\u5bb6\uff0c\u5206\u4eab\u6700\u65b0\u7814\u7a76\u6210\u679c\u548c\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u5305\u62ec\u8bba\u6587\u5f81\u96c6\u3001\u540c\u884c\u8bc4\u5ba1\u3001\u4f1a\u8bae\u62a5\u544a\u548c\u8bba\u6587\u96c6\u51fa\u7248", "result": "\u6210\u529f\u4e3e\u529e\u4e86WACA 2025\u7814\u8ba8\u4f1a\uff0c\u6536\u96c6\u5e76\u51fa\u7248\u4e86\u5173\u4e8e\u53ef\u9002\u5e94\u4e91\u67b6\u6784\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\uff0c\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u5b66\u672f\u4ea4\u6d41", "conclusion": "WACA 2025\u4e3a\u53ef\u9002\u5e94\u4e91\u67b6\u6784\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u4ea4\u6d41\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u8bba\u6587\u96c6\u8bb0\u5f55\u4e86\u5f53\u524d\u7814\u7a76\u8fdb\u5c55\u548c\u672a\u6765\u65b9\u5411"}}
