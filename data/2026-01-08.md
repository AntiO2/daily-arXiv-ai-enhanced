<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [The Pneuma Project: Reifying Information Needs as Relational Schemas to Automate Discovery, Guide Preparation, and Align Data with Intent](https://arxiv.org/abs/2601.03618)
*Muhammad Imam Luthfi Balaka,Raul Castro Fernandez*

Main category: cs.DB

TL;DR: Pneuma-Seeker是一个基于语言模型的系统，通过迭代交互帮助用户表达和满足模糊、演化的信息需求，将需求具体化为关系数据模型，并逐步收敛生成可用文档。


<details>
  <summary>Details</summary>
Motivation: 数据发现和准备是数据管理生命周期中的持续瓶颈，特别是当用户意图模糊、演化或难以操作化时。需要一种系统来帮助用户表达和满足信息需求。

Method: 系统结合三种架构思想：上下文专业化以减少LLM在子任务中的负担，指挥式规划器组装动态执行计划，以及基于共享状态的收敛机制。整合了RAG、代理框架和结构化数据准备技术，支持半自动、语言引导的工作流。

Result: 通过LLM用户模拟评估显示，系统能够帮助发现潜在意图、引导探索并生成适合用途的文档。同时作为新兴文档层，捕获机构知识并支持组织记忆。

Conclusion: Pneuma-Seeker通过语言模型驱动的平台，有效解决了数据发现和准备中的意图表达问题，支持迭代交互和渐进收敛，为模糊信息需求提供了实用的解决方案。

Abstract: Data discovery and preparation remain persistent bottlenecks in the data management lifecycle, especially when user intent is vague, evolving, or difficult to operationalize. The Pneuma Project introduces Pneuma-Seeker, a system that helps users articulate and fulfill information needs through iterative interaction with a language model-powered platform. The system reifies the user's evolving information need as a relational data model and incrementally converges toward a usable document aligned with that intent. To achieve this, the system combines three architectural ideas: context specialization to reduce LLM burden across subtasks, a conductor-style planner to assemble dynamic execution plans, and a convergence mechanism based on shared state. The system integrates recent advances in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation to support semi-automatic, language-guided workflows. We evaluate the system through LLM-based user simulations and show that it helps surface latent intent, guide discovery, and produce fit-for-purpose documents. It also acts as an emergent documentation layer, capturing institutional knowledge and supporting organizational memory.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication](https://arxiv.org/abs/2601.03390)
*Daniel Qian,Xiyu Hao,Jinkun Geng,Yuncheng Yao,Aurojit Panda,Jinyang Li,Anirudh Sivaraman*

Main category: cs.DC

TL;DR: Aspen是一个无领导者的拜占庭容错协议，通过基于时钟同步和网络延迟估计的排序层，在无竞争条件下实现近最优的2Δ+ε延迟，即使在网络延迟不可预测时也能保持快速路径。


<details>
  <summary>Details</summary>
Motivation: 随着BFT协议在面向用户的许可区块链应用（如支付）中使用，提供低延迟变得至关重要。现有的无领导者乐观快速路径协议在无竞争时能实现2Δ延迟，但并发竞争请求会导致副本分歧并触发昂贵的恢复过程。

Method: Aspen采用无领导者架构，通过基于松散同步时钟和网络延迟估计的最佳努力排序层来移除无竞争条件。协议需要n=3f+2p+1个副本，其中2p个额外副本允许快速路径在最多p个副本因网络延迟而分歧时继续执行。当乐观条件不满足时，回退到PBFT风格协议。

Result: 在广域分布式副本实验中，Aspen在75毫秒内提交请求，相比先前协议有1.2到3.3倍的改进，同时支持每秒19,000个请求。

Conclusion: Aspen通过创新的排序层设计，在保持拜占庭容错的同时实现了近最优的低延迟，为面向用户的区块链应用提供了高效的共识解决方案。

Abstract: As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2Δ$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.
  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2Δ+ \varepsilon$, where $\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\times$ improvement compared to previous protocols, while supporting 19,000 requests per second.

</details>


### [3] [Majorum: Ebb-and-Flow Consensus with Dynamic Quorums](https://arxiv.org/abs/2601.03862)
*Francesco D'Amato,Roberto Saltini,Thanh-Hai Tran,Yann Vonlanthen,Luca Zanolini*

Main category: cs.DC

TL;DR: Majorum是一个ebb-and-flow协议，结合了动态可用性协议和部分同步最终性协议，在乐观条件下只需3个时隙即可最终确定区块，每个时隙仅需一次投票阶段。


<details>
  <summary>Details</summary>
Motivation: 动态可用性协议虽然能在诚实参与者离线后重新加入时保持活跃性，但无法在网络分区或长时间异步情况下提供强安全性保证。需要结合部分同步最终性协议来解决这一问题。

Method: Majorum采用ebb-and-flow架构，动态可用性组件基于基于仲裁的协议(TOB-SVD)，结合部分同步最终性协议，在乐观条件下每个时隙只需一次投票阶段。

Result: 在乐观条件下，Majorum只需3个时隙即可最终确定区块，每个时隙仅需一次投票阶段，能够连续最终确定扩展前一个已最终确定区块的新区块。

Conclusion: Majorum通过ebb-and-flow架构有效结合了动态可用性和最终性保证，在保持高效性的同时提供了更强的安全性，特别适合需要应对参与者动态变化的共识场景。

Abstract: Dynamic availability is the ability of a consensus protocol to remain live despite honest participants going offline and later rejoining. A well-known limitation is that dynamically available protocols, on their own, cannot provide strong safety guarantees during network partitions or extended asynchrony. Ebb-and-flow protocols [SP21] address this by combining a dynamically available protocol with a partially synchronous finality protocol that irrevocably finalizes a prefix.
  We present Majorum, an ebb-and-flow construction whose dynamically available component builds on a quorum-based protocol (TOB-SVD). Under optimistic conditions, Majorum finalizes blocks in as few as three slots while requiring only a single voting phase per slot. In particular, when conditions remain favourable, each slot finalizes the next block extending the previously finalized one.

</details>


### [4] [A Scheduling Framework for Efficient MoE Inference on Edge GPU-NDP Systems](https://arxiv.org/abs/2601.03992)
*Qi Wu,Chao Fang,Jiayuan Chen,Ye Lin,Yueqi Zhang,Yichuan Bai,Yuan Du,Li Du*

Main category: cs.DC

TL;DR: 提出一个针对边缘GPU-NDP系统的MoE推理优化框架，通过张量并行、负载均衡调度和无数据集预取策略，实现2.41倍平均加速


<details>
  <summary>Details</summary>
Motivation: MoE模型在边缘部署时面临三大挑战：1) NDP单元间的严重负载不均衡；2) NDP单元内GPU利用率不足；3) 需要大量数据预分析来应对不可预测的专家激活模式

Method: 提出三方面优化：1) 利用MoE推理中未充分探索的张量并行，将大型专家参数分区到多个NDP单元并行计算；2) 负载均衡感知调度算法在NDP单元和GPU间分配专家计算；3) 无数据集预取策略主动加载频繁访问的专家

Result: 实验结果显示，该框架使GPU-NDP系统在端到端延迟上相比最先进方法平均加速2.41倍，最高达2.56倍

Conclusion: 该框架有效解决了MoE在边缘GPU-NDP系统部署的关键挑战，显著提升了资源受限环境下的推理效率

Abstract: Mixture-of-Experts (MoE) models facilitate edge deployment by decoupling model capacity from active computation, yet their large memory footprint drives the need for GPU systems with near-data processing (NDP) capabilities that offload experts to dedicated processing units. However, deploying MoE models on such edge-based GPU-NDP systems faces three critical challenges: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation within NDP units, and 3) extensive data pre-profiling necessitated by unpredictable expert activation patterns for pre-fetching. To address these challenges, this paper proposes an efficient inference framework featuring three key optimizations. First, the underexplored tensor parallelism in MoE inference is exploited to partition and compute large expert parameters across multiple NDP units simultaneously towards edge low-batch scenarios. Second, a load-balancing-aware scheduling algorithm distributes expert computations across NDP units and GPU to maximize resource utilization. Third, a dataset-free pre-fetching strategy proactively loads frequently accessed experts to minimize activation delays. Experimental results show that our framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches, significantly enhancing MoE inference efficiency in resource-constrained environments.

</details>


### [5] [Hummingbird: SLO-Oriented GPU Preemption at Microsecond-scale](https://arxiv.org/abs/2601.04071)
*Tiancheng Hu,Chenxi Wang,Ting Cao,Jin Qin,Lei Chen,Xinyu Xiao,Junhao Hu,Hongliang Tian,Shoumeng Yan,Huimin Cui,Quan Chen,Tao Xie*

Main category: cs.DC

TL;DR: Hummingbird是一个面向SLO的GPU调度系统，通过在闭源GPU上实现微秒级抢占并有效利用空闲GPU时间片，显著提高了高优先级任务的SLO达成率和GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 现有GPU共享技术（包括空间和时间共享）在提高利用率的同时难以同时保证SLO遵守和最大化效率，主要原因是闭源GPU上缺乏细粒度任务调度能力。

Method: Hummingbird通过在闭源GPU上实现微秒级抢占机制，能够有效收割空闲GPU时间片，实现细粒度的任务调度。

Result: 相比最先进的空间和时间共享方法，Hummingbird将高优先级任务的SLO达成率分别提高了9.7倍和3.5倍；与独占执行相比，高优先级任务与低优先级任务共同运行时SLO达成率仅下降不到1%；低优先级任务吞吐量比最先进的时间共享方法高出2.4倍。

Conclusion: Hummingbird在保证SLO的同时显著提升了GPU利用率，为解决闭源GPU上的细粒度调度问题提供了有效方案。

Abstract: Existing GPU-sharing techniques, including spatial and temporal sharing, aim to improve utilization but face challenges in simultaneously ensuring SLO adherence and maximizing efficiency due to the lack of fine-grained task scheduling on closed-source GPUs. This paper presents Hummingbird, an SLO-oriented GPU scheduling system that overcomes these challenges by enabling microsecond-scale preemption on closed-source GPUs while effectively harvesting idle GPU time slices. Comprehensive evaluations across diverse GPU architectures reveal that Hummingbird improves the SLO attainment of high-priority tasks by 9.7x and 3.5x compared to the state-of-the-art spatial and temporal-sharing approaches. When compared to executing exclusively, the SLO attainment of the high-priority task, collocating with low-priority tasks on Hummingbird, only drops by less than 1%. Meanwhile, the throughput of the low-priority task outperforms the state-of-the-art temporal-sharing approaches by 2.4x. Hummingbird demonstrates significant effectiveness in ensuring the SLO while enhancing GPU utilization.

</details>


### [6] [Failure-Resilient and Carbon-Efficient Deployment of Microservices over the Cloud-Edge Continuum](https://arxiv.org/abs/2601.04123)
*Francisco Ponce,Simone Gazza,Andrea D'Iapico,Roberto Amadini,Antonio Brogi,Stefano Forti,Saverio Giallorenzo,Pierluigi Plebani,Davide Usai,Monica Vitali,Gianluigi Zavattaro,Jacopo Soldani*

Main category: cs.DC

TL;DR: FREEDA工具链用于在云边连续体上自动化部署微服务应用，平衡容错性、性能和环保目标，通过动态调整部署配置来适应变化条件并减少碳排放。


<details>
  <summary>Details</summary>
Motivation: 在异构动态的云边基础设施上部署微服务应用需要平衡相互冲突的目标，如故障恢复能力、性能和环境可持续性。现有解决方案难以同时满足这些需求。

Method: 开发FREEDA工具链，持续适应变化的操作条件、资源可用性和可持续性约束，通过迁移服务、调整配置选择和重新平衡工作负载来自主重新配置部署。

Result: FREEDA能够自主重新配置部署，成功在弹性、效率和环境影响之间实现最优平衡，有效应对资源耗尽、节点故障和碳强度波动等现实挑战。

Conclusion: FREEDA工具链展示了在云边连续体上实现故障恢复和碳效率部署的可行性，为平衡微服务应用的弹性、性能和可持续性提供了有效解决方案。

Abstract: Deploying microservice-based applications (MSAs) on heterogeneous and dynamic Cloud-Edge infrastructures requires balancing conflicting objectives, such as failure resilience, performance, and environmental sustainability. In this article, we introduce the FREEDA toolchain, designed to automate the failure-resilient and carbon-efficient deployment of MSAs over the Cloud-Edge Continuum.
  The FREEDA toolchain continuously adapts deployment configurations to changing operational conditions, resource availability, and sustainability constraints, aiming to maintain the MSA quality and service continuity while reducing carbon emissions. We also introduce an experimental suite using diverse simulated and emulated scenarios to validate the effectiveness of the toolchain against real-world challenges, including resource exhaustion, node failures, and carbon intensity fluctuations. The results demonstrate FREEDA's capability to autonomously reconfigure deployments by migrating services, adjusting flavour selections, or rebalancing workloads, successfully achieving an optimal balance among resilience, efficiency, and environmental impact.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [The Anatomy of a Successful Student Scrum Team: Motivation, Personalities, and Academic Adaptation](https://arxiv.org/abs/2601.03364)
*Nadia Damianova,Santiago Berrezueta-Guzman*

Main category: cs.SE

TL;DR: 研究分析学生团队在混合工作环境下如何调整Scrum实践以适应学术时间表，发现轻量级工具协调、一周冲刺和灵活仪式有助于平衡敏捷方法与学业要求。


<details>
  <summary>Details</summary>
Motivation: 虽然敏捷方法和Scrum在软件工程教育中被广泛教授，但缺乏关于这些实践在长期运行、学生主导的项目中如何适应学术和混合工作约束的实证证据。

Method: 进行为期一年的案例研究，分析8人学生开发团队如何调整Scrum实践以适应学期节奏、考试、旅行和兼职可用性。使用Discord、Notion和GitHub的定性观察和工件，以及贡献指标和自定义沟通有效性指数(0.76/1.00)来评估三个维度。

Result: 研究发现：(1)轻量级工具协调即使在远程期间也能实现稳定进展；(2)一周冲刺和灵活仪式有助于协调Scrum与学术义务；(3)共享动机、角色清晰度和兼容的工作风格与流程机制同样重要。

Conclusion: 为在混合、项目式学习环境中采用敏捷方法的教师和学生团队提出实用建议，强调需要调整Scrum实践以适应学术时间表和混合工作约束。

Abstract: Agile methods, and Scrum in particular, are widely taught in software engineering education; however, there is limited empirical evidence on how these practices function in long-running, student-led projects under academic and hybrid work constraints. This paper presents a year-long case study of an eight-person student development team tasked with designing and implementing a virtual reality game that simulates a university campus and provides program-related educational content. We analyze how the team adapted Scrum practices (sprint structure, roles, backlog management) to fit semester rhythms, exams, travel, and part-time availability, and how communication and coordination were maintained in a hybrid on-site/remote environment. Using qualitative observations and artifacts from Discord, Notion, and GitHub, as well as contribution metrics and a custom communication effectiveness index (score: 0.76/1.00), we evaluate three dimensions: (1) the effectiveness of collaboration tools, (2) the impact of hybrid work on communication and productivity, and (3) the feasibility of aligning Scrum with academic timelines. Our findings show that (i) lightweight, tool-mediated coordination enabled stable progress even during remote periods; (ii) one-week sprints and flexible ceremonies helped reconcile Scrum with academic obligations; and (iii) shared motivation, role clarity, and compatible working styles were as critical as process mechanics. We propose practical recommendations for instructors and student teams adopting agile methods in hybrid, project-based learning settings.

</details>


### [8] [RepoShapley: Shapley-Enhanced Context Filtering for Repository-Level Code Completion](https://arxiv.org/abs/2601.03378)
*Yu Huo,Siyu Zhang,Kun Zeng,Yuquan Lu,Cheng Yang,Yifu Guo,Xiaoying Tang*

Main category: cs.SE

TL;DR: RepoShapley：基于Shapley边际贡献的仓库级代码补全上下文过滤框架，通过联盟感知的块选择提升补全质量，减少有害上下文和不必要检索。


<details>
  <summary>Details</summary>
Motivation: 仓库级代码补全虽然受益于检索增强生成，但跨文件证据控制困难，因为块的效用通常是交互依赖的：有些片段只有在与互补上下文配对时才有帮助，而其他片段在冲突时会损害解码。

Method: 提出RepoShapley框架，包含ChunkShapley模块：通过(i)单块探测估计带符号加权效应，(ii)捕捉饱和和干扰的代理游戏，(iii)小检索集的精确Shapley计算，(iv)使用冻结生成器的解码最优联盟选择的有界后验证，构建离线标签。通过离散控制令牌将验证的保留/丢弃决策和检索触发蒸馏到单一模型中。

Result: 在多个基准测试和骨干模型上的实验表明，RepoShapley提高了补全质量，同时减少了有害上下文和不必要的检索。

Conclusion: RepoShapley通过联盟感知的上下文过滤有效解决了仓库级代码补全中的跨文件证据控制问题，为检索增强代码补全提供了更精确的上下文选择机制。

Abstract: Repository-level code completion benefits from retrieval-augmented generation (RAG). However, controlling cross-file evidence is difficult because chunk utility is often interaction-dependent: some snippets help only when paired with complementary context, while others harm decoding when they conflict. We propose RepoShapley, a coalition-aware context filtering framework supervised by Shapley-style marginal contributions. Our module ChunkShapley constructs offline labels by (i) single-chunk probing with teacher-forced likelihood to estimate signed, weighted effects, (ii) a surrogate game that captures saturation and interference, (iii) exact Shapley computation for small retrieval sets, and (iv) bounded post-verification that selects a decoding-optimal coalition using the frozen generator. We distill verified $KEEP$ or $DROP$ decisions and retrieval triggering into a single model via discrete control tokens. Experiments across benchmarks and backbones show that RepoShapley improves completion quality while reducing harmful context and unnecessary retrieval. Code: https://anonymous.4open.science/r/a7f3c9.

</details>


### [9] [An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS](https://arxiv.org/abs/2601.03430)
*Mohamed Ouf,Shayan Noei,Zeph Van Iterson,Mariam Guizani,Ying Zou*

Main category: cs.SE

TL;DR: 对比OSS4SG与传统OSS项目：OSS4SG社区更稳定粘性高，传统OSS吸引更多流动贡献者；OSS4SG全年参与稳定，传统OSS有季节性波动；OSS4SG核心贡献者负责代码质量和问题解决，传统OSS中核心贡献者专注代码质量，临时贡献者处理问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注传统开源软件，但对面向社会公益的开源软件（OSS4SG）的社区动态和贡献模式了解不足。OSS4SG具有使命驱动的特性，需要研究这种特性如何影响其开发实践，以确保项目的可持续性和长期影响力。

Method: 对1039个GitHub仓库进行大规模实证研究，包括422个OSS4SG项目和617个传统OSS项目。比较社区结构、贡献者参与度和编码实践，分析社区稳定性、贡献者流动性和参与模式。

Result: OSS4SG项目社区更稳定粘性高（63.4%），传统OSS项目更具吸引力但贡献者流动率高（75.4%）。OSS4SG全年参与稳定，传统OSS有季节性波动。OSS4SG核心贡献者同时负责代码质量和问题解决，传统OSS中核心贡献者专注代码质量，临时贡献者处理问题。

Conclusion: OSS4SG项目的使命驱动特性塑造了独特的社区动态：形成更稳定粘性的社区，全年参与稳定，核心贡献者承担更全面的责任。这些发现有助于理解使命驱动开源项目的可持续发展机制。

Abstract: Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and "sticky" (63.4%) communities, whereas conventional OSS projects are more "magnetic" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality.

</details>


### [10] [CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models](https://arxiv.org/abs/2601.03432)
*Danny Brahman,Mohammad Mahoor*

Main category: cs.SE

TL;DR: 论文提出了CodeEval基准数据集和RunCodeEval执行框架，用于多维度评估LLM的Python代码生成能力，覆盖24个编程方面和3个熟练度级别。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集无法准确评估LLM的代码生成能力，难以识别具体优缺点，阻碍了针对性地提升模型推理和代码合成能力。

Method: 提出CodeEval多维度基准数据集，模拟学术编程课程评估过程，包含24个Python编程方面、3个熟练度级别（初级、中级、高级）以及类和函数两种问题类型。同时开发了RunCodeEval开源执行框架，提供完整的评估流水线。

Result: 创建了一个全面的评估体系，使研究人员能够快速获得模型在不同复杂度级别、问题类型和编程类别上的详细表现分析，识别具体优缺点。

Conclusion: CodeEval和RunCodeEval的组合实现了针对性评估，能够指导LLM编程能力的改进，填补了现有基准在代码生成评估方面的空白。

Abstract: Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies.

</details>


### [11] [Bootstrapping Code Translation with Weighted Multilanguage Exploration](https://arxiv.org/abs/2601.03512)
*Yuhan Wu,Huan Zhang,Wei Cheng,Chen Shen,Jingyue Yang,Wei Hu*

Main category: cs.SE

TL;DR: BootTrans：利用测试套件的功能不变性和跨语言可移植性，通过引导方法解决多语言代码翻译中并行数据稀缺和优化不平衡问题


<details>
  <summary>Details</summary>
Motivation: 多语言代码翻译面临两大关键障碍：1）并行数据稀缺且缺乏可执行的测试预言；2）处理不同语言对时存在优化不平衡问题

Method: 提出BootTrans引导方法，利用测试套件的功能不变性，将丰富的枢轴语言单元测试适配为多语言RL训练的通用验证预言。采用双池架构（种子池和探索池）通过执行引导的经验收集逐步扩展训练数据，并设计语言感知加权机制动态优先处理较难的翻译方向

Result: 在HumanEval-X和TransCoder-Test基准测试中，相比基线LLM在所有翻译方向上均取得显著改进，消融实验验证了引导和加权组件的有效性

Conclusion: BootTrans通过测试套件引导和语言感知加权，有效解决了多语言代码翻译中的数据稀缺和优化不平衡问题，显著提升了翻译性能

Abstract: Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components.

</details>


### [12] [Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day](https://arxiv.org/abs/2601.03513)
*Yi Wang,Zhenting Huang,Zhaohan Ding,Ruoxue Liao,Yuan Huang,Xinzijian Liu,Jiajun Xie,Siheng Chen,Linfeng Zhang*

Main category: cs.SE

TL;DR: Deploy-Master是一个自动化工作流，用于大规模发现、构建、验证和发布开源科学软件工具，解决了科学软件部署困难的问题，在一天内成功容器化了50,112个工具。


<details>
  <summary>Details</summary>
Motivation: 开源科学软件虽然丰富，但大多数工具难以编译、配置和重用，这限制了可重复性、大规模评估以及科学工具在现代AI-for-Science和智能体工作流中的实际集成。这种部署瓶颈阻碍了科学计算的规模化发展。

Method: Deploy-Master采用一站式智能体工作流，包括：1) 基于涵盖90+科学和工程领域的分类法进行大规模工具发现；2) 从50万个公共仓库中逐步筛选出52,550个可执行工具候选；3) 自动推断构建规范；4) 基于执行的验证；5) 容器化部署；6) 在SciencePedia中注册供搜索和重用。

Result: 在一天内完成了52,550次构建尝试，成功为50,112个科学工具创建了可重现的运行时环境。每个成功工具都通过最小可执行命令验证，并在SciencePedia中注册。研究还提供了5万工具规模的部署跟踪数据，揭示了吞吐量、成本分布、失败模式和规范不确定性等大规模部署特征。

Conclusion: Deploy-Master展示了通过自动化工作流大规模部署科学软件的可行性，揭示了为什么科学软件难以操作化，并强调了共享、可观察的执行基板作为可扩展AI-for-Science和智能体科学基础的重要性。

Abstract: Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.
  We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.
  Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science.

</details>


### [13] [Do Autonomous Agents Contribute Test Code? A Study of Tests in Agentic Pull Requests](https://arxiv.org/abs/2601.03556)
*Sabrina Haque,Sarvesh Ingale,Christoph Csallner*

Main category: cs.SE

TL;DR: 对AI驱动的代码提交中测试实践的实证研究，发现包含测试的PR随时间更常见、规模更大、耗时更长，但合并率相似，不同AI代理在测试采用和测试/生产代码比例上存在差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码工具越来越多地提交代码合并请求，需要了解测试在这些AI驱动工作流中的表现情况，以评估其对软件正确性和长期可维护性的影响。

Method: 使用AIDev数据集，对AI驱动的代码合并请求进行实证研究，分析测试包含频率、在PR生命周期中的引入时机，以及包含测试的PR与不包含测试的PR在规模、周转时间和合并结果方面的差异。

Result: 包含测试的PR随时间推移更常见，通常规模更大、完成时间更长，但合并率基本相似。不同AI代理在测试采用率和测试代码与生产代码的比例上存在差异。

Conclusion: 研究提供了AI驱动代码合并请求中测试行为的描述性视图，为未来自主软件开发研究提供了实证基础，表明测试实践在AI驱动开发中正在演变但存在差异。

Abstract: Testing is a critical practice for ensuring software correctness and long-term maintainability. As agentic coding tools increasingly submit pull requests (PRs), it becomes essential to understand how testing appears in these agent-driven workflows. Using the AIDev dataset, we present an empirical study of test inclusion in agentic pull requests. We examine how often tests are included, when they are introduced during the PR lifecycle and how test-containing PRs differ from non-test PRs in terms of size, turnaround time, and merge outcomes. Across agents, test-containing PRs are more common over time and tend to be larger and take longer to complete, while merge rates remain largely similar. We also observe variation across agents in both test adoption and the balance between test and production code within test PRs. Our findings provide a descriptive view of testing behavior in agentic pull requests and offer empirical grounding for future studies of autonomous software development.

</details>


### [14] [Auditable DevOps Automation via VSM and GQM](https://arxiv.org/abs/2601.03574)
*Mamdouh Alenezi*

Main category: cs.SE

TL;DR: 提出VSM-GQM-DevOps框架，将价值流映射、目标-问题-指标范式与DevOps自动化结合，帮助组织基于数据驱动决策来优先考虑自动化投资，改善交付性能与项目管理成果。


<details>
  <summary>Details</summary>
Motivation: 许多组织难以从战略项目管理成果（如减少浪费、交付可预测性、跨团队协调、面向客户的质量）的角度来论证和优先考虑DevOps自动化工作。需要一种系统化方法来连接自动化投资与可衡量的业务成果。

Method: 提出VSM-GQM-DevOps统一框架：1) 使用价值流映射可视化端到端交付系统并量化延迟、返工和交接；2) 应用目标-问题-指标范式将利益相关者目标转化为最小化、决策相关的测量模型；3) 基于成熟度匹配的DevOps自动化来修复经验观察到的瓶颈。框架提供可追溯性，从观察到的问题到目标对齐的问题、指标和自动化候选方案。

Result: 框架提供了一种可辩护的优先级排序方法，平衡预期影响、置信度和成本。同时定义了多站点、纵向混合方法验证协议，结合基于遥测的准实验分析（中断时间序列和受控推出）与访谈和回顾的定性三角验证。

Conclusion: 该研究贡献了一个经过验证的路径和一套实用工具，使组织能够选择能够显著改善交付性能和项目管理成果的自动化投资，为DevOps自动化提供了数据驱动的决策支持框架。

Abstract: DevOps automation can accelerate software delivery, yet many organizations still struggle to justify and prioritize automation work in terms of strategic project-management outcomes such as waste reduction, delivery predictability, cross-team coordination, and customer-facing quality. This paper presents \textit{VSM--GQM--DevOps}, a unified, traceable framework that integrates (i) Value Stream Mapping (VSM) to visualize the end-to-end delivery system and quantify delays, rework, and handoffs, (ii) the Goal--Question--Metric (GQM) paradigm to translate stakeholder objectives into a minimal, decision-relevant measurement model (combining DORA with project and team outcomes), and (iii) maturity-aligned DevOps automation to remediate empirically observed bottlenecks through small, reversible interventions. The framework operationalizes traceability from observed waste to goal-aligned questions, metrics, and automation candidates, and provides a defensible prioritization approach that balances expected impact, confidence, and cost. We also define a multi-site, longitudinal mixed-method validation protocol that combines telemetry-based quasi-experimental analysis (interrupted time series and, where feasible, controlled rollouts) with qualitative triangulation from interviews and retrospectives. The expected contribution is a validated pathway and a set of practical instruments that enables organizations to select automation investments that demonstrably improve both delivery performance and project-management outcomes.

</details>


### [15] [On the Robustness of Fairness Practices: A Causal Framework for Systematic Evaluation](https://arxiv.org/abs/2601.03621)
*Verya Monjezi,Ashish Kumar,Ashutosh Trivedi,Gang Tan,Saeid Tizpaz-Niari*

Main category: cs.SE

TL;DR: 论文探讨机器学习公平性实践在现实数据问题（如错误标签、缺失数据、分布偏移）下的可靠性问题


<details>
  <summary>Details</summary>
Motivation: 机器学习在关键社会应用中可能产生不公平决策，尽管已有公平性干预措施，但这些实践在现实数据问题下的可靠性尚未明确

Method: 通过分析现有公平性实践（包括敏感特征处理、非敏感属性选择、偏差缓解技术）在数据质量问题下的表现来评估其可靠性

Result: 论文核心是提出并探讨公平性实践在错误标签、缺失数据和分布偏移等现实挑战下的可靠性问题

Conclusion: 需要更深入地评估现有公平性实践在现实数据问题下的可靠性和泛化能力

Abstract: Machine learning (ML) algorithms are increasingly deployed to make critical decisions in socioeconomic applications such as finance, criminal justice, and autonomous driving. However, due to their data-driven and pattern-seeking nature, ML algorithms may develop decision logic that disproportionately distributes opportunities, benefits, resources, or information among different population groups, potentially harming marginalized communities. In response to such fairness concerns, the software engineering and ML communities have made significant efforts to establish the best practices for creating fair ML software. These include fairness interventions for training ML models, such as including sensitive features, selecting non-sensitive attributes, and applying bias mitigators. But how reliably can software professionals tasked with developing data-driven systems depend on these recommendations? And how well do these practices generalize in the presence of faulty labels, missing data, or distribution shifts? These questions form the core theme of this paper.

</details>


### [16] [Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test](https://arxiv.org/abs/2601.03640)
*Mohd Ariful Haque,Kishor Datta Gupta,Mohammad Ashiqur Rahman,Roy George*

Main category: cs.SE

TL;DR: 论文提出了一个专门测试LLM在代码生成中数据转录准确性的基准测试，要求模型将高精度十进制常数原样嵌入Python代码并执行简单聚合计算。


<details>
  <summary>Details</summary>
Motivation: 现实软件任务中经常需要将数据（如加密常数、协议测试向量、白名单、校准表）精确转录到代码中，这些操作对微小错误敏感但可能产生语法有效的程序。现有代码生成评估主要关注算法推理，缺乏对数据完整性的专门测试。

Method: 设计了一个极简的转录到代码基准测试：给定高精度十进制常数列表，模型必须生成Python代码原样嵌入这些常数并执行简单聚合计算。采用基于精确字符串包含的评估协议，分析框架用于表征状态跟踪和长时程生成失败。

Result: 该基准测试作为一个紧凑的压力测试，补充现有代码生成评估，专注于数据完整性而非算法推理能力。

Conclusion: 论文提出了一个专门针对LLM代码生成中数据转录可靠性的基准测试，填补了现有评估在数据完整性方面的空白，可作为现有代码生成评估的重要补充。

Abstract: Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning.

</details>


### [17] [From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level](https://arxiv.org/abs/2601.03731)
*Jia Li,Yuxin Su,Michael R. Lyu*

Main category: cs.SE

TL;DR: RepoReason是一个用于评估大语言模型在仓库级别推理能力的白盒诊断基准，通过执行驱动的变异框架和动态程序切片来量化推理能力，发现前沿模型存在聚合缺陷问题。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型演变为自主代理，评估其在真实世界、相互依赖的文件系统中保持逻辑一致性的仓库级推理能力变得至关重要。当前基准测试通常在孤立的代码片段和黑盒评估之间波动，缺乏对推理过程的深入诊断。

Method: 提出RepoReason白盒诊断基准，采用执行驱动的变异框架，利用环境作为语义预言机来重新生成真实状态，避免记忆化问题。建立基于动态程序切片的细粒度诊断系统，使用三个正交指标量化推理能力：ESV（读取负载）、MCL（模拟深度）和DFI（集成宽度）。

Result: 对前沿模型（如Claude-4.5-Sonnet、DeepSeek-v3.1-Terminus）的全面评估揭示了普遍的聚合缺陷，集成宽度成为主要的认知瓶颈。模型在跨文件系统集成推理方面存在显著困难。

Conclusion: RepoReason提供了细粒度的白盒洞察，为优化下一代代理式软件工程提供了诊断工具。研究结果表明需要改进模型在复杂、相互依赖的代码库中的集成推理能力。

Abstract: As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.

</details>


### [18] [Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study](https://arxiv.org/abs/2601.03780)
*Md Ahasanuzzaman,Bram Adams,Emad Fallahzadeh,Gustavo A. Oliva,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 该研究首次系统分析了代码生成基准测试中编程概念的代表性，发现HumanEval和MBPP等基准测试仅覆盖一半的知识单元，且分布高度倾斜，而真实项目则覆盖全部知识单元且分布均衡。研究者提出基于提示的LLM框架生成新任务来平衡基准测试，增强后的基准测试显示LLM性能显著下降12.54-44.82%，表明现有基准测试高估了LLM的真实代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码生成能力通常通过基准测试（如HumanEval）评估，但这些基准测试中涵盖的编程概念是否代表真实世界项目的需求尚未得到系统研究。如果基准测试中的概念代表性不足，评估结果可能不完整，无法准确反映LLM在实际应用中的表现。

Method: 研究提出基于知识单元（KUs）的分析框架，将编程语言能力分解为语言构造和API的连贯集合。首先分析HumanEval和MBPP两个Python基准测试的KU覆盖情况，并与30个真实Python项目进行对比。然后提出基于提示的LLM框架，通过合成KU相关任务来重新平衡基准测试的KU分布，生成440个新任务并增强现有基准测试。

Result: 研究发现每个基准测试仅覆盖20个已识别KUs的一半，而真实项目则覆盖所有KUs且分布相对均衡。基准测试任务呈现高度倾斜的KU分布。增强后的基准测试显著提高了KU覆盖率，分布对齐度提升超过60%。在增强基准测试上评估最先进的LLM显示，性能出现一致且统计显著的下降（12.54-44.82%），表明现有基准测试因有限的KU覆盖而高估了LLM性能。

Conclusion: 现有代码生成基准测试在编程概念覆盖方面存在显著不足，无法准确评估LLM的真实代码生成能力。通过基于知识单元的任务合成方法可以创建更具代表性的评估基准，为构建更现实的LLM代码生成能力评估提供了可操作的指导。

Abstract: Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined.
  To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions.
  To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities.

</details>


### [19] [Once Upon a Team: Investigating Bias in LLM-Driven Software Team Composition and Task Allocation](https://arxiv.org/abs/2601.03857)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Amleto Di Salle,Patrizio Pelliccione,Gemma Catolino,Fabio Palomba*

Main category: cs.SE

TL;DR: 研究发现LLMs在软件工程团队组建和任务分配中存在系统性偏见，会加剧人口统计学不平等


<details>
  <summary>Details</summary>
Motivation: LLMs在软件工程任务中应用日益广泛，但当用于团队组建和任务分配等社会敏感决策时，存在公平性担忧。先前研究显示LLMs可能复制刻板印象，但这些分析多为探索性且孤立考察敏感属性

Method: 研究分析了候选人国家和代词的组合效应，使用三个LLM模型进行3000次模拟决策，考察人口统计学属性如何影响选择可能性和任务分配

Result: 发现系统性差异：人口统计学属性显著影响选择可能性和任务分配，即使考虑专业知识因素。任务分配进一步反映刻板印象，技术和领导角色在不同群体间分配不均

Conclusion: LLMs在软件工程环境中加剧了人口统计学不平等，强调了需要公平性评估的重要性

Abstract: LLMs are increasingly used to boost productivity and support software engineering tasks. However, when applied to socially sensitive decisions such as team composition and task allocation, they raise concerns of fairness. Prior studies have revealed that LLMs may reproduce stereotypes; however, these analyses remain exploratory and examine sensitive attributes in isolation. This study investigates whether LLMs exhibit bias in team composition and task assignment by analyzing the combined effects of candidates' country and pronouns. Using three LLMs and 3,000 simulated decisions, we find systematic disparities: demographic attributes significantly shaped both selection likelihood and task allocation, even when accounting for expertise-related factors. Task distributions further reflected stereotypes, with technical and leadership roles unevenly assigned across groups. Our findings indicate that LLMs exacerbate demographic inequities in software engineering contexts, underscoring the need for fairness-aware assessment.

</details>


### [20] [Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design](https://arxiv.org/abs/2601.03878)
*Giovanni Rosa,David Moreno-Lumbreras,Gregorio Robles,Jesús M. González-Barahona*

Main category: cs.SE

TL;DR: 该论文提出一个实证研究设计，使用CURRANTE工具探索人类在规范制定和测试细化中的干预如何影响LLM生成代码的质量和动态过程。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型越来越多地集成到软件开发工作流中，但它们在结构化、规范驱动的过程中的行为仍然缺乏深入理解。需要研究人类在规范制定和测试细化中的干预如何影响LLM生成代码的质量和动态。

Method: 使用CURRANTE（一个Visual Studio Code扩展）进行人类在环的工作流研究。该工具引导开发者通过三个顺序阶段：规范制定、测试生成和函数实现。参与者将解决LiveCodeBench数据集中的中等难度问题，工具会记录细粒度的交互日志、有效性指标（通过率、全通过完成度）、效率指标（通过时间）和迭代行为。

Result: 研究尚未完成，但预期结果将提供关于人类干预如何影响LLM生成代码质量的实证见解，并为下一代开发环境的设计提供指导。

Conclusion: 该研究将为设计能够更好协调人类推理与模型驱动代码生成的下一代开发环境提供实证基础，帮助理解如何优化LLM在软件开发中的集成。

Abstract: Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.

</details>


### [21] [Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures](https://arxiv.org/abs/2601.03988)
*Nicolas Lacroix,Mireille Blay-Fornarino,Sébastien Mosser,Frederic Precioso*

Main category: cs.SE

TL;DR: 本文评估小型语言模型(SLMs)在提取机器学习管道阶段方面的能力，以解决现有方法在可扩展性和领域多样性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 从源代码中提取机器学习管道的结构阶段对于深入理解数据科学实践至关重要。然而，机器学习生态系统的不断演变（算法、库、数据集等）带来了多样性挑战。现有方法要么依赖不可扩展的手动标注，要么使用无法很好支持领域多样性的机器学习分类器，因此需要更灵活可靠的解决方案。

Method: 基于两个相关参考研究进行验证性研究：首先使用Cochran's Q检验比较多个SLMs，选择最佳模型；然后通过两个独立的McNemar's检验与参考研究对比；通过额外的Cochran's Q检验分析分类定义变化对性能的影响；最后使用Pearson卡方检验进行拟合优度分析，比较数据科学实践的见解。

Result: 研究评估了SLMs在代码理解和分类能力方面的表现，以确定它们是否能解决现有方法的局限性，并推动对数据科学实践的理解。

Conclusion: SLMs具有利用其代码理解和分类能力解决机器学习管道阶段提取挑战的潜力，能够提供更灵活可靠的解决方案，并增进对数据科学实践的理解。

Abstract: Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.
  Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.
  Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.

</details>


### [22] [An Ontology-Based Approach to Security Risk Identification of Container Deployments in OT Contexts](https://arxiv.org/abs/2601.04010)
*Yannick Landeck,Dian Balta,Martin Wimmer,Christian Knierim*

Main category: cs.SE

TL;DR: 提出基于本体的容器安全风险评估方法CSRO，用于OT环境中特权容器的自动化、可复现风险识别


<details>
  <summary>Details</summary>
Motivation: OT环境中容器化应用常需特权访问网络接口或执行管理任务，这降低了容器默认隔离性并带来安全风险。现有方法缺乏可复现性、跨上下文可解释性以及与部署工件的技术集成

Method: 提出基于模型的方法，实现为容器安全风险本体(CSRO)，集成五个关键领域：对抗行为、上下文假设、攻击场景、风险评估规则和容器安全工件

Result: 案例研究评估表明，CSRO实现了从工件到风险级别的端到端形式化风险计算，支持自动化和可复现的风险识别

Conclusion: CSRO目前专注于技术层面的容器级处理措施，但其模块化和灵活设计为扩展到主机级和组织级风险因素提供了坚实基础

Abstract: In operational technology (OT) contexts, containerised applications often require elevated privileges to access low-level network interfaces or perform administrative tasks such as application monitoring. These privileges reduce the default isolation provided by containers and introduce significant security risks. Security risk identification for OT container deployments is challenged by hybrid IT/OT architectures, fragmented stakeholder knowledge, and continuous system changes. Existing approaches lack reproducibility, interpretability across contexts, and technical integration with deployment artefacts. We propose a model-based approach, implemented as the Container Security Risk Ontology (CSRO), which integrates five key domains: adversarial behaviour, contextual assumptions, attack scenarios, risk assessment rules, and container security artefacts. Our evaluation of CSRO in a case study demonstrates that the end-to-end formalisation of risk calculation, from artefact to risk level, enables automated and reproducible risk identification. While CSRO currently focuses on technical, container-level treatment measures, its modular and flexible design provides a solid foundation for extending the approach to host-level and organisational risk factors.

</details>


### [23] [Smells Depend on the Context: An Interview Study of Issue Tracking Problems and Smells in Practice](https://arxiv.org/abs/2601.04124)
*Lloyd Montgomery,Clara Lüders,Christian Rahe,Walid Maalej*

Main category: cs.SE

TL;DR: 该研究通过访谈26位软件工程从业者，识别了ITS中的14个常见问题，发现文献中的31个ITS异味大多在实际中不出现或不构成问题，强调ITS问题高度依赖上下文因素。


<details>
  <summary>Details</summary>
Motivation: 虽然研究者广泛分析ITS数据以自动化或辅助特定活动，但对软件开发团队在ITS中遇到的实际挑战以及某些实践和工作方式何时被视为问题知之甚少。

Method: 对来自不同组织和行业的26位经验丰富的软件工程从业者进行深度访谈研究，询问他们遇到的一般问题以及文献中讨论的31个ITS异味的相关性，并对访谈笔记应用主题分析。

Result: 识别了14个常见问题，包括问题可发现性、僵尸问题、工作流臃肿和缺乏工作流执行等。参与者表示许多ITS异味在实际中不出现或不构成问题，ITS问题和异味高度依赖上下文因素。

Conclusion: ITS问题和异味高度依赖上下文因素，如ITS配置、工作流阶段和团队规模。研究讨论了潜在的工具体系来配置、监控和可视化ITS异味以应对这些挑战。

Abstract: Issue Tracking Systems (ITSs) enable software developers and managers to collect and resolve issues collaboratively. While researchers have extensively analysed ITS data to automate or assist specific activities such as issue assignments, duplicate detection, or priority prediction, developer studies on ITSs remain rare. Particularly, little is known about the challenges Software Engineering (SE) teams encounter in ITSs and when certain practices and workarounds (such as leaving issue fields like "priority" empty) are considered problematic. To fill this gap, we conducted an in-depth interview study with 26 experienced SE practitioners from different organisations and industries. We asked them about general problems encountered, as well as the relevance of 31 ITS smells (aka potentially problematic practices) discussed in the literature. By applying Thematic Analysis to the interview notes, we identified 14 common problems including issue findability, zombie issues, workflow bloat, and lack of workflow enforcement. Participants also stated that many of the ITS smells do not occur or are not problematic. Our results suggest that ITS problems and smells are highly dependent on context factors such as ITS configuration, workflow stage, and team size. We also discuss potential tooling solutions to configure, monitor, and visualise ITS smells to cope with these challenges.

</details>
