<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.SE](#cs.SE) [Total: 16]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [The Tragedy of Chain Commons](https://arxiv.org/abs/2602.20341)
*Ignacio Amores-Sesar,Mirza Ahad Baig,Seth Gilbert,Ray Neiheiser,Michelle X. Yeo*

Main category: cs.DC

TL;DR: 论文系统研究了BFT共识中解耦设计（排序与执行分离）的安全隐患，发现了新的gaslighting攻击，证明了攻击抵抗性与资源利用率之间的根本性权衡，并提出了中间模型解决方案。


<details>
  <summary>Details</summary>
Motivation: 现代区块链追求高吞吐量和低延迟，但交易执行和验证成为共识关键路径的瓶颈。解耦设计（将排序与执行分离）虽然提升了性能，但可能导致无效交易留在账本中，增加存储成本并引发新的战略行为风险。

Method: 提出了一个形式化框架来分析共识与执行之间的交互关系，使用该框架证明了在解耦模型中存在一种新的gaslighting攻击，并证明了攻击抵抗性与资源容量利用率之间的根本性权衡。针对基于领导者的协议，讨论了一种中间模型。

Result: 发现了之前未被识别的gaslighting攻击，证明了在解耦模型中确定性地同时实现攻击抵抗性和高资源利用率是不可能的。提出的中间模型能够在抵抗gaslighting攻击的同时实现高吞吐量和低延迟。

Conclusion: 解耦设计虽然提升了性能，但引入了新的安全风险。需要在安全性和效率之间做出权衡，提出的中间模型为解决这一权衡提供了可行方案。

Abstract: Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency.

</details>


### [2] [Circumventing the FLP Impossibility Result with Open Atomic Ethernet](https://arxiv.org/abs/2602.20444)
*Paul Borrill*

Main category: cs.DC

TL;DR: 本文认为FLP不可能性定理不是物理定律，而是特定系统模型的定理，并提出Open Atomic Ethernet通过拒绝异步模型基础来规避FLP限制。


<details>
  <summary>Details</summary>
Motivation: FLP不可能性结果被认为是分布式计算中最基本的负面结果之一，四十年来该领域一直将其视为不可移动的约束。本文旨在挑战这一传统观点，展示如何通过不同的系统模型设计来规避FLP限制。

Method: 提出"双同步"概念来描述OAE的关键特性：每轮边界上双方都能达成结果共识的有界时间双边决议。通过在第二层构建基于交换的双同步协议，OAE绕过了FLP异步模型的负载假设。

Result: OAE通过拒绝异步模型的基础假设，实现了确定性原子协调，而没有违反任何不可能性结果，从而规避了FLP限制。

Conclusion: FLP不可能性定理不是分布式计算的绝对约束，而是特定模型下的定理。通过采用双同步模型和适当的协议设计，可以在实际系统中实现确定性共识，这为分布式系统设计开辟了新方向。

Abstract: The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result.

</details>


### [3] [Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning](https://arxiv.org/abs/2602.20450)
*Nihal Balivada,Shrey Gupta,Shashank Shreedhar Bhatt,Suyash Gupta*

Main category: cs.DC

TL;DR: Terraform是一种新颖的联邦学习客户端选择方法，通过梯度更新和确定性选择算法选择异构客户端进行重训练，相比先前工作准确率提升高达47%


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式客户端-服务器架构中训练全局模型时，由于客户端间的统计异构性，通常导致准确率低于传统机器学习算法。现有方法使用模型更新（如损失和偏差）选择客户端，但这些更新不能准确表示客户端的异构性，且选择方法缺乏确定性。

Method: Terraform采用双管齐下的方法：1）使用梯度更新而非传统损失和偏差来表征客户端异构性；2）采用确定性选择算法选择异构客户端进行重训练。

Result: Terraform相比先前工作实现了高达47%的准确率提升。通过全面的消融研究和训练时间分析，证明了其效率和鲁棒性。

Conclusion: Terraform通过梯度更新和确定性选择算法的结合，有效解决了联邦学习中统计异构性导致的准确率下降问题，为联邦学习客户端选择提供了更鲁棒的方法。

Abstract: Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform.

</details>


### [4] [A Granularity Characterization of Task Scheduling Effectiveness](https://arxiv.org/abs/2602.20561)
*Sana Taghipour Anvar,David Kaeli*

Main category: cs.DC

TL;DR: 论文提出了一个任务粒度分析框架，通过任务图依赖拓扑结构来预测调度开销增长，从而解释动态调度在强扩展中的性能崩溃现象。


<details>
  <summary>Details</summary>
Motivation: 基于任务的运行时系统为并行科学应用提供了灵活的负载平衡和可移植性，但其强扩展性能对任务粒度高度敏感。随着并行度增加，调度开销可能从可忽略变为主导，导致某些算法性能急剧下降。虽然这种现象在经验上被广泛观察到，但缺乏对算法结构如何影响动态调度效益的系统理解。

Method: 引入了一个粒度表征框架，将调度开销增长直接与任务图依赖拓扑结构联系起来。基于依赖结构而非问题规模单独决定开销随并行度扩展的观察，使用简单的粒度度量来表征执行行为，指示调度开销何时能被并行计算分摊，何时会主导性能。

Result: 通过具有不同依赖模式的代表性并行工作负载的实验评估，证明所提出的表征能够解释实践中观察到的渐进和突变的强扩展崩溃。进一步表明，从依赖拓扑导出的开销模型能够准确预测强扩展极限，并实现实用的运行时决策规则，用于选择动态或静态执行，无需进行详尽的强扩展研究或大量离线调优。

Conclusion: 任务依赖拓扑结构而非问题规模单独决定了调度开销的扩展行为。提出的框架为理解动态调度在强扩展中的性能限制提供了理论基础，并提供了实用的运行时决策机制，能够根据任务图结构预测何时动态调度有益、何时静态调度更优。

Abstract: Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning.

</details>


### [5] [Lagom: Unleashing the Power of Communication and Computation Overlapping for Distributed LLM Training](https://arxiv.org/abs/2602.20656)
*Guanbin Xu,ZhenGuo Xu,Yuzhe Li,Youhui Bai,Ping Gong,Chaoyi Ruan,Cheng Li*

Main category: cs.DC

TL;DR: Lagom系统通过协同调优通信参数来平衡计算与通信资源使用，在计算成为瓶颈时优化分布式大模型训练中的通信计算重叠，将优化复杂度从指数级降至线性级。


<details>
  <summary>Details</summary>
Motivation: 在分布式大模型训练中，重叠通信与计算至关重要，但当计算成为瓶颈时，优化这种重叠仍然具有挑战性。现有方法难以在计算受限情况下有效平衡计算与通信资源。

Method: 提出Lagom系统，通过协同调优通信参数来平衡计算与通信资源使用。引入统一成本模型和基于优先级的搜索算法，将优化复杂度从指数级降至线性级。

Result: 在高带宽和低带宽GPU集群上的评估显示，Lagom在多种模型和并行化策略下，相比NCCL和AutoCCL分别实现了1.07-1.33倍和1.03-1.27倍的加速。

Conclusion: Lagom通过协同调优通信参数有效解决了计算成为瓶颈时的通信计算重叠优化问题，显著提升了分布式大模型训练效率。

Abstract: Overlapping communication with computation is crucial for distributed large-model training, yet optimizing it - especially when computation becomes the bottleneck-remains challenging. We present Lagom, a system that co-tunes communication parameters to balance resource usage between computation and communication. By introducing a unified cost model and a priority-based search algorithm, Lagom reduces optimization complexity from exponential to linear. Evaluations on high- and low-bandwidth GPU clusters show that Lagom achieves 1.07-1.33x and 1.03-1.27x speedup over NCCL and AutoCCL across diverse models and parallelizations.

</details>


### [6] [A Morton-Type Space-Filling Curve for Pyramid Subdivision and Hybrid Adaptive Mesh Refinement](https://arxiv.org/abs/2602.20887)
*David Knapp,Johannes Albrecht Holke,Thomas Spenke,Carsten Burstedde*

Main category: cs.DC

TL;DR: 本文提出将金字塔元素作为新的功能元素类型，用于连接三维混合元素网格中的四面体和六面体元素，实现了完整的混合元素动态自适应网格细化框架。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥基于树的自适应网格细化在三维混合元素网格中的潜力，需要解决四面体和六面体元素之间的连接问题，避免悬挂边。

Method: 引入金字塔元素作为新的功能元素类型；提出定义良好的空间填充曲线；解决元素和森林级别的金字塔细化挑战；设计必要的功能并泛化全局并行算法。

Result: 演示证实了这种完整的混合元素动态自适应网格细化框架的效率和可扩展性。

Conclusion: 金字塔元素的引入使得三维混合元素网格能够实现高效的动态自适应网格细化，为复杂几何的模拟提供了更灵活的网格处理能力。

Abstract: The forest-of-refinement-trees approach allows for dynamic adaptive mesh refinement (AMR) at negligible cost. While originally developed for quadrilateral and hexahedral elements, previous work established the theory and algorithms for unstructured meshes of simplicial and prismatic elements. To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes, this paper introduces the pyramid as a new functional element type; its primary purpose is to connect tetrahedral and hexahedral elements without hanging edges.We present a well-defined space-filling curve (SFC) for the pyramid and detail how the unique challenges on the element and forest level associated with the pyramidal refinement are resolved. We propose the necessary functional design and generalize the fundamental global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange to fully support this new element. Our demonstrations confirm the efficiency and scalability of this complete, hybrid-element dynamic AMR framework.

</details>


### [7] [Is a LOCAL algorithm computable?](https://arxiv.org/abs/2602.21022)
*Antonio Cruciani,Avinandan Das,Massimo Equi,Henrik Lievonen,Diep Luong-Le,Augusto Modanese,Jukka Suomela*

Main category: cs.DC

TL;DR: 本文揭示LOCAL模型中节点状态更新函数是否可计算这一被忽视的区别，对局部可检查标记问题（LCL）有重要影响，并与对图规模n的认知直接相关。


<details>
  <summary>Details</summary>
Motivation: LOCAL模型中关于节点状态更新函数是否可计算的现有定义模糊且自相矛盾，这一区别通常被忽略。本文旨在证明这一区别对LCL问题有实际影响，并与对图规模n的认知密切相关。

Method: 通过构造一个具体的LCL问题Π，展示其在三种不同条件下的复杂度差异：1) 不可计算模型；2) 已知n上界的可计算模型；3) 未知n的可计算模型。并证明这一联系对一般LCL问题成立。

Result: 存在LCL问题Π：在不可计算模型中O(log n)轮可解；已知n上界的可计算模型中O(log n)轮可解；未知n的可计算模型中需要Ω(√n)轮。且对任意LCL问题，若有n的上界，则其在可计算和不可计算模型中的轮复杂度相同。

Conclusion: LOCAL模型中节点状态更新函数的可计算性假设对LCL问题复杂度有实质性影响，且与对图规模n的认知直接相关。这一发现挑战了该区别可安全忽略的传统观点。

Abstract: Common definitions of the "standard" LOCAL model tend to be sloppy and even self-contradictory on one point: do the nodes update their state using an arbitrary function or a computable function? So far, this distinction has been safe to neglect, since problems where it matters seem contrived and quite different from e.g. typical local graph problems studied in this context.
  We show that this question matters even for locally checkable labeling problems (LCLs), perhaps the most widely studied family of problems in the context of the LOCAL model. Furthermore, we show that assumptions about computability are directly connected to another aspect already recognized as highly relevant: whether we have any knowledge of $n$, the size of the graph. Concretely, we show that there is an LCL problem $Π$ with the following properties:
  1. $Π$ can be solved in $O(\log n)$ rounds if the \textsf{LOCAL} model is uncomputable.
  2. $Π$ can be solved in $O(\log n)$ rounds in the computable model if we know any upper bound on $n$.
  3. $Π$ requires $Ω(\sqrt{n})$ rounds in the computable model if we do not know anything about $n$.
  We also show that the connection between computability and knowledge of $n$ holds in general: for any LCL problem $Π$, if you have any bound on $n$, then $Π$ has the same round complexity in the computable and uncomputable models.

</details>


### [8] [ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments](https://arxiv.org/abs/2602.21140)
*Haley Li,Xinglu Wang,Cong Feng,Chunxu Zuo,Yanan Wang,Hei Lo,Yufei Cui,Bingji Wang,Duo Cui,Shuming Jing,Yizhou Shan,Ying Xiong,Jiannan Wang,Yong Zhang,Zhenan Fan*

Main category: cs.DC

TL;DR: ReviveMoE：一种无需重启服务实例即可实现大规模LLM部署快速故障恢复的方法


<details>
  <summary>Details</summary>
Motivation: 随着LLM部署扩展到更多硬件，系统单点故障概率显著增加，云运营商需要处理这些不可避免的故障。传统的重启LLM服务实例方法在模型即服务推理场景中成本高昂，因为重新加载模型权重和重新编译计算图会给传入请求带来显著延迟。

Method: ReviveMoE方法支持传统LLM架构（MoE和注意力机制在同一硬件上）和分离式架构（MoE与注意力机制分离）。该方法基于华为的xDeepServe服务平台和XCCL通信库构建，并集成到华为云MaaS中。

Result: ReviveMoE实现了大规模LLM部署的快速故障恢复，避免了服务实例重启带来的延迟问题。

Conclusion: ReviveMoE为云运营商提供了一种高效的故障恢复解决方案，能够在不中断服务的情况下处理LLM部署中的硬件故障，提高了系统的可靠性和可用性。

Abstract: As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library.

</details>


### [9] [Scaling State-Space Models on Multiple GPUs with Tensor Parallelism](https://arxiv.org/abs/2602.21144)
*Anurag Dutt,Nimit Shah,Hazem Masarani,Anshul Gandhi*

Main category: cs.DC

TL;DR: 本文提出了一种针对选择性状态空间模型（SSM）的高效张量并行推理设计，解决了多GPU部署中的通信瓶颈，显著提升了推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 选择性状态空间模型已成为大语言模型的重要架构，但其推理性能受限于单GPU的内存和带宽。虽然张量并行在Transformer中广泛应用，但应用于选择性SSM块存在挑战，因为SSM混合器将大投影与序列级循环状态更新和局部混合耦合，其效率依赖于保持局部性和避免关键路径中的同步。

Method: 提出通信高效的张量并行设计，解决三个工程挑战：1) 通过SSM状态缓存实现预填充和解码阶段的TTFT改进；2) 对混合器的打包参数张量进行分区，使循环更新保持局部性同时最小化通信；3) 使用量化AllReduce降低TP聚合开销。

Result: 在Mamba、Falcon-Mamba和Zamba三种SSM架构上评估，2GPU时吞吐量提升1.6-2.1倍，4GPU时提升2.6-4.0倍，长上下文场景收益最大。量化AllReduce进一步带来10-18%的吞吐量提升。

Conclusion: 该研究为选择性状态空间模型提供了有效的多GPU张量并行推理解决方案，显著提升了部署效率和性能，特别是在长上下文工作负载中。

Abstract: Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.
  This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer's packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead.

</details>


### [10] [Circumventing the CAP Theorem with Open Atomic Ethernet](https://arxiv.org/abs/2602.21182)
*Paul Borrill*

Main category: cs.DC

TL;DR: 该论文认为Open Atomic Ethernet通过引入双向同步和八价网状拓扑，大幅减少了应用可见的"软分区"频率和持续时间，改变了CAP权衡的工程实现方式。


<details>
  <summary>Details</summary>
Motivation: CAP定理通常被视为系统定律，但在实际工程实践中，分区现象的可观测性以及底层网络如何处理消息语义信息至关重要。论文旨在探讨如何通过改变网络架构来改变CAP权衡在应用层面的可见性。

Method: 提出Open Atomic Ethernet架构，它通过两个关键技术：(1) 用有界时间的双向端点状态协调（称为双向同步）替代传统的"发送即忘"链路语义；(2) 采用八价网状拓扑避免Clos汇聚点，每个节点都可以作为本地修复生成树的根节点。

Result: 该架构不能完全消除硬图分割，但能将应用可见的"软分区"频率和持续时间大幅减少，能够在数百纳秒内检测和修复主要网络故障。

Conclusion: Open Atomic Ethernet改变了CAP权衡的工程实现方式，通过降低软分区的可见性，为分布式系统提供了更好的操作实践。论文将这一观点与Brewer的原始CAP框架、Gilbert和Lynch的形式化、Lee等人的CAL定理以及Abadi的PACELC扩展联系起来。

Abstract: The CAP theorem is routinely treated as a systems law: under network partition, a replicated service must sacrifice either consistency or availability. The theorem is correct within its standard asynchronous network model, but operational practice depends on where partition-like phenomena become observable and on how lower layers discard or preserve semantic information about message fate. This paper argues that Open Atomic Ethernet (OAE) shifts the engineering regime in which CAP tradeoffs become application-visible by (i) replacing fire-and-forget link semantics with bounded-time bilateral reconciliation of endpoint state -- the property we call bisynchrony -- and (ii) avoiding Clos funnel points via an octavalent mesh in which each node can act as the root of a locally repaired spanning tree. The result is not the elimination of hard graph cuts, but a drastic reduction in the frequency and duration of application-visible "soft partitions" by detecting and healing dominant fabric faults within hundreds of nanoseconds. We connect this view to Brewer's original CAP framing, the formalization by Gilbert and Lynch, the CAL theorem of Lee et al., which replaces binary partition tolerance with a quantitative measure of apparent latency, and Abadi's PACELC extension.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Mitigating "Epistemic Debt" in Generative AI-Scaffolded Novice Programming using Metacognitive Scripts](https://arxiv.org/abs/2602.20206)
*Sreecharan Sankaranarayanan*

Main category: cs.SE

TL;DR: 研究探讨了AI编程对新手学习的影响，发现无限制使用AI会导致"能力崩溃" - 虽然表面生产力高，但实际维护能力极差，而通过"解释门"的脚手架式AI辅助能显著改善学习效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，出现了"氛围编程"现象，新手程序员更关注语义意图而非语法实现。研究者担心如果没有教学护栏，这种模式会与认知技能获取根本性错配，导致新手外包了形成图式所需的内在认知负荷，而非仅仅卸载外在负荷，从而积累"认知债务"，产生"脆弱专家"。

Method: 采用被试间实验设计(N=78)，使用基于Claude 3.5 Sonnet的Cursor IDE插件。参与者分为三组：手动控制组、无限制AI组（外包模式）、脚手架AI组（卸载模式）。脚手架组采用新颖的"解释门"机制，利用实时LLM-as-a-Judge框架，在生成代码集成前强制执行"教回"协议。

Result: 结果显示"能力崩溃"现象：无限制AI用户的生产力与脚手架组相当(p<.001 vs.手动组)，但在后续AI黑屏维护任务中失败率高达77%，而脚手架组仅为39%。质性分析表明，成功的氛围编码者会自然进行自我脚手架，将AI视为顾问而非承包商。

Conclusion: 研究强调未来学习系统必须强制执行元认知摩擦，以防止大规模产生不可维护的代码。成功的AI辅助编程需要将AI作为认知卸载工具而非认知外包工具，通过教学性干预确保学习者真正理解代码逻辑。

Abstract: The democratization of Large Language Models (LLMs) has given rise to ``Vibe Coding," a workflow where novice programmers prioritize semantic intent over syntactic implementation. While this lowers barriers to entry, we hypothesize that without pedagogical guardrails, it is fundamentally misaligned with cognitive skill acquisition. Drawing on the distinction between Cognitive Offloading and Cognitive Outsourcing, we argue that unrestricted AI encourages novices to outsource the Intrinsic Cognitive Load required for schema formation, rather than merely offloading Extraneous Load. This accumulation of ``Epistemic Debt" creates ``Fragile Experts" whose high functional utility masks critically low corrective competence.
  To quantify and mitigate this debt, we conducted a between-subjects experiment (N=78) using a custom Cursor IDE plugin backed by Claude 3.5 Sonnet. Participants represented "AI-Native" learners across three conditions: Manual (Control), Unrestricted AI (Outsourcing), and Scaffolded AI (Offloading). The Scaffolded condition utilized a novel ``Explanation Gate," leveraging a real-time LLM-as-a-Judge framework to enforce a ``Teach-Back" protocol before generated code could be integrated.
  Results reveal a ``Collapse of Competence": while Unrestricted AI users matched the productivity of the Scaffolded group (p < .001 vs. Manual), they suffered a 77% failure rate in a subsequent AI-Blackout maintenance task, compared to only 39% in the Scaffolded group. Qualitative analysis suggests that successful vibe coders naturally engage in self-scaffolding, treating the AI as a consultant rather than a contractor. We discuss the implications for the maintainability of AI-generated software and propose that future learning systems must enforce Metacognitive Friction to prevent the mass production of unmaintainable code.

</details>


### [12] [CodeHacker: Automated Test Case Generation for Detecting Vulnerabilities in Competitive Programming Solutions](https://arxiv.org/abs/2602.20213)
*Jingwei Shi,Xinxiang Yin,Jing Huang,Jinman Zhao,Shengyu Tao*

Main category: cs.SE

TL;DR: CodeHacker是一个自动化代理框架，专门生成针对性对抗测试用例来暴露程序提交中的潜在漏洞，通过多策略攻击和校准阶段提高代码生成评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试缺乏对细微边界情况的覆盖，导致错误解决方案也能通过测试。需要更强大的测试用例来暴露LLM代码生成中的潜在漏洞。

Method: 采用多策略方法（压力测试、反哈希攻击、逻辑特定目标攻击），模拟竞赛编程中的黑客机制。引入校准阶段，通过自生成的对抗探针迭代优化验证器和检查器。

Result: CodeHacker显著提高了现有数据集的真负率，有效过滤掉之前被接受的错误解决方案。生成的对抗案例作为训练数据能提升RL训练模型在LiveCodeBench等基准上的性能。

Conclusion: CodeHacker框架通过自动化生成针对性对抗测试用例，提高了代码生成评估的鲁棒性，同时生成的对抗数据还能用于提升模型性能。

Abstract: The evaluation of Large Language Models (LLMs) for code generation relies heavily on the quality and robustness of test cases. However, existing benchmarks often lack coverage for subtle corner cases, allowing incorrect solutions to pass. To bridge this gap, we propose CodeHacker, an automated agent framework dedicated to generating targeted adversarial test cases that expose latent vulnerabilities in program submissions. Mimicking the hack mechanism in competitive programming, CodeHacker employs a multi-strategy approach, including stress testing, anti-hash attacks, and logic-specific targeting to break specific code submissions. To ensure the validity and reliability of these attacks, we introduce a Calibration Phase, where the agent iteratively refines its own Validator and Checker via self-generated adversarial probes before evaluating contestant code.Experiments demonstrate that CodeHacker significantly improves the True Negative Rate (TNR) of existing datasets, effectively filtering out incorrect solutions that were previously accepted. Furthermore, generated adversarial cases prove to be superior training data, boosting the performance of RL-trained models on benchmarks like LiveCodeBench.

</details>


### [13] [PhantomRun: Auto Repair of Compilation Errors in Embedded Open Source Software](https://arxiv.org/abs/2602.20284)
*Han Fu,Andreas Ermedahl,Sigrid Eldh,Kristian Wiklund,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: PhantomRun是一个基于大语言模型的自动化框架，用于修复嵌入式系统CI编译失败问题，成功修复率可达45%。


<details>
  <summary>Details</summary>
Motivation: 嵌入式软件的持续集成（CI）管道在编译时经常失败，消耗大量开发人员调试时间。研究发现硬件依赖是编译失败的主要原因，其次是语法错误和构建脚本问题。

Method: PhantomRun利用大语言模型（LLMs）生成和验证CI编译失败的修复方案。框架通过为GitHub Actions和GitLab CI以及四种不同构建系统提供适配层，解决嵌入式系统项目中多样化的构建基础设施和工具链挑战。利用构建日志、源代码、历史修复记录和编译器错误消息来合成修复方案。

Result: 评估显示PhantomRun在目标项目中成功修复了高达45%的CI编译失败，证明了基于LLM的修复在嵌入式系统CI管道中的可行性。

Conclusion: 基于大语言模型的自动化修复框架能够有效处理嵌入式系统CI编译失败问题，大多数修复只需要相对较小的改动，自动化修复具有潜力，前提是能够处理多样化的设置和缺乏测试数据的问题。

Abstract: Continuous Integration (CI) pipelines for embedded software sometimes fail during compilation, consuming significant developer time for debugging. We study four major open-source embedded system projects, spanning over 4000 build failures from the project's CI runs. We find that hardware dependencies account for the majority of compilation failures, followed by syntax errors and build-script issues. Most repairs need relatively small changes, making automated repair potentially suitable as long as the diverse setups and lack of test data can be handled.
  In this paper, we present PhantomRun, an automated framework that leverages large language models (LLMs) to generate and validate fixes for CI compilation failures. The framework addresses the challenge of diverse build infrastructures and tool chains across embedded system projects by providing an adaptation layer for GitHub Actions and GitLab CI and four different build systems. PhantomRun utilizes build logs, source code, historical fixes, and compiler error messages to synthesize fixes using LLMs. Our evaluations show that PhantomRun successfully repairs up to 45% of CI compilation failures across the targeted projects, demonstrating the viability of LLM-based repairs for embedded-system CI pipelines.

</details>


### [14] [Quantifying the Expectation-Realisation Gap for Agentic AI Systems](https://arxiv.org/abs/2602.20292)
*Sebastian Lobentanzer*

Main category: cs.SE

TL;DR: AI工具的实际生产力提升远低于预期，软件工程中开发者预期24%加速但实际被拖慢19%，临床文档工具声称节省数分钟但实际不到1分钟，临床决策支持工具外部验证性能远低于开发者报告指标


<details>
  <summary>Details</summary>
Motivation: 揭示AI系统部署中期望与现实之间的系统性差距，量化这种期望-实现差距，为更现实的AI工具评估和部署提供依据

Method: 回顾软件工程、临床文档和临床决策支持领域的对照试验和独立验证研究，量化期望与实现之间的差距

Result: 软件工程：开发者预期24%加速但实际被拖慢19%（43个百分点校准误差）；临床文档：厂商声称节省数分钟但实际不到1分钟，一个广泛部署工具无统计显著效果；临床决策支持：外部验证性能显著低于开发者报告指标

Conclusion: 期望-实现差距由工作流集成摩擦、验证负担、测量构造不匹配和系统异质性驱动，需要结构化规划框架，要求明确的量化效益预期并考虑人类监督成本

Abstract: Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic heterogeneity in treatment effects. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in.

</details>


### [15] [UAMTERS: Uncertainty-Aware Mutation Analysis for DL-enabled Robotic Software](https://arxiv.org/abs/2602.20334)
*Chengjie Lu,Jiahui Wu,Shaukat Ali,Malaika Din Hashmi,Sebastian Mathias Thomle Mason,Francois Picard,Mikkel Labori Olsen,Thomas Peyrucain*

Main category: cs.SE

TL;DR: 提出UAMTERS框架，针对DL赋能机器人软件的不确定性进行变异分析，通过不确定性感知的变异算子和评分指标评估测试套件在不确定性环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 自适应机器人常集成深度学习组件以增强自主性，但DL软件固有的不确定性使其在动态环境中的可靠性难以保证。现有测试生成技术和传统变异分析缺乏针对DL软件不确定性的评估方法。

Method: 提出UAMTERS框架：1）设计不确定性感知变异算子，显式注入随机不确定性到DL赋能机器人软件中；2）提出变异评分指标，量化测试套件在不同不确定性水平下检测故障的能力。

Result: 在三个机器人案例研究中评估UAMTERS，证明其能更有效地区分测试套件质量，并捕捉DL软件中由不确定性引发的故障。

Conclusion: UAMTERS填补了针对DL赋能机器人软件不确定性进行变异分析的技术空白，为评估测试套件在不确定性环境下的有效性提供了系统化框架。

Abstract: Self-adaptive robots adjust their behaviors in response to unpredictable environmental changes. These robots often incorporate deep learning (DL) components into their software to support functionality such as perception, decision-making, and control, enhancing autonomy and self-adaptability. However, the inherent uncertainty of DL-enabled software makes it challenging to ensure its dependability in dynamic environments. Consequently, test generation techniques have been developed to test robot software, and classical mutation analysis injects faults into the software to assess the test suite's effectiveness in detecting the resulting failures. However, there is a lack of mutation analysis techniques to assess the effectiveness under the uncertainty inherent to DL-enabled software. To this end, we propose UAMTERS, an uncertainty-aware mutation analysis framework that introduces uncertainty-aware mutation operators to explicitly inject stochastic uncertainty into DL-enabled robotic software, simulating uncertainty in its behavior. We further propose mutation score metrics to quantify a test suite's ability to detect failures under varying levels of uncertainty. We evaluate UAMTERS across three robotic case studies, demonstrating that UAMTERS more effectively distinguishes test suite quality and captures uncertainty-induced failures in DL-enabled software.

</details>


### [16] [Codified Context: Infrastructure for AI Agents in a Complex Codebase](https://arxiv.org/abs/2602.20478)
*Aristidis Vasilopoulos*

Main category: cs.SE

TL;DR: 提出一个三组件编码上下文基础设施，解决LLM代理助手在多代理项目中缺乏持久记忆的问题，包含热记忆宪法、19个专业代理和冷记忆知识库。


<details>
  <summary>Details</summary>
Motivation: LLM代理助手缺乏持久记忆，导致跨会话一致性差、忘记项目约定、重复已知错误。现有配置方法难以扩展到大型多代理项目。

Method: 开发三组件基础设施：1)热记忆宪法（编码约定、检索钩子和编排协议）；2)19个专业领域专家代理；3)冷记忆知识库（34个按需规范文档）。在108,000行C#分布式系统中构建。

Result: 报告了283个开发会话中的基础设施增长和交互模式的定量指标，以及四个观察性案例研究，展示了编码上下文如何跨会话传播以防止故障和保持一致性。

Conclusion: 提出的编码上下文框架解决了LLM代理在多代理项目中的持久记忆问题，已作为开源配套仓库发布，能够有效防止故障并保持项目一致性。

Abstract: LLM-based agentic coding assistants lack persistent memory: they lose coherence across sessions, forget project conventions, and repeat known mistakes. Recent studies characterize how developers configure agents through manifest files, but an open challenge remains how to scale such configurations for large, multi-agent projects. This paper presents a three-component codified context infrastructure developed during construction of a 108,000-line C# distributed system: (1) a hot-memory constitution encoding conventions, retrieval hooks, and orchestration protocols; (2) 19 specialized domain-expert agents; and (3) a cold-memory knowledge base of 34 on-demand specification documents. Quantitative metrics on infrastructure growth and interaction patterns across 283 development sessions are reported alongside four observational case studies illustrating how codified context propagates across sessions to prevent failures and maintain consistency. The framework is published as an open-source companion repository.

</details>


### [17] [A Case Study on Runtime Verification of a Continuous Deployment Process](https://arxiv.org/abs/2602.20598)
*Shoma Ansai,Masaki Waga*

Main category: cs.SE

TL;DR: 该研究应用运行时监控到基于FluxCD的持续部署流程，发现FluxCD在镜像推送到GHCR后5分钟内不一定能检测到新镜像，但在10分钟内总能检测到，同时验证了SyMon监控工具在近实时监控中的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索运行时监控在FluxCD持续部署流程中的应用效果，特别是监控部署更新的检测延迟和监控工具的性能表现。

Method: 使用SyMon监控工具对由GitHub Actions、GitHub Container Registry、FluxCD和Kubernetes应用组成的系统进行日志监控，将FluxCD轮询日志解析最新镜像标签视为部署更新检测。

Result: 研究发现FluxCD在镜像推送到GHCR后5分钟内不一定能检测到新镜像，但在收集的日志中10分钟内总能检测到；同时SyMon监控工具在近实时监控中速度足够快。

Conclusion: 运行时监控在FluxCD持续部署流程中可行，SyMon适合近实时监控，但需要注意FluxCD检测新镜像可能存在5-10分钟的延迟。

Abstract: We report our experience in applying runtime monitoring to a FluxCD-based continuous deployment (CD) process. Our target system consists of GitHub Actions, GitHub Container Registry (GHCR), FluxCD, and an application running on Kubernetes. We monitored its logs using SyMon. In our setting, we regard a deployment update as detected when FluxCD's polling log resolves the latest image tag. Through the case study, we found that FluxCD did not always detect a new image within five minutes after it was pushed to GHCR, whereas it always did so within ten minutes in the collected logs. Moreover, our results show that SyMon is fast enough for near-real-time monitoring in our setting.

</details>


### [18] [SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference](https://arxiv.org/abs/2602.20610)
*Cuong Chi Le,Minh V. T Pham,Tung Vu Duy,Cuong Duc Van,Huy N. Phan,Hoang N. Phan,Tien N. Nguyen*

Main category: cs.SE

TL;DR: SpecMind是一个基于大语言模型的框架，通过反馈驱动的多轮提示方法迭代生成和优化程序后置条件，相比单次提示方法在准确性和完整性上显著提升。


<details>
  <summary>Details</summary>
Motivation: 规范对于确保程序正确性至关重要，但手动编写规范既困难又耗时。现有的基于大语言模型的单次提示方法生成的后置条件往往不准确，需要更有效的方法来生成高质量的规范。

Method: SpecMind将大语言模型视为交互式和探索性推理器而非单次生成器。采用反馈驱动的多轮提示方法，通过隐式和显式正确性反馈迭代优化候选后置条件，并自主决定何时停止。这种方法促进更深入的代码理解，通过探索性尝试更好地对齐真实程序行为。

Result: 实证评估表明，SpecMind在生成后置条件的准确性和完整性方面显著优于最先进的方法。

Conclusion: SpecMind通过将大语言模型作为交互式推理器，采用反馈驱动的迭代方法，有效提升了程序后置条件生成的质量，为解决规范编写难题提供了新思路。

Abstract: Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.

</details>


### [19] [An LLM-driven Scenario Generation Pipeline Using an Extended Scenic DSL for Autonomous Driving Safety Validation](https://arxiv.org/abs/2602.20644)
*Fida Khandaker Safa,Yupeng Jiang,Xi Zheng*

Main category: cs.SE

TL;DR: 提出一个可扩展、可验证的流水线，利用大语言模型和概率性中间表示，从真实事故报告中自动提取语义场景配置并生成模拟就绪场景，用于自动驾驶系统安全验证。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法有效将包含文本摘要和草图的多模态事故报告转化为精确、可执行的模拟场景，限制了自动驾驶系统安全验证的可扩展性。

Method: 使用大语言模型（GPT-4o mini）和概率性中间表示（扩展的Scenic领域特定语言），通过中间Scenic DSL层分离高层语义理解和低层场景渲染，从事故报告中提取语义场景配置并生成模拟场景。

Result: 在NHTSA CIREN数据库案例评估中，知识提取准确率高：环境和道路网络属性100%正确，轨迹提取分别为97%和98%。在CARLA模拟器中执行生成的场景，在2000个场景变体中一致触发了预期的交通规则违规。

Conclusion: 该流水线为自动驾驶系统安全验证提供了法律依据充分、可扩展且可验证的方法，能够有效利用真实事故报告生成高质量的模拟测试场景。

Abstract: Real-world crash reports, which combine textual summaries and sketches, are valuable for scenario-based testing of autonomous driving systems (ADS). However, current methods cannot effectively translate this multimodal data into precise, executable simulation scenarios, hindering the scalability of ADS safety validation. In this work, we propose a scalable and verifiable pipeline that uses a large language model (GPT-4o mini) and a probabilistic intermediate representation (an Extended Scenic domain-specific language) to automatically extract semantic scenario configurations from crash reports and generate corresponding simulation-ready scenarios. Unlike earlier approaches such as ScenicNL and LCTGen (which generate scenarios directly from text) or TARGET (which uses deterministic mappings from traffic rules), our method introduces an intermediate Scenic DSL layer to separate high-level semantic understanding from low-level scenario rendering, reducing errors and capturing real-world variability. We evaluated the pipeline on cases from the NHTSA CIREN database. The results show high accuracy in knowledge extraction: 100% correctness for environmental and road network attributes, and 97% and 98% for oracle and actor trajectories, respectively, compared to human-derived ground truth. We executed the generated scenarios in the CARLA simulator using the Autoware driving stack, and they consistently triggered the intended traffic-rule violations (such as opposite-lane crossing and red-light running) across 2,000 scenario variations. These findings demonstrate that the proposed pipeline provides a legally grounded, scalable, and verifiable approach to ADS safety validation.

</details>


### [20] [Agile V: A Compliance-Ready Framework for AI-Augmented Engineering -- From Concept to Audit-Ready Delivery](https://arxiv.org/abs/2602.20684)
*Christopher Koch,Joshua Andreas Wellbrock*

Main category: cs.SE

TL;DR: Agile V框架将敏捷迭代与V模型验证结合为无限循环，通过AI代理自动生成审计就绪文档，实现100%需求级验证，每个周期仅需6次人工交互，估计成本降低10-50倍。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅助工程工作流缺乏内置机制来在机器速度交付时维护任务级验证和监管可追溯性，需要解决验证与审计文档自动生成的问题。

Method: Agile V框架将敏捷迭代与V模型验证合并为连续无限循环，部署专门的AI代理处理需求、设计、构建、测试和合规，由强制性人工审批门控制。

Result: 硬件在环系统案例研究（约500行代码，8个需求，54个测试）支持所有三个假设：自动生成审计就绪文档，实现100%需求级通过率，每个周期仅需6次提示，估计成本比COCOMO II基准降低10-50倍。

Conclusion: Agile V框架通过在开发过程中嵌入独立验证和审计工件生成，实现了机器速度交付下的任务级验证和监管可追溯性，显著降低了成本并提高了效率。

Abstract: Current AI-assisted engineering workflows lack a built-in mechanism to maintain task-level verification and regulatory traceability at machine-speed delivery. Agile V addresses this gap by embedding independent verification and audit artifact generation into each task cycle. The framework merges Agile iteration with V-Model verification into a continuous Infinity Loop, deploying specialized AI agents for requirements, design, build, test, and compliance, governed by mandatory human approval gates. We evaluate three hypotheses: (H1) audit-ready artifacts emerge as a by-product of development, (H2) 100% requirement-level verification is achievable with independent test generation, and (H3) verified increments can be delivered with single-digit human interactions per cycle. A feasibility case study on a Hardware-in-the-Loop system (about 500 LOC, 8 requirements, 54 tests) supports all three hypotheses: audit-ready documentation was generated automatically (H1), 100% requirement-level pass rate was achieved (H2), and only 6 prompts per cycle were required (H3), yielding an estimated 10-50x cost reduction versus a COCOMO II baseline (sensitivity range from pessimistic to optimistic assumptions). We invite independent replication to validate generalizability.

</details>


### [21] [PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring](https://arxiv.org/abs/2602.20717)
*Xiting Liu,Yuetong Liu,Yitong Zhang,Jia Li,Shi-Min Hu*

Main category: cs.SE

TL;DR: PackMonitor是首个能从根本上消除LLM在依赖推荐中包幻觉的方法，通过监控解码过程并限制输出到权威包列表，将幻觉率降至零。


<details>
  <summary>Details</summary>
Motivation: LLM在软件依赖推荐中存在严重的包幻觉问题，现有方法只能降低但不能消除幻觉，导致持续的软件安全风险。作者认为包幻觉在理论上是可预防的，因为包有效性可通过有限且可枚举的权威包列表来判定。

Method: PackMonitor通过三个关键技术实现：1) 上下文感知解析器，持续监控模型输出并仅在安装命令生成时触发干预；2) 包名干预器，严格限制解码空间到权威包列表；3) DFA缓存机制，支持扩展到数百万个包且开销可忽略。

Result: 在五个广泛使用的LLM上的实验表明，PackMonitor作为免训练、即插即用的解决方案，能持续将包幻觉率降至零，同时保持低延迟推理并保留原始模型能力。

Conclusion: PackMonitor首次实现了从根本上消除LLM包幻觉的目标，通过理论可预防性的洞察和实用的监控干预机制，为LLM在软件依赖推荐中的可信应用提供了可靠保障。

Abstract: As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminate them, leaving persistent software security risks.
  In this work, we argue that package hallucinations are theoretically preventable based on the key insight that package validity is decidable through finite and enumerable authoritative package lists. Building on this, we propose PackMonitor, the first approach capable of fundamentally eliminating package hallucinations by continuously monitoring the model's decoding process and intervening when necessary. To implement this in practice, PackMonitor addresses three key challenges: (1) determining when to trigger intervention via a Context-Aware Parser that continuously monitors model outputs and selectively activates intervening only during installation command generation; (2) resolving how to intervene by employing a Package-Name Intervenor that strictly limits the decoding space to an authoritative package list; and (3) ensuring monitoring efficiency through a DFA-Caching Mechanism that enables scalability to millions of packages with negligible overhead. Extensive experiments on five widely used LLMs demonstrate that PackMonitor is a training-free, plug-and-play solution that consistently reduces package hallucination rates to zero while maintaining low-latency inference and preserving original model capabilities.

</details>


### [22] [Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs](https://arxiv.org/abs/2602.20799)
*Guangsheng Ou,Qiming Zhang,Sirong Chen,Anji Li,Dong Xu,Tiancheng Luo,Dekun Dai,Cuiyun Gao,Long Wang,Jun Zhou,Mingwei Liu,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出UCD-Training框架，通过代码图构建和两阶段训练（依赖保持的持续预训练+图基监督微调）来解决LLM在未见代码库上的性能不佳和幻觉问题，并引入UnseenCodeBench基准进行评估。


<details>
  <summary>Details</summary>
Motivation: LLM在新发布的软件框架上表现不佳且易产生幻觉，因为训练时未接触这些环境。现有方法如RAG只能部分缓解，仅通过提示注入知识不足以让模型理解代码库组件间内在关系并进行正确组合推理。未见代码库通常只有源代码，缺乏高质量使用导向的训练数据，现有数据合成方法难以捕捉使用场景。

Method: UCD-Training两阶段训练框架：1) 解析源代码构建代码图；2) 依赖保持的持续预训练(CPT)使用文件级依赖数据；3) 图基监督微调(SFT)在三种合成数据上：单跳关系推理数据、组合API推理数据、代码库利用数据，均增强了显式推理轨迹。

Result: 在多个代码库上进行了综合实验，并引入了新的基准UnseenCodeBench用于未见代码库的代码生成评估。

Conclusion: UCD-Training通过代码图构建和两阶段训练，有效解决了LLM在未见代码库上的理解和推理问题，为代码生成任务提供了新的解决方案和评估基准。

Abstract: In the context of newly release software frameworks, large language models (LLMs) often exhibit poor performance and a high rate of hallucination, as they are not exposed to such environments during training. Although inference-time augmentation techniques such as retrieval-augmented generation (RAG) can partially mitigate hallucinations, knowledge injection through prompting alone is insufficient to enable models to fully understand the intrinsic relationships among different components of a codebase, or to reason about the correct compositions and apply. Although explicit knowledge injection can be achieved through post-training, compared with public code domains, unseen codebases typically provide only source code and lack large volumes of high-quality, usage-oriented code that can be directly leveraged as training data. Consequently, existing data synthesis approaches are insufficient to adequately capture unseen codebases usage scenarios when restricted to source code alone. To address these challenges, we propose UCD-Training, a two-stage training framework for reasoning-aware data synthesis grounded in a code graph constructed from unseen codebases. UCD-Training first parses the source code to build a code graph, then conducts dependency-preserving continued pretraining (CPT) using file-level dependency data, followed by graph-grounded supervised fine-tuning (SFT) on three types of synthesized data augmented with explicit reasoning traces: (1) single-hop relation reasoning data, (2) compositional API reasoning data, and (3) codebase utilization data. We further introduce a new benchmark, UnseenCodeBench, for code generation on unseen codebases and conduct comprehensive experiments across multiple codebases.

</details>


### [23] [Toward an Agentic Infused Software Ecosystem](https://arxiv.org/abs/2602.20979)
*Mark Marron*

Main category: cs.SE

TL;DR: 提出Agentic Infused Software Ecosystem (AISE)框架，包含AI代理、编程语言/API、运行时环境三大支柱，旨在构建协同发展的软件生态系统。


<details>
  <summary>Details</summary>
Motivation: 为了充分发挥AI代理在软件开发中的潜力，需要重新思考软件生态系统本身，构建一个能够支持AI代理与人类开发者协同工作的新架构。

Method: 提出AISE框架，基于三大支柱：1) AI代理本身；2) 编程语言和API作为代理完成任务和与人类交互的通信基础；3) 运行时环境和生态系统，提供代理与外部世界交互的能力。

Result: 提出了一个全面的AISE框架概念，为未来AI代理驱动的软件开发生态系统提供了理论基础和架构方向。

Conclusion: 要实现AISE愿景，三大支柱必须协同发展，既要满足当前和未来AI代理的需求，也要支持人类开发者的工作，构建一个真正协同的软件生态系统。

Abstract: Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming language and APIs (or tools) that these agents use to accomplish tasks, and increasingly, serve as the communication substrate that humans and AI agents interact and collaborate through. The final pillar is the runtime environment and ecosystem that agents operate within, and which provide the capabilities that programmatic agents use to interface with (and effect actions in) the external world. To realize the vision of AISE, all three pillars must be advanced in a holistic manner, and critically, in a manner that is synergistic for AI agents as they exist today, those that will exist in the future, and for the human developers that work alongside them.

</details>


### [24] [A Modular Multi-Document Framework for Scientific Visualization and Simulation in Java](https://arxiv.org/abs/2602.21026)
*David Heddle*

Main category: cs.SE

TL;DR: 开发了一个用于JVM生态系统的模块化多文档界面框架，专注于科学可视化和模拟，通过架构分离实现可视化层、模拟引擎和硬件加速3D渲染的解耦。


<details>
  <summary>Details</summary>
Motivation: 为科学和工程桌面应用提供长期可维护的框架，解决传统可视化系统中依赖耦合问题，特别是避免2D应用强制依赖3D功能。

Method: 采用模块化设计，将3D功能隔离为独立模块，定义核心抽象、线程模型、模拟集成策略和依赖隔离方法，通过Maven Central提供公开访问。

Result: 成功实现了一个可扩展框架，通过气体膨胀模拟案例展示了架构内聚性，支持实时3D模拟与同步2D熵图绘制。

Conclusion: 该框架为JVM生态系统提供了有效的科学可视化解决方案，通过模块化设计实现了良好的架构分离，适用于长期运行的科研和工程应用。

Abstract: This paper presents the design and implementation of a modular multi-document interface (MDI) framework for scientific visualization and simulation in the Java Virtual Machine (JVM) ecosystem. The framework emphasizes architectural separation between visualization layers, simulation engines, and optional hardware-accelerated 3D rendering. 3D functionality is isolated into a separate module to prevent unnecessary dependency coupling in 2D-only applications. We describe the core abstractions, threading model, simulation integration strategy, and dependency isolation approach. A case study involving a real-time 3D gas expansion simulation integrated with synchronized 2D entropy plotting demonstrates architectural cohesion. The framework is publicly available via Maven Central and targets long-lived scientific and engineering desktop applications.

</details>


### [25] [Automated Detection and Mitigation of Dependability Failures in Healthcare Scenarios through Digital Twins](https://arxiv.org/abs/2602.21037)
*Bruno Guindani,Matteo Camilli,Livia Lestingi,Marcello M. Bersani*

Main category: cs.SE

TL;DR: M-GENGAR：基于数字孪生的医疗CPS可靠性保障方法，通过形式化建模、场景检测和缓解策略合成，在呼吸机案例中87.5%的场景下表现优于人类决策。


<details>
  <summary>Details</summary>
Motivation: 医疗CPS（患者-设备-医生三元组）存在系统异构性和人机行为不确定性，现有临床决策支持系统缺乏主动可靠性保障方法，需要在患者安全受损前识别和缓解故障场景。

Method: 基于闭环数字孪生范式，结合随机混合自动机建模、数据驱动的患者动态学习、统计模型检查，通过离线关键场景检测（模型空间探索和多样性分析）识别违反可靠性要求的场景，并自动合成缓解策略。

Result: 在呼吸机用例评估中，87.5%的场景下，通过形式化博弈论分析合成的策略在稳定患者生命体征方面至少与人类决策同等有效，且相关指标平均比正常健康值接近20%。

Conclusion: M-GENGAR为医疗CPS提供了一种主动的可靠性保障方法，能够系统识别关键故障场景并自动合成有效的缓解策略，在呼吸机案例中验证了其优于人类决策的潜力。

Abstract: Medical Cyber-Physical Systems (CPSs) integrating Patients, Devices, and healthcare personnel (Physicians) form safety-critical PDP triads whose dependability is challenged by system heterogeneity and uncertainty in human and physiological behavior. While existing clinical decision support systems support clinical practice, there remains a need for proactive, reliability-oriented methodologies capable of identifying and mitigating failure scenarios before patient safety is compromised. This paper presents M-GENGAR, a methodology based on a closed-loop Digital Twin (DT) paradigm for dependability assurance of medical CPSs. The approach combines Stochastic Hybrid Automata modeling, data-driven learning of patient dynamics, and Statistical Model Checking with an offline critical scenario detection phase that integrates model-space exploration and diversity analysis to systematically identify and classify scenarios violating expert-defined dependability requirements. M-GENGAR also supports the automated synthesis of mitigation strategies, enabling runtime feedback and control within the DT loop. We evaluate M-GENGAR on a representative use case study involving a pulmonary ventilator. Results show that, in 87.5% of the evaluated scenarios, strategies synthesized through formal game-theoretic analysis stabilize patient vital metrics at least as effectively as human decision-making, while maintaining relevant metrics 20% closer to nominal healthy values on average.

</details>


### [26] [Validation of an analyzability model for quantum software: a family of experiments](https://arxiv.org/abs/2602.21074)
*Ana Díaz-Muñoz,José A. Cruz-Lemus,Moisés Rodríguez,Maria Teresa Baldassarre,Mario Piattini*

Main category: cs.SE

TL;DR: 该研究通过系列实验验证了基于ISO/IEC 25010标准的混合软件可分析性模型中量子组件部分的有效性，证明该模型能有效区分不同可分析性水平的量子算法，并与人类感知一致。


<details>
  <summary>Details</summary>
Motivation: 混合软件（结合经典和量子组件）的可分析性是其可维护性和工业应用的关键因素。需要验证先前提出的基于ISO/IEC 25010标准的混合软件可分析性模型中量子组件的有效性。

Method: 采用系列实验方法，包含四项研究，涉及学术和专业环境中不同背景的参与者。评估模型测量量子算法可分析性的能力，并检验模型计算的可分析性水平与参与者对算法复杂性感知之间的关系。

Result: 结果表明，提出的模型能有效区分具有不同可分析性水平的量子软件组件，并且与人类感知保持一致，增强了其在量子计算领域的有效性。

Conclusion: 该研究通过实证验证了混合软件可分析性模型中量子组件的有效性，为量子软件工程提供了可靠的分析工具，有助于促进量子软件的工业应用。

Abstract: The analyzability of hybrid software, which integrates both classical and quantum components, is a key factor in ensuring its maintainability and industrial adoption. This article presents the empirical validation, through a family of experiments, of the quantum component of a previously proposed hybrid software analyzability model based on the ISO/IEC 25010 standard. The experimental series consists of four studies involving participants with diverse profiles in both academic and professional settings. In these experiments, the model's ability to effectively measure the analyzability of quantum algorithms is assessed, and the relationship between the analyzability levels computed by the model and the participant's perceptions of the complexity of these algorithms is examined. The results indicate that the proposed model effectively distinguishes between quantum software components with varying levels of analyzability and aligns with human perception, reinforcing its validity in quantum computing.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [cuRPQ: A High-Performance GPU-Based Framework for Processing Regular and Conjunctive Regular Path Queries](https://arxiv.org/abs/2602.20748)
*Sungwoo Park,Seohyeon Kim,Min-Soo Kim*

Main category: cs.DB

TL;DR: cuRPQ：首个GPU加速的正则路径查询处理框架，性能提升数个数量级


<details>
  <summary>Details</summary>
Motivation: 正则路径查询（RPQ）及其变体（如CRPQ）在图分析中日益重要，但现有方法计算开销大，且没有GPU加速方案

Method: 提出cuRPQ框架，包含新型遍历算法、高效访问集管理方案和并发探索-物化策略，专门针对GPU优化

Result: 实验表明cuRPQ性能比现有最优方法高出数个数量级，且不会出现内存溢出错误

Conclusion: cuRPQ是首个成功实现GPU加速的正则路径查询处理框架，显著提升了查询性能

Abstract: Regular path queries (RPQs) are fundamental for path-constrained reachability analysis, and more complex variants such as conjunctive regular path queries (CRPQs) are increasingly used in graph analytics. Evaluating these queries is computationally expensive, but to the best of our knowledge, no prior work has explored GPU acceleration. In this paper, we propose cuRPQ, a high-performance GPU-optimized framework for processing RPQs and CRPQs. cuRPQ addresses the key GPU challenges through a novel traversal algorithm, an efficient visited-set management scheme, and a concurrent exploration-materialization strategy. Extensive experiments show that cuRPQ outperforms state-of-the-art methods by orders of magnitude, without out-of-memory errors.

</details>


### [28] [RISK: Efficiently processing rich spatial-keyword queries on encrypted geo-textual data](https://arxiv.org/abs/2602.20952)
*Zhen Lv,Cong Cao,Hongwei Huo,Jiangtao Cui,Yanguo Peng,Hui Li,Yingfan Liu*

Main category: cs.DB

TL;DR: RISK：一种支持丰富空间-文本查询的加密地理文本数据模型，基于k最近邻四叉树构建，同时支持安全范围查询和k最近邻查询，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对称可搜索加密方案针对地理文本数据使用任务特定、不兼容的索引来处理孤立的安全查询（如范围查询或k最近邻查询），导致多索引开销过大，限制了实际应用。

Method: 提出RISK模型，采用"先文本后空间"的方式，基于新颖的k最近邻四叉树（kQ-tree）构建，该树嵌入了代表性和区域最近邻信息，并使用标准密码学工具（如密钥哈希函数和对称加密）对kQ-tree进行加密。

Result: 在三个真实世界数据集和一个合成数据集上的实验表明，RISK在1%范围查询和10最近邻查询的响应时间上分别比现有最优方法至少快0.5和4个数量级，同时在IND-CKA2模型下可证明安全，并可扩展到多方场景和动态更新。

Conclusion: RISK成功解决了现有地理文本数据对称可搜索加密方案中多索引开销过大的问题，提供了一种统一、高效且安全的解决方案，支持丰富的空间-文本查询，具有实际应用价值。

Abstract: Symmetric searchable encryption (SSE) for geo-textual data has attracted significant attention. However, existing schemes rely on task-specific, incompatible indices for isolated specific secure queries (e.g., range or k-nearest neighbor spatial-keyword queries), limiting practicality due to prohibitive multi-index overhead. To address this, we propose RISK, a model for rich spatial-keyword queries on encrypted geo-textual data. In a textual-first-then-spatial manner, RISK is built on a novel k-nearest neighbor quadtree (kQ-tree) that embeds representative and regional nearest neighbors, with the kQ-tree further encrypted using standard cryptographic tools (e.g., keyed hash functions and symmetric encryption). Overall, RISK seamlessly supports both secure range and k-nearest neighbor queries, is provably secure under IND-CKA2 model, and extensible to multi-party scenarios and dynamic updates. Experiments on three real-world and one synthetic datasets show that RISK outperforms state-of-the-art methods by at least 0.5 and 4 orders of magnitude in response time for 1% range queries and 10-nearest neighbor queries, respectively.

</details>
