<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.SE](#cs.SE) [Total: 38]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [From NL2SQL to NL2GeoSQL: GeoSQL-Eval for automated evaluation of LLMs on PostGIS queries](https://arxiv.org/abs/2509.25264)
*Shuyang Hou,Haoyue Jiao,Ziqi Liu,Lutong Xie,Guanyu Chen,Shaowen Wu,Xuefeng Guan,Huayi Wu*

Main category: cs.DB

TL;DR: 本文提出了GeoSQL-Eval框架和GeoSQL-Bench数据集，用于评估LLM在PostGIS空间数据库中的查询生成能力，填补了空间数据库系统化基准的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注通用关系数据库或Google Earth Engine代码生成，缺乏针对空间数据库的系统化基准，而PostGIS环境中的空间函数、几何数据类型和执行语义的复杂性使得扩展NL2SQL任务具有挑战性。

Method: 基于Webb的知识深度模型，构建包含四个认知维度、五个熟练度级别和二十个任务类别的评估框架；开发包含14178个问题的基准数据集，涵盖三种任务类型、340个PostGIS函数和82个领域特定数据库；系统评估了24个代表性模型。

Result: 通过熵权重和统计分析揭示了模型在性能、错误分布和资源消耗模式上的差异；建立了公开的GeoSQL-Eval排行榜，支持全球研究团队进行持续测试和比较。

Conclusion: 这些贡献不仅扩展了NL2SQL应用的边界，还为评估LLM在空间数据库环境中的性能提供了标准化、可解释和可扩展的框架，为地理信息科学、城市研究和空间分析中的模型优化和应用提供了有价值的见解。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
progress in natural language understanding and structured query generation
(NL2SQL). However, extending these advances to GeoSQL tasks in the PostGIS
environment remains challenging due to the complexity of spatial functions,
geometric data types, and execution semantics. Existing evaluations primarily
focus on general relational databases or Google Earth Engine code generation,
leaving a lack of systematic benchmarks tailored to spatial databases. To
address this gap, this study introduces GeoSQL-Eval, the first end-to-end
automated evaluation framework for PostGIS query generation. Built upon Webb's
Depth of Knowledge (DOK) model, the framework encompasses four cognitive
dimensions, five proficiency levels, and twenty task categories, providing a
comprehensive assessment of model performance in terms of knowledge
acquisition, syntactic generation, semantic alignment, execution accuracy, and
robustness. In parallel, we developed GeoSQL-Bench, a benchmark dataset
comprising 14178 questions that span three task types, 340 PostGIS functions,
and 82 domain-specific databases. Leveraging this framework, we systematically
evaluated 24 representative models across six categories, applying
entropy-weighting and statistical analyses to reveal differences in
performance, error distributions, and resource consumption patterns.
Furthermore, we established a public GeoSQL-Eval leaderboard that enables
global research teams to conduct ongoing testing and comparison. These
contributions not only extend the boundaries of NL2SQL applications but also
provide a standardized, interpretable, and scalable framework for evaluating
LLM performance in spatial database contexts, offering valuable insights for
model optimization and applications in geographic information science, urban
studies, and spatial analysis.

</details>


### [2] [ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging](https://arxiv.org/abs/2509.25285)
*Jun Kawasaki*

Main category: cs.DB

TL;DR: ActorDB是一个新颖的数据库架构，集成了单写入者actor模型、增量视图维护和零信任安全模型，旨在降低现代数据密集型应用的架构复杂度。


<details>
  <summary>Details</summary>
Motivation: 通过统一actor持久化、流处理和安全性等复杂概念，为开发者提供更健壮、安全且友好的平台，避免手动集成多个独立系统。

Method: 采用单写入者actor模型处理写入操作，结合增量视图维护技术，并将零信任安全模型作为核心组件集成到数据库架构中。

Result: 提出了核心架构设计，讨论了关键权衡，并定义了MVP的性能标准以验证该方法。

Conclusion: ActorDB通过统一这些强大但复杂的概念，能够为现代数据密集型应用提供更简单、更安全的开发平台。

Abstract: This paper presents ActorDB ( Dekigoto ) , a novel database architecture that
tightly integrates a single-writer actor model for writes, Incremental View
Maintenance (IVM), and a zero-trust security model as a core component. The
primary contribution of this work is the unification of these powerful but
complex concepts into a single, cohesive system designed to reduce
architectural complexity for developers of modern, data-intensive applications.
We argue that by providing these capabilities out-of-the-box, ActorDB can offer
a more robust, secure, and developer-friendly platform compared to solutions
that require manual integration of separate systems for actor persistence,
stream processing, and security. We present the core architecture, discuss the
critical trade-offs in its design, and define the performance criteria for a
Minimum Viable Product (MVP) to validate our approach.

</details>


### [3] [PAT: Pattern-Perceptive Transformer for Error Detection in Relational Databases](https://arxiv.org/abs/2509.25907)
*Jian Fu,Xixian Han,Xiaolong Wan,Wenjian Wang*

Main category: cs.DB

TL;DR: 提出了PAT框架，一种基于Transformer的关系数据库错误检测方法，通过属性感知模式学习和准令牌排列分词器来提高检测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有错误检测方法在处理异构属性时成本高，且数据预处理策略未能充分利用数据序列的变长特性，导致准确性降低。

Method: 1. 学习模式模块捕获属性特定的数据分布；2. QTA分词器根据长度和词类型划分单元格序列，生成词自适应数据令牌；3. 将数据令牌与属性特定模式令牌交错，联合学习共享数据特征和可区分模式特征。

Result: 在广泛实验中，PAT相比最先进的数据错误检测方法实现了优异的F1分数，同时在使用紧凑QTA分词器时显著减少了模型参数和FLOPs。

Conclusion: PAT框架通过属性感知模式学习和高效分词器设计，在关系数据库错误检测中实现了高准确性和效率。

Abstract: Error detection in relational databases is critical for maintaining data
quality and is fundamental to tasks such as data cleaning and assessment.
Current error detection studies mostly employ the multi-detector approach to
handle heterogeneous attributes in databases, incurring high costs.
Additionally, their data preprocessing strategies fail to leverage the
variable-length characteristic of data sequences, resulting in reduced
accuracy. In this paper, we propose an attribute-wise PAttern-perceptive
Transformer (PAT) framework for error detection in relational databases. First,
PAT introduces a learned pattern module that captures attribute-specific data
distributions through learned embeddings during model training. Second, the
Quasi-Tokens Arrangement (QTA) tokenizer is designed to divide the cell
sequence based on its length and word types, and then generate the
word-adaptive data tokens, meanwhile providing compact hyperparameters to
ensure efficiency. By interleaving data tokens with the attribute-specific
pattern tokens, PAT jointly learns shared data features across different
attributes and pattern features that are distinguishable and unique in each
specified attribute. Third, PAT visualizes the attention map to interpret its
error detection mechanism. Extensive experiments show that PAT achieves
excellent F1 scores compared to state-of-the-art data error detection methods.
Moreover, PAT significantly reduces the model parameters and FLOPs when
applying the compact QTA tokenizer.

</details>


### [4] [Experiversum: an Ecosystem for Curating and Enhancing Data-Driven Experimental Science](https://arxiv.org/abs/2509.26102)
*Genoveva Vargas-Solar,Umberto Costa,Jérôme Darmont,Javier Espinosa-Oviedo,Carmem Hara,Sabine Loudcher,Regina Motz,Martin A. Musicante,José-Luis Zechinelli-Martini*

Main category: cs.DB

TL;DR: Experiversum是一个基于数据湖仓的生态系统，支持探索性实验的整理、记录和可复现性，通过迭代数据循环促进结构化研究，并捕获元数据和协作决策。


<details>
  <summary>Details</summary>
Motivation: 解决探索性研究中的可复现性和透明度问题，促进跨学科的数据驱动实践，连接探索性研究和可复现研究之间的鸿沟。

Method: 构建基于数据湖仓的生态系统，支持迭代数据循环，捕获实验元数据和协作决策，通过案例研究验证系统有效性。

Result: 在Earth、Life和Political Sciences领域的案例研究中证明，Experiversum能够促进透明的工作流程和多视角结果解释。

Conclusion: Experiversum成功连接了探索性和可复现性研究，鼓励跨学科的可问责和稳健的数据驱动实践。

Abstract: This paper introduces Experiversum, a lakehouse-based ecosystem that supports
the curation, documentation and reproducibility of exploratory experiments.
Experiversum enables structured research through iterative data cycles, while
capturing metadata and collaborative decisions. Demonstrated through case
studies in Earth, Life and Political Sciences, Experiversum promotes
transparent workflows and multi-perspective result interpretation. Experiversum
bridges exploratory and reproducible research, encouraging accountable and
robust data-driven practices across disciplines.

</details>


### [5] [The Grammar of FAIR: A Granular Architecture of Semantic Units for FAIR Semantics, Inspired by Biology and Linguistics](https://arxiv.org/abs/2509.26434)
*Lars Vogt,Barend Mons*

Main category: cs.DB

TL;DR: 本文提出了FAIR语法，这是一个基于语义单元的模块化架构，旨在弥合人类认知与机器可操作性之间的语义鸿沟，为FAIR数据和知识提供统一的语义框架。


<details>
  <summary>Details</summary>
Motivation: 当前数字基础设施缺乏统一的语义框架来连接人类认知和机器可操作性，无法充分实现FAIR原则。

Method: 引入语义单元概念，包括原子陈述单元和复合单元，实现语义模块化；借鉴生物系统层次结构和自然语言语法，构建语义优先的基础架构。

Result: 开发了FAIR语法架构，使语义单元能够映射到FAIR数字对象，实现格式无关的语义传递。

Conclusion: FAIR语法为跨生态系统基础设施提供了语义优先的基础，为模块化、AI就绪和引用粒度的学术交流铺平了道路。

Abstract: The FAIR Principles aim to make data and knowledge Findable, Accessible,
Interoperable, and Reusable, yet current digital infrastructures often lack a
unifying semantic framework that bridges human cognition and
machine-actionability. In this paper, we introduce the Grammar of FAIR: a
granular and modular architecture for FAIR semantics built on the concept of
semantic units. Semantic units, comprising atomic statement units and composite
compound units, implement the principle of semantic modularisation, decomposing
data and knowledge into independently identifiable, semantically meaningful,
and machine-actionable units. A central metaphor guiding our approach is the
analogy between the hierarchy of level of organisation in biological systems
and the hierarchy of levels of organisation in information systems: both are
structured by granular building blocks that mediate across multiple
perspectives while preserving functional unity. Drawing further inspiration
from concept formation and natural language grammar, we show how these building
blocks map to FAIR Digitial Objects (FDOs), enabling format-agnostic semantic
transitivity from natural language token models to schema-based
representations. This dual biological-linguistic analogy provides a
semantics-first foundation for evolving cross-ecosystem infrastructures, paving
the way for the Internet of FAIR Data and Services (IFDS) and a future of
modular, AI-ready, and citation-granular scholarly communication.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Permuting Transactions in Ethereum Blocks: An Empirical Study](https://arxiv.org/abs/2509.25415)
*Jan Droll*

Main category: cs.DC

TL;DR: 本文通过分析33.5万个以太坊主网区块的交易排列组合，评估随机交易排序的技术可行性，发现约22%的区块排列会违反协议，主要由于私有挖矿交易或接近gas限制的区块，但大多数区块的gas消耗偏差不超过10%。


<details>
  <summary>Details</summary>
Motivation: 评估随机交易排序在以太坊生态系统中的技术可行性，以缓解中心化效应和提高公平性，但需要考虑交易和区块的gas限制及协议规则。

Method: 对超过33.5万个以太坊主网区块的交易进行多次排列和执行，量化协议违规、执行错误和gas消耗偏差。

Result: 22%的区块排列因协议违规而无效；几乎所有在排列中出现执行错误的交易都是私有挖矿交易；仅6%的交易显示gas消耗偏差；98%的区块排列gas消耗偏差不超过10%。

Conclusion: 从技术角度看，如果交易选择处理得当，随机交易排序可能是可行的。

Abstract: Several recent proposals implicitly or explicitly suggest making use of
randomized transaction ordering within a block to mitigate centralization
effects and to improve fairness in the Ethereum ecosystem. However,
transactions and blocks are subject to gas limits and protocol rules. In a
randomized transaction order, the behavior of transactions may change depending
on other transactions in the same block, leading to invalid blocks and varying
gas consumptions. In this paper, we quantify and characterize protocol
violations, execution errors and deviations in gas consumption of blocks and
transactions to examine technical deployability. For that, we permute and
execute the transactions of over 335,000 Ethereum Mainnet blocks multiple
times. About 22% of block permutations are invalid due to protocol violations
caused by privately mined transactions or blocks close to their gas limit.
Also, almost all transactions which show execution errors under permutation but
not in the original order are privately mined transactions. Only 6% of
transactions show deviations in gas consumption and 98% of block permutations
deviate at most 10% from their original gas consumption. From a technical
perspective, these results suggest that randomized transaction ordering may be
feasible if transaction selection is handled carefully.

</details>


### [7] [Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches](https://arxiv.org/abs/2509.25555)
*Amirreza Sokhankhosh,Khalid Hassan,Sara Rouhani*

Main category: cs.DC

TL;DR: 提出两种新颖框架SSFL和BSFL来改进SplitFed Learning，SSFL通过分片机制提升可扩展性和性能，BSFL在SSFL基础上引入区块链架构增强安全性和公平性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习(FL)和分割学习(SL)存在局限性——FL对客户端计算要求高，SL训练时间长。SplitFed Learning(SFL)作为混合方法虽然结合了两者优势，但仍继承了SL的可扩展性、性能和安全性问题。

Method: SSFL通过将SL服务器的工作负载和通信开销分布到多个并行分片中来解决问题；BSFL在SSFL基础上用基于区块链的架构替换集中式服务器，采用委员会驱动的共识机制，并包含评估机制来排除被投毒或篡改的模型更新。

Result: 实验评估显示，SSFL相比基线SL和SFL方法，性能和可扩展性分别提高了31.2%和85.2%；BSFL对数据投毒攻击的抵抗力提高了62.7%，同时在正常操作条件下保持优越性能。

Conclusion: BSFL是首个实现端到端去中心化SplitFed Learning系统的区块链赋能框架，有效解决了现有方法的可扩展性、性能和安全性问题。

Abstract: Collaborative and distributed learning techniques, such as Federated Learning
(FL) and Split Learning (SL), hold significant promise for leveraging sensitive
data in privacy-critical domains. However, FL and SL suffer from key
limitations -- FL imposes substantial computational demands on clients, while
SL leads to prolonged training times. To overcome these challenges, SplitFed
Learning (SFL) was introduced as a hybrid approach that combines the strengths
of FL and SL. Despite its advantages, SFL inherits scalability, performance,
and security issues from SL. In this paper, we propose two novel frameworks:
Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning
(BSFL). SSFL addresses the scalability and performance constraints of SFL by
distributing the workload and communication overhead of the SL server across
multiple parallel shards. Building upon SSFL, BSFL replaces the centralized
server with a blockchain-based architecture that employs a committee-driven
consensus mechanism to enhance fairness and security. BSFL incorporates an
evaluation mechanism to exclude poisoned or tampered model updates, thereby
mitigating data poisoning and model integrity attacks. Experimental evaluations
against baseline SL and SFL approaches show that SSFL improves performance and
scalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases
resilience to data poisoning attacks by 62.7% while maintaining superior
performance under normal operating conditions. To the best of our knowledge,
BSFL is the first blockchain-enabled framework to implement an end-to-end
decentralized SplitFed Learning system.

</details>


### [8] [LAPIS: A Performance Portable, High Productivity Compiler Framework](https://arxiv.org/abs/2509.25605)
*Brian Kelley,Sivasankaran Rajamanickam*

Main category: cs.DC

TL;DR: LAPIS是一个基于MLIR的编译器，解决了编程模型在可移植性、性能和生产力三个维度上的平衡问题，特别针对计算科学和机器学习融合场景，能够自动优化稀疏和密集线性代数内核，并集成PyTorch和Kokkos代码。


<details>
  <summary>Details</summary>
Motivation: 现有编程模型在可移植性、性能和生产力三个关键维度上存在不平衡：计算科学模型关注性能和可移植性，机器学习模型关注可移植性和生产力。当计算科学与机器学习用例融合时，不同框架需要手动集成，且现有框架缺乏对新架构的易扩展性。

Method: 基于MLIR开发LAPIS编译器，采用Kokkos生态系统的原则构建方言，能够自动降低计算科学和AI用例中的稀疏和密集线性代数内核，并促进PyTorch和Kokkos之间的代码集成。

Result: LAPIS能够在不同架构上实现可移植性，与默认MLIR实现相比表现出良好的内核性能，同时支持框架向新架构的扩展。

Conclusion: LAPIS成功解决了编程模型在可移植性、性能和生产力三个维度的平衡问题，为计算科学与机器学习的融合提供了有效的编译器解决方案，并支持框架的易扩展性。

Abstract: Portability, performance, and productivity are three critical dimensions for
evaluating a programming model or compiler infrastructure. Several modern
programming models for computational science focus on performance and
portability. On the other end, several machine learning focused programming
models focus on portability and productivity. A clear solution that is strong
in all three dimensions has yet to emerge. A second related problem arises when
use cases from computational science converge with machine learning. The
disparate popular frameworks of these fields require programmers to manually
integrate codes written in different frameworks. Finally, several programming
frameworks lack easy options for extensibility as any new computer architecture
change require complex changes to the programming models. We present LAPIS, an
MLIR-based compiler that addresses all three of these challenges. We
demonstrate that LAPIS can automatically lower sparse and dense linear algebra
kernels from computational science and artificial intelligence use cases. We
also show how LAPIS facilitates the integration of codes between PyTorch and
Kokkos. We compare kernel performance with the default MLIR implementations on
diverse architectures to demonstrate portability. By developing a dialect that
is built on the principles of the Kokkos ecosystem, LAPIS also allows
extensibility of the framework to new architectures.

</details>


### [9] [PAST: Pilot and Adaptive Orchestration for Timely and Resilient Service Delivery in Edge-Assisted UAV Networks under Spatio-Temporal Dynamics](https://arxiv.org/abs/2509.25700)
*Houyi Qi,Minghui Liwang,Liqun Fu,Sai Zou,Xinlei Yi,Wei Ni,Huaiyu Dai*

Main category: cs.DC

TL;DR: PAST框架通过整合PilotAO和AdaptAO机制，为无人机边缘网络提供稳定灵活的资源交易方案，在动态环境中实现高效资源利用和任务性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统现货交易存在协商延迟和能耗问题，传统期货交易难以适应动态不确定的无人机边缘环境，需要新的资源交易框架来平衡稳定性和灵活性。

Method: 提出PAST框架，包含两个互补机制：PilotAO（基于风险感知和超额预订的早期决策模块）和AdaptAO（基于无人机移动性、供需变化和协议性能动态更新协议和超额预订率的智能适应模块）。

Result: 在真实数据集上的实验表明，PAST在决策开销、任务完成延迟、资源利用率和社会福利方面持续优于基准方法。

Conclusion: PAST通过预测性规划和实时调整相结合，为提升低空任务性能提供了稳健且自适应的实践参考。

Abstract: Incentive-driven resource trading is essential for UAV applications with
intensive, time-sensitive computing demands. Traditional spot trading suffers
from negotiation delays and high energy costs, while conventional futures
trading struggles to adapt to the dynamic, uncertain UAV-edge environment. To
address these challenges, we propose PAST (pilot-and-adaptive stable trading),
a novel framework for edge-assisted UAV networks with spatio-temporal dynamism.
PAST integrates two complementary mechanisms: PilotAO (pilot trading agreements
with overbooking), a risk-aware, overbooking-enabled early-stage
decision-making module that establishes long-term, mutually beneficial
agreements and boosts resource utilization; and AdaptAO (adaptive trading
agreements with overbooking rate update), an intelligent adaptation module that
dynamically updates agreements and overbooking rates based on UAV mobility,
supply-demand variations, and agreement performance. Together, these mechanisms
enable both stability and flexibility, guaranteeing individual rationality,
strong stability, competitive equilibrium, and weak Pareto optimality.
Extensive experiments on real-world datasets show that PAST consistently
outperforms benchmark methods in decision-making overhead, task completion
latency, resource utilization, and social welfare. By combining predictive
planning with real-time adjustments, PAST offers a valuable reference on robust
and adaptive practice for improving low-altitude mission performance.

</details>


### [10] [Accelerating LLM Inference with Precomputed Query Storage](https://arxiv.org/abs/2509.25919)
*Jay H. Park,Youngju Cho,Choungsol Lee,Moonwook Oh,Euiseong Seo*

Main category: cs.DC

TL;DR: StorInfer是一个存储辅助的LLM推理系统，通过预计算和存储可预测的查询-响应对来降低延迟，在语义匹配时绕过GPU推理直接返回存储响应。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理在资源受限环境（如设备端或边缘部署）中的高延迟问题，通过存储预计算结果来加速响应时间。

Method: 使用LLM驱动的生成器自适应生成多样化和去重的查询，采用自适应查询掩码防止相似查询再生，以及自适应采样促进语义多样性。查询-响应对通过向量数据库索引以实现快速相似性检索。

Result: 生成了15万个独特的预计算对（占用830MB存储空间），实现了高达17.3%的延迟降低，且响应质量无损失。

Conclusion: 存储辅助推理在可预测查询分布场景中具有实用性和可扩展性，展示了利用存储作为高效低延迟LLM部署主要推动力的有前景方向。

Abstract: Large language model (LLM) inference often suffers from high latency,
particularly in resource-constrained environments such as on-device or edge
deployments. To address this challenge, we present StorInfer, a novel
storage-assisted LLM inference system that accelerates response time by
precomputing and storing predictable query-response pairs offline. When a user
query semantically matches a precomputed query, StorInfer bypasses expensive
GPU inference and instantly returns the stored response, significantly reducing
latency and compute costs. To maximize coverage and effectiveness, StorInfer
employs an LLM-driven generator that adaptively produces diverse and
deduplicated queries based on a given knowledge base. This is achieved via two
techniques: adaptive query masking, which prevents regeneration of similar
queries, and adaptive sampling, which dynamically tunes generation parameters
to promote semantic diversity. The resulting query-response pairs are embedded
and indexed using a disk-backed vector database to enable fast,
similarity-based retrieval at runtime. Using this approach, we generated 150K
unique precomputed pairs (taking up to 830 MB of storage space), achieving up
to 17.3% latency reduction with no loss in response quality. Our evaluation
across multiple QA datasets demonstrates the practicality and scalability of
storage-assisted inference, especially in scenarios with predictable query
distributions. StorInfer highlights a promising direction in leveraging storage
as a primary enabler for efficient, low-latency LLM deployment.

</details>


### [11] [Enabling Time-Aware Priority Traffic Management over Distributed FPGA Nodes](https://arxiv.org/abs/2509.26043)
*Alberto Scionti,Paolo Savio,Francesco Lubrano,Federico Stirano,Antonino Nespola,Olivier Terzo,Corrado De Sio,Luca Sterpone*

Main category: cs.DC

TL;DR: 该论文扩展了开源NIC实现Corundum的功能，通过硬件实现时间感知的流量管理来控制不同流量类别的带宽分配。


<details>
  <summary>Details</summary>
Motivation: 现代网络接口卡(NICs)和FPGA已从简单设备发展为复杂的异构系统，能够卸载主机CPU的复杂任务。作者旨在利用现代FPGA的高性能网络接口能力，实现智能NIC的时间感知流量管理功能。

Method: 通过扩展Corundum开源NIC实现，在硬件中启用时间感知流量管理，并在AXI总线上暴露专用控制寄存器，使NIC驱动程序能够轻松配置不同优先级队列的传输带宽。每个控制寄存器关联特定传输队列，设置队列在传输窗口中获得输出端口访问权限的时间比例。

Result: 实验评估表明，该方法能够正确管理为不同传输流预留的带宽。

Conclusion: 该研究成功实现了基于硬件的带宽控制机制，通过时间感知流量管理有效管理不同流量类别的传输带宽。

Abstract: Network Interface Cards (NICs) greatly evolved from simple basic devices
moving traffic in and out of the network to complex heterogeneous systems
offloading host CPUs from performing complex tasks on in-transit packets. These
latter comprise different types of devices, ranging from NICs accelerating
fixed specific functions (e.g., on-the-fly data compression/decompression,
checksum computation, data encryption, etc.) to complex Systems-on-Chip (SoC)
equipped with both general purpose processors and specialized engines
(Smart-NICs). Similarly, Field Programmable Gate Arrays (FPGAs) moved from pure
reprogrammable devices to modern heterogeneous systems comprising
general-purpose processors, real-time cores and even AI-oriented engines.
Furthermore, the availability of high-speed network interfaces (e.g., SFPs)
makes modern FPGAs a good choice for implementing Smart-NICs. In this work, we
extended the functionalities offered by an open-source NIC implementation
(Corundum) by enabling time-aware traffic management in hardware, and using
this feature to control the bandwidth associated with different traffic
classes. By exposing dedicated control registers on the AXI bus, the driver of
the NIC can easily configure the transmission bandwidth of different
prioritized queues. Basically, each control register is associated with a
specific transmission queue (Corundum can expose up to thousands of
transmission and receiving queues), and sets up the fraction of time in a
transmission window which the queue is supposed to get access the output port
and transmit the packets. Queues are then prioritized and associated to
different traffic classes through the Linux QDISC mechanism. Experimental
evaluation demonstrates that the approach allows to properly manage the
bandwidth reserved to the different transmission flows.

</details>


### [12] [Efficient Distributed Training via Dual Batch Sizes and Cyclic Progressive Learning](https://arxiv.org/abs/2509.26092)
*Kuan-Wei Lu,Ding-Yong Hong,Pangfeng Liu,Jan-Jan Wu*

Main category: cs.DC

TL;DR: 提出了一种结合双批次大小学习和循环渐进学习的混合分布式训练方法，在保持训练效率的同时提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习中，使用大批次训练虽然能加速训练过程，但会导致模型泛化能力下降和准确率降低。需要一种方法既能利用硬件资源最大化训练效率，又能保持模型性能。

Method: 基于参数服务器框架的双批次大小学习方案，同时使用最大硬件支持的大批次和增强泛化的小批次；结合循环渐进学习方案，在训练过程中从低到高逐步调整图像分辨率以减少时间开销。

Result: 在CIFAR-100上，相比传统方法准确率提升3.3%，训练时间减少10.6%；在ImageNet上准确率提升0.1%，训练时间减少35.7%。

Conclusion: 该混合方法有效解决了大批次训练导致的泛化问题，在提升模型准确率的同时显著减少了训练时间，为分布式机器学习提供了有效的解决方案。

Abstract: Distributed machine learning is critical for training deep learning models on
large datasets and with numerous parameters. Current research primarily focuses
on leveraging additional hardware resources and powerful computing units to
accelerate the training process. As a result, larger batch sizes are often
employed to speed up training. However, training with large batch sizes can
lead to lower accuracy due to poor generalization. To address this issue, we
propose the dual batch size learning scheme, a distributed training method
built on the parameter server framework. This approach maximizes training
efficiency by utilizing the largest batch size that the hardware can support
while incorporating a smaller batch size to enhance model generalization. By
using two different batch sizes simultaneously, this method reduces testing
loss and enhances generalization, with minimal extra training time.
Additionally, to mitigate the time overhead caused by dual batch size learning,
we propose the cyclic progressive learning scheme. This technique gradually
adjusts image resolution from low to high during training, significantly
boosting training speed. By combining cyclic progressive learning with dual
batch size learning, our hybrid approach improves both model generalization and
training efficiency. Experimental results using ResNet-18 show that, compared
to conventional training methods, our method can improve accuracy by 3.3% while
reducing training time by 10.6% on CIFAR-100, and improve accuracy by 0.1%
while reducing training time by 35.7% on ImageNet.

</details>


### [13] [AGOCS -- Accurate Google Cloud Simulator Framework](https://arxiv.org/abs/2509.26120)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: AGOCS是一个基于真实Google集群工作负载轨迹的高保真云工作负载模拟器，可在桌面机器上用于日常研究。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够精确模拟真实云工作负载的桌面级模拟器，为云研究提供便利工具。

Method: 基于12.5K节点Google集群一个月的工作负载轨迹，使用Scala语言实现，注重并行执行和易扩展设计。

Result: 实现了能够揭示作业、任务和节点精确参数的模拟框架，并提供实际资源使用统计。

Conclusion: AGOCS提供了一个开源的高保真云工作负载模拟解决方案，为云研究社区贡献了有价值的工具。

Abstract: This paper presents the Accurate Google Cloud Simulator (AGOCS) - a novel
high-fidelity Cloud workload simulator based on parsing real workload traces,
which can be conveniently used on a desktop machine for day-to-day research.
Our simulation is based on real-world workload traces from a Google Cluster
with 12.5K nodes, over a period of a calendar month. The framework is able to
reveal very precise and detailed parameters of the executed jobs, tasks and
nodes as well as to provide actual resource usage statistics. The system has
been implemented in Scala language with focus on parallel execution and an
easy-to-extend design concept. The paper presents the detailed structural
framework for AGOCS and discusses our main design decisions, whilst also
suggesting alternative and possibly performance enhancing future approaches.
The framework is available via the Open Source GitHub repository.

</details>


### [14] [Parallax: Efficient LLM Inference Service over Decentralized Environment](https://arxiv.org/abs/2509.26182)
*Chris Tong,Youhe Jiang,Gufeng Chen,Tianyi Zhao,Sibian Lu,Wenjie Qu,Eric Yang,Lynn Ai,Binhang Yuan*

Main category: cs.DC

TL;DR: Parallax是一个去中心化LLM服务系统，通过两阶段调度器将异构GPU池转化为高效推理平台，解决了去中心化环境中GPU异构性、网络带宽限制和动态可用性的调度挑战。


<details>
  <summary>Details</summary>
Motivation: 中心化LLM推理服务依赖专用GPU集群和高带宽互联，成本高昂。利用去中心化GPU池是吸引人的替代方案，但GPU异构性、网络带宽限制和动态可用性使得高效调度成为核心挑战。

Method: 采用两阶段调度器：1)模型分配阶段，在内存和链路带宽约束下将每个副本的层分配到不同GPU上，联合优化延迟和吞吐量；2)请求时GPU流水线选择阶段，将不同副本的层拼接成端到端执行链，平衡负载并适应当前条件。

Result: 在真实志愿者节点上部署开源LLM进行评估，相比去中心化基线方法，Parallax持续降低延迟并提高吞吐量。

Conclusion: 原则性调度可以使志愿者计算成为LLM推理的实用且经济可行的基础架构。

Abstract: Deploying a large language model (LLM) inference service remains costly
because centralized serving depends on specialized GPU clusters and
high-bandwidth interconnects in datacenters. An appealing alternative is to
leverage collaborative decentralized GPU pools. However, heterogeneity in GPU
and limited interconnected network bandwidth, along with potentially dynamic
availability, make efficient scheduling the central challenge in this scenario.
In this paper, we present Parallax, a decentralized LLM serving system that
turns a pool of heterogeneous GPUs into an efficient inference platform via a
two-phase scheduler. Parallax decomposes planning into (i) model allocation,
which places layers of each replica across diverse GPUs to jointly optimize
latency and throughput under memory and link-bandwidth constraints, and (ii)
request-time GPU pipeline selection, which stitches layers from different
replicas into end-to-end execution chains that balance load and adapt to
current conditions. We implement Parallax and evaluate it on open-source LLMs
deployed over real volunteer nodes. Parallax consistently reduces latency and
increases throughput relative to decentralized baselines, demonstrating that
principled scheduling can make volunteer compute a practical, affordable
substrate for LLM inference.
  Github Repo at: https://github.com/GradientHQ/parallax.

</details>


### [15] [I Like To Move It - Computation Instead of Data in the Brain](https://arxiv.org/abs/2509.26193)
*Fabian Czappa,Marvin Kaster,Felix Wolf*

Main category: cs.DC

TL;DR: 提出了一种新算法，通过移动计算而非数据来显著减少脑模拟中的通信开销，将连接性更新时间缩短6倍，尖峰交换时间缩短两个数量级。


<details>
  <summary>Details</summary>
Motivation: 脑模拟面临巨大计算挑战，包括约10^11个神经元和10^14个突触的连接组，其中结构可塑性对记忆形成和学习至关重要，但突触更新和尖峰交换的通信开销限制了可扩展性。

Method: 使用Barnes-Hut近似算法将计算复杂度从O(n^2)降低到O(n log n)，并开发新算法通过移动计算而非数据来减少通信开销。

Result: 新算法将连接性更新时间缩短6倍，尖峰交换时间缩短超过两个数量级。

Conclusion: 该算法通过优化通信策略显著提升了脑模拟的可扩展性和效率。

Abstract: The detailed functioning of the human brain is still poorly understood. Brain
simulations are a well-established way to complement experimental research, but
must contend with the computational demands of the approximately $10^{11}$
neurons and the $10^{14}$ synapses connecting them, the network of the latter
referred to as the connectome. Studies suggest that changes in the connectome
(i.e., the formation and deletion of synapses, also known as structural
plasticity) are essential for critical tasks such as memory formation and
learning. The connectivity update can be efficiently computed using a
Barnes-Hut-inspired approximation that lowers the computational complexity from
$O(n^2)$ to $O(n log n)$, where n is the number of neurons. However, updating
synapses, which relies heavily on RMA, and the spike exchange between neurons,
which requires all-to-all communication at every time step, still hinder
scalability. We present a new algorithm that significantly reduces the
communication overhead by moving computation instead of data. This shrinks the
time it takes to update connectivity by a factor of six and the time it takes
to exchange spikes by more than two orders of magnitude.

</details>


### [16] [Efficient Construction of Large Search Spaces for Auto-Tuning](https://arxiv.org/abs/2509.26253)
*Floris-Jan Willemsen,Rob V. van Nieuwpoort,Ben van Werkhoven*

Main category: cs.DC

TL;DR: 将基于约束的自动调优搜索空间构建重新表述为约束满足问题(CSP)，通过优化CSP求解器显著提升构建效率


<details>
  <summary>Details</summary>
Motivation: 自动调优中搜索空间构建可能耗时数分钟到数天，现有技术缺乏理论基础且仅适用于特定子集

Method: 开发运行时解析器将用户约束转换为求解器优化表达式，优化CSP求解器以利用自动调优约束的常见结构

Result: 优化求解器相比暴力枚举提升4个数量级，相比未优化CSP求解器提升3个数量级，相比基于链树的领先框架提升1-2个数量级

Conclusion: 消除了自动调优的关键可扩展性障碍，为探索之前无法达到的问题规模提供了即插即用解决方案

Abstract: Automatic performance tuning, or auto-tuning, accelerates high-performance
codes by exploring vast spaces of code variants. However, due to the large
number of possible combinations and complex constraints, constructing these
search spaces can be a major bottleneck. Real-world applications have been
encountered where the search space construction takes minutes to hours or even
days. Current state-of-the-art techniques for search space construction, such
as chain-of-trees, lack a formal foundation and only perform adequately on a
specific subset of search spaces.
  We show that search space construction for constraint-based auto-tuning can
be reformulated as a Constraint Satisfaction Problem (CSP). Building on this
insight with a CSP solver, we develop a runtime parser that translates
user-defined constraint functions into solver-optimal expressions, optimize the
solver to exploit common structures in auto-tuning constraints, and integrate
these and other advances in open-source tools. These contributions
substantially improve performance and accessibility while preserving
flexibility.
  We evaluate our approach using a diverse set of benchmarks, demonstrating
that our optimized solver reduces construction time by four orders of magnitude
versus brute-force enumeration, three orders of magnitude versus an unoptimized
CSP solver, and one to two orders of magnitude versus leading auto-tuning
frameworks built on chain-of-trees. We thus eliminate a critical scalability
barrier for auto-tuning and provide a drop-in solution that enables the
exploration of previously unattainable problem scales in auto-tuning and
related domains.

</details>


### [17] [CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching of Fault Propagations](https://arxiv.org/abs/2509.26529)
*Shangshu Qian,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: CSnake是一个故障注入框架，通过因果拼接技术来暴露分布式系统中的自维持级联故障。它使用故障因果分析(FCA)来识别故障传播链，并通过三阶段测试预算分配协议和本地兼容性检查来高效发现复杂的故障传播条件。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测技术难以在部署前暴露自维持级联故障，因为这些故障通常需要特定条件的复杂组合才能触发。级联故障涉及一系列故障传播，每个传播由不同条件激活，这使得检测变得困难。

Method: CSnake采用因果拼接技术，将不同测试中的多个单故障注入因果连接来模拟复杂故障传播链。使用故障因果分析(FCA)比较故障注入运行和配置文件运行的执行轨迹，识别额外触发的故障。采用三阶段测试预算分配协议优先处理具有独特和多样化因果后果的故障，并进行本地兼容性检查确保路径约束兼容。

Result: CSnake在五个系统中检测到15个导致自维持级联故障的bug，其中5个已被确认，2个已修复。

Conclusion: CSnake框架通过因果拼接和故障因果分析，有效暴露了分布式系统中的自维持级联故障，证明了其在检测复杂故障传播方面的有效性。

Abstract: Recent studies have revealed that self-sustaining cascading failures in
distributed systems frequently lead to widespread outages, which are
challenging to contain and recover from. Existing failure detection techniques
struggle to expose such failures prior to deployment, as they typically require
a complex combination of specific conditions to be triggered. This challenge
stems from the inherent nature of cascading failures, as they typically involve
a sequence of fault propagations, each activated by distinct conditions.
  This paper presents CSnake, a fault injection framework to expose
self-sustaining cascading failures in distributed systems. CSnake uses the
novel idea of causal stitching, which causally links multiple single-fault
injections in different tests to simulate complex fault propagation chains. To
identify these chains, CSnake designs a counterfactual causality analysis of
fault propagations - fault causality analysis (FCA): FCA compares the execution
trace of a fault injection run with its corresponding profile run (i.e., same
test w/o the injection) and identifies any additional faults triggered, which
are considered to have a causal relationship with the injected fault.
  To address the large search space of fault and workload combinations, CSnake
employs a three-phase allocation protocol of test budget that prioritizes
faults with unique and diverse causal consequences, increasing the likelihood
of uncovering conditional fault propagations. Furthermore, to avoid incorrectly
connecting fault propagations from workloads with incompatible conditions,
CSnake performs a local compatibility check that approximately checks the
compatibility of the path constraints associated with connected fault
propagations with low overhead.
  CSnake detected 15 bugs that cause self-sustaining cascading failures in five
systems, five of which have been confirmed with two fixed.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [WARP -- Web-Augmented Real-time Program Repairer: A Real-Time Compilation Error Resolution using LLMs and Web-Augmented Synthesis](https://arxiv.org/abs/2509.25192)
*Anderson de Lima Luiz*

Main category: cs.SE

TL;DR: WARP是一个利用大型语言模型和动态网络增强合成的系统，用于实时解决编译错误，通过监控开发者终端、智能检测错误，并结合精调代码LLM与网络资源来提供修复方案。


<details>
  <summary>Details</summary>
Motivation: 编译错误显著影响软件开发效率，现有方法如传统IDE快速修复和纯LLM方法在修复率和语义正确性方面存在不足。

Method: WARP系统主动监控开发者终端，智能检测编译错误，将精调的代码LLM理解与从开发者论坛和官方文档等最新网络资源检索的相关解决方案、解释和代码片段协同结合。

Result: 在CGP基准测试（包含C/C++、Python和Go错误）上，WARP实现了72.5%的正确编译修复率，在语义正确性方面优于基线LLM方法和传统IDE快速修复。

Conclusion: WARP通过结合LLM和动态网络增强合成，有效解决了编译错误修复问题，主要技术挑战在于从噪声网络数据中实现高精度合成。

Abstract: Compilation errors represent a significant bottleneck in software development
productivity. This paper introduces WARP (Web-Augmented Real-time Program
Repairer), a novel system that leverages Large Language Models (LLMs) and
dynamic web-augmented synthesis for real-time resolution of these errors. WARP
actively monitors developer terminals, intelligently detects compilation
errors, and synergistically combines the understanding of a fine-tuned Code-LLM
with relevant solutions, explanations, and code snippets retrieved from
up-to-date web sources like developer forums and official documentation.
Experimental results on our curated benchmark, CGP (featuring C/C++, Python,
and Go errors), demonstrate WARP achieves a superior fix rate (72.5 % Compiles
correctly) and higher semantic correctness compared to baseline LLM-only
approaches and traditional IDE quick-fixes. Key technical challenges in
achieving high-accuracy synthesis from noisy web data.

</details>


### [19] [Devstral: Fine-tuning Language Models for Coding Agent Applications](https://arxiv.org/abs/2509.25193)
*Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexander H. Liu,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Andy Ehrenberg,Andy Lo,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Clément Denoix,Corentin Barreau,Darius Dabert Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Graham Neubig,Guillaume Lample,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Malo Delignon,JeanHadrien Chabran,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Khyathi Raghavi Chandu,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patrick von Platen,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Roman Soletskyi,Romain Sauvestre,Sagar Vaze,Sanchit Gandhi,Sandeep Subramanian,Shashwat Dalal,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibaut Lavril,Thibault Schueller,Thomas Foubert,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xingyao Wang,Xuanyu Zhang,Yihan Wan,Yunhao Tang*

Main category: cs.SE

TL;DR: Devstral-Small是一个轻量级开源代码代理模型，在100B参数以下的模型中性能最佳，具有24B参数，速度快且易于部署。


<details>
  <summary>Details</summary>
Motivation: 开发一个轻量级但性能优异的代码代理模型，使其在保持小规模的同时仍能与更大模型竞争。

Method: 通过专门设计和开发针对代理软件开发的模型架构和专业化方法。

Result: Devstral-Small在24B参数规模下实现了与比其大一个数量级的模型相竞争的性能。

Conclusion: Devstral-Small证明了轻量级模型在代码代理任务中可以达到与大规模模型相媲美的性能，同时具有更好的部署效率。

Abstract: We introduce Devstral-Small, a lightweight open source model for code agents
with the best performance among models below 100B size. In this technical
report, we give an overview of how we design and develop a model and craft
specializations in agentic software development. The resulting model,
Devstral-Small is a small 24B model, fast and easy to serve. Despite its size,
Devstral-Small still attains competitive performance compared to models more
than an order of magnitude larger.

</details>


### [20] [Automated Code Development for PDE Solvers Using Large Language Models](https://arxiv.org/abs/2509.25194)
*Haoyang Wu,Xinxin Zhang,Lailai Zhu*

Main category: cs.SE

TL;DR: LLM-PDEveloper是一个零样本多智能体LLM框架，专门为偏微分方程库的二次开发者自动化代码开发，将数学和算法描述直接转换为源代码。


<details>
  <summary>Details</summary>
Motivation: 利用基础模型（特别是大语言模型）的跨领域知识、文本处理和推理能力来开发偏微分方程数值库的软件，现有研究主要自动化终端用户的情况设置和执行，但缺乏针对二次开发者的自动化代码开发工具。

Method: 采用零样本多智能体LLM框架，通过端到端的数学到代码方法，将数学和算法描述直接转换为源代码，生成新的求解器/模块并适配现有模块，实现自增强的代码库扩展管道。

Result: 在三个任务上进行了演示：1）为新PDE构建求解器，2）为给定PDE实现新边界条件，3）修改现有求解器以包含附加项，取得了中等成功率。分析了LLM导致的语法错误并提出了有效修复方法。

Conclusion: LLM-PDEveloper展示了利用LLM自动化PDE库代码开发的可行性，识别了某些语义错误的机制，为未来研究提供了指导。

Abstract: Foundation models -- large language models (LLMs) in particular -- have
become ubiquitous, shaping daily life and driving breakthroughs across science,
engineering, and technology. Harnessing their broad cross-domain knowledge,
text-processing, and reasoning abilities for software development, e.g.,
numerical libraries for solving partial differential equations (PDEs), is
therefore attracting growing interest. Yet existing studies mainly automate
case setup and execution for end users. We introduce LLM-PDEveloper, a
zero-shot, multi-agent LLM framework that automates code development for PDE
libraries, specifically targeting secondary developers. By translating
mathematical and algorithmic descriptions directly into source code,
LLM-PDEveloper generates new solvers/modules and adapts existing ones. This
end-to-end math-to-code approach enables a self-augmenting pipeline that
continuously expands the codebase of a library, extends its capacities, and
broadens its scope. We demonstrate LLM-PDEveloper on three tasks: 1) build a
solver for a new PDE, 2) implement new BCs for a given PDE, and 3) modify an
existing solver to incorporate additional terms, achieving moderate success
rates. Failures due to syntactic errors made by LLMs are analyzed and we
propose effective fixes. We also identify the mechanisms underlying certain
semantic errors, guiding future research.

</details>


### [21] [Understanding Practitioners Perspectives on Monitoring Machine Learning Systems](https://arxiv.org/abs/2509.25195)
*Hira Naveed,John Grundy,Chetan Arora,Hourieh Khalajzadeh,Omar Haggag*

Main category: cs.SE

TL;DR: 该论文通过调查91位ML从业者，探讨了机器学习系统监控的策略、挑战和改进机会，发现从业者面临模型性能下降、延迟超时和安全违规等运行时问题，期望未来监控工具能提供自动监控生成、性能公平性监控支持以及问题解决建议。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习系统的非确定性特性，在生产环境中可能导致不可预见和危险的结果。为了及时检测不良行为并防止组织遭受财务和声誉损失，监控这些系统至关重要。

Method: 通过对91位ML从业者进行全球调查，收集关于当前ML系统监控实践的多样化见解，进行定性和定量分析。

Result: 研究发现从业者经常面临模型性能下降、延迟超时和安全违规等运行时问题；大多数偏好自动监控以提高效率，但许多人仍依赖手动方法；监控工具的初始设置和配置复杂，监控增加了额外工作负担并导致警报疲劳。

Conclusion: 从业者期望的改进包括：自动生成和部署监控器、改进性能和公平性监控支持、提供解决运行时问题的建议，这些见解为未来开发更符合从业者需求的ML监控工具提供了宝贵指导。

Abstract: Given the inherent non-deterministic nature of machine learning (ML) systems,
their behavior in production environments can lead to unforeseen and
potentially dangerous outcomes. For a timely detection of unwanted behavior and
to prevent organizations from financial and reputational damage, monitoring
these systems is essential. This paper explores the strategies, challenges, and
improvement opportunities for monitoring ML systems from the practitioners
perspective. We conducted a global survey of 91 ML practitioners to collect
diverse insights into current monitoring practices for ML systems. We aim to
complement existing research through our qualitative and quantitative analyses,
focusing on prevalent runtime issues, industrial monitoring and mitigation
practices, key challenges, and desired enhancements in future monitoring tools.
Our findings reveal that practitioners frequently struggle with runtime issues
related to declining model performance, exceeding latency, and security
violations. While most prefer automated monitoring for its increased
efficiency, many still rely on manual approaches due to the complexity or lack
of appropriate automation solutions. Practitioners report that the initial
setup and configuration of monitoring tools is often complicated and
challenging, particularly when integrating with ML systems and setting alert
thresholds. Moreover, practitioners find that monitoring adds extra workload,
strains resources, and causes alert fatigue. The desired improvements from the
practitioners perspective are: automated generation and deployment of monitors,
improved support for performance and fairness monitoring, and recommendations
for resolving runtime issues. These insights offer valuable guidance for the
future development of ML monitoring tools that are better aligned with
practitioners needs.

</details>


### [22] [APRIL: API Synthesis with Automatic Prompt Optimization and Reinforcement Learning](https://arxiv.org/abs/2509.25196)
*Hua Zhong,Shan Jiang,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: APRIL结合了基于LLM的合成、自动提示优化(APO)和基于可验证奖励的强化学习(RLVR)，显著提升了大型库中API合成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统组件合成方法搜索空间大且依赖手工规范，而LLM生成代码存在幻觉和上下文信息不足的问题，需要更可靠的API合成方法。

Method: 使用APO迭代优化提示，同时通过RLVR对策略进行微调以实现功能正确性，构建高效的合成流程。

Result: 在81个真实世界科学Python库API上评估，相比使用专家提示的未微调LLM，APRIL取得了显著改进。

Conclusion: APO和RLVR的集成为大型库中的组件式API合成提供了稳健、可扩展的路径。

Abstract: APIs are central to modern software development, yet composing new APIs from
large libraries is difficult due to the exponential search space; traditional
component-based synthesis relies on costly exploration and hand-crafted
specifications. While large language models (LLMs) can generate implementations
from natural language, hallucinations and limited access to up-to-date
contextual information often yield incorrect code. In this paper, we present
APRIL, an approach that combines LLM-based synthesis with Automatic Prompt
Optimization (APO) and Reinforcement Learning from Verifiable Rewards (RLVR):
APO iteratively refines prompts for a frozen model, while RLVR fine-tunes the
policy toward functional correctness, producing an efficient synthesis
pipeline. Evaluated on 81 real-world APIs from widely used scientific Python
libraries and benchmarked against instruction-tuned but unfine-tuned LLMs
guided by expert prompts, APRIL achieves substantial improvements. These
results indicate that integrating APO and RLVR provides a robust, scalable path
for component-based API synthesis in large libraries.

</details>


### [23] [Towards Repository-Level Program Verification with Large Language Models](https://arxiv.org/abs/2509.25197)
*Si Cheng Zhong,Xujie Si*

Main category: cs.SE

TL;DR: 提出了RVBench（首个仓库级验证基准）和RagVerus框架，通过检索增强生成和上下文感知提示来解决跨模块依赖和全局上下文问题，显著提高了多模块仓库的证明合成效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法主要关注孤立的函数级验证任务，忽视了跨模块依赖和全局上下文等关键挑战，难以扩展到真实世界项目的自动化形式验证。

Method: 引入RagVerus框架，结合检索增强生成和上下文感知提示技术，自动化多模块仓库的证明合成。

Result: 在受限模型推理预算下，RagVerus将现有基准的证明通过率提高了三倍，并在更具挑战性的RVBench基准上实现了27%的相对改进。

Conclusion: RagVerus提供了一个可扩展且样本高效的验证解决方案，能够有效处理仓库级别的形式验证挑战。

Abstract: Recent advancements in large language models (LLMs) suggest great promises in
code and proof generations. However, scaling automated formal verification to
real-world projects requires resolving cross-module dependencies and global
contexts, which are crucial challenges overlooked by existing LLM-based methods
with a special focus on targeting isolated, function-level verification tasks.
To systematically explore and address the significant challenges of verifying
entire software repositories, we introduce RVBench, the first verification
benchmark explicitly designed for repository-level evaluation, constructed from
four diverse and complex open-source Verus projects.
  We further introduce RagVerus, an extensible framework that synergizes
retrieval-augmented generation with context-aware prompting to automate proof
synthesis for multi-module repositories. RagVerus triples proof pass rates on
existing benchmarks under constrained model inference budgets, and achieves a
27% relative improvement on the more challenging RVBench benchmark,
demonstrating a scalable and sample-efficient verification solution.

</details>


### [24] [CircInspect: Integrating Visual Circuit Analysis, Abstraction, and Real-Time Development in Quantum Debugging](https://arxiv.org/abs/2509.25199)
*Mushahid Khan,Prashant J. Nair,Olivia Di Matteo*

Main category: cs.SE

TL;DR: CircInspect是一个用于调试Python和PennyLane量子程序的交互式工具，通过断点和实时开发功能帮助用户分析量子电路组件、监控输出、可视化结构变化。


<details>
  <summary>Details</summary>
Motivation: 量子软件开发面临独特挑战，包括量子计算的概率性、特殊算法原语和硬件噪声，传统软件工程工具不足以应对这些问题。

Method: 开发了CircInspect工具，利用断点和实时软件开发功能，支持分析孤立量子电路组件、监控程序输出、可视化结构变化和信息抽象。

Result: CircInspect为量子程序调试提供了专门工具，能够增强用户对量子程序的理解和调试效率。

Conclusion: CircInspect填补了量子软件开发调试工具的空白，通过交互式功能有效解决了量子程序调试的复杂性问题。

Abstract: Software bugs typically result from errors in specifications or code
translation. While classical software engineering has evolved with various
tools and methodologies to tackle such bugs, the emergence of quantum computing
presents unique challenges. Quantum software development introduces
complexities due to the probabilistic nature of quantum computing, distinct
algorithmic primitives, and potential hardware noise. In this paper, we
introduce CircInspect, an interactive tool tailored for debugging quantum
programs in Python and PennyLane. By leveraging breakpoints and real-time
software development features, \toolname~empowers users to analyze isolated
quantum circuit components, monitor program output, visualize structural
changes, and abstract information to enhance comprehension.

</details>


### [25] [Generating High-Quality Datasets for Code Editing via Open-Source Language Models](https://arxiv.org/abs/2509.25203)
*Zekai Zhang,Mingwei Liu,Zhenxi Chen,Linxi Liang,Yuxuan Chen,Guangsheng Ou,Yanlin Wang,Dan Li,Xin Peng,Zibin Zheng*

Main category: cs.SE

TL;DR: 提出了CanItEdit开源管道，利用多个LLM合成真实的代码编辑三元组，构建了OCEDataFT数据集，通过微调显著提升了代码编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提交的数据集通常存在噪声大、多样性不足、无法反映真实世界编辑指令风格的问题，需要构建更高质量的代码编辑数据集。

Method: 开发CanItEdit开源管道，利用多个LLM合成代码编辑三元组，生成简洁和详细两种指令，并通过差异和主题过滤保证数据质量和多样性，构建OCEDataFT数据集。

Result: 在三个先进基础模型上微调OCEDataFT数据集后，在CanItEdit基准测试中pass@1相对提升4.50%到20.79%，性能接近闭源系统，与GPT-4的差距缩小到仅3.54%。

Conclusion: 该方法无需依赖专有资源或人工标注，就能显著提升代码编辑模型的性能，缩小与顶级闭源系统的差距。

Abstract: Code editing plays a vital role in software engineering, requiring developers
to adjust existing code according to natural language instructions while
keeping functionality intact and avoiding unnecessary modifications. However,
commit-based datasets commonly used for this task are often noisy, lack
diversity, and fail to reflect the style of real-world edit instructions. To
address this, we introduce CanItEdit, an open-source pipeline that leverages
multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces
both concise "lazy" instructions and more detailed "descriptive" ones, and
applies filtering based on diffs and topics to guarantee data quality and
variety. Using this process, we construct OCEDataFT, a curated dataset of 20K
samples. Fine-tuning three advanced base models on OCEDataFT leads to
significant performance boosts on the CanItEdit benchmark, with relative pass@1
improvements ranging from 4.50% to 20.79%. Notably, the resulting models
achieve performance close to closed-source systems, narrowing the gap to GPT-4
to just 3.54%, without relying on proprietary resources or manual annotation.

</details>


### [26] [A Benchmark for Localizing Code and Non-Code Issues in Software Projects](https://arxiv.org/abs/2509.25242)
*Zejun Zhang,Jian Wang,Qingyun Yang,Yifan Pan,Yi Tang,Yi Li,Zhenchang Xing,Tian Zhang,Xuandong Li,Guoan Zhang*

Main category: cs.SE

TL;DR: 提出了MULocBench数据集，包含1,100个GitHub Python项目问题，用于评估问题定位方法，发现现有方法性能不足（Acc@5、F1低于40%）


<details>
  <summary>Details</summary>
Motivation: 现有问题定位基准（如SWE-Bench和LocBench）主要关注pull-request问题和代码位置，忽略了其他证据和非代码文件，存在局限性

Method: 构建包含46个流行GitHub Python项目的1,100个问题的综合数据集，评估最先进的定位方法和五种基于LLM的提示策略

Result: 当前技术存在显著局限性：即使在文件级别，性能指标（Acc@5、F1）仍低于40%，难以泛化到现实多面问题解决

Conclusion: MULocBench为问题解决的项目定位研究提供了更现实的测试平台，公开数据集以促进未来研究

Abstract: Accurate project localization (e.g., files and functions) for issue
resolution is a critical first step in software maintenance. However, existing
benchmarks for issue localization, such as SWE-Bench and LocBench, are limited.
They focus predominantly on pull-request issues and code locations, ignoring
other evidence and non-code files such as commits, comments, configurations,
and documentation. To address this gap, we introduce MULocBench, a
comprehensive dataset of 1,100 issues from 46 popular GitHub Python projects.
Comparing with existing benchmarks, MULocBench offers greater diversity in
issue types, root causes, location scopes, and file types, providing a more
realistic testbed for evaluation. Using this benchmark, we assess the
performance of state-of-the-art localization methods and five LLM-based
prompting strategies. Our results reveal significant limitations in current
techniques: even at the file level, performance metrics (Acc@5, F1) remain
below 40%. This underscores the challenge of generalizing to realistic,
multi-faceted issue resolution. To enable future research on project
localization for issue resolution, we publicly release MULocBench at
https://huggingface.co/datasets/somethingone/MULocBench.

</details>


### [27] [Reinforcement Learning-Guided Chain-of-Draft for Token-Efficient Code Generation](https://arxiv.org/abs/2509.25243)
*Xunzhu Tang,Iyiola Emmanuel Olatunji,Tiezhu Sun,Jacques Klein,Tegawende F. Bissyande*

Main category: cs.SE

TL;DR: MultiCoD是一个基于强化学习的框架，通过从Chain-of-Draft生成的多个候选方案中选择最优解，提高代码生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成中表现出表面流畅性，但在需要正确性和语义对齐的结构化推理任务中表现不佳。现有的CoT提示方法冗长低效，CoD提示虽然更简洁，但LLMs的随机性导致解决方案质量不一，难以选择最优解。

Method: 使用策略引导提示鼓励多样化的推理风格，将解决方案选择建模为上下文多臂老虎机问题。通过奖励函数优化可解释特征，包括代码复杂度、推理结构和策略元数据，平衡正确性、效率和清晰度。

Result: 在MBPP、BigCodeBench、SWE-bench Verified和Defects4J等基准测试中，MultiCoD优于或与标准提示、CoT和CoD基线相当，同时通过多候选设计将用户账单减少50%以上，提高了LLM响应质量。

Conclusion: MultiCoD通过强化学习框架有效解决了代码生成中的推理质量选择问题，实现了更可持续和可扩展的实际部署。

Abstract: LLMs demonstrate surface-level fluency in code generation but struggle with
structured reasoning tasks requiring correctness and semantic alignment. While
Chain-of-Thought (CoT) prompting enhances reasoning through intermediate steps,
it suffers from verbosity and inefficiency. Chain-of-Draft (CoD) prompting
offers more concise reasoning, but the stochastic nature of LLMs produces
varying solution quality, making optimal selection challenging. We propose
\multicod, a reinforcement learning framework that learns to select the most
promising candidate from CoD-generated solutions. Our approach uses
strategy-guided prompting to encourage diverse reasoning styles and models
solution selection as a contextual bandit problem. The framework optimizes
interpretable features including code complexity, reasoning structure, and
strategic metadata through a reward function balancing correctness, efficiency,
and clarity. Experiments on MBPP, BigCodeBench, SWE-bench Verified, and
Defects4J show \multicod~outperforms and in some cases, on par with standard
prompting, CoT, and CoD baselines while achieving cost and token efficiency
from the user's perspective through a multi-candidate design that charges only
for the selected output, reducing user billing by over 50\% and improving LLM
response quality, making \multicod~more sustainable and scalable for real-world
deployment. Our code is available: https://anonymous.4open.science/r/MultiCoD.

</details>


### [28] [Protocode: Prototype-Driven Interpretability for Code Generation in LLMs](https://arxiv.org/abs/2509.25247)
*Krishna Vamshi Bodla,Haizhao Yang*

Main category: cs.SE

TL;DR: 本文研究如何自动采样上下文学习(ICL)演示来提升大语言模型在代码生成任务中的性能和可解释性。通过AST分析识别代码中受演示影响最大的区域，实验表明高质量的ICL演示能改善模型性能，而低质量演示则会损害性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码生成中的广泛应用，虽然提高了开发效率，但也带来了次优解决方案和不安全代码的风险。需要提升模型性能和生成代码的可解释性。

Method: 使用AST分析MBPP测试集输出，识别代码中受ICL演示影响最大的区域。研究自动采样ICL演示的方法，比较不同质量演示对模型性能的影响。

Result: 高质量ICL演示不仅使输出更易解释，还在pass@10指标上带来正向性能提升。相反，低质量ICL演示会负面影响模型性能，甚至低于基础模型。

Conclusion: 高效的ICL采样策略对模型性能至关重要，合适的演示选择能显著影响模型在特定任务上的表现。

Abstract: Since the introduction of Large Language Models (LLMs), they have been widely
adopted for various tasks such as text summarization, question answering,
speech-to-text translation, and more. In recent times, the use of LLMs for code
generation has gained significant attention, with tools such as Cursor and
Windsurf demonstrating the ability to analyze massive code repositories and
recommend relevant changes. Big tech companies have also acknowledged the
growing reliance on LLMs for code generation within their codebases. Although
these advances significantly improve developer productivity, increasing
reliance on automated code generation can proportionally increase the risk of
suboptimal solutions and insecure code. Our work focuses on automatically
sampling In-Context Learning (ICL) demonstrations which can improve model
performance and enhance the interpretability of the generated code. Using
AST-based analysis on outputs from the MBPP test set, we identify regions of
code most influenced by the chosen demonstrations. In our experiments, we show
that high-quality ICL demonstrations not only make outputs easier to interpret
but also yield a positive performance improvement on the pass@10 metric.
Conversely, poorly chosen ICL demonstrations affected the LLM performance on
the pass@10 metric negatively compared to the base model. Overall, our approach
highlights the importance of efficient sampling strategies for ICL, which can
affect the performance of the model on any given task.

</details>


### [29] [BuildBench: Benchmarking LLM Agents on Compiling Real-World Open-Source Software](https://arxiv.org/abs/2509.25248)
*Zehua Zhang,Ati Priya Bajaj,Divij Handa,Siyu Liu,Arvind S Raj,Hongkai Chen,Hulin Wang,Yibo Liu,Zion Leonahenahe Basque,Souradip Nath,Vishal Juneja,Nikhil Chapre,Yan Shoshitaishvili,Adam Doupé,Chitta Baral,Ruoyu Wang*

Main category: cs.SE

TL;DR: 提出了一个更挑战性的基准BUILD-BENCH，用于评估LLM代理在开源软件编译任务中的表现，并开发了OSS-BUILD-AGENT系统在该基准上取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的开源软件编译方法依赖手动制定的规则和工作流，无法适应需要定制配置或环境设置的开源项目。最近使用大语言模型的方法只在高质量开源软件子集上进行评估，低估了实际编译挑战。

Method: 提出了BUILD-BENCH基准，包含质量、规模和特征更多样化的开源软件。开发了OSS-BUILD-AGENT系统，具有增强的构建指令检索模块，能够适应异构开源软件特征。

Result: OSS-BUILD-AGENT在BUILD-BENCH基准上取得了最先进的性能表现。

Conclusion: BUILD-BENCH基准能够真实反映代理处理复杂软件工程任务的能力，该基准将推动软件开发和软件安全领域的创新。

Abstract: Automatically compiling open-source software (OSS) projects is a vital,
labor-intensive, and complex task, which makes it a good challenge for LLM
Agents. Existing methods rely on manually curated rules and workflows, which
cannot adapt to OSS that requires customized configuration or environment
setup. Recent attempts using Large Language Models (LLMs) used selective
evaluation on a subset of highly rated OSS, a practice that underestimates the
realistic challenges of OSS compilation. In practice, compilation instructions
are often absent, dependencies are undocumented, and successful builds may even
require patching source files or modifying build scripts. We propose a more
challenging and realistic benchmark, BUILD-BENCH, comprising OSS that are more
diverse in quality, scale, and characteristics. Furthermore, we propose a
strong baseline LLM-based agent, OSS-BUILD-AGENT, an effective system with
enhanced build instruction retrieval module that achieves state-of-the-art
performance on BUILD-BENCH and is adaptable to heterogeneous OSS
characteristics. We also provide detailed analysis regarding different
compilation method design choices and their influence to the whole task,
offering insights to guide future advances. We believe performance on
BUILD-BENCH can faithfully reflect an agent's ability to tackle compilation as
a complex software engineering tasks, and, as such, our benchmark will spur
innovation with a significant impact on downstream applications in the fields
of software development and software security.

</details>


### [30] [RANGER -- Repository-Level Agent for Graph-Enhanced Retrieval](https://arxiv.org/abs/2509.25257)
*Pratik Shah,Rajat Ghosh,Aryan Singhal,Debojyoti Dutta*

Main category: cs.SE

TL;DR: RANGER是一个仓库级代码检索代理，能够处理代码实体查询和自然语言查询，通过构建知识图谱和双阶段检索管道在多个ASE任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的代码检索系统主要关注代码实体查询，而缺乏对自然语言查询的有效处理。需要一种能够同时处理两种查询类型的通用代码检索系统。

Method: 首先构建包含层次结构和跨文件依赖关系的知识图谱，节点包含文本描述和嵌入。然后使用双阶段检索管道：代码实体查询通过Cypher查询处理，自然语言查询通过MCTS引导的图探索处理。

Result: 在CodeSearchNet和RepoQA上优于使用Qwen3-8B等强模型嵌入的基线方法；在RepoBench上实现更好的跨文件依赖检索；在CrossCodeEval上与BM25结合获得最高的代码补全精确匹配率。

Conclusion: RANGER通过知识图谱和双阶段检索策略有效解决了代码实体和自然语言两种查询类型，在多个ASE任务中表现出色，填补了现有工作的空白。

Abstract: General-purpose automated software engineering (ASE) includes tasks such as
code completion, retrieval, repair, QA, and summarization. These tasks require
a code retrieval system that can handle specific queries about code entities,
or code entity queries (for example, locating a specific class or retrieving
the dependencies of a function), as well as general queries without explicit
code entities, or natural language queries (for example, describing a task and
retrieving the corresponding code). We present RANGER, a repository-level code
retrieval agent designed to address both query types, filling a gap in recent
works that have focused primarily on code-entity queries. We first present a
tool that constructs a comprehensive knowledge graph of the entire repository,
capturing hierarchical and cross-file dependencies down to the variable level,
and augments graph nodes with textual descriptions and embeddings to bridge the
gap between code and natural language. RANGER then operates on this graph
through a dual-stage retrieval pipeline. Entity-based queries are answered
through fast Cypher lookups, while natural language queries are handled by
MCTS-guided graph exploration. We evaluate RANGER across four diverse
benchmarks that represent core ASE tasks including code search, question
answering, cross-file dependency retrieval, and repository-level code
completion. On CodeSearchNet and RepoQA it outperforms retrieval baselines that
use embeddings from strong models such as Qwen3-8B. On RepoBench, it achieves
superior cross-file dependency retrieval over baselines, and on CrossCodeEval,
pairing RANGER with BM25 delivers the highest exact match rate in code
completion compared to other RAG methods.

</details>


### [31] [Automatically Generating Web Applications from Requirements Via Multi-Agent Test-Driven Development](https://arxiv.org/abs/2509.25297)
*Yuxuan Wan,Tingshuo Liang,Jiakai Xu,Jingyu Xiao,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: TDDev是一个基于测试驱动开发的LLM代理框架，能够从自然语言描述或设计图像自动生成全栈Web应用，包括测试用例、前后端代码，并通过迭代优化确保功能正确性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型只能生成前端网页，无法创建功能完整的全栈应用。开发全栈Web应用复杂耗时，需要掌握多种技术栈。

Method: 采用测试驱动开发方法，自动生成可执行测试用例，生成前后端代码，模拟用户交互，并通过迭代优化直到满足所有需求。

Result: 在多种应用场景实验中，TDDev相比现有最佳方法整体准确率提升14.4%，能够无需人工干预生成可靠、高质量的Web应用。

Conclusion: TDDev框架有效解决了全栈自动化中的关键挑战，包括需求不明确、多文件复杂依赖关系，以及功能和视觉需求的双重满足。

Abstract: Developing full-stack web applications is complex and time-intensive,
demanding proficiency across diverse technologies and frameworks. Although
recent advances in multimodal large language models (MLLMs) enable automated
webpage generation from visual inputs, current solutions remain limited to
front-end tasks and fail to deliver fully functional applications. In this
work, we introduce TDDev, the first test-driven development (TDD)-enabled
LLM-agent framework for end-to-end full-stack web application generation. Given
a natural language description or design image, TDDev automatically derives
executable test cases, generates front-end and back-end code, simulates user
interactions, and iteratively refines the implementation until all requirements
are satisfied. Our framework addresses key challenges in full-stack automation,
including underspecified user requirements, complex interdependencies among
multiple files, and the need for both functional correctness and visual
fidelity. Through extensive experiments on diverse application scenarios, TDDev
achieves a 14.4% improvement on overall accuracy compared to state-of-the-art
baselines, demonstrating its effectiveness in producing reliable, high-quality
web applications without requiring manual intervention.

</details>


### [32] [Detecting and Fixing API Misuses of Data Science Libraries Using Large Language Models](https://arxiv.org/abs/2509.25378)
*Akalanka Galappaththi,Francisco Ribeiro,Sarah Nadi*

Main category: cs.SE

TL;DR: DSCHECKER是一个基于LLM的方法，用于检测和修复数据科学库中的API误用，通过整合API指令和数据信息来提高性能。


<details>
  <summary>Details</summary>
Motivation: 数据科学库（如scikit-learn和pandas）的数据中心特性使得API误用检测更具挑战性，需要专门的方法来解决这个问题。

Method: 使用三个LLM模型和来自五个数据科学库的误用案例进行实验，结合API指令和数据特定细节，并实现具有自适应函数调用机制的DSCHECKER代理。

Result: 最佳模型检测F1分数达到61.18%，修复了51.28%的误用；DSCHECKER代理在真实场景下达到48.65%检测F1分数，修复了39.47%的误用。

Conclusion: 基于LLM的API误用检测和修复在真实场景中具有应用前景，DSCHECKER方法有效提升了数据科学库的API使用质量。

Abstract: Data science libraries, such as scikit-learn and pandas, specialize in
processing and manipulating data. The data-centric nature of these libraries
makes the detection of API misuse in them more challenging. This paper
introduces DSCHECKER, an LLM-based approach designed for detecting and fixing
API misuses of data science libraries. We identify two key pieces of
information, API directives and data information, that may be beneficial for
API misuse detection and fixing. Using three LLMs and misuses from five data
science libraries, we experiment with various prompts. We find that
incorporating API directives and data-specific details enhances Dschecker's
ability to detect and fix API misuses, with the best-performing model achieving
a detection F1-score of 61.18 percent and fixing 51.28 percent of the misuses.
Building on these results, we implement Dschecker agent which includes an
adaptive function calling mechanism to access information on demand, simulating
a real-world setting where information about the misuse is unknown in advance.
We find that Dschecker agent achieves 48.65 percent detection F1-score and
fixes 39.47 percent of the misuses, demonstrating the promise of LLM-based API
misuse detection and fixing in real-world scenarios.

</details>


### [33] [A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects](https://arxiv.org/abs/2509.25397)
*Johan Linåker,Cailean Osborne,Jennifer Ding,Ben Burtenshaw*

Main category: cs.SE

TL;DR: 对14个开源大语言模型项目的开发者进行访谈研究，分析开源LLM项目的协作模式、动机和组织结构，揭示了五种不同的组织模型和多样化的协作生态系统。


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型正在促进AI研究和创新生态系统，但对其开发过程中的协作方式缺乏全面研究，限制了我们对开源LLM项目如何启动、组织和治理的理解。

Method: 采用探索性分析方法，通过对来自草根项目、研究机构、初创公司和大型科技公司的14个开源LLM开发者进行半结构化访谈，涵盖北美、欧洲、非洲和亚洲的项目。

Result: 研究发现：1）开源LLM项目的协作远超模型本身，包括数据集、基准测试、开源框架、排行榜等；2）开发者具有多样化的社会、经济和技术动机；3）识别出五种不同的组织模型，在控制集中度和社区参与策略上各不相同。

Conclusion: 为寻求支持全球社区构建更开放AI未来的利益相关者提供实用建议。

Abstract: The proliferation of open large language models (LLMs) is fostering a vibrant
ecosystem of research and innovation in artificial intelligence (AI). However,
the methods of collaboration used to develop open LLMs both before and after
their public release have not yet been comprehensively studied, limiting our
understanding of how open LLM projects are initiated, organized, and governed
as well as what opportunities there are to foster this ecosystem even further.
We address this gap through an exploratory analysis of open collaboration
throughout the development and reuse lifecycle of open LLMs, drawing on
semi-structured interviews with the developers of 14 open LLMs from grassroots
projects, research institutes, startups, and Big Tech companies in North
America, Europe, Africa, and Asia. We make three key contributions to research
and practice. First, collaboration in open LLM projects extends far beyond the
LLMs themselves, encompassing datasets, benchmarks, open source frameworks,
leaderboards, knowledge sharing and discussion forums, and compute
partnerships, among others. Second, open LLM developers have a variety of
social, economic, and technological motivations, from democratizing AI access
and promoting open science to building regional ecosystems and expanding
language representation. Third, the sampled open LLM projects exhibit five
distinct organizational models, ranging from single company projects to
non-profit-sponsored grassroots projects, which vary in their centralization of
control and community engagement strategies used throughout the open LLM
lifecycle. We conclude with practical recommendations for stakeholders seeking
to support the global community building a more open future for AI.

</details>


### [34] [PIPer: On-Device Environment Setup via Online Reinforcement Learning](https://arxiv.org/abs/2509.25455)
*Alexander Kovrigin,Aleksandra Eliseeva,Konstantin Grotov,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: 本文提出了一种专门用于自动化环境设置的方法，通过结合监督微调和强化学习来训练模型，使较小模型在环境设置任务上达到与大型模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 环境设置是软件工程中的持续挑战，现有大型语言模型在此任务上表现有限，需要专门的方法来提升自动化环境设置的性能。

Method: 结合监督微调生成正确的Bash脚本，并使用带可验证奖励的强化学习来适应环境设置任务。

Result: 在EnvBench-Python基准测试中，该方法使Qwen3-8B模型（可在消费级硬件上运行）的性能与Qwen3-32B和GPT-4o等大型模型相当。

Conclusion: 通过专门的方法训练，较小模型也能在环境设置任务上达到与大型模型相当的性能，为软件工程研究和实践提供了有效解决方案。

Abstract: Environment setup-the process of configuring the system to work with a
specific software project-represents a persistent challenge in Software
Engineering (SE). Automated environment setup methods could assist developers
by providing fully configured environments for arbitrary repositories without
manual effort. This also helps SE researchers to scale execution-based
benchmarks. However, recent studies reveal that even state-of-the-art Large
Language Models (LLMs) achieve limited success in automating this task. To
address this limitation, we tune a specialized model for environment setup. We
combine supervised fine-tuning for generating correct Bash scripts and
Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task
of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model
runnable on consumer hardware) to perform on par with larger models-Qwen3-32B
and GPT-4o. The training code and model checkpoints are available online:
https://github.com/JetBrains-Research/PIPer.

</details>


### [35] [BloomAPR: A Bloom's Taxonomy-based Framework for Assessing the Capabilities of LLM-Powered APR Solutions](https://arxiv.org/abs/2509.25465)
*Yinghang Ma,Jiho Shin,Leuson Da Silva,Zhen Ming,Jiang,Song Wang,Foutse Khomh,Shin Hwei Tan*

Main category: cs.SE

TL;DR: 提出了BloomAPR框架，基于布鲁姆分类学动态评估LLM驱动的自动程序修复能力，发现现有方法在记忆层面表现良好但高级推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在数据污染风险和动态多样性评估能力有限的问题，需要更可靠的评估框架来评估LLM驱动的程序修复解决方案。

Method: 基于布鲁姆分类学构建动态评估框架，使用Defects4J作为案例研究，评估了ChatRepair和CigaR在GPT-3.5-Turbo、Llama-3.1和StarCoder-2三种LLM下的表现。

Result: LLM驱动的修复方案在记忆层面表现最佳（修复81.57%的bug），在理解层面合成bug上性能提升60.66%，但在应用层面语法变化时性能下降（修复43.32%），在分析层面真实项目中仅修复13.46%-41.34%的bug。

Conclusion: 现有LLM驱动的程序修复方案在高级推理能力上存在明显不足，迫切需要发展更先进的评估基准来更可靠地评估软件工程解决方案。

Abstract: Recent advances in large language models (LLMs) have accelerated the
development of AI-driven automated program repair (APR) solutions. However,
these solutions are typically evaluated using static benchmarks such as
Defects4J and SWE-bench, which suffer from two key limitations: (1) the risk of
data contamination, potentially inflating evaluation results due to overlap
with LLM training data, and (2) limited ability to assess the APR capabilities
in dynamic and diverse contexts. In this paper, we introduced BloomAPR, a novel
dynamic evaluation framework grounded in Bloom's Taxonomy. Our framework offers
a structured approach to assess the cognitive capabilities of LLM-powered APR
solutions across progressively complex reasoning levels. Using Defects4J as a
case study, we evaluated two state-of-the-art LLM-powered APR solutions,
ChatRepair and CigaR, under three different LLMs: GPT-3.5-Turbo, Llama-3.1, and
StarCoder-2. Our findings show that while these solutions exhibit basic
reasoning skills and effectively memorize bug-fixing patterns (fixing up to
81.57% of bugs at the Remember layer), their performance increases with
synthetically generated bugs (up to 60.66% increase at the Understand layer).
However, they perform worse on minor syntactic changes (fixing up to 43.32% at
the Apply layer), and they struggle to repair similar bugs when injected into
real-world projects (solving only 13.46% to 41.34% bugs at the Analyze layer).
These results underscore the urgent need for evolving benchmarks and provide a
foundation for more trustworthy evaluation of LLM-powered software engineering
solutions.

</details>


### [36] [AGNOMIN - Architecture Agnostic Multi-Label Function Name Prediction](https://arxiv.org/abs/2509.25514)
*Yonatan Gizachew Achamyeleh,Tongtao Zhang,Joshua Hyunki Kim,Gabriel Garcia,Shih-Yuan Yu,Anton Kocheturov,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: AGNOMIN是一种架构无关的多标签函数名预测方法，通过构建特征增强层次图和使用分层图神经网络，在剥离二进制文件中实现跨架构的函数名预测，显著提升精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在软件逆向工程中面临架构特定限制、数据稀缺和命名约定多样化的问题，影响后续漏洞分析和补丁工作的效率。

Method: 构建特征增强层次图（FEHGs），结合控制流图、函数调用图和动态学习的PCode特征，使用分层图神经网络生成跨架构一致的函数表示，并采用Renée启发的解码器进行函数名预测。

Result: 在9,000个ELF可执行二进制文件上的评估显示，AGNOMIN在测试数据集上精度提升达27.17%，召回率提升达55.86%，对未见架构的泛化能力比最接近的基线高5.89%召回率。

Conclusion: AGNOMIN在安全黑客松中成功帮助逆向工程师分析和修补跨架构的易受攻击二进制文件，证明了其实际效用和可扩展的安全评估能力。

Abstract: Function name prediction is crucial for understanding stripped binaries in
software reverse engineering, a key step for \textbf{enabling subsequent
vulnerability analysis and patching}. However, existing approaches often
struggle with architecture-specific limitations, data scarcity, and diverse
naming conventions. We present AGNOMIN, a novel architecture-agnostic approach
for multi-label function name prediction in stripped binaries. AGNOMIN builds
Feature-Enriched Hierarchical Graphs (FEHGs), combining Control Flow Graphs,
Function Call Graphs, and dynamically learned \texttt{PCode} features. A
hierarchical graph neural network processes this enriched structure to generate
consistent function representations across architectures, vital for
\textbf{scalable security assessments}. For function name prediction, AGNOMIN
employs a Ren\'ee-inspired decoder, enhanced with an attention-based head layer
and algorithmic improvements.
  We evaluate AGNOMIN on a comprehensive dataset of 9,000 ELF executable
binaries across three architectures, demonstrating its superior performance
compared to state-of-the-art approaches, with improvements of up to 27.17\% in
precision and 55.86\% in recall across the testing dataset. Moreover, AGNOMIN
generalizes well to unseen architectures, achieving 5.89\% higher recall than
the closest baseline. AGNOMIN's practical utility has been validated through
security hackathons, where it successfully aided reverse engineers in analyzing
and patching vulnerable binaries across different architectures.

</details>


### [37] [M&SCheck: Towards a Checklist to Support Software Engineering Newcomers to the Modeling and Simulation Area](https://arxiv.org/abs/2509.25625)
*Luiza Martins de Freitas Cintra,Philipp Zech,Mohamad Kassab,Eliomar Araújo Lima,Sofia Larissa da Costa Paiva,Valdemar Vicente Graciano Neto*

Main category: cs.SE

TL;DR: 本文提出了一个初步检查清单，帮助初学者在建模与仿真中选择合适的范式（DEVS、系统动力学、基于代理的仿真），以应对数字孪生、智慧城市等复杂生态系统开发中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生、智慧城市、工业4.0/5.0等复杂动态生态系统的出现，软件开发周期中需要集成建模与仿真。但软件工程师特别是初学者在选择合适的形式化方法时面临困难。

Method: 基于DEVS、系统动力学和基于代理的仿真三种主要形式化方法，建立了一个初步检查清单，并通过试点研究和专家咨询进行验证。

Result: 初步结果显示：(i) 检查清单的建议与原始研究中选用的形式化方法一致；(ii) 专家反馈积极。

Conclusion: 该检查清单能够有效帮助初学者在建模与仿真项目中选择最合适的形式化方法。

Abstract: The advent of increasingly complex and dynamic ecosystems, such as digital
twins (DT), smart cities and Industry 4.0 and 5.0, has made evident the need to
include modeling and simulation (M&S) in the software development life cycle.
Such disruptive systems include simulation models in their own architecture
(such as DT) or require the use of simulation models to represent the high
degree of movement and the multiplicity of interactions that occur between the
involved systems. However, when software engineers (particularly the newcomers)
need to use M&S in their projects, they often pose themselves an important
question: which formalism should I use? In this direction, the main
contribution of this paper is the establishment of a preliminary checklist with
questions to assist beginners in M&S in choosing the most appropriate paradigm
to solve their problems. The checklist is based on three main formalisms: DEVS,
System Dynamics and Agent-Based Simulation. A pilot study was carried out and
an expert was consulted. The preliminary results show (i) conformance between
the suggestion given by the checklist and the formalism selected in the
original studies used as input for evaluating the checklist, and (ii) a
positive feedback from the expert.

</details>


### [38] [Explainable Fault Localization for Programming Assignments via LLM-Guided Annotation](https://arxiv.org/abs/2509.25676)
*Fang Liu,Tianze Wang,Li Zhang,Zheyu Yang,Jing Jiang,Zian Sun*

Main category: cs.SE

TL;DR: FLAME是一种针对编程作业的细粒度、可解释的故障定位方法，通过LLM引导的注释和模型集成来提高定位准确性和教育价值。


<details>
  <summary>Details</summary>
Motivation: 现有故障定位技术在教育环境中面临挑战：方法级定位粒度太粗，缺乏解释性反馈；行级定位方法直接预测行号不适合LLM，无法提供可操作的错误修复指导。

Method: FLAME利用编程作业的丰富上下文信息引导LLM识别错误代码行，通过注释而非直接预测行号的方式，结合加权多模型投票策略聚合多个LLM的结果来确定每行代码的可疑度。

Result: 实验结果表明FLAME在编程作业上优于最先进的故障定位基线方法，在top-1位置成功定位了比最佳基线多207个错误，在Defects4J基准测试中也优于所有基线。

Conclusion: FLAME不仅适用于教育环境，还能有效泛化到通用软件代码库，提供准确且具有教育价值的故障定位解决方案。

Abstract: Providing timely and personalized guidance for students' programming
assignments, offers significant practical value for helping students complete
assignments and enhance their learning. In recent years, various automated
Fault Localization (FL) techniques have demonstrated promising results in
identifying errors in programs. However, existing FL techniques face challenges
when applied to educational contexts. Most approaches operate at the method
level without explanatory feedback, resulting in granularity too coarse for
students who need actionable insights to identify and fix their errors. While
some approaches attempt line-level fault localization, they often depend on
predicting line numbers directly in numerical form, which is ill-suited to
LLMs. To address these challenges, we propose FLAME, a fine-grained,
explainable Fault Localization method tailored for programming assignments via
LLM-guided Annotation and Model Ensemble. FLAME leverages rich contextual
information specific to programming assignments to guide LLMs in identifying
faulty code lines. Instead of directly predicting line numbers, we prompt the
LLM to annotate faulty code lines with detailed explanations, enhancing both
localization accuracy and educational value. To further improve reliability, we
introduce a weighted multi-model voting strategy that aggregates results from
multiple LLMs to determine the suspiciousness of each code line. Extensive
experimental results demonstrate that FLAME outperforms state-of-the-art fault
localization baselines on programming assignments, successfully localizing 207
more faults at top-1 over the best-performing baseline. Beyond educational
contexts, FLAME also generalizes effectively to general-purpose software
codebases, outperforming all baselines on the Defects4J benchmark.

</details>


### [39] [DeepCodeSeek: Real-Time API Retrieval for Context-Aware Code Generation](https://arxiv.org/abs/2509.25716)
*Esakkivel Esakkiraja,Denis Akhiyarov,Aditya Shanmugham,Chitra Ganapathy*

Main category: cs.SE

TL;DR: 提出了一种新的代码生成技术，通过扩展代码索引来预测所需API，解决了现有RAG查询文档应用的局限性，实现了高质量的端到端代码生成。


<details>
  <summary>Details</summary>
Motivation: 当前搜索技术仅限于标准RAG查询文档应用，存在API泄露问题，且无法处理企业特定代码中API使用意图不明确的情况。

Method: 构建了基于真实ServiceNow脚本的新数据集，开发了包含合成数据集生成、监督微调和强化学习的后训练流程，优化了0.6B重排序器。

Result: 实现了87.86%的top-40检索准确率，紧凑的0.6B重排序器性能优于更大的8B模型，同时延迟降低2.5倍。

Conclusion: 该方法成功解决了企业特定代码的细微差别，无需大型模型的计算开销，为自动补全和智能AI应用提供了高质量的代码生成能力。

Abstract: Current search techniques are limited to standard RAG query-document
applications. In this paper, we propose a novel technique to expand the code
and index for predicting the required APIs, directly enabling high-quality,
end-to-end code generation for auto-completion and agentic AI applications. We
address the problem of API leaks in current code-to-code benchmark datasets by
introducing a new dataset built from real-world ServiceNow Script Includes that
capture the challenge of unclear API usage intent in the code. Our evaluation
metrics show that this method achieves 87.86% top-40 retrieval accuracy,
allowing the critical context with APIs needed for successful downstream code
generation. To enable real-time predictions, we develop a comprehensive
post-training pipeline that optimizes a compact 0.6B reranker through synthetic
dataset generation, supervised fine-tuning, and reinforcement learning. This
approach enables our compact reranker to outperform a much larger 8B model
while maintaining 2.5x reduced latency, effectively addressing the nuances of
enterprise-specific code without the computational overhead of larger models.

</details>


### [40] [Are Classical Clone Detectors Good Enough For the AI Era?](https://arxiv.org/abs/2509.25754)
*Ajmain Inqiad Alam,Palash Roy,Farouq Al-omari,Chanchal Roy,Banani Roy,Kevin Schneider*

Main category: cs.SE

TL;DR: 评估9种经典代码克隆检测工具在AI生成代码上的有效性，发现经过有效规范化处理后的工具仍能保持相当检测能力，但与传统人工编写代码相比存在性能差异。


<details>
  <summary>Details</summary>
Motivation: AI生成代码的普及带来了语法和语义上的变化，这对传统基于人工编写代码优化的代码克隆检测工具提出了新挑战，需要评估这些工具在新范式下的有效性。

Method: 使用GPTCloneBench基准测试9种广泛使用的CCD工具，同时在BigCloneBench和SemanticCloneBench等传统基准上进行对比测试，分析规范化技术的作用并进行可扩展性和执行时间分析。

Result: 经典CCD工具特别是经过有效规范化技术增强的工具，在检测AI生成克隆时仍保持相当有效性，但与传统基准相比某些工具表现出显著性能差异。

Conclusion: 规范化技术在提高检测准确性方面发挥重要作用，研究为实际CCD工具选择提供了详细的可扩展性和执行时间分析支持。

Abstract: The increasing adoption of AI-generated code has reshaped modern software
development, introducing syntactic and semantic variations in cloned code.
Unlike traditional human-written clones, AI-generated clones exhibit systematic
syntactic patterns and semantic differences learned from large-scale training
data. This shift presents new challenges for classical code clone detection
(CCD) tools, which have historically been validated primarily on human-authored
codebases and optimized to detect syntactic (Type 1-3) and limited semantic
clones. Given that AI-generated code can produce both syntactic and complex
semantic clones, it is essential to evaluate the effectiveness of classical CCD
tools within this new paradigm. In this paper, we systematically evaluate nine
widely used CCD tools using GPTCloneBench, a benchmark containing
GPT-3-generated clones. To contextualize and validate our results, we further
test these detectors on established human-authored benchmarks, BigCloneBench
and SemanticCloneBench, to measure differences in performance between
traditional and AI-generated clones. Our analysis demonstrates that classical
CCD tools, particularly those enhanced by effective normalization techniques,
retain considerable effectiveness against AI-generated clones, while some
exhibit notable performance variation compared to traditional benchmarks. This
paper contributes by (1) evaluating classical CCD tools against AI-generated
clones, providing critical insights into their current strengths and
limitations; (2) highlighting the role of normalization techniques in improving
detection accuracy; and (3) delivering detailed scalability and execution-time
analyses to support practical CCD tool selection.

</details>


### [41] [LogPilot: Intent-aware and Scalable Alert Diagnosis for Large-scale Online Service Systems](https://arxiv.org/abs/2509.25874)
*Zhihan Jiang,Jinyang Liu,Yichen Li,Haiyu Huang,Xiao He,Tieying Zhang,Jianjun Chen,Yi Li,Rui Shi,Michael R. Lyu*

Main category: cs.SE

TL;DR: LogPilot是一个基于大语言模型的自动化日志告警诊断框架，通过意图感知和可扩展的方法，能够在一分钟内完成诊断，成本仅为每告警0.074美元，在真实场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化工具在告警诊断中存在两个主要问题：告警无关的日志范围界定不准确，以及无法有效组织复杂数据以支持推理。这导致值班工程师仍需手动检查大量日志来定位根因。

Method: LogPilot采用意图感知方法，通过解析告警定义（如PromQL）来精确识别因果相关的日志和请求。为实现可扩展性，它将每个请求的执行重构为时空日志链，聚类相似链以识别重复执行模式，并为LLM提供代表性样本进行诊断。

Result: 在火山引擎云的真实告警评估中，LogPilot相比最先进方法，根因摘要的有用性提高了50.34%，精确定位准确率提高了54.79%。诊断时间在一分钟内，每告警成本仅0.074美元，已成功部署到生产环境。

Conclusion: LogPilot提供了一个自动化且实用的服务告警诊断解决方案，通过意图感知的日志范围界定和基于聚类的可扩展方法，有效解决了现有工具的局限性。

Abstract: Effective alert diagnosis is essential for ensuring the reliability of
large-scale online service systems. However, on-call engineers are often
burdened with manually inspecting massive volumes of logs to identify root
causes. While various automated tools have been proposed, they struggle in
practice due to alert-agnostic log scoping and the inability to organize
complex data effectively for reasoning. To overcome these limitations, we
introduce LogPilot, an intent-aware and scalable framework powered by Large
Language Models (LLMs) for automated log-based alert diagnosis. LogPilot
introduces an intent-aware approach, interpreting the logic in alert
definitions (e.g., PromQL) to precisely identify causally related logs and
requests. To achieve scalability, it reconstructs each request's execution into
a spatiotemporal log chain, clusters similar chains to identify recurring
execution patterns, and provides representative samples to the LLMs for
diagnosis. This clustering-based approach ensures the input is both rich in
diagnostic detail and compact enough to fit within the LLM's context window.
Evaluated on real-world alerts from Volcano Engine Cloud, LogPilot improves the
usefulness of root cause summarization by 50.34% and exact localization
accuracy by 54.79% over state-of-the-art methods. With a diagnosis time under
one minute and a cost of only $0.074 per alert, LogPilot has been successfully
deployed in production, offering an automated and practical solution for
service alert diagnosis.

</details>


### [42] [Red Teaming Program Repair Agents: When Correct Patches can Hide Vulnerabilities](https://arxiv.org/abs/2509.25894)
*Simin Chen,Yixin He,Suman Jana,Baishakhi Ray*

Main category: cs.SE

TL;DR: SWExploit是一种针对LLM驱动的自动程序修复代理的攻击方法，通过生成对抗性GitHub问题描述，诱导代理生成功能正确但存在漏洞的补丁。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注APR生成补丁的功能正确性，而忽略了潜在的安全风险。由于GitHub平台的开放性，恶意用户可能提交误导性问题，诱导APR代理生成存在漏洞的补丁。

Method: SWExploit包含三个主要步骤：(1)程序分析识别漏洞注入点；(2)生成对抗性问题描述，提供误导性复现和错误信息；(3)基于APR代理输出迭代优化对抗性问题。

Result: 在三个代理流水线和五个后端LLM上的实证评估显示，SWExploit能生成功能正确但存在漏洞的补丁，攻击成功率可达0.91，而基线攻击成功率均低于0.20。

Conclusion: 该研究首次挑战了传统假设——通过所有测试的补丁必然可靠安全，揭示了当前APR代理评估范式的关键局限性。

Abstract: LLM-based agents are increasingly deployed for software maintenance tasks
such as automated program repair (APR). APR agents automatically fetch GitHub
issues and use backend LLMs to generate patches that fix the reported bugs.
However, existing work primarily focuses on the functional correctness of
APR-generated patches, whether they pass hidden or regression tests, while
largely ignoring potential security risks. Given the openness of platforms like
GitHub, where any user can raise issues and participate in discussions, an
important question arises: Can an adversarial user submit a valid issue on
GitHub that misleads an LLM-based agent into generating a functionally correct
but vulnerable patch? To answer this question, we propose SWExploit, which
generates adversarial issue statements designed to make APR agents produce
patches that are functionally correct yet vulnerable. SWExploit operates in
three main steps: (1) program analysis to identify potential injection points
for vulnerable payloads; (2) adversarial issue generation to provide misleading
reproduction and error information while preserving the original issue
semantics; and (3) iterative refinement of the adversarial issue statements
based on the outputs of the APR agents. Empirical evaluation on three agent
pipelines and five backend LLMs shows that SWExploit can produce patches that
are both functionally correct and vulnerable (the attack success rate on the
correct patch could reach 0.91, whereas the baseline ASRs are all below 0.20).
Based on our evaluation, we are the first to challenge the traditional
assumption that a patch passing all tests is inherently reliable and secure,
highlighting critical limitations in the current evaluation paradigm for APR
agents.

</details>


### [43] [R-Log: Incentivizing Log Analysis Capability in LLMs via Reasoning-based Reinforcement Learning](https://arxiv.org/abs/2509.25987)
*Yilun Liu,Ziang Chen,Song Xu,Minggui He,Shimin Tao,Weibin Meng,Yuming Xie,Tao Han,Chunguang Zhao,Jingzhou Du,Daimeng Wei,Shenglin Zhang,Yongqian Sun*

Main category: cs.SE

TL;DR: 提出R-Log方法，通过推理式学习解决LLM在日志分析中的领域差异和幻觉问题，在真实日志数据上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的方法在日志分析中存在领域差异导致过拟合，以及不平衡损失计算导致关键细节被淹没产生幻觉的问题。

Method: 提出推理式范式R-Log，模拟人类工程师结构化分析过程，使用强化学习在运维环境中优化模型，通过联合奖励函数减少幻觉。

Result: 在五个日志分析任务中优于现有方法，特别是在未见场景中提升228.05%，R-Log-fast版本实现5倍加速同时保持93%效能。

Conclusion: R-Log通过推理式学习和强化学习有效解决了日志分析中的领域差异和幻觉问题，展现了优越的性能和实用性。

Abstract: The growing complexity of log data in modern software systems has prompted
the use of Large Language Models (LLMs) for automated log analysis. Current
approaches typically rely on direct supervised fine-tuning (SFT) on log-label
pairs. However, this exacerbates the domain discrepancy between general-purpose
LLMs and specialized log data, causing overfitting. Furthermore, SFT's
imbalanced loss computation often allows lengthy contexts to overwhelm
critical, concise details in model answers, leading to hallucinations. To
address these limitations, we propose R-Log, a novel reasoning-based paradigm
that mirrors the structured, step-by-step analytical process of human
engineers. This approach enhances generalizability by learning the underlying
rules behind conclusions. We further employ Reinforcement Learning (RL) to
optimize the model within a simulated O&M environment, thereby reducing
hallucinations by directly rewarding correct outcomes. R-Log is first
cold-started on a curated dataset of 2k+ reasoning trajectories, guided by 13
strategies from manual O&M practices, to establish an initial reasoning
capability. This ability is then refined via RL using a joint reward function.
Empirical evaluations on real-world logs show that R-Log outperforms existing
methods across five log analysis tasks, particularly in unseen scenarios (by
228.05%). We also designed R-Log-fast with 5x speedup while keeping 93% of the
efficacy.

</details>


### [44] [Using GPT to build a Project Management assistant for Jira environments](https://arxiv.org/abs/2509.26014)
*Joel Garcia-Escribano,Arkaitz Carbajo,Mikel Egaña Aranguren,Unai Lopez-Novoa*

Main category: cs.SE

TL;DR: JiraGPT Next是一个基于GPT大语言模型的软件，作为Jira项目管理工具的插件，通过自然语言界面帮助项目经理处理大量数据，简化信息检索过程。


<details>
  <summary>Details</summary>
Motivation: 项目经理需要处理大量多样化数据，但现有工具学习曲线陡峭且需要复杂编程语言来检索数据，这给项目管理带来挑战。

Method: 开发JiraGPT Next作为Jira的插件，利用GPT大语言模型提供自然语言界面，让用户可以用自然语言查询和检索项目数据。

Result: 论文评估了GPT在此背景下的准确性，包括不同提示对完成特定任务效果的影响。

Conclusion: JiraGPT Next通过自然语言界面有效简化了项目经理处理大量数据的过程，提高了数据检索的便捷性和效率。

Abstract: In the domain of Project Management, the sheer volume of data is a challenge
that project managers continually have to deal with. Effectively steering
projects from inception to completion requires handling of diverse information
streams, including timelines, budgetary considerations, and task dependencies.
To navigate this data-driven landscape with precision and agility, project
managers must rely on efficient and sophisticated tools. These tools have
become essential, as they enable project managers to streamline communication,
optimize resource allocation, and make informed decisions in real-time.
However, many of these tools have steep learning curves and require using
complex programming languages to retrieve the exact data that project managers
need. In this work we present JiraGPT Next, a software that uses the GPT Large
Language Model to ease the process by which project managers deal with large
amounts of data. It is conceived as an add-on for Jira, one of the most popular
Project Management tools, and provides a natural language interface to retrieve
information. This work presents the design decisions behind JiraGPT Next and an
evaluation of the accuracy of GPT in this context, including the effects of
providing different prompts to complete a particular task.

</details>


### [45] [Evaluating the impact of code smell refactoring on the energy consumption of Android applications](https://arxiv.org/abs/2509.26031)
*Hina Anwar,Dietmar Pfahl,Satish N. Srirama*

Main category: cs.SE

TL;DR: 研究探索Android应用中常见代码重构对性能和能耗的影响，发现某些代码异味重构（如重复代码和类型检查）可降低能耗达10.8%，但能耗减少与执行时间变化无直接关联。


<details>
  <summary>Details</summary>
Motivation: 移动应用能耗问题备受关注，研究表明通过提高应用质量可改善移动设备能耗，频繁重构是实现这一目标的方法之一。

Method: 实验研究Android应用中几种常见代码重构的性能和能耗影响，分析不同代码异味重构组合的能耗变化。

Result: 重复代码和类型检查的代码异味重构可降低能耗达10.8%，但能耗减少与执行时间变化无直接关系，不同重构顺序组合对能耗影响各异。

Conclusion: 需要进一步研究软件应用大小、年龄、开发经验等因素与代码异味数量、类型以及重构后能耗性能影响的相关性。

Abstract: Energy consumption of mobile apps is a domain that is receiving a lot of
attention from researchers. Recent studies indicate that the energy consumption
of mobile devices could be improved by improving the quality of mobile apps.
Frequent refactoring is one way of achieving this goal. In this paper, we
explore the performance and energy impact of several common code refactorings
in Android apps. Experimental results indicate that some code smell
refactorings positively impact the energy consumption of Android apps.
Refactoring of the code smells "Duplicated code" and "Type checking" reduce
energy consumption by up to 10.8%. Significant reduction in energy consumption,
however, does not seem to be directly related to the increase or decrease of
execution time. In addition, the energy impact over permutations of code smell
refactorings in the selected Android apps was small. When analyzing the order
in which refactorings were made across code smell types, it turned out that
some permutations resulted in a reduction and some in an increase of energy
consumption for the analyzed apps. More research needs to be done to
investigate how factors like size and age of software apps, experience, and
number of contributors to app development correlate with (a) the number and
type of code smells found and (b) the impact of energy consumption and
performance after refactoring.

</details>


### [46] [Agent-based code generation for the Gammapy framework](https://arxiv.org/abs/2509.26110)
*Dmitriy Kostunin,Vladimir Sotnikov,Sergo Golovachev,Abhay Mehta,Tim Lukas Holch,Elisa Jones*

Main category: cs.SE

TL;DR: 开发了一个针对Gammapy科学库的代码生成代理，能够编写、执行和验证代码，以解决专业科学库因缺乏文档和API不稳定导致的LLM代码生成困难。


<details>
  <summary>Details</summary>
Motivation: 专业科学库（如Gammapy）缺乏文档、示例和社区支持，且API不稳定，使得基于有限或过时数据训练的LLM难以有效生成代码。

Method: 开发了一个代理系统，能够在受控环境中编写、执行和验证Gammapy库的代码，并提供了Web演示和基准测试套件。

Result: 成功构建了针对Gammapy的代码生成代理系统，包括Web演示界面和评估框架。

Conclusion: 该方法为解决专业科学库的代码生成问题提供了可行方案，并规划了后续改进步骤。

Abstract: Software code generation using Large Language Models (LLMs) is one of the
most successful applications of modern artificial intelligence. Foundational
models are very effective for popular frameworks that benefit from
documentation, examples, and strong community support. In contrast, specialized
scientific libraries often lack these resources and may expose unstable APIs
under active development, making it difficult for models trained on limited or
outdated data. We address these issues for the Gammapy library by developing an
agent capable of writing, executing, and validating code in a controlled
environment. We present a minimal web demo and an accompanying benchmarking
suite. This contribution summarizes the design, reports our current status, and
outlines next steps.

</details>


### [47] [A Multi-Language Object-Oriented Programming Benchmark for Large Language Models](https://arxiv.org/abs/2509.26111)
*Shuai Wang,Liang Ding,Li Shen,Yong Luo,Han Hu,Lefei Zhang,Fu Lin*

Main category: cs.SE

TL;DR: 提出了MultiOOP基准测试，解决现有代码生成基准测试的三个不平衡问题：单语言偏向、任务粒度单一和测试用例不足。该基准覆盖6种编程语言，包含267个面向对象编程任务，并引入pass@o指标和自动测试用例增强框架。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准测试存在三个主要不平衡：85.7%只关注单一编程语言，94.3%只针对函数级或语句级任务，超过80%平均包含少于10个测试用例。这些限制阻碍了对LLMs代码生成能力的全面评估。

Method: 1) 设计MultiOOP基准，覆盖Python、PHP、C++、C#、Java、JavaScript六种语言，每种语言267个面向对象编程任务；2) 开发翻译器将单语言OOP基准扩展到多语言环境；3) 提出pass@o指标评估OOP概念掌握程度；4) 构建自动测试用例增强框架确保评估可靠性。

Result: 评估14个主流LLMs发现：1) 性能显著下降：与函数级任务相比，pass@1分数下降高达65.6个百分点；2) 跨语言差异：GPT-4o mini在Python中pass@1为48.06%，但在其他语言中仅为0.12%-15.26%；3) 概念差距：pass@o分数比pass@k低1.1-19.2分，表明LLMs常生成可执行代码但未完全掌握核心OOP概念。

Conclusion: MultiOOP基准解决了现有代码生成评估的不平衡问题，揭示了LLMs在面向对象编程和多语言代码生成方面的局限性。该基准将促进对LLMs代码生成能力更平衡和全面的评估。

Abstract: Establishing fair and robust benchmarks is essential for evaluating
intelligent code generation by large language models (LLMs). Our survey of 35
existing benchmarks uncovers three major imbalances: 85.7% focus on a single
programming language; 94.3% target only function-level or statement-level
tasks; and over 80% include fewer than ten test cases on average. To address
these gaps, we propose MultiOOP, a multi-language object-oriented programming
benchmark covering six popular languages (Python, PHP, C++, C#, Java,
JavaScript) with 267 tasks per language. We design a translator that extends an
existing single-language OOP benchmark and the pass@o metric to a multilingual
setting. Moreover, we propose an automated framework for augmenting test cases
to ensure the reliability of the evaluation results. We evaluate 14 mainstream
LLMs under zero-shot prompting and report three key findings: 1) Substantial
performance degradation: pass@1 scores on MultiOOP drop by up to 65.6
percentage points compared to function-level tasks (e.g., HumanEval). 2)
Cross-language variability: GPT-4o mini achieves pass@1 of 48.06% in Python but
only 0.12%-15.26% in other languages, indicating limited multilingual
generalization. 3) Conceptual gaps: pass@o scores are consistently 1.1-19.2
points lower than pass@k, demonstrating that LLMs often generate executable
code without fully capturing core OOP concepts. Our benchmark, metric
extensions, and evaluation scripts will be publicly released to foster a more
balanced and comprehensive assessment of LLMs in object-oriented code
generation. Our code and data will be released at
https://github.com/alphadl/OOP-eval and
https://huggingface.co/datasets/codeai-dteam/MultiOOP respectively.

</details>


### [48] [Understanding Collective Social Behavior in OSS Communities: A Co-editing Network Analysis of Activity Cascades](https://arxiv.org/abs/2509.26173)
*Lisi Qarkaxhija,Maximilian Carparo,Stefan Menzel,Bernhard Sendhoff,Ingo Scholtes*

Main category: cs.SE

TL;DR: 该研究分析了开源软件社区中开发者的集体社会行为，发现提交活动具有突发性特征，并通过共同编辑网络模型揭示了活动级联现象，开发了预测开发者流失的方法。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发者的集体社会行为对于建模和预测开源软件社区的长期动态和可持续性至关重要，特别是探究提交活动的突发性现象背后的社会机制。

Method: 采用基于网络的建模框架，通过共同编辑网络捕捉开发者互动，开发识别活动级联的方法，分析50个主要开源社区的大型数据集。

Result: 活动级联在超过一半的研究项目中是统计显著现象，基于此开发了简单实用的流失预测方法，能够预测哪些开发者可能离开项目。

Conclusion: 研究揭示了开源软件社区中涌现的集体社会动态，强调了活动级联对于理解协作软件项目中开发者流失和保留的重要性。

Abstract: Understanding the collective social behavior of software developers is
crucial to model and predict the long-term dynamics and sustainability of Open
Source Software (OSS) communities. To this end, we analyze temporal activity
patterns of developers, revealing an inherently ``bursty'' nature of commit
contributions. To investigate the social mechanisms behind this phenomenon, we
adopt a network-based modelling framework that captures developer interactions
through co-editing networks. Our framework models social interactions, where a
developer editing the code of other developers triggers accelerated activity
among collaborators. Using a large data set on 50 major OSS communities, we
further develop a method that identifies activity cascades, i.e. the
propagation of developer activity in the underlying co-editing network. Our
results suggest that activity cascades are a statistically significant
phenomenon in more than half of the studied projects. We further show that our
insights can be used to develop a simple yet practical churn prediction method
that forecasts which developers are likely to leave a project. Our work sheds
light on the emergent collective social dynamics in OSS communities and
highlights the importance of activity cascades to understand developer churn
and retention in collaborative software projects.

</details>


### [49] [Hamster: A Large-Scale Study and Characterization of Developer-Written Tests](https://arxiv.org/abs/2509.26204)
*Rangeet Pan,Tyler Stennett,Raju Pavuluri,Nate Levin,Alessandro Orso,Saurabh Sinha*

Main category: cs.SE

TL;DR: 对170万Java开发者测试用例的实证研究发现，当前自动化测试生成工具难以生成符合开发者实际测试特征（如测试范围、断言类型、模拟使用等）的测试。


<details>
  <summary>Details</summary>
Motivation: 理解开发者编写的测试特征，评估自动化测试生成工具生成真实代表性测试的能力，填补现有研究空白。

Method: 对开源仓库中170万个Java测试用例进行大规模实证研究，分析测试范围、测试装置、断言、输入类型和模拟使用等特征，并与两种最先进ATG工具生成的测试进行比较。

Result: 绝大多数开发者编写的测试具有当前ATG工具无法复现的特征，存在显著的能力差距。

Conclusion: 基于研究发现，提出了有前景的研究方向，帮助缩小工具能力与开发者测试实践之间的差距，推动ATG工具生成更符合开发者实际需求的测试。

Abstract: Automated test generation (ATG), which aims to reduce the cost of manual test
suite development, has been investigated for decades and has produced countless
techniques based on a variety of approaches: symbolic analysis, search-based,
random and adaptive-random, learning-based, and, most recently,
large-language-model-based approaches. However, despite this large body of
research, there is still a gap in our understanding of the characteristics of
developer-written tests and, consequently, in our assessment of how well ATG
techniques and tools can generate realistic and representative tests. To bridge
this gap, we conducted an extensive empirical study of developer-written tests
for Java applications, covering 1.7 million test cases from open-source
repositories. Our study is the first of its kind in studying aspects of
developer-written tests that are mostly neglected in the existing literature,
such as test scope, test fixtures and assertions, types of inputs, and use of
mocking. Based on the characterization, we then compare existing tests with
those generated by two state-of-the-art ATG tools. Our results highlight that a
vast majority of developer-written tests exhibit characteristics that are
beyond the capabilities of current ATG tools. Finally, based on the insights
gained from the study, we identify promising research directions that can help
bridge the gap between current tool capabilities and more effective tool
support for developer testing practices. We hope that this work can set the
stage for new advances in the field and bring ATG tools closer to generating
the types of tests developers write.

</details>


### [50] [UniSage: A Unified and Post-Analysis-Aware Sampling for Microservices](https://arxiv.org/abs/2509.26336)
*Zhouruixing Zhu,Zhihan Jiang,Tianyi Yang,Pinjia He*

Main category: cs.SE

TL;DR: UniSage是一个统一框架，采用后分析感知范式对追踪和日志进行采样，通过轻量级多模态异常检测和根因分析指导采样策略，在2.5%采样率下能捕获56.5%关键追踪和96.25%相关日志，显著提升根因分析准确性。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统中追踪和日志数据量激增带来存储开销和故障诊断困难，现有采样方法会丢弃故障相关信息，阻碍系统行为诊断的透明度。

Method: UniSage首先对完整数据流进行轻量级多模态异常检测和根因分析，然后采用双支柱采样策略：分析引导采样器优先处理根因分析涉及的数据，边缘案例采样器确保捕获罕见但关键的行为。

Result: 在2.5%采样率下，UniSage捕获56.5%的关键追踪和96.25%的相关日志，下游根因分析准确率(AC@1)提升42.45%，处理10分钟遥测数据仅需不到5秒。

Conclusion: UniSage通过后分析感知采样范式有效解决了分布式系统中追踪和日志数据的存储与诊断难题，在保持高效率的同时显著提升了故障诊断能力。

Abstract: Traces and logs are essential for observability and fault diagnosis in modern
distributed systems. However, their ever-growing volume introduces substantial
storage overhead and complicates troubleshooting. Existing approaches typically
adopt a sample-before-analysis paradigm: even when guided by data heuristics,
they inevitably discard failure-related information and hinder transparency in
diagnosing system behavior. To address this, we introduce UniSage, the first
unified framework to sample both traces and logs using a post-analysis-aware
paradigm. Instead of discarding data upfront, UniSagefirst performs lightweight
and multi-modal anomaly detection and root cause analysis (RCA) on the complete
data stream. This process yields fine-grained, service-level diagnostic
insights that guide a dual-pillar sampling strategy for handling both normal
and anomalous scenarios: an analysis-guided sampler prioritizes data implicated
by RCA, while an edge-case-based sampler ensures rare but critical behaviors
are captured. Together, these pillars ensure comprehensive coverage of critical
signals without excessive redundancy. Extensive experiments demonstrate that
UniSage significantly outperforms state-of-the-art baselines. At a 2.5%
sampling rate, it captures 56.5% of critical traces and 96.25% of relevant
logs, while improving the accuracy (AC@1) of downstream root cause analysis by
42.45%. Furthermore, its efficient pipeline processes 10 minutes of telemetry
data in under 5 seconds, demonstrating its practicality for production
environments.

</details>


### [51] [Institutional Policy Pathways for Supporting Research Software: Global Trends and Local Practices](https://arxiv.org/abs/2509.26422)
*Michelle Barker,Jeremy Cohen,Pedro Hernández Serrano,Daniel S. Katz,Kim Martin,Dan Rudmann,Hugh Shanahan*

Main category: cs.SE

TL;DR: 该论文分析了研究机构在支持研究软件开发和使用方面的政策缺失问题，介绍了PRO4RS工作组为推进研究软件政策发展所做的工作。


<details>
  <summary>Details</summary>
Motivation: 研究软件在现代科学中日益重要，但研究机构在培训基础设施、认可和奖励结构方面发展不足，无法支持研究软件最佳实践的广泛应用和技术角色的长期支持。

Method: PRO4RS工作组（ReSA和RDA的联合倡议）通过分析全球机构的研究软件政策发展，识别关键政策缺口。

Result: 研究发现研究机构层面的研究软件政策仍然有限或缺乏广度，特别是在研究评估改革中考虑研究软件人员的政策方面存在关键缺口。

Conclusion: 研究机构需要实施和采用稳健的政策来支持研究软件的开发、使用和可持续性，以解决现代研究环境中的这一基本问题。

Abstract: As research software becomes increasingly central to modern science,
research-performing organisations (RPOs) need to ensure that their investment
in people, skills and infrastructure around research software produces
sustainable and maintainable software that improves the research they perform,
which in turn improves the overall institution and its reputation and funding,
for example, by competing with peers who lack this approach. However, research
institution management and recognition of research software and its personnel
has mostly often developed in an ad hoc manner. RPO training infrastructures,
recognition and reward structures, have not developed at a sufficient rate to
support and encourage both the widespread use of research software best
practices and the long-term support for technical roles that is required. To
begin to address this fundamental problem for modern research environments,
RPOs must implement and adopt robust policies to support research software
development, use, and sustainability. Despite growing momentum from funders and
publishers around FAIR and open science principles, research
institutional-level policies specifically addressing research software remain
limited or lacking in breadth.
  This article outlines the work of the Policies in Research Organisations for
Research Software (PRO4RS) Working Group (WG), a joint initiative of the
Research Software Alliance (ReSA) and the Research Data Alliance (RDA), which
examined and advanced research software policy development across institutions
worldwide. After consideration of the rationale for institutional policies on
research software, the PRO4RS WG outputs and analysis are utilised to highlight
critical policy gaps, particularly related to consideration of research
software personnel in policy work focused on reform of research assessment.

</details>


### [52] [EQ-Robin: Generating Multiple Minimal Unique-Cause MC/DC Test Suites](https://arxiv.org/abs/2509.26458)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: EQ-Robin是一个轻量级流水线，通过生成多个语义等价的布尔表达式变体，为每个变体应用Robin's Rule算法，产生多样化的最小MC/DC测试套件，以规避因非法输入导致的覆盖失败风险。


<details>
  <summary>Details</summary>
Motivation: 现有Robin's Rule算法虽然能生成理论最小N+1个测试用例，但只产生单一测试套件。如果其中某个必需的'independence pair'测试用例违反系统约束，整个套件就无法达到100%覆盖率。

Method: 通过对抽象语法树(AST)进行代数重排，系统性地生成语义等价的单布尔表达式变体，然后对每个结构变体应用Robin's Rule算法。

Result: 能够产生多样化的最小Unique-Cause MC/DC测试套件集合，在保持N+1最小性保证的同时，提供发现有效测试套件的弹性路径。

Conclusion: EQ-Robin为在现实约束条件下确保鲁棒的MC/DC覆盖提供了实用解决方案，计划在TCAS-II衍生的单布尔表达式上进行评估验证。

Abstract: Modified Condition/Decision Coverage (MC/DC), particularly its strict
Unique-Cause form, is a cornerstone of safety-critical software verification. A
recent algorithm, "Robin's Rule," introduced a deterministic method to
construct the theoretical minimum of N+1 test cases for Singular Boolean
Expressions (SBEs). However, this approach yields only a single test suite,
introducing a critical risk: if a test case forming a required 'independence
pair' is an illegal input forbidden by system constraints, the suite fails to
achieve 100% coverage. This paper proposes EQ-Robin, a lightweight pipeline
that systematically generates a family of minimal Unique-Cause MC/DC suites to
mitigate this risk. We introduce a method for systematically generating
semantically equivalent SBEs by applying algebraic rearrangements to an
Abstract Syntax Tree (AST) representation of the expression. By applying
Robin's Rule to each structural variant, a diverse set of test suites can be
produced. This provides a resilient path to discovering a valid test suite that
preserves the N+1 minimality guarantee while navigating real-world constraints.
We outline an evaluation plan on TCAS-II-derived SBEs to demonstrate how
EQ-Robin offers a practical solution for ensuring robust MC/DC coverage.

</details>


### [53] [ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service Systems](https://arxiv.org/abs/2509.26463)
*Junsong Pu,Yichen Li,Zhuangbin Chen,Jinyang Liu,Zhihan Jiang,Jianjun Chen,Rui Shi,Zibin Zheng,Tieying Zhang*

Main category: cs.SE

TL;DR: ErrorPrism：一种自动化重建微服务系统中错误传播路径的工具，通过静态分析和LLM代理的迭代反向搜索，在字节跳动67个生产微服务中达到97.0%的准确率。


<details>
  <summary>Details</summary>
Motivation: 云服务系统中由于故障的级联效应，可靠性管理具有挑战性。错误包装实践虽然丰富了错误上下文，但也带来了从最终日志消息回溯完整错误传播路径的可追溯性问题。现有方法无法有效解决此问题。

Method: ErrorPrism首先对服务代码仓库进行静态分析，构建函数调用图并将日志字符串映射到相关候选函数，显著减少路径搜索空间。然后使用LLM代理执行迭代反向搜索，准确重建完整的多跳错误路径。

Result: 在字节跳动的67个生产微服务上评估，ErrorPrism对102个真实世界错误的重建路径准确率达到97.0%，优于现有的静态分析和基于LLM的方法。

Conclusion: ErrorPrism为工业微服务系统中的根本原因分析提供了一个有效且实用的工具。

Abstract: Reliability management in cloud service systems is challenging due to the
cascading effect of failures. Error wrapping, a practice prevalent in modern
microservice development, enriches errors with context at each layer of the
function call stack, constructing an error chain that describes a failure from
its technical origin to its business impact. However, this also presents a
significant traceability problem when recovering the complete error propagation
path from the final log message back to its source. Existing approaches are
ineffective at addressing this problem. To fill this gap, we present ErrorPrism
in this work for automated reconstruction of error propagation paths in
production microservice systems. ErrorPrism first performs static analysis on
service code repositories to build a function call graph and map log strings to
relevant candidate functions. This significantly reduces the path search space
for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an
iterative backward search to accurately reconstruct the complete, multi-hop
error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism
achieves 97.0% accuracy in reconstructing paths for 102 real-world errors,
outperforming existing static analysis and LLM-based approaches. ErrorPrism
provides an effective and practical tool for root cause analysis in industrial
microservice systems.

</details>


### [54] [Towards Verified Code Reasoning by LLMs](https://arxiv.org/abs/2509.26546)
*Meghana Sistla,Gogul Balakrishnan,Pat Rondon,José Cambronero,Michele Tufano,Satish Chandra*

Main category: cs.SE

TL;DR: 提出了一种自动验证代码推理代理答案的方法，通过提取代理响应的形式化表示，并使用形式化验证和程序分析工具来验证其推理步骤。


<details>
  <summary>Details</summary>
Motivation: 由于LLM代理在代码推理中答案不总是正确，在需要高精度的场景下（如理解新代码库、代码审查、自动化代码生成验证）缺乏可信度，需要手动验证，这会降低开发效率。

Method: 提取代理响应的形式化表示，然后使用形式化验证和程序分析工具来验证代理的推理步骤。

Result: 在20个未初始化变量错误检测中，形式化验证成功验证了13/20个例子的代理推理；在20个程序等价性查询中，成功捕获了6/8个代理的错误判断。

Conclusion: 该方法能够有效自动验证代码推理代理的答案，提高代理的可信度和实用性。

Abstract: While LLM-based agents are able to tackle a wide variety of code reasoning
questions, the answers are not always correct. This prevents the agent from
being useful in situations where high precision is desired: (1) helping a
software engineer understand a new code base, (2) helping a software engineer
during code review sessions, and (3) ensuring that the code generated by an
automated code generation system meets certain requirements (e.g. fixes a bug,
improves readability, implements a feature).
  As a result of this lack of trustworthiness, the agent's answers need to be
manually verified before they can be trusted. Manually confirming responses
from a code reasoning agent requires human effort and can result in slower
developer productivity, which weakens the assistance benefits of the agent. In
this paper, we describe a method to automatically validate the answers provided
by a code reasoning agent by verifying its reasoning steps. At a very high
level, the method consists of extracting a formal representation of the agent's
response and, subsequently, using formal verification and program analysis
tools to verify the agent's reasoning steps.
  We applied this approach to a benchmark set of 20 uninitialized variable
errors detected by sanitizers and 20 program equivalence queries. For the
uninitialized variable errors, the formal verification step was able to
validate the agent's reasoning on 13/20 examples, and for the program
equivalence queries, the formal verification step successfully caught 6/8
incorrect judgments made by the agent.

</details>


### [55] [Black-box Context-free Grammar Inference for Readable & Natural Grammars](https://arxiv.org/abs/2509.26616)
*Mohammad Rifat Arefin,Shanto Rahman,Christoph Csallner*

Main category: cs.SE

TL;DR: NatGI是一个基于LLM引导的语法推断框架，通过括号引导的探索、LLM驱动的生成和层次化delta调试，在语法推断的准确性、可读性和可扩展性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒上下文无关语法推断工具（如Arvada、TreeVada、Kedavra）在处理大型复杂语言时存在可扩展性、可读性和准确性不足的问题。

Method: 1. 括号引导的bubble探索：利用括号等语法线索生成结构良好的语法片段
2. LLM驱动的bubble生成和非终端标签：使用LLM生成有意义的非终端名称并选择更有前景的合并
3. 层次化delta调试：系统性地简化解析树，减少不必要的规则

Result: 在从小型语言到大型语言（lua、c、mysql）的综合基准测试中，NatGI在F1分数上始终优于强基线方法，平均F1分数达到0.57，比表现最佳的基线TreeVada高出25个百分点。

Conclusion: NatGI通过LLM引导的方法生成了具有有意义的非终端名称和紧凑结构的规则，这些规则更符合人类直觉，使开发者和研究人员能够在保持高准确性的同时，轻松检查、验证和推理诱导语法的结构和语义。

Abstract: Black-box context-free grammar inference is crucial for program analysis,
reverse engineering, and security, yet existing tools such as Arvada, TreeVada,
and Kedavra struggle with scalability, readability, and accuracy on large,
complex languages. We present NatGI, a novel LLM-guided grammar inference
framework that extends TreeVada's parse tree recovery with three key
innovations: bracket-guided bubble exploration, LLM- driven bubble generation
and non-terminal labeling, and hierarchical delta debugging (HDD) for
systematic tree simplification. Bracket-guided exploration leverages syntactic
cues such as parentheses to propose well- structured grammar fragments, while
LLM guidance produces meaningful non-terminal names and selects more promising
merges. Finally, HDD incrementally reduces unnecessary rules, which makes the
grammars both compact and interpretable. In our experiments, we evaluate NatGI
on a comprehensive benchmark suite ranging from small languages to larger ones
such as lua, c, and mysql. Our results show that NatGI consistently outperforms
strong baselines in terms of F1 score. On average, NatGI achieves an F1 score
of 0.57, which is 25pp (percentage points) higher than the best-performing
baseline, TreeVada. In the case of interpretability, our generated grammars
perform significantly better than those produced by existing approaches.
Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules
with meaningful non-terminal names and compact structures that align more
closely with human intuition. As a result, developers and researchers can
achieve higher accuracy while still being able to easily inspect, verify, and
reason about the structure and semantics of the induced grammars.

</details>
