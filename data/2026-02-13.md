<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 25]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SAFuzz: Semantic-Guided Adaptive Fuzzing for LLM-Generated Code](https://arxiv.org/abs/2602.11209)
*Ziyi Yang,Kalit Inani,Keshav Kabra,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.SE

TL;DR: SAFuzz：一种结合LLM引导自适应模糊测试的混合测试框架，用于高效检测AI生成代码中的算法漏洞，相比现有方法提升检测精度并减少时间成本。


<details>
  <summary>Details</summary>
Motivation: 当前测试框架难以跟上AI编码助手生成代码的速度，传统模糊测试方法资源分配均匀且缺乏对算法漏洞模式的语义理解，导致资源使用效率低下和漏洞遗漏。

Method: 提出SAFuzz混合测试框架，整合基于提示的行为多样化、带有问题特定预言机的测试用例生成，以及基于LLM的预测器来实现自适应资源分配和动态早期停止。

Result: 在CSES算法问题上评估，将漏洞判别精度从77.9%提升到85.7%，相比SOTA GreenFuzz减少1.71倍时间成本，同时保持相当的召回率；与现有单元测试生成方法结合可将bug检测召回率从67.3%提升到79.5%。

Conclusion: SAFuzz通过LLM引导的自适应模糊测试有效解决了AI生成代码测试的挑战，在精度和效率上均有显著提升，并能与现有方法形成互补增益。

Abstract: While AI-coding assistants accelerate software development, current testing frameworks struggle to keep pace with the resulting volume of AI-generated code. Traditional fuzzing techniques often allocate resources uniformly and lack semantic awareness of algorithmic vulnerability patterns, leading to inefficient resource usage and missed vulnerabilities. To address these limitations, we present a hybrid testing framework that leverages LLM-guided adaptive fuzzing to detect algorithmic vulnerabilities efficiently. Our system SAFuzz integrates prompt-based behavioral diversification, harness generation with problem-specific oracles, and an LLM-based predictor to enable adaptive resource allocation and dynamic early stopping. Evaluating SAFuzz on CSES algorithmic problems, we improve vulnerability discrimination precision from 77.9% to 85.7% and achieve a 1.71x reduction in time cost compared to SOTA GreenFuzz while maintaining comparable recall. We further observe that combining our approach with existing unit test generation methods yields complementary gains, increasing the bug detection recall from 67.3% to 79.5%.

</details>


### [2] [SWE-MiniSandbox: Container-Free Reinforcement Learning for Building Software Engineering Agents](https://arxiv.org/abs/2602.11210)
*Danlong Yuan,Wei Wu,Zhengren Wang,Xueliang Zhao,Huishuai Zhang,Dongyan Zhao*

Main category: cs.SE

TL;DR: SWE-MiniSandbox：一种轻量级、无容器的RL训练方法，用于软件工程代理，通过内核级隔离机制替代传统容器，显著降低存储开销和准备时间。


<details>
  <summary>Details</summary>
Motivation: 现有基于容器的RL训练管道存在存储开销大、环境设置慢、需要容器管理权限等问题，限制了软件工程代理的可扩展性，特别是在资源受限的研究环境中。

Method: 提出SWE-MiniSandbox方法，使用内核级机制为每个任务创建隔离工作空间，替代传统的每实例容器；采用轻量级环境预缓存技术，消除对庞大容器镜像的依赖。

Result: 磁盘使用量降至容器基线的约5%，环境准备时间减少至约25%，同时保持与标准容器管道相当的评估性能。

Conclusion: SWE-MiniSandbox通过消除对重型容器基础设施的依赖，为RL驱动的软件工程代理提供了实用且可访问的扩展基础，特别适合资源受限的研究环境。

Abstract: Reinforcement learning (RL) has become a key paradigm for training software engineering (SWE) agents, but existing pipelines typically rely on per-task containers for isolation. At scale, pre-built container images incur substantial storage overhead, slow environment setup, and require container-management privileges. We propose SWE-MiniSandbox, a lightweight, container-free method that enables scalable RL training of SWE agents without sacrificing isolation. Instead of relying on per-instance containers, SWE-MiniSandbox executes each task in an isolated workspace backed by kernel-level mechanisms, substantially reducing system overhead. It leverages lightweight environment pre-caching techniques to eliminate the need for bulky container images. As a result, our approach lowers disk usage to approximately 5\% of that required by container-based pipelines and reduces environment preparation time to about 25\% of the container baseline. Empirical results demonstrate that SWE-MiniSandbox achieves evaluation performance comparable to standard container-based pipelines. By removing the dependency on heavy container infrastructure, SWE-MiniSandbox offers a practical and accessible foundation for scaling RL-based SWE agents, particularly in resource-constrained research environments.

</details>


### [3] [Patient Digital Twins for Chronic Care: Technical Hurdles, Lessons Learned, and the Road Ahead](https://arxiv.org/abs/2602.11223)
*Micheal P. Papazoglou,Bernd J. Krämer,Mira Raheem,Amal Elgammal*

Main category: cs.SE

TL;DR: 论文探讨了患者医疗数字孪生（PMDT）在慢性病管理中的应用，通过本体驱动建模和联邦分析试点展示了其可行性，并分析了技术挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病是全球发病、死亡和医疗成本的主要负担，但当前的医疗系统仍然分散且主要是被动反应式的。患者医疗数字孪生（PMDT）提供了一个范式转变的机会：创建全面、持续更新的患者数字副本，整合临床、基因组、生活方式和生活质量数据。

Method: 采用本体驱动建模和联邦分析试点方法，通过QUALITOP肿瘤学研究和分布式AI平台进行实施。技术实现包括与HL7 FHIR和OMOP标准对齐、嵌入隐私治理、扩展联邦查询以及设计直观的临床医生界面。

Result: 早期实施证实了PMDT的可行性，但也面临挑战：标准对齐、隐私治理、联邦查询扩展和界面设计。技术成果包括多模态蓝图的自动推理和患者结果的预测分析。

Conclusion: 通过反思这些经验，为软件工程师提供了可行的见解，并确定了机会领域（如领域特定语言和模型驱动工程），以推动PMDT向可信赖、自适应的慢性护理生态系统发展。

Abstract: Chronic diseases constitute the principal burden of morbidity, mortality, and healthcare costs worldwide, yet current health systems remain fragmented and predominantly reactive. Patient Medical Digital Twins (PMDTs) offer a paradigm shift: holistic, continuously updated digital counterparts of patients that integrate clinical, genomic, lifestyle, and quality-of-life data. We report early implementations of PMDTs via ontology-driven modeling and federated analytics pilots. Insights from the QUALITOP oncology study and a distributed AI platform confirm both feasibility and challenges: aligning with HL7 FHIR and OMOP standards, embedding privacy governance, scaling federated queries, and designing intuitive clinician interfaces. We also highlight technical gains, such as automated reasoning over multimodal blueprints and predictive analytics for patient outcomes. By reflecting on these experiences, we outline actionable insights for software engineers and identify opportunities, such as DSLs and model-driven engineering, to advance PMDTs toward trustworthy, adaptive chronic care ecosystems.

</details>


### [4] [Agent-Diff: Benchmarking LLM Agents on Enterprise API Tasks via Code Execution with State-Diff-Based Evaluation](https://arxiv.org/abs/2602.11224)
*Hubert M. Pysklo,Artem Zhuravel,Patrick D. Watson*

Main category: cs.SE

TL;DR: Agent-Diff是一个用于评估基于代码执行的智能体LLM的基准框架，通过沙盒化真实API接口访问，使用状态差异合约定义任务成功标准，在标准化环境中评估不同模型在真实企业软件工作流中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前智能体LLM的评估面临挑战：模型性能差异大，受外部工具访问、提示结构、智能体框架等多种因素影响。现有基准需要在沙盒化控制变量和生态有效性之间权衡，缺乏既能使用真实API接口又能标准化评估环境的方法。

Method: 1. 状态差异合约：将过程与结果分离，通过环境状态的预期变化定义任务成功，而非模糊的轨迹或参数匹配。2. 标准化沙盒：提供统一的脚本层，所有模型都通过该层执行代码访问外部API（Slack、Box、Linear、Google Calendar等）。

Result: 1. 开发了Agent-Diff框架，支持对9个LLM在224个企业软件工作流任务上进行基准测试。2. 通过消融实验评估了API文档访问对基准性能的贡献，验证了框架的鲁棒性。

Conclusion: Agent-Diff成功结合了沙盒化控制和生态有效性的优势，通过状态差异合约和标准化沙盒，为智能体LLM在真实世界任务上的评估提供了统一、可比较的基准框架。

Abstract: We present Agent-Diff, a novel benchmarking framework for evaluating agentic Large Language Models (LLMs) on real-world tasks that execute code via external APIs. Agentic LLM performance varies due to differences in models, external tool access, prompt structures, and agentic frameworks. Benchmarks must make fundamental trade-offs between a sandboxed approach that controls for variation in software environments and more ecologically valid approaches employing real services. Agent-Diff attempts to capture the desirable features of both of these approaches by including access to the real API interfaces for software services while sandboxing the environment in which calls are made, processed, and evaluated. This approach relies on two key innovations. The first is a novel state-diff contract, which separates process from outcome - rather than fuzzy trace or parameter matching, we define task success as whether the expected change in environment state was achieved. The second is a novel sandbox that provides a standardized scripting layer that all models use to execute code against external APIs (Slack, Box, Linear, Google Calendar). Thus, we can evaluate different agentic LLMs against a standardized set of contracts using a unified sandbox while still evaluating their performance on real-world service interfaces. Using the Agent-Diff framework, we provide benchmarks for nine LLMs across 224 tasks utilizing enterprise software workflows. In addition, we evaluate the robustness of the framework with ablation experiments to assess the contribution of access to API documentation on benchmark performance. Code and data: https://github.com/agent-diff-bench/agent-diff.

</details>


### [5] [Improving the Robustness of Large Language Models for Code Tasks via Fine-tuning with Perturbed Data](https://arxiv.org/abs/2602.11411)
*Yang Liu,Armstrong Foundjem,Xingfang Wu,Heng Li,Foutse Khomh*

Main category: cs.SE

TL;DR: 通过使用扰动数据集微调LLMs，可以显著提升代码相关任务中对输入扰动的鲁棒性，但会带来轻微的性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在软件开发中日益重要，确保这些模型处理多样化输入时的鲁棒性至关重要，因为输入变化可能导致错误或不安全的代码输出。

Method: 系统评估LLM鲁棒性，通过在字符级、词级和句子级扰动的数据集上微调模型，并与基础模型及在未扰动数据集上微调的模型进行比较。

Result: 使用扰动数据集微调显著提高了模型鲁棒性（RD通常下降4%-6%），特别是对于鲁棒性相对较弱的模型。但相比未扰动数据集微调，通常会导致轻微性能下降（pass@1通常下降1%-3%），偶尔也能观察到性能提升。

Conclusion: 使用扰动数据微调LLMs能有效增强代码任务的鲁棒性，代价是轻微性能下降，强调了在代码应用中平衡鲁棒性和性能的重要性。

Abstract: Context: In the fast-paced evolution of software development, Large Language Models (LLMs) have become indispensable tools for tasks such as code generation, completion, analysis, and bug fixing. Ensuring the robustness of these models against potential vulnerabilities from handling diverse inputs is critical, as variations in input can lead to incorrect or insecure code outputs.
  Objective: This work aims to improve the robustness of LLMs for coding-related tasks against potential adversarial inputs. Specifically, we investigate how fine-tuning LLMs with perturbed datasets impacts their robustness against input perturbations.
  Method: We systematically evaluated LLM robustness by fine-tuning models using datasets perturbed at character-level, word-level, and sentence-level, comparing results against base models and models fine-tuned on unperturbed datasets.
  Results: Fine-tuning LLMs with perturbed datasets significantly improves model robustness (RD usually drops around 4\% - 6\%), especially for models with relatively weak robustness. However, this fine-tuning process typically results in a slight performance decrease (pass@1 usually drops around 1\% - 3\%) compared to fine-tuning with unperturbed datasets, although occasional performance improvements are observed.
  Conclusion \& Implications: Fine-tuning LLMs for coding tasks with perturbed data effectively enhances their robustness at the cost of a minor performance reduction, emphasizing the importance of balancing the robustness and performance of LLMs for coding applications.

</details>


### [6] [A Grounded Theory of Debugging in Professional Software Engineering Practice](https://arxiv.org/abs/2602.11435)
*Haolin Li,Michael Coblenz*

Main category: cs.SE

TL;DR: 专业开发者的调试是一个结构化的诊断过程，他们通过更新系统心智模型来指导信息收集，交替使用导航和执行策略，结合正向和反向推理模式，并根据代码库上下文、复杂性和熟悉度调整方法。


<details>
  <summary>Details</summary>
Motivation: 调试是软件工程中的核心但复杂活动。先前研究记录了调试策略和工具使用，但缺乏解释经验丰富的开发者如何在大型真实代码库中推理bug的理论。本研究旨在填补这一理论空白。

Method: 采用扎根理论方法的定性研究，观察7名专业开发者和5名专业直播编码者在他们自己的代码库中处理17个调试任务，捕捉调试的多样化上下文。

Result: 调试被理论化为结构化的迭代诊断过程，开发者通过交替导航和执行策略收集信息，使用正向和反向追踪推理模式，并根据代码库上下文、复杂性和熟悉度调整方法。他们还会收集外部资源补充代码证据，利用经验系统构建心智模型。

Conclusion: 提出了专业调试的扎根理论，揭示了实践中的人为中心维度，对工具设计和软件工程教育具有重要启示。

Abstract: Debugging is a central yet complex activity in software engineering. Prior studies have documented debugging strategies and tool usage, but little theory explains how experienced developers reason about bugs in large, real-world codebases. We conducted a qualitative study using a grounded theory approach. We observed seven professional developers and five professional live-coding streamers working on 17 debugging tasks in their own codebases, capturing diverse contexts of debugging. We theorize debugging as a structured, iterative diagnostic process in which programmers update a mental model of the system to guide information gathering. Developers gather information by alternating between navigation and execution strategies, employing forward and backward tracing modes of reasoning and adapting these approaches according to codebase context, complexity, and familiarity. Developers also gather external resources to complement code-based evidence, with their experience enabling them to systematically construct a mental model. We contribute a grounded theory of professional debugging that surfaces the human-centered dimensions of the practice, with implications for tool design and software engineering education.

</details>


### [7] [Addressing OSS Community Managers' Challenges in Contributor Retention](https://arxiv.org/abs/2602.11447)
*Zixuan Feng,Katie Kimura,Bianca Trinkenreich,Igor Steinmacher,Marco Gerosa,Anita Sarma*

Main category: cs.SE

TL;DR: 该研究针对开源软件社区贡献者流失问题，开发了一个预测性分析框架和原型工具，帮助社区管理者提前识别潜在流失风险并采取干预措施。


<details>
  <summary>Details</summary>
Motivation: 开源软件社区管理者面临贡献者流失的挑战，现有工具（如仪表板）仅提供回顾性分析而非预测性洞察，无法早期识别潜在流失风险，导致管理者负担加重而非获得支持。

Method: 采用设计科学研究范式，通过混合方法进行问题识别和解决方案设计：包括半结构化访谈、多声文献综述、社区调查，然后通过迭代的构建-评估循环开发并完善流失风险诊断策略，最终实现为基于Web的原型，并收集了100多名开源从业者的反馈，在两个开源社区进行了实地评估。

Result: 研究提供了：(1) 关于开源软件贡献者流失管理挑战的实证见解；(2) 支持开源社区管理者流失管理工作的可操作策略；(3) 为未来开发或验证开源可持续性理论研究的实用框架。

Conclusion: 该研究成功开发了一个预测性分析框架和工具原型，能够帮助开源社区管理者更有效地管理贡献者流失问题，为开源可持续性研究提供了理论和实践基础。

Abstract: Open-source software (OSS) community managers face significant challenges in retaining contributors, as they must monitor activity and engagement while navigating complex dynamics of collaboration. Current tools designed for managing contributor retention (e.g., dashboards) fall short by providing retrospective rather than predictive insights to identify potential disengagement early. Without understanding how to anticipate and prevent disengagement, new solutions risk burdening community managers rather than supporting retention management. Following the Design Science Research paradigm, we employed a mixed-methods approach for problem identification and solution design to address contributor retention. To identify the challenges hindering retention management in OSS, we conducted semi-structured interviews, a multi-vocal literature review, and community surveys. Then through an iterative build-evaluate cycle, we developed and refined strategies for diagnosing retention risks and informing engagement efforts. We operationalized these strategies into a web-based prototype, incorporating feedback from 100+ OSS practitioners, and conducted an in situ evaluation across two OSS communities. Our study offers (1) empirical insights into the challenges of contributor retention management in OSS, (2) actionable strategies that support OSS community managers' retention efforts, and (3) a practical framework for future research in developing or validating theories about OSS sustainability.

</details>


### [8] [Search-Based Quantum Program Testing via Commuting Pauli String](https://arxiv.org/abs/2602.11487)
*Asmar Muqeet,Shaukat Ali,Paolo Arcaini*

Main category: cs.SE

TL;DR: SB-QOPS是一种基于搜索的量子程序测试方法，通过交换Pauli字符串和测量中心化预言机，有效测试量子程序并减少对完整程序规范的需求。


<details>
  <summary>Details</summary>
Motivation: 现有量子软件测试方法依赖简单测试输入和统计预言机，需要昂贵的程序规范，且在真实量子计算机上的验证有限。需要更有效的测试方法来应对这些挑战。

Method: SB-QOPS将测试用例重新定义为Pauli字符串，引入利用其交换性质的测量中心化预言机，通过基于期望值的适应度函数系统探索搜索空间，提高测试预算利用率。

Result: 在真实量子计算机和模拟器上对最多29个量子比特的电路进行大规模评估，SB-QOPS显著优于QOPS，对最多29个量子比特的电路实现100%的故障检测分数，并展示跨量子平台的移植性。

Conclusion: SB-QOPS通过搜索策略和测量中心化预言机有效解决了量子程序测试的挑战，在真实量子计算机上表现出色，具有跨平台移植性，为量子软件测试提供了实用解决方案。

Abstract: Quantum software testing is important for reliable quantum software engineering. Despite recent advances, existing quantum software testing approaches rely on simple test inputs and statistical oracles, costly program specifications, and limited validation on real quantum computers. To address these challenges, we propose SB-QOPS, a search-based quantum program testing approach via commuting Pauli strings. SB-QOPS, as a direct extension to a previously proposed QOPS approach, redefines test cases in terms of Pauli strings and introduces a measurement-centric oracle that exploits their commutation properties, enabling effective testing of quantum programs while reducing the need for full program specifications. By systematically exploring the search space through an expectation-value-based fitness function, SB-QOPS improves test budget utilization and increases the likelihood of uncovering subtle faults. We conduct a large-scale empirical evaluation on quantum circuits of up to 29 qubits on real quantum computers and emulators. We assess three search strategies: Genetic Algorithm, Hill Climbing, and the (1+1) Evolutionary Algorithm, and evaluate SB-QOPS under both simulated and real noisy conditions. Experiments span three quantum computing platforms: IBM, IQM, and Quantinuum. Results show that SB-QOPS significantly outperforms QOPS, achieving a fault-detection score of 100% for circuits up to 29 qubits, and demonstrating portability across quantum platforms.

</details>


### [9] [How Smart Is Your GUI Agent? A Framework for the Future of Software Interaction](https://arxiv.org/abs/2602.11514)
*Sidong Feng,Chunyang Chen*

Main category: cs.SE

TL;DR: 提出GUI Agent Autonomy Levels (GAL)六层框架，用于明确GUI代理的自主性程度，帮助衡量可信软件交互的进展


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理被描述为具有截然不同的自主性程度，这模糊了其能力、责任和风险，需要概念清晰化

Method: 提出GUI Agent Autonomy Levels (GAL)六层框架，使自主性明确化

Result: 建立了系统化的自主性评估框架，有助于基准测试可信软件交互的进展

Conclusion: GAL框架为GUI代理提供了概念清晰度，有助于明确能力、责任和风险，推动可信软件交互的发展

Abstract: GUI agents are rapidly becoming a new interaction to software, allowing people to navigate web, desktop and mobile rather than execute them click by click. Yet ``agent'' is described with radically different degrees of autonomy, obscuring capability, responsibility and risk. We call for conceptual clarity through GUI Agent Autonomy Levels (GAL), a six-level framework that makes autonomy explicit and helps benchmark progress toward trustworthy software interaction.

</details>


### [10] [Do Not Treat Code as Natural Language: Implications for Repository-Level Code Generation and Beyond](https://arxiv.org/abs/2602.11671)
*Minh Le-Anh,Huyen Nguyen,Khanh An Tran,Nam Le Hai,Linh Ngo Van,Nghi D. Q. Bui,Bach Le*

Main category: cs.SE

TL;DR: Hydra是一个仓库级代码生成框架，通过结构感知索引和依赖感知检索解决现有RAG方法在代码生成中忽略代码结构和依赖关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索增强生成的代码生成方法主要借鉴NLP策略，使用分块索引和相似性检索，导致代码单元间连贯性丢失，忽略结构关系，且相似性驱动方法经常错过功能相关的依赖（如辅助函数、类、全局变量）。

Method: 提出Hydra框架：1) 结构感知索引策略，将仓库表示为函数、类和变量的层次树，保留代码结构和依赖；2) 轻量级依赖感知检索器，显式识别和检索目标函数所需的真实依赖；3) 混合检索机制，结合依赖感知检索和相似性检索，提供基本构建块和实际使用示例。

Result: 在DevEval和RepoExec基准测试中取得最先进性能，在Pass@1上超过最强基线5%以上，甚至能让较小模型匹配或超过依赖现有检索器的更大模型性能。

Conclusion: Hydra通过将代码视为结构化代码而非自然语言，有效解决了仓库级代码生成中的结构依赖问题，显著提升了代码大模型在复杂仓库环境下的性能。

Abstract: Large language models for code (CodeLLMs) have demonstrated remarkable success in standalone code completion and generation, sometimes even surpassing human performance, yet their effectiveness diminishes in repository-level settings where cross-file dependencies and structural context are essential. Existing Retrieval-Augmented Generation (RAG) approaches often borrow strategies from NLP, relying on chunking-based indexing and similarity-based retrieval. Chunking results in the loss of coherence between code units and overlooks structural relationships, while similarity-driven methods frequently miss functionally relevant dependencies such as helper functions, classes, or global variables. To address these limitations, we present Hydra, a repository-level code generation framework that treats code as structured code rather than natural language. Our approach introduces (i) a structure-aware indexing strategy that represents repositories as hierarchical trees of functions, classes, and variables, preserving code structure and dependencies, (ii) a lightweight dependency-aware retriever (DAR) that explicitly identifies and retrieves the true dependencies required by a target function, and (iii) a hybrid retrieval mechanism that combines DAR with similarity-based retrieval to provide both essential building blocks and practical usage examples. Extensive experiments on the challenging DevEval and RepoExec benchmarks, both requiring function implementation from real-world repositories with complex large repository context, show that Hydra achieves state-of-the-art performance across open- and closed-source CodeLLMs. Notably, our method establishes a new state of the art in repository-level code generation, surpassing strongest baseline by over 5% in Pass@1 and even enabling smaller models to match or exceed the performance of much larger ones that rely on existing retrievers.

</details>


### [11] [Beyond Code: Empirical Insights into How Team Dynamics Influence OSS Project Selection](https://arxiv.org/abs/2602.11692)
*Shashiwadana Nirmani,Hourieh Khalajzadeh,Mojtaba Shahin,Xiao Liu*

Main category: cs.SE

TL;DR: 研究发现开源贡献者在选择项目时，除了技术因素外，团队动态（如沟通响应性、语气、回复清晰度）是重要考量因素，且这些偏好多与贡献者的动机类型相关。


<details>
  <summary>Details</summary>
Motivation: 当前开源项目推荐系统主要关注技术属性，忽视了影响贡献者加入和留存的协作与社区因素。研究旨在探索团队动态如何影响项目选择，以及这些偏好如何随贡献者动机而变化。

Method: 对198名开源从业者进行在线调查，结合定量和定性分析方法，捕捉贡献者对团队动态的感知。

Result: 沟通相关的团队动态（响应性、语气、回复清晰度）在所有从业者中一致被优先考虑。但这些团队动态的相对重要性因贡献者动机而异：追求声誉或社交网络的从业者更偏好鼓励多样参与的包容性社区。

Conclusion: 理解团队动态如何与贡献者动机相匹配，能为从业者的项目选择行为提供宝贵见解。这些发现可为设计未来"人类感知"的项目推荐系统提供参考，更好地考虑社交协作质量和动机匹配。

Abstract: Open-source software (OSS) development relies on effective collaboration among distributed contributors. Yet, current OSS project recommendation systems primarily emphasize technical attributes, overlooking the collaboration and community aspects that influence contributors' decisions to join and remain in projects. This study investigates how team dynamics within OSS communities influence project selection and how these preferences vary across contributors' motivations. We conducted an online survey with 198 OSS practitioners, combining quantitative and qualitative analyses to capture contributors' perceptions of team dynamics. The results reveal that communication-related team dynamics such as responsiveness, tone, and clarity of replies are consistently prioritized across practitioners. However, the relative importance of these team dynamics differs according to contributors' motivations. For instance, practitioners motivated by gaining reputation or networking preferred inclusive project communities that encouraged diverse participation. These findings highlight that understanding how team dynamics align with contributors' motivations provides valuable insights into practitioners' project selection behaviour. Those insights can inform the design of future human-aware project recommendation systems that better account for social collaboration quality and motivational fit.

</details>


### [12] [WebTestPilot: Agentic End-to-End Web Testing against Natural Language Specification by Inferring Oracles with Symbolized GUI Elements](https://arxiv.org/abs/2602.11724)
*Xiwen Teoh,Yun Lin,Duc-Minh Nguyen,Ruofei Ren,Wenjie Zhang,Jin Song Dong*

Main category: cs.SE

TL;DR: WebTestPilot：一个基于LLM的代理，通过符号化GUI元素和推断前后条件作为隐式预言机，解决VLM代理在Web测试中的幻觉问题，显著提升bug检测精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型（VLM）的代理在端到端Web测试中存在固有幻觉问题：当检测到需求与Web应用不一致时，难以区分是模型幻觉还是真实应用bug。这涉及两个核心技术挑战：1）隐式预言机推断挑战（代理需自行判断应用行为是否正确）；2）概率推断挑战（LLM的不一致推理削弱了其作为预言机的可信度）。现有方法无法有效捕获隐式预言机。

Method: WebTestPilot采用两种关键技术：1）符号化层：检测并将关键GUI元素符号化为变量；2）将自然语言规范转换为步骤序列，每个步骤配备基于符号推断的前后条件作为预言机。该预言机捕获数据、时间和因果依赖，从而验证隐式需求。

Result: 在注入bug的Web应用基准测试中，WebTestPilot达到99%的任务完成率，bug检测精度96%、召回率96%，显著优于最佳基线（精度提升70%，召回率提升27）。该代理能泛化到不同的自然语言输入和模型规模。

Conclusion: WebTestPilot通过符号化GUI元素和推断前后条件作为隐式预言机，有效解决了VLM代理在Web测试中的幻觉问题，显著提升了bug检测的准确性和可靠性，为自然语言到端到端测试提供了有效解决方案。

Abstract: Visual language model (VLM) agents show great promise in automating end-to-end (E2E) web testing against requirements in natural language. However, the probabilistic nature of language models can have inherent hallucinations. Therefore, given a detected inconsistency between the requirement and the web application, it is hard to distinguish whether it stems from the hallucination or a real application bug. Addressing this issue presents two core technical challenges: the implicit oracle inference challenge, where the agent must act as its own oracle to implicitly decide if the application's behavior is correct without guidance, and the probabilistic inference challenge, where an LLM's inconsistent reasoning undermines its trustworthiness as an oracle. Existing LLM-based approaches fail to capture such implicit oracles, either by treating any page navigation that doesn't crash as a success, or by checking each state in isolation, thus missing bugs dependent on context from prior steps.
  We introduce WebTestPilot, an LLM-based agent designed to address these challenges. WebTestPilot uses (1) a symbolization layer which detects and symbolizes critical GUI elements on the web application into symbols (i.e., variables) and (2) translates natural language specification into a sequence of steps, each of which is equipped with inferred pre- and post-conditions over the symbols as an oracle. This oracle captures data, temporal, and causal dependencies, enabling the validation of implicit requirements. To advance research in this area, we build a benchmark of bug-injected web apps for evaluating NL-to-E2E testing. The results show that WebTestPilot achieves a task completion rate of 99%, with 96% precision and 96% recall in bug detection, outperforming the best baseline (+70 precision, +27 recall). The agent generalizes across diverse natural language inputs and model scales.

</details>


### [13] [Leveraging Language Models to Discover Evidence-Based Actions for OSS Sustainability](https://arxiv.org/abs/2602.11746)
*Nafiz Imtiaz Khan,Vladimir Filkov*

Main category: cs.SE

TL;DR: 使用LLM从软件工程文献中挖掘证据，生成可操作的、有研究支持的OSS项目可持续性建议（ReACTs）


<details>
  <summary>Details</summary>
Motivation: 现有OSS可持续性预测模型虽然准确，但特征往往是高层次的社会技术信号，不直接可操作。大量软件工程实证研究积累了改善项目健康的具体实践证据，但未被充分利用。

Method: 设计RAG管道和两层提示策略：第一层探索开源LLM和提示技术，从829篇ICSE和FSE论文中提取候选ReACTs；第二层应用后续提示过滤幻觉，提取影响和证据，评估合理性和精确度。

Result: 生成1,922个ReACTs，其中1,312个通过严格质量标准，组织成面向实践的类别，可与APEX等工具的项目信号连接。

Conclusion: 提供了一种可重复、可扩展的方法，将分散的研究发现转化为结构化、基于证据的行动，指导OSS项目实现可持续性。

Abstract: When successful, Open Source Software (OSS) projects create enormous value, but most never reach a sustainable state. Recent work has produced accurate models that forecast OSS sustainability, yet these models rarely tell maintainers what to do: their features are often high-level socio-technical signals that are not directly actionable. Decades of empirical software engineering research have accumulated a large but underused body of evidence on concrete practices that improve project health.
  We close this gap by using LLMs as evidence miners over the SE literature. We design a RAG-pipeline and a two-layer prompting strategy that extract researched actionables (ReACTs): concise, evidence-linked recommendations mapping to specific OSS practices. In the first layer, we systematically explore open LLMs and prompting techniques, selecting the best-performing combination to derive candidate ReACTs from 829 ICSE and FSE papers. In the second layer, we apply follow-up prompting to filter hallucinations, extract impact and evidence, and assess soundness and precision.
  Our pipeline yields 1,922 ReACTs, of which 1,312 pass strict quality criteria and are organized into practice-oriented categories connectable to project signals from tools like APEX. The result is a reproducible, scalable approach turning scattered research findings into structured, evidence-based actions guiding OSS projects toward sustainability.

</details>


### [14] [AmbiBench: Benchmarking Mobile GUI Agents Beyond One-Shot Instructions in the Wild](https://arxiv.org/abs/2602.11750)
*Jiazheng Sun,Mingxuan Li,Yingying Zhang,Jiayang Niu,Yachen Wu,Ruihan Jin,Shuyu Lei,Pengrongrui Tan,Zongyu Zhang,Ruoyi Wang,Jiachen Yang,Boyu Yang,Jiacheng Liu,Xin Peng*

Main category: cs.SE

TL;DR: AmbiBench：首个针对移动GUI智能体的意图对齐基准，引入指令清晰度分类，评估智能体在模糊指令下的主动澄清和交互能力


<details>
  <summary>Details</summary>
Motivation: 现有基准假设用户指令完整明确，只评估单轮执行，忽视了智能体的意图对齐能力。实际场景中用户指令通常模糊不完整，需要智能体通过交互澄清意图

Method: 基于认知差距理论提出四种指令清晰度分类：详细、标准、不完整、模糊。构建包含240个生态有效任务的严格数据集，开发MUSE自动化评估框架，采用MLLM-as-a-judge多智能体架构进行三维度细粒度评估

Result: 揭示了最先进智能体在不同清晰度水平下的性能边界，量化了主动交互带来的收益，验证了MUSE评估与人类判断的强相关性

Conclusion: 重新定义了评估标准，为真正理解用户意图的下一代智能体奠定了基础，从单向指令执行转向双向意图对齐

Abstract: Benchmarks are paramount for gauging progress in the domain of Mobile GUI Agents. In practical scenarios, users frequently fail to articulate precise directives containing full task details at the onset, and their expressions are typically ambiguous. Consequently, agents are required to converge on the user's true intent via active clarification and interaction during execution. However, existing benchmarks predominantly operate under the idealized assumption that user-issued instructions are complete and unequivocal. This paradigm focuses exclusively on assessing single-turn execution while overlooking the alignment capability of the agent. To address this limitation, we introduce AmbiBench, the first benchmark incorporating a taxonomy of instruction clarity to shift evaluation from unidirectional instruction following to bidirectional intent alignment. Grounded in Cognitive Gap theory, we propose a taxonomy of four clarity levels: Detailed, Standard, Incomplete, and Ambiguous. We construct a rigorous dataset of 240 ecologically valid tasks across 25 applications, subject to strict review protocols. Furthermore, targeting evaluation in dynamic environments, we develop MUSE (Mobile User Satisfaction Evaluator), an automated framework utilizing an MLLM-as-a-judge multi-agent architecture. MUSE performs fine-grained auditing across three dimensions: Outcome Effectiveness, Execution Quality, and Interaction Quality. Empirical results on AmbiBench reveal the performance boundaries of SoTA agents across different clarity levels, quantify the gains derived from active interaction, and validate the strong correlation between MUSE and human judgment. This work redefines evaluation standards, laying the foundation for next-generation agents capable of truly understanding user intent.

</details>


### [15] [Verifiable Provenance of Software Artifacts with Zero-Knowledge Compilation](https://arxiv.org/abs/2602.11887)
*Javier Ron,Martin Monperrus*

Main category: cs.SE

TL;DR: 提出基于零知识虚拟机（zkVM）的源代码溯源验证方法，通过在zkVM中执行编译器生成编译输出和密码学证明，验证源代码和编译器的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前源代码溯源验证在实践中仍然困难，最流行的可重现构建技术需要匹配和重新执行构建工具链和环境，过程复杂。需要一种更简单有效的验证方法。

Method: 在零知识虚拟机（zkVM）中执行编译器，生成编译输出和密码学证明，证明编译是在声明的源代码和编译器上执行的。使用RISC Zero zkVM和ChibiCC C编译器实现概念验证。

Result: 在200个合成程序、31个OpenSSL和21个libsodium源文件上评估，zk编译适用于实际软件，成功阻止了所有针对编译器替换、源代码篡改、输出操纵和重放攻击的对抗性测试。

Conclusion: 基于zkVM的编译方法为源代码溯源提供了强大的安全保证，能够有效验证二进制文件是否来自声明的源代码，解决了传统方法的复杂性挑战。

Abstract: Verifying that a compiled binary originates from its claimed source code is a fundamental security requirement, called source code provenance. Achieving verifiable source code provenance in practice remains challenging. The most popular technique, called reproducible builds, requires difficult matching and reexecution of build toolchains and environments. We propose a novel approach to verifiable provenance based on compiling software with zero-knowledge virtual machines (zkVMs). By executing a compiler within a zkVM, our system produces both the compiled output and a cryptographic proof attesting that the compilation was performed on the claimed source code with the claimed compiler. We implement a proof-of-concept implementation using the RISC Zero zkVM and the ChibiCC C compiler, and evaluate it on 200 synthetic programs as well as 31 OpenSSL and 21 libsodium source files. Our results show that zk-compilation is applicable to real-world software and provides strong security guarantees: all adversarial tests targeting compiler substitution, source tampering, output manipulation, and replay attacks are successfully blocked.

</details>


### [16] [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation](https://arxiv.org/abs/2602.11904)
*Weixing Zhang,Bowen Jiang,Yuhong Fu,Anne Koziolek,Regina Hebig,Daniel Strüber*

Main category: cs.SE

TL;DR: LLMs在文本DSL语法和实例协同演化中表现良好，但规模增大时性能下降，Claude在40行修改时保持85%召回率，GPT在大实例上失败


<details>
  <summary>Details</summary>
Motivation: 文本DSL语法演化时，现有模型驱动工程方法无法保留布局和注释等人性化信息，需要评估LLMs在协同演化中的潜力

Method: 使用Claude Sonnet 4.5和GPT-5.2在10个案例语言上各运行10次，评估正确性和人性化信息保留能力

Result: 小规模案例表现优秀（≥94%精确率和召回率，修改少于20行），但规模增大时性能下降：Claude在40行时保持85%召回率，GPT在大实例上失败；响应时间随实例规模显著增加

Conclusion: LLM在文本DSL协同演化中有效，但当前限制在于规模扩展性，语法演化复杂性和删除粒度比变更类型影响更大

Abstract: Software languages evolve over time for reasons such as feature additions. When grammars evolve, textual instances that originally conformed to them may become outdated. While model-driven engineering provides many techniques for co-evolving models with metamodel changes, these approaches are not designed for textual DSLs and may lose human-relevant information such as layout and comments. This study systematically evaluates the potential of large language models (LLMs) for co-evolving grammars and instances of textual DSLs. Using Claude Sonnet 4.5 and GPT-5.2 across ten case languages with ten runs each, we assess both correctness and preservation of human-oriented information. Results show strong performance on small-scale cases ($\geq$94% precision and recall for instances requiring fewer than 20 modified lines), but performance degraded with scale: Claude maintains 85% recall at 40 lines, while GPT fails on the largest instances. Response time increases substantially with instance size, and grammar evolution complexity and deletion granularity affect performance more than change type. These findings clarify when LLM-based co-evolution is effective and where current limitations remain.

</details>


### [17] [Improving Code Generation via Small Language Model-as-a-judge](https://arxiv.org/abs/2602.11911)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 本文研究使用小型语言模型作为代码正确性判断器，发现现代SLMs在区分正确与错误代码实现方面优于现有方法，能以低成本达到与大型LLMs相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成方法中，大型语言模型对不常见或领域特定语言表现不佳，而开源模型需要数十亿参数才能匹配商业工具性能，成本高昂。现有RankEF方法未评估分类准确性，且使用相对陈旧的模型，无法确定该方法是否仍能帮助公司低成本训练自己的代码生成器。

Method: 训练多个最先进的小型语言模型作为代码正确性判断器，评估它们区分正确与错误代码实现的能力。将现代SLMs用作代码排序器，并与现有方法RankEF进行比较。

Result: 现代SLMs在代码正确性判断方面优于RankEF，即使不使用执行信息。作为代码排序器时，它们比RankEF获得更高的性能提升，并且以低成本达到比自身大5-25倍的大型LLMs的竞争性性能。

Conclusion: 小型语言模型可以作为有效的代码正确性判断器，以低成本实现与大型语言模型相当的性能，为公司在特定领域开发高效代码生成器提供了经济可行的解决方案。

Abstract: Large language models (LLMs) have shown remarkable capabilities in automated code generation. While effective for mainstream languages, they may underperform on less common or domain-specific languages, prompting companies to develop in-house code generators. While open-source models can be trained for this, only LLMs with tens of billions of parameters match the performance of commercial tools, demanding costly training and deployment. Recent work proposed supporting code generation with smaller models (SLMs) by generating multiple candidate solutions and using another SLM to select the most likely correct one. The most recent work in this area is the one by Sun et al. [29] presenting RankEF, a T5 model trained to rank code solutions using both execution-based and non-execution-based information. However, Sun et al. do not assess the T5 ranker's classification accuracy, that is, how often it misjudges correct implementations as incorrect or vice versa, leaving open questions about the reliability of LMs as code correctness judges for other tasks (e.g., automated code review). Moreover, their experiments involve relatively old models, making it unclear the extent to which such a methodology would still help companies in cheaply training their own code generators with performance comparable to those of massive LLMs. We present a study addressing these limitations. We train several state-of-the-art SLMs as code correctness judges and assess their ability to discriminate between correct and wrong implementations. We show that modern SLMs outperform RankEF, even without exploiting execution-based information. When used as code rankers, they achieve higher performance gains than RankEF and perform competitively with LLMs 5-25x larger, at a fraction of the cost.

</details>


### [18] [Studying Quality Improvements Recommended via Manual and Automated Code Review](https://arxiv.org/abs/2602.11925)
*Giuseppe Crupi,Rosalia Tufano,Gabriele Bavota*

Main category: cs.SE

TL;DR: 研究比较人类与ChatGPT-4在代码审查中的表现，发现AI能发现更多问题但仅能识别10%的人类发现的质量问题，两者具有互补性


<details>
  <summary>Details</summary>
Motivation: 虽然已有许多基于深度学习的代码审查自动化技术，但尚不清楚这些方法能否像人类审查者一样推荐质量改进。本研究旨在探究人类与AI代码审查的相似性和差异

Method: 通过挖掘研究收集240个PR中的739条人类评论，手动分类质量改进类型；然后让ChatGPT审查相同PR，比较两者推荐的质量改进

Result: ChatGPT平均推荐代码变更数量是人类的2.4倍，但仅能发现10%的人类报告的质量问题；约40%的AI额外评论指向有意义的质问题

Conclusion: 人类与AI代码审查具有互补性，当前AI只能作为人类审查的额外质量检查，不能替代人类审查或节省审查时间，因为人类仍需验证AI报告的问题

Abstract: Several Deep Learning (DL)-based techniques have been proposed to automate code review. Still, it is unclear the extent to which these approaches can recommend quality improvements as a human reviewer. We study the similarities and differences between code reviews performed by humans and those automatically generated by DL models, using ChatGPT-4 as representative of the latter. In particular, we run a mining-based study in which we collect and manually inspect 739 comments posted by human reviewers to suggest code changes in 240 PRs. The manual inspection aims at classifying the type of quality improvement recommended by human reviewers (e.g., rename variable/constant). Then, we ask ChatGPT to perform a code review on the same PRs and we compare the quality improvements it recommends against those suggested by the human reviewers. We show that while, on average, ChatGPT tends to recommend a higher number of code changes as compared to human reviewers (~2.4x more), it can only spot 10% of the quality issues reported by humans. However, ~40% of the additional comments generated by the LLM point to meaningful quality issues. In short, our findings show the complementarity of manual and AI-based code review. This finding suggests that, in its current state, DL-based code review can be used as a further quality check on top of the one performed by humans, but should not be considered as a valid alternative to them nor as a mean to save code review time, since human reviewers would still need to perform their manual inspection while also validating the quality issues reported by the DL-based technique.

</details>


### [19] [Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?](https://arxiv.org/abs/2602.11988)
*Thibaud Gloaguen,Niels Mündler,Mark Müller,Veselin Raychev,Martin Vechev*

Main category: cs.SE

TL;DR: 研究发现：为代码生成代理提供上下文文件（如AGENTS.md）反而会降低任务成功率并增加20%以上推理成本，建议人类编写的上下文文件应仅描述最小需求。


<details>
  <summary>Details</summary>
Motivation: 软件开发中普遍存在为代码生成代理定制上下文文件（如AGENTS.md）的做法，虽然代理开发者强烈推荐，但缺乏对这些上下文文件在实际任务中有效性的严谨研究。

Method: 在两个互补设置中评估代码代理的任务完成性能：1) 使用LLM生成的上下文文件评估SWE-bench基准任务；2) 使用包含开发者提交的上下文文件的新问题集合进行评估。

Result: 上下文文件相比不提供仓库上下文会降低任务成功率，同时增加20%以上推理成本。上下文文件鼓励更广泛的探索（如更彻底的测试和文件遍历），代理倾向于遵循其指令。

Conclusion: 上下文文件中的不必要要求会使任务变得更难，人类编写的上下文文件应仅描述最小需求，避免过度约束。

Abstract: A widespread practice in software development is to tailor coding agents to repositories using context files, such as AGENTS.md, by either manually or automatically generating them. Although this practice is strongly encouraged by agent developers, there is currently no rigorous investigation into whether such context files are actually effective for real-world tasks. In this work, we study this question and evaluate coding agents' task completion performance in two complementary settings: established SWE-bench tasks from popular repositories, with LLM-generated context files following agent-developer recommendations, and a novel collection of issues from repositories containing developer-committed context files.
  Across multiple coding agents and LLMs, we find that context files tend to reduce task success rates compared to providing no repository context, while also increasing inference cost by over 20%. Behaviorally, both LLM-generated and developer-provided context files encourage broader exploration (e.g., more thorough testing and file traversal), and coding agents tend to respect their instructions. Ultimately, we conclude that unnecessary requirements from context files make tasks harder, and human-written context files should describe only minimal requirements.

</details>


### [20] [An Empirical Study of the Imbalance Issue in Software Vulnerability Detection](https://arxiv.org/abs/2602.12038)
*Yuejun Guo,Qiang Hu,Qiang Tang,Yves Le Traon*

Main category: cs.SE

TL;DR: 该研究验证了代码漏洞检测中数据不平衡问题是导致深度学习模型性能不稳定的核心原因，并通过实证研究发现现有不平衡解决方案在不同数据集和评估指标上表现各异，但没有一个能在所有指标上都表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在自动化漏洞检测方面前景广阔，但现有模型在不同数据集上表现不稳定。研究者推测这可能源于数据不平衡问题（漏洞代码数量极少），需要验证这一假设并探索现有不平衡解决方案在漏洞检测中的表现。

Method: 使用9个开源数据集和2个最先进的深度学习模型进行全面的实证研究，验证数据不平衡问题的影响，并测试多种现有不平衡解决方案（如Focal loss、mean false error、class-balanced loss、随机过采样等）在漏洞检测任务中的表现。

Result: 研究证实了数据不平衡是导致模型性能不稳定的核心原因。现有不平衡解决方案在不同数据集和评估指标上表现各异：Focal loss更适合提高精确率，mean false error和class-balanced loss能提升召回率，随机过采样有助于改善F1分数，但没有一个解决方案能在所有指标上都表现出色。

Conclusion: 数据不平衡确实是漏洞检测中深度学习模型性能不稳定的关键因素。现有不平衡解决方案各有侧重但都不全面，需要进一步探索外部影响因素并开发新的解决方案来全面提升漏洞检测性能。

Abstract: Vulnerability detection is crucial to protect software security. Nowadays, deep learning (DL) is the most promising technique to automate this detection task, leveraging its superior ability to extract patterns and representations within extensive code volumes. Despite its promise, DL-based vulnerability detection remains in its early stages, with model performance exhibiting variability across datasets. Drawing insights from other well-explored application areas like computer vision, we conjecture that the imbalance issue (the number of vulnerable code is extremely small) is at the core of the phenomenon. To validate this, we conduct a comprehensive empirical study involving nine open-source datasets and two state-of-the-art DL models. The results confirm our conjecture. We also obtain insightful findings on how existing imbalance solutions perform in vulnerability detection. It turns out that these solutions perform differently as well across datasets and evaluation metrics. Specifically: 1) Focal loss is more suitable to improve the precision, 2) mean false error and class-balanced loss encourages the recall, and 3) random over-sampling facilitates the F1-measure. However, none of them excels across all metrics. To delve deeper, we explore external influences on these solutions and offer insights for developing new solutions.

</details>


### [21] [ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair](https://arxiv.org/abs/2602.12058)
*Zhiyong Chen,Jialun Cao,Chang Xu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: ModelWisdom是一个交互式环境，通过可视化技术和大型语言模型增强TLA+模型检查的可解释性，提供可视化、图优化、模型摘要和模型修复功能。


<details>
  <summary>Details</summary>
Motivation: TLA+模型检查虽然提供强正确性保证，但实践中存在解释反例困难、理解大型状态转移图困难、修复错误模型困难等问题。现有TLA+工具箱的可视化工具功能有限，缺乏折叠、颜色高亮和语义解释，限制了可扩展性和可解释性。

Method: 开发ModelWisdom交互环境，包含四个核心功能：1) 模型可视化（颜色化违规高亮、点击链接从转换到TLA+代码、违规状态与属性映射）；2) 图优化（基于树的结构化、节点/边折叠）；3) 模型摘要（通过LLM总结解释子图、预处理和部分解释）；4) 模型修复（提取错误信息、支持迭代调试）。

Result: ModelWisdom将原始模型检查器输出转化为交互式、可解释的工作流，提高了对复杂TLA+规范的理解，减少了调试工作量。提供了网站和演示视频。

Conclusion: ModelWisdom通过可视化技术和LLM增强了TLA+模型检查的可解释性和可操作性，将原始输出转化为交互式工作流，显著改善了理解和调试体验。

Abstract: Model checking in TLA+ provides strong correctness guarantees, yet practitioners continue to face significant challenges in interpreting counterexamples, understanding large state-transition graphs, and repairing faulty models. These difficulties stem from the limited explainability of raw model-checker output and the substantial manual effort required to trace violations back to source specifications. Although the TLA+ Toolbox includes a state diagram viewer, it offers only a static, fully expanded graph without folding, color highlighting, or semantic explanations, which limits its scalability and interpretability. We present ModelWisdom, an interactive environment that uses visualization and large language models to make TLA+ model checking more interpretable and actionable. ModelWisdom offers: (i) Model Visualization, with colorized violation highlighting, click-through links from transitions to TLA+ code, and mapping between violating states and broken properties; (ii) Graph Optimization, including tree-based structuring and node/edge folding to manage large models; (iii) Model Digest, which summarizes and explains subgraphs via large language models (LLMs) and performs preprocessing and partial explanations; and (iv) Model Repair, which extracts error information and supports iterative debugging. Together, these capabilities turn raw model-checker output into an interactive, explainable workflow, improving understanding and reducing debugging effort for nontrivial TLA+ specifications. The website to ModelWisdom is available: https://model-wisdom.pages.dev. A demonstrative video can be found at https://www.youtube.com/watch?v=plyZo30VShA.

</details>


### [22] [Performance Antipatterns: Angel or Devil for Power Consumption?](https://arxiv.org/abs/2602.12079)
*Alessandro Aneggi,Vincenzo Stoico,Andrea Janes*

Main category: cs.SE

TL;DR: 研究验证了性能反模式不仅降低微服务响应速度，部分还会显著增加能耗，但并非所有性能问题都转化为能耗问题


<details>
  <summary>Details</summary>
Motivation: 虽然已知性能反模式会降低微服务系统响应性，但它们对能耗的影响尚未充分研究。本文旨在实证探究Smith和Williams定义的性能反模式是否也会对能耗产生负面影响

Method: 将10种反模式实现为独立微服务，在受控负载条件下评估，收集性能、CPU和DRAM功耗、资源利用率等同步测量数据，每种反模式进行30次重复运行

Result: 所有反模式都如预期降低了性能，但只有部分反模式在响应时间和功耗增加之间表现出统计学显著关系。一些反模式达到CPU饱和，限制了功耗增长；而另一些反模式（如不必要处理、The Ramp）显示出能效-性能耦合的低效特征

Conclusion: 虽然所有注入的性能反模式都会增加响应时间，但只有部分同时表现为明显的能耗反模式。研究为识别既是性能问题又是能耗问题的反模式提供了系统基础，并为设计更节能的微服务架构提供了可操作的见解

Abstract: Performance antipatterns are known to degrade the responsiveness of microservice-based systems, but their impact on energy consumption remains largely unexplored. This paper empirically investigates whether widely studied performance antipatterns defined by Smith and Williams also negatively influence power usage. We implement ten antipatterns as isolated microservices and evaluate them under controlled load conditions, collecting synchronized measurements of performance, CPU and DRAM power consumption, and resource utilization across 30 repeated runs per antipattern. The results show that while all antipatterns degrade performance as expected, only a subset exhibit a statistically significant relationship between response time and increased power consumption. Specifically, several antipatterns reach CPU saturation, capping power draw regardless of rising response time, whereas others (\eg Unnecessary Processing, The Ramp) demonstrate energy-performance coupling indicative of inefficiency. Our results show that, while all injected performance antipatterns increase response time as expected, only a subset also behaves as clear energy antipatterns, with several cases reaching a nearly constant CPU power level where additional slowdowns mainly translate into longer execution time rather than higher instantaneous power consumption. The study provides a systematic foundation for identifying performance antipatterns that also behave as energy antipatterns and offers actionable insights for designing more energy-efficient microservices architectures.

</details>


### [23] [PPTAM$η$: Energy Aware CI/CD Pipeline for Container Based Applications](https://arxiv.org/abs/2602.12081)
*Alessandro Aneggi,Xiaozhou Li,Andrea Janes*

Main category: cs.SE

TL;DR: PPTAMη：自动化流水线，集成能耗测量到GitLab CI中，用于容器化API系统，协调负载生成、容器监控和硬件功率探针，在每个提交收集可比较的指标。


<details>
  <summary>Details</summary>
Motivation: 现代基于容器的微服务通过快速部署周期演进，但CI/CD流水线很少测量能耗，尽管先前工作表明设计模式、代码异味和重构会影响能源效率。需要使能耗对开发者可见。

Method: PPTAMη自动化流水线集成功率和能耗测量到GitLab CI中，协调负载生成、容器监控和硬件功率探针，为每个提交收集可比较的指标。支持开发者查看能耗、测试工程师进行版本比较、研究人员进行趋势分析。

Result: 在JWT认证的API上评估PPTAMη，跨越四个提交收集性能和能耗指标，总结架构、测量方法和验证。

Conclusion: PPTAMη使能耗在CI/CD流水线中可见，支持版本比较和趋势分析，有助于提高容器化API系统的能源效率意识。

Abstract: Modern container-based microservices evolve through rapid deployment cycles, but CI/CD pipelines still rarely measure energy consumption, even though prior work shows that design patterns, code smells and refactorings affect energy efficiency. We present PPTAM$η$, an automated pipeline that integrates power and energy measurement into GitLab CI for containerised API systems, coordinating load generation, container monitoring and hardware power probes to collect comparable metrics at each commit. The pipeline makes energy visible to developers, supports version comparison for test engineers and enables trend analysis for researchers. We evaluate PPTAM$η$ on a JWT-authenticated API across four commits, collecting performance and energy metrics and summarising the architecture, measurement methodology and validation.

</details>


### [24] [On the Adoption of AI Coding Agents in Open-source Android and iOS Development](https://arxiv.org/abs/2602.12144)
*Muhammad Ahmad Khan,Hasnain Ali,Muneeb Rana,Muhammad Saqib Ilyas,Abdul Ali Bangash*

Main category: cs.SE

TL;DR: 首篇针对开源移动项目中AI代理生成代码的类别级实证研究，分析Android和iOS项目中2,901个AI编写的PR接受行为，发现Android项目接受更多AI PR且接受率更高，不同任务类别接受率差异显著。


<details>
  <summary>Details</summary>
Motivation: AI编码代理在软件开发中日益重要，但其对移动开发的影响缺乏实证研究。需要了解AI代理在不同移动平台、不同任务类别中的代码贡献接受情况，为设计平台感知的代理系统提供依据。

Method: 使用AIDev数据集中的2,901个AI编写的pull requests，分析193个已验证的Android和iOS开源GitHub仓库。研究跨移动平台、AI代理和任务类别的PR接受行为，包括接受率、解决时间等指标，并进行演化分析。

Result: Android项目接收的AI PR数量是iOS的2倍，且接受率更高（71% vs 63%）。常规任务（功能、修复、UI）PR接受率最高，重构和构建等结构性变更接受率较低且解决时间更长。Android项目的PR解决时间在2025年中前有所改善。

Conclusion: 这是首个基于证据的AI代理对开源移动项目影响的特征描述，为评估代理生成代码贡献建立了实证基准。研究结果为设计平台感知的代理系统提供了重要参考，表明平台差异和任务类型显著影响AI代码贡献的接受度。

Abstract: AI coding agents are increasingly contributing to software development, yet their impact on mobile development has received little empirical attention. In this paper, we present the first category-level empirical study of agent-generated code in open-source mobile app projects. We analyzed PR acceptance behaviors across mobile platforms, agents, and task categories using 2,901 AI-authored pull requests (PRs) in 193 verified Android and iOS open-source GitHub repositories in the AIDev dataset. We find that Android projects have received 2x more AI-authored PRs and have achieved higher PR acceptance rate (71%) than iOS (63%), with significant agent-level variation on Android. Across task categories, PRs with routine tasks (feature, fix, and ui) achieve the highest acceptance, while structural changes like refactor and build achieve lower success and longer resolution times. Furthermore, our evolution analysis shows improvement in PR resolution time on Android through mid-2025 before it declined again. Our findings offer the first evidence-based characterization of AI agents effects on OSS mobile projects and establish empirical baselines for evaluating agent-generated contributions to design platform aware agentic systems.

</details>


### [25] [Automated Test Suite Enhancement Using Large Language Models with Few-shot Prompting](https://arxiv.org/abs/2602.12256)
*Alex Chudic,Gül Çalıklı*

Main category: cs.SE

TL;DR: 本文实证研究不同测试样例来源（人工、SBST、LLM）对LLM生成单元测试质量的影响，发现人工编写的样例能产生最佳覆盖率和正确性，且基于问题描述和代码相似性选择样例效果最好。


<details>
  <summary>Details</summary>
Motivation: 单元测试对代码功能验证至关重要，但手动编写耗时费力。传统工具（如SBST）生成的测试缺乏可读性和实用性，而LLM在零样本测试生成方面已有研究，但其少样本学习潜力尚未充分探索。当前代码库中混合了人工、LLM和传统工具生成的测试，需要评估不同来源样例对LLM生成测试质量的影响。

Method: 使用GPT-4o在HumanEval和ClassEval数据集上进行实验，研究不同测试样例来源（人工、SBST、LLM）对少样本提示效果的影响。评估基于检索的方法选择相关样例，包括基于问题描述和代码相似性的组合方法。不仅评估正确性和覆盖率，还评估可读性、认知复杂度和可维护性。

Result: LLM通过少样本提示能生成高质量测试，其中人工编写的样例产生最佳覆盖率和正确性。基于问题描述和代码相似性组合选择样例的方法能持续产生最有效的少样本提示。测试质量评估包括多个维度：正确性、覆盖率、可读性、认知复杂度和可维护性。

Conclusion: 少样本提示能有效提升LLM生成单元测试的质量，人工编写的样例是最佳来源。基于问题描述和代码相似性选择样例的检索方法效果最好。这对混合人机代码库中测试套件的改进具有实际意义。

Abstract: Unit testing is essential for verifying the functional correctness of code modules (e.g., classes, methods), but manually writing unit tests is often labor-intensive and time-consuming. Unit tests generated by tools that employ traditional approaches, such as search-based software testing (SBST), lack readability, naturalness, and practical usability. LLMs have recently provided promising results and become integral to developers' daily practices. Consequently, software repositories now include a mix of human-written tests, LLM-generated tests, and those from tools employing traditional approaches such as SBST. While LLMs' zero-shot capabilities have been widely studied, their few-shot learning potential for unit test generation remains underexplored. Few-shot prompting enables LLMs to learn from examples in the prompt, and automatically retrieving such examples could enhance test suites. This paper empirically investigates how few-shot prompting with different test artifact sources, comprising human, SBST, or LLM, affects the quality of LLM-generated unit tests as program comprehension artifacts and their contribution to improving existing test suites by evaluating not only correctness and coverage but also readability, cognitive complexity, and maintainability in hybrid human-AI codebases. We conducted experiments on HumanEval and ClassEval datasets using GPT-4o, which is integrated into GitHub Copilot and widely used among developers. We also assessed retrieval-based methods for selecting relevant examples. Our results show that LLMs can generate high-quality tests via few-shot prompting, with human-written examples producing the best coverage and correctness. Additionally, selecting examples based on the combined similarity of problem description and code consistently yields the most effective few-shot prompts.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [26] [Filtered Approximate Nearest Neighbor Search in Vector Databases: System Design and Performance Analysis](https://arxiv.org/abs/2602.11443)
*Abylay Amanbayev,Brian Tsan,Tri Dang,Florin Rusu*

Main category: cs.DB

TL;DR: 系统化评估向量数据库中过滤策略性能，提出新数据集和GLS相关性指标，发现算法适配比原始索引性能更重要，为混合搜索提供实用指南


<details>
  <summary>Details</summary>
Motivation: 虽然检索增强生成(RAG)应用越来越依赖过滤近似最近邻搜索(FANNS)，但缺乏对通用过滤策略在向量数据库中性能的系统理解

Method: 系统化过滤策略分类学，在FAISS、Milvus和pgvector中评估集成效果，提出新数据集MoReVec和GLS相关性指标，扩展ANN-Benchmarks支持过滤向量搜索

Result: 发现算法适配比原始索引性能更重要：Milvus通过混合近似/精确执行实现更好的召回稳定性；pgvector的基于成本的查询优化器经常选择次优执行计划；分区索引(IVFFlat)在低选择性查询中优于图索引(HNSW)

Conclusion: 为混合搜索工作负载提供了选择索引类型和配置查询优化器的实用指南，扩展的基准测试工具可用于进一步研究

Abstract: Retrieval-Augmented Generation (RAG) applications increasingly rely on Filtered Approximate Nearest Neighbor Search (FANNS) to combine semantic retrieval with metadata constraints. While algorithmic innovations for FANNS have been proposed, there remains a lack of understanding regarding how generic filtering strategies perform within Vector Databases. In this work, we systematize the taxonomy of filtering strategies and evaluate their integration into FAISS, Milvus, and pgvector. To provide a robust benchmarking framework, we introduce a new relational dataset, \textit{MoReVec}, consisting of two tables, featuring 768-dimensional text embeddings and a rich schema of metadata attributes. We further propose the \textit{Global-Local Selectivity (GLS)} correlation metric to quantify the relationship between filters and query vectors.
  Our experiments reveal that algorithmic adaptations within the engine often override raw index performance. Specifically, we find that: (1) \textit{Milvus} achieves superior recall stability through hybrid approximate/exact execution; (2) \textit{pgvector}'s cost-based query optimizer frequently selects suboptimal execution plans, favoring approximate index scans even when exact sequential scans would yield perfect recall at comparable latency; and (3) partition-based indexes (IVFFlat) outperform graph-based indexes (HNSW) for low-selectivity queries. To facilitate this analysis, we extend the widely-used \textit{ANN-Benchmarks} to support filtered vector search and make it available online. Finally, we synthesize our findings into a set of practical guidelines for selecting index types and configuring query optimizers for hybrid search workloads.

</details>


### [27] [Fast Tuning the Index Construction Parameters of Proximity Graphs in Vector Databases](https://arxiv.org/abs/2602.11573)
*Wenyang Zhou,Jiadong Xie,Yingfan Liu,Zhihao Yin,Jeffrey Xu Yu,Hui Li,Zhangqian Mu,Xiaotian Qiao,Jiangtao Cui*

Main category: cs.DB

TL;DR: FastPGT：一种高效调优近邻图构建参数的框架，通过同时构建多个图减少重复计算，相比现有方法VDTuner实现最高2.37倍加速


<details>
  <summary>Details</summary>
Motivation: 高维向量空间中的k-近似最近邻搜索在向量数据库和检索增强生成中至关重要。近邻图方法虽为当前最优，但其构建参数对搜索性能影响显著，而参数调优过程需要反复构建和测试图索引，计算成本高昂。现有方法未针对这一过程进行优化。

Method: 提出FastPGT框架：1）通过同时构建多个近邻图来加速参数评估，减少重复计算；2）修改现有最优调优模型，使其能一次性推荐多个参数，便于利用同时构建多图的方法进行高效评估。

Result: 在真实数据集上的实验表明，FastPGT相比当前最优方法VDTuner实现了最高2.37倍的加速，且不牺牲调优质量。

Conclusion: FastPGT有效解决了近邻图构建参数调优中的计算瓶颈问题，通过同时构建多个图的方法显著提升了调优效率，为高维向量搜索的实际应用提供了更高效的参数优化方案。

Abstract: k-approximate nearest neighbor search (k-ANNS) in high-dimensional vector spaces is a fundamental problem across many fields. With the advent of vector databases and retrieval-augmented generation, k-ANNS has garnered increasing attention. Among existing methods, proximity graphs (PG) based approaches are the state-of-the-art (SOTA) methods. However, the construction parameters of PGs significantly impact their search performance. Before constructing a PG for a given dataset, it is essential to tune these parameters, which first recommends a set of promising parameters and then estimates the quality of each parameter by building the corresponding PG and then testing its k-ANNS performance. Given that the construction complexity of PGs is superlinear, building and evaluating graph indexes accounts for the primary cost of parameter tuning. Unfortunately, there is currently no method considered and optimized this process.In this paper, we introduce FastPGT, an efficient framework for tuning the PG construction parameters. FastPGT accelerates parameter estimation by building multiple PGs simultaneously, thereby reducing repeated computations. Moreover, we modify the SOTA tuning model to recommend multiple parameters at once, which can be efficiently estimated using our method of building multiple PGs simultaneously. Through extensive experiments on real-world datasets, we demonstrate that FastPGT achieves up to 2.37x speedup over the SOTA method VDTuner, without compromising tuning quality.

</details>


### [28] [Towards a theory of Façade-X data access: satisfiability of SPARQL basic graph patterns](https://arxiv.org/abs/2602.11756)
*Luigi Asprino,Enrico Daga*

Main category: cs.DB

TL;DR: 本文研究Façade-X数据访问方法中基本图模式的满足性问题，提出理论框架和算法，并通过实验验证可行性。


<details>
  <summary>Details</summary>
Motivation: 知识图谱数据集成需要处理多种格式的数据源，Façade-X方法虽然能提供统一访问，但并非所有SPARQL查询都能在Façade-X图上得到解。需要研究基本图模式的满足性问题来支持更高效的数据集成系统。

Method: 提出Façade-X的巩固理论框架，研究基本图模式在Façade-X数据源上的满足性问题，设计相应的判定算法，并通过概念验证实现进行实验验证。

Result: 建立了Façade-X基本图模式满足性的理论框架，开发了判定算法，实验证明该方法在实际应用中可行，包括处理真实世界查询。

Conclusion: 研究成果为研究Façade-X数据访问的SPARQL查询执行策略奠定了基础，有助于开发更高效的知识图谱数据集成系统。

Abstract: Data integration is the primary use case for knowledge graphs. However, integrated data are not typically graphs but come in different formats, for example, CSV, XML, or a relational database. Façade-X is a recently proposed method for providing direct access to an open-ended set of data formats. The method includes a meta-model that specialises RDF to fit general data structures. This model allows to express SPARQL queries targeting data sources with those structures. Previous work formalised Façade-X and demonstrated how it can theoretically represent any format expressible with a context-free grammar, as well as the relational model. A reference implementation, SPARQL Anything, demonstrates the feasibility of the approach in practice. It is noteworthy that Façade-X utilises a fraction of RDF, and, consequently, not all SPARQL queries yield a solution (i.e. are satisfiable) when evaluated over a Façade-X graph. In this article, we consolidate Façade-X, and we study the satisfiability of basic graph patterns. The theory is accompanied by an algorithm for deciding the satisfiability of basic graph patterns on Façade-X data sources. Furthermore, we provide extensive experiments with a proof-of-concept implementation, demonstrating practical feasibility, including with real-world queries. Our results pave the way for studying query execution strategies for Façade-X data access with SPARQL and supporting developers to build more efficient data integration systems for knowledge graphs.

</details>


### [29] [Data-Driven Trajectory Imputation for Vessel Mobility Analysis](https://arxiv.org/abs/2602.11890)
*Giannis Spiliopoulos,Alexandros Troupiotis-Kapeliaris,Kostas Patroumpas,Nikolaos Liapis,Dimitrios Skoutas,Dimitris Zissis,Nikos Bikakis*

Main category: cs.DB

TL;DR: HABIT：基于H3聚合的轻量级可配置船舶轨迹插补框架，利用历史AIS数据提取和分析船舶运动模式来填补轨迹缺失段


<details>
  <summary>Details</summary>
Motivation: AIS数据中存在大量轨迹缺失段，严重影响数据质量和分析准确性。现有插补方法主要针对车辆轨迹设计，未充分考虑船舶特有的运动模式（如平滑转弯、港口机动、恶劣天气航行等）

Method: 提出HABIT框架，基于H3地理网格系统，从历史AIS数据中提取、分析和索引船舶运动模式，通过数据驱动方式填补缺失轨迹段。框架轻量级且可配置

Result: 在不同时间段、数据密度和船舶类型的AIS数据上进行实证研究，HABIT在准确性上与基线方法相当，在延迟性能方面表现更好，同时考虑了船舶特性和运动模式

Conclusion: HABIT为船舶轨迹插补提供了有效的解决方案，能够处理AIS数据中的缺失段，在保持准确性的同时提升计算效率，适用于海事监控、安全、物流等多种应用

Abstract: Modeling vessel activity at sea is critical for a wide range of applications, including route planning, transportation logistics, maritime safety, and environmental monitoring. Over the past two decades, the Automatic Identification System (AIS) has enabled real-time monitoring of hundreds of thousands of vessels, generating huge amounts of data daily. One major challenge in using AIS data is the presence of large gaps in vessel trajectories, often caused by coverage limitations or intentional transmission interruptions. These gaps can significantly degrade data quality, resulting in inaccurate or incomplete analysis. State-of-the-art imputation approaches have mainly been devised to tackle gaps in vehicle trajectories, even when the underlying road network is not considered. But the motion patterns of sailing vessels differ substantially, e.g., smooth turns, maneuvering near ports, or navigating in adverse weather conditions. In this application paper, we propose HABIT, a lightweight, configurable H3 Aggregation-Based Imputation framework for vessel Trajectories. This data-driven framework provides a valuable means to impute missing trajectory segments by extracting, analyzing, and indexing motion patterns from historical AIS data. Our empirical study over AIS data across various timeframes, densities, and vessel types reveals that HABIT produces maritime trajectory imputations performing comparably to baseline methods in terms of accuracy, while performing better in terms of latency while accounting for vessel characteristics and their motion patterns.

</details>


### [30] [Designing and Comparing RPQ Semantics](https://arxiv.org/abs/2602.11949)
*Victor Marsault,Antoine Meyer*

Main category: cs.DB

TL;DR: 该论文提出了一个框架来分类和比较属性图数据库查询语言中的正则路径查询语义，分析了不同语义的数学特性，并提出了新的语义设计思路。


<details>
  <summary>Details</summary>
Motivation: 现代属性图数据库查询语言（如Cypher、PGQL、GSQL、GQL）虽然基于正则路径查询，但为了显式输出路径，它们偏离了经典的同态语义，导致查询可能匹配无限多条路径。这些语言使用不同的临时标准来选择有限子集（如Cypher使用轨迹语义，PGQL/GSQL使用最短路径语义），缺乏统一的理论框架来理解和比较这些语义。

Method: 作者提出了一个形式化框架，将RPQ语义视为从数据库和查询到有限路径集合的数学函数。他们定义了一系列可能的性质，并分析这些性质之间的兼容性（哪些性质互斥或不可同时满足）。通过这个框架，他们提出了几个新的RPQ语义作为示例。

Result: 研究表明某些语义性质是相互排斥的，有些性质无法同时满足。作者提出的新RPQ语义为未来图数据库查询语言的语义设计提供了新的思路和可能性。

Conclusion: 该框架为理解和比较不同RPQ语义提供了理论基础，有助于更好地选择、设计和评估图数据库查询语言的语义。提出的新语义可能为未来语言设计提供灵感，推动该领域向更系统化的方向发展。

Abstract: Modern property graph database query languages such as Cypher, PGQL, GSQL, and the standard GQL draw inspiration from the formalism of regular path queries (RPQs). In order to output walks explicitly, they depart from the classical and well-studied homomorphism semantics. However, it then becomes difficult to present results to users because RPQs may match infinitely many walks. The aforementioned languages use ad-hoc criteria to select a finite subset of those matches. For instance, Cypher uses trail semantics, discarding walks with repeated edges; PGQL and GSQL use shortest walk semantics, retaining only the walks of minimal length among all matched walks; and GQL allows users to choose from several semantics. Even though there is academic research on these semantics, it focuses almost exclusively on evaluation efficiency.
  In an attempt to better understand, choose and design RPQ semantics, we present a framework to categorize and compare them according to other criteria. We formalize several possible properties, pertaining to the study of RPQ semantics seen as mathematical functions mapping a database and a query to a finite set of walks. We show that some properties are mutually exclusive, or cannot be met. We also give several new RPQ semantics as examples. Some of them may provide ideas for the design of new semantics for future graph database query languages.

</details>


### [31] [DIVER: A Robust Text-to-SQL System with Dynamic Interactive Value Linking and Evidence Reasoning](https://arxiv.org/abs/2602.12064)
*Yafeng Nan,Haifeng Sun,Zirui Zhuang,Qi Qi,Guojun Chu,Jianxin Liao,Dan Pei,Jingyu Wang*

Main category: cs.DB

TL;DR: DIVER是一个通过动态交互式值链接自动进行证据推理的鲁棒Text-to-SQL系统，无需专家协助即可处理大规模动态数据库值，显著提升现有模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA Text-to-SQL模型严重依赖专家编写的证据来澄清模式和值链接，在现实场景中缺乏专家协助时性能会严重下降（执行准确率下降超过10%），这源于用户查询的模糊性和大规模动态数据库值的复杂性。

Method: DIVER使用包含多样化工具的兼容工具箱探测数据库，在结构化工作空间（CoTF：思维链和事实链）的约束下，基于探测结果进行反思并选择新工具进行下一轮探测。通过这种自动迭代过程识别现有方法遗漏的模式和值链接，从而推断SQL函数和公式的正确用法并生成高质量证据。

Result: 1) DIVER系统显著增强了各种Text-to-SQL模型的鲁棒性，在执行准确率（EX）上提升高达10.82%，在有效效率分数（VES）上提升高达16.09%。2) 动态交互式值链接显著提高了现有系统的鲁棒性和模式与值链接的准确性，特别是在面对大规模动态数据库值的挑战时。

Conclusion: DIVER通过自动化的动态交互式值链接实现了无需专家协助的鲁棒Text-to-SQL，解决了现有方法在现实场景中因缺乏专家证据而性能崩溃的问题，为处理大规模动态数据库值提供了有效的解决方案。

Abstract: In the era of large language models, Text-to-SQL, as a natural language interface for databases, is playing an increasingly important role. The sota Text-to-SQL models have achieved impressive accuracy, but their performance critically relies on expert-written evidence, which typically clarifies schema and value linking that existing models struggle to identify. Such limitations stem from the ambiguity of user queries and, more importantly, the complexity of comprehending large-scale and dynamic database values. Consequently, in real-world scenarios where expert assistance is unavailable, existing methods suffer a severe performance collapse, with execution accuracy dropping by over 10%. This underscores their lack of robustness. To address this, we propose DIVER, a robust system that automates evidence reasoning with dynamic interactive value linking. It leverages a compatible toolbox containing diverse tools to probe the database. Then, restricted by a structured workspace (CoTF, Chain of Thoughts and Facts), it reflects based on probe results and selects a new tool for next round of probing. Through this automatically iterative process, DIVER identifies schema and value linking missed by existing methods. Based on these accurate linkings, DIVER is able to infer correct usage of SQL functions and formulas and generate high-quality evidence, achieving robust Text-to-SQL without expert assistance. Extensive experiments demonstrate that: 1) The DIVER system significantly enhances the robustness of various Text-to-SQL models, improving performance by up to 10.82% in Execution Accuracy (EX) and 16.09% in Valid Efficiency Score (VES). 2) Our dynamic interactive value linking significantly improves the robustness of existing systems and the accuracy of schema and value linking, especially when confronted with challenges posed by large-scale, dynamic database values.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [32] [Real Life Is Uncertain. Consensus Should Be Too!](https://arxiv.org/abs/2602.11362)
*Reginald Frank,Soujanya Ponnapalli,Octavio Lomeli,Neil Giridharan,Marcos K Aguilera,Natacha Crooks*

Main category: cs.DC

TL;DR: 该论文主张用概率性故障模型替代传统的f阈值故障模型，以更真实地反映实际故障情况，并优化共识协议的成本和性能。


<details>
  <summary>Details</summary>
Motivation: 传统共识协议基于f阈值故障模型（最多f台机器故障），但该模型过度简化了现实世界，限制了在成本或性能方面的优化机会。实际故障具有复杂性和细微差别，需要更精确的模型来捕捉。

Method: 提出概率性故障模型，利用个体机器的"故障曲线"来更精确地描述故障行为。该方法允许绕过传统瓶颈（如多数仲裁交集），探索更灵活的共识机制。

Result: 概率性共识协议能够更准确地反映实际故障模式，为系统设计提供更多优化空间，有望实现更可靠、高效、经济且可持续的分布式系统。

Conclusion: 概率性故障模型比传统阈值模型更能反映现实故障情况，为共识协议设计开辟了新方向，能够构建更适应实际环境的分布式系统。

Abstract: Modern distributed systems rely on consensus protocols to build a fault-tolerant-core upon which they can build applications. Consensus protocols are correct under a specific failure model, where up to $f$ machines can fail. We argue that this $f$-threshold failure model oversimplifies the real world and limits potential opportunities to optimize for cost or performance. We argue instead for a probabilistic failure model that captures the complex and nuanced nature of faults observed in practice. Probabilistic consensus protocols can explicitly leverage individual machine \textit{failure curves} and explore side-stepping traditional bottlenecks such as majority quorum intersection, enabling systems that are more reliable, efficient, cost-effective, and sustainable.

</details>


### [33] [RL over Commodity Networks: Overcoming the Bandwidth Barrier with Lossless Sparse Deltas](https://arxiv.org/abs/2602.11456)
*Chaoyi Ruan,Geng Luo,Xinyi Wan,Long Zhao,Qinghe Wang,Jiaan Zhu,Duling Xu,Guanbin Xu,Dehui Wei,Xiang Liu,Cheng Li,Haifeng Sun,Congcong Miao,Jialin Li*

Main category: cs.DC

TL;DR: SparrowRL：针对松散耦合GPU集群的稀疏增量RL训练系统，通过仅传输1%变化的参数，在普通网络环境下实现接近RDMA集群的性能


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练需要频繁同步大模型参数，依赖昂贵的RDMA HPC集群。普通以太网/WAN连接无法承受全权重广播（8B模型同步需100+秒），限制了RL训练的普及性

Method: 基于RL微调产生稀疏更新的观察（仅1%参数变化），设计稀疏增量检查点表示，将增量提取与多流传输流水线化，传输与生成重叠，结合吞吐量和带宽感知调度及租约容错

Result: 在4B-14B Qwen3模型跨4个地理区域部署中，SparrowRL将8B模型每步传输负载减少79倍，WAN上吞吐量提升2.4-9.5倍，与理想RDMA单数据中心基线的差距缩小到8.91%

Conclusion: SparrowRL使RL训练在普通网络松散耦合GPU集群上可行，通过按需跨云GPU提供比预留RDMA集群高1.21-1.59倍的每美元token数，降低了RL训练基础设施成本

Abstract: LLM post-training with reinforcement learning (RL) requires frequent synchronization of large model parameters between the trainer and distributed rollout actors. High-throughput RL post-training therefore relies on dedicated RDMA HPC clusters, an infrastructure cost most organizations cannot absorb. A natural alternative is to aggregate loosely-coupled GPUs over standard Ethernet and WAN links, but this commodity connectivity cannot sustain full-weight broadcasts: synchronizing an 8B model can take over 100~seconds on bandwidth-limited links, while rollout generation typically takes tens of seconds.
  Toward making RL practical in this regime, we observe that RL fine-tuning yields highly sparse per-step updates, with only around 1\% of parameter elements changing. Atop this insight, we present SparrowRL, a novel high-performance RL training system that preserves bit-exact updates without dropping or quantizing information, designed for commodity-networked, loosely-coupled GPU resources. SparrowRL represents each step as a sparse delta checkpoint, pipelines delta extraction with multi-stream transmission, overlaps transfer with rollout generation, and coordinates heterogeneous workers with throughput- and bandwidth-aware scheduling plus lease-based fault tolerance. On Qwen3 models from 4B to 14B deployed across up to four geographic regions, SparrowRL reduces per-step transfer payload by 79$\times$ for Qwen3-8B and improves throughput by 2.4--9.5$\times$ over full-weight broadcast across WAN, narrowing the throughput gap relative to an ideal RDMA single-datacenter baseline to within 8.91\%. By leveraging on-demand, cross-cloud GPUs over commodity links, SparrowRL delivers 1.21--1.59$\times$ higher tokens per dollar than reserved RDMA clusters at comparable throughput.

</details>


### [34] [Differentially Private Perturbed Push-Sum Protocol and Its Application in Non-Convex Optimization](https://arxiv.org/abs/2602.11544)
*Yiming Zhou,Kaiping Xue,Enhong Chen*

Main category: cs.DC

TL;DR: 提出DPPS协议级差分隐私方案和PartPSP非凸优化算法，通过敏感度估计和部分通信机制平衡隐私与效用


<details>
  <summary>Details</summary>
Motivation: 现有去中心化网络隐私保护方法多为特定下游任务设计，缺乏通用协议级解决方案，且隐私保护会影响优化性能

Method: 1) DPPS协议：轻量级差分隐私通信协议，引入敏感度估计机制，每轮只需广播一个标量；2) PartPSP算法：将模型参数分为本地和共享两部分，仅对共享参数应用DPPS，降低噪声注入量

Result: 理论证明PartPSP在非凸目标下收敛，部分通信机制在相同隐私预算下获得更好优化性能；实验验证DPPS隐私保护有效性，PartPSP优于现有隐私保护去中心化优化算法

Conclusion: DPPS提供通用协议级隐私保护方案，PartPSP通过部分通信机制平衡隐私与效用，为去中心化网络提供高效隐私保护框架

Abstract: In decentralized networks, nodes cannot ensure that their shared information will be securely preserved by their neighbors, making privacy vulnerable to inference by curious nodes. Adding calibrated random noise before communication to satisfy differential privacy offers a proven defense; however, most existing methods are tailored to specific downstream tasks and lack a general, protocol-level privacy-preserving solution. To bridge this gap, we propose Differentially Private Perturbed Push-Sum (DPPS), a lightweight differential privacy protocol for decentralized communication. Since protocol-level differential privacy introduces the unique challenge of obtaining the sensitivity for each communication round, DPPS introduces a novel sensitivity estimation mechanism that requires each node to compute and broadcast only one scalar per round, enabling rigorous differential privacy guarantees. This design allows DPPS to serve as a plug-and-play, low-cost privacy-preserving solution for downstream applications built on it. To provide a concrete instantiation of DPPS and better balance the privacy-utility trade-off, we design PartPSP, a privacy-preserving decentralized algorithm for non-convex optimization that integrates a partial communication mechanism. By partitioning model parameters into local and shared components and applying DPPS only to the shared parameters, PartPSP reduces the dimensionality of consensus data, thereby lowering the magnitude of injected noise and improving optimization performance. We theoretically prove that PartPSP converges under non-convex objectives and, with partial communication, achieves better optimization performance under the same privacy budget. Experimental results validate the effectiveness of DPPS's privacy-preserving and demonstrate that PartPSP outperforms existing privacy-preserving decentralized optimization algorithms.

</details>


### [35] [LAER-MoE: Load-Adaptive Expert Re-layout for Efficient Mixture-of-Experts Training](https://arxiv.org/abs/2602.11686)
*Xinyi Liu,Yujie Wang,Fangcheng Fu,Xuefeng Xiao,Huixia Li,Jiashi Li,Bin Cui*

Main category: cs.DC

TL;DR: LAER-MoE提出了一种新的MoE训练框架，通过完全分片专家并行(FSEP)和负载均衡规划器解决专家并行训练中的负载不均衡问题，实现了最高1.69倍的训练加速。


<details>
  <summary>Details</summary>
Motivation: 专家并行训练中，动态路由导致专家间负载严重不均衡，少数过载专家成为训练瓶颈，限制了MoE模型的训练效率。

Method: 提出完全分片专家并行(FSEP)范式，将每个专家参数完全分片到所有设备，通过All-to-All通信在训练期间按专家粒度恢复部分专家参数，实现专家参数的灵活重布局；采用细粒度通信调度减少开销，并开发负载均衡规划器制定专家重布局策略和token路由方案。

Result: 在A100集群上的实验表明，LAER-MoE相比当前最先进的训练系统实现了最高1.69倍的加速。

Conclusion: LAER-MoE通过创新的完全分片专家并行范式和负载均衡规划，有效解决了MoE训练中的负载不均衡问题，显著提升了训练效率。

Abstract: Expert parallelism is vital for effectively training Mixture-of-Experts (MoE) models, enabling different devices to host distinct experts, with each device processing different input data. However, during expert parallel training, dynamic routing results in significant load imbalance among experts: a handful of overloaded experts hinder overall iteration, emerging as a training bottleneck.
  In this paper, we introduce LAER-MoE, an efficient MoE training framework. The core of LAER-MoE is a novel parallel paradigm, Fully Sharded Expert Parallel (FSEP), which fully partitions each expert parameter by the number of devices and restores partial experts at expert granularity through All-to-All communication during training. This allows for flexible re-layout of expert parameters during training to enhance load balancing. In particular, we perform fine-grained scheduling of communication operations to minimize communication overhead. Additionally, we develop a load balancing planner to formulate re-layout strategies of experts and routing schemes for tokens during training. We perform experiments on an A100 cluster, and the results indicate that our system achieves up to 1.69x acceleration compared to the current state-of-the-art training systems. Source code available at https://github.com/PKU-DAIR/Hetu-Galvatron/tree/laer-moe.

</details>


### [36] [Designing Scalable Rate Limiting Systems: Algorithms, Architecture, and Distributed Solutions](https://arxiv.org/abs/2602.11741)
*Bo Guan*

Main category: cs.DC

TL;DR: 本文提出了一种基于Redis Sorted Set的分布式限流系统架构，通过Rolling Window算法在准确性和内存成本之间取得平衡，采用三层架构管理限流规则，并在Redis Cluster上部署以实现可用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 设计一个同时具备准确性、可用性和可扩展性的限流器是分布式系统的基本挑战，主要源于算法精度、可用性、一致性和分区容忍性之间的权衡。需要找到一种能在生产环境中实际应用的解决方案。

Method: 采用Redis内存缓存数据库及其Sorted Set数据结构，提供O(log(N))时间复杂度操作；实现Rolling Window限流算法；使用服务器端Lua脚本将清理、计数和插入捆绑为原子操作；设计三层架构管理限流规则；在Redis Cluster上部署，通过数据分片和复制提供可用性和可扩展性。

Result: 量化了Rolling Window算法相对于Token Bucket和Fixed Window算法在准确性和内存成本之间的权衡；通过Lua脚本消除了并发环境中的竞态条件；通过规则参数哈希实现规则变更而无需修改缓存脚本；接受CAP定理中的AP（可用性和分区容忍性）作为实际工程权衡。

Conclusion: 提出的分布式限流系统架构在生产环境中是可行的，通过Redis Sorted Set和Rolling Window算法在效率、低延迟和精度之间取得平衡，采用三层架构和Redis Cluster部署满足了可用性和可扩展性需求，接受AP权衡是实际工程中的合理选择。

Abstract: Designing a rate limiter that is simultaneously accurate, available, and scalable presents a fundamental challenge in distributed systems, primarily due to the trade-offs between algorithmic precision, availability, consistency, and partition tolerance. This article presents a concrete architecture for a distributed rate limiting system in a production-grade environment. Our design chooses the in-memory cache database, the Redis, along with its Sorted Set data structure, which provides $O(log (N))$ time complexity operation for the key-value pair dataset with efficiency and low latency, and maintains precision. The core contribution is quantifying the accuracy and memory cost trade-off of the chosen Rolling Window as the implemented rate limiting algorithm against the Token Bucket and Fixed Window algorithms. In addition, we explain how server-side Lua scripting is critical to bundling cleanup, counting, and insertion into a single atomic operation, thereby eliminating race conditions in concurrent environments. In the system architecture, we propose a three-layer architecture that manages the storage and updating of the limit rules. Through script load by hashing the rule parameters, rules can be changed without modifying the cached scripts. Furthermore, we analyze the deployment of this architecture on a Redis Cluster, which provides the availability and scalability by data sharding and replication. We explain the acceptance of AP (Availability and Partition Tolerance) from the CAP theorem as the pragmatic engineering trade-off for this use case.

</details>


### [37] [An Auction-Based Mechanism for Optimal Task Allocation and Resource Aware Containerization](https://arxiv.org/abs/2602.11998)
*Ramakant kumar*

Main category: cs.DC

TL;DR: 提出AUC-RAC拍卖机制，通过Docker Swarm连接多个本地服务器，优化物联网设备计算任务卸载和分配


<details>
  <summary>Details</summary>
Motivation: 分布式计算使多个计算设备能够协作执行资源密集型任务，在物联网环境中并行执行任务至关重要。然而，在基于云的物联网容器化中，管理资源和优化成本仍然是成功执行任务的挑战。

Method: 提出AUC-RAC拍卖机制，利用Docker Swarm连接多个本地服务器（管理节点和工作节点），使用Docker容器化同时执行任务。物联网设备将任务发送给管理节点，管理节点将任务详情发送给所有工作节点参与拍卖竞标过程，基于资源充足性优化计算任务分配。

Result: 实验分析表明，该方法通过实现本地服务器之间的协作，为物联网设备提供了改进的卸载和计算密集型服务。

Conclusion: AUC-RAC拍卖机制能够有效优化物联网环境中计算任务的卸载和分配，提高资源利用效率和服务质量。

Abstract: Distributed computing has enabled cooperation between multiple computing devices for the simultaneous execution of resource-hungry tasks. Such execution also plays a pivotal role in the parallel execution of numerous tasks in the Internet of Things (IoT) environment. Leveraging the computing resources of multiple devices, the offloading and processing of computationintensive tasks can be carried out more efficiently. However, managing resources and optimizing costs remain challenging for successfully executing tasks in cloud-based containerization for IoT. This paper proposes AUC-RAC, an auction-based mechanism for efficient offloading of computation tasks among multiple local servers in the context of IoT devices. The approach leverages the concept of Docker swarm, which connects multiple local servers in the form of Manager Node (MN) and Worker Nodes (WNs). It uses Docker containerization to execute tasks simultaneously. In this system, IoT devices send tasks to the MN, which then sends the task details to all its WNs to participate in the auction-based bidding process. The auctionbased bidding process optimizes the allocation of computation tasks among multiple systems, considering their resource sufficiency. The experimental analysis establishes that the approach offers improved offloading and computation-intensive services for IoT devices by enabling cooperation between local servers.

</details>


### [38] [Contention Resolution, With and Without a Global Clock](https://arxiv.org/abs/2602.12070)
*Zixi Cai,Kuowen Chen,Shengquan Du,Tsvi Kopelowitz,Seth Pettie,Ben Plosk*

Main category: cs.DC

TL;DR: 本文研究了全局时钟与本地时钟假设下的竞争解决协议，展示了全局时钟带来的算法优势，分析了期望延迟与高概率延迟之间的复杂度差距，并证明了无法同时优化两种延迟指标。


<details>
  <summary>Details</summary>
Motivation: 竞争解决是分布式计算中的经典问题，传统研究假设各方只能访问本地时钟（自唤醒以来的时间）。本文的动机是探索全局时钟假设，这在技术上更现实且算法上更有趣，能够丰富问题并引入新技术。

Method: 设计了新的竞争解决协议，分析了全局时钟与本地时钟假设下的性能差异。研究了期望延迟与高概率延迟两种度量标准下的复杂度差距，并证明了同时优化两种指标的不可行性。

Result: 1) 设计了全局时钟下的新协议，延迟为O(n(log log n)^{1+o(1)})，比本地时钟协议有约log n的复杂度优势；2) 发现期望延迟与高概率延迟之间存在log n因子差距；3) 证明无法同时优化两种延迟指标。

Conclusion: 全局时钟假设为竞争解决问题带来了算法优势，期望延迟与高概率延迟之间存在固有的复杂度差距，且无法同时优化两种指标，这为分布式协议设计提供了新的理论见解。

Abstract: In the Contention Resolution problem $n$ parties each wish to have exclusive use of a shared resource for one unit of time. The problem has been studied since the early 1970s, under a variety of assumptions on feedback given to the parties, how the parties wake up, knowledge of $n$, and so on. The most consistent assumption is that parties do not have access to a global clock, only their local time since wake-up. This is surprising because the assumption of a global clock is both technologically realistic and algorithmically interesting. It enriches the problem, and opens the door to entirely new techniques. Our primary results are: [1] We design a new Contention Resolution protocol that guarantees latency $$O\left(\left(n\log\log n\log^{(3)} n\log^{(4)} n\cdots \log^{(\log^* n)} n\right)\cdot 2^{\log^* n}\right) \le n(\log\log n)^{1+o(1)}$$ in expectation and with high probability. This already establishes at least a roughly $\log n$ complexity gap between randomized protocols in GlobalClock and LocalClock. [2] Prior analyses of randomized ContentionResolution protocols in LocalClock guaranteed a certain latency with high probability, i.e., with probability $1-1/\text{poly}(n)$. We observe that it is just as natural to measure expected latency, and prove a $\log n$-factor complexity gap between the two objectives for memoryless protocols. The In-Expectation complexity is $Θ(n \log n/\log\log n)$ whereas the With-High-Probability latency is $Θ(n\log^2 n/\log\log n)$. Three of these four upper and lower bounds are new. [3] Given the complexity separation above, one would naturally want a ContentionResolution protocol that is optimal under both the In-Expectation and With-High-Probability metrics. This is impossible! It is even impossible to achieve In-Expectation latency $o(n\log^2 n/(\log\log n)^2)$ and With-High-Probability latency $n\log^{O(1)} n$ simultaneously.

</details>


### [39] [OServe: Accelerating LLM Serving via Spatial-Temporal Workload Orchestration](https://arxiv.org/abs/2602.12151)
*Youhe Jiang,Fangcheng Fu,Taiyi Wang,Guoliang He,Eiko Yoneki*

Main category: cs.DC

TL;DR: OServe是一个针对大语言模型服务的系统，通过异构灵活模型部署解决工作负载的时空异质性问题，相比现有系统性能提升最高2倍。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统通常假设工作负载在空间上均匀、时间上稳定，采用同质静态模型部署，但实际工作负载存在显著的时空异质性（空间上请求计算内存需求各异，时间上工作负载组成动态变化），导致性能次优。

Method: 1. 引入新颖的工作负载感知调度算法，根据实时工作负载特征优化异构模型部署；2. 提出高效的工作负载自适应切换方法，根据预测的工作负载变化迁移模型部署。

Result: 在真实世界跟踪实验中，OServe相比最先进的服务系统性能提升最高2倍（平均1.5倍）。

Conclusion: OServe通过异构灵活模型部署有效解决了LLM服务中的时空异质性问题，显著提升了服务性能。

Abstract: Serving Large Language Models (LLMs) can benefit immensely from parallelizing both the model and input requests across multiple devices, but incoming workloads exhibit substantial spatial and temporal heterogeneity. Spatially, workloads comprise heterogeneous requests with varying compute and memory demands. Temporally, workload composition varies over time. Nevertheless, existing systems typically assume spatially uniform and temporally stable workloads, employing a homogeneous, static model deployment. This mismatch between the assumption and real-world spatial-temporal heterogeneity results in suboptimal performance. We present OServe, an LLM serving system with heterogeneous and flexible model deployment that addresses both spatial and temporal heterogeneity. First, OServe introduces a novel workload-aware scheduling algorithm that optimizes heterogeneous model deployments according to real-time workload characteristics. Second, OServe proposes an efficient workload-adaptive switching method that migrates model deployments in response to predicted workload changes. Experiments on real-world traces show that OServe improves performance by up to 2$\times$ (average: 1.5$\times$) compared to state-of-the-art serving systems.

</details>
