<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://arxiv.org/abs/2512.19758)
*Wang Bin,Ao Yang,Kedan Li,Aofan Liu,Hui Li,Guibo Luo,Weixiang Huang,Yan Zhuang*

Main category: cs.SE

TL;DR: 提出了一种新的注意力距离度量方法，利用大语言模型分析代码上下文关系，替代传统物理距离，显著提升定向灰盒模糊测试的效率。


<details>
  <summary>Details</summary>
Motivation: 现有定向灰盒模糊测试方法仅测量种子执行路径与目标位置之间的物理距离，忽略了代码段之间的逻辑关系，在复杂二进制文件中可能导致冗余或误导性指导，削弱了实际效果。

Method: 引入注意力距离这一新度量标准，利用大语言模型进行上下文分析，计算代码元素之间的注意力分数，揭示它们的内在联系。在相同的AFLGo配置下，仅将物理距离替换为注意力距离。

Result: 在38个真实漏洞复现实验中，注意力距离比传统方法平均提升3.43倍测试效率；相比最先进的定向模糊测试工具DAFL和WindRanger，分别提升2.89倍和7.13倍。将注意力距离集成到DAFL和WindRanger中也持续提升了它们的原始性能。

Conclusion: 注意力距离通过捕捉代码逻辑关系，有效解决了传统物理距离度量的局限性，显著提升了定向模糊测试的效率和效果，具有良好的通用性和实际应用价值。

Abstract: In the domain of software security testing, Directed Grey-Box Fuzzing (DGF) has garnered widespread attention for its efficient target localization and excellent detection performance. However, existing approaches measure only the physical distance between seed execution paths and target locations, overlooking logical relationships among code segments. This omission can yield redundant or misleading guidance in complex binaries, weakening DGF's real-world effectiveness. To address this, we introduce \textbf{attention distance}, a novel metric that leverages a large language model's contextual analysis to compute attention scores between code elements and reveal their intrinsic connections. Under the same AFLGo configuration -- without altering any fuzzing components other than the distance metric -- replacing physical distances with attention distances across 38 real vulnerability reproduction experiments delivers a \textbf{3.43$\times$} average increase in testing efficiency over the traditional method. Compared to state-of-the-art directed fuzzers DAFL and WindRanger, our approach achieves \textbf{2.89$\times$} and \textbf{7.13$\times$} improvements, respectively. To further validate the generalizability of attention distance, we integrate it into DAFL and WindRanger, where it also consistently enhances their original performance. All related code and datasets are publicly available at https://github.com/TheBinKing/Attention\_Distance.git.

</details>


### [2] [A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows](https://arxiv.org/abs/2512.19769)
*Ivan Daunis*

Main category: cs.SE

TL;DR: 提出声明式LLM代理系统，将工作流规范与实现分离，支持多语言后端和部署环境，显著减少开发时间和部署速度


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统将代理逻辑与特定编程语言和部署模型紧密耦合，导致开发复杂、部署困难，需要一种更灵活、可移植的解决方案

Method: 采用声明式系统，通过统一DSL表达常见代理工作流模式（数据序列化、过滤、RAG检索、API编排），将代理开发从应用程序编程转变为配置管理

Result: 在PayPal实际电商工作流中评估，处理每日数百万次交互：开发时间减少60%，部署速度提升3倍，非工程师可安全修改代理行为，编排开销低于100ms

Conclusion: 声明式DSL方法能有效解耦代理工作流规范与实现，大幅提升开发效率和部署灵活性，使复杂工作流表达更简洁，支持A/B测试和多环境部署

Abstract: Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises).
  Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison.
  We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.

</details>


### [3] [Larger Is Not Always Better: Leveraging Structured Code Diffs for Comment Inconsistency Detection](https://arxiv.org/abs/2512.19883)
*Phong Nguyen,Anh M. T. Bui,Phuong T. Nguyen*

Main category: cs.SE

TL;DR: 基于CodeT5+的即时代码-注释不一致性检测方法，通过将代码变更分解为有序的修改活动序列，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 代码与注释之间的语义一致性对程序理解、调试和维护至关重要。现有方法通常忽略代码演进的结构复杂性，且存在隐私和资源挑战。

Method: 提出基于CodeT5+的即时CCI检测方法，将代码变更分解为替换、删除、添加等有序修改活动序列，以更好地捕捉代码变更与过时注释之间的关联。

Result: 在JITDATA和CCIBENCH基准数据集上的实验表明，该方法比现有最优模型提升13.54%的F1分数，比微调LLMs（DeepSeek-Coder、CodeLlama、Qwen2.5-Coder）提升4.18%-10.94%。

Conclusion: 通过建模代码变更的结构复杂性，基于CodeT5+的即时CCI检测方法能有效识别代码-注释不一致问题，优于现有方法。

Abstract: Ensuring semantic consistency between source code and its accompanying comments is crucial for program comprehension, effective debugging, and long-term maintainability. Comment inconsistency arises when developers modify code but neglect to update the corresponding comments, potentially misleading future maintainers and introducing errors. Recent approaches to code-comment inconsistency (CCI) detection leverage Large Language Models (LLMs) and rely on capturing the semantic relationship between code changes and outdated comments. However, they often ignore the structural complexity of code evolution, including historical change activities, and introduce privacy and resource challenges. In this paper, we propose a Just-In-Time CCI detection approach built upon the CodeT5+ backbone. Our method decomposes code changes into ordered sequences of modification activities such as replacing, deleting, and adding to more effectively capture the correlation between these changes and the corresponding outdated comments. Extensive experiments conducted on publicly available benchmark datasets-JITDATA and CCIBENCH--demonstrate that our proposed approach outperforms recent state-of-the-art models by up to 13.54% in F1-Score and achieves an improvement ranging from 4.18% to 10.94% over fine-tuned LLMs including DeepSeek-Coder, CodeLlama and Qwen2.5-Coder.

</details>


### [4] [Neuron-Guided Interpretation of Code LLMs: Where, Why, and How?](https://arxiv.org/abs/2512.19980)
*Zhe Yin,Xiaodong Gu,Beijun Shen*

Main category: cs.SE

TL;DR: 论文研究了代码大语言模型的神经元级可解释性，发现语言特异性神经元和概念层，并展示了在代码生成、克隆检测和代码摘要等任务中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有NLP的神经元解释技术不适用于源代码，因为编程语言具有形式化、层次化和可执行的特点。代码LLMs的内部可解释性研究不足，需要专门的分析方法。

Method: 实证研究Llama-3.1-8B和Qwen2.5-Coder-32B在多语言代码输入（C++、Java、Python、Go、JavaScript）上的神经元选择性，测量神经元选择性和层间贡献，识别语言特异性神经元和概念层。

Result: 发现：(1) 存在专门处理特定语言的神经元和通用的生成神经元；(2) 低层主要编码语言特定语法，中层捕获跨语言的语义抽象（概念层）。在代码生成、克隆检测和代码摘要任务中应用这些发现都获得了性能提升。

Conclusion: 代码LLMs具有语言特异性神经元和跨语言概念层，这些发现可用于改进代码生成、克隆检测和代码摘要等任务，在多语言环境中获得一致性能提升。

Abstract: Code language models excel on code intelligence tasks, yet their internal interpretability is underexplored. Existing neuron interpretability techniques from NLP are suboptimal for source code due to programming languages formal, hierarchical, and executable nature. We empirically investigate code LLMs at the neuron level, localizing language-specific neurons (selectively responsive to one language) and concept layers (feed-forward layers encoding language-agnostic code representations). We analyze Llama-3.1-8B and Qwen2.5-Coder-32B on multilingual inputs in C++, Java, Python, Go, and JavaScript, measuring neuron selectivity and layerwise contributions during generation. We find (1) neurons specialized for individual languages alongside a universal subset supporting general-purpose generation; and (2) lower layers mainly encode language-specific syntax, while middle layers capture semantic abstractions shared across languages, emerging as concept layers. We demonstrate utility on three tasks: neuron-guided fine-tuning for code generation, clone detection via concept-layer embeddings, and concept-layer-guided transfer for code summarization, each yielding consistent gains in multilingual settings.

</details>


### [5] [Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing](https://arxiv.org/abs/2512.20083)
*Wenzhao Wu,Yahui Tang,Mingfei Cheng,Wenbing Tang,Yuan Zhou,Yang Liu*

Main category: cs.SE

TL;DR: 提出了NoD-DGMT框架，用于检测具身智能体任务规划中的非最优决策问题，通过多样性引导的蜕变测试发现智能体完成任务但效率低下的情况。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法主要关注功能正确性，忽视了生成计划的非功能性最优性，这可能导致显著的性能下降和资源浪费。需要解决智能体成功完成任务但效率低下的非最优决策问题。

Method: 提出了NoD-DGMT框架，采用多样性引导的蜕变测试方法。设计了四种新颖的蜕变关系来捕捉基本的最优性属性：位置绕行次优性、动作最优性完备性、条件细化单调性和场景扰动不变性。引入多样性引导的选择策略，主动选择探索不同违规类别的测试用例。

Result: 在AI2-THOR模拟器上对四种最先进的规划模型进行实验，NoD-DGMT平均违规检测率达到31.9%，多样性引导过滤器将检测率提高了4.3%，多样性得分平均提高了3.3。显著优于六种基线方法，相对最佳基线提高了16.8%，在不同模型架构和任务复杂度上表现一致优越。

Conclusion: NoD-DGMT框架有效解决了具身智能体任务规划中的非最优决策检测问题，通过多样性引导的蜕变测试方法能够高效发现效率低下的规划行为，为资源受限应用中的智能体优化提供了重要工具。

Abstract: As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.

</details>


### [6] [AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration](https://arxiv.org/abs/2512.20159)
*Ruiqi Wang,Xinchen Wang,Cuiyun Gao,Chun Yong Chong,Xin Xia,Qing Liao*

Main category: cs.SE

TL;DR: AXIOM是一个基于扰动的代码评估基准合成框架，通过规则引导的扰动和多源质量校准，生成具有平衡分数分布的代码评估数据集，解决现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有代码评估基准存在严重局限性：一些使用粗粒度的二元标签，将丰富的代码行为简化为单一比特信息；另一些提出细粒度但主观、定义模糊的评估标准，导致人工标注分数不可靠；此外，它们通常使用不受控制的数据合成方法，导致分数分布不平衡，不能很好地代表真实世界的代码生成场景。

Method: AXIOM采用两阶段框架：1) 规则引导的扰动：提示LLMs应用预定义的扰动规则序列来修改现有高质量程序的功能和代码质量，精确控制每个程序的目标分数以实现平衡的分数分布；2) 多源质量校准：首先选择子集...

Result: 论文提出了AXIOM框架，但摘要中未提供具体的实验结果数据。

Conclusion: AXIOM通过扰动方法合成具有平衡分数分布的代码评估基准，解决了现有基准的局限性，为可靠评估代码评估能力提供了更好的数据集。

Abstract: Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.
  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...

</details>


### [7] [Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair](https://arxiv.org/abs/2512.20203)
*Zhenlei Ye,Xiaobing Sun,Sicong Cao,Lili Bo,Bin Li*

Main category: cs.SE

TL;DR: 提出了一种名为\sysname的LLM-based漏洞修复方法，该方法不仅关注修复内容，还提供需要修复的位置信息，并通过质量评估改进迭代修复策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的漏洞修复方法存在两个主要限制：1) 忽略需要修复的位置信息，只关注修复内容；2) 在迭代过程中缺乏对生成候选补丁的质量评估。

Method: \sysname方法首先提供需要修复的位置信息，然后通过两个维度评估补丁质量：是否引入新漏洞和污点语句覆盖率。在迭代修复过程中选择最佳补丁进行下一轮修复。

Result: 在VulnLoc+数据集（包含40个C/C++漏洞）上的实验表明，\sysname相比现有方法有显著改进：生成27个合理补丁，比基线多8-22个；修复了比现有方法多8-13个漏洞。

Conclusion: \sysname通过结合位置信息和补丁质量评估，显著提升了LLM-based漏洞修复的效果，在真实世界漏洞修复任务中表现出优越性能。

Abstract: The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.
  To tackle the two limitations, we propose \sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.

</details>


### [8] [Toward Explaining Large Language Models in Software Engineering Tasks](https://arxiv.org/abs/2512.20328)
*Antonio Vitale,Khai-Nguyen Nguyen,Denys Poshyvanyk,Rocco Oliveto,Simone Scalabrino,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: FeatureSHAP：首个面向软件工程任务、完全自动化、模型无关的可解释性框架，基于Shapley值，通过系统输入扰动和任务特定相似性比较，为LLM输出提供高层次特征归因。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在软件工程任务自动化方面取得进展，但其黑盒特性阻碍了在高风险和安全关键领域的应用，这些领域需要可解释性和透明度来建立信任、问责和有效的人类监督。现有可解释AI方法缺乏与软件工程实践者推理方式一致的领域特定解释。

Method: 基于Shapley值的模型无关可解释性框架，通过系统输入扰动和任务特定相似性比较，将模型输出归因于高层次输入特征。兼容开源和专有LLMs，专门针对软件工程任务设计。

Result: 在代码生成和代码摘要两个双模态软件工程任务上评估，FeatureSHAP相比基线方法能更少关注不相关输入特征，产生保真度更高的解释。37名实践者参与的调查显示，FeatureSHAP能帮助实践者更好地解释模型输出并做出更明智的决策。

Conclusion: FeatureSHAP代表了软件工程实用可解释AI的重要一步，为LLMs在软件工程任务中提供透明度和可解释性，促进在高风险和安全关键领域的可信应用。

Abstract: Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.

</details>


### [9] [Comment Traps: How Defective Commented-out Code Augment Defects in AI-Assisted Code Generation](https://arxiv.org/abs/2512.20334)
*Yuan Huang,Yukang Zhou,Xiangping Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: AI编码助手（GitHub Copilot和Cursor）会受到注释代码中缺陷的影响，导致生成更多有缺陷的代码，即使有明确忽略指令也无法完全避免。


<details>
  <summary>Details</summary>
Motivation: 随着AI代码生成工具的普及，研究发现生成的代码可能存在缺陷。先前研究主要关注代码上下文对缺陷生成的影响，但忽略了注释代码中缺陷的影响。AI编码助手对注释代码的解读会影响其生成的代码质量。

Method: 评估GitHub Copilot和Cursor两种AI编码助手如何受到有缺陷注释代码的影响。通过实验分析注释代码中的缺陷如何影响生成的代码质量。

Result: 实验结果显示，上下文中有缺陷的注释代码会导致AI编码助手生成更多有缺陷的代码，最高可达58.17%。工具并非简单复制缺陷代码，而是主动推理完成不完整的缺陷模式，即使存在格式干扰也会继续生成缺陷代码。即使明确指示忽略有缺陷的注释代码，缺陷减少率也不超过21.84%。

Conclusion: AI编码助手对注释代码中缺陷的敏感性表明需要改进其鲁棒性和安全性措施，以减少缺陷代码的生成。

Abstract: With the rapid development of large language models in code generation, AI-powered editors such as GitHub Copilot and Cursor are revolutionizing software development practices. At the same time, studies have identified potential defects in the generated code. Previous research has predominantly examined how code context influences the generation of defective code, often overlooking the impact of defects within commented-out code (CO code). AI coding assistants' interpretation of CO code in prompts affects the code they generate.
  This study evaluates how AI coding assistants, GitHub Copilot and Cursor, are influenced by defective CO code. The experimental results show that defective CO code in the context causes AI coding assistants to generate more defective code, reaching up to 58.17 percent. Our findings further demonstrate that the tools do not simply copy the defective code from the context. Instead, they actively reason to complete incomplete defect patterns and continue to produce defective code despite distractions such as incorrect indentation or tags. Even with explicit instructions to ignore the defective CO code, the reduction in defects does not exceed 21.84 percent. These findings underscore the need for improved robustness and security measures in AI coding assistants.

</details>


### [10] [A Comprehensive Study of Bugs in Modern Distributed Deep Learning Systems](https://arxiv.org/abs/2512.20345)
*Xiaoxue Ma,Wanwei Zhan,Jiale Chen,Yishu Li,Jacky Keung,Federica Sarro*

Main category: cs.SE

TL;DR: 首次对专用分布式深度学习框架进行大规模实证分析，研究了849个真实问题，建立了症状-原因-修复的映射关系，发现45.1%的bug症状是分布式框架特有的。


<details>
  <summary>Details</summary>
Motivation: 深度学习处理海量数据时，单设备训练受计算和内存限制，分布式训练成为必要。虽然通用框架提供分布式能力，但需要大量手动工作，因此需要专用框架。然而，对这些专用框架的实践挑战缺乏系统研究。

Method: 对DeepSpeed、Megatron-LM和Colossal-AI三个专用分布式框架的849个真实问题进行分析，构建了包含34种bug症状、28种根本原因和6种修复模式的分类体系，并建立了症状、原因和修复之间的显式映射关系。

Result: 45.1%的bug症状是分布式框架特有的，其中设置失败、内存问题和性能异常最为普遍。95%的通信设置阶段问题只出现在分布式环境中。超过60%的问题可以通过版本和依赖管理、分布式功能、API和通信调优来解决。

Conclusion: 专用分布式框架存在独特的挑战，需要系统性的理解和解决方案。研究提供了可操作的启示，包括改进版本管理、优化通信设置、增强错误诊断工具等，以帮助开发者更好地使用这些框架。

Abstract: In today's data-driven era, deep learning is vital for processing massive datasets, yet single-device training is constrained by computational and memory limits. Distributed deep learning overcomes these challenges by leveraging multiple GPUs or machines in parallel. While general-purpose frameworks (e.g., TensorFlow and PyTorch) provide distributed capabilities, these are often add-on features that demand significant manual effort for advanced parallelism, underscoring the need for specialized frameworks. This study conducts the first large-scale empirical analysis of practitioner challenges in dedicated distributed frameworks. We examine 849 real-world issues from DeepSpeed, Megatron-LM, and Colossal-AI and construct a taxonomy of 34 bug symptoms, 28 root causes, and 6 fix patterns. Crucially, we establish explicit mappings between symptoms, causes, and fixes across distributed training stages, enabling a systematic understanding of how issues emerge and are resolved. Our results show that 45.1\% of bug symptoms are unique to distributed frameworks, with setup failures, memory issues, and performance anomalies being the most prevalent. Moreover, 95\% of issues in the communication setup stage occur exclusively in distributed contexts. We also find over 60\% of cases can be resolved through version and dependency management, and distributed feature, API, and communication tuning. Based on these findings, we provide actionable implications.

</details>


### [11] [Identifying Appropriately-Sized Services with Deep Reinforcement Learning](https://arxiv.org/abs/2512.20381)
*Syeda Tasnim Fabiha,Saad Shafiq,Wesley Klewerton Guez Assunção,Nenad Medvidović*

Main category: cs.SE

TL;DR: Rake是一种基于深度强化学习的服务分解技术，直接从实现工件中识别适当规模的服务，无需特定文档或项目人员，在模块化质量和业务能力对齐方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有服务分解方法通常依赖文档、项目人员访问或预先知道服务数量等假设，这些假设在许多现实场景中不成立。需要一种直接从实现工件识别适当规模服务的方法。

Method: 提出Rake技术，基于深度强化学习，利用系统文档和源代码在实现方法级别指导服务分解。支持可定制的目标函数，平衡模块化质量和业务能力对齐。

Result: 在四个开源遗留项目上测试，相比两种最先进技术，Rake平均实现7-14%更高的模块化质量和18-22%更强的业务能力对齐。

Conclusion: Rake有效解决了服务分解的挑战，无需特定假设，平衡了模块化质量和业务能力对齐。结果表明仅优化业务上下文可能降低紧耦合系统的分解质量，需要平衡目标。

Abstract: Service-based architecture (SBA) has gained attention in industry and academia as a means to modernize legacy systems. It refers to a design style that enables systems to be developed as suites of small, loosely coupled, and autonomous components (services) that encapsulate functionality and communicate via language-agnostic APIs. However, defining appropriately sized services that capture cohesive subsets of system functionality remains challenging. Existing work often relies on the availability of documentation, access to project personnel, or a priori knowledge of the target number of services, assumptions that do not hold in many real-world scenarios. Our work addresses these limitations using a deep reinforcement learning-based approach to identify appropriately sized services directly from implementation artifacts. We present Rake, a reinforcement learning-based technique that leverages available system documentation and source code to guide service decomposition at the level of implementation methods. Rake does not require specific documentation or access to project personnel and is language-agnostic. It also supports a customizable objective function that balances modularization quality and business capability alignment, i.e., the degree to which a service covers the targeted business capability. We applied Rake to four open-source legacy projects and compared it with two state-of-the-art techniques. On average, Rake achieved 7-14 percent higher modularization quality and 18-22 percent stronger business capability alignment. Our results further show that optimizing solely for business context can degrade decomposition quality in tightly coupled systems, highlighting the need for balanced objectives.

</details>


### [12] [SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization](https://arxiv.org/abs/2512.20482)
*Revanth Gangi Reddy,Ye Liu,Wenting Zhao,JaeHyeok Doo,Tarun Suresh,Daniel Lee,Caiming Xiong,Yingbo Zhou,Semih Yavuz,Shafiq Joty*

Main category: cs.SE

TL;DR: SweRank+框架结合SweRankMulti（跨语言代码排序工具）和SweRankAgent（代理搜索设置），通过多轮迭代推理改进多语言代码库中的问题定位性能


<details>
  <summary>Details</summary>
Motivation: 现有问题定位方法通常是Python中心的单次搜索，无法有效处理大规模多语言代码库中自然语言错误描述到相关函数的映射需求

Method: SweRankMulti包含代码嵌入检索器和列表式LLM重排序器，使用多语言问题定位数据集训练；SweRankAgent采用代理搜索循环，通过记忆缓冲进行多轮推理积累定位候选

Result: 在多语言问题定位基准测试中，SweRankMulti达到新的SOTA性能，SweRankAgent进一步提升了单次排序的定位效果

Conclusion: SweRank+框架通过结合跨语言代码排序和多轮代理推理，显著改进了多语言代码库中的问题定位能力

Abstract: Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [13] [Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform](https://arxiv.org/abs/2512.19842)
*Andrea Sordello,Marco Mellia,Idilio Drago,Rodolfo Valentim,Francesco Musumeci,Massimo Tornatore,Federico Cerutti,Martino Trevisan,Alessio Botta,Willen Borges Coelho*

Main category: cs.DC

TL;DR: Holoscope是一个轻量级云原生平台，用于简化分布式望远镜（被动）和蜜罐（主动）传感器的部署和管理，通过K3s和WireGuard提供安全连接和自动化节点管理


<details>
  <summary>Details</summary>
Motivation: 互联网攻击的复杂性和规模需要能够跨不同网络监控恶意流量的分布式、协作式观测站。现有解决方案在部署和管理分布式传感器方面存在挑战

Method: 基于K3s和WireGuard构建的云原生平台，采用模块化设计和基础设施即代码原则，支持动态传感器编排、自动化恢复和处理，提供安全连接和自动化节点加入功能

Result: 在欧洲和巴西的多个机构和云网络中成功部署和运行Holoscope，实现了对大规模攻击现象的统一可见性，同时保持了易集成性和安全合规性

Conclusion: Holoscope是一个有效的分布式攻击监测平台，能够在资源受限环境中提供弹性操作，简化了分布式传感器网络的部署和管理

Abstract: The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.

</details>


### [14] [UCCL-EP: Portable Expert-Parallel Communication](https://arxiv.org/abs/2512.19849)
*Ziming Mao,Yihan Zhang,Chihan Cui,Kaichao You,Zhongjie Chen,Zhiying Xu,Scott Shenker,Costin Raiciu,Yang Zhou,Ion Stoica*

Main category: cs.DC

TL;DR: UCCL-EP：一种可移植的专家并行通信系统，通过GPU-CPU控制通道替代GPU发起的RDMA，在异构GPU和NIC平台上实现高性能


<details>
  <summary>Details</summary>
Motivation: 现有专家并行通信系统（如DeepEP）虽然性能优秀，但缺乏跨异构GPU和NIC平台的移植性，这源于其架构需要GPU与NIC之间的紧密垂直集成

Method: 使用高吞吐量的GPU-CPU控制通道，将紧凑的令牌路由命令传输到多线程CPU代理，由代理代表GPU执行GPUDirect RDMA操作，并使用RDMA立即数据模拟各种排序语义

Result: 在EFA上，分发和组合吞吐量比现有最佳EP解决方案提升高达2.1倍；在NVIDIA平台上性能与DeepEP相当；在NVIDIA+EFA平台上SGLang令牌吞吐量提升40%；在16节点AMD+Broadcom平台上DeepSeek-V3训练吞吐量提升45%

Conclusion: UCCL-EP实现了跨异构硬件平台的高性能专家并行通信，解决了现有系统移植性差的问题，为MoE工作负载提供了可扩展的解决方案

Abstract: Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.
  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.

</details>


### [15] [An Adaptive Distributed Stencil Abstraction for GPUs](https://arxiv.org/abs/2512.19851)
*Aditya Bhosale,Laxmikant Kale*

Main category: cs.DC

TL;DR: 提出基于Charm++的自适应分布式抽象，用于多节点GPU上的模板计算，提供NumPy-like语法，支持动态资源弹性伸缩


<details>
  <summary>Details</summary>
Motivation: Python科学计算生态系统主要局限于单节点并行，NumPy原型与超级计算机高性能执行之间存在差距；硬件加速器普及和能效需求使资源适应性成为关键，但传统HPC抽象仍显僵化

Method: 使用基于自适应Charm++运行时的CharmTyles框架，构建自适应分布式抽象，提供熟悉的NumPy-like语法以减少从原型到生产代码的移植工作

Result: 展示了通过动态调整运行应用程序在不同节点数上的资源弹性；性能分析显示相关开销可控；相比专用高性能模板DSL和通用NumPy替代方案，该抽象实现了显著性能提升

Conclusion: 该自适应分布式抽象成功弥合了NumPy原型与多节点GPU高性能执行之间的差距，提供资源弹性和显著性能改进，同时保持熟悉的编程接口

Abstract: The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.

</details>


### [16] [Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions](https://arxiv.org/abs/2512.19972)
*Pengchao Han,Xi Huang,Yi Fang,Guojun Han*

Main category: cs.DC

TL;DR: 本文对协作学习中知识蒸馏（KD）的内存与知识机制进行全面综述，重点分析KD如何在不同协作模式下提取、存储和共享知识，并探讨任务异构性等挑战。


<details>
  <summary>Details</summary>
Motivation: 协作学习已成为大规模智能系统的关键范式，但知识蒸馏在协作学习中如何利用内存和知识的机制尚未充分探索。本文旨在填补这一空白，深入理解KD在协作学习中的知识提取、存储和共享机制。

Method: 提供协作学习中知识蒸馏的全面综述，定义并分类KD过程中的内存和知识，探索其相互关系。分析分布式、分层和去中心化等协作学习模式，重点研究任务异构性（包括联邦学习、多智能体域适应、联邦多模态学习等），并考虑模型、数据、资源异构性及隐私问题。

Result: 建立了KD在协作学习中内存和知识机制的系统性分析框架，对现有工作基于内存和知识处理方式进行分类，揭示了不同协作模式下KD的有效性如何受内存和知识动态影响。

Conclusion: 本文为协作学习中知识蒸馏的内存和知识机制提供了系统性的理解框架，指出了现有挑战并提出了未来研究方向，有助于推动协作学习领域KD技术的进一步发展。

Abstract: Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.

</details>


### [17] [Scaling Point-based Differentiable Rendering for Large-scale Reconstruction](https://arxiv.org/abs/2512.20017)
*Hexu Zhao,Xiaoteng Liu,Xiwen Min,Jianhao Huang,Youming Deng,Yanfei Li,Ang Li,Jinyang Li,Aurojit Panda*

Main category: cs.DC

TL;DR: Gaian是一个用于点基可微分渲染的通用分布式训练系统，通过统一API和优化数据局部性，显著减少通信开销并提高训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的点基可微分渲染分布式训练系统存在两个主要问题：1）与特定PBDR方法紧密耦合，缺乏通用性；2）数据局部性差导致严重的通信开销。这限制了PBDR在高分辨率和大规模场景中的应用。

Method: Gaian提供了一个统一的API，足够表达现有PBDR方法，同时暴露丰富的数据访问信息。系统利用这些信息优化数据局部性，减少通信开销。通过实现4种PBDR算法来验证系统。

Result: 在6个数据集和最多128个GPU的测试中，Gaian减少了高达91%的通信开销，训练吞吐量提高了1.50-3.71倍，实现了高性能和资源效率。

Conclusion: Gaian是一个通用的分布式PBDR训练系统，通过统一API和数据局部性优化，显著提升了训练效率和可扩展性，为高分辨率大规模3D场景重建提供了有效的解决方案。

Abstract: Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.

</details>


### [18] [FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling](https://arxiv.org/abs/2512.20064)
*Yaojian Chen,Si-Qiu Gong,Lin Gan,Yanfei Liu,An Yang,Yinuo Wang,Chao-yang Lu,Guangwen Yang*

Main category: cs.DC

TL;DR: Fast-MPS是一个用于可扩展MPS采样的多级并行框架，结合数据并行和tensor并行，在Gaussian Boson Sampling中实现了10倍加速，支持数千进程和超大规模MPS计算。


<details>
  <summary>Details</summary>
Motivation: 随着问题复杂度增加，MPS规模迅速增长，传统数据并行受限于内存和I/O，模型并行缺乏可扩展性，需要新的并行框架来处理大规模MPS采样。

Method: 提出Fast-MPS多级并行框架：1) 跨样本的数据并行；2) 沿bond维度的tensor并行；3) 通过压缩和重叠消除内存和I/O压力；4) 在大规模MPS采样中恢复数据并行。

Result: 在Gaussian Boson Sampling中实现超过10倍加速，扩展到数千个进程，支持8,176个site和bond维度chi=10^4的大规模模拟，显著超越现有技术水平。

Conclusion: Fast-MPS通过创新的多级并行设计成功解决了大规模MPS采样的可扩展性问题，在高性能tensor网络应用中展现出巨大潜力。

Abstract: Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.

</details>


### [19] [Population Protocols Revisited: Parity and Beyond](https://arxiv.org/abs/2512.20163)
*Leszek Gąsieniec,Tytus Grodzicki,Tomasz Jurdziński,Jakub Kowalski,Grzegorz Stachowiak*

Main category: cs.DC

TL;DR: 本文提出了一种新的种群协议计算范式，通过引入权重系统、鲁棒时钟机制和异常检测切换机制，首次实现了时间和空间高效的奇偶性和同余性计算协议。


<details>
  <summary>Details</summary>
Motivation: 近二十年来，种群协议研究取得了显著进展，但在Presburger算术框架中，对于奇偶性等同余性谓词，一直缺乏同时具备时间和空间效率的协议。这一空白成为该领域的重要挑战。

Method: 提出新的计算范式，整合种群权重系统、鲁棒时钟机制、高效异常检测与切换机制。该范式允许协议不完全优化，但更注重通用性、鲁棒性和概率保证，支持高效多阶段稳定种群协议的通用设计。

Result: 首次实现了高效的奇偶性和同余性协议，使用O(log³ n)状态，在O(log³ n)时间内实现静默稳定。权重系统还实现了单进制和二进制表示之间的隐式转换。

Conclusion: 新范式填补了种群协议中同余性谓词计算的时间-空间效率空白，提出的权重系统具有广泛应用前景，可用于计算和表示（子）种群规模等问题。

Abstract: For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.
  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\log^3 n)$ states and achieve silent stabilisation in $O(\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.

</details>


### [20] [SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication](https://arxiv.org/abs/2512.20178)
*Chen Zhuang,Lingqi Zhang,Benjamin Brock,Du Wu,Peng Chen,Toshio Endo,Satoshi Matsuoka,Mohamed Wahib*

Main category: cs.DC

TL;DR: 提出分布式稀疏矩阵乘法(SpMM)的通信优化框架，通过细粒度稀疏感知通信策略和分层通信策略，在128个GPU上实现高达221.5倍的加速。


<details>
  <summary>Details</summary>
Motivation: 分布式稀疏矩阵乘法(SpMM)是高性能计算和深度学习的基础操作，但现有实现存在显著的通信开销问题，限制了性能和可扩展性。

Method: 提出两层优化：1) 细粒度稀疏感知通信策略，利用稀疏矩阵模式减少通信开销；2) 分层通信策略，结合稀疏感知策略与GPU系统的两层网络架构，减少慢速链路上的冗余通信。

Result: 在真实数据集上评估，框架在128个GPU上展现出强可扩展性，相比四个先进基线(CAGNET、SPA、BCL、CoLa)分别实现221.5倍、56.0倍、23.4倍、8.8倍的几何平均加速。

Conclusion: 通过分析现有分布式SpMM实现中的通信低效问题，提出的稀疏感知和分层通信优化策略能显著减少通信开销，实现高性能和强可扩展的分布式稀疏矩阵乘法。

Abstract: Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\times$, 56.0$\times$, 23.4$\times$, and 8.8$\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.

</details>


### [21] [Reaching Agreement Among Reasoning LLM Agents](https://arxiv.org/abs/2512.20184)
*Chaoyi Ruan,Yiliang Wang,Ziji Shi,Jialin Li*

Main category: cs.DC

TL;DR: 提出Aegean共识协议解决多智能体推理中的协调问题，相比现有静态工作流降低延迟1.2-20倍，保持答案质量


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协调依赖静态启发式工作流（如固定循环限制和屏障同步），存在计算资源浪费、延迟高（因落后节点）、可能过早达成临时共识等问题。需要类似经典分布式共识问题的理论基础。

Method: 提出多智能体精炼问题的形式化模型，包括正确性保证定义和智能体推理的形式语义。设计Aegean共识协议用于随机推理智能体，实现Aegean-Serve共识感知服务引擎，通过并发智能体执行中的增量法定人数检测实现早期终止。

Result: 在四个数学推理基准测试中，Aegean提供可证明的安全性和活性保证，相比最先进基线降低延迟1.2-20倍，答案质量保持在2.5%以内。在本地GPU部署和商业API提供商上均获得一致增益。

Conclusion: 基于共识的协调消除了落后节点延迟而不牺牲正确性，为多智能体推理提供了可靠的形式化基础。

Abstract: Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.
  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.

</details>


### [22] [Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs](https://arxiv.org/abs/2512.20210)
*Yinan Ni,Xiao Yang,Yuqi Tang,Zhimin Qiu,Chen Wang,Tingzhou Yuan*

Main category: cs.DC

TL;DR: P-LoRA：一种面向LoRA微调LLM的服务器无感知推理系统，通过预测性适配器预加载和页面式内存管理，显著降低冷启动延迟并提升吞吐量


<details>
  <summary>Details</summary>
Motivation: 在服务器无感知环境中部署LoRA微调的LLM推理服务面临两大挑战：反应式适配器加载导致显著的冷启动延迟，以及频繁的适配器交换造成严重的GPU内存碎片化

Method: 提出P-LoRA系统，包含两个关键技术：1) 轻量级LSTM流量预测器，预测适配器需求并主动从主机内存预加载到GPU；2) 受操作系统虚拟内存启发的页面式适配器内存管理机制

Result: 实验结果显示，P-LoRA相比S-LoRA实现1.52倍吞吐量提升，在高并发场景下平均TTFT降低35%，冷启动延迟减少68%，GPU内存利用率保持在87%以上

Conclusion: P-LoRA通过预测性预加载和智能内存管理，有效解决了服务器无感知环境中LoRA微调LLM推理的冷启动和内存碎片化问题，显著提升了系统性能

Abstract: The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.

</details>


### [23] [Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults](https://arxiv.org/abs/2512.20394)
*Mohammad Walid Charrwi,Zaid Hussain*

Main category: cs.DC

TL;DR: 提出基于强化学习的故障感知路由方案，用于高斯互连网络，通过PPO代理学习绕过故障区域，显著提升故障环境下的包投递率


<details>
  <summary>Details</summary>
Motivation: 随着NoC和无线传感器网络规模扩大，网络拓扑对性能至关重要。高斯互连网络具有直径和对称性优势，但传统自适应路由技术对节点和链路故障敏感，特别是在高斯分布的节点故障（如热热点或物理损坏集群）下，确定性路由面临严重挑战

Method: 提出基于强化学习的故障感知路由方案，使用PPO（近端策略优化）代理，设计特定奖励结构惩罚接近故障区域的行为，系统动态学习绕过故障区域

Result: 实验结果表明，RL代理显著优于自适应最短路径路由算法：在40%故障密度下，RL的包投递率（PDR）达到0.95，而贪婪算法仅为0.66；在20%低网络负载下，RL为0.57，贪婪算法为0.43，RL在管理拥塞方面表现更优

Conclusion: 基于强化学习的故障感知路由方案在高斯互连网络中有效，特别适用于随机、易故障的拓扑环境，能够动态适应故障并维持高通信可靠性

Abstract: As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies

</details>


### [24] [WOC: Dual-Path Weighted Object Consensus Made Efficient](https://arxiv.org/abs/2512.20485)
*Tanisha Fonseca,Gengrui Zhang*

Main category: cs.DC

TL;DR: WOC提出了一种双路径共识协议，根据操作访问模式动态路由到快速路径（独立操作）或慢速路径（冲突操作），同时处理节点异构性和工作负载独立性。


<details>
  <summary>Details</summary>
Motivation: 现有共识协议要么优化节点异构性（如Cabinet），要么优化工作负载独立性（如EPaxos），但无法同时处理两者。需要一种能同时应对节点性能差异和操作并行性的协议。

Method: WOC采用双路径架构：1）快速路径处理独立操作，使用对象特定的加权仲裁，单次网络往返完成；2）慢速路径处理冲突或共享对象，采用节点加权共识并由领导者协调。根据访问模式动态路由操作。

Result: 评估显示，在独立对象比例超过70%的工作负载下，WOC比Cabinet吞吐量提高达4倍，同时在高争用场景下保持同等性能。

Conclusion: WOC成功解决了现有共识协议无法同时优化节点异构性和工作负载独立性的问题，通过双路径设计实现了更好的性能平衡。

Abstract: Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [25] [Risk-Aware GPU-Assisted Cardinality Estimation for Cost-Based Query Optimizers](https://arxiv.org/abs/2512.19750)
*Ilsun Chang*

Main category: cs.DB

TL;DR: GACE：基于GPU辅助的基数估计架构，通过风险门控检测不确定区间，仅在需要时调用GPU测量，提升查询优化器稳定性并降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 现实工作负载常违反静态统计假设，导致基数估计失效、决策不稳定和计划翻转率增加。传统方法无法有效处理统计过时、数据偏斜、连接相关性、绑定变量隐藏分布和采样偏差等问题。

Method: 提出GACE混合辅助架构：1）风险门控检测估计不确定性区间；2）GPU测量引擎执行高速探测；3）显式成本核算测量开销。仅在风险区间选择性调用GPU测量，保持稳定区域低开销。

Result: GACE在保持稳定区域低开销的同时，显著提升计划稳定性，减少问题场景下的尾延迟（P99）。通过硬件加速测量量化了开销和盈亏平衡点。

Conclusion: GACE成功地将GPU加速测量与查询优化器结合，通过选择性测量策略在保证低开销的同时解决了传统基数估计的局限性，为实际数据库系统提供了实用的性能改进方案。

Abstract: Cardinality estimation is a cornerstone of cost-based optimizers (CBOs), yet real-world workloads often violate the assumptions behind static statistics, degrading decision stability and increasing plan flip rates. We empirically characterize failures caused by stale statistics, skew, join correlations, hidden distributions in bind variables, and sampling bias, and quantify the overhead and break-even points of hardware-accelerated measurement.
  We propose GACE (GPU-Assisted Cardinality Estimation), a hybrid auxiliary architecture that augments rather than replaces the optimizer. GACE selectively invokes GPU-based measurement only in risky intervals via a Risky Gate that detects estimation uncertainty, and a GPU Measurement Engine that performs high-speed probing with explicit cost accounting for the measurement itself. This design preserves low overhead in stable regions while improving plan stability and reducing tail latency (P99) in problematic scenarios.

</details>


### [26] [Automated Training of Learned Database Components with Generative AI](https://arxiv.org/abs/2512.20271)
*Angjela Davitkova,Sebastian Michel*

Main category: cs.DB

TL;DR: 使用生成模型（如GPT）合成训练数据来增强学习型数据库组件的可行性研究


<details>
  <summary>Details</summary>
Motivation: 深度学习在数据库优化中应用广泛，但获取高质量训练数据仍然是一个重大挑战。本文探索使用生成模型来合成训练数据，以解决数据获取难题。

Method: 通过初始可行性研究，探索生成模型（如GPT）生成真实查询分布和执行计划的能力。讨论数据可扩展性和标注等关键挑战及潜在解决方案。

Result: 初步结果表明，生成模型能够有效增强训练数据集，提高学习型数据库技术的适应性。

Conclusion: 生成模型在合成数据库训练数据方面具有潜力，能够解决数据获取难题，但需要进一步解决可扩展性和标注等挑战。

Abstract: The use of deep learning for database optimization has gained significant traction, offering improvements in indexing, cardinality estimation, and query optimization. However, acquiring high-quality training data remains a significant challenge. This paper explores the possibility of using generative models, such as GPT, to synthesize training data for learned database components. We present an initial feasibility study investigating their ability to produce realistic query distributions and execution plans for database workloads. Additionally, we discuss key challenges, such as data scalability and labeling, along with potential solutions. The initial results suggest that generative models can effectively augment training datasets, improving the adaptability of learned database techniques.

</details>
