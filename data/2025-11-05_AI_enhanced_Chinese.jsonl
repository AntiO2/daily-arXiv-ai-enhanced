{"id": "2511.01941", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.01941", "abs": "https://arxiv.org/abs/2511.01941", "authors": ["Sogol Masoumzadeh"], "title": "Detecting Vulnerabilities from Issue Reports for Internet-of-Things", "comment": "ACCEPTED/To Appear in the Proceedings of the 40th IEEE/ACM\n  International Conference on Automated Software Engineering (ASE) 2025.\n  https://conf.researchr.org/details/ase-2025/ase-2025-student-research-competition/5/Detecting-Vulnerabilities-from-Issue-Reports-for-Internet-of-Things", "summary": "Timely identification of issue reports reflecting software vulnerabilities is\ncrucial, particularly for Internet-of-Things (IoT) where analysis is slower\nthan non-IoT systems. While Machine Learning (ML) and Large Language Models\n(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use\nremains unexplored. We are the first to tackle this problem by proposing two\napproaches: (1) combining ML and LLMs with Natural Language Processing (NLP)\ntechniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects\nand (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000\nGitHub issues for classifying \\vul. Our best performance belongs to a Support\nVector Machine (SVM) trained on BERT NLP features, achieving an Area Under the\nreceiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT\nachieves 0.26 accuracy, emphasizing the importance of exposing all data during\ntraining. Our contributions set the stage for accurately detecting IoT\nvulnerabilities from issue reports, similar to non-IoT systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63a2\u7d22\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u7269\u8054\u7f51\u8f6f\u4ef6\u4e2d\u7684\u6f0f\u6d1e\u95ee\u9898\u62a5\u544a\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u5e76\u572821\u4e2aEclipse\u7269\u8054\u7f51\u9879\u76ee\u4e2d\u9a8c\u8bc1\u6027\u80fd\u3002", "motivation": "\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u6f0f\u6d1e\u95ee\u9898\u62a5\u544a\u7684\u53ca\u65f6\u8bc6\u522b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u8054\u7f51\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u63a2\u7d22\uff0c\u800c\u7269\u8054\u7f51\u7cfb\u7edf\u7684\u5206\u6790\u901f\u5ea6\u6bd4\u975e\u7269\u8054\u7f51\u7cfb\u7edf\u66f4\u6162\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a(1) \u7ed3\u5408\u673a\u5668\u5b66\u4e60\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u68c0\u6d4b21\u4e2aEclipse\u7269\u8054\u7f51\u9879\u76ee\u7684\u6f0f\u6d1e\u6307\u793a\u95ee\u9898\uff1b(2) \u572811,000\u4e2aGitHub\u95ee\u9898\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u7684BERT\u63a9\u7801\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u6f0f\u6d1e\u5206\u7c7b\u3002", "result": "\u57fa\u4e8eBERT NLP\u7279\u5f81\u8bad\u7ec3\u7684SVM\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cAUC\u8fbe\u52300.65\uff1b\u5fae\u8c03\u7684BERT\u6a21\u578b\u51c6\u786e\u7387\u4e3a0.26\uff0c\u8868\u660e\u8bad\u7ec3\u65f6\u66b4\u9732\u6240\u6709\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ece\u7269\u8054\u7f51\u95ee\u9898\u62a5\u544a\u4e2d\u51c6\u786e\u68c0\u6d4b\u6f0f\u6d1e\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5b9e\u73b0\u4e86\u4e0e\u975e\u7269\u8054\u7f51\u7cfb\u7edf\u7c7b\u4f3c\u7684\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2511.02108", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02108", "abs": "https://arxiv.org/abs/2511.02108", "authors": ["Steven Cho", "Stefano Ruberto", "Valerio Terragni"], "title": "Metamorphic Testing of Large Language Models for Natural Language Processing", "comment": null, "summary": "Using large language models (LLMs) to perform natural language processing\n(NLP) tasks has become increasingly pervasive in recent times. The versatile\nnature of LLMs makes them applicable to a wide range of such tasks. While the\nperformance of recent LLMs is generally outstanding, several studies have shown\nthat they can often produce incorrect results. Automatically identifying these\nfaulty behaviors is extremely useful for improving the effectiveness of LLMs.\nOne obstacle to this is the limited availability of labeled datasets, which\nnecessitates an oracle to determine the correctness of LLM behaviors.\nMetamorphic testing (MT) is a popular testing approach that alleviates this\noracle problem. At the core of MT are metamorphic relations (MRs), which define\nrelationships between the outputs of related inputs. MT can expose faulty\nbehaviors without the need for explicit oracles (e.g., labeled datasets). This\npaper presents the most comprehensive study of MT for LLMs to date. We\nconducted a literature review and collected 191 MRs for NLP tasks. We\nimplemented a representative subset (36 MRs) to conduct a series of experiments\nwith three popular LLMs, running approximately 560,000 metamorphic tests. The\nresults shed light on the capabilities and opportunities of MT for LLMs, as\nwell as its limitations.", "AI": {"tldr": "\u672c\u6587\u5bf9LLMs\u8fdb\u884c\u53d8\u5f62\u6d4b\u8bd5\u7684\u5168\u9762\u7814\u7a76\uff0c\u6536\u96c6\u4e86191\u4e2aNLP\u4efb\u52a1\u7684\u53d8\u5f62\u5173\u7cfb\uff0c\u5b9e\u65bd\u4e8636\u4e2a\u4ee3\u8868\u6027\u5173\u7cfb\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8fd0\u884c\u7ea656\u4e07\u6b21\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u53d8\u5f62\u6d4b\u8bd5\u5728LLMs\u4e2d\u7684\u80fd\u529b\u3001\u673a\u4f1a\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u867d\u7136LLMs\u5728NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u9519\u8bef\u7ed3\u679c\u3002\u81ea\u52a8\u8bc6\u522b\u8fd9\u4e9b\u9519\u8bef\u884c\u4e3a\u5bf9\u63d0\u9ad8LLMs\u6709\u6548\u6027\u5f88\u6709\u7528\uff0c\u4f46\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u96c6\u3002\u53d8\u5f62\u6d4b\u8bd5\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e2aoracle\u95ee\u9898\u3002", "method": "\u8fdb\u884c\u6587\u732e\u7efc\u8ff0\u6536\u96c6191\u4e2aNLP\u4efb\u52a1\u7684\u53d8\u5f62\u5173\u7cfb\uff0c\u5b9e\u73b036\u4e2a\u4ee3\u8868\u6027\u5173\u7cfb\uff0c\u4f7f\u7528\u4e09\u4e2a\u6d41\u884cLLMs\u8fd0\u884c\u7ea6560,000\u6b21\u53d8\u5f62\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u53d8\u5f62\u6d4b\u8bd5\u5728LLMs\u4e2d\u7684\u80fd\u529b\u548c\u673a\u4f1a\uff0c\u540c\u65f6\u4e5f\u66b4\u9732\u4e86\u5176\u5c40\u9650\u6027\u3002", "conclusion": "\u53d8\u5f62\u6d4b\u8bd5\u662f\u8bc6\u522bLLMs\u9519\u8bef\u884c\u4e3a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4f46\u5b58\u5728\u4e00\u5b9a\u5c40\u9650\u6027\u3002"}}
{"id": "2511.02197", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02197", "abs": "https://arxiv.org/abs/2511.02197", "authors": ["Shufan Wang", "Xing Hu", "Junkai Chen", "Zhiyuan Pan", "Xin Xia"], "title": "Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs", "comment": "13 pages, 4 figures", "summary": "With the widespread application of large language models (LLMs) in the field\nof code intelligence, increasing attention has been paid to the reliability and\ncontrollability of their outputs in code reasoning tasks. Confidence estimation\nserves as an effective and convenient approach for evaluating these aspects.\nThis paper proposes a confidence analysis and enhancement framework for LLMs\ntailored to code reasoning tasks. We conduct a comprehensive empirical study on\nthe confidence reliability of mainstream LLMs across different tasks, and\nfurther evaluate the effectiveness of techniques such as prompt strategy\noptimisation and mathematical calibration (e.g., Platt Scaling) in improving\nconfidence reliability. Our results show that DeepSeek-Reasoner achieves the\nbest performance across various tasks, outperforming other models by up to\n$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance\nScore, respectively. The hybrid strategy combining the reassess prompt strategy\nand Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$\nover the original performance in the aforementioned three metrics. These\nresults indicate that models with reasoning capabilities demonstrate superior\nconfidence reliability, and that the hybrid strategy is the most effective in\nenhancing the confidence reliability of various models. Meanwhile, we elucidate\nthe impact of different task complexities, model scales, and strategies on\nconfidence performance, and highlight that the confidence of current LLMs in\ncomplex reasoning tasks still has considerable room for improvement. This study\nnot only provides a research foundation and technical reference for the\napplication of confidence in LLM-assisted software engineering, but also points\nthe way for future optimisation and engineering deployment of confidence\nmechanisms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u7684LLM\u7f6e\u4fe1\u5ea6\u5206\u6790\u4e0e\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u53d1\u73b0DeepSeek-Reasoner\u5728\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u7ed3\u5408\u91cd\u65b0\u8bc4\u4f30\u63d0\u793a\u7b56\u7565\u548cPlatt Scaling\u7684\u6df7\u5408\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u667a\u80fd\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8f93\u51fa\u5728\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u63a7\u6027\u53d7\u5230\u5173\u6ce8\uff0c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6210\u4e3a\u8bc4\u4f30\u8fd9\u4e9b\u7279\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u7f6e\u4fe1\u5ea6\u5206\u6790\u4e0e\u589e\u5f3a\u6846\u67b6\uff0c\u5bf9\u4e3b\u6d41LLM\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u63d0\u793a\u7b56\u7565\u4f18\u5316\u548c\u6570\u5b66\u6821\u51c6\uff08\u5982Platt Scaling\uff09\u7b49\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "result": "DeepSeek-Reasoner\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5728ECE\u3001Brier Score\u548cPerformance Score\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u5176\u4ed6\u6a21\u578b\u9ad8\u51fa0.680\u30010.636\u548c13.652\u3002\u6df7\u5408\u7b56\u7565\u5728\u4e0a\u8ff0\u4e09\u4e2a\u6307\u6807\u4e0a\u6bd4\u539f\u59cb\u6027\u80fd\u63d0\u5347\u4e860.541\u30010.628\u548c15.084\u3002", "conclusion": "\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\uff0c\u6df7\u5408\u7b56\u7565\u5728\u589e\u5f3a\u5404\u79cd\u6a21\u578b\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u65b9\u9762\u6700\u4e3a\u6709\u6548\u3002\u5f53\u524dLLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2511.02203", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02203", "abs": "https://arxiv.org/abs/2511.02203", "authors": ["Gerhard Yu", "Mithila Sivakumar", "Alvine B. Belle", "Soude Ghari", "Song Wang", "Timothy C. Lethbridge"], "title": "LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases", "comment": null, "summary": "Assurance cases allow verifying the correct implementation of certain\nnon-functional requirements of mission-critical systems, including their\nsafety, security, and reliability. They can be used in the specification of\nautonomous driving, avionics, air traffic control, and similar systems. They\naim to reduce risks of harm of all kinds including human mortality,\nenvironmental damage, and financial loss. However, assurance cases often tend\nto be organized as extensive documents spanning hundreds of pages, making their\ncreation, review, and maintenance error-prone, time-consuming, and tedious.\nTherefore, there is a growing need to leverage (semi-)automated techniques,\nsuch as those powered by generative AI and large language models (LLMs), to\nenhance efficiency, consistency, and accuracy across the entire assurance-case\nlifecycle. In this paper, we focus on assurance case review, a critical task\nthat ensures the quality of assurance cases and therefore fosters their\nacceptance by regulatory authorities. We propose a novel approach that\nleverages the \\textit{LLM-as-a-judge} paradigm to automate the review process.\nSpecifically, we propose new predicate-based rules that formalize\nwell-established assurance case review criteria, allowing us to craft LLM\nprompts tailored to the review task. Our experiments on several\nstate-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show\nthat, while most LLMs yield relatively good review capabilities, DeepSeek-R1\nand GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately\noutperforming GPT-4.1. However, our experimental results also suggest that\nhuman reviewers are still needed to refine the reviews LLMs yield.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM-as-a-judge\u8303\u5f0f\u7684\u4fdd\u969c\u6848\u4f8b\u81ea\u52a8\u5ba1\u67e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c13\u8bcd\u89c4\u5219\u5f62\u5f0f\u5316\u5ba1\u67e5\u6807\u51c6\uff0c\u5b9e\u9a8c\u8868\u660eDeepSeek-R1\u548cGPT-4.1\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4ecd\u9700\u4eba\u5de5\u5b8c\u5584\u3002", "motivation": "\u4fdd\u969c\u6848\u4f8b\u5bf9\u5173\u952e\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u4eba\u5de5\u5ba1\u67e5\u8fc7\u7a0b\u5197\u957f\u6613\u9519\uff0c\u9700\u8981\u5229\u7528\u751f\u6210\u5f0fAI\u548cLLM\u63d0\u9ad8\u6548\u7387\u3001\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528LLM-as-a-judge\u8303\u5f0f\uff0c\u63d0\u51fa\u65b0\u7684\u57fa\u4e8e\u8c13\u8bcd\u7684\u89c4\u5219\u6765\u5f62\u5f0f\u5316\u4fdd\u969c\u6848\u4f8b\u5ba1\u67e5\u6807\u51c6\uff0c\u5e76\u9488\u5bf9\u5ba1\u67e5\u4efb\u52a1\u5b9a\u5236LLM\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u5148\u8fdbLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cDeepSeek-R1\u548cGPT-4.1\u8868\u73b0\u6700\u4f18\uff0cDeepSeek-R1\u6700\u7ec8\u8d85\u8d8aGPT-4.1\uff0c\u4f46LLM\u5ba1\u67e5\u4ecd\u9700\u4eba\u5de5\u5b8c\u5584\u3002", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u652f\u6301\u4fdd\u969c\u6848\u4f8b\u5ba1\u67e5\uff0cDeepSeek-R1\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5b8c\u5168\u81ea\u52a8\u5316\u5ba1\u67e5\u4ecd\u9700\u4eba\u7c7b\u4e13\u5bb6\u7684\u53c2\u4e0e\u548c\u7ec6\u5316\u3002"}}
{"id": "2511.01860", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01860", "abs": "https://arxiv.org/abs/2511.01860", "authors": ["Leszek Sliwko"], "title": "A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks", "comment": "This is the accepted author's version of the paper. The final\n  published version is available in Global Journal of Computer Science and\n  Technology, 2019", "summary": "This review analyzes deployed and actively used workload schedulers'\nsolutions and presents a taxonomy in which those systems are divided into\nseveral hierarchical groups based on their architecture and design. While other\ntaxonomies do exist, this review has focused on the key design factors that\naffect the throughput and scalability of a given solution, as well as the\nincremental improvements which bettered such an architecture. This review gives\nspecial attention to Google's Borg, which is one of the most advanced and\npublished systems of this kind.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u5206\u6790\u4e86\u5df2\u90e8\u7f72\u548c\u6b63\u5728\u4f7f\u7528\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u4f53\u7cfb\u3002\u7279\u522b\u5173\u6ce8Google\u7684Borg\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u7c7b\u4f53\u7cfb\u4e0d\u591f\u5b8c\u5584\uff0c\u672c\u6587\u91cd\u70b9\u5173\u6ce8\u5f71\u54cd\u8c03\u5ea6\u5668\u541e\u5410\u91cf\u548c\u53ef\u6269\u5c55\u6027\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\uff0c\u4ee5\u53ca\u6539\u8fdb\u67b6\u6784\u7684\u6e10\u8fdb\u5f0f\u4f18\u5316\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5df2\u90e8\u7f72\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\uff0c\u5efa\u7acb\u57fa\u4e8e\u67b6\u6784\u548c\u8bbe\u8ba1\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u7279\u522b\u6df1\u5165\u7814\u7a76\u4e86Google Borg\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u8bc6\u522b\u4e86\u5f71\u54cd\u8c03\u5ea6\u5668\u6027\u80fd\u7684\u5173\u952e\u8bbe\u8ba1\u56e0\u7d20\uff0c\u5e76\u5c55\u793a\u4e86\u67b6\u6784\u4f18\u5316\u7684\u6f14\u8fdb\u8def\u5f84\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7406\u89e3\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u7684\u8bbe\u8ba1\u539f\u7406\u548c\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u7279\u522b\u7a81\u51fa\u4e86Borg\u7cfb\u7edf\u5728\u67b6\u6784\u8bbe\u8ba1\u65b9\u9762\u7684\u5148\u8fdb\u6027\u3002"}}
{"id": "2511.02352", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02352", "abs": "https://arxiv.org/abs/2511.02352", "authors": ["Sanket Mhatre", "Yasharth Bajpai", "Sumit Gulwani", "Emerson Murphy-Hill", "Gustavo Soares"], "title": "SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks", "comment": null, "summary": "AI coding agents have shown great progress on Python software engineering\nbenchmarks like SWE-Bench, and for other languages like Java and C in\nbenchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language\nranking #5 in the TIOBE index -- remains absent from such benchmarks. We\nintroduce SWE-Sharp-Bench, a reproducible software engineering benchmark for\nC\\# featuring 150 instances from 17 repositories. Evaluating identical\nmodel-agent configurations across languages reveals a significant performance\ngap: while 70% of Python tasks in SWE-Bench Verified are solved, $only 40% of\nour C\\# tasks are resolved. We open-source SWE-Sharp-Bench and our entire\ncuration pipeline.", "AI": {"tldr": "SWE-Sharp-Bench\u662f\u9996\u4e2a\u9488\u5bf9C#\u8bed\u8a00\u7684\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b150\u4e2a\u5b9e\u4f8b\uff0c\u586b\u8865\u4e86C#\u5728AI\u7f16\u7801\u4ee3\u7406\u8bc4\u4f30\u4e2d\u7684\u7a7a\u767d\u3002", "motivation": "C#\u4f5c\u4e3a\u6392\u540d\u7b2c5\u7684\u4f01\u4e1a\u7ea7\u7f16\u7a0b\u8bed\u8a00\uff0c\u5728\u73b0\u6709\u7684AI\u7f16\u7801\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982SWE-Bench\uff09\u4e2d\u7f3a\u5931\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30C#\u4e0a\u7684AI\u7f16\u7801\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b150\u4e2a\u5b9e\u4f8b\u7684SWE-Sharp-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d617\u4e2a\u4ee3\u7801\u4ed3\u5e93\uff0c\u5e76\u5f00\u6e90\u4e86\u6574\u4e2a\u6784\u5efa\u6d41\u7a0b\u3002", "result": "\u8de8\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff1aPython\u4efb\u52a1\u89e3\u51b3\u7387\u4e3a70%\uff0c\u800cC#\u4efb\u52a1\u4ec5\u4e3a40%\u3002", "conclusion": "C#\u5728AI\u7f16\u7801\u4ee3\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u660e\u663e\u843d\u540e\u4e8ePython\uff0c\u9700\u8981\u66f4\u591a\u9488\u5bf9C#\u7684\u4f18\u5316\u548c\u7814\u7a76\u3002"}}
{"id": "2511.01861", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01861", "abs": "https://arxiv.org/abs/2511.01861", "authors": ["Johan Messchendorp", "Mohammad Al-Turany", "Volker Friese", "Thorsten Kollegger", "Bastian Loeher", "Jochen Markert", "Andrew Mistry", "Thomas Neff", "Adrian Oeftiger", "Michael Papenbrock", "Stephane Pietri", "Shahab Sanjari", "Tobias Stockmanns"], "title": "Conceptual Design Report for FAIR Computing", "comment": "88 pages, Conceptual Design Report for FAIR Computing", "summary": "This Conceptual Design Report (CDR) presents the plans of the computing\ninfrastructure for research at FAIR, Darmstadt, Germany. It presents the\ncomputing requirements of the various research groups, the policies for the\ncomputing and storage infrastructure, the foreseen FAIR computing model\nincluding the open data, software and services policies and architecture for\nthe periods starting in 2028 with the \"first science (plus)\" phase to the\nmodularized start version of FAIR. The overall ambition is to create a\nfederated and centrally-orchestrated infrastructure serving the large diversity\nof the research lines present with sufficient scalability and flexibility to\ncope with future data challenges that will be present at FAIR.", "AI": {"tldr": "\u672c\u6982\u5ff5\u8bbe\u8ba1\u62a5\u544a\u89c4\u5212\u4e86\u5fb7\u56fd\u8fbe\u59c6\u65bd\u5854\u7279FAIR\u7814\u7a76\u4e2d\u5fc3\u4ece2028\u5e74\"\u9996\u6279\u79d1\u5b66+\"\u9636\u6bb5\u5230\u6a21\u5757\u5316\u542f\u52a8\u7248\u672c\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\uff0c\u65e8\u5728\u521b\u5efa\u8054\u90a6\u5316\u3001\u96c6\u4e2d\u7f16\u6392\u7684\u57fa\u7840\u8bbe\u65bd\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u7814\u7a76\u9700\u6c42\u3002", "motivation": "\u4e3aFAIR\u5404\u7814\u7a76\u7ec4\u63d0\u4f9b\u8ba1\u7b97\u9700\u6c42\u652f\u6301\uff0c\u5efa\u7acb\u80fd\u591f\u5e94\u5bf9\u672a\u6765\u6570\u636e\u6311\u6218\u7684\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u5236\u5b9a\u8ba1\u7b97\u548c\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u653f\u7b56\uff0c\u8bbe\u8ba1\u5305\u542b\u5f00\u653e\u6570\u636e\u3001\u8f6f\u4ef6\u548c\u670d\u52a1\u653f\u7b56\u7684FAIR\u8ba1\u7b97\u6a21\u578b\u67b6\u6784\u3002", "result": "\u63d0\u51fa\u4e86\u4ece2028\u5e74\u5f00\u59cb\u7684\u9636\u6bb5\u6027\u5b9e\u65bd\u8ba1\u5212\uff0c\u6db5\u76d6\"\u9996\u6279\u79d1\u5b66+\"\u9636\u6bb5\u5230\u6a21\u5757\u5316\u542f\u52a8\u7248\u672c\u7684\u8ba1\u7b97\u57fa\u7840\u8bbe\u65bd\u84dd\u56fe\u3002", "conclusion": "\u76ee\u6807\u662f\u521b\u5efa\u8054\u90a6\u5316\u3001\u96c6\u4e2d\u7f16\u6392\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3aFAIR\u7684\u591a\u6837\u5316\u7814\u7a76\u8def\u7ebf\u63d0\u4f9b\u8db3\u591f\u53ef\u6269\u5c55\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765\u7684\u6570\u636e\u6311\u6218\u3002"}}
{"id": "2511.01896", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.01896", "abs": "https://arxiv.org/abs/2511.01896", "authors": ["Alessandro Padella", "Francesco Vinci", "Massimiliano de Leoni"], "title": "An Experimental Comparison of Alternative Techniques for Event-Log Augmentation", "comment": null, "summary": "Process mining analyzes and improves processes by examining transactional\ndata stored in event logs, which record sequences of events with timestamps.\nHowever, the effectiveness of process mining, especially when combined with\nmachine or deep learning, depends on having large event logs. Event log\naugmentation addresses this limitation by generating additional traces that\nsimulate realistic process executions while considering various perspectives\nlike time, control-flow, workflow, resources, and domain-specific attributes.\nAlthough prior research has explored event-log augmentation techniques, there\nhas been no comprehensive comparison of their effectiveness. This paper reports\non an evaluation of seven state-of-the-art augmentation techniques across eight\nevent logs. The results are also compared with those obtained by a baseline\ntechnique based on a stochastic transition system. The comparison has been\ncarried on analyzing four different aspects: similarity, preservation of\npredictive information, information loss/enhancement, and computational times\nrequired. Results show that, considering the different criteria, a technique\nbased on a stochastic transition system combined with resource queue modeling\nwould provide higher quality synthetic event logs. Event-log augmentation\ntechniques are also compared with traditional data-augmentation techniques,\nshowing that the former provide significant benefits, whereas the latter fail\nto consider process constraints.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e03\u79cd\u6700\u5148\u8fdb\u7684\u4e8b\u4ef6\u65e5\u5fd7\u589e\u5f3a\u6280\u672f\uff0c\u53d1\u73b0\u57fa\u4e8e\u968f\u673a\u8f6c\u79fb\u7cfb\u7edf\u7ed3\u5408\u8d44\u6e90\u961f\u5217\u5efa\u6a21\u7684\u6280\u672f\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u4e14\u4e8b\u4ef6\u65e5\u5fd7\u589e\u5f3a\u6280\u672f\u76f8\u6bd4\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u8fc7\u7a0b\u6316\u6398\u4f9d\u8d56\u4e8e\u5927\u578b\u4e8b\u4ef6\u65e5\u5fd7\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5f80\u5f80\u7f3a\u4e4f\u8db3\u591f\u6570\u636e\u3002\u4e8b\u4ef6\u65e5\u5fd7\u589e\u5f3a\u6280\u672f\u901a\u8fc7\u751f\u6210\u6a21\u62df\u771f\u5b9e\u8fc7\u7a0b\u6267\u884c\u7684\u989d\u5916\u8f68\u8ff9\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u4f46\u4e4b\u524d\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6280\u672f\u6709\u6548\u6027\u7684\u5168\u9762\u6bd4\u8f83\u3002", "method": "\u5728\u516b\u4e2a\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u8bc4\u4f30\u4e03\u79cd\u6700\u5148\u8fdb\u7684\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u4e0e\u57fa\u4e8e\u968f\u673a\u8f6c\u79fb\u7cfb\u7edf\u7684\u57fa\u7ebf\u6280\u672f\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ece\u76f8\u4f3c\u6027\u3001\u9884\u6d4b\u4fe1\u606f\u4fdd\u7559\u3001\u4fe1\u606f\u635f\u5931/\u589e\u5f3a\u548c\u8ba1\u7b97\u65f6\u95f4\u56db\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u968f\u673a\u8f6c\u79fb\u7cfb\u7edf\u7ed3\u5408\u8d44\u6e90\u961f\u5217\u5efa\u6a21\u7684\u6280\u672f\u5728\u4e0d\u540c\u6807\u51c6\u4e0b\u80fd\u63d0\u4f9b\u66f4\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u4e8b\u4ef6\u65e5\u5fd7\u3002\u4e8b\u4ef6\u65e5\u5fd7\u589e\u5f3a\u6280\u672f\u76f8\u6bd4\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u6280\u672f\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u540e\u8005\u65e0\u6cd5\u8003\u8651\u8fc7\u7a0b\u7ea6\u675f\u3002", "conclusion": "\u4e8b\u4ef6\u65e5\u5fd7\u589e\u5f3a\u6280\u672f\u80fd\u6709\u6548\u89e3\u51b3\u8fc7\u7a0b\u6316\u6398\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5176\u4e2d\u57fa\u4e8e\u968f\u673a\u8f6c\u79fb\u7cfb\u7edf\u7ed3\u5408\u8d44\u6e90\u961f\u5217\u5efa\u6a21\u7684\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u8fc7\u7a0b\u6316\u6398\u4e0e\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2511.02399", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02399", "abs": "https://arxiv.org/abs/2511.02399", "authors": ["Junwei Liu", "Chen Xu", "Chong Wang", "Tong Bai", "Weitong Chen", "Kaseng Wong", "Yiling Lou", "Xin Peng"], "title": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents", "comment": "14 pages, 6 figures", "summary": "Recent advances in large language model agents offer the promise of\nautomating end-to-end software development from natural language requirements.\nHowever, existing approaches largely adopt linear, waterfall-style pipelines,\nwhich oversimplify the iterative nature of real-world development and struggle\nwith complex, large-scale projects. To address these limitations, we propose\nEvoDev, an iterative software development framework inspired by feature-driven\ndevelopment. EvoDev decomposes user requirements into a set of user-valued\nfeatures and constructs a Feature Map, a directed acyclic graph that explicitly\nmodels dependencies between features. Each node in the feature map maintains\nmulti-level information, including business logic, design, and code, which is\npropagated along dependencies to provide context for subsequent development\niterations. We evaluate EvoDev on challenging Android development tasks and\nshow that it outperforms the best-performing baseline, Claude Code, by a\nsubstantial margin of 56.8%, while improving single-agent performance by\n16.0%-76.6% across different base LLMs, highlighting the importance of\ndependency modeling, context propagation, and workflow-aware agent design for\ncomplex software projects. Our work summarizes practical insights for designing\niterative, LLM-driven development frameworks and informs future training of\nbase LLMs to better support iterative software development.", "AI": {"tldr": "EvoDev\u662f\u4e00\u4e2a\u53d7\u7279\u6027\u9a71\u52a8\u5f00\u53d1\u542f\u53d1\u7684\u8fed\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u9700\u6c42\u5206\u89e3\u4e3a\u7279\u6027\u5e76\u6784\u5efa\u7279\u6027\u6620\u5c04\u56fe\u6765\u663e\u5f0f\u5efa\u6a21\u7279\u6027\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728Android\u5f00\u53d1\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u7ebf\u6027\u7684\u7011\u5e03\u5f0f\u6d41\u7a0b\uff0c\u8fc7\u5ea6\u7b80\u5316\u4e86\u771f\u5b9e\u4e16\u754c\u5f00\u53d1\u7684\u8fed\u4ee3\u6027\u8d28\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u5927\u89c4\u6a21\u9879\u76ee\u3002", "method": "\u5c06\u7528\u6237\u9700\u6c42\u5206\u89e3\u4e3a\u7279\u6027\u96c6\uff0c\u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\u5f62\u5f0f\u7684\u7279\u6027\u6620\u5c04\u56fe\uff0c\u6bcf\u4e2a\u8282\u70b9\u7ef4\u62a4\u4e1a\u52a1\u903b\u8f91\u3001\u8bbe\u8ba1\u548c\u4ee3\u7801\u7b49\u591a\u7ea7\u4fe1\u606f\uff0c\u5e76\u6cbf\u4f9d\u8d56\u5173\u7cfb\u4f20\u64ad\u4e0a\u4e0b\u6587\u3002", "result": "\u5728Android\u5f00\u53d1\u4efb\u52a1\u4e2d\uff0cEvoDev\u6bd4\u6700\u4f73\u57fa\u7ebfClaude Code\u6027\u80fd\u63d0\u534756.8%\uff0c\u5728\u4e0d\u540c\u57fa\u7840LLM\u4e0a\u5355\u667a\u80fd\u4f53\u6027\u80fd\u63d0\u534716.0%-76.6%\u3002", "conclusion": "\u4f9d\u8d56\u5efa\u6a21\u3001\u4e0a\u4e0b\u6587\u4f20\u64ad\u548c\u5de5\u4f5c\u6d41\u611f\u77e5\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u5bf9\u590d\u6742\u8f6f\u4ef6\u9879\u76ee\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u8bbe\u8ba1\u8fed\u4ee3\u5f0fLLM\u9a71\u52a8\u5f00\u53d1\u6846\u67b6\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2511.01862", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.01862", "abs": "https://arxiv.org/abs/2511.01862", "authors": ["Vanessa Sochat", "Daniel Milroy"], "title": "Possible Futures for Cloud Cost Models", "comment": "10 pages", "summary": "Cloud is now the leading software and computing hardware innovator, and is\nchanging the landscape of compute to one that is optimized for artificial\nintelligence and machine learning (AI/ML). Computing innovation was initially\ndriven to meet the needs of scientific computing. As industry and consumer\nusage of computing proliferated, there was a shift to satisfy a multipolar\ncustomer base. Demand for AI/ML now dominates modern computing and innovation\nhas centralized on cloud. As a result, cost and resource models designed to\nserve AI/ML use cases are not currently well suited for science. If resource\ncontention resulting from a unipole consumer makes access to contended\nresources harder for scientific users, a likely future is running scientific\nworkloads where they were not intended. In this article, we discuss the past,\ncurrent, and possible futures of cloud cost models for the continued support of\ndiscovery and science.", "AI": {"tldr": "\u4e91\u8ba1\u7b97\u5df2\u6210\u4e3aAI/ML\u521b\u65b0\u7684\u4e3b\u5bfc\u529b\u91cf\uff0c\u4f46\u5f53\u524d\u7684\u8d44\u6e90\u6210\u672c\u6a21\u578b\u4e0d\u9002\u5408\u79d1\u5b66\u8ba1\u7b97\u9700\u6c42\uff0c\u53ef\u80fd\u5bfc\u81f4\u79d1\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u5728\u4e0d\u9002\u5408\u7684\u73af\u5883\u4e2d\u8fd0\u884c\u3002", "motivation": "\u5206\u6790\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u4ece\u79d1\u5b66\u8ba1\u7b97\u9a71\u52a8\u5230AI/ML\u4e3b\u5bfc\u7684\u8f6c\u53d8\uff0c\u63a2\u8ba8\u5982\u4f55\u7ee7\u7eed\u652f\u6301\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u56de\u987e\u548c\u8d8b\u52bf\u5206\u6790\uff0c\u8ba8\u8bba\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u7684\u6f14\u53d8\u53ca\u5176\u5bf9\u79d1\u5b66\u8ba1\u7b97\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0AI/ML\u9700\u6c42\u4e3b\u5bfc\u7684\u4e91\u8ba1\u7b97\u521b\u65b0\u5bfc\u81f4\u8d44\u6e90\u6a21\u578b\u4e0d\u9002\u5408\u79d1\u5b66\u8ba1\u7b97\uff0c\u53ef\u80fd\u9020\u6210\u79d1\u5b66\u5de5\u4f5c\u8d1f\u8f7d\u8fd0\u884c\u73af\u5883\u4e0d\u5339\u914d\u3002", "conclusion": "\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u4e91\u8ba1\u7b97\u6210\u672c\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u652f\u6301\u79d1\u5b66\u53d1\u73b0\uff0c\u907f\u514d\u79d1\u5b66\u8ba1\u7b97\u88ab\u8fb9\u7f18\u5316\u3002"}}
{"id": "2511.01942", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.DL"], "pdf": "https://arxiv.org/pdf/2511.01942", "abs": "https://arxiv.org/abs/2511.01942", "authors": ["Khalil Rejiba", "Sang-Hyeok Lee", "Christina Gasper", "Martina Freund", "Sandra Korte-Kerzel", "Ulrich Kerzel"], "title": "Towards Defect Phase Diagrams: From Research Data Management to Automated Workflows", "comment": null, "summary": "Defect phase diagrams provide a unified description of crystal defect states\nfor materials design and are central to the scientific objectives of the\nCollaborative Research Centre (CRC) 1394. Their construction requires the\nsystematic integration of heterogeneous experimental and simulation data across\nresearch groups and locations. In this setting, research data management (RDM)\nis a key enabler of new scientific insight by linking distributed research\nactivities and making complex data reproducible and reusable.\n  To address the challenge of heterogeneous data sources and formats, a\ncomprehensive RDM infrastructure has been established that links experiment,\ndata, and analysis in a seamless workflow. The system combines: (1) a joint\nelectronic laboratory notebook and laboratory information management system,\n(2) easy-to-use large-object data storage, (3) automatic metadata extraction\nfrom heterogeneous and proprietary file formats, (4) interactive provenance\ngraphs for data exploration and reuse, and (5) automated reporting and analysis\nworkflows. The two key technological elements are the openBIS electronic\nlaboratory notebook and laboratory information management system, and a newly\ndeveloped companion application that extends openBIS with large-scale data\nhandling, automated metadata capture, and federated access to distributed\nresearch data.\n  This integrated approach reduces friction in data capture and curation,\nenabling traceable and reusable datasets that accelerate the construction of\ndefect phase diagrams across institutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7efc\u5408\u7814\u7a76\u6570\u636e\u7ba1\u7406\u57fa\u7840\u8bbe\u65bd\uff0c\u7528\u4e8e\u6784\u5efa\u7f3a\u9677\u76f8\u56fe\uff0c\u901a\u8fc7\u6574\u5408\u5f02\u6784\u5b9e\u9a8c\u548c\u6a21\u62df\u6570\u636e\uff0c\u5b9e\u73b0\u8de8\u7814\u7a76\u7ec4\u548c\u5730\u70b9\u7684\u6570\u636e\u53ef\u91cd\u590d\u4f7f\u7528\u3002", "motivation": "\u6784\u5efa\u7f3a\u9677\u76f8\u56fe\u9700\u8981\u7cfb\u7edf\u6574\u5408\u5f02\u6784\u7684\u5b9e\u9a8c\u548c\u6a21\u62df\u6570\u636e\uff0c\u4f46\u6570\u636e\u6765\u6e90\u548c\u683c\u5f0f\u7684\u591a\u6837\u6027\u5e26\u6765\u4e86\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u7684\u7814\u7a76\u6570\u636e\u7ba1\u7406\u6765\u8fde\u63a5\u5206\u5e03\u5f0f\u7814\u7a76\u6d3b\u52a8\u3002", "method": "\u5efa\u7acb\u7efc\u5408RDM\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ec\uff1a\u8054\u5408\u7535\u5b50\u5b9e\u9a8c\u5ba4\u7b14\u8bb0\u672c\u548c\u5b9e\u9a8c\u5ba4\u4fe1\u606f\u7ba1\u7406\u7cfb\u7edf\u3001\u6613\u7528\u7684\u5927\u5bf9\u8c61\u6570\u636e\u5b58\u50a8\u3001\u81ea\u52a8\u5143\u6570\u636e\u63d0\u53d6\u3001\u4ea4\u4e92\u5f0f\u6eaf\u6e90\u56fe\u3001\u81ea\u52a8\u5316\u62a5\u544a\u548c\u5206\u6790\u5de5\u4f5c\u6d41\uff0c\u6838\u5fc3\u662fopenBIS\u7cfb\u7edf\u53ca\u5176\u914d\u5957\u5e94\u7528\u3002", "result": "\u8be5\u96c6\u6210\u65b9\u6cd5\u51cf\u5c11\u4e86\u6570\u636e\u6355\u83b7\u548c\u6574\u7406\u7684\u6469\u64e6\uff0c\u5b9e\u73b0\u4e86\u53ef\u8ffd\u8e2a\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u96c6\uff0c\u52a0\u901f\u4e86\u8de8\u673a\u6784\u7684\u7f3a\u9677\u76f8\u56fe\u6784\u5efa\u3002", "conclusion": "\u7efc\u5408RDM\u57fa\u7840\u8bbe\u65bd\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784\u6570\u636e\u96c6\u6210\u95ee\u9898\uff0c\u4e3a\u6750\u6599\u8bbe\u8ba1\u4e2d\u7684\u7f3a\u9677\u76f8\u56fe\u6784\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2511.02434", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02434", "abs": "https://arxiv.org/abs/2511.02434", "authors": ["Dominik Fuch\u00df", "Haoyu Liu", "Sophie Corallo", "Tobias Hey", "Jan Keim", "Johannes von Geisau", "Anne Koziolek"], "title": "Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition", "comment": null, "summary": "Identifying architecturally relevant entities in textual artifacts is crucial\nfor Traceability Link Recovery (TLR) between Software Architecture\nDocumentation (SAD) and source code. While Software Architecture Models (SAMs)\ncan bridge the semantic gap between these artifacts, their manual creation is\ntime-consuming. Large Language Models (LLMs) offer new capabilities for\nextracting architectural entities from SAD and source code to construct SAMs\nautomatically or establish direct trace links. This paper presents two\nLLM-based approaches: ExArch extracts component names as simple SAMs from SAD\nand source code to eliminate the need for manual SAM creation, while ArTEMiS\nidentifies architectural entities in documentation and matches them with\n(manually or automatically generated) SAM entities. Our evaluation compares\nagainst state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC\nachieves strong performance (F1: 0.87) but requires manually created SAMs;\nExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS\nis on par with the traditional heuristic-based SWATTR (F1: 0.81) and can\nsuccessfully replace it when integrated with TransArC. The combination of\nArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.\nOur results demonstrate that LLMs can effectively identify architectural\nentities in textual artifacts, enabling automated SAM generation and TLR,\nmaking architecture-code traceability more practical and accessible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u8bc6\u522b\u8f6f\u4ef6\u67b6\u6784\u76f8\u5173\u5b9e\u4f53\u5e76\u5efa\u7acb\u67b6\u6784\u6587\u6863\u4e0e\u6e90\u4ee3\u7801\u4e4b\u95f4\u7684\u53ef\u8ffd\u6eaf\u6027\u94fe\u63a5\uff0c\u65e0\u9700\u624b\u52a8\u521b\u5efa\u8f6f\u4ef6\u67b6\u6784\u6a21\u578b\u3002", "motivation": "\u8f6f\u4ef6\u67b6\u6784\u6587\u6863\u4e0e\u6e90\u4ee3\u7801\u4e4b\u95f4\u7684\u53ef\u8ffd\u6eaf\u6027\u94fe\u63a5\u6062\u590d\u9700\u8981\u8bc6\u522b\u67b6\u6784\u76f8\u5173\u5b9e\u4f53\uff0c\u4f46\u624b\u52a8\u521b\u5efa\u8f6f\u4ef6\u67b6\u6784\u6a21\u578b\u8017\u65f6\u8d39\u529b\u3002LLM\u4e3a\u81ea\u52a8\u63d0\u53d6\u67b6\u6784\u5b9e\u4f53\u63d0\u4f9b\u4e86\u65b0\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1aExArch\u4ece\u67b6\u6784\u6587\u6863\u548c\u6e90\u4ee3\u7801\u4e2d\u63d0\u53d6\u7ec4\u4ef6\u540d\u79f0\u4f5c\u4e3a\u7b80\u5355\u67b6\u6784\u6a21\u578b\uff1bArTEMiS\u8bc6\u522b\u6587\u6863\u4e2d\u7684\u67b6\u6784\u5b9e\u4f53\u5e76\u4e0e\u67b6\u6784\u6a21\u578b\u5b9e\u4f53\u8fdb\u884c\u5339\u914d\u3002", "result": "ExArch\u8fbe\u5230\u4e0e\u9700\u8981\u624b\u52a8\u67b6\u6784\u6a21\u578b\u7684TransArC\u76f8\u5f53\u7684\u6027\u80fd\uff08F1: 0.86 vs 0.87\uff09\uff1bArTEMiS\u4e0e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5SWATTR\u6027\u80fd\u76f8\u5f53\uff08F1: 0.81\uff09\uff0c\u4e14\u4e0eTransArC\u96c6\u6210\u65f6\u53ef\u66ff\u4ee3SWATTR\u3002\u4e24\u79cd\u65b9\u6cd5\u7ec4\u5408\u4f18\u4e8e\u65e0\u9700\u624b\u52a8\u67b6\u6784\u6a21\u578b\u7684\u6700\u4f73\u57fa\u7ebfArDoCode\u3002", "conclusion": "LLM\u80fd\u6709\u6548\u8bc6\u522b\u6587\u672c\u5de5\u4ef6\u4e2d\u7684\u67b6\u6784\u5b9e\u4f53\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u67b6\u6784\u6a21\u578b\u751f\u6210\u548c\u53ef\u8ffd\u6eaf\u6027\u94fe\u63a5\u6062\u590d\uff0c\u4f7f\u67b6\u6784-\u4ee3\u7801\u53ef\u8ffd\u6eaf\u6027\u66f4\u52a0\u5b9e\u7528\u548c\u6613\u7528\u3002"}}
{"id": "2511.01863", "categories": ["cs.DC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.01863", "abs": "https://arxiv.org/abs/2511.01863", "authors": ["Robert Fabian Lindermann", "Paul-Niklas Ken Kandora", "Simon Caspar Zeller", "Adrian Asmund Fessler", "Steffen Rebennack"], "title": "SPHERE: Spherical partitioning for large-scale routing optimization", "comment": null, "summary": "We study shortest-path routing in large weighted, undirected graphs, where\nexpanding search frontiers raise time and memory costs for exact solvers. We\npropose \\emph{SPHERE}, a source-target-aware heuristic that identifies an\n$s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count.\nSelecting an anchor $a$ in this overlap partitions the task into two\nsubproblems with unchanged problem-topology, $s\\to a$ and $a\\to t$; if either\nremains large, the procedure recurses on its induced subgraph. Because the cut\nlies inside the overlap, concatenating the resulting subpaths yields a valid\n$s\\to t$ route without boundary repair. SPHERE is independent of the downstream\nsolver (e.g., Dijkstra) and exposes parallelism across subproblems. On large\nnetworks, it achieves faster runtimes and smaller optimality gaps than\nLouvain-based routing and a METIS-based pipeline, even on graphs with more than\na million nodes and edges, while also outperforming Dijkstra in runtime.", "AI": {"tldr": "SPHERE\u662f\u4e00\u79cd\u6e90-\u76ee\u6807\u611f\u77e5\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522bs-t\u91cd\u53e0\u533a\u57df\u6765\u5206\u5272\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u63d0\u9ad8\u5927\u89c4\u6a21\u56fe\u7684\u8def\u7531\u6548\u7387\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u52a0\u6743\u65e0\u5411\u56fe\u4e2d\uff0c\u6269\u5c55\u641c\u7d22\u8fb9\u754c\u4f1a\u589e\u52a0\u7cbe\u786e\u6c42\u89e3\u5668\u7684\u65f6\u95f4\u548c\u5185\u5b58\u6210\u672c\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u8bc6\u522b\u6e90\u8282\u70b9s\u548c\u76ee\u6807\u8282\u70b9t\u4e4b\u95f4\u7684\u91cd\u53e0\u533a\u57df\uff0c\u9009\u62e9\u951a\u70b9a\u5c06\u95ee\u9898\u5206\u5272\u4e3as\u2192a\u548ca\u2192t\u4e24\u4e2a\u5b50\u95ee\u9898\uff0c\u9012\u5f52\u5904\u7406\u5927\u578b\u5b50\u56fe\uff0c\u65e0\u9700\u8fb9\u754c\u4fee\u590d\u3002", "result": "\u5728\u8d85\u8fc7\u767e\u4e07\u8282\u70b9\u548c\u8fb9\u7684\u5927\u578b\u7f51\u7edc\u4e0a\uff0cSPHERE\u6bd4\u57fa\u4e8eLouvain\u7684\u8def\u7531\u548cMETIS\u6d41\u6c34\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u8fd0\u884c\u65f6\u95f4\u548c\u66f4\u5c0f\u7684\u6700\u4f18\u6027\u5dee\u8ddd\uff0c\u4e14\u8fd0\u884c\u65f6\u95f4\u4f18\u4e8eDijkstra\u7b97\u6cd5\u3002", "conclusion": "SPHERE\u662f\u4e00\u79cd\u72ec\u7acb\u4e8e\u4e0b\u6e38\u6c42\u89e3\u5668\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u5927\u89c4\u6a21\u56fe\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u6700\u77ed\u8def\u5f84\u8def\u7531\uff0c\u5e76\u652f\u6301\u5b50\u95ee\u9898\u7684\u5e76\u884c\u5904\u7406\u3002"}}
{"id": "2511.02002", "categories": ["cs.DB", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02002", "abs": "https://arxiv.org/abs/2511.02002", "authors": ["Xiangru Jian", "Zhengyuan Dong", "M. Tamer \u00d6zsu"], "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations", "comment": "Working paper", "summary": "In recent years, querying semantic web data using SPARQL has remained\nchallenging, especially for non-expert users, due to the language's complex\nsyntax and the prerequisite of understanding intricate data structures. To\naddress these challenges, we propose InteracSPARQL, an interactive SPARQL query\ngeneration and refinement system that leverages natural language explanations\n(NLEs) to enhance user comprehension and facilitate iterative query refinement.\nInteracSPARQL integrates LLMs with a rule-based approach to first produce\nstructured explanations directly from SPARQL abstract syntax trees (ASTs),\nfollowed by LLM-based linguistic refinements. Users can interactively refine\nqueries through direct feedback or LLM-driven self-refinement, enabling the\ncorrection of ambiguous or incorrect query components in real time. We evaluate\nInteracSPARQL on standard benchmarks, demonstrating significant improvements in\nquery accuracy, explanation clarity, and overall user satisfaction compared to\nbaseline approaches. Our experiments further highlight the effectiveness of\ncombining rule-based methods with LLM-driven refinements to create more\naccessible and robust SPARQL interfaces.", "AI": {"tldr": "InteracSPARQL\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0fSPARQL\u67e5\u8be2\u751f\u6210\u548c\u4f18\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548cLLM\u589e\u5f3a\uff0c\u5e2e\u52a9\u975e\u4e13\u5bb6\u7528\u6237\u66f4\u8f7b\u677e\u5730\u6784\u5efa\u548c\u4f18\u5316\u8bed\u4e49\u7f51\u67e5\u8be2\u3002", "motivation": "SPARQL\u67e5\u8be2\u8bed\u8a00\u8bed\u6cd5\u590d\u6742\u4e14\u9700\u8981\u7406\u89e3\u590d\u6742\u6570\u636e\u7ed3\u6784\uff0c\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u66f4\u6613\u7528\u7684\u67e5\u8be2\u63a5\u53e3\u3002", "method": "\u7ed3\u5408LLM\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\uff0c\u4eceSPARQL\u62bd\u8c61\u8bed\u6cd5\u6811\u751f\u6210\u7ed3\u6784\u5316\u89e3\u91ca\uff0c\u7136\u540e\u901a\u8fc7LLM\u8fdb\u884c\u8bed\u8a00\u4f18\u5316\uff0c\u652f\u6301\u7528\u6237\u4ea4\u4e92\u5f0f\u53cd\u9988\u548cLLM\u9a71\u52a8\u7684\u81ea\u6211\u4f18\u5316\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u67e5\u8be2\u51c6\u786e\u6027\u3001\u89e3\u91ca\u6e05\u6670\u5ea6\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u65b9\u6cd5\u548cLLM\u9a71\u52a8\u7684\u4f18\u5316\u80fd\u591f\u521b\u5efa\u66f4\u6613\u8bbf\u95ee\u548c\u9c81\u68d2\u7684SPARQL\u63a5\u53e3\u3002"}}
{"id": "2511.02445", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02445", "abs": "https://arxiv.org/abs/2511.02445", "authors": ["Eriks Klotins", "Magnus Ahlgren", "Nicolas Martin Vivaldi", "Even-Andre Karlsson"], "title": "When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations", "comment": null, "summary": "Purpose: Continuous Software Engineering (CSE) promises improved efficiency,\nquality, and responsiveness in software-intensive organizations. However, fully\nadopting CSE is often constrained by complex products, legacy systems,\norganizational inertia, and regulatory requirements. In this paper, we examine\nfour industrial cases from the automation, automotive, retail, and chemical\nsectors to explore how such constraints shape CSE adoption in practice.\nMethods: We apply and extend a previously proposed CSE Industry Readiness Model\nto assess the current and potential levels of adoption in each case. Through\nexpert interviews and narrative synthesis, we identify common driving forces\nand adoption barriers, including organizational preparedness,\ncross-organizational dependencies, and limited customer demand for continuous\ndelivery. Results: Based on our findings, we propose an updated readiness model\nthat introduces additional levels of internal and external feedback,\ndistinguishes market- and organization-facing constraints, and better guides\npractitioners in setting realistic CSE adoption goals. Conclusions: Our results\nhighlight that while full end-to-end CSE adoption may not always be feasible,\nmeaningful internal improvements are still possible and beneficial. This study\nprovides empirically grounded guidance for organizations navigating partial or\nconstrained CSE transformations.", "AI": {"tldr": "\u7814\u7a76\u56db\u4e2a\u5de5\u4e1a\u6848\u4f8b\uff0c\u5206\u6790\u7ea6\u675f\u6761\u4ef6\u4e0b\u6301\u7eed\u8f6f\u4ef6\u5de5\u7a0b\uff08CSE\uff09\u7684\u91c7\u7528\u60c5\u51b5\uff0c\u63d0\u51fa\u66f4\u65b0\u7684\u5c31\u7eea\u5ea6\u6a21\u578b\u6765\u6307\u5bfc\u5b9e\u8df5\u3002", "motivation": "\u6301\u7eed\u8f6f\u4ef6\u5de5\u7a0b\u627f\u8bfa\u63d0\u5347\u6548\u7387\u3001\u8d28\u91cf\u548c\u54cd\u5e94\u80fd\u529b\uff0c\u4f46\u590d\u6742\u4ea7\u54c1\u3001\u9057\u7559\u7cfb\u7edf\u3001\u7ec4\u7ec7\u60ef\u6027\u548c\u76d1\u7ba1\u8981\u6c42\u9650\u5236\u4e86\u5176\u5168\u9762\u91c7\u7528\u3002", "method": "\u5e94\u7528\u5e76\u6269\u5c55CSE\u884c\u4e1a\u5c31\u7eea\u5ea6\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u5bb6\u8bbf\u8c08\u548c\u53d9\u4e8b\u7efc\u5408\u8bc4\u4f30\u56db\u4e2a\u5de5\u4e1a\u6848\u4f8b\u7684\u91c7\u7528\u73b0\u72b6\u548c\u6f5c\u529b\u3002", "result": "\u8bc6\u522b\u4e86\u7ec4\u7ec7\u51c6\u5907\u5ea6\u3001\u8de8\u7ec4\u7ec7\u4f9d\u8d56\u6027\u548c\u5ba2\u6237\u5bf9\u6301\u7eed\u4ea4\u4ed8\u9700\u6c42\u6709\u9650\u7b49\u5171\u540c\u9a71\u52a8\u56e0\u7d20\u548c\u969c\u788d\uff0c\u63d0\u51fa\u4e86\u66f4\u65b0\u7684\u5c31\u7eea\u5ea6\u6a21\u578b\u3002", "conclusion": "\u867d\u7136\u5168\u9762\u7aef\u5230\u7aefCSE\u91c7\u7528\u53ef\u80fd\u4e0d\u53ef\u884c\uff0c\u4f46\u6709\u610f\u4e49\u7684\u5185\u90e8\u6539\u8fdb\u4ecd\u7136\u53ef\u80fd\u4e14\u6709\u76ca\uff0c\u4e3a\u7ec4\u7ec7\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\u8fdb\u884cCSE\u8f6c\u578b\u63d0\u4f9b\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2511.01866", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.01866", "abs": "https://arxiv.org/abs/2511.01866", "authors": ["Benjamin Kubwimana", "Qijing Huang"], "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs", "comment": "Published in the Proceedings of the 2025 IEEE International Symposium\n  on Workload Characterization (IISWC 2025)", "summary": "Edge intelligence paradigm is increasingly demanded by the emerging\nautonomous systems, such as robotics. Beyond ensuring privacy-preserving\noperation and resilience in connectivity-limited environments, edge deployment\noffers significant energy and cost advantages over cloud-based solutions.\nHowever, deploying large language models (LLMs) for reasoning tasks on edge\nGPUs faces critical challenges from strict latency constraints and limited\ncomputational resources. To navigate these constraints, developers must balance\nmultiple design factors - choosing reasoning versus non-reasoning\narchitectures, selecting appropriate model sizes, allocating token budgets, and\napplying test-time scaling strategies - to meet target latency and optimize\naccuracy. Yet guidance on optimal combinations of these variables remains\nscarce. In this work, we present EdgeReasoning, a comprehensive study\ncharacterizing the deployment of reasoning LLMs on edge GPUs. We systematically\nquantify latency-accuracy tradeoffs across various LLM architectures and model\nsizes. We systematically evaluate prompt-based and model-tuning-based\ntechniques for reducing reasoning token length while maintaining performance\nquality. We further profile test-time scaling methods with varying degrees of\nparallelism to maximize accuracy under strict latency budgets. Through these\nanalyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency\nconfigurations, offering systematic guidance for optimal edge deployment of\nreasoning LLMs.", "AI": {"tldr": "EdgeReasoning\u662f\u4e00\u4e2a\u5173\u4e8e\u5728\u8fb9\u7f18GPU\u4e0a\u90e8\u7f72\u63a8\u7406\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u6027\u7814\u7a76\uff0c\u7cfb\u7edf\u91cf\u5316\u4e86\u4e0d\u540cLLM\u67b6\u6784\u548c\u6a21\u578b\u5927\u5c0f\u7684\u5ef6\u8fdf-\u7cbe\u5ea6\u6743\u8861\uff0c\u4e3a\u8fb9\u7f18\u63a8\u7406LLM\u7684\u6700\u4f18\u90e8\u7f72\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u8fb9\u7f18\u90e8\u7f72LLM\u7528\u4e8e\u63a8\u7406\u4efb\u52a1\u9762\u4e34\u4e25\u683c\u7684\u5ef6\u8fdf\u7ea6\u675f\u548c\u6709\u9650\u7684\u8ba1\u7b97\u8d44\u6e90\u6311\u6218\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5728\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u67b6\u6784\u9009\u62e9\u3001\u6a21\u578b\u5927\u5c0f\u3001token\u9884\u7b97\u5206\u914d\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u7b49\u591a\u4e2a\u8bbe\u8ba1\u56e0\u7d20\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u8fd9\u4e9b\u53d8\u91cf\u6700\u4f18\u7ec4\u5408\u7684\u6307\u5bfc\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8e\u63d0\u793a\u548c\u6a21\u578b\u5fae\u8c03\u7684\u6280\u672f\u4ee5\u51cf\u5c11\u63a8\u7406token\u957f\u5ea6\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u8d28\u91cf\uff1b\u5206\u6790\u4e0d\u540c\u5e76\u884c\u5ea6\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u4ee5\u5728\u4e25\u683c\u5ef6\u8fdf\u9884\u7b97\u4e0b\u6700\u5927\u5316\u7cbe\u5ea6\uff1b\u901a\u8fc7\u5206\u6790\u6620\u5c04\u53ef\u5b9e\u73b0\u7684\u7cbe\u5ea6-\u5ef6\u8fdf\u914d\u7f6e\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u7814\u7a76\u91cf\u5316\u4e86\u5404\u79cdLLM\u67b6\u6784\u548c\u6a21\u578b\u5927\u5c0f\u7684\u5ef6\u8fdf-\u7cbe\u5ea6\u6743\u8861\uff0c\u8bc4\u4f30\u4e86\u51cf\u5c11\u63a8\u7406token\u957f\u5ea6\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "EdgeReasoning\u4e3a\u63a8\u7406LLM\u5728\u8fb9\u7f18GPU\u4e0a\u7684\u6700\u4f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u6307\u5bfc\uff0c\u901a\u8fc7\u6620\u5c04\u7cbe\u5ea6-\u5ef6\u8fdf\u914d\u7f6e\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\u4e0b\u505a\u51fa\u6700\u4f18\u8bbe\u8ba1\u51b3\u7b56\u3002"}}
{"id": "2511.02062", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02062", "abs": "https://arxiv.org/abs/2511.02062", "authors": ["Yuting Yang", "Tiancheng Yuan", "Jamal Hashim", "Thiago Garrett", "Jeffrey Qian", "Ann Zhang", "Yifan Wang", "Weijia Song", "Ken Birman"], "title": "Vortex: Hosting ML Inference and Knowledge Retrieval Services With Tight Latency and Throughput Requirements", "comment": null, "summary": "There is growing interest in deploying ML inference and knowledge retrieval\nas services that could support both interactive queries by end users and more\ndemanding request flows that arise from AIs integrated into a end-user\napplications and deployed as agents. Our central premise is that these latter\ncases will bring service level latency objectives (SLOs). Existing ML serving\nplatforms use batching to optimize for high throughput, exposing them to\nunpredictable tail latencies. Vortex enables an SLO-first approach. For\nidentical tasks, Vortex's pipelines achieve significantly lower and more stable\nlatencies than TorchServe and Ray Serve over a wide range of workloads, often\nenabling a given SLO target at more than twice the request rate. When RDMA is\navailable, the Vortex advantage is even more significant.", "AI": {"tldr": "Vortex\u662f\u4e00\u4e2a\u9762\u5411SLO\uff08\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff09\u7684ML\u63a8\u7406\u670d\u52a1\u5e73\u53f0\uff0c\u901a\u8fc7\u4f18\u5316\u6279\u5904\u7406\u673a\u5236\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u76f8\u6bd4TorchServe\u548cRay Serve\u5728\u76f8\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u80fd\u652f\u6301\u66f4\u9ad8\u7684\u8bf7\u6c42\u7387\u3002", "motivation": "\u968f\u7740ML\u63a8\u7406\u548c\u77e5\u8bc6\u68c0\u7d22\u670d\u52a1\u9700\u6c42\u7684\u589e\u957f\uff0c\u7279\u522b\u662f\u5728AI\u4ee3\u7406\u96c6\u6210\u5230\u7ec8\u7aef\u7528\u6237\u5e94\u7528\u4e2d\u7684\u573a\u666f\u4e0b\uff0c\u9700\u8981\u6ee1\u8db3\u4e25\u683c\u7684\u670d\u52a1\u6c34\u5e73\u5ef6\u8fdf\u76ee\u6807\uff08SLOs\uff09\u3002\u73b0\u6709ML\u670d\u52a1\u5e73\u53f0\u901a\u8fc7\u6279\u5904\u7406\u4f18\u5316\u541e\u5410\u91cf\uff0c\u4f46\u9762\u4e34\u4e0d\u53ef\u9884\u6d4b\u7684\u5c3e\u90e8\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "Vortex\u91c7\u7528SLO\u4f18\u5148\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u6279\u5904\u7406\u673a\u5236\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002\u5f53RDMA\u53ef\u7528\u65f6\uff0c\u5176\u4f18\u52bf\u66f4\u52a0\u663e\u8457\u3002", "result": "\u5728\u76f8\u540c\u4efb\u52a1\u4e0b\uff0cVortex\u76f8\u6bd4TorchServe\u548cRay Serve\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u4e14\u66f4\u7a33\u5b9a\u7684\u5ef6\u8fdf\uff0c\u5728\u5e7f\u6cdb\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8303\u56f4\u5185\uff0c\u901a\u5e38\u80fd\u5728\u8d85\u8fc7\u4e24\u500d\u7684\u8bf7\u6c42\u7387\u4e0b\u8fbe\u5230\u7ed9\u5b9a\u7684SLO\u76ee\u6807\u3002", "conclusion": "Vortex\u8bc1\u660e\u4e86SLO\u4f18\u5148\u65b9\u6cd5\u5728ML\u63a8\u7406\u670d\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u5ef6\u8fdf\u6027\u80fd\u5e76\u652f\u6301\u66f4\u9ad8\u7684\u8bf7\u6c42\u7387\uff0c\u7279\u522b\u662f\u5728RDMA\u73af\u5883\u4e0b\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2511.02475", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02475", "abs": "https://arxiv.org/abs/2511.02475", "authors": ["J\u00fcrgen Cito", "Dominik Bork"], "title": "Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering", "comment": null, "summary": "Generative AI enables rapid ``vibe coding,\" where natural language prompts\nyield working software systems. While this lowers barriers to software\ncreation, it also collapses the boundary between prototypes and engineered\nsoftware, leading to fragile systems that lack robustness, security, and\nmaintainability. We argue that this shift motivates a reimagining of software\nmodels. Rather than serving only as upfront blueprints, models can be recovered\npost-hoc from AI-generated code to restore comprehension, expose risks, and\nguide refinement. In this role, models serve as mediators between human intent,\nAI generation, and long-term system evolution, providing a path toward\nsustainable AI-driven software engineering.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u964d\u4f4e\u4e86\u8f6f\u4ef6\u5f00\u53d1\u95e8\u69db\uff0c\u4f46\u4e5f\u5bfc\u81f4\u539f\u578b\u4e0e\u5de5\u7a0b\u5316\u8f6f\u4ef6\u754c\u9650\u6a21\u7cca\uff0c\u4ea7\u751f\u8106\u5f31\u7cfb\u7edf\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u8f6f\u4ef6\u6a21\u578b\u7684\u4f5c\u7528\uff0c\u5c06\u5176\u4f5c\u4e3aAI\u751f\u6210\u4ee3\u7801\u4e0e\u4eba\u7c7b\u610f\u56fe\u4e4b\u95f4\u7684\u4e2d\u4ecb\u3002", "motivation": "\u751f\u6210\u5f0fAI\u4f7f\"\u6c1b\u56f4\u7f16\u7a0b\"\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u5bfc\u81f4\u8f6f\u4ef6\u539f\u578b\u4e0e\u5de5\u7a0b\u5316\u7cfb\u7edf\u754c\u9650\u6a21\u7cca\uff0c\u7f3a\u4e4f\u5065\u58ee\u6027\u3001\u5b89\u5168\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u8f6f\u4ef6\u6a21\u578b\u5728AI\u9a71\u52a8\u5f00\u53d1\u4e2d\u7684\u89d2\u8272\u3002", "method": "\u63d0\u51fa\u5c06\u8f6f\u4ef6\u6a21\u578b\u4f5c\u4e3a\u4e8b\u540e\u4eceAI\u751f\u6210\u4ee3\u7801\u4e2d\u6062\u590d\u7684\u5de5\u5177\uff0c\u800c\u4e0d\u662f\u524d\u671f\u84dd\u56fe\u3002\u6a21\u578b\u4f5c\u4e3a\u4eba\u7c7b\u610f\u56fe\u3001AI\u751f\u6210\u548c\u7cfb\u7edf\u957f\u671f\u6f14\u8fdb\u4e4b\u95f4\u7684\u4e2d\u4ecb\u3002", "result": "\u901a\u8fc7\u4e8b\u540e\u6a21\u578b\u6062\u590d\uff0c\u53ef\u4ee5\u6062\u590d\u5bf9\u7cfb\u7edf\u7684\u7406\u89e3\u3001\u66b4\u9732\u98ce\u9669\u5e76\u6307\u5bfc\u6539\u8fdb\uff0c\u4e3a\u53ef\u6301\u7eed\u7684AI\u9a71\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u8def\u5f84\u3002", "conclusion": "\u8f6f\u4ef6\u6a21\u578b\u5e94\u91cd\u65b0\u5b9a\u4f4d\u4e3aAI\u751f\u6210\u4ee3\u7801\u4e0e\u4eba\u7c7b\u610f\u56fe\u4e4b\u95f4\u7684\u4e2d\u4ecb\u89d2\u8272\uff0c\u4e3a\u53ef\u6301\u7eed\u7684AI\u9a71\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2511.01871", "categories": ["cs.DC", "68M15, 93B12"], "pdf": "https://arxiv.org/pdf/2511.01871", "abs": "https://arxiv.org/abs/2511.01871", "authors": ["S. Tsiramua", "H. Meladze", "T. Davitashvili", "J. M. Sanchez", "F. Criado-Aldeanueva"], "title": "Structural Analysis of Multi-Core Processor and Reliability Evaluation Model", "comment": null, "summary": "In the present paper, the models of structural analysis and evaluation of\nefficiency indicators (reliability, fault tolerance, viability, and\nflexibility) of a multi core processor with variable structure, equipped with\nmulti functional cores, are considered. Using logical probabilistic methods,\nthe following has been developed: models for evaluating the reliability and\nfault tolerance of processor cores as multi functional elements; logical\nprobabilistic models of the shortest paths, flexibility, and performance\nconditions for successful operation of multi core processors based on multi\nfunctional cores; and models for estimating the reliability, fault tolerance,\nand lifetime of multi core processors considering all possible states of\nperformance. The results of the structural analysis of two core and four core\nprocessors and the trends of increasing the efficiency indicators of multi core\nprocessors are presented.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u591a\u6838\u5904\u7406\u5668\u7ed3\u6784\u5206\u6790\u548c\u6548\u7387\u6307\u6807\u8bc4\u4f30\u6a21\u578b\uff0c\u5305\u62ec\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u3001\u751f\u5b58\u6027\u548c\u7075\u6d3b\u6027\uff0c\u4f7f\u7528\u903b\u8f91\u6982\u7387\u65b9\u6cd5\u5206\u6790\u591a\u529f\u80fd\u6838\u5fc3\u7684\u591a\u6838\u5904\u7406\u5668\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u591a\u6838\u5904\u7406\u5668\u4e2d\u591a\u529f\u80fd\u6838\u5fc3\u7684\u7ed3\u6784\u7279\u6027\u548c\u6548\u7387\u6307\u6807\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u72b6\u6001\u4e0b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u903b\u8f91\u6982\u7387\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u53ef\u9760\u6027\u3001\u5bb9\u9519\u6027\u8bc4\u4f30\u6a21\u578b\uff0c\u6700\u77ed\u8def\u5f84\u3001\u7075\u6d3b\u6027\u548c\u6027\u80fd\u6761\u4ef6\u6a21\u578b\uff0c\u4ee5\u53ca\u8003\u8651\u6240\u6709\u53ef\u80fd\u6027\u80fd\u72b6\u6001\u7684\u5bff\u547d\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5bf9\u53cc\u6838\u548c\u56db\u6838\u5904\u7406\u5668\u8fdb\u884c\u4e86\u7ed3\u6784\u5206\u6790\uff0c\u5c55\u793a\u4e86\u591a\u6838\u5904\u7406\u5668\u6548\u7387\u6307\u6807\u63d0\u5347\u7684\u8d8b\u52bf\u3002", "conclusion": "\u901a\u8fc7\u903b\u8f91\u6982\u7387\u6a21\u578b\u6709\u6548\u8bc4\u4f30\u4e86\u591a\u6838\u5904\u7406\u5668\u7684\u7ed3\u6784\u6548\u7387\u548c\u6027\u80fd\u6307\u6807\uff0c\u4e3a\u591a\u6838\u5904\u7406\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2511.02096", "categories": ["cs.DB", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.02096", "abs": "https://arxiv.org/abs/2511.02096", "authors": ["Savo Tomovic"], "title": "Numbering Combinations for Compact Representation of Many-to-Many Relationship Sets", "comment": null, "summary": "In this paper we propose an approach to implement specific relation-ship set\nbetween two entities called combinatorial relationship set. For the\ncombinatorial relationship set B between entity sets G and I the mapping\ncardinality is many-to-many. Additionally, entities from G can be uniquely\nencoded with a pair of values (h, k) generated with the procedure for numbering\ncombinations of entities from I. The encoding procedure is based on\ncombinatorial number system that provides a representation of all possible k\n-combinations of a set of n elements by a single number. In general\nmany-to-many relationship sets are represented by a relation or table, while\nthe combinatorial relationship is not physically stored as separate table.\nHowever, all information is encapsulated into a single column added to G. The\nnew column is a candidate key in G. Additional operation named Rank-Join to\nfundamental relational-algebra is presented to combine information from g and i\nassociated with a combinatorial relationship set. Motivation for combinatorial\nrelationship originates from challenges in designing and implementing\nmultivalued dimensions and bridge tables in data-warehouse models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u7ec4\u5408\u5173\u7cfb\u96c6\u7684\u5b9e\u73b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e24\u4e2a\u5b9e\u4f53\u95f4\u7684\u591a\u5bf9\u591a\u5173\u7cfb\uff0c\u901a\u8fc7\u7ec4\u5408\u6570\u7cfb\u7edf\u7f16\u7801\u5b9e\u4f53\uff0c\u907f\u514d\u7269\u7406\u5b58\u50a8\u5173\u7cfb\u8868\u3002", "motivation": "\u52a8\u673a\u6e90\u4e8e\u6570\u636e\u4ed3\u5e93\u6a21\u578b\u4e2d\u591a\u503c\u7ef4\u5ea6\u548c\u6865\u63a5\u8868\u7684\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7ec4\u5408\u6570\u7cfb\u7edf\u5bf9\u5b9e\u4f53\u8fdb\u884c\u552f\u4e00\u7f16\u7801\uff0c\u5c06\u7ec4\u5408\u5173\u7cfb\u4fe1\u606f\u5c01\u88c5\u5230\u5355\u4e2a\u5217\u4e2d\uff0c\u4e0d\u7269\u7406\u5b58\u50a8\u5173\u7cfb\u8868\uff0c\u5e76\u5f15\u5165Rank-Join\u64cd\u4f5c\u5230\u5173\u7cfb\u4ee3\u6570\u3002", "result": "\u5b9e\u73b0\u4e86\u7ec4\u5408\u5173\u7cfb\u96c6\u7684\u8868\u793a\uff0c\u65b0\u5217\u6210\u4e3a\u5019\u9009\u952e\uff0c\u6240\u6709\u5173\u7cfb\u4fe1\u606f\u5c01\u88c5\u5230\u5355\u4e2a\u5217\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5bf9\u591a\u5173\u7cfb\u7684\u8868\u793a\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5173\u7cfb\u8868\u7684\u7269\u7406\u5b58\u50a8\u9700\u6c42\u3002"}}
{"id": "2511.02713", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02713", "abs": "https://arxiv.org/abs/2511.02713", "authors": ["Qianru Meng", "Zhaochun Ren", "Joost Visser"], "title": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation", "comment": null, "summary": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs.", "AI": {"tldr": "ReleaseEval\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u53d1\u5e03\u8bf4\u660e\u751f\u6210\u7684\u53ef\u590d\u73b0\u5f00\u6e90\u57fa\u51c6\uff0c\u5305\u542b94,987\u6761\u53d1\u5e03\u8bf4\u660e\uff0c\u652f\u6301\u4e09\u79cd\u4e0d\u540c\u8f93\u5165\u7c92\u5ea6\u7684\u4efb\u52a1\u8bbe\u7f6e\uff0c\u8bc4\u4f30\u663e\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u7ed3\u6784\u5316\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u53d1\u5e03\u8bf4\u660e\u751f\u6210\u4e2d\u6570\u636e\u96c6\u9650\u5236\uff08\u7f3a\u4e4f\u660e\u786e\u8bb8\u53ef\u3001\u53ef\u590d\u73b0\u6027\u5dee\uff09\u548c\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u5b8c\u6574\uff08\u4e3b\u8981\u4f9d\u8d56\u63d0\u4ea4\u6d88\u606f\u800c\u5ffd\u7565\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\uff09\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaReleaseEval\u57fa\u51c6\uff0c\u5305\u542b94,987\u6761\u53d1\u5e03\u8bf4\u660e\uff0c\u652f\u6301\u4e09\u79cd\u4efb\u52a1\u8bbe\u7f6e\uff1acommit2sum\uff08\u57fa\u4e8e\u63d0\u4ea4\u6d88\u606f\uff09\u3001tree2sum\uff08\u5305\u542b\u63d0\u4ea4\u6811\u7ed3\u6784\uff09\u3001diff2sum\uff08\u5229\u7528\u4ee3\u7801\u5dee\u5f02\uff09\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728tree2sum\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\uff0c\u4f46\u5728diff2sum\u4efb\u52a1\u4e2d\u4ecd\u6709\u56f0\u96be\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u5229\u7528\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u4f46\u5728\u4ece\u957f\u4ee3\u7801\u5dee\u5f02\u4e2d\u62bd\u8c61\u4fe1\u606f\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002"}}
{"id": "2511.01872", "categories": ["cs.DC", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.01872", "abs": "https://arxiv.org/abs/2511.01872", "authors": ["Etash Guha", "Tianxiao Jiang", "Andrew Deng", "Jian Zhang", "Muthu Annamalai"], "title": "Learned Cost Model for Placement on Reconfigurable Dataflow Hardware", "comment": "7 pages, 2 figures, 2 tables, DAC Conference style (2022)", "summary": "Mapping a dataflow-graph of an ML model onto a reconfigurable system is\ndifficult, as different mappings have different throughputs and consume\nresource constraints differently. To solve this, a model to evaluate the\nthroughput of mappings is necessary as measuring throughput completely is\nexpensive. Many use a hand-designed analytical model, relying on proxy features\nor intuition, introducing error. We provide a Learned Approach that predicts\nthroughput 31%-52% more accurately over a variety of graphs. In addition, our\napproach shows no accuracy degradation after removing performance annotations.\nWe show that using this approach results in 5.6% faster compiled graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b66\u4e60\u65b9\u6cd5\u6765\u9884\u6d4b\u6570\u636e\u6d41\u56fe\u5728\u53ef\u91cd\u6784\u7cfb\u7edf\u4e0a\u7684\u541e\u5410\u91cf\uff0c\u6bd4\u4f20\u7edf\u624b\u5de5\u5206\u6790\u6a21\u578b\u51c6\u786e\u5ea6\u63d0\u9ad831%-52%\uff0c\u5e76\u80fd\u751f\u62105.6%\u66f4\u5feb\u7684\u7f16\u8bd1\u56fe", "motivation": "\u5c06ML\u6a21\u578b\u7684\u6570\u636e\u6d41\u56fe\u6620\u5c04\u5230\u53ef\u91cd\u6784\u7cfb\u7edf\u5f88\u56f0\u96be\uff0c\u4e0d\u540c\u6620\u5c04\u5177\u6709\u4e0d\u540c\u541e\u5410\u91cf\u4e14\u8d44\u6e90\u6d88\u8017\u4e0d\u540c\u3002\u5b8c\u5168\u6d4b\u91cf\u541e\u5410\u91cf\u6210\u672c\u9ad8\u6602\uff0c\u800c\u73b0\u6709\u624b\u5de5\u5206\u6790\u6a21\u578b\u4f9d\u8d56\u4ee3\u7406\u7279\u5f81\u6216\u76f4\u89c9\uff0c\u5b58\u5728\u8bef\u5dee", "method": "\u4f7f\u7528\u5b66\u4e60\u65b9\u6cd5\u6765\u9884\u6d4b\u6620\u5c04\u7684\u541e\u5410\u91cf\uff0c\u8be5\u65b9\u6cd5\u5728\u79fb\u9664\u6027\u80fd\u6ce8\u91ca\u540e\u4ecd\u4fdd\u6301\u51c6\u786e\u5ea6", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u56fe\u4e0a\u7684\u541e\u5410\u91cf\u9884\u6d4b\u51c6\u786e\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad831%-52%\uff0c\u4f7f\u7528\u8be5\u65b9\u6cd5\u7f16\u8bd1\u7684\u56fe\u901f\u5ea6\u63d0\u9ad85.6%", "conclusion": "\u5b66\u4e60\u65b9\u6cd5\u662f\u9884\u6d4b\u6570\u636e\u6d41\u56fe\u5728\u53ef\u91cd\u6784\u7cfb\u7edf\u4e0a\u541e\u5410\u91cf\u7684\u6709\u6548\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u624b\u5de5\u5206\u6790\u6a21\u578b"}}
{"id": "2511.02611", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.02611", "abs": "https://arxiv.org/abs/2511.02611", "authors": ["Andrea D'Ascenzo", "Julian Meffert", "Petra Mutzel", "Fabrizio Rossi"], "title": "Accelerating Graph Similarity Search through Integer Linear Programming", "comment": null, "summary": "The Graph Edit Distance (GED) is an important metric for measuring the\nsimilarity between two (labeled) graphs. It is defined as the minimum cost\nrequired to convert one graph into another through a series of (elementary)\nedit operations. Its effectiveness in assessing the similarity of large graphs\nis limited by the complexity of its exact calculation, which is NP-hard\ntheoretically and computationally challenging in practice. The latter can be\nmitigated by switching to the Graph Similarity Search under GED constraints,\nwhich determines whether the edit distance between two graphs is below a given\nthreshold. A popular framework for solving Graph Similarity Search under GED\nconstraints in a graph database for a query graph is the\nfilter-and-verification framework. Filtering discards unpromising graphs, while\nthe verification step certifies the similarity between the filtered graphs and\nthe query graph. To improve the filtering step, we define a lower bound based\non an integer linear programming formulation. We prove that this lower bound\ndominates the effective branch match-based lower bound and can also be computed\nefficiently. Consequently, we propose a graph similarity search algorithm that\nuses a hierarchy of lower bound algorithms and solves a novel integer\nprogramming formulation that exploits the threshold parameter. An extensive\ncomputational experience on a well-assessed test bed shows that our approach\nsignificantly outperforms the state-of-the-art algorithm on most of the\nexamined thresholds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u65b0\u578b\u56fe\u76f8\u4f3c\u6027\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u652f\u914d\u6027\u4e0b\u754c\u548c\u5229\u7528\u9608\u503c\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u7f16\u8f91\u8ddd\u79bb\u7ea6\u675f\u4e0b\u7684\u56fe\u76f8\u4f3c\u6027\u641c\u7d22\u6027\u80fd\u3002", "motivation": "\u56fe\u7f16\u8f91\u8ddd\u79bb(GED)\u662f\u8861\u91cf\u56fe\u76f8\u4f3c\u6027\u7684\u91cd\u8981\u6307\u6807\uff0c\u4f46\u5176\u7cbe\u786e\u8ba1\u7b97\u662fNP\u96be\u95ee\u9898\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002\u56fe\u76f8\u4f3c\u6027\u641c\u7d22\u901a\u8fc7\u8bbe\u5b9a\u9608\u503c\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u8fc7\u6ee4-\u9a8c\u8bc1\u6846\u67b6\u4e2d\u7684\u8fc7\u6ee4\u6b65\u9aa4\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u4e0b\u754c\u5b9a\u4e49\uff0c\u8bc1\u660e\u8be5\u4e0b\u754c\u652f\u914d\u73b0\u6709\u7684\u5206\u652f\u5339\u914d\u4e0b\u754c\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002\u8bbe\u8ba1\u4e86\u4f7f\u7528\u4e0b\u754c\u7b97\u6cd5\u5c42\u6b21\u7ed3\u6784\u7684\u65b0\u7b97\u6cd5\uff0c\u5e76\u5229\u7528\u9608\u503c\u53c2\u6570\u6784\u5efa\u65b0\u9896\u7684\u6574\u6570\u89c4\u5212\u6a21\u578b\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5927\u91cf\u8ba1\u7b97\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6d4b\u8bd5\u9608\u503c\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u4e0b\u754c\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u56fe\u76f8\u4f3c\u6027\u641c\u7d22\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u56fe\u76f8\u4f3c\u6027\u8ba1\u7b97\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02736", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02736", "abs": "https://arxiv.org/abs/2511.02736", "authors": ["Madalena Sasportes", "Grischa Liebel", "Miguel Goul\u00e3o"], "title": "Investigating the Experience of Autistic Individuals in Software Engineering", "comment": null, "summary": "Context: Autism spectrum disorder (ASD) leads to various issues in the\neveryday life of autistic individuals, often resulting in unemployment and\nmental health problems. To improve the inclusion of autistic adults, existing\nstudies have highlighted the strengths these individuals possess in comparison\nto non-autistic individuals, e.g., high attention to detail or excellent\nlogical reasoning skills. If fostered, these strengths could be valuable in\nsoftware engineering activities, such for identifying specific kinds of bugs in\ncode. However, existing work in SE has primarily studied the challenges of\nautistic individuals and possible accommodations, with little attention their\nstrengths. Objective: Our goal is to analyse the experiences of autistic\nindividuals in software engineering activities, such as code reviews, with a\nparticular emphasis on strengths. Methods: This study combines Social-Technical\nGrounded Theory through semi-structured interviews with 16 autistic software\nengineers and a survey with 49 respondents, including 5 autistic participants.\nWe compare the emerging themes with the theory by Gama et al. on the Effect of\nNeurodivergent Cognitive Dysfunctions in Software Engineering Performance.\nResults: Our results suggest that autistic software engineers are often skilled\nin logical thinking, attention to detail, and hyperfocus in programming; and\nthey enjoy learning new programming languages and programming-related\ntechnologies. Confirming previous work, they tend to prefer written\ncommunication and remote work. Finally, we report a high comfort level in\ninteracting with AI-based systems. Conclusions: Our findings extend existing\nwork by providing further evidence on the strengths of autistic software\nengineers.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5728\u8f6f\u4ef6\u5de5\u7a0b\u6d3b\u52a8\u4e2d\u7684\u7ecf\u9a8c\uff0c\u7279\u522b\u5173\u6ce8\u4ed6\u4eec\u7684\u4f18\u52bf\uff0c\u5305\u62ec\u903b\u8f91\u601d\u7ef4\u3001\u7ec6\u8282\u5173\u6ce8\u548c\u8d85\u4e13\u6ce8\u7f16\u7a0b\u80fd\u529b\u3002", "motivation": "\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u5e38\u5bfc\u81f4\u5c31\u4e1a\u56f0\u96be\u548c\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6311\u6218\u800c\u975e\u4f18\u52bf\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u81ea\u95ed\u75c7\u4e2a\u4f53\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u4f18\u52bf\uff0c\u5982\u903b\u8f91\u63a8\u7406\u80fd\u529b\u548c\u7ec6\u8282\u5173\u6ce8\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u793e\u4f1a\u6280\u672f\u624e\u6839\u7406\u8bba\uff0c\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u8bbf\u8c0816\u540d\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u548c49\u540d\u53d7\u8bbf\u8005\uff08\u5305\u62ec5\u540d\u81ea\u95ed\u75c7\u53c2\u4e0e\u8005\uff09\u7684\u8c03\u67e5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u5728\u903b\u8f91\u601d\u7ef4\u3001\u7ec6\u8282\u5173\u6ce8\u548c\u7f16\u7a0b\u8d85\u4e13\u6ce8\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u559c\u6b22\u5b66\u4e60\u65b0\u7f16\u7a0b\u8bed\u8a00\uff0c\u504f\u597d\u4e66\u9762\u6c9f\u901a\u548c\u8fdc\u7a0b\u5de5\u4f5c\uff0c\u4e14\u5bf9AI\u7cfb\u7edf\u4e92\u52a8\u611f\u5230\u8212\u9002\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6269\u5c55\u4e86\u73b0\u6709\u5de5\u4f5c\uff0c\u4e3a\u81ea\u95ed\u75c7\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u7684\u4f18\u52bf\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u8bc1\u636e\u3002"}}
{"id": "2511.01881", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01881", "abs": "https://arxiv.org/abs/2511.01881", "authors": ["Zhengxin Fang", "Hui Ma", "Gang Chen", "Rajkumar Buyya"], "title": "HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing", "comment": null, "summary": "Microservice architecture has become a dominant paradigm in application\ndevelopment due to its advantages of being lightweight, flexible, and\nresilient. Deploying microservice applications in the container-based cloud\nenables fine-grained elastic resource allocation. Autoscaling is an effective\napproach to dynamically adjust the resource provisioned to containers. However,\nthe intricate microservice dependencies and the deployment scheme of the\ncontainer-based cloud bring extra challenges of resource scaling. This article\nproposes a novel autoscaling approach named HGraphScale. In particular,\nHGraphScale captures microservice dependencies and the deployment scheme by a\nnewly designed hierarchical graph neural network, and makes effective scaling\nactions for rapidly changing user requests workloads. Extensive experiments\nbased on real-world traces of user requests are conducted to evaluate the\neffectiveness of HGraphScale. The experiment results show that the HGraphScale\noutperforms existing state-of-the-art autoscaling approaches by reducing at\nmost 80.16\\% of the average response time under a certain VM rental budget of\napplication providers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHGraphScale\u7684\u65b0\u578b\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5fae\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\u548c\u90e8\u7f72\u65b9\u6848\uff0c\u5728\u5bb9\u5668\u4e91\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u8d44\u6e90\u6269\u7f29\u5bb9\u3002", "motivation": "\u5fae\u670d\u52a1\u67b6\u6784\u5728\u5bb9\u5668\u4e91\u4e2d\u90e8\u7f72\u65f6\uff0c\u590d\u6742\u7684\u5fae\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\u548c\u90e8\u7f72\u65b9\u6848\u7ed9\u8d44\u6e90\u6269\u7f29\u5bb9\u5e26\u6765\u4e86\u989d\u5916\u6311\u6218\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u6765\u6355\u6349\u5fae\u670d\u52a1\u4f9d\u8d56\u5173\u7cfb\u548c\u90e8\u7f72\u65b9\u6848\uff0c\u57fa\u4e8e\u5feb\u901f\u53d8\u5316\u7684\u7528\u6237\u8bf7\u6c42\u8d1f\u8f7d\u505a\u51fa\u6709\u6548\u7684\u6269\u7f29\u5bb9\u51b3\u7b56\u3002", "result": "\u5728\u771f\u5b9e\u7528\u6237\u8bf7\u6c42\u8f68\u8ff9\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGraphScale\u5728\u7279\u5b9aVM\u79df\u8d41\u9884\u7b97\u4e0b\uff0c\u6700\u591a\u80fd\u51cf\u5c1180.16%\u7684\u5e73\u5747\u54cd\u5e94\u65f6\u95f4\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u65b9\u6cd5\u3002", "conclusion": "HGraphScale\u901a\u8fc7\u5206\u5c42\u56fe\u795e\u7ecf\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u5fae\u670d\u52a1\u67b6\u6784\u5728\u5bb9\u5668\u4e91\u4e2d\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.02674", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.02674", "abs": "https://arxiv.org/abs/2511.02674", "authors": ["Tim Otto"], "title": "EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union Search across Data Lakes", "comment": "Copyright 2025 IEEE. This is the author's version of the work that\n  has been accepted for publication in Proceedings of the IEEE International\n  Conference on Big Data (IEEE BigData 2025). The final version of record is\n  available at: tba", "summary": "Data lakes enable easy maintenance of heterogeneous data in its native form.\nWhile this flexibility can accelerate data ingestion, it shifts the complexity\nof data preparation and query processing to data discovery tasks. One such task\nis Table Union Search (TUS), which identifies tables that can be unioned with a\ngiven input table. In this work, we present EasyTUS, a comprehensive framework\nthat leverages Large Language Models (LLMs) to perform efficient and scalable\nTable Union Search across data lakes. EasyTUS implements the search pipeline as\nthree modular steps: Table Serialization for consistent formatting and\nsampling, Table Representation that utilizes LLMs to generate embeddings, and\nVector Search that leverages approximate nearest neighbor indexing for semantic\nmatching. To enable reproducible and systematic evaluation, in this paper, we\nalso introduce TUSBench, a novel standardized benchmarking environment within\nthe EasyTUS framework. TUSBench supports unified comparisons across approaches\nand data lakes, promoting transparency and progress in the field. Our\nexperiments using TUSBench show that EasyTUS consistently outperforms most of\nthe state-of the-art approaches, achieving improvements in average of up to\n34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,\nand up to 7.7x faster query processing performance. Furthermore, EasyTUS\nmaintains strong performance even in metadata-absent settings, highlighting its\nrobustness and adaptability across data lakes.", "AI": {"tldr": "EasyTUS\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u53ef\u6269\u5c55\u8868\u8054\u5408\u641c\u7d22\u7684\u6846\u67b6\uff0c\u5305\u542b\u8868\u5e8f\u5217\u5316\u3001\u8868\u8868\u793a\u548c\u5411\u91cf\u641c\u7d22\u4e09\u4e2a\u6a21\u5757\u5316\u6b65\u9aa4\uff0c\u5e76\u5f15\u5165\u4e86TUSBench\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u73af\u5883\u3002", "motivation": "\u6570\u636e\u6e56\u867d\u7136\u4fbf\u4e8e\u7ef4\u62a4\u5f02\u6784\u6570\u636e\uff0c\u4f46\u589e\u52a0\u4e86\u6570\u636e\u53d1\u73b0\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u8868\u8054\u5408\u641c\u7d22\u4efb\u52a1\u9700\u8981\u8bc6\u522b\u53ef\u4e0e\u7ed9\u5b9a\u8f93\u5165\u8868\u8fdb\u884c\u8054\u5408\u7684\u8868\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u4e09\u6b65\u6cd5\uff1a\u8868\u5e8f\u5217\u5316\u8fdb\u884c\u683c\u5f0f\u5316\u548c\u91c7\u6837\uff0c\u8868\u8868\u793a\u5229\u7528LLM\u751f\u6210\u5d4c\u5165\u5411\u91cf\uff0c\u5411\u91cf\u641c\u7d22\u4f7f\u7528\u8fd1\u4f3c\u6700\u8fd1\u90bb\u7d22\u5f15\u8fdb\u884c\u8bed\u4e49\u5339\u914d\u3002", "result": "\u5b9e\u9a8c\u663e\u793aEasyTUS\u5728\u5e73\u5747\u7cbe\u5ea6\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534734.3%\uff0c\u6570\u636e\u51c6\u5907\u901f\u5ea6\u63d0\u534779.2\u500d\uff0c\u67e5\u8be2\u5904\u7406\u6027\u80fd\u63d0\u53477.7\u500d\uff0c\u5728\u5143\u6570\u636e\u7f3a\u5931\u573a\u666f\u4e0b\u4ecd\u4fdd\u6301\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "EasyTUS\u6846\u67b6\u5728\u8868\u8054\u5408\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u6548\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u6570\u636e\u6e56\u4e2d\u7684\u6570\u636e\u53d1\u73b0\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02810", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02810", "abs": "https://arxiv.org/abs/2511.02810", "authors": ["Suddhasvatta Das", "Kevin Gary"], "title": "Formalizing Regression Testing for Agile and Continuous Integration Environments", "comment": "This is the first attempt to formalize regression testing in agile\n  context as a continuous/near-continuos activity. This formalization will help\n  practitioners and researchers to answer 'when', 'what' and 'how much'\n  question of regression testing in real world time constrained agile projects.\n  This work is currently under review with Software Quality Journal", "summary": "Software developed using modern agile practices delivers a stream of software\nversions that require continuous regression testing rather than testing once\nclose to the delivery or maintenance phase, as assumed by classical\nregression-testing theory. In this work, we formalize the phenomenon of\ncontinuous or near-continuous regression testing using successive builds as a\ntime-ordered chain, where each build contains the program, requirements, and\nthe accompanying tests. We also formalize the regression test window between\nany two builds, which captures the limited time budget available for regression\ntesting. As the time limit is set to infinity and the chain is closed to two\nbuilds, the model degenerates to retest-all, thereby preserving semantics for\nthe classical two-version case. The formalization is validated by directly\nrepresenting two state-of-the-art agile regression testing algorithms in terms\nof build-tuple operations without requiring auxiliary assumptions, followed by\nproof of the soundness and completeness of our formalization.", "AI": {"tldr": "\u672c\u6587\u4e3a\u654f\u6377\u5f00\u53d1\u4e2d\u7684\u6301\u7eed\u56de\u5f52\u6d4b\u8bd5\u5efa\u7acb\u4e86\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u5c06\u8fde\u7eed\u6784\u5efa\u89c6\u4e3a\u65f6\u95f4\u6709\u5e8f\u94fe\uff0c\u5e76\u5b9a\u4e49\u4e86\u56de\u5f52\u6d4b\u8bd5\u7a97\u53e3\u6982\u5ff5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6a21\u578b\u80fd\u51c6\u786e\u8868\u793a\u73b0\u6709\u654f\u6377\u56de\u5f52\u6d4b\u8bd5\u7b97\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u654f\u6377\u5f00\u53d1\u5b9e\u8df5\u4ea7\u751f\u8fde\u7eed\u7684\u8f6f\u4ef6\u7248\u672c\u6d41\uff0c\u9700\u8981\u6301\u7eed\u56de\u5f52\u6d4b\u8bd5\uff0c\u800c\u4f20\u7edf\u56de\u5f52\u6d4b\u8bd5\u7406\u8bba\u5047\u8bbe\u5728\u4ea4\u4ed8\u6216\u7ef4\u62a4\u9636\u6bb5\u8fdb\u884c\u4e00\u6b21\u6d4b\u8bd5\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5f53\u524d\u9700\u6c42\u3002", "method": "\u5c06\u8fde\u7eed\u6784\u5efa\u5f62\u5f0f\u5316\u4e3a\u65f6\u95f4\u6709\u5e8f\u94fe\uff0c\u6bcf\u4e2a\u6784\u5efa\u5305\u542b\u7a0b\u5e8f\u3001\u9700\u6c42\u548c\u6d4b\u8bd5\uff0c\u5b9a\u4e49\u56de\u5f52\u6d4b\u8bd5\u7a97\u53e3\u6765\u6355\u83b7\u6709\u9650\u7684\u6d4b\u8bd5\u65f6\u95f4\u9884\u7b97\uff0c\u5e76\u901a\u8fc7\u6784\u5efa\u5143\u7ec4\u64cd\u4f5c\u76f4\u63a5\u8868\u793a\u73b0\u6709\u654f\u6377\u56de\u5f52\u6d4b\u8bd5\u7b97\u6cd5\u3002", "result": "\u8be5\u5f62\u5f0f\u5316\u6a21\u578b\u80fd\u591f\u51c6\u786e\u8868\u793a\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u654f\u6377\u56de\u5f52\u6d4b\u8bd5\u7b97\u6cd5\uff0c\u65e0\u9700\u8f85\u52a9\u5047\u8bbe\uff0c\u5e76\u8bc1\u660e\u4e86\u5f62\u5f0f\u5316\u7684\u6b63\u786e\u6027\u548c\u5b8c\u5907\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u4e3a\u654f\u6377\u73af\u5883\u4e0b\u7684\u6301\u7eed\u56de\u5f52\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5f53\u65f6\u95f4\u9650\u5236\u8bbe\u4e3a\u65e0\u7a77\u5927\u4e14\u94fe\u7b80\u5316\u4e3a\u4e24\u4e2a\u6784\u5efa\u65f6\uff0c\u6a21\u578b\u9000\u5316\u4e3a\u4f20\u7edf\u7684\u91cd\u6d4b\u5168\u90e8\u65b9\u6cd5\uff0c\u4fdd\u6301\u4e86\u4e0e\u7ecf\u5178\u7406\u8bba\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.01888", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01888", "abs": "https://arxiv.org/abs/2511.01888", "authors": ["Cynthia Marcelino", "Thomas Pusztai", "Stefan Nastic"], "title": "Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions", "comment": "26th International Middleware Conference (MIDDLEWARE 25)", "summary": "Serverless computing provides infrastructure management and elastic\nauto-scaling, therefore reducing operational overhead. By design serverless\nfunctions are stateless, which means they typically leverage external remote\nservices to store and exchange data. Transferring data over a network typically\ninvolves serialization and deserialization. These operations usually require\nmultiple data copies and transitions between user and kernel space, resulting\nin overhead from context switching and memory allocation, contributing\nsignificantly to increased latency and resource consumption. To address these\nissues, we present Roadrunner, a sidecar shim that enables near-zero copy and\nserialization-free data transfer between WebAssembly-based serverless\nfunctions. Roadrunner reduces the multiple copies between user space and kernel\nspace by mapping the function memory and moving the data along a dedicated\nvirtual data hose, bypassing the costly processes of serialization and\ndeserialization. This approach reduces data movement overhead and context\nswitching, achieving near-native latency performance for WebAssembly-based\nserverless functions. Our experimental results demonstrate that Roadrunner\nsignificantly improves the inter-function communication latency from 44% up to\n89%, reducing the serialization overhead in 97% of data transfer, and\nincreasing throughput by 69 times compared to state-of-the-art\nWebAssembly-based serverless functions.", "AI": {"tldr": "Roadrunner\u662f\u4e00\u4e2a\u8fb9\u8f66shim\uff0c\u901a\u8fc7\u96f6\u62f7\u8d1d\u548c\u514d\u5e8f\u5217\u5316\u6570\u636e\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347WebAssembly\u65e0\u670d\u52a1\u5668\u51fd\u6570\u7684\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\u51fd\u6570\u95f4\u6570\u636e\u4f20\u8f93\u9700\u8981\u5e8f\u5217\u5316\u548c\u591a\u6b21\u5185\u5b58\u62f7\u8d1d\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u5207\u6362\u548c\u5185\u5b58\u5206\u914d\u5f00\u9500\uff0c\u589e\u52a0\u4e86\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u4f7f\u7528\u8fb9\u8f66shim\u6280\u672f\uff0c\u901a\u8fc7\u6620\u5c04\u51fd\u6570\u5185\u5b58\u548c\u4e13\u7528\u865a\u62df\u6570\u636e\u7ba1\u9053\uff0c\u7ed5\u8fc7\u5e8f\u5217\u5316\u548c\u53cd\u5e8f\u5217\u5316\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u96f6\u62f7\u8d1d\u6570\u636e\u4f20\u8f93\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRoadrunner\u5c06\u51fd\u6570\u95f4\u901a\u4fe1\u5ef6\u8fdf\u964d\u4f4e44-89%\uff0c\u51cf\u5c1197%\u7684\u6570\u636e\u4f20\u8f93\u5e8f\u5217\u5316\u5f00\u9500\uff0c\u541e\u5410\u91cf\u63d0\u534769\u500d\u3002", "conclusion": "Roadrunner\u901a\u8fc7\u96f6\u62f7\u8d1d\u548c\u514d\u5e8f\u5217\u5316\u6570\u636e\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u4e86WebAssembly\u65e0\u670d\u52a1\u5668\u51fd\u6570\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u539f\u751f\u5ef6\u8fdf\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.02711", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.02711", "abs": "https://arxiv.org/abs/2511.02711", "authors": ["Daren Chao", "Kaiwen Chen", "Naiqing Guan", "Nick Koudas"], "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data", "comment": null, "summary": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora.", "AI": {"tldr": "ReDD\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u53d1\u73b0\u67e5\u8be2\u7279\u5b9a\u6a21\u5f0f\u3001\u586b\u5145\u5173\u7cfb\u8868\u548c\u4f7f\u7528\u7edf\u8ba1\u6821\u51c6\u7684\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u9519\u8bef\u7387\u4ece30%\u964d\u81f31%\u4ee5\u4e0b\u3002", "motivation": "\u975e\u7ed3\u6784\u5316\u6570\u636e\u666e\u904d\u5b58\u5728\uff0c\u4f46\u5206\u6790\u67e5\u8be2\u9700\u8981\u7ed3\u6784\u5316\u8868\u793a\u3002\u73b0\u6709\u65b9\u6cd5\u5982RAG\u7f3a\u4e4f\u6a21\u5f0f\u610f\u8bc6\u4e14\u96be\u4ee5\u8de8\u6587\u6863\u5bf9\u9f50\uff0c\u5bfc\u81f4\u9ad8\u9519\u8bef\u7387\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a\u8fed\u4ee3\u6a21\u5f0f\u53d1\u73b0(ISD)\u8bc6\u522b\u6700\u5c0f\u53ef\u8fde\u63a5\u6a21\u5f0f\uff0c\u8868\u683c\u6570\u636e\u586b\u5145(TDP)\u4f7f\u7528\u57fa\u4e8eLLM\u9690\u85cf\u72b6\u6001\u7684\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u63d0\u53d6\u548c\u6821\u6b63\u6570\u636e\u3002\u6838\u5fc3\u8d21\u732e\u662fSCAPE\u7edf\u8ba1\u6821\u51c6\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u548cSCAPE-HYB\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cReDD\u5c06\u6570\u636e\u63d0\u53d6\u9519\u8bef\u7387\u4ece\u6700\u9ad830%\u964d\u81f31%\u4ee5\u4e0b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6a21\u5f0f\u5b8c\u6574\u6027(100%\u53ec\u56de\u7387)\u548c\u7cbe\u786e\u5ea6\u3002", "conclusion": "ReDD\u7684\u6a21\u5757\u5316\u8bbe\u8ba1\u80fd\u591f\u7cbe\u7ec6\u63a7\u5236\u51c6\u786e\u6027\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u9ad8\u98ce\u9669\u5206\u6790\u67e5\u8be2\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.02827", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.02827", "abs": "https://arxiv.org/abs/2511.02827", "authors": ["Mohamed Almukhtar", "Anwar Ghammam", "Marouane Kessentini", "Hua Ming"], "title": "From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu", "comment": "Accepted for publication in the proceedings of IEEE/ACM 48th\n  International Conference on Software Engineering", "summary": "In an era shaped by Generative Artificial Intelligence for code generation\nand the rising adoption of Python-based Machine Learning systems (MLS),\nsoftware quality has emerged as a major concern. As these systems grow in\ncomplexity and importance, a key obstacle lies in understanding exactly how\nspecific code changes affect overall quality-a shortfall aggravated by the lack\nof quality assessment tools and a clear mapping between ML systems code changes\nand their quality effects. Although prior work has explored code changes in\nMLS, it mostly stops at what the changes are, leaving a gap in our knowledge of\nthe relationship between code changes and the MLS quality. To address this gap,\nwe conducted a large-scale empirical study of 3,340 open-source Python ML\nprojects, encompassing more than 3.7 million commits and 2.7 trillion lines of\ncode. We introduce PyQu, a novel tool that leverages low level software metrics\nto identify quality-enhancing commits with an average accuracy, precision, and\nrecall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic\nanalysis, we identified 61 code changes, each demonstrating a direct impact on\nenhancing software quality, and we classified them into 13 categories based on\ncontextual characteristics. 41% of the changes are newly discovered by our\nstudy and have not been identified by state-of-the-art Python changes detection\ntools. Our work offers a vital foundation for researchers, practitioners,\neducators, and tool developers, advancing the quest for automated quality\nassessment and best practices in Python-based ML software.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86PyQu\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u67903400\u4e2a\u5f00\u6e90Python\u673a\u5668\u5b66\u4e60\u9879\u76ee\u7684370\u4e07\u6b21\u63d0\u4ea4\uff0c\u8bc6\u522b\u51fa61\u79cd\u80fd\u76f4\u63a5\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u5e76\u5c06\u5176\u5206\u4e3a13\u4e2a\u7c7b\u522b\u3002", "motivation": "\u5728\u751f\u6210\u5f0fAI\u4ee3\u7801\u751f\u6210\u548cPython\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u666e\u53ca\u7684\u80cc\u666f\u4e0b\uff0c\u7f3a\u4e4f\u7406\u89e3\u4ee3\u7801\u53d8\u66f4\u5982\u4f55\u5f71\u54cd\u7cfb\u7edf\u8d28\u91cf\u7684\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u73b0\u6709\u7814\u7a76\u672a\u80fd\u5efa\u7acb\u4ee3\u7801\u53d8\u66f4\u4e0eML\u7cfb\u7edf\u8d28\u91cf\u4e4b\u95f4\u7684\u660e\u786e\u5173\u7cfb\u3002", "method": "\u5bf93400\u4e2a\u5f00\u6e90Python\u673a\u5668\u5b66\u4e60\u9879\u76ee\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u6db5\u76d6370\u4e07\u6b21\u63d0\u4ea4\u548c2.7\u4e07\u4ebf\u884c\u4ee3\u7801\uff0c\u5f00\u53d1PyQu\u5de5\u5177\u5229\u7528\u4f4e\u7ea7\u8f6f\u4ef6\u6307\u6807\u8bc6\u522b\u8d28\u91cf\u63d0\u5347\u63d0\u4ea4\uff0c\u5e76\u8fdb\u884c\u4e3b\u9898\u5206\u6790\u3002", "result": "PyQu\u5de5\u5177\u5728\u8bc6\u522b\u8d28\u91cf\u63d0\u5347\u63d0\u4ea4\u65f6\u5e73\u5747\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u8fbe\u52300.84\uff0c\u5e73\u5747F1\u5206\u65700.85\uff1b\u53d1\u73b0\u4e8661\u79cd\u80fd\u76f4\u63a5\u63d0\u5347\u8f6f\u4ef6\u8d28\u91cf\u7684\u4ee3\u7801\u53d8\u66f4\uff0c\u5176\u4e2d41%\u662f\u73b0\u6709Python\u53d8\u66f4\u68c0\u6d4b\u5de5\u5177\u672a\u80fd\u8bc6\u522b\u7684\u65b0\u53d1\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u4ece\u4e1a\u8005\u3001\u6559\u80b2\u5de5\u4f5c\u8005\u548c\u5de5\u5177\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u63a8\u52a8\u4e86Python\u673a\u5668\u5b66\u4e60\u8f6f\u4ef6\u81ea\u52a8\u5316\u8d28\u91cf\u8bc4\u4f30\u548c\u6700\u4f73\u5b9e\u8df5\u7684\u63a2\u7d22\u3002"}}
{"id": "2511.01893", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.01893", "abs": "https://arxiv.org/abs/2511.01893", "authors": ["Bin Ma", "Viktor Nikitin", "Xi Wang", "Tekin Bicer", "Dong Li"], "title": "mLR: Scalable Laminography Reconstruction based on Memoization", "comment": null, "summary": "ADMM-FFT is an iterative method with high reconstruction accuracy for\nlaminography but suffers from excessive computation time and large memory\nconsumption. We introduce mLR, which employs memoization to replace the\ntime-consuming Fast Fourier Transform (FFT) operations based on an unique\nobservation that similar FFT operations appear in iterations of ADMM-FFT. We\nintroduce a series of techniques to make the application of memoization to\nADMM-FFT performance-beneficial and scalable. We also introduce variable\noffloading to save CPU memory and scale ADMM-FFT across GPUs within and across\nnodes. Using mLR, we are able to scale ADMM-FFT on an input problem of\n2Kx2Kx2K, which is the largest input problem laminography reconstruction has\never worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%\nperformance improvement on average (up to 65.4%), compared to the original\nADMM-FFT.", "AI": {"tldr": "mLR\u901a\u8fc7\u8bb0\u5fc6\u5316\u6280\u672f\u4f18\u5316ADMM-FFT\u7b97\u6cd5\uff0c\u7528\u5b58\u50a8\u7684FFT\u7ed3\u679c\u66ff\u4ee3\u91cd\u590d\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\u5e76\u51cf\u5c11\u5185\u5b58\u6d88\u8017\uff0c\u652f\u6301\u5927\u89c4\u6a21\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u3002", "motivation": "ADMM-FFT\u7b97\u6cd5\u5728\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u4e2d\u7cbe\u5ea6\u9ad8\u4f46\u8ba1\u7b97\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u8fc7\u5927\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u8bb0\u5fc6\u5316\u6280\u672f\u5b58\u50a8\u91cd\u590d\u51fa\u73b0\u7684FFT\u8ba1\u7b97\u7ed3\u679c\uff0c\u5f15\u5165\u53d8\u91cf\u5378\u8f7d\u6280\u672f\u8282\u7701CPU\u5185\u5b58\uff0c\u5e76\u5728\u591aGPU\u8282\u70b9\u95f4\u6269\u5c55ADMM-FFT\u7b97\u6cd5\u3002", "result": "\u6210\u529f\u57282Kx2Kx2K\u7684\u6700\u5927\u89c4\u6a21\u8f93\u5165\u95ee\u9898\u4e0a\u8fd0\u884cADMM-FFT\uff0c\u76f8\u6bd4\u539f\u7b97\u6cd5\u5e73\u5747\u6027\u80fd\u63d0\u534752.8%\uff0c\u6700\u9ad8\u8fbe65.4%\u3002", "conclusion": "mLRA\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86ADMM-FFT\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u66f4\u5927\u89c4\u6a21\u7684\u5c42\u6790\u6210\u50cf\u91cd\u5efa\u95ee\u9898\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.02034", "categories": ["cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.02034", "abs": "https://arxiv.org/abs/2511.02034", "authors": ["Shashank Motepalli", "Naman Garg", "Gengrui Zhang", "Hans-Arno Jacobsen"], "title": "GPoS: Geospatially-aware Proof of Stake", "comment": "Published in ACM TWEB", "summary": "Geospatial decentralization is essential for blockchains, ensuring regulatory\nresilience, robustness, and fairness. We empirically analyze five major Proof\nof Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,\nrevealing that a few geographic regions dominate consensus voting power,\nresulting in limited geospatial decentralization. To address this, we propose\nGeospatially aware Proof of Stake (GPoS), which integrates geospatial diversity\nwith stake-based voting power. Experimental evaluation demonstrates an average\n45% improvement in geospatial decentralization, as measured by the Gini\ncoefficient of Eigenvector centrality, while incurring minimal performance\noverhead in BFT protocols, including HotStuff and CometBFT. These results\ndemonstrate that GPoS can improve geospatial decentralization {while, in our\nexperiments, incurring minimal overhead} to consensus performance.", "AI": {"tldr": "\u63d0\u51faGPoS\u534f\u8bae\uff0c\u901a\u8fc7\u6574\u5408\u5730\u7406\u7a7a\u95f4\u591a\u6837\u6027\u4e0e\u6743\u76ca\u6295\u7968\u6743\uff0c\u663e\u8457\u63d0\u5347PoS\u533a\u5757\u94fe\u7684\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\u7a0b\u5ea6\uff0c\u5e73\u5747\u6539\u558445%\uff0c\u540c\u65f6\u5bf9BFT\u534f\u8bae\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "motivation": "\u5f53\u524d\u4e3b\u8981PoS\u533a\u5757\u94fe\u5b58\u5728\u5730\u7406\u96c6\u4e2d\u95ee\u9898\uff0c\u5c11\u6570\u5730\u533a\u4e3b\u5bfc\u5171\u8bc6\u6295\u7968\u6743\uff0c\u9650\u5236\u4e86\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\uff0c\u5f71\u54cd\u76d1\u7ba1\u97e7\u6027\u3001\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa\u5730\u7406\u7a7a\u95f4\u611f\u77e5\u6743\u76ca\u8bc1\u660e(GPoS)\uff0c\u5c06\u5730\u7406\u7a7a\u95f4\u591a\u6837\u6027\u7eb3\u5165\u6743\u76ca\u6295\u7968\u673a\u5236\uff0c\u5728HotStuff\u548cCometBFT\u7b49BFT\u534f\u8bae\u4e2d\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793aGPoS\u5e73\u5747\u63d0\u534745%\u7684\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\u7a0b\u5ea6\uff08\u57fa\u4e8e\u7279\u5f81\u5411\u91cf\u4e2d\u5fc3\u6027\u7684\u57fa\u5c3c\u7cfb\u6570\uff09\uff0c\u540c\u65f6\u5bf9\u5171\u8bc6\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "GPoS\u80fd\u6709\u6548\u6539\u5584PoS\u533a\u5757\u94fe\u7684\u5730\u7406\u53bb\u4e2d\u5fc3\u5316\uff0c\u4e14\u5728\u5b9e\u8df5\u4e2d\u51e0\u4e4e\u4e0d\u589e\u52a0\u6027\u80fd\u5f00\u9500\u3002"}}
{"id": "2511.02168", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02168", "abs": "https://arxiv.org/abs/2511.02168", "authors": ["Octavian Alexandru Trifan", "Karthik Sangaiah", "Muhammad Awad", "Muhammad Osama", "Sumanth Gudaparthi", "Alexandru Nicolau", "Alexander Veidenbaum", "Ganesh Dasika"], "title": "Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs", "comment": null, "summary": "As large language models (LLMs) continue to scale, their workloads\nincreasingly rely on distributed execution across multiple GPUs. However, the\nconventional bulk synchronous parallel~(BSP) model used in such settings\nintroduces significant performance inefficiencies. To characterize these\nbottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel\nData Locality, and Kernel Launch Overhead) as an analytical framework. We\npropose moving beyond the rigid BSP model to address key inefficiencies in\ndistributed GPU execution. By exploiting libraries like Iris for Triton, we\ngain access to in-kernel communication primitives that enable the design of\nnovel fine-grained programming patterns, offering greater flexibility and\nperformance than traditional BSP-based approaches. These patterns\nsystematically eliminate the three taxes by creating direct, tile-level\nproducer-consumer pipelines and replacing global barriers with fine-grained\ndataflow synchronization. Applying this methodology to critical kernels, from\nthe foundational All-Gather + general matrix multiplication operation to the\ncomplex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end\nlatency over BSP-based approaches, establishing a more programmable and\nefficient paradigm for distributed LLM workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8d85\u8d8a\u4f20\u7edfBSP\u6a21\u578b\u7684\u5206\u5e03\u5f0fGPU\u6267\u884c\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7f16\u7a0b\u6a21\u5f0f\u6d88\u9664\"\u4e09\u5927\u7a0e\u6536\"\u74f6\u9888\uff0c\u5728LLM\u5206\u5e03\u5f0f\u63a8\u7406\u4e2d\u83b7\u5f9710-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u63d0\u5347", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u4f20\u7edfBSP\u6a21\u578b\u5728\u5206\u5e03\u5f0fGPU\u6267\u884c\u4e2d\u5f15\u5165\u663e\u8457\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u6267\u884c\u8303\u5f0f\u6765\u89e3\u51b3\u6548\u7387\u95ee\u9898", "method": "\u5229\u7528Iris for Triton\u5e93\u7684\u5185\u6838\u901a\u4fe1\u539f\u8bed\uff0c\u8bbe\u8ba1\u7ec6\u7c92\u5ea6\u7f16\u7a0b\u6a21\u5f0f\uff0c\u5efa\u7acb\u76f4\u63a5\u7684tile\u7ea7\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u6d41\u6c34\u7ebf\uff0c\u7528\u7ec6\u7c92\u5ea6\u6570\u636e\u6d41\u540c\u6b65\u66ff\u4ee3\u5168\u5c40\u5c4f\u969c", "result": "\u5728\u5173\u952e\u5185\u6838\uff08\u4eceAll-Gather+\u77e9\u9635\u4e58\u6cd5\u5230\u590d\u6742Flash Decode\u7b97\u6cd5\uff09\u4e0a\uff0c\u76f8\u6bd4BSP\u65b9\u6cd5\u5b9e\u73b0\u4e8610-20%\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u52a0\u901f", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u66f4\u53ef\u7f16\u7a0b\u548c\u9ad8\u6548\u7684\u5206\u5e03\u5f0fLLM\u5de5\u4f5c\u8d1f\u8f7d\u6267\u884c\u8303\u5f0f\uff0c\u7cfb\u7edf\u6027\u5730\u6d88\u9664\u4e86\u4e09\u5927\u6027\u80fd\u74f6\u9888"}}
{"id": "2511.02248", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02248", "abs": "https://arxiv.org/abs/2511.02248", "authors": ["Xingqi Cui", "Chieh-Jan Mike Liang", "Jiarong Xing", "Haoran Qiu"], "title": "From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models", "comment": "16 pages, 13 figures", "summary": "Serving large generative models such as LLMs and multi- modal transformers\nrequires balancing user-facing SLOs (e.g., time-to-first-token,\ntime-between-tokens) with provider goals of efficiency and cost reduction.\nExisting solutions rely on static provisioning or model-level autoscaling, both\nof which treat the model as a monolith. This coarse-grained resource management\nleads to degraded performance or significant resource underutilization due to\npoor adaptability to dynamic inference traffic that is common online.\n  The root cause of this inefficiency lies in the internal structure of\ngenerative models: they are executed as graphs of interconnected operators.\nThrough detailed characterization and systematic analysis, we find that\noperators are heterogeneous in their compute and memory footprints and exhibit\ndiverse sensitivity to workload and resource factors such as batch size,\nsequence length, and traffic rate. This heterogeneity suggests that the\noperator, rather than the entire model, is the right granularity for scaling\ndecisions.\n  We propose an operator-level autoscaling framework, which allocates resources\nat finer (operator)-granularity, optimizing the scaling, batching, and\nplacement based on individual operator profiles. Evaluated on production-scale\ntraces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less\nenergy, or under fixed resources achieves 1.6x higher throughput with 5% less\nenergy. These results show that the operator, rather than the model, is\nfundamentally a more effective unit for scaling large generative workloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u5b50\u7ea7\u81ea\u52a8\u6269\u7f29\u5bb9\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8d44\u6e90\u5206\u914d\u4f18\u5316\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u670d\u52a1\u6548\u7387", "motivation": "\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5c06\u6a21\u578b\u89c6\u4e3a\u6574\u4f53\u8fdb\u884c\u9759\u6001\u914d\u7f6e\u6216\u6a21\u578b\u7ea7\u6269\u7f29\u5bb9\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u6216\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u63a8\u7406\u6d41\u91cf", "method": "\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u5185\u90e8\u7b97\u5b50\u56fe\u7ed3\u6784\uff0c\u5206\u6790\u7b97\u5b50\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u7279\u5f81\u5dee\u5f02\uff0c\u63d0\u51fa\u7b97\u5b50\u7ea7\u8d44\u6e90\u5206\u914d\u6846\u67b6\uff0c\u4f18\u5316\u6269\u7f29\u5bb9\u3001\u6279\u5904\u7406\u548c\u90e8\u7f72\u7b56\u7565", "result": "\u5728\u751f\u4ea7\u89c4\u6a21trace\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51cf\u5c1140% GPU\u548c35%\u80fd\u8017\uff0c\u6216\u5728\u56fa\u5b9a\u8d44\u6e90\u4e0b\u63d0\u53471.6\u500d\u541e\u5410\u91cf\u5e76\u964d\u4f4e5%\u80fd\u8017", "conclusion": "\u7b97\u5b50\u6bd4\u6a21\u578b\u66f4\u9002\u5408\u4f5c\u4e3a\u5927\u578b\u751f\u6210\u5de5\u4f5c\u8d1f\u8f7d\u6269\u7f29\u5bb9\u7684\u57fa\u672c\u5355\u5143\uff0c\u80fd\u66f4\u6709\u6548\u5730\u5e73\u8861\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\u548c\u8d44\u6e90\u6548\u7387"}}
{"id": "2511.02257", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02257", "abs": "https://arxiv.org/abs/2511.02257", "authors": ["Oguz Selvitopi", "Emin Ozturk", "Jie Chen", "Ponnuswamy Sadayappan", "Robert G. Edwards", "Ayd\u0131n Bulu\u00e7"], "title": "Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators", "comment": null, "summary": "Computation of correlation functions is a key operation in Lattice quantum\nchromodynamics (LQCD) simulations to extract nuclear physics observables. These\nfunctions involve many binary batch tensor contractions, each tensor possibly\noccupying hundreds of MBs of memory. Performing these contractions on GPU\naccelerators poses the challenge of scheduling them as to optimize tensor reuse\nand reduce data traffic. In this work we propose two fast novel scheduling\nalgorithms that reorder contractions to increase temporal locality via\ninput/intermediate tensor reuse. Our schedulers take advantage of\napplication-specific features, such as contractions being binary and locality\nwithin contraction trees, to optimize the objective of minimizing peak memory.\nWe integrate them into the LQCD analysis software suite Redstar and improve\ntime-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,\nwhich is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data\ntraffic, resulting in upto 1.9x faster correlation function computation time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u8c03\u5ea6\u7b97\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316LQCD\u6a21\u62df\u4e2d\u76f8\u5173\u51fd\u6570\u8ba1\u7b97\u7684\u5185\u5b58\u4f7f\u7528\u548c\u6570\u636e\u4f20\u8f93\u6548\u7387\uff0c\u901a\u8fc7\u91cd\u65b0\u6392\u5e8f\u5f20\u91cf\u6536\u7f29\u64cd\u4f5c\u6765\u63d0\u9ad8\u65f6\u95f4\u5c40\u90e8\u6027\u3002", "motivation": "LQCD\u6a21\u62df\u4e2d\u7684\u76f8\u5173\u51fd\u6570\u8ba1\u7b97\u6d89\u53ca\u5927\u91cf\u4e8c\u8fdb\u5236\u6279\u91cf\u5f20\u91cf\u6536\u7f29\u64cd\u4f5c\uff0c\u6bcf\u4e2a\u5f20\u91cf\u53ef\u80fd\u5360\u7528\u6570\u767eMB\u5185\u5b58\u3002\u5728GPU\u52a0\u901f\u5668\u4e0a\u6267\u884c\u8fd9\u4e9b\u6536\u7f29\u64cd\u4f5c\u65f6\uff0c\u5982\u4f55\u4f18\u5316\u8c03\u5ea6\u4ee5\u63d0\u9ad8\u5f20\u91cf\u91cd\u7528\u7387\u548c\u51cf\u5c11\u6570\u636e\u6d41\u91cf\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u5feb\u901f\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5229\u7528\u5e94\u7528\u7279\u5b9a\u7279\u5f81\uff08\u5982\u4e8c\u8fdb\u5236\u6536\u7f29\u548c\u6536\u7f29\u6811\u5185\u7684\u5c40\u90e8\u6027\uff09\u6765\u91cd\u65b0\u6392\u5e8f\u6536\u7f29\u64cd\u4f5c\uff0c\u901a\u8fc7\u8f93\u5165/\u4e2d\u95f4\u5f20\u91cf\u91cd\u7528\u589e\u52a0\u65f6\u95f4\u5c40\u90e8\u6027\uff0c\u4ee5\u6700\u5c0f\u5316\u5cf0\u503c\u5185\u5b58\u4e3a\u76ee\u6807\u3002", "result": "\u8c03\u5ea6\u5668\u5b9e\u73b0\u4e86\u9ad8\u8fbe2.1\u500d\u7684\u5cf0\u503c\u5185\u5b58\u6539\u8fdb\uff0c\u53cd\u6620\u4e3a\u9ad8\u8fbe4.2\u500d\u7684\u9a71\u9010\u51cf\u5c11\uff0c\u9ad8\u8fbe1.8\u500d\u7684\u6570\u636e\u6d41\u91cf\u51cf\u5c11\uff0c\u6700\u7ec8\u4f7f\u76f8\u5173\u51fd\u6570\u8ba1\u7b97\u65f6\u95f4\u52a0\u5feb\u9ad8\u8fbe1.9\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8c03\u5ea6\u7b97\u6cd5\u6210\u529f\u96c6\u6210\u5230LQCD\u5206\u6790\u8f6f\u4ef6\u5957\u4ef6Redstar\u4e2d\uff0c\u663e\u8457\u6539\u5584\u4e86\u6c42\u89e3\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u63d0\u5347LQCD\u6a21\u62df\u7684\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.02293", "categories": ["cs.DC", "cs.CV", "C.2.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2511.02293", "abs": "https://arxiv.org/abs/2511.02293", "authors": ["Taisuke Noguchi", "Takuya Azumi"], "title": "3D Point Cloud Object Detection on Edge Devices for Split Computing", "comment": "6 pages. This version includes minor lstlisting configuration\n  adjustments for successful compilation. No changes to content or layout.\n  Originally published at ACM/IEEE RAGE 2024", "summary": "The field of autonomous driving technology is rapidly advancing, with deep\nlearning being a key component. Particularly in the field of sensing, 3D point\ncloud data collected by LiDAR is utilized to run deep neural network models for\n3D object detection. However, these state-of-the-art models are complex,\nleading to longer processing times and increased power consumption on edge\ndevices. The objective of this study is to address these issues by leveraging\nSplit Computing, a distributed machine learning inference method. Split\nComputing aims to lessen the computational burden on edge devices, thereby\nreducing processing time and power consumption. Furthermore, it minimizes the\nrisk of data breaches by only transmitting intermediate data from the deep\nneural network model. Experimental results show that splitting after\nvoxelization reduces the inference time by 70.8% and the edge device execution\ntime by 90.0%. When splitting within the network, the inference time is reduced\nby up to 57.1%, and the edge device execution time is reduced by up to 69.5%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5206\u5272\u8ba1\u7b97\u6280\u672f\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u4e2d\u76843D\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5728\u70b9\u4e91\u5904\u7406\u7684\u4e0d\u540c\u9636\u6bb5\u5206\u5272\u795e\u7ecf\u7f51\u7edc\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fb9\u7f18\u8bbe\u5907\u7684\u63a8\u7406\u65f6\u95f4\u548c\u529f\u8017\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u76843D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\uff0c\u5bfc\u81f4\u8fb9\u7f18\u8bbe\u5907\u5904\u7406\u65f6\u95f4\u957f\u3001\u529f\u8017\u5927\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5272\u8ba1\u7b97\u6280\u672f\uff0c\u5728\u70b9\u4e91\u4f53\u7d20\u5316\u540e\u6216\u7f51\u7edc\u5185\u90e8\u8fdb\u884c\u5206\u5272\uff0c\u5c06\u90e8\u5206\u8ba1\u7b97\u4efb\u52a1\u5206\u914d\u5230\u4e91\u7aef\uff0c\u51cf\u5c11\u8fb9\u7f18\u8bbe\u5907\u8d1f\u62c5\u3002", "result": "\u5728\u4f53\u7d20\u5316\u540e\u5206\u5272\u53ef\u4f7f\u63a8\u7406\u65f6\u95f4\u51cf\u5c1170.8%\uff0c\u8fb9\u7f18\u8bbe\u5907\u6267\u884c\u65f6\u95f4\u51cf\u5c1190.0%\uff1b\u5728\u7f51\u7edc\u5185\u90e8\u5206\u5272\u53ef\u4f7f\u63a8\u7406\u65f6\u95f4\u51cf\u5c1157.1%\uff0c\u8fb9\u7f18\u8bbe\u5907\u6267\u884c\u65f6\u95f4\u51cf\u5c1169.5%\u3002", "conclusion": "\u5206\u5272\u8ba1\u7b97\u80fd\u6709\u6548\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a763D\u76ee\u6807\u68c0\u6d4b\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u63d0\u9ad8\u8fb9\u7f18\u8bbe\u5907\u6027\u80fd\uff0c\u540c\u65f6\u901a\u8fc7\u4f20\u8f93\u4e2d\u95f4\u6570\u636e\u51cf\u5c11\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002"}}
{"id": "2511.02647", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02647", "abs": "https://arxiv.org/abs/2511.02647", "authors": ["Xiumei Deng", "Zehui Xiong", "Binbin Chen", "Dong In Kim", "Merouane Debbah", "H. Vincent Poor"], "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks", "comment": null, "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.", "AI": {"tldr": "FedAttn\u662f\u4e00\u4e2a\u5c06\u8054\u90a6\u5b66\u4e60\u8303\u5f0f\u96c6\u6210\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u5206\u5e03\u5f0fLLM\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5730\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u5468\u671f\u6027KV\u77e9\u9635\u4ea4\u6362\u805a\u5408\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3001\u901a\u4fe1\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u90e8\u7f72LLM\u5728\u534f\u4f5c\u573a\u666f\u4e2d\u7684\u9690\u79c1\u6f0f\u6d1e\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8ba1\u7b97\u74f6\u9888\u7b49\u6311\u6218\u3002", "method": "\u5c06\u8054\u90a6\u5b66\u4e60\u8303\u5f0f\u96c6\u6210\u5230\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u53c2\u4e0e\u8005\u6267\u884c\u672c\u5730\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u5b9a\u671f\u4ea4\u6362\u548c\u805a\u5408KV\u77e9\u9635\uff0c\u57fa\u4e8e\u7ed3\u6784\u5bf9\u5076\u6027\u5c06\u8054\u90a6\u4f18\u5316\u6280\u672f\u79fb\u690d\u5230\u534f\u4f5cLLM\u63a8\u7406\u3002", "result": "\u7406\u8bba\u5206\u6790\u4e86\u672c\u5730\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u548c\u5f02\u6784\u4ee4\u724c\u76f8\u5173\u6027\u5982\u4f55\u5f71\u54cdTransformer\u5757\u95f4\u7684\u8bef\u5dee\u4f20\u64ad\u52a8\u6001\uff0c\u63ed\u793a\u4e86\u54cd\u5e94\u8d28\u91cf\u4e0e\u901a\u4fe1/\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u7a00\u758f\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94KV\u805a\u5408\u7684\u4f18\u5316\u673a\u4f1a\u3002", "conclusion": "FedAttn\u4e3a\u5b9e\u9645\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6f5c\u529b\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u3001\u901a\u4fe1\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.02655", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2511.02655", "abs": "https://arxiv.org/abs/2511.02655", "authors": ["Johansell Villalobos", "Josef Ruzicka", "Silvio Rizzi"], "title": "Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks", "comment": null, "summary": "Scientific computing in the exascale era demands increased computational\npower to solve complex problems across various domains. With the rise of\nheterogeneous computing architectures the need for vendor-agnostic, performance\nportability frameworks has been highlighted. Libraries like Kokkos have become\nessential for enabling high-performance computing applications to execute\nefficiently across different hardware platforms with minimal code changes. In\nthis direction, this paper presents preliminary time-to-solution results for\ntwo representative scientific computing applications: an N-body simulation and\na structured grid simulation. Both applications used a distributed memory\napproach and hardware acceleration through four performance portability\nframeworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single\nnode of the Polaris supercomputer using four NVIDIA A100 GPUs revealed\nsignificant performance variability among frameworks. OCCA demonstrated faster\nexecution times for small-scale validation problems, likely due to JIT\ncompilation, however its lack of optimized reduction algorithms may limit\nscalability for larger simulations while using its out of the box API. OpenMP\nperformed poorly in the structured grid simulation most likely due to\ninefficiencies in inter-node data synchronization and communication. These\nfindings highlight the need for further optimization to maximize each\nframework's capabilities. Future work will focus on enhancing reduction\nalgorithms, data communication, memory management, as wells as performing\nscalability studies, and a comprehensive statistical analysis to evaluate and\ncompare framework performance.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6846\u67b6\uff08Kokkos\u3001OpenMP\u3001RAJA\u3001OCCA\uff09\u5728N\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u6846\u67b6\u5728Polaris\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u968f\u7740\u5f02\u6784\u8ba1\u7b97\u67b6\u6784\u7684\u5174\u8d77\uff0c\u79d1\u5b66\u8ba1\u7b97\u9700\u8981\u8de8\u786c\u4ef6\u5e73\u53f0\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u4ee3\u7801\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u7684\u9ad8\u6548\u6267\u884c\u3002", "method": "\u5728Polaris\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684\u5355\u4e2a\u8282\u70b9\u4e0a\uff0c\u4f7f\u7528\u56db\u4e2aNVIDIA A100 GPU\uff0c\u901a\u8fc7\u56db\u79cd\u6027\u80fd\u53ef\u79fb\u690d\u6027\u6846\u67b6\uff08Kokkos\u3001OpenMP\u3001RAJA\u3001OCCA\uff09\u8fd0\u884cN\u4f53\u6a21\u62df\u548c\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u5185\u5b58\u65b9\u6cd5\u548c\u786c\u4ef6\u52a0\u901f\u3002", "result": "\u4e0d\u540c\u6846\u67b6\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff1aOCCA\u5728\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u95ee\u9898\u4e0a\u6267\u884c\u66f4\u5feb\uff08\u53ef\u80fd\u7531\u4e8eJIT\u7f16\u8bd1\uff09\uff0c\u4f46\u5176\u7f3a\u4e4f\u4f18\u5316\u7684\u5f52\u7ea6\u7b97\u6cd5\u53ef\u80fd\u9650\u5236\u5927\u89c4\u6a21\u6a21\u62df\u7684\u53ef\u6269\u5c55\u6027\uff1bOpenMP\u5728\u7ed3\u6784\u5316\u7f51\u683c\u6a21\u62df\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u53ef\u80fd\u7531\u4e8e\u8282\u70b9\u95f4\u6570\u636e\u540c\u6b65\u548c\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u3002", "conclusion": "\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u6700\u5927\u5316\u5404\u6846\u67b6\u6027\u80fd\uff0c\u672a\u6765\u5de5\u4f5c\u5c06\u91cd\u70b9\u6539\u8fdb\u5f52\u7ea6\u7b97\u6cd5\u3001\u6570\u636e\u901a\u4fe1\u3001\u5185\u5b58\u7ba1\u7406\uff0c\u5e76\u8fdb\u884c\u53ef\u6269\u5c55\u6027\u7814\u7a76\u548c\u5168\u9762\u7684\u7edf\u8ba1\u5206\u6790\u6765\u8bc4\u4f30\u6bd4\u8f83\u6846\u67b6\u6027\u80fd\u3002"}}
{"id": "2511.02743", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.02743", "abs": "https://arxiv.org/abs/2511.02743", "authors": ["Fedor Ryabinin", "Alexey Gotsman", "Pierre Sutra"], "title": "Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)", "comment": "Extended version of a paper in OPODIS'25: International Conference on\n  Principles of Distributed Systems", "summary": "Classical state-machine replication protocols, such as Paxos, rely on a\ndistinguished leader process to order commands. Unfortunately, this approach\nmakes the leader a single point of failure and increases the latency for\nclients that are not co-located with it. As a response to these drawbacks,\nEgalitarian Paxos introduced an alternative, leaderless approach, that allows\nreplicas to order commands collaboratively. Not relying on a single leader\nallows the protocol to maintain non-zero throughput with up to $f$ crashes of\nany processes out of a total of $n = 2f+1$. The protocol furthermore allows any\nprocess to execute a command $c$ fast, in $2$ message delays, provided no more\nthan $e = \\lceil\\frac{f+1}{2}\\rceil$ other processes fail, and all concurrently\nsubmitted commands commute with $c$; the latter condition is often satisfied in\npractical systems.\n  Egalitarian Paxos has served as a foundation for many other replication\nprotocols. But unfortunately, the protocol is very complex, ambiguously\nspecified and suffers from nontrivial bugs. In this paper, we present EPaxos*\n-- a simpler and correct variant of Egalitarian Paxos. Our key technical\ncontribution is a simpler failure-recovery algorithm, which we have rigorously\nproved correct. Our protocol also generalizes Egalitarian Paxos to cover the\nwhole spectrum of failure thresholds $f$ and $e$ such that $n \\ge \\max\\{2e+f-1,\n2f+1\\}$ -- the number of processes that we show to be optimal.", "AI": {"tldr": "EPaxos*\u662f\u4e00\u4e2a\u7b80\u5316\u4e14\u6b63\u786e\u7684Egalitarian Paxos\u53d8\u4f53\uff0c\u901a\u8fc7\u66f4\u7b80\u5355\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\u89e3\u51b3\u4e86\u539f\u534f\u8bae\u7684\u590d\u6742\u6027\u548c\u9519\u8bef\u95ee\u9898\uff0c\u5e76\u6269\u5c55\u5230\u6700\u4f18\u8fdb\u7a0b\u6570\u8303\u56f4\u3002", "motivation": "\u89e3\u51b3Egalitarian Paxos\u534f\u8bae\u7684\u590d\u6742\u6027\u3001\u6a21\u7cca\u89c4\u8303\u548c\u4e25\u91cd\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7b80\u5355\u3001\u6b63\u786e\u7684\u65e0\u9886\u5bfc\u8005\u72b6\u6001\u673a\u590d\u5236\u534f\u8bae\u3002", "method": "\u63d0\u51faEPaxos*\u534f\u8bae\uff0c\u4f7f\u7528\u7b80\u5316\u7684\u6545\u969c\u6062\u590d\u7b97\u6cd5\uff0c\u5e76\u6269\u5c55\u5230\u652f\u6301n \u2265 max{2e+f-1, 2f+1}\u7684\u6700\u4f18\u8fdb\u7a0b\u6570\u914d\u7f6e\u3002", "result": "\u5f00\u53d1\u4e86\u7ecf\u8fc7\u4e25\u683c\u8bc1\u660e\u6b63\u786e\u7684\u534f\u8bae\u53d8\u4f53\uff0c\u5728\u4fdd\u6301\u539f\u534f\u8bae\u4f18\u70b9\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u590d\u6742\u6027\u548c\u6b63\u786e\u6027\u95ee\u9898\u3002", "conclusion": "EPaxos*\u6210\u529f\u7b80\u5316\u4e86Egalitarian Paxos\uff0c\u63d0\u4f9b\u4e86\u6b63\u786e\u6027\u4fdd\u8bc1\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u4f18\u7684\u8fdb\u7a0b\u6570\u914d\u7f6e\u8303\u56f4\u3002"}}
