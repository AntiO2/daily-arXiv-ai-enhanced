<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 19]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Putting green software principles into practice](https://arxiv.org/abs/2601.09741)
*James Uther*

Main category: cs.SE

TL;DR: 本文介绍了一个在公有云上运行的实时产品向绿色软件转型的实践案例，重点探讨了利用无服务器系统的成本影响来驱动效率提升的具体解决方案，并总结了在该项目中有效的绿色软件原则。


<details>
  <summary>Details</summary>
Motivation: 尽管测量和减少计算系统碳排放的需求和理论方法已被充分理解，但现实世界中的实际案例仍然有限。本文旨在填补这一空白，通过一个真实产品的绿色转型实践，为业界提供可借鉴的经验。

Method: 采用实践导向的方法，描述了一个在公有云上运行的实时产品向绿色软件转型的完整过程。特别关注利用无服务器系统的成本影响来驱动效率提升，通过成本优化间接实现碳排放减少。

Result: 成功实施了绿色软件转型，找到了具体的实践解决方案，特别是通过无服务器系统的成本优化实现了效率提升。项目最终总结出了一套在该项目中有效的绿色软件原则。

Conclusion: 通过实际案例证明，利用无服务器系统的成本影响可以有效驱动绿色软件转型。本文总结的绿色软件原则为类似项目提供了有价值的实践指导，强调了成本优化与碳排放减少之间的协同效应。

Abstract: The need and theoretical methods for measuring and reducing CO2 emitted by computing systems are well understood, but real-world examples are still limited. We describe a journey towards green software for a live product running on a public cloud. We discuss practical solutions found, in particular using the cost implications of serverless systems to drive efficiency. We end with some `green software' principles that worked well in this project.

</details>


### [2] [A Governance Model for IoT Data in Global Manufacturing](https://arxiv.org/abs/2601.09744)
*Vignesh Alagappan*

Main category: cs.SE

TL;DR: 提出了一种针对全球制造业物联网平台的联邦治理模型，强调契约驱动的互操作性、策略即代码执行和以资产为中心的责任制，以解决分布式运营环境下的数据治理挑战。


<details>
  <summary>Details</summary>
Motivation: 全球制造业环境中，工业物联网平台产生大量运营数据，虽然数据摄取和存储技术已成熟，但企业在规模化治理物联网数据方面仍面临系统性挑战。这些挑战并非源于工具限制，而是缺乏与分布式运营所有权、异构源系统和边缘持续变化现实相匹配的治理模型。

Method: 提出联邦治理模型，强调契约驱动的互操作性、策略即代码执行和以资产为中心的责任制。该模型在架构边界实施治理，实现语义一致性、质量保证和法规遵从，无需集中控制运营技术系统。基于制造业物联网需求和约束分析，贡献了系统架构和设计框架。

Result: 提出了一个完整的治理模型和设计框架，能够解决分布式制造环境中的物联网数据治理挑战。该模型支持语义一致性、质量保证和法规遵从，同时适应边缘环境的持续变化。

Conclusion: 该联邦治理模型为全球制造业组织提供了有效的物联网数据治理解决方案，通过契约驱动、策略即代码和资产中心化的方法，在保持分布式运营自主性的同时实现治理目标。实证验证是未来的工作方向。

Abstract: Industrial IoT platforms in global manufacturing environments generate continuous operational data across production assets, utilities, and connected products. While data ingestion and storage capabilities have matured significantly, enterprises continue to face systemic challenges in governing IoT data at scale. These challenges are not rooted in tooling limitations but in the absence of a governance model that aligns with the realities of distributed operational ownership, heterogeneous source systems, and continuous change at the edge. This paper presents a federated governance model that emphasizes contract-driven interoperability, policy-as-code enforcement, and asset-centric accountability across global manufacturing organizations. The model addresses governance enforcement at architectural boundaries, enabling semantic consistency, quality assurance, and regulatory compliance without requiring centralized control of operational technology systems. This work contributes a systems architecture and design framework grounded in analysis of manufacturing IoT requirements and constraints; empirical validation remains future work

</details>


### [3] [Enhancing Formal Software Specification with Artificial Intelligence](https://arxiv.org/abs/2601.09745)
*Antonio Abu Nassar,Eitan Farchi*

Main category: cs.SE

TL;DR: 利用AI增强的自然语言+轻量数学符号作为中间规范语言，显著降低形式化规范成本，实现早期验证和正确性设计


<details>
  <summary>Details</summary>
Motivation: 传统形式化软件规范虽然能早期检测错误和明确不变量，但由于高符号开销和专业知识要求，工业采用有限。需要找到平衡形式化严谨性和实用性的方法。

Method: 使用自然语言增强轻量数学符号（LaTeX格式）作为中间规范语言，通过AI进行评审和精炼，然后生成代码。区分系统分析师需要控制的部分（受益于形式化规范）和无需控制的部分。

Result: 应用于组织知识增长的非平凡模拟中，该方法实现了早期验证、明确不变量和正确性设计，显著减少开发工作量，并首次尝试就产生正确实现。

Conclusion: AI进步使得在保留形式化规范许多优点的同时大幅降低成本成为可能，自然语言+轻量数学符号的中间规范语言是实用且有效的解决方案。

Abstract: Formal software specification is known to enable early error detection and explicit invariants, yet it has seen limited industrial adoption due to its high notation overhead and the expertise required to use traditional formal languages. This paper presents a case study showing that recent advances in artificial intelligence make it possible to retain many of the benefits of formal specification while substantially reducing these costs. The necessity of a clear distinction between what is controlled by the system analyst and can highly benefits from the rigor of formal specification and what need not be controlled is demonstrated. We use natural language augmented with lightweight mathematical notation and written in \LaTeX\ as an intermediate specification language, which is reviewed and refined by AI prior to code generation. Applied to a nontrivial simulation of organizational knowledge growth, this approach enables early validation, explicit invariants, and correctness by design, while significantly reducing development effort and producing a correct implementation on the first attempt.

</details>


### [4] [R-LAM: Reproducibility-Constrained Large Action Models for Scientific Workflow Automation](https://arxiv.org/abs/2601.09749)
*Suriya Sureshkumar*

Main category: cs.SE

TL;DR: R-LAM是一个为科学工作流自动化设计的可重现性约束框架，通过结构化动作模式、确定性执行策略和显式溯源跟踪，解决了大型动作模型在科学应用中缺乏可重现性、可审计性和确定性执行的问题。


<details>
  <summary>Details</summary>
Motivation: 大型动作模型在科学工作流自动化中具有潜力，但科学工作流对可重现性、可审计性和确定性执行有严格要求，而通用的基于LLM的智能体无法满足这些要求。无约束的动作生成会导致静默状态变化、非确定性执行和不可重现的实验结果，限制了LAMs在科学环境中的应用。

Method: R-LAM框架引入了结构化动作模式、确定性执行策略和显式溯源跟踪，确保每个动作和中间产物都可审计和可重放。框架支持故障感知执行循环和受控工作流分叉，使迭代实验不会损害可重现性。实现为轻量级Python框架并作为开源PyPI包发布。

Result: 对代表性科学工作流的实验评估表明，与无约束的基于LLM的智能体相比，R-LAM提高了可重现性成功率和执行可靠性，同时保持了对工作流执行的自适应控制。

Conclusion: R-LAM为科学工作流自动化提供了一个可重现性约束框架，通过结构化动作、确定性执行和溯源跟踪解决了LAMs在科学应用中的关键限制，促进了可重现研究。

Abstract: Large Action Models (LAMs) extend large language models by enabling autonomous decision-making and tool execution, making them promising for automating scientific workflows. However, scientific workflows impose strict requirements on reproducibility, auditability, and deterministic execution, which are not satisfied by generic LLM-based agents. Unconstrained action generation can lead to silent state changes, non-deterministic executions, and irreproducible experimental results, limiting the applicability of LAMs in scientific settings.
  In this paper, we propose R-LAM, a reproducibility-constrained framework for applying Large Action Models to scientific workflow automation. R-LAM introduces structured action schemas, deterministic execution policies, and explicit provenance tracking to ensure that every action and intermediate artifact is auditable and replayable. The framework supports failure-aware execution loops and controlled workflow forking, enabling iterative experimentation without compromising reproducibility.
  We implement R-LAM as a lightweight Python framework and release it as an open-source PyPI package to facilitate reproducible research. An experimental evaluation of representative scientific workflows demonstrates that R-LAM improves reproducibility success rates and execution reliability compared to unconstrained LLM-based agents, while retaining adaptive control over workflow execution.

</details>


### [5] [SAGE: Tool-Augmented LLM Task Solving Strategies in Scalable Multi-Agent Environments](https://arxiv.org/abs/2601.09750)
*Robert K. Strehlow,Tobias Küster,Oskar F. Kupke,Brandon Llanque Kurps,Fikret Sivrikaya,Sahin Albayrak*

Main category: cs.SE

TL;DR: SAGE是一个基于OPACA框架的对话AI接口，专注于动态工具集成和零样本提示方法，通过模块化设计支持多种模型和代理策略。


<details>
  <summary>Details</summary>
Motivation: 现实应用中LLM需要访问实时信息和工具，但现有工具定义固定且难以集成新工具，特别是领域特定工具。需要动态工具集成和鲁棒的零样本提示方法。

Method: 基于OPACA框架构建SAGE对话AI接口，支持动态工具发现和执行，提供模块化架构，可切换不同模型（GPT、LLAMA等），集成多种提示方法和代理策略。

Result: 实现了多种任务解决策略，在综合基准服务集上评估，结果显示了不同策略的优缺点，整体表现有前景。

Conclusion: SAGE和OPACA框架提供了动态工具集成和模块化AI对话解决方案，支持多种模型和代理策略，开源可用。

Abstract: Large language models (LLMs) have proven to work well in question-answering scenarios, but real-world applications often require access to tools for live information or actuation. For this, LLMs can be extended with tools, which are often defined in advance, also allowing for some fine-tuning for specific use cases. However, rapidly evolving software landscapes and individual services require the constant development and integration of new tools. Domain- or company-specific tools can greatly elevate the usefulness of an LLM, but such custom tools can be problematic to integrate, or the LLM may fail to reliably understand and use them. For this, we need strategies to define new tools and integrate them into the LLM dynamically, as well as robust and scalable zero-shot prompting methods that can make use of those tools in an efficient manner. In this paper, we present SAGE, a specialized conversational AI interface, based on the OPACA framework for tool discovery and execution. The integration with OPACA makes it easy to add new tools or services for the LLM to use, while SAGE itself presents rich extensibility and modularity. This not only provides the ability to seamlessly switch between different models (e.g. GPT, LLAMA), but also to add and select prompting methods, involving various setups of differently prompted agents for selecting and executing tools and evaluating the results. We implemented a number of task-solving strategies, making use of agentic concepts and prompting methods in various degrees of complexity, and evaluated those against a comprehensive set of benchmark services. The results are promising and highlight the distinct strengths and weaknesses of different task-solving strategies. Both SAGE and the OPACA framework, as well as the different benchmark services and results, are available as Open Source/Open Data on GitHub.

</details>


### [6] [Investigating Tool-Memory Conflicts in Tool-Augmented LLMs](https://arxiv.org/abs/2601.09760)
*Jiali Cheng,Rui Pan,Hadi Amiri*

Main category: cs.SE

TL;DR: 论文提出了一种新的知识冲突类型——工具-记忆冲突(TMC)，即工具增强型大语言模型中内部参数知识与外部工具知识之间的矛盾，发现现有LLMs在STEM任务上尤其容易受此影响，且现有冲突解决技术效果有限。


<details>
  <summary>Details</summary>
Motivation: 工具增强型大语言模型虽然应用广泛，但容易受到知识冲突的影响。作者发现现有研究尚未充分探讨一种特定类型的知识冲突：内部参数知识与外部工具知识之间的矛盾，这种冲突在STEM相关任务中尤为突出，需要系统性的研究和解决方案。

Method: 1. 提出工具-记忆冲突(TMC)的新概念；2. 评估现有LLMs在TMC方面的表现，特别关注STEM任务；3. 分析不同条件下工具知识和参数知识的优先级差异；4. 评估现有的冲突解决技术，包括基于提示的方法和基于RAG的方法。

Result: 1. 现有LLMs确实存在工具-记忆冲突问题，在STEM任务上表现尤为明显；2. 在不同条件下，工具知识和参数知识会被优先考虑的情况不同；3. 现有的冲突解决技术（包括提示方法和RAG方法）都无法有效解决工具-记忆冲突。

Conclusion: 工具-记忆冲突是工具增强型大语言模型中一个尚未解决的重要问题，现有方法无法有效应对这一挑战，需要开发新的技术来解决内部参数知识与外部工具知识之间的矛盾，特别是在STEM领域。

Abstract: Tool-augmented large language models (LLMs) have powered many applications. However, they are likely to suffer from knowledge conflict. In this paper, we propose a new type of knowledge conflict -- Tool-Memory Conflict (TMC), where the internal parametric knowledge contradicts with the external tool knowledge for tool-augmented LLMs. We find that existing LLMs, though powerful, suffer from TMC, especially on STEM-related tasks. We also uncover that under different conditions, tool knowledge and parametric knowledge may be prioritized differently. We then evaluate existing conflict resolving techniques, including prompting-based and RAG-based methods. Results show that none of these approaches can effectively resolve tool-memory conflicts.

</details>


### [7] [Explicating Tacit Regulatory Knowledge from LLMs to Auto-Formalize Requirements for Compliance Test Case Generation](https://arxiv.org/abs/2601.09762)
*Zhiyi Xue,Xiaohong Chen,Min Zhang*

Main category: cs.SE

TL;DR: RAFT框架通过多LLM自适应净化聚合策略，自动形式化需求并生成合规测试，在金融、汽车、电力领域达到专家水平性能。


<details>
  <summary>Details</summary>
Motivation: 高度监管领域的合规测试主要依赖人工，需要专家将复杂法规转化为可执行测试用例。虽然大语言模型有自动化潜力，但其幻觉问题限制了可靠应用。现有混合方法通过形式化模型约束LLM，但仍依赖昂贵的人工建模。

Method: RAFT采用自适应净化-聚合策略，从多个LLM中显式提取隐含的监管知识，并整合为三个制品：领域元模型、形式化需求表示和可测试性约束。这些制品动态注入提示词，指导高精度需求形式化和自动化测试生成。

Result: 在金融、汽车和电力领域的实验表明，RAFT达到专家级性能，显著优于现有最先进方法，同时减少了整体生成和审查时间。

Conclusion: RAFT框架通过自动形式化需求和生成合规测试，解决了监管领域合规测试自动化中的幻觉问题，减少了人工建模成本，实现了高效可靠的合规测试自动化。

Abstract: Compliance testing in highly regulated domains is crucial but largely manual, requiring domain experts to translate complex regulations into executable test cases. While large language models (LLMs) show promise for automation, their susceptibility to hallucinations limits reliable application. Existing hybrid approaches mitigate this issue by constraining LLMs with formal models, but still rely on costly manual modeling. To solve this problem, this paper proposes RAFT, a framework for requirements auto-formalization and compliance test generation via explicating tacit regulatory knowledge from multiple LLMs. RAFT employs an Adaptive Purification-Aggregation strategy to explicate tacit regulatory knowledge from multiple LLMs and integrate it into three artifacts: a domain meta-model, a formal requirements representation, and testability constraints. These artifacts are then dynamically injected into prompts to guide high-precision requirement formalization and automated test generation. Experiments across financial, automotive, and power domains show that RAFT achieves expert-level performance, substantially outperforms state-of-the-art (SOTA) methods while reducing overall generation and review time.

</details>


### [8] [LLM-Based Agentic Systems for Software Engineering: Challenges and Opportunities](https://arxiv.org/abs/2601.09822)
*Yongjian Tang,Thomas Runkler*

Main category: cs.SE

TL;DR: 本文系统综述了基于大语言模型的多智能体系统在软件工程全生命周期中的应用，探讨了语言模型选择、评估基准、框架协议等关键话题，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型取得了进展，但复杂的软件工程任务需要更协作和专门化的方法。当前缺乏对LLM-based多智能体系统在软件工程领域应用的系统性综述，需要为研究者和实践者提供该前沿领域的全面洞察。

Method: 采用系统性文献综述方法，分析LLM-based多智能体系统在软件开发生命周期各阶段的应用，包括需求工程、代码生成、静态代码检查、测试和调试等。深入探讨语言模型选择、软件工程评估基准、最先进的智能体框架和通信协议等主题。

Result: 系统梳理了LLM-based多智能体系统在软件工程领域的应用现状，识别了该领域的关键挑战，包括多智能体协调、人机协作、计算成本优化和有效数据收集等方面。

Conclusion: 本文为研究者和实践者提供了软件工程领域智能体系统前沿景观的宝贵见解，并指出了未来研究方向，包括多智能体编排、人机协调、计算成本优化和有效数据收集等关键问题。

Abstract: Despite recent advancements in Large Language Models (LLMs), complex Software Engineering (SE) tasks require more collaborative and specialized approaches. This concept paper systematically reviews the emerging paradigm of LLM-based multi-agent systems, examining their applications across the Software Development Life Cycle (SDLC), from requirements engineering and code generation to static code checking, testing, and debugging. We delve into a wide range of topics such as language model selection, SE evaluation benchmarks, state-of-the-art agentic frameworks and communication protocols. Furthermore, we identify key challenges and outline future research opportunities, with a focus on multi-agent orchestration, human-agent coordination, computational cost optimization, and effective data collection. This work aims to provide researchers and practitioners with valuable insights into the current forefront landscape of agentic systems within the software engineering domain.

</details>


### [9] [Adoption and Evolution of Code Style and Best Programming Practices in Open-Source Projects](https://arxiv.org/abs/2601.09832)
*Alvari Kupari,Nasser Giacaman,Valerio Terragni*

Main category: cs.SE

TL;DR: 分析1036个GitHub热门Java项目的代码风格和编程实践采用情况，发现普遍存在违反规范的现象，Javadoc和命名违规最常见，Google Java风格指南违规较多，声称遵循代码风格的项目合规性略高。


<details>
  <summary>Details</summary>
Motivation: 代码风格规范对软件质量至关重要，但实际采用情况缺乏大规模实证研究。本文旨在通过分析开源Java项目，了解代码风格和编程实践的采用程度、演变趋势以及常见违规类型。

Method: 分析1036个GitHub热门Java项目，研究代码风格和编程实践的采用与演变；对活跃仓库进行月度跟踪，监测编码标准遵循度的变化；识别最常见的违规类型和模式。

Result: 发现普遍存在代码风格违规，Javadoc和命名违规最常见；Google Java风格指南违规较多，且现代静态分析工具常忽略这些类别；声称遵循代码风格的项目合规性略高。

Conclusion: 研究揭示了开源Java项目中代码风格实践的采用现状，指出了改进的关键领域，为提升代码质量提供了重要见解，并提出了未来改进方向。

Abstract: Following code style conventions in software projects is essential for maintaining overall code quality. Adhering to these conventions improves maintainability, understandability, and extensibility. Additionally, following best practices during software development enhances performance and reduces the likelihood of errors. This paper analyzes 1,036 popular open-source JAVA projects on GITHUB to study how code style and programming practices are adopted and evolve over time, examining their prevalence and the most common violations. Additionally, we study a subset of active repositories on a monthly basis to track changes in adherence to coding standards over time. We found widespread violations across repositories, with Javadoc and Naming violations being the most common. We also found a significant number of violations of the GOOGLE Java Style Guide in categories often missed by modern static analysis tools. Furthermore, repositories claiming to follow code-style practices exhibited slightly higher overall adherence to code-style and best-practices. The results provide valuable insights into the adoption of code style and programming practices, highlighting key areas for improvement in the open-source development community. Furthermore, the paper identifies important lessons learned and suggests future directions for improving code quality in JAVA projects.

</details>


### [10] [On Fun for Teaching Large Programming Courses](https://arxiv.org/abs/2601.09842)
*Walid Maalej*

Main category: cs.SE

TL;DR: 该研究提出了一套10个物理趣味活动，用于在大型软件工程课程中提高学生参与度和概念理解，通过三年500+学生实践和访谈研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统大型讲座教学中，学生容易感到无聊、分心和脱离，教师也难以进行有效的互动。需要创新方法来激活学生，帮助他们记忆和反思抽象的软件开发概念。

Method: 开发了10个物理趣味活动目录，包括LA-OLA算法执行、纸飞机模拟对象消息和指针、在教室中遍历树或递归结构等。在500+学生的大型课程中连续三年实践，并对15名前学生和14名全球经验丰富的教育工作者进行访谈研究。

Result: 趣味活动能帮助学生保持专注、记忆关键概念并进行课后反思。但活动的简洁性和与教学概念的明确关联性对其接受度和有效性至关重要。

Conclusion: 物理趣味活动是提高大型软件工程课程教学效果的有效方法，但需要精心设计以确保活动简洁且与学习目标紧密相关。

Abstract: Teaching software development basics to hundreds of students in a frontal setting is cost-efficient and thus still common in universities. However, in a large lecture hall, students can easily get bored, distracted, and disengaged. The frontal setting can also frustrate lecturers since interaction opportunities are limited and hard to scale. Fun activities can activate students and, if well designed, can also help remember and reflect on abstract software development concepts. We present a novel catalogue of ten physical fun activities, developed over years to reflect on basic programming and software development concepts. The catalogue includes the execution of a LA-OLA algorithm as in stadiums, using paper planes to simulate object messages and pointers, and traversing a lecture hall as a tree or a recursive structure. We report our experience of using the activities in a large course with 500+ students three years in a row. We also conducted an interview study with 15 former students of the course and 14 experienced educators from around the globe. The results suggest that the fun activities can enable students to stay focused, remember key concepts, and reflect afterwards. However, keeping the activities concise and clearly linked to the concepts taught seems to be key to their acceptance and effectiveness.

</details>


### [11] [Beyond Strict Rules: Assessing the Effectiveness of Large Language Models for Code Smell Detection](https://arxiv.org/abs/2601.09873)
*Saymon Souza,Amanda Santana,Eduardo Figueiredo,Igor Muzetti,João Eduardo Montandon,Lionel Briand*

Main category: cs.SE

TL;DR: 评估四种大语言模型（DeepSeek-R1、GPT-5 mini、Llama-3.3、Qwen2.5-Code）在检测9种代码异味中的效果，并提出结合LLM与静态分析工具的混合策略，在5种异味上F1分数优于单独方法。


<details>
  <summary>Details</summary>
Motivation: 代码异味影响软件可维护性和可靠性，传统静态分析工具规则僵化，而LLM具有灵活性和适应性，但其在代码异味检测方面的应用尚未充分探索。

Method: 1) 评估四种LLM在30个Java项目中检测9种代码异味的效果；2) 创建包含268个代码异味候选的基准数据集，由76名开发者人工标注；3) 提出并评估结合LLM与静态分析工具的混合检测策略。

Result: 1) LLM在结构简单的异味（如Large Class、Long Method）上表现良好；2) 不同LLM和工具在不同异味上各有优势；3) 混合策略在9种异味中的5种上F1分数优于单独使用LLM或工具，但对复杂异味会产生更多误报。

Conclusion: 最优检测策略取决于检测目标是优先召回率还是精确度。混合策略在多数情况下表现更好，但需要权衡误报率。

Abstract: Code smells are symptoms of potential code quality problems that may affect software maintainability, thus increasing development costs and impacting software reliability. Large language models (LLMs) have shown remarkable capabilities for supporting various software engineering activities, but their use for detecting code smells remains underexplored. However, unlike the rigid rules of static analysis tools, LLMs can support flexible and adaptable detection strategies tailored to the unique properties of code smells. This paper evaluates the effectiveness of four LLMs -- DeepSeek-R1, GPT-5 mini, Llama-3.3, and Qwen2.5-Code -- for detecting nine code smells across 30 Java projects. For the empirical evaluation, we created a ground-truth dataset by asking 76 developers to manually inspect 268 code-smell candidates. Our results indicate that LLMs perform strongly for structurally straightforward smells, such as Large Class and Long Method. However, we also observed that different LLMs and tools fare better for distinct code smells. We then propose and evaluate a detection strategy that combines LLMs and static analysis tools. The proposed strategy outperforms LLMs and tools in five out of nine code smells in terms of F1-Score. However, it also generates more false positives for complex smells. Therefore, we conclude that the optimal strategy depends on whether Recall or Precision is the main priority for code smell detection.

</details>


### [12] [Self-reflection in Automated Qualitative Coding: Improving Text Annotation through Secondary LLM Critique](https://arxiv.org/abs/2601.09905)
*Zackary Okun Dunivin,Mobina Noori,Seth Frey,Curtis Atkinson*

Main category: cs.SE

TL;DR: 提出兩階段LLM工作流程：第一階段LLM應用人工設計的編碼簿進行標註，第二階段LLM批評者通過自我反思重新評估正標籤，顯著降低錯誤率。


<details>
  <summary>Details</summary>
Motivation: 現有零樣本和少樣本LLM分類器在大型數據集上會產生不可接受的錯誤率，即使使用精心設計的提示詞也無法解決。

Method: 兩階段工作流程：1) 第一線LLM使用人工設計的LLM適應編碼簿進行初始標註；2) 第二線LLM批評者重新閱讀原文和第一階段理由，進行自我反思並做出最終決策。

Result: 第一階段LLM假陽性率達8%-54%，經第二階段自我反思後，F1分數提升0.04-0.25，兩個表現最差的編碼從0.52/0.55提升至0.69/0.79。識別出兩類主要錯誤：誤解和元討論。

Conclusion: 第一階段注重召回率配合第二階段批判性反思，能實現精確度優先、計算輕量的控制。自我反思可整合到現有LLM輔助標註流程中，減少噪聲並挽救不可用的分類器。

Abstract: Large language models (LLMs) allow for sophisticated qualitative coding of large datasets, but zero- and few-shot classifiers can produce an intolerable number of errors, even with careful, validated prompting. We present a simple, generalizable two-stage workflow: an LLM applies a human-designed, LLM-adapted codebook; a secondary LLM critic performs self-reflection on each positive label by re-reading the source text alongside the first model's rationale and issuing a final decision. We evaluate this approach on six qualitative codes over 3,000 high-content emails from Apache Software Foundation project evaluation discussions. Our human-derived audit of 360 positive annotations (60 passages by six codes) found that the first-line LLM had a false-positive rate of 8% to 54%, despite F1 scores of 0.74 and 1.00 in testing. Subsequent recoding of all stage-one annotations via a second self-reflection stage improved F1 by 0.04 to 0.25, bringing two especially poor performing codes up to 0.69 and 0.79 from 0.52 and 0.55 respectively. Our manual evaluation identified two recurrent error classes: misinterpretation (violations of code definitions) and meta-discussion (debate about a project evaluation criterion mistaken for its use as a decision justification). Code-specific critic clauses addressing observed failure modes were especially effective with testing and refinement, replicating the codebook-adaption process for LLM interpretation in stage-one. We explain how favoring recall in first-line LLM annotation combined with secondary critique delivers precision-first, compute-light control. With human guidance and validation, self-reflection slots into existing LLM-assisted annotation pipelines to reduce noise and potentially salvage unusable classifiers.

</details>


### [13] [S$^2$F: Principled Hybrid Testing With Fuzzing, Symbolic Execution, and Sampling](https://arxiv.org/abs/2601.10068)
*Lianjing Wang,Yufeng Zhang,Kenli Li,Zhenbang Chen,Xu Zhou,Pengfei Wang,Guangning Song,Ji Wang*

Main category: cs.SE

TL;DR: S²F提出了一种新的混合测试架构，结合了传统符号执行的精确性和定制符号执行引擎的可扩展性，通过优化符号执行和采样的使用，显著提升了边缘覆盖率和崩溃发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的混合测试工具未能充分利用符号执行和采样的潜力：1）定制符号执行引擎过度剪枝分支，导致等待模糊测试种子浪费大量时间并错过崩溃发现机会；2）现有方法未在适当分支应用采样，无法充分发挥采样能力。

Method: 提出新型混合测试架构，结合传统符号执行的精确性和定制符号执行引擎的可扩展性；基于此架构提出模糊测试、符号执行和采样结合的原则；实现为S²F混合测试工具。

Result: 在15个真实世界程序上的实验表明，S²F相比最先进工具平均提升6.14%的边缘覆盖率和32.6%的崩溃发现率；发现了3个先前未知的真实世界程序崩溃。

Conclusion: S²F通过优化符号执行和采样的结合方式，显著提升了混合测试的效果，证明了所提架构和原则的有效性，能够发现更多代码覆盖和潜在安全漏洞。

Abstract: Hybrid testing that integrates fuzzing, symbolic execution, and sampling has demonstrated superior testing efficiency compared to individual techniques. However, the state-of-the-art (SOTA) hybrid testing tools do not fully exploit the capabilities of symbolic execution and sampling in two key aspects. First, the SOTA hybrid testing tools employ tailored symbolic execution engines that tend to over-prune branches, leading to considerable time wasted waiting for seeds from the fuzzer and missing opportunities to discover crashes. Second, existing methods do not apply sampling to the appropriate branches and therefore cannot utilize the full capability of sampling. To address these two limitations, we propose a novel hybrid testing architecture that combines the precision of conventional symbolic execution with the scalability of tailored symbolic execution engines. Based on this architecture, we propose several principles for combining fuzzing, symbolic execution, and sampling. We implement our method in a hybrid testing tool S$^2$F. To evaluate its effectiveness, we conduct extensive experiments on 15 real-world programs. Experimental results demonstrate that S$^2$F outperforms the SOTA tool, achieving an average improvement of 6.14% in edge coverage and 32.6% in discovered crashes. Notably, our tool uncovers three previously unknown crashes in real-world programs.

</details>


### [14] [Mark My Works Autograder for Programming Courses](https://arxiv.org/abs/2601.10093)
*Yiding Qiu,Seyed Mahdi Azimi,Artem Lensky*

Main category: cs.SE

TL;DR: 开发了Mark My Works本地自动评分系统，结合传统单元测试和LLM生成解释，为编程课程提供及时详细反馈


<details>
  <summary>Details</summary>
Motivation: 大型编程课程难以为学生代码提供及时、详细的反馈，需要自动化解决方案来改善教学效果

Method: 使用基于角色的提示分析提交内容，结合传统单元测试和LLM生成解释，批判代码质量并生成教学反馈，保持推理过程透明

Result: 在191名学生课程中测试，AI评分与人工评分无线性相关(r=-0.177,p=0.124)，但两者分布相似左偏；AI评分更保守(平均59.95 vs 80.53)，但生成的技术反馈更详细

Conclusion: AI自动评分系统能识别与人工相似的质量层次，提供更详细的技术反馈，但评分哲学不同，可作为编程课程的有价值补充工具

Abstract: Large programming courses struggle to provide timely, detailed feedback on student code. We developed Mark My Works, a local autograding system that combines traditional unit testing with LLM-generated explanations. The system uses role-based prompts to analyze submissions, critique code quality, and generate pedagogical feedback while maintaining transparency in its reasoning process.
  We piloted the system in a 191-student engineering course, comparing AI-generated assessments with human grading on 79 submissions. While AI scores showed no linear correlation with human scores (r = -0.177, p = 0.124), both systems exhibited similar left-skewed distributions, suggesting they recognize comparable quality hierarchies despite different scoring philosophies. The AI system demonstrated more conservative scoring (mean: 59.95 vs 80.53 human) but generated significantly more detailed technical feedback.

</details>


### [15] [Repository Intelligence Graph: Deterministic Architectural Map for LLM Code Assistants](https://arxiv.org/abs/2601.10112)
*Tsvi Cherny-Shahar,Amiram Yehudai*

Main category: cs.SE

TL;DR: 提出Repository Intelligence Graph (RIG)和SPADE提取器，为代码仓库构建确定性架构图，显著提升AI编码代理在多语言项目中的构建和测试理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码仓库感知的编码代理在多语言项目中难以恢复构建和测试结构，因为跨语言依赖关系分布在异构的构建系统和工具中。

Method: 提出RIG（确定性、证据支持的架构图）表示可构建组件、聚合器、运行器、测试、外部包和包管理器，通过明确的依赖和覆盖边连接。开发SPADE提取器从构建和测试工件中构建RIG，并暴露为LLM友好的JSON视图。

Result: 在8个仓库上评估3个商业代理，提供RIG将平均准确率提高12.2%，完成时间减少53.9%，每个正确答案的时间减少57.8%。多语言仓库改进更大（准确率+17.7%，效率+69.5%）。

Conclusion: RIG能显著改善编码代理对仓库结构的理解，将失败从结构误解转向基于正确结构的推理错误，但基于图的推理质量仍是关键因素。

Abstract: Repository aware coding agents often struggle to recover build and test structure, especially in multilingual projects where cross language dependencies are encoded across heterogeneous build systems and tooling. We introduce the Repository Intelligence Graph (RIG), a deterministic, evidence backed architectural map that represents buildable components, aggregators, runners, tests, external packages, and package managers, connected by explicit dependency and coverage edges that trace back to concrete build and test definitions. We also present SPADE, a deterministic extractor that constructs RIG from build and test artifacts (currently with an automatic CMake plugin based on the CMake File API and CTest metadata), and exposes RIG as an LLM friendly JSON view that agents can treat as the authoritative description of repository structure.
  We evaluate three commercial agents (Claude Code, Cursor, Codex) on eight repositories spanning low to high build oriented complexity, including the real world MetaFFI project. Each agent answers thirty structured questions per repository with and without RIG in context, and we measure accuracy, wall clock completion time, and efficiency (seconds per correct answer). Across repositories and agents, providing RIG improves mean accuracy by 12.2\% and reduces completion time by 53.9\%, yielding a mean 57.8\% reduction in seconds per correct answer. Gains are larger in multilingual repositories, which improve by 17.7\% in accuracy and 69.5\% in efficiency on average, compared to 6.6\% and 46.1\% in single language repositories. Qualitative analysis suggests that RIG shifts failures from structural misunderstandings toward reasoning mistakes over a correct structure, while rare regressions highlight that graph based reasoning quality remains a key factor.

</details>


### [16] [Towards Online Malware Detection using Process Resource Utilization Metrics](https://arxiv.org/abs/2601.10164)
*Themistoklis Diamantopoulos,Dimosthenis Natsos,Andreas L. Symeonidis*

Main category: cs.SE

TL;DR: 提出一种基于在线学习的动态恶意软件检测方法，利用进程资源利用指标作为行为特征，通过持续更新模型来适应不断演变的恶意软件威胁。


<details>
  <summary>Details</summary>
Motivation: 云计算和物联网的快速发展增加了计算资源的互联性，使得恶意软件传播速度加快。现有机器学习方法依赖大型标注数据集、固定模型训练，且假设训练好的模型长期有效，忽视了恶意软件不断演变的复杂性，因此难以检测随时间变化的恶意软件攻击。

Method: 提出在线学习方法进行动态恶意软件检测，通过整合时间信息持续更新模型，使用进程资源利用指标作为行为特征。该方法能够增量适应新出现的威胁，有效检测零日恶意软件。

Result: 与传统批处理算法相比，该方法能有效检测零日恶意软件，并在数据可用性有限的情况下表现良好，而传统批处理方法在这些场景中往往表现不稳定。

Conclusion: 在线学习方法能够克服现有恶意软件检测方法的局限性，通过持续更新模型适应不断演变的威胁，在检测零日恶意软件和数据有限场景下表现优异。

Abstract: The rapid growth of Cloud Computing and Internet of Things (IoT) has significantly increased the interconnection of computational resources, creating an environment where malicious software (malware) can spread rapidly. To address this challenge, researchers are increasingly utilizing Machine Learning approaches to identify malware through behavioral (i.e. dynamic) cues. However, current approaches are limited by their reliance on large labeled datasets, fixed model training, and the assumption that a trained model remains effective over time-disregarding the ever-evolving sophistication of malware. As a result, they often fail to detect evolving malware attacks that adapt over time. This paper proposes an online learning approach for dynamic malware detection, that overcomes these limitations by incorporating temporal information to continuously update its models using behavioral features, specifically process resource utilization metrics. By doing so, the proposed models can incrementally adapt to emerging threats and detect zero-day malware effectively. Upon evaluating our approach against traditional batch algorithms, we find it effective in detecting zero-day malware. Moreover, we demonstrate its efficacy in scenarios with limited data availability, where traditional batch-based approaches often struggle to perform reliably.

</details>


### [17] [Agentic Pipelines in Embedded Software Engineering: Emerging Practices and Challenges](https://arxiv.org/abs/2601.10220)
*Simin Sun,Miroslaw Staron*

Main category: cs.SE

TL;DR: 嵌入式软件工程团队正在探索如何将生成式AI整合到安全关键、资源受限的开发环境中，面临确定性、可靠性和可追溯性等独特挑战。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在变革软件开发工作流，但嵌入式软件工程组织首次面临将AI整合到安全关键、资源受限环境中的挑战，需要满足确定性、可靠性和可追溯性的严格要求。

Method: 通过对四家公司的十位资深专家进行定性研究，采用半结构化焦点小组访谈和结构化头脑风暴会议，识别实践与挑战。

Result: 识别出11个新兴实践和14个挑战，涉及生成式AI工具的编排、负责任治理和可持续采用等方面。

Conclusion: 嵌入式软件工程团队正在重新思考工作流、角色和工具链，以实现向智能代理管道和生成式AI增强开发的可持续转型。

Abstract: A new transformation is underway in software engineering, driven by the rapid adoption of generative AI in development workflows. Similar to how version control systems once automated manual coordination, AI tools are now beginning to automate many aspects of programming. For embedded software engineering organizations, however, this marks their first experience integrating AI into safety-critical and resource-constrained environments. The strict demands for determinism, reliability, and traceability pose unique challenges for adopting generative technologies.
  In this paper, we present findings from a qualitative study with ten senior experts from four companies who are evaluating generative AI-augmented development for embedded software. Through semi-structured focus group interviews and structured brainstorming sessions, we identified eleven emerging practices and fourteen challenges related to the orchestration, responsible governance, and sustainable adoption of generative AI tools. Our results show how embedded software engineering teams are rethinking workflows, roles, and toolchains to enable a sustainable transition toward agentic pipelines and generative AI-augmented development.

</details>


### [18] [Evolving with AI: A Longitudinal Analysis of Developer Logs](https://arxiv.org/abs/2601.10258)
*Agnia Sergeyuk,Eric Huang,Dariia Karaeva,Anastasiia Serova,Yaroslav Golubev,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: AI编码助手在长期使用中显著增加代码产量和删除量，但开发者主观感受主要是生产力提升，其他维度变化不大


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注AI编码助手的短期使用或主观感知，缺乏对长期使用如何重塑日常编码实践的实证理解

Method: 混合方法研究：结合800名开发者两年的细粒度遥测数据和62名专业人员的问卷调查，分析工作流的五个维度变化

Result: 遥测数据显示AI用户产生更多代码但也删除更多；问卷调查显示开发者主要感知生产力提升，其他维度变化不大

Conclusion: AI编码助手正在无声地重构软件开发工作流，为未来AI增强工具设计提供了实证依据和启示

Abstract: AI-powered coding assistants are rapidly becoming fixtures in professional IDEs, yet their sustained influence on everyday development remains poorly understood. Prior research has focused on short-term use or self-reported perceptions, leaving open questions about how sustained AI use reshapes actual daily coding practices in the long term. We address this gap with a mixed-method study of AI adoption in IDEs, combining longitudinal two-year fine-grained telemetry from 800 developers with a survey of 62 professionals. We analyze five dimensions of workflow change: productivity, code quality, code editing, code reuse, and context switching. Telemetry reveals that AI users produce substantially more code but also delete significantly more. Meanwhile, survey respondents report productivity gains and perceive minimal changes in other dimensions. Our results offer empirical insights into the silent restructuring of software workflows and provide implications for designing future AI-augmented tooling.

</details>


### [19] [Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs](https://arxiv.org/abs/2601.10496)
*Ali Al-Kaswan,Claudio Spiess,Prem Devanbu,Arie van Deursen,Maliheh Izadi*

Main category: cs.SE

TL;DR: 提出一个暴露感知评估框架，量化训练数据中错误代码与修复代码的暴露程度如何影响LLM偏好，发现暴露会扭曲错误修复评估，LLM可能传播记忆的错误。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多用于代码生成和调试，但其输出仍可能包含源自训练数据的错误。需要区分LLM是偏好正确代码还是熟悉的错误版本，这可能受训练期间接触的内容影响。

Method: 引入暴露感知评估框架，使用ManySStuBs4J基准，应用Data Portraits在Stack-V2语料库上进行成员测试，估计每个错误和修复变体在训练中是否被看到。然后按暴露程度分层示例，使用代码补全和多种基于似然的评分指标比较模型偏好。

Result: 大多数示例(67%)在训练数据中没有任何变体；当只有一个变体存在时，修复比错误更频繁出现。在模型生成中，模型复制错误行远多于修复，错误暴露示例放大了这种趋势，修复暴露示例仅显示边际改善。在似然评分中，最小和最大令牌概率指标在所有条件下始终偏好修复代码，表明对正确修复的稳定偏向。相比之下，像基尼系数这样的指标在只有错误变体被看到时会反转偏好。

Conclusion: 暴露会扭曲错误修复评估，并突显LLM在实践中可能传播记忆错误的风险。需要暴露感知评估来准确衡量模型性能。

Abstract: Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Federated Unlearning in Edge Networks: A Survey of Fundamentals, Challenges, Practical Applications and Future Directions](https://arxiv.org/abs/2601.09978)
*Jer Shyuan Ng,Wathsara Daluwatta,Shehan Edirimannage,Charitha Elvitigala,Asitha Kottahachchi Kankanamge Don,Ibrahim Khalil,Heng Zhang,Dusit Niyato*

Main category: cs.DC

TL;DR: 联邦学习（FL）虽然解决了数据本地化和隐私问题，但缺乏对数据删除请求（如被遗忘权）的支持。联邦遗忘学习（FUL）作为新兴研究领域，旨在分布式异构环境中消除个体客户端或数据子集对全局FL模型的影响。


<details>
  <summary>Details</summary>
Motivation: 随着连接设备和隐私敏感应用的普及，联邦学习被广泛采用，但现有FL框架无法满足法规（如被遗忘权）要求的数据删除需求。需要将机器遗忘概念扩展到联邦设置中，以构建可信、合规、以用户为中心的联邦系统。

Method: 这是一篇综述论文，首先介绍FUL的基础知识，然后回顾解决三大实施挑战（通信成本、资源分配、安全与隐私）的FUL框架，讨论FUL在现代分布式计算机网络中的应用，并突出开放挑战和未来研究方向。

Result: 论文通过整合现有知识和映射开放问题，为研究人员和从业者提供了基础性参考，旨在推动FL向可信、合规、以用户为中心的系统发展。

Conclusion: 联邦遗忘学习是联邦学习的重要扩展，能够满足数据删除的法规要求。该领域面临通信成本、资源分配、安全隐私等挑战，需要进一步研究以构建真正可信的联邦系统。

Abstract: The proliferation of connected devices and privacy-sensitive applications has accelerated the adoption of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing raw data. While FL addresses data locality and privacy concerns, it does not inherently support data deletion requests that are increasingly mandated by regulations such as the Right to be Forgotten (RTBF). In centralized learning, this challenge has been studied under the concept of Machine Unlearning (MU), that focuses on efficiently removing the influence of specific data samples or clients from trained models. Extending this notion to federated settings has given rise to Federated Unlearning (FUL), a new research area concerned with eliminating the contributions of individual clients or data subsets from the global FL model in a distributed and heterogeneous environment. In this survey, we first introduce the fundamentals of FUL. Then, we review the FUL frameworks that are proposed to address the three main implementation challenges, i.e., communication cost, resource allocation as well as security and privacy. Furthermore, we discuss applications of FUL in the modern distributed computer networks. We also highlight the open challenges and future research opportunities. By consolidating existing knowledge and mapping open problems, this survey aims to serve as a foundational reference for researchers and practitioners seeking to advance FL to build trustworthy, regulation-compliant and user-centric federated systems.

</details>


### [21] [Distributed Linearly Separable Computation with Arbitrary Heterogeneous Data Assignment](https://arxiv.org/abs/2601.10177)
*Ziting Zhang,Kai Wan,Minquan Cheng,Shuo Shao,Giuseppe Caire*

Main category: cs.DC

TL;DR: 研究异构分布式线性可分计算问题，针对任意异构数据分配，提出通用计算方案和逆界，刻画任务函数可计算维度与通信成本之间的基本权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究多假设同构设置（每个工作节点持有相同数量的数据集），但实际分布式系统中数据分配往往是异构的。本文旨在研究更一般的任意异构数据分配场景，探索在这种现实条件下线性可分任务函数的可计算维度与通信成本之间的基本权衡。

Method: 针对任意异构数据分配，首先在整数通信成本约束下提出通用计算方案和通用逆界，通过刻画数据分配的结构来建立理论界限。然后将方案和逆界扩展到分数通信成本情况。

Result: 在整数通信成本下，提出的通用计算方案和逆界在某些参数范围内重合，表明达到了最优权衡。同时成功将结果扩展到分数通信成本场景。

Conclusion: 本文首次系统研究了任意异构数据分配下的分布式线性可分计算问题，建立了可计算维度与通信成本之间的基本权衡理论，为实际分布式系统中的计算任务调度提供了理论指导。

Abstract: Distributed linearly separable computation is a fundamental problem in large-scale distributed systems, requiring the computation of linearly separable functions over different datasets across distributed workers. This paper studies a heterogeneous distributed linearly separable computation problem, including one master and N distributed workers. The linearly separable task function involves Kc linear combinations of K messages, where each message is a function of one dataset. Distinguished from the existing homogeneous settings that assume each worker holds the same number of datasets, where the data assignment is carefully designed and controlled by the data center (e.g., the cyclic assignment), we consider a more general setting with arbitrary heterogeneous data assignment across workers, where `arbitrary' means that the data assignment is given in advance and `heterogeneous' means that the workers may hold different numbers of datasets. Our objective is to characterize the fundamental tradeoff between the computable dimension of the task function and the communication cost under arbitrary heterogeneous data assignment. Under the constraint of integer communication costs, for arbitrary heterogeneous data assignment, we propose a universal computing scheme and a universal converse bound by characterizing the structure of data assignment, where they coincide under some parameter regimes. We then extend the proposed computing scheme and converse bound to the case of fractional communication costs.

</details>


### [22] [SCRamble: Adaptive Decentralized Overlay Construction for Blockchain Networks](https://arxiv.org/abs/2601.10277)
*Evangelos Kolyvas,Alexandros Antonov,Spyros Voulgaris*

Main category: cs.DC

TL;DR: SCRamble是一个去中心化协议，通过创新的链路选择策略显著减少区块链网络中的区块传播时间，从而提高交易吞吐量和系统安全性。


<details>
  <summary>Details</summary>
Motivation: 区块链发展15年来，交易吞吐量仍然是关键挑战，通常受限于每秒有限交易数。限制这一指标的根本因素是底层P2P网络中区块传播的网络延迟，这通常通过随机连接形成。加速区块传播不仅能提高交易速率，还能通过减少分叉概率增强系统安全性。

Method: SCRamble采用创新的链路选择策略，整合两种启发式方法：1）评估来自相邻节点区块到达时间的评分机制；2）考虑网络延迟的第二种启发式方法。

Result: SCRamble协议显著减少了区块链网络中的区块传播时间。

Conclusion: SCRamble通过创新的链路选择策略有效解决了区块链网络中的区块传播延迟问题，为提高交易吞吐量和系统安全性提供了解决方案。

Abstract: Despite being under development for over 15 years, transaction throughput remains one of the key challenges confronting blockchains, which typically has a cap of a limited number of transactions per second. A fundamental factor limiting this metric is the network latency associated with the block propagation throughout of the underlying peer-to-peer network, typically formed through random connections. Accelerating the dissemination of blocks not only improves transaction rates, but also enhances system security by reducing the probability of forks. This paper introduces SCRamble: a decentralized protocol that significantly reduces block dissemination time in blockchain networks. SCRamble's effectiveness is attributed to its innovative link selection strategy, which integrates two heuristics: a scoring mechanism that assesses block arrival times from neighboring peers, and a second heuristic that takes network latency into account.

</details>


### [23] [Mitigating GIL Bottlenecks in Edge AI Systems](https://arxiv.org/abs/2601.10582)
*Mridankan Mandal,Smit Sanjay Shende*

Main category: cs.DC

TL;DR: 针对边缘设备上Python AI代理的运行时优化挑战，提出基于阻塞率指标的轻量级分析工具和自适应运行时系统，解决GIL导致的线程池扩展性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署基于Python的AI代理面临运行时优化挑战：需要高线程数来掩盖I/O延迟，但Python的全局解释器锁(GIL)会序列化执行。简单的线程池扩展会导致"饱和悬崖"现象，在过度配置线程数时造成显著的吞吐量下降。

Method: 提出了一个轻量级分析工具和自适应运行时系统，使用阻塞率指标(beta)来区分真正的I/O等待和GIL争用。采用基于库的解决方案，无需手动调优即可实现接近最优性能。

Result: 该方法在七个边缘AI工作负载配置上达到93.9%的平均效率，在真实ML推理任务中实现96.5%的最优性能。与多进程(受限于8倍内存开销)和asyncio(受CPU密集型阶段阻塞)相比表现更优。即使在Python 3.13t(无GIL)环境下，单核设备上的饱和悬崖问题仍然存在，验证了beta指标在GIL和无GIL环境中的有效性。

Conclusion: 该研究为边缘AI系统提供了实用的运行时优化方案，通过阻塞率指标有效解决了Python在边缘设备上的线程扩展问题，为资源受限环境下的AI代理部署提供了有效解决方案。

Abstract: Deploying Python based AI agents on resource-constrained edge devices presents a runtime optimization challenge: high thread counts are needed to mask I/O latency, yet Python's Global Interpreter Lock (GIL) serializes execution. We demonstrate that naive thread-pool scaling causes a "saturation cliff": >= 20% throughput degradation at overprovisioned thread counts (N >= 512) on edge-representative configurations. We present a lightweight profiling tool and adaptive runtime system using a Blocking Ratio metric (beta) that distinguishes genuine I/O wait from GIL contention. Our library-based solution achieves 96.5% of optimal performance without manual tuning, outperforming multiprocessing (limited by ~8x memory overhead on devices with 512 MB-2 GB RAM) and asyncio (blocked by CPU-bound phases). Evaluation across seven edge AI workload profiles, including real ML inference with ONNX Runtime MobileNetV2, demonstrates 93.9% average efficiency. Comparative experiments with Python 3.13t (free threading) show that while GIL elimination enables ~4x throughput on multi-core edge devices, the saturation cliff persists on single-core devices, validating our beta metric for both GIL and no-GIL environments. This provides practical optimization for edge AI systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: Multiverse是一个新型软件事务内存系统，结合了无版本化和多版本化事务的优点，支持两者并发执行，在保持无版本事务高性能的同时，为长时读取提供快速版本化支持。


<details>
  <summary>Details</summary>
Motivation: 现有STM系统在处理长时读取（访问大量频繁更新地址）时存在局限。多版本化方法虽然支持这类工作负载，但开销大且会降低不需要版本化的事务性能。需要一种既能保持无版本事务高性能，又能支持长时读取的解决方案。

Method: 提出Multiverse STM，支持版本化和无版本化事务并发执行。系统设计确保无版本事务能达到最先进无版本STM的性能，同时为版本化事务提供快速支持以处理长时读取。

Result: 实验表明：对于没有长时读取的常见工作负载，Multiverse性能相当或更好；对于包含长时读取和频繁更新的工作负载，Multiverse显著优于现有STM，吞吐量在某些情况下比其他STM快几个数量级。

Conclusion: Multiverse成功结合了无版本和多版本化STM的优点，在保持无版本事务高性能的同时，为长时读取工作负载提供了显著性能提升，解决了现有STM系统的关键局限性。

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [25] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 开发了两个工具Babel和ORION来解决FAIR数据原则在实际互操作中的挑战，通过标识符映射和数据模型转换实现知识库的真正互操作。


<details>
  <summary>Details</summary>
Motivation: 尽管许多资源遵循FAIR原则并良好注释，但由于标识符方案和数据模型的差异，这些资源在实际中仍然难以有效互操作。

Method: 创建了两个工具：Babel通过策划标识符映射创建等价标识符簇，提供高性能API；ORION将知识库转换为社区管理的通用数据模型。

Result: 成功开发了Babel和ORION工具，能够支持数据互操作，并创建了一个完全互操作的知识库库，可在https://robokop.renci.org下载使用。

Conclusion: Babel和ORION工具有效解决了FAIR数据在实际互操作中的挑战，为科学数据生态系统提供了实用的互操作解决方案。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [26] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: SDP算法通过冗余计数排名发现top-k函数依赖，使用单调上界剪枝搜索空间，比穷举方法更快更省内存。


<details>
  <summary>Details</summary>
Motivation: 传统函数依赖发现算法存在两个问题：1) 计算成本过高，在大规模和维度数据上速度慢；2) 结果集过大，难以识别有用的依赖关系。

Method: 提出SDP算法，通过冗余计数排名发现top-k函数依赖，使用单调上界进行搜索空间剪枝。优化包括：按分区基数排序属性、使用分区基数矩阵收紧边界、全局调度器优先探索有希望的分支。

Result: 在40多个数据集上的实验表明，SDP比穷举方法快得多且使用更少内存。

Conclusion: SDP算法通过选择性发现和剪枝策略，有效解决了传统函数依赖发现的计算成本和结果集过大的问题。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [27] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 通过合并结构相似的SQL语句和事务来提升应用端事务处理性能，提出TransactionMerger中间件，在TPC-C和Spree应用中实现最高3.52倍的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 传统事务处理存在性能瓶颈，特别是在应用端执行相似语句时产生冗余操作和争用。通过合并结构相似的事务可以消除冗余读取、减少争用，从而提升整体性能。

Method: 提出TransactionMerger中间件，采用三种技术：1) 基于SQL语义合并相似语句；2) 消除冗余读取；3) 通过预计算聚合效果合并事务间的争用语句。同时开发静态分析工具识别合并机会而不违反隔离性。

Result: 在TPC-C基准测试中实现最高2.65倍的吞吐量提升，在真实世界应用Spree中实现3.52倍的吞吐量提升，验证了事务合并的有效性。

Conclusion: 事务合并是提升应用端事务处理性能的有效方法，TransactionMerger中间件能够在不违反隔离性的前提下显著提升系统吞吐量，具有实际应用价值。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [28] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 提出将数学数据模型转换为关系模型及非关系约束的算法，证明其高效、可靠、完整且最优，应用于家谱树建模，并提供SQL/VBA代码示例


<details>
  <summary>Details</summary>
Motivation: 需要将数学数据模型方案自动转换为关系数据库方案，同时保持非关系约束，以便在MatBase数据库管理系统中实现智能数据建模

Method: 设计伪代码算法，将数学数据模型方案转换为关系模型及相关非关系约束集，证明算法性能，应用于家谱树子宇宙建模案例

Result: 算法被证明非常快速、可靠、完整且最优，成功应用于家谱树建模，提供了SQL和VBA代码示例用于约束实施

Conclusion: 该算法有效实现了数学数据模型到关系数据库的转换，为MatBase系统提供了实用的建模工具，并提供了约束实施的具体指导

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>
