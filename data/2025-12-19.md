<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 6]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.DC](#cs.DC) [Total: 11]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [DP-Bench: A Benchmark for Evaluating Data Product Creation Systems](https://arxiv.org/abs/2512.15798)
*Faisal Chowdhury,Sola Shirai,Sarthak Dash,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.DB

TL;DR: DP-Bench：首个用于评估自动数据产品创建的基准，基于现有ELT和Text-to-SQL基准构建，提供LLM基线方法


<details>
  <summary>Details</summary>
Motivation: 数据产品旨在解决特定问题或业务需求，超越原始数据服务。尽管业界已有十多年的手动或半自动创建经验，但缺乏评估自动数据产品创建的基准。

Method: 利用现有ELT（提取-加载-转换）和Text-to-SQL基准创建DP-Bench基准，并提出多种基于LLM的方法作为自动生成数据产品的基线。

Result: 创建了首个自动数据产品创建基准DP-Bench，并提供了基于LLM的基线方法，相关资源已在HuggingFace平台开源。

Conclusion: DP-Bench填补了自动数据产品创建评估领域的空白，为未来研究提供了标准化测试平台和基线方法。

Abstract: A data product is created with the intention of solving a specific problem, addressing a specific business usecase or meeting a particular need, going beyond just serving data as a raw asset. Data products enable end users to gain greater insights about their data. Since it was first introduced over a decade ago, there has been considerable work, especially in industry, to create data products manually or semi-automatically. However, there exists hardly any benchmark to evaluate automatic data product creation. In this work, we present a benchmark, first of its kind, for this task. We call it DP-Bench. We describe how this benchmark was created by taking advantage of existing work in ELT (Extract-Load-Transform) and Text-to-SQL benchmarks. We also propose a number of LLM based approaches that can be considered as baselines for generating data products automatically. We make the DP-Bench and supplementary materials available in https://huggingface.co/datasets/ibm-research/dp-bench .

</details>


### [2] [Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive](https://arxiv.org/abs/2512.15815)
*Valeria Granata,Francois Liot,Xing Wang,Steen Lysgaard,Ivano E. Castelli,Tejs Vegge,Nicola Marzari,Giovanni Pizzi*

Main category: cs.DB

TL;DR: BIG-MAP Archive是一个基于云的私有存储库，专为大型联盟设计，提供安全、可控的数据共享，支持细粒度权限管理，并可轻松部署到其他联盟。


<details>
  <summary>Details</summary>
Motivation: 大型联盟（如研究合作或产业伙伴关系）中的数据共享面临组织和技术的双重挑战，需要共同平台来促进协作、促进发现交流并确保敏感数据的安全访问。

Method: 基于InvenioRDM构建云端的学科性私有存储库，提供可扩展架构、用户友好界面、强大的安全性和访问控制，支持细粒度权限管理和正式化上传流程。

Result: BIG-MAP Archive成功实现了在大型联盟内的安全可控数据共享，确保数据机密性的同时支持灵活的基于权限的访问，已应用于BATTERY 2030+等联盟。

Conclusion: 该存储库为大型联盟提供了量身定制的数据共享解决方案，相比通用存储库更具针对性，可轻松重新部署到其他联盟如MaterialsCommons4.eu和RAISE。

Abstract: Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).

</details>


### [3] [Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers](https://arxiv.org/abs/2512.16083)
*Thanh Dat Hoang,Thanh Tam Nguyen,Thanh Trung Huynh,Hongzhi Yin,Quoc Viet Hung Nguyen*

Main category: cs.DB

TL;DR: 提出GRaST-SQL框架，通过LLM感知的列排序、图变换器重排序和Steiner树启发式算法，有效压缩Text2SQL提示，解决大型数据库模式超出LLM上下文限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Text2SQL系统在处理大型真实数据库时面临挑战，因为完整模式（数百张表、数万列）会超出LLM上下文限制。现有缓解方法要么依赖昂贵的多步提示管道，要么独立过滤列而忽略列间结构关系。

Method: 提出GRaST-SQL框架：1) 使用查询感知的LLM编码器（包含值和元数据）对列进行排序；2) 通过基于函数依赖的轻量图变换器对互连列进行重排序；3) 使用Steiner树启发式算法选择保持连接性的子模式。

Result: 在真实数据集上，GRaST-SQL实现了接近完美的召回率，比CodeS、SchemaExP、Qwen重排序器和嵌入检索器具有更高的精度，同时保持亚秒级中位延迟，可扩展到23,000+列的模式。

Conclusion: GRaST-SQL是一个开源、LLM高效的模式过滤框架，能有效压缩Text2SQL提示，解决大型数据库模式超出LLM上下文限制的问题，在Spider 2.0等真实世界基准测试中表现出色。

Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.

</details>


### [4] [ModelTables: A Corpus of Tables about Models](https://arxiv.org/abs/2512.16106)
*Zhengyuan Dong,Victor Zhong,Renée J. Miller*

Main category: cs.DB

TL;DR: ModelTables是一个模型湖中表格的基准数据集，捕捉了性能与配置表格的结构化语义，包含60K模型和90K表格，用于评估表格搜索方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本检索方法忽视了模型性能与配置表格的结构化语义，需要专门的基准来支持模型湖中结构化数据的语义检索、比较和组织。

Method: 从Hugging Face模型卡片、GitHub READMEs和相关论文构建表格语料库，建立多源真实标注（论文引用、模型链接继承、共享训练数据集），评估数据湖搜索算子和信息检索基线。

Result: 基于联合的语义表格检索达到54.8% P@1，基于表格的密集检索达到66.5% P@1，元数据混合检索达到54.1%，显示表格搜索方法仍有改进空间。

Conclusion: ModelTables是首个大规模AI模型结构化数据基准，为开发更准确的语义检索、结构化比较和模型知识组织提供了基础。

Abstract: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.

</details>


### [5] [Multi-granularity Spatiotemporal Flow Patterns](https://arxiv.org/abs/2512.16255)
*Chrysanthi Kosyfaki,Nikos Mamoulis,Reynold Cheng,Ben Kai*

Main category: cs.DB

TL;DR: 提出一种从多粒度时空数据中发现重要ODT（起点-目的地-时间）模式的算法，包括精确枚举、优化方法和近似解决方案。


<details>
  <summary>Details</summary>
Motivation: 分析不同时空粒度下的对象或数据流动可以揭示有趣的洞察或趋势。例如，交通公司通过聚合乘客出行数据（如统计从一个区域到另一个区域的乘客数量）可以分析移动行为。本文旨在发现区域间乘客流动在不同粒度下的重要趋势。

Method: 定义ODT（起点-目的地-时间）模式，提出自底向上的枚举算法。采用优化技术大幅减少搜索空间和计算成本。提出模式变体（约束模式和top-k模式）以适应不同应用场景。最后提出基于生成-测试方法的近似解决方案，快速识别特定大小的ODT模式。

Result: 在三个真实数据集上评估方法的效率和有效性，展示了其中有趣的ODT流动模式。

Conclusion: 本文提出的ODT模式发现方法能够有效识别多粒度时空数据中的重要流动趋势，为交通分析等应用提供有价值的洞察。

Abstract: Analyzing flow of objects or data at different granularities of space and time can unveil interesting insights or trends. For example, transportation companies, by aggregating passenger travel data (e.g., counting passengers traveling from one region to another), can analyze movement behavior. In this paper, we study the problem of finding important trends in passenger movements between regions at different granularities. We define Origin (O), Destination (D), and Time (T ) patterns (ODT patterns) and propose a bottom-up algorithm that enumerates them. We suggest and employ optimizations that greatly reduce the search space and the computational cost of pattern enumeration. We also propose pattern variants (constrained patterns and top-k patterns) that could be useful to differ- ent applications scenarios. Finally, we propose an approximate solution that fast identifies ODT patterns of specific sizes, following a generate-and-test approach. We evaluate the efficiency and effectiveness of our methods on three real datasets and showcase interesting ODT flow patterns in them.

</details>


### [6] [Subset Sampling over Joins](https://arxiv.org/abs/2512.16321)
*Aryan Esmailpour,Xiao Hu,Jinchao Huang,Stavros Sintos*

Main category: cs.DB

TL;DR: 研究连接操作上的子集采样问题，提出针对无环连接的三种高效算法，实现接近最优的时间和空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现代应用（如关系数据上的机器学习）经常需要从关系连接隐式定义的集合中进行采样，而连接结果可能指数级大于输入，传统方法计算不可行。

Method: 提出三种算法：1) 静态索引用于生成多个独立子集样本；2) 一次性算法用于生成单个子集样本；3) 动态索引支持元组插入，同时维护一次性样本或生成多个独立样本。

Result: 针对无环连接，实现了接近最优的时间和空间复杂度，相对于输入大小和期望样本大小。

Conclusion: 首次为连接操作上的子集采样问题提供了高效解决方案，支持多种可分解函数（如乘积、求和、最小值、最大值），解决了大数据分析中的关键采样需求。

Abstract: Subset sampling (also known as Poisson sampling), where the decision to include any specific element in the sample is made independently of all others, is a fundamental primitive in data analytics, enabling efficient approximation by processing representative subsets rather than massive datasets. While sampling from explicit lists is well-understood, modern applications -- such as machine learning over relational data -- often require sampling from a set defined implicitly by a relational join. In this paper, we study the problem of \emph{subset sampling over joins}: drawing a random subset from the join results, where each join result is included independently with some probability. We address the general setting where the probability is derived from input tuple weights via decomposable functions (e.g., product, sum, min, max). Since the join size can be exponentially larger than the input, the naive approach of materializing all join results to perform subset sampling is computationally infeasible. We propose the first efficient algorithms for subset sampling over acyclic joins: (1) a \emph{static index} for generating multiple (independent) subset samples over joins; (2) a \emph{one-shot} algorithm for generating a single subset sample over joins; (3) a \emph{dynamic index} that can support tuple insertions, while maintaining a one-shot sample or generating multiple (independent) samples. Our techniques achieve near-optimal time and space complexity with respect to the input size and the expected sample size.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies](https://arxiv.org/abs/2512.15804)
*Balreet Grewal,James Graham,Jeff Muizelaar,Jan Honza Odvarko,Suhaib Mujahid,Marco Castelluccio,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: XBIDetective使用视觉语言模型自动检测跨浏览器不一致性，通过对比Firefox和Chrome的网页截图，能准确识别渲染差异和动态元素。


<details>
  <summary>Details</summary>
Motivation: 浏览器渲染bug通常只在特定条件下触发，难以检测。跨浏览器不一致性(XBIs)可作为检测线索，但现有视觉和DOM分析方法难以处理动态和交互元素。

Method: 开发XBIDetective工具，自动在Firefox和Chrome中捕获网站截图，使用视觉语言模型分析差异。评估了现成模型和微调模型在1,052个网站上的表现。

Result: 微调后的VLM能79%准确识别跨浏览器差异，检测动态元素和广告的准确率分别为84%和85%。工具在自动化回归测试、大规模网站监控和bug报告分类方面有实用价值。

Conclusion: 视觉语言模型能有效检测跨浏览器不一致性，特别是处理动态内容。XBIDetective展示了在浏览器开发和质量保证中的实际应用潜力。

Abstract: Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.

</details>


### [8] [CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory](https://arxiv.org/abs/2512.15813)
*Nishant Gaurav,Adit Akarsh,Tejas Ravishankar,Manoj Bajaj*

Main category: cs.SE

TL;DR: CodeMem提出了一种通过代码实现程序性记忆的架构，用于构建可重复使用的确定性可靠智能体工作流，解决现有工具使用AI代理在重复任务中的概率不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 当前工具使用AI代理存在动作空间有限、上下文效率低下和概率不稳定等问题，不适合处理重复任务。虽然CodeAct、DynaSaur、Code Mode等先前工作通过使用Python语言作为动作空间解决了前两个问题，但概率不稳定问题仍然存在，因为LLM的随机性导致相同任务在不同执行中可能产生不同轨迹。

Method: 提出CodeMem架构，通过代码实现程序性记忆，用于构建和运行可重复使用的智能体工作流。该方法利用代码作为记忆形式，确保工作流的确定性可靠性，使智能体能够一致地执行重复任务。

Result: 论文提出了一个能够实现确定性可靠性的智能体工作流架构，通过程序性记忆解决LLM代理在重复任务中的概率不稳定问题，使智能体能够像n8n和Zapier等平台构建的工作流一样可靠高效。

Conclusion: CodeMem通过代码实现程序性记忆，为构建可重复使用的确定性可靠智能体工作流提供了解决方案，解决了当前工具使用AI代理在重复任务处理中的关键限制，特别是概率不稳定问题。

Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.

</details>


### [9] [OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering](https://arxiv.org/abs/2512.15979)
*Mia Mohammad Imran,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: 提出OLAF框架，将LLM标注视为测量过程而非纯自动化活动，强调可靠性、校准、漂移等关键构念，旨在提升软件工程研究中LLM标注的透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: LLM在软件工程实证研究中被广泛用于标注任务，但现有研究缺乏对可靠性、校准、漂移的标准化度量，且经常遗漏关键配置细节，导致标注结果的可靠性和可复现性不足。

Method: 提出概念框架OLAF，组织六个关键构念：可靠性、校准、漂移、共识、聚合和透明度，将LLM标注视为测量过程而非纯自动化活动。

Result: 建立了LLM标注的测量框架，为未来软件工程研究中LLM标注的方法论讨论和实证工作提供了理论基础。

Conclusion: LLM标注应被视为测量过程，需要系统化框架来确保透明度和可复现性，OLAF框架为软件工程研究中的LLM标注提供了方法论基础。

Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.

</details>


### [10] [Embedding Software Intent: Lightweight Java Module Recovery](https://arxiv.org/abs/2512.15980)
*Yirui He,Yuqi Huai,Xingyu Chen,Joshua Garcia*

Main category: cs.SE

TL;DR: ClassLAR：一种基于类名和语言模型的轻量级方法，用于从单体Java系统中恢复Java模块，性能优于现有技术且速度快3.99-10.5倍


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模不断扩大，仅依赖代码级抽象已不切实际。虽然架构抽象有助于管理这些系统，但保持其与实际代码的一致性一直存在问题。Java 9引入的JPMS（Java平台模块系统）通过在语言级别支持显式模块规范来解决这一限制，但将现有单体项目模块化为JPMS模块仍是一个开放挑战，因为现有架构恢复技术无法有效恢复模块。

Method: ClassLAR（基于类名和语言模型的架构恢复）是一种新颖、轻量级且高效的方法，使用完全限定类名从单体Java系统中恢复Java模块。该方法利用语言模型从包名和类名中提取语义信息，捕捉结构和功能意图。

Result: 在20个流行的Java项目评估中，ClassLAR在架构级相似性指标上优于所有最先进的技术，同时执行时间比现有技术快3.99到10.50倍。

Conclusion: ClassLAR为解决现有Java项目向JPMS模块化迁移的挑战提供了一种有效的解决方案，通过轻量级方法实现了更好的架构恢复效果和显著的速度优势。

Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.

</details>


### [11] [LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)](https://arxiv.org/abs/2512.16070)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: LLM4Perf：基于大语言模型的反馈式采样框架，用于多目标性能建模，在68.8%的场景中优于传统方法


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的性能高度依赖复杂配置选项，现有采样方法在多目标优化和利用文档语义信息方面存在不足。大语言模型（LLMs）的成功应用启发我们探索：LLMs能否作为多目标性能建模的有效采样器？

Method: 提出LLM4Perf反馈式框架，系统评估LLM引导的采样过程。该框架利用LLM的双重能力：配置空间剪枝和反馈驱动的策略优化，在四个真实世界的高度可配置系统中进行实证研究。

Result: LLM4Perf在112个评估场景中的77个（68.8%）取得最佳性能。LLM的配置空间剪枝能力显著，在448个案例中的410个（91.5%）改进了基线方法的性能。研究还分析了LLM选择和超参数对效果的影响。

Conclusion: LLMs在性能工程中具有显著有效性，其成功机制源于配置空间剪枝和反馈驱动的策略优化能力。该研究为LLM在性能建模中的应用提供了有力证据和具体见解。

Abstract: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

</details>


### [12] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 该论文对2015-2025年间42篇关于Apache Kafka的同行评审研究进行了结构化综合，识别了9种常用设计模式，分析了使用趋势和基准测试实践，并提出了统一的分类法和决策启发式方法。


<details>
  <summary>Details</summary>
Motivation: 尽管Apache Kafka已成为高吞吐量事件流处理的基础平台并得到广泛应用，但关于可重用架构设计模式和可复现基准测试方法的研究在学术和工业出版物中仍然分散，缺乏系统性整理。

Method: 对2015-2025年间42篇同行评审研究进行结构化综合分析，识别出9种常用Kafka设计模式，分析其共同使用趋势、领域特定部署和基准测试实践，包括TPCx Kafka、Yahoo Streaming Benchmark等标准套件和自定义工作负载。

Result: 研究发现配置披露、评估严谨性和可复现性存在显著不一致，限制了跨研究比较和实际复现。研究提供了统一的分类法、模式基准矩阵和可操作的决策启发式方法。

Conclusion: 该研究为架构师和研究人员设计可复现、高性能和容错的基于Kafka的事件流系统提供了实用指导，通过统一框架解决了现有研究的碎片化问题。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [13] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: LLM作为代码生成评估器(LaaJ)在COBOL现代化场景中存在盲点，结合轻量级分析检查器提供提示可显著提升错误检测率至94%。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估器(LaaJ)在代码生成中虽具可扩展性，但容易忽略领域特定问题，在关键评估任务中可靠性存疑。特别是在COBOL现代化等工业场景中，生产级LaaJ仍会遗漏关键错误。

Method: 1) 分析生成的COBOL程序和LaaJ判断，基于专家知识构建初步分类法；2) 开发轻量级分析检查工具，能检测30+种实践中的领域特定问题；3) 将检查器输出作为分析提示动态注入LaaJ提示中，促使其重新审视可能忽略的方面。

Result: 实验在100个程序测试集上使用4个生产级LaaJ：LaaJ单独仅检测约45%的错误，分析检查器单独缺乏解释深度。结合使用时，LaaJ+提示配置达到最高94%的覆盖率，产生质量更高、更准确的解释。

Conclusion: 分析-LLM混合方法能显著提升部署管道中的评估可靠性，证明结合轻量级分析工具与LLM评估器可有效弥补各自的局限性，在工业代码生成场景中实现更可靠的评估。

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [14] [Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation](https://arxiv.org/abs/2512.16335)
*Yibiao Yang,Qingyang Li,Maolin Sun,Jiangchang Wu,Yuming Zhou*

Main category: cs.SE

TL;DR: 该研究比较了基于bug诱导提交(BIC)的策略与谱基故障定位(SBFL)技术在编译器故障定位中的效果，发现BIC策略在实际应用中表现相当甚至优于SBFL技术。


<details>
  <summary>Details</summary>
Motivation: 编译器故障可能导致严重后果，但现有SBFL技术在实际编译器调试中的有效性尚未与开发者常用的BIC策略进行系统比较。本研究旨在填补这一空白，评估两种策略在编译器故障定位中的实际效果。

Method: 采用BIC策略(Basic方法)：识别最近的好版本和最早的问题版本，通过二分查找定位bug诱导提交，将该提交中修改的所有文件标记为潜在故障文件。使用包含60个GCC bug和60个LLVM bug的基准测试集，将Basic方法与代表性SBFL技术进行严格比较。

Result: 分析表明，Basic方法在性能上与最先进的SBFL技术相当，在许多情况下甚至优于SBFL技术，特别是在关键的Top-1和Top-5排名指标上表现更佳。

Conclusion: 该研究为SBFL技术在真实编译器调试场景中的实际有效性提供了新见解。建议未来研究在开发和评估新的编译器故障隔离方法时，将Basic方法作为基线进行比较。

Abstract: Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.

</details>


### [15] [An Empirical Study of the Realism of Mutants in Deep Learning](https://arxiv.org/abs/2512.16741)
*Zaheed Ahmed,Philip Makedonski,Jens Grabowski*

Main category: cs.SE

TL;DR: 该研究首次实证比较了深度学习中的预训练和后训练变异方法在真实性方面的表现，发现预训练变异体比后训练变异体与真实故障具有更强的耦合性和行为相似性，但计算成本更高。


<details>
  <summary>Details</summary>
Motivation: 变异分析在传统软件开发中已成熟，但在深度学习中应用时，其核心假设（变异体行为与真实故障相似）尚未得到验证。需要比较预训练和后训练两种变异方法在深度学习中的真实性表现。

Method: 引入统计框架量化变异体与真实故障的耦合强度和行为相似性，使用公开的bug数据集（CleanML、DeepFD、DeepLocalize、defect4ML），用最先进的工具生成代表两种方法的变异体。

Result: 预训练变异体比后训练变异体表现出更强的耦合性和更高的行为相似性，表明其真实性更高。但预训练变异计算成本显著更高。

Conclusion: 预训练变异方法在真实性方面优于后训练变异，但其高计算成本凸显了需要开发更有效的后训练变异算子，以达到或超过预训练变异体所展示的真实性水平。

Abstract: Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.
  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.
  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.

</details>


### [16] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 本文首次对LLM在软件工程任务中的概念级可解释性进行研究，通过概念激活向量分析发现LLM内部将注释作为独立潜在概念，并能区分不同类型注释，激活/抑制这些概念会导致性能显著变化（-90%到+67%），代码总结任务最能激活注释概念。


<details>
  <summary>Details</summary>
Motivation: 虽然注释是源代码的非功能性元素，但大型语言模型在执行软件工程任务时经常依赖它们。然而，模型在何处依赖注释以及这种依赖如何影响性能，目前仍不清楚。本研究旨在通过概念级可解释性分析来理解LLM在SE任务中对注释的内部表示和依赖机制。

Method: 使用概念激活向量（CAV）分析LLM在三种SE任务（代码补全、翻译和精炼）中的内部注释表示。通过系统性地激活和抑制LLM嵌入空间中的注释概念，观察性能变化。同时进行受控实验，使用相同代码输入让LLM执行10个不同的SE任务，测量其潜在表示中注释概念的激活程度。

Result: LLM不仅将注释内部化为不同的潜在概念，还能区分Javadocs、内联和多行注释等子类型。激活/抑制这些概念会导致显著、模型特定且任务依赖的性能变化（范围从-90%到+67%）。代码总结任务最能激活注释概念，而代码补全任务对注释的敏感性最弱。

Conclusion: 这项研究为构建新的SE工具和模型开辟了新方向，即通过推理和操作内部概念表示而非仅依赖表层输入。研究结果表明，理解LLM内部概念表示对于优化SE任务性能至关重要，特别是对于注释相关的任务。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [17] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: CAFFE是一个用于评估LLM反事实公平性的结构化测试框架，通过定义明确的测试组件、自动生成测试数据和使用语义相似度评估，相比现有方法能更全面地检测偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现代软件系统中的重要性日益增长，公平性问题变得越来越紧迫。现有方法主要使用蜕变测试来检测公平性问题，但需要更系统、更结构化的测试框架来评估LLM的反事实公平性。

Method: CAFFE框架包含三个核心部分：(1) 通过明确定义的组件（包括提示意图、对话上下文、输入变体、预期公平性阈值和测试环境配置）形式化LLM公平性测试用例；(2) 自动生成有针对性的测试数据；(3) 使用语义相似度指标评估模型响应。

Result: 在三种不同架构家族的LLM上进行的实验表明，CAFFE相比现有的蜕变测试方法，能够实现更广泛的偏见覆盖和更可靠的不公平行为检测。

Conclusion: CAFFE为LLM反事实公平性测试提供了一个结构化、意图感知的框架，能够更全面、更可靠地检测模型中的偏见问题，为LLM公平性评估提供了新的视角和方法。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: LOG.io 是一个为分布式数据流水线设计的解决方案，支持正确的回滚恢复和细粒度数据溯源捕获，适用于无服务器架构，性能在特定场景下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 分布式数据流水线需要可靠的故障恢复机制和数据溯源能力，特别是在无服务器架构中。现有解决方案如ABS协议在某些场景下存在性能限制，需要更灵活、非阻塞的恢复方案。

Method: LOG.io 采用基于日志的回滚恢复协议，支持通用编程模型，包括非确定性算子、外部系统交互和自定义代码。它是非阻塞的，允许失败算子独立恢复而不中断其他算子，支持数据并行化和算子动态扩展。

Result: 在SAP Data Intelligence系统中的实验表明：当流水线存在慢算子且事件吞吐量适中时，LOG.io在正常处理时与ABS相当，在恢复时优于ABS。其他情况下ABS表现更好，但数据并行化能显著减少LOG.io开销而ABS无法改善。数据溯源捕获的开销在所有实验中均小于1.5%。

Conclusion: LOG.io为分布式数据流水线提供了有效的回滚恢复和细粒度数据溯源解决方案，在特定场景下优于传统方法，并通过数据并行化减少开销，具有实用价值。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [19] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: MMA通过多路径内存访问技术解决PCIe带宽瓶颈，提升GPU与主机内存间数据传输带宽4.62倍，显著改善LLM服务的首token时间和模型切换延迟。


<details>
  <summary>Details</summary>
Motivation: PCIe带宽已成为大语言模型性能的关键瓶颈，特别是在前缀缓存获取和模型切换场景中。当前异构协议（PCIe和NVLink）限制了主机内存与GPU间的带宽仅为单个PCIe链路带宽，导致服务器内部带宽未充分利用。

Method: 提出多路径内存访问（MMA）方案，首次实现GPU与主机内存间的高效多路径数据传输。通过动态库注入实现无缝部署，使LLM应用无需代码修改即可受益。

Result: 在测试平台中，MMA显著提升GPU与内存间数据传输带宽，峰值带宽达到245GB/s，相比原生单路径带宽提升4.62倍。端到端评估显示，MMA将LLM服务的首token时间减少1.14-2.38倍，将vLLM睡眠模式下的模型切换延迟降低1.12-2.48倍。

Conclusion: MMA有效解决了PCIe带宽瓶颈问题，通过多路径数据传输充分利用服务器内部带宽，显著提升LLM服务性能，且部署简便无需代码修改。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [20] [Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin](https://arxiv.org/abs/2512.16058)
*Yifei Qiu,Tianle Liao,Xin Jin,Shaohua Wu,Dusit Niyato,Qinyu Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种面向目标的语义孪生（GOST）框架，作为传统数字孪生的替代方案，用于解决空间-空-地-海一体化网络中的计算开销、同步延迟和语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 空间-空-地-海一体化网络（SAGSIN）作为6G系统的核心，需要实时态势感知和智能运维。传统数字孪生技术虽然能创建虚拟副本和模拟物理系统行为，但在SAGSIN环境中面临计算开销大、模型同步延迟和跨系统语义鸿沟等根本性限制。

Method: 提出面向目标的语义孪生（GOST）框架，强调"效用"而非"保真度"，利用语义技术和面向目标原则构建轻量级、任务特定的表示。框架分为三层：基于知识的语义、数据驱动的语义和面向目标原则。详细介绍了GOST的核心使能技术构建方法，并提出了多维评估框架。

Result: 通过针对远程卫星-无人机网络协同跟踪任务的案例研究，证明GOST在感知数据时效性和协同跟踪方面显著优于传统数字孪生。GOST能够有效解决传统数字孪生在SAGSIN环境中的局限性。

Conclusion: GOST作为一种变革性的孪生范式，通过优先考虑效用而非保真度，利用语义技术和面向目标原则，为SAGSIN的发展提供了指导方向，并提出了未来的研究方向。

Abstract: A space-air-ground-sea integrated network (SAGSIN) has emerged as a cornerstone of 6G systems, establishing a unified global architecture by integrating multi-domain network resources. Motivated by the demand for real-time situational awareness and intelligent operational maintenance, digital twin (DT) technology was initially regarded as a promising solution, owing to its capability to create virtual replicas and emulate physical system behaviors. However, in the context of SAGSIN, the high-fidelity, full-scale modeling paradigm inherent to conventional DTs encounters fundamental limitations, including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps. To address these limitations, this survey paper proposes a novel twinning framework: goal-oriented semantic twin (GOST). Unlike DTs that pursue physical mirroring, GOST prioritizes ``utility'' over ``fidelity,'' leveraging semantic technologies and goal-oriented principles to construct lightweight, task-specific representations. This paper systematically articulates the GOST framework through three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Furthermore, we provide a comprehensive tutorial on constructing GOST by detailing its core enabling technologies and introduce a multidimensional evaluation framework for GOST. We present a case study targeting collaborative tracking tasks in remote satellite-UAV networks, demonstrating that GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking. Finally, we outline research directions, establishing GOST as a transformative twinning paradigm to guide the development of SAGSIN.

</details>


### [21] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 本文提出INITSCOPE框架和SCABENCH基准测试，通过分析代码加载与执行关系来诊断和缓解serverless冷启动问题，相比现有工具提升40%定位准确率和减少64%诊断工作量。


<details>
  <summary>Details</summary>
Motivation: Serverless计算简化了部署和扩展，但冷启动延迟仍然是主要性能瓶颈。现有研究将缓解措施视为黑盒优化，而本文将其视为开发者可见的设计问题，旨在提供更有效的诊断和缓解方案。

Method: 1) 分析81个开源serverless系统的issue报告，建立初始化反模式、修复策略和诊断挑战的分类体系；2) 开发SCABENCH可复现基准测试；3) 提出INITSCOPE轻量级分析框架，将加载的代码与执行的代码关联起来。

Result: 在SCABENCH上，INITSCOPE相比现有工具将定位准确率提升高达40%，诊断工作量减少64%。开发者研究表明，使用该框架能获得更高的任务准确率和更快的诊断速度。

Conclusion: 该研究推动了serverless设计中基于证据、性能感知的冷启动缓解实践。研究工件已公开可用，供未来研究和改进。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [22] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 提出一个在线调度框架，通过条件负载均衡、动态分区和作业迁移来解决MIG GPU共享中的资源争用和碎片化问题，显著提升系统效率


<details>
  <summary>Details</summary>
Motivation: 现代GPU工作负载需要高效资源共享，但NVIDIA的MIG技术虽然提供硬件级GPU分区，仍面临PCIe带宽等共享组件的资源争用问题，以及由于MIG配置有限导致的GPU碎片化问题，特别是作业到达和终止后的刚性配置文件放置约束

Method: 提出在线调度框架，集成条件负载均衡、动态分区和作业迁移技术，动态调整作业放置以最小化争用，并重组GPU分配以应对内部和外部碎片化

Result: 实验结果显示该方法显著提升系统效率，当所有技术都应用时，makespan（完成时间）最高提升35%

Conclusion: 提出的调度框架有效解决了MIG GPU共享中的资源争用和碎片化问题，通过动态调整和重组分配显著提高了系统效率

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [23] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 提出Staggered Batch Scheduling (SBS)调度机制，通过缓冲请求形成最优执行批次，解决DP+EP架构中内部同步成本高导致的TTFT延迟问题


<details>
  <summary>Details</summary>
Motivation: 随着LLM服务向复杂的分布式架构（特别是P/D分离的大规模DP+EP范式）演进，引入了独特的调度挑战。与传统部署不同，DP+EP架构具有高内部同步成本，即时请求调度会导致严重的引擎内部排队和并行化气泡，降低首令牌时间(TTFT)

Method: 提出Staggered Batch Scheduling (SBS)机制，通过有意识地缓冲请求来形成最优执行批次，实现时间解耦以消除内部排队气泡。同时利用缓冲创建的调度窗口，引入负载感知全局分配策略，平衡DP单元在Prefill和Decode阶段的计算负载

Result: 在部署Deepseek-V3的生产H800集群上，相比最先进的即时调度基线，系统将TTFT降低了30%-40%，吞吐量提高了15%-20%

Conclusion: SBS调度机制通过时间解耦和负载感知全局分配，有效解决了DP+EP架构中的调度挑战，显著提升了LLM服务的响应时间和吞吐量性能

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [24] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: Lotus提出了一种在分解内存架构上实现锁分解的分布式事务系统，通过将锁管理从内存节点转移到计算节点来消除RDMA网络接口卡瓶颈，提升系统可扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有分解内存上的分布式事务系统中，内存节点的RDMA网络接口卡成为主要性能瓶颈，这源于大量用于锁操作的单边原子操作，限制了系统的可扩展性。

Method: 1) 锁分解：将锁从数据中分离，所有锁操作在计算节点执行；2) 应用感知锁管理：利用OLTP工作负载的局部性进行锁分片并保持负载均衡；3) 锁优先事务协议：将锁定阶段作为读写事务执行的第一步；4) 无锁重建恢复机制：将锁视为临时状态，避免重建锁。

Result: 实验结果显示，与最先进的分解内存事务系统相比，Lotus将事务吞吐量提升最高达2.1倍，延迟降低最高达49.4%。

Conclusion: Lotus通过锁分解有效解决了分解内存架构中RDMA网络接口卡的瓶颈问题，实现了高可扩展的分布式事务处理，并通过创新的锁管理机制和恢复策略提升了系统性能。

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [25] [FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store](https://arxiv.org/abs/2512.16148)
*Zhisheng Hu,Jiacheng Shen,Ming-Chang Yang*

Main category: cs.DC

TL;DR: FlexKV是一个内存解耦的键值存储系统，通过索引代理技术将索引动态卸载到计算节点，解决了现有系统对单边原子操作的过度依赖和计算侧缓存效率受限的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有内存解耦的键值存储系统存在两个关键问题：1）索引处理过度依赖单边原子操作，2）计算侧缓存效率受限，导致性能不佳。需要一种新方法来提升内存解耦KV存储的性能。

Method: FlexKV采用索引代理技术，将索引动态卸载到计算节点，利用其强大CPU加速索引处理并维护高性能计算侧缓存。具体包括：1）基于秩感知的热度检测算法平衡索引负载；2）两级计算节点内存优化方案；3）RPC聚合的缓存管理机制减少缓存一致性开销。

Result: 实验结果显示，与最先进的内存解耦KV存储相比，FlexKV将吞吐量提升高达2.94倍，延迟降低高达85.2%。

Conclusion: FlexKV通过索引代理技术有效解决了内存解耦KV存储的性能瓶颈，显著提升了系统性能，为内存解耦架构下的数据管理提供了高效解决方案。

Abstract: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.

</details>


### [26] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 本文描述了一个专为科学AI工作负载设计的联邦计算平台，提供从模型开发到部署的全生命周期集成服务，支持可重复部署和跨基础设施的透明访问。


<details>
  <summary>Details</summary>
Motivation: 科学AI工作负载需要可重复、透明的分布式基础设施访问，以及覆盖完整机器学习生命周期的集成用户体验。现有解决方案往往缺乏对科学工作负载的专门支持，特别是在可重复部署和跨云连续体部署方面。

Method: 构建一个联邦计算平台，通过全面服务目录提供集成用户体验，包括：1) 模型开发（专用交互式开发环境）；2) 训练（GPU资源、标注工具、实验跟踪、联邦学习支持）；3) 部署（覆盖云连续体的多种部署选项）。平台还提供AI模型可追溯性和可重复性工具，与不同AI模型提供商、数据集和存储资源集成。

Result: 平台成功实现了对物理分布式e-Infrastructures的一致透明访问，提供了完整的机器学习生命周期支持，包括开发、训练和部署功能。平台具有高度可定制性，能够降低外部社区的采用门槛。

Conclusion: 该联邦计算平台为科学AI工作负载提供了全面的解决方案，通过集成服务和可定制设计，有效支持了机器学习全生命周期管理，促进了科学AI应用的开发和部署。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [27] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 提出了一种CPU-GPU协同推理框架，通过GPU专家缓存机制减少数据传输，利用CPU多线程处理缓存未命中，在消费级硬件上实现MoE模型的高效推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算需求高，MoE模型虽通过选择性激活参数提高效率，但仍需大量内存超出消费级GPU容量。传统卸载方法在CPU和GPU间传输权重会引入延迟，影响推理性能。

Method: 提出CPU-GPU协同推理框架，包含GPU专家缓存机制减少数据传输，通过缓存命中加速推理；将计算卸载到CPU处理缓存未命中，利用CPU多线程优化。

Result: 评估显示该框架在单请求推理场景下能提升性能，最大化消费级系统的硬件利用率。

Conclusion: CPU-GPU协作框架通过专家缓存和计算卸载，有效解决了MoE模型在消费级硬件上的部署挑战，展示了硬件协同利用的潜力。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


### [28] [Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint](https://arxiv.org/abs/2512.16792)
*Endar Suprih Wihidayat,Sieteng Soh,Kwan-Wu Chin,Duc-son Pham*

Main category: cs.DC

TL;DR: 提出了多阶段边缘服务器升级(M-ESU)问题，通过多阶段规划优化MEC系统升级，包括服务器部署/升级决策和任务卸载优化，以最大化满足延迟要求的任务数量。


<details>
  <summary>Details</summary>
Motivation: 现有的MEC系统需要长期升级以适应任务增长和延迟要求变化，但缺乏考虑多阶段预算约束和成本折旧的系统规划方法。

Method: 提出M-ESU框架，包含MILP精确求解模型和M-ESU/H启发式算法，考虑预算约束、成本折旧、任务增长、服务器容量等多重因素。

Result: M-ESU/H在小网络中接近最优解(差距1.25%)且运行速度快几个数量级；在大网络中相比其他启发式方法提升任务满足率高达21.57%。

Conclusion: M-ESU为MEC系统长期规划提供了有效的解决方案，M-ESU/H算法具有良好性能和实用性，适用于大规模网络部署。

Abstract: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.

</details>
