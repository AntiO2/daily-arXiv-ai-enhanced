<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 15]
- [cs.DB](#cs.DB) [Total: 11]
- [cs.SE](#cs.SE) [Total: 27]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [The Category Mistake of Cislunar Time: Why NASA Cannot Synchronize What Doesn't Exist](https://arxiv.org/abs/2602.18641)
*Paul Borrill*

Main category: cs.DC

TL;DR: 该论文认为美国白宫协调月球时间(LTC)计划存在哲学范畴错误，将"同步时间"视为本体实体而非认识论构造，揭示了该工程计划建立在哲学混淆之上。


<details>
  <summary>Details</summary>
Motivation: 分析美国白宫2024年4月指示NASA在2026年12月前建立协调月球时间(LTC)计划的哲学基础。该计划假设可以通过在月球表面部署原子钟、计算相对论修正并通过LunaNet分发同步时间来构建统一时间标准。

Method: 使用Ryle和Spekkens提出的范畴错误概念，通过FITO假设、Spekkens的莱布尼茨操作主义、Wood-Spekkens微调论证以及本体论与认识论解释的区分来分析月球时间计划。借鉴量子力学中解决长期谜题的概念框架。

Result: 论文表明，将"同步时间"视为可以传输的本体实体而非观察者相对时钟关系的模型依赖表示是一个范畴错误。解决量子"神秘性"的同一概念移动——识别什么是认识论与什么是本体论——也消解了月球时间计划的表面一致性。

Conclusion: 月球时间计划是一个建立在哲学混淆之上的工程项目。作者提出了基于双边原子相互作用而非单向时间分发的交易性替代方案。

Abstract: In April 2024, the White House directed NASA to establish Coordinated Lunar Time (LTC) by December 2026. The programme assumes that a unified time standard can be constructed by deploying atomic clocks on the lunar surface, computing relativistic corrections, and distributing synchronized time via LunaNet. This paper argues that the entire enterprise rests on a category mistake in the sense introduced by Ryle and developed by Spekkens in quantum foundations: it treats "synchronized time" as an ontic entity -- something that exists independently and can be transmitted from authoritative sources to dependent receivers -- when it is in fact an epistemic construct: a model-dependent representation of observer-relative clock relationships. We analyze the cislunar time programme through the lens of Forward-In-Time-Only (FITO) assumptions, Spekkens' Leibnizian operationalism, the Wood-Spekkens fine-tuning argument, and the distinction between ontic and epistemic interpretations that has dissolved long-standing puzzles in quantum mechanics. We show that the same conceptual move that dissolves quantum "mysteries" -- recognizing what is epistemic versus what is ontic -- dissolves the apparent coherence of the cislunar time programme and reveals it as an engineering project built on a philosophical confusion. We sketch a transactional alternative grounded in bilateral atomic interactions rather than unidirectional time distribution.

</details>


### [2] [What Distributed Computing Got Wrong: The Category Mistake That Turned Design Choices into Laws of Nature](https://arxiv.org/abs/2602.18723)
*Paul Borrill*

Main category: cs.DC

TL;DR: 该论文认为分布式计算中的经典不可能性结果（如FLP、两将军问题、CAP定理）并非物理限制，而是源于"仅向前时间信息流"这一设计选择错误，而非自然法则。


<details>
  <summary>Details</summary>
Motivation: 论文旨在挑战分布式计算领域对经典不可能性定理的根本理解，指出这些结果被错误地视为物理限制，而实际上只是特定设计范式（仅向前时间信息流）的产物。

Method: 采用六步论证框架：1)引入范畴错误和本体/认知区分；2)识别仅向前时间信息流为隐藏公理；3)应用莱布尼茨原理证明该模型包含多余本体结构；4)探索放弃该假设的后果；5)证明不可能性定理仅适用于仅向前时间系统；6)提出交易式替代方案。

Result: 论证表明分布式计算中的经典不可能性结果并非物理限制，而是源于特定设计选择。通过放弃仅向前时间信息流假设，采用双边交易模型，可以解决这些"不可能"问题。

Conclusion: 分布式计算领域50年来一直在错误的设计空间中优化。经典不可能性定理是关于仅向前时间系统的定理，而非物理定律。采用交易式双边交互可以消除这些表面上的不可能性。

Abstract: The foundational impossibility results of distributed computing -- the Fischer-Lynch-Paterson theorem, the Two Generals Problem, the CAP theorem -- are widely understood as discoveries about the physical limits of coordination. This paper argues that they are nothing of the sort. They are consequences of a category mistake: treating Forward-In-Time-Only (FITO) information flow as a law of nature rather than recognizing it as a design choice inherited from Shannon's channel model and Lamport's happened-before relation. We develop this argument in six steps. First, we introduce the category mistake framework from Ryle through Spekkens' ontic/epistemic distinction in quantum foundations. Second, we identify FITO as the hidden axiom that unifies the classical impossibility results. Third, we apply Spekkens' Leibnizian principle to show that FITO-based models contain surplus ontological structure. Fourth, we develop the counterfactual: what changes when FITO is dropped. Fifth, we demonstrate that the impossibility theorems are theorems about FITO systems, not about physics. Sixth, we sketch the transactional alternative -- bilateral interactions that dissolve the apparent impossibilities by replacing unidirectional message passing with atomic bilateral transactions. The implication is that distributed computing has spent fifty years optimizing within the wrong design space.

</details>


### [3] [BiScale: Energy-Efficient Disaggregated LLM Serving via Phase-Aware Placement and DVFS](https://arxiv.org/abs/2602.18755)
*Omar Basit,Yunzhao Liu,Z. Jonny Kong,Y. Charlie Hu*

Main category: cs.DC

TL;DR: BiScale是一个用于解耦LLM服务的双层能量优化框架，通过联合优化预填充和解码阶段的放置和DVFS，在满足SLO约束的同时显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: LLM推理能耗巨大，现有的自动扩展方法粒度太粗无法跟踪快速的工作负载波动，而细粒度的DVFS在解耦架构下又面临阶段不对称动态和资源配置与频率控制的耦合问题。

Method: BiScale采用双层优化框架：在粗粒度时间尺度上计算阶段感知的放置和基准频率以最小化能耗；在细粒度时间尺度上，对预填充阶段使用模型预测控制（MPC）考虑队列演化和未来TTFT影响，对解码阶段使用轻量级松弛感知适应来利用其更平滑的内存受限动态。

Result: 在16个H100集群上服务Llama 3.3 70B并使用生产风格trace进行评估，BiScale在满足TTFT/TPOT SLO的同时，相比DistServe在预填充阶段减少能耗达39%，在解码阶段减少能耗达48%。

Conclusion: BiScale通过分层设计实现了跨时间尺度的协调控制，在保持严格服务SLO的同时显著降低了LLM推理的能耗，为解决解耦LLM服务中的能量优化问题提供了有效方案。

Abstract: Prefill/decode disaggregation is increasingly adopted in LLM serving to improve the latency-throughput tradeoff and meet strict TTFT and TPOT SLOs. However, LLM inference remains energy-hungry: autoscaling alone is too coarse-grained to track fast workload fluctuations, and applying fine-grained DVFS under disaggregation is complicated by phase-asymmetric dynamics and coupling between provisioning and frequency control.
  We present BiScale, a two-tier energy optimization framework for disaggregated LLM serving. BiScale jointly optimizes placement and DVFS across prefill and decode using predictive latency and power models. At coarse timescales, BiScale computes phase-aware placement and baseline frequencies that minimize energy while satisfying SLO constraints. At fine timescales, BiScale dynamically adapts GPU frequency per iteration using stage-specific control: model predictive control (MPC) for prefill to account for queue evolution and future TTFT impact, and lightweight slack-aware adaptation for decode to exploit its smoother, memory-bound dynamics. This hierarchical design enables coordinated control across timescales while preserving strict serving SLOs.
  Evaluation on a 16x H100 cluster serving Llama 3.3 70B with production-style traces shows that BiScale meets TTFT/TPOT SLOs while reducing energy by up to 39% in prefill and 48% in decode relative to DistServe.

</details>


### [4] [Carbon-aware decentralized dynamic task offloading in MIMO-MEC networks via multi-agent reinforcement learning](https://arxiv.org/abs/2602.18797)
*Mubshra Zulfiqar,Muhammad Ayzed Mirza,Basit Qureshi*

Main category: cs.DC

TL;DR: CADDTO-PPO：基于多智能体PPO的碳感知去中心化动态任务卸载框架，用于多用户MIMO-MEC系统，联合优化碳排放、缓冲区延迟和能量浪费


<details>
  <summary>Details</summary>
Motivation: 大规模物联网微服务需要将可再生能源收集集成到移动边缘计算中，以实现可持续的eScience基础设施。随机任务到达与间歇性绿色能源之间的时空不匹配，以及多天线上行链路中复杂的用户间干扰，使得实时资源管理变得复杂。传统集中式优化和离策略强化学习在密集网络中面临可扩展性和信令开销问题。

Method: 提出CADDTO-PPO框架，将多用户MIMO-MEC系统建模为去中心化部分可观测马尔可夫决策过程。采用带参数共享的去中心化执行架构，使物联网智能体仅基于局部观测自主做出细粒度功率控制和卸载决策。设计了碳优先奖励结构，自适应地优先使用绿色时隙进行数据传输。

Result: 实验结果表明CADDTO-PPO优于深度确定性策略梯度和基于Lyapunov的基线方法。该框架实现了最低的碳强度，并在极端流量负载下保持接近零的数据包溢出率。架构分析验证了框架具有恒定的O(1)推理复杂度。

Conclusion: CADDTO-PPO为未来可持续物联网部署提供了一个理论轻量级、可扩展的解决方案，能够有效管理随机任务和间歇性能源之间的时空不匹配问题，同时降低碳排放。

Abstract: Massive internet of things microservices require integrating renewable energy harvesting into mobile edge computing (MEC) for sustainable eScience infrastructures. Spatiotemporal mismatches between stochastic task arrivals and intermittent green energy along with complex inter-user interference in multi-antenna (MIMO) uplinks complicate real-time resource management. Traditional centralized optimization and off-policy reinforcement learning struggle with scalability and signaling overhead in dense networks. This paper proposes CADDTO-PPO, a carbon-aware decentralized dynamic task offloading framework based on multi-agent proximal policy optimization. The multi-user MIMO-MEC system is modeled as a Decentralized Partially Observable Markov Decision Process (DEC-POMDP) to jointly minimize carbon emissions and buffer latency and energy wastage. A scalable architecture utilizes decentralized execution with parameter sharing (DEPS), which enables autonomous IoT agents to make fine-grained power control and offloading decisions based solely on local observations. Additionally, a carbon-first reward structure adaptively prioritizes green time slots for data transmission to decouple system throughput from grid-dependent carbon footprints. Finally, experimental results demonstrate CADDTO-PPO outperforms deep deterministic policy gradient (DDPG) and lyapunov-based baselines. The framework achieves the lowest carbon intensity and maintains near-zero packet overflow rates under extreme traffic loads. Architectural profiling validates the framework to demonstrate a constant $O(1)$ inference complexity and theoretical lightweight feasibility for future generation sustainable IoT deployments.

</details>


### [5] [WANSpec: Leveraging Global Compute Capacity for LLM Inference](https://arxiv.org/abs/2602.18931)
*Noah Martin,Fahad Dogar*

Main category: cs.DC

TL;DR: WANSpec通过将推测解码的草稿模型卸载到利用率较低的数据中心，减少高需求数据中心的计算负载，同时保持低延迟


<details>
  <summary>Details</summary>
Motivation: 全球数据中心GPU资源分布不均，高需求数据中心（运行大模型）负载过重，而其他数据中心（运行小模型）利用率不足，导致LLM推理延迟问题

Method: 提出WANSpec系统，利用推测解码技术，将草稿模型部署到利用率较低的数据中心，通过冗余策略避免延迟增加

Result: 实验表明WANSpec能将高需求数据中心草稿模型的前向传播减少50%以上，同时保持低延迟

Conclusion: WANSpec通过智能利用地理分布的计算资源，有效缓解数据中心容量问题，提高资源利用率

Abstract: Data centers capable of running large language models (LLMs) are spread across the globe. Some have high end GPUs for running the most advanced models (100B+ parameters), and others are only suitable for smaller models (1B parameters). The most capable GPUs are under high demand thanks to the rapidly expanding applications of LLMs. Choosing the right location to run an LLM inference workload can have consequences on the latency of requests due to these high demands. In this work, we explore options to shift some aspects of inference to the under-utilized data centers. We first observe the varying delays affecting inference in AWS services from different regions, demonstrating that load is not spread evenly. We then introduce WANSpec, which offloads part of LLM generation to the under-utilized data centers. In doing so, WANSpec can mitigate capacity issues as well as effectively use on-site compute (ie at universities) to augment cloud providers. This is done with speculative decoding, a widely used technique to speed up auto-regressive decoding, by moving the draft model to the under-utilized compute resources. Our experiments in simulation and cloud deployments show that WANSpec can judiciously employ redundancy to avoid increases in latency while still reducing the forward passes of speculative decoding's draft model in high demand data centers by over 50%.

</details>


### [6] [ucTrace: A Multi-Layer Profiling Tool for UCX-driven Communication](https://arxiv.org/abs/2602.19084)
*Emir Gencer,Mohammad Kefah Taha Issa,Ilyas Turimbetov,James D. Trotter,Didem Unat*

Main category: cs.DC

TL;DR: ucTrace是一个新的性能分析工具，专门用于分析和可视化HPC环境中UCX驱动的通信，填补了现有工具在UCX层面细粒度通信跟踪的空白。


<details>
  <summary>Details</summary>
Motivation: 现有性能分析工具缺乏UCX层面的细粒度通信跟踪，无法捕获传输层行为，或者仅限于特定的MPI实现，这限制了系统管理员和开发人员优化HPC通信性能的能力。

Method: 开发了ucTrace工具，通过在UCX层面分析消息传递，将主机和设备（如GPU和NIC）之间的操作直接链接到其原始的MPI函数，并提供交互式可视化来分析进程和设备特定的交互。

Result: ucTrace能够分析MPI点对点行为在不同UCX设置下的表现，比较不同MPI库的Allreduce操作，分析线性求解器的通信模式，研究NUMA绑定效果，以及分析大规模GPU加速的GROMACS分子动力学模拟。

Conclusion: ucTrace填补了HPC通信分析的重要空白，为系统管理员、库开发者和应用开发者提供了优化性能和调试通信模式的强大工具，该工具已开源发布。

Abstract: UCX is a communication framework that enables low-latency, high-bandwidth communication in HPC systems. With its unified API, UCX facilitates efficient data transfers across multi-node CPU-GPU clusters. UCX is widely used as the transport layer for MPI, particularly in GPU-aware implementations. However, existing profiling tools lack fine-grained communication traces at the UCX level, do not capture transport-layer behavior, or are limited to specific MPI implementations.
  To address these gaps, we introduce ucTrace, a novel profiler that exposes and visualizes UCX-driven communication in HPC environments. ucTrace provides insights into MPI workflows by profiling message passing at the UCX level, linking operations between hosts and devices (e.g., GPUs and NICs) directly to their originating MPI functions. Through interactive visualizations of process- and device-specific interactions, ucTrace helps system administrators, library and application developers optimize performance and debug communication patterns in large-scale workloads. We demonstrate ucTrace's features through a wide range of experiments including MPI point-to-point behavior under different UCX settings, Allreduce comparisons across MPI libraries, communication analysis of a linear solver, NUMA binding effects, and profiling of GROMACS MD simulations with GPU acceleration at scale. ucTrace is publicly available at https://github.com/ParCoreLab/ucTrace.

</details>


### [7] [A Formal Framework for Predicting Distributed System Performance under Faults](https://arxiv.org/abs/2602.19088)
*Ziwei Zhou,Si Liu,Zhou Zhou,Peixin Wang,MIn Zhang*

Main category: cs.DC

TL;DR: PERF：首个能够系统预测分布式系统在不同故障场景下性能的形式化框架，通过故障注入器和模型组合实现性能属性统计分析


<details>
  <summary>Details</summary>
Motivation: 当前分布式系统在复杂环境中运行，不可避免地涉及故障甚至对抗行为。从形式化设计直接预测系统在这些环境下的性能是一个长期存在的挑战。

Method: 提出一个形式化框架，包含故障注入器和多种故障类型（可作为库重用），通过模型组合将系统和故障注入器集成到统一模型中，适用于吞吐量和延迟等性能属性的统计分析。该框架在Maude中形式化，并实现为自动化工具PERF。

Result: 应用于代表性分布式系统时，PERF能够准确预测系统在不同故障设置下的性能，形式化设计的估计结果与实际部署的评估结果一致。

Conclusion: PERF是首个能够系统预测分布式系统在多样化故障场景下性能的形式化框架，为分布式系统的性能分析和可靠性评估提供了有效工具。

Abstract: Today's distributed systems operate in complex environments that inevitably involve faults and even adversarial behaviors. Predicting their performance under such environments directly from formal designs remains a longstanding challenge. We present the first formal framework that systematically enables performance prediction of distributed systems across diverse faulty scenarios. Our framework features a fault injector together with a wide range of faults, reusable as a library, and model compositions that integrate the system and the fault injector into a unified model suitable for statistical analysis of performance properties such as throughput and latency. We formalize the framework in Maude and implement it as an automated tool, PERF. Applied to representative distributed systems, PERF accurately predicts system performance under varying fault settings, with estimations from formal designs consistent with evaluations on real deployments.

</details>


### [8] [Asymptotic Subspace Consensus in Dynamic Networks](https://arxiv.org/abs/2602.19121)
*Matthias Függer,Thomas Nowak*

Main category: cs.DC

TL;DR: 论文提出了渐近子空间共识问题，要求进程输出收敛到公共子空间且保持在初始向量凸包内，这是对渐近共识的松弛。在遗忘消息对手模型中给出了完全可解性刻画，并展示了现有算法在较弱通信假设下的优雅降级能力。


<details>
  <summary>Details</summary>
Motivation: 传统渐近共识要求输出收敛到单个点（零维仿射子空间），这在弱通信假设下可能无法实现。论文提出渐近子空间共识作为更宽松的替代方案，允许输出收敛到更高维度的子空间，从而在较弱网络条件下仍能达成某种形式的共识。

Method: 在遗忘消息对手模型中分析渐近子空间共识问题。通过理论分析给出完全可解性刻画，证明现有渐近共识算法在较弱通信条件下能优雅降级为子空间共识。同时分析达到低于初始维度的收敛速率界限。

Result: 获得了渐近子空间共识在遗忘消息对手模型中的完全可解性特征。证明了现有渐近共识算法在弱通信假设下能自然降级为子空间共识。给出了达到低维子空间的收敛速率界限。

Conclusion: 渐近子空间共识是渐近共识的有用松弛，在较弱通信条件下仍可实现。现有算法具有优雅降级特性，为分布式系统设计提供了更灵活的理论基础。

Abstract: We introduce the problem of asymptotic subspace consensus, which requires the outputs of processes to converge onto a common subspace while remaining inside the convex hull of initial vectors.This is a relaxation of asymptotic consensus in which outputs have to converge to a single point, i.e., a zero-dimensional affine subspace.
  We give a complete characterization of the solvability of asymptotic subspace consensus in oblivious message adversaries. In particular, we show that a large class of algorithms used for asymptotic consensus gracefully degrades to asymptotic subspace consensus in distributed systems with weaker assumptions on the communication network. We also present bounds on the rate by which a lower-than-initial dimension is reached.

</details>


### [9] [Semantic Conflict Model for Collaborative Data Structures](https://arxiv.org/abs/2602.19231)
*Georgii Semenov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 提出一种用于协作数据结构的冲突模型，支持显式、本地优先的冲突解决，无需中央协调，通过操作间的语义依赖识别冲突，并使用三路合并进行解决。


<details>
  <summary>Details</summary>
Motivation: 现有协作系统存在冲突解决不透明或依赖中央协调的问题。CRDTs虽然能确保收敛，但其冲突解决是隐式且不透明的；而现有协调技术通常需要中央协调。需要一种既能显式解决冲突，又能保持本地优先且无需中央协调的方法。

Method: 提出一个冲突模型，通过操作间的语义依赖识别冲突，通过将冲突操作重新基于到协调操作上来解决冲突，使用复制日志上的三路合并技术。在协作寄存器上演示该方法，包括显式公式化的最后写入者胜出寄存器和支持半自动协调的多寄存器实体。

Result: 该方法能够在协作数据结构中实现显式、本地优先的冲突解决，无需中央协调。通过语义依赖识别冲突，并通过三路合并技术有效解决冲突，在协作寄存器上得到验证。

Conclusion: 提出的冲突模型为协作数据结构提供了显式、本地优先的冲突解决方法，克服了现有CRDTs隐式解决和依赖中央协调的局限性，为异步协作系统提供了更透明和灵活的冲突管理方案。

Abstract: Digital collaboration systems support asynchronous work over replicated data, where conflicts arise when concurrent operations cannot be unambiguously integrated into a shared history. While Conflict-Free Replicated Data Types (CRDTs) ensure convergence through built-in conflict resolution, this resolution is typically implicit and opaque to users, whereas existing reconciliation techniques often rely on centralized coordination. This paper introduces a conflict model for collaborative data structures that enables explicit, local-first conflict resolution without central coordination. The model identifies conflicts using semantic dependencies between operations and resolves them by rebasing conflicting operations onto a reconciling operation via a three-way merge over a replicated journal. We demonstrate our approach on collaborative registers, including an explicit formulation of the Last-Writer-Wins Register and a multi-register entity supporting semi-automatic reconciliation.

</details>


### [10] [Complex Event Processing in the Edge: A Combined Optimization Approach for Data and Code Placement](https://arxiv.org/abs/2602.19338)
*Halit Uyanık,Tolga Ovatman*

Main category: cs.DC

TL;DR: 提出一个基于约束编程优化的Python库，用于在物联网设备上平衡复杂事件处理任务图的执行成本，优化关键路径性能，提高吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 物联网设备硬件和计算能力有限，但需要处理日益多样化的输入数据和复杂任务（如复杂事件处理）。现有解决方案未能充分考虑边缘设备的限制，需要一种能优化任务执行分配的方法。

Method: 采用约束编程优化方法，平衡复杂事件处理任务图中不同路径的执行成本，优化关键路径性能。实现为Python库，能够自适应优化代码和I/O分配，抽象通信细节，并在物联网设备间虚拟化共享内存。

Result: 优化关键路径性能提高了复杂事件处理操作中的吞吐量，减少了多个设备间的延迟。实现的库使小规模物联网设备能够自适应优化执行。

Conclusion: 通过约束编程优化方法平衡复杂事件处理任务图的执行成本，可以有效提高物联网边缘设备的性能表现，为资源受限环境提供了可行的优化解决方案。

Abstract: The increasing variety of input data and complexity of tasks that are handled by the devices of internet of things (IoT) environments require solutions that consider the limited hardware and computation power of the edge devices. Complex event processing (CEP), can be given as an example, which involves reading and aggregating data from multiple sources to infer triggering of important events. In this study, we balance the execution costs between different paths of the CEP task graph with a constrained programming optimization approach and improve critical path performance. The proposed approach is implemented as a Python library, allowing small-scale IoT devices to adaptively optimize code and I/O assignments and improve overall latency and throughput. The implemented library abstracts away the communication details and allows virtualization of a shared memory between IoT devices. The results show that optimizing critical path performance increases throughput and reduces delay across multiple devices during CEP operations.

</details>


### [11] [Why iCloud Fails: The Category Mistake of Cloud Synchronization](https://arxiv.org/abs/2602.19433)
*Paul Borrill*

Main category: cs.DC

TL;DR: iCloud Drive的云同步语义与POSIX文件系统存在根本性差异，这种差异源于将分布式因果图投影到线性时间链的结构性错误，导致与Time Machine、git等工具不兼容，而Open Atomic Ethernet的事务语义可解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 本文旨在分析iCloud Drive等云存储系统为何在与传统开发工具（如Time Machine、git、自动化工具链）结合时出现故障，揭示这些故障源于将分布式系统错误地建模为线性时间系统的"范畴错误"。

Method: 通过统一分析iCloud与各种工具的兼容性问题，提供直接证据包括文档化的损坏事件和涉及366GB分歧状态的案例研究，识别五个相互关联的不兼容性，并展示同样的范畴错误在网络结构中如何导致拓扑知识破坏。

Result: 研究发现iCloud的故障源于将分布式因果图投影到线性时间链的结构性错误，这种"范畴错误"不仅影响云存储，在网络结构中表现为链路抖动时会导致认知崩溃。Open Atomic Ethernet的双向、可逆、守恒保持的事务语义提供了解决这些问题的结构基础。

Conclusion: iCloud等系统的根本问题在于违背了物理现实，试图用前向时间假设掩盖网络分区带来的互一致性破坏。解决方案不是对抗物理规律，而是采用符合物理现实的协议行为，如Open Atomic Ethernet的事务语义，通过保持守恒和可逆性来避免认知崩溃。

Abstract: iCloud Drive presents a filesystem interface but implements cloud synchronization semantics that diverge from POSIX in fundamental ways. This divergence is not an implementation bug; it is a Category Mistake -- the same one that pervades distributed computing wherever Forward-In-Time-Only (FITO) assumptions are embedded into protocol design. Parker et al. showed in 1983 that network partitioning destroys mutual consistency; iCloud adds a user interface that conceals this impossibility behind a facade of seamlessness. This document presents a unified analysis of why iCloud fails when composed with Time Machine, git, automated toolchains, and general-purpose developer workflows, supported by direct evidence including documented corruption events and a case study involving 366 GB of divergent state accumulated through normal use. We show that the failures arise from five interlocking incompatibilities rooted in a single structural error: the projection of a distributed causal graph onto a linear temporal chain. We then show how the same Category Mistake, when it occurs in network fabrics as link flapping, destroys topology knowledge through epistemic collapse. Finally, we argue that Open Atomic Ethernet (OAE) transactional semantics -- bilateral, reversible, and conservation-preserving -- provide the structural foundation for resolving these failures, not by defeating physics, but by aligning protocol behavior with physical reality.

</details>


### [12] [GPU-Resident Gaussian Process Regression Leveraging Asynchronous Tasks with HPX](https://arxiv.org/abs/2602.19683)
*Henrik Möllmann,Dirk Pflüger,Alexander Strack*

Main category: cs.DC

TL;DR: GPRat库扩展了GPU支持的Gaussian Process预测，通过CUDA优化和HPX并行实现了比CPU版本最高4.6倍的加速，并在大数据集上超越cuSOLVER性能11%


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归虽然广泛应用，但精确求解器的立方复杂度限制了其可扩展性。现有CPU实现无法充分利用现代GPU的并行计算能力，需要开发高效的GPU加速解决方案。

Method: 扩展GPRat库，实现完全GPU驻留的GP预测流水线。采用分块算法，利用优化的CUDA库进行线性代数运算，结合HPX任务并行框架和多个CUDA流，探索最佳CUDA流数量配置。

Result: GPU实现在训练样本超过128个的数据集上提供加速：Cholesky分解加速最高4.3倍，GP预测加速最高4.6倍。结合HPX和多个CUDA流，GPRat在大数据集上性能超越cuSOLVER达11%。

Conclusion: GPU加速的GPRat库显著提升了高斯过程回归的计算效率，特别是在大数据集上。HPX与多CUDA流的结合实现了优于专用GPU库的性能，为大规模GP应用提供了可行的解决方案。

Abstract: Gaussian processes (GPs) are a widely used regression tool, but the cubic complexity of exact solvers limits their scalability. To address this challenge, we extend the GPRat library by incorporating a fully GPU-resident GP prediction pipeline. GPRat is an HPX-based library that combines task-based parallelism with an intuitive Python API.
  We implement tiled algorithms for the GP prediction using optimized CUDA libraries, thereby exploiting massive parallelism for linear algebra operations. We evaluate the optimal number of CUDA streams and compare the performance of our GPU implementation to the existing CPU-based implementation. Our results show the GPU implementation provides speedups for datasets larger than 128 training samples. We observe speedups of up to 4.3 for the Cholesky decomposition itself and 4.6 for the GP prediction. Furthermore, combining HPX with multiple CUDA streams allows GPRat to match, and for large datasets, surpass cuSOLVER's performance by up to 11 percent.

</details>


### [13] [A Risk-Aware UAV-Edge Service Framework for Wildfire Monitoring and Emergency Response](https://arxiv.org/abs/2602.19742)
*Yulun Huang,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出一个无人机边缘计算框架，联合优化路径规划、机队规模和边缘服务配置，用于野火监测，显著降低响应时间、能耗和机队规模


<details>
  <summary>Details</summary>
Motivation: 野火监测需要及时的数据收集和处理以实现早期检测和快速响应。无人机辅助的边缘计算是一种有前景的方法，但联合最小化端到端服务响应时间同时满足能量、重访时间和容量约束仍然具有挑战性

Method: 提出一个集成框架，联合优化无人机路径规划、机队规模和边缘服务配置。包括：基于火灾历史加权的聚类以优先处理高风险区域；服务质量感知的边缘分配平衡邻近性和计算负载；2-opt路径优化与自适应机队规模调整；动态应急重路由机制

Result: 实验显示，与遗传算法、粒子群优化和贪心基线相比，该框架将平均响应时间降低70.6-84.2%，能耗降低73.8-88.4%，机队规模减少26.7-42.1%。应急机制在233秒内响应，远低于300秒时限，对正常操作影响可忽略

Conclusion: 该框架通过联合优化多个相互依赖的子问题（聚类、边缘分配、路径规划、机队规模），实现了高效的野火监测系统，显著提升了响应速度、能源效率和资源利用率

Abstract: Wildfire monitoring demands timely data collection and processing for early detection and rapid response. UAV-assisted edge computing is a promising approach, but jointly minimizing end-to-end service response time while satisfying energy, revisit time, and capacity constraints remains challenging. We propose an integrated framework that co-optimizes UAV route planning, fleet sizing, and edge service provisioning for wildfire monitoring. The framework combines fire-history-weighted clustering to prioritize high-risk areas, Quality of Service (QoS)-aware edge assignment balancing proximity and computational load, 2-opt route optimization with adaptive fleet sizing, and a dynamic emergency rerouting mechanism. The key insight is that these subproblems are interdependent: clustering decisions simultaneously shape patrol efficiency and edge workloads, while capacity constraints feed back into feasible configurations. Experiments show that the proposed framework reduces average response time by 70.6--84.2%, energy consumption by 73.8--88.4%, and fleet size by 26.7--42.1% compared to GA, PSO, and greedy baselines. The emergency mechanism responds within 233 seconds, well under the 300-second deadline, with negligible impact on normal operations.

</details>


### [14] [Linear Reservoir: A Diagonalization-Based Optimization](https://arxiv.org/abs/2602.19802)
*Romain de Coudenhove,Yannis Bendi-Ouis,Anthony Strock,Xavier Hinaut*

Main category: cs.DC

TL;DR: 提出一种基于对角化的线性回声状态网络优化方法，将储层状态更新的计算复杂度从O(N²)降低到O(N)，通过特征基变换实现独立元素操作，并提出三种应用方法。


<details>
  <summary>Details</summary>
Motivation: 标准线性ESN的储层状态更新需要O(N²)的矩阵乘法计算，计算复杂度高，限制了网络规模和效率。需要降低计算成本同时保持预测精度。

Method: 将储层动态在循环矩阵的特征基中重新表述，使循环更新变为独立的元素级操作。提出三种方法：1）特征基权重变换（EWT）保持标准ESN动态；2）端到端特征基训练（EET）在变换空间直接优化读出权重；3）直接参数生成（DPG）绕过矩阵对角化直接采样特征值和特征向量。

Result: 在所有实验中，方法在保持预测精度的同时显著提高了计算速度，计算复杂度从O(N²)降低到O(N)，性能与标准线性ESN相当。

Conclusion: 该方法可作为标准线性ESN计算和训练的替代方案，并建议线性ESN向直接选择特征值的范式转变，为高效ESN实现提供了新途径。

Abstract: We introduce a diagonalization-based optimization for Linear Echo State Networks (ESNs) that reduces the per-step computational complexity of reservoir state updates from O(N^2) to O(N). By reformulating reservoir dynamics in the eigenbasis of the recurrent matrix, the recurrent update becomes a set of independent element-wise operations, eliminating the matrix multiplication. We further propose three methods to use our optimization depending on the situation: (i) Eigenbasis Weight Transformation (EWT), which preserves the dynamics of standard and trained Linear ESNs, (ii) End-to-End Eigenbasis Training (EET), which directly optimizes readout weights in the transformed space and (iii) Direct Parameter Generation (DPG), that bypasses matrix diagonalization by directly sampling eigenvalues and eigenvectors, achieving comparable performance than standard Linear ESNs. Across all experiments, both our methods preserve predictive accuracy while offering significant computational speedups, making them a replacement of standard Linear ESNs computations and training, and suggesting a shift of paradigm in linear ESN towards the direct selection of eigenvalues.

</details>


### [15] [Mitigating Artifacts in Pre-quantization Based Scientific Data Compressors with Quantization-aware Interpolation](https://arxiv.org/abs/2602.20097)
*Pu Jiao,Sheng Di,Jiannan Tian,Mingze Xia,Xuan Wu,Yang Zhang,Xin Liang,Franck Cappello*

Main category: cs.DC

TL;DR: 本文提出一种量化感知插值算法，用于缓解基于预量化的有损压缩器产生的伪影，在保持高压缩吞吐量的同时提升解压数据质量。


<details>
  <summary>Details</summary>
Motivation: 基于预量化的有损压缩器虽然具有极高的吞吐量，但在中等或较大用户指定误差边界下通常数据质量较低。需要解决预量化压缩器产生的伪影问题。

Method: 1) 分析预量化压缩器中伪影特征，理解量化索引与压缩误差的关联；2) 提出新颖的量化感知插值算法改进解压数据；3) 在共享内存和分布式内存环境中并行化算法以获得高性能。

Result: 使用五个真实世界数据集评估算法，实验表明该伪影缓解算法能有效提升基于预量化压缩器解压数据的质量，同时保持其高压缩吞吐量。

Conclusion: 提出的量化感知插值算法成功缓解了预量化压缩器的伪影问题，在保持高吞吐量的同时显著提升了压缩数据质量，为科学数据压缩提供了有效解决方案。

Abstract: Error-bounded lossy compression has been regarded as a promising way to address the ever-increasing amount of scientific data in today's high-performance computing systems. Pre-quantization, a critical technique to remove sequential dependency and enable high parallelism, is widely used to design and develop high-throughput error-controlled data compressors. Despite the extremely high throughput of pre-quantization based compressors, they generally suffer from low data quality with medium or large user-specified error bounds. In this paper, we investigate the artifacts generated by pre-quantization based compressors and propose a novel algorithm to mitigate them. Our contributions are fourfold: (1) We carefully characterize the artifacts in pre-quantization based compressors to understand the correlation between the quantization index and compression error; (2) We propose a novel quantization-aware interpolation algorithm to improve the decompressed data; (3) We parallelize our algorithm in both shared-memory and distributed-memory environments to obtain high performance; (4) We evaluate our algorithm and validate it with two leading pre-quantization based compressors using five real-world datasets. Experiments demonstrate that our artifact mitigation algorithm can effectively improve the quality of decompressed data produced by pre-quantization based compressors while maintaining their high compression throughput.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [Vibe Coding on Trial: Operating Characteristics of Unanimous LLM Juries](https://arxiv.org/abs/2602.18492)
*Muhammad Aziz Ullah,Abdul Serwadda*

Main category: cs.DB

TL;DR: 研究使用LLM陪审团评估AI生成的SQL代码质量，通过构建一致同意的委员会来减少误接受，同时保持高通过率


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成工具缺乏可靠机制来判断哪些模型生成的查询可以安全接受，而不需要人工审查。需要一种方法来区分安全和不安全的AI生成代码。

Method: 首先在82个MySQL文本到SQL任务上基准测试15个开源模型，选出6个最佳模型。然后构建大小为1到6的一致同意委员会，只有当所有成员都认为SQL正确时才接受。使用执行基础协议进行评估。

Result: 单个模型评估表现不均，小型一致同意委员会能显著减少误接受，同时仍能通过许多良好查询。委员会的具体组成对结果有显著影响。

Conclusion: 使用强模型组成的小型一致同意委员会可以有效平衡安全性和实用性，为AI代码生成提供可靠的自动审查机制。

Abstract: Large Language Models (LLMs) are now good enough at coding that developers can describe intent in plain language and let the tool produce the first code draft, a workflow increasingly built into tools like GitHub Copilot, Cursor, and Replit. What is missing is a reliable way to tell which model written queries are safe to accept without sending everything to a human. We study the application of an LLM jury to run this review step. We first benchmark 15 open models on 82 MySQL text to SQL tasks using an execution grounded protocol to get a clean baseline of which models are strong. From the six best models we build unanimous committees of sizes 1 through 6 that see the prompt, schema, and candidate SQL and accept it only when every member says it is correct. This rule matches safety first deployments where false accepts are more costly than false rejects. We measure true positive rate, false positive rate and Youden J and we also look at committees per generator. Our results show that single model judges are uneven, that small unanimous committees of strong models can cut false accepts while still passing many good queries, and that the exact committee composition matters significantly.

</details>


### [17] [RDBLearn: Simple In-Context Prediction Over Relational Databases](https://arxiv.org/abs/2602.18495)
*Yanlin Zhang,Linjie Xu,Quan Gan,David Wipf,Minjie Wang*

Main category: cs.DB

TL;DR: RDBLearn将表格上下文学习扩展到关系数据库，通过自动关系聚合特征化目标行，使用现成表格基础模型进行预测，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的预测任务通常存在于关系数据库中，信号分布在多个关联表中，而现有的表格上下文学习方法仅适用于单一平面表，无法处理关系数据。

Method: 提出RDBLearn方法：1）自动对每个目标行使用关系聚合特征化其关联记录；2）物化生成的增强表；3）运行现成的表格基础模型进行预测。提供scikit-learn风格的工具包接口。

Result: 在RelBench和4DBInfer数据集上的评估显示，RDBLearn是表现最好的基础模型方法，有时甚至优于在每个数据集上训练或微调的强监督基线。

Conclusion: RDBLearn成功将表格上下文学习扩展到关系预测任务，提供易用的工具包，在多个关系数据集上展现强大性能，为关系数据库中的预测任务提供了有效解决方案。

Abstract: Recent advances in tabular in-context learning (ICL) show that a single pretrained model can adapt to new prediction tasks from a small set of labeled examples, avoiding per-task training and heavy tuning. However, many real-world tasks live in relational databases, where predictive signal is spread across multiple linked tables rather than a single flat table. We show that tabular ICL can be extended to relational prediction with a simple recipe: automatically featurize each target row using relational aggregations over its linked records, materialize the resulting augmented table, and run an off-the-shelf tabular foundation model on it. We package this approach in \textit{RDBLearn} (https://github.com/HKUSHXLab/rdblearn), an easy-to-use toolkit with a scikit-learn-style estimator interface that makes it straightforward to swap different tabular ICL backends; a complementary agent-specific interface is provided as well. Across a broad collection of RelBench and 4DBInfer datasets, RDBLearn is the best-performing foundation model approach we evaluate, at times even outperforming strong supervised baselines trained or fine-tuned on each dataset.

</details>


### [18] [PIPE-RDF: An LLM-Assisted Pipeline for Enterprise RDF Benchmarking](https://arxiv.org/abs/2602.18497)
*Suraj Ranganath*

Main category: cs.DB

TL;DR: PIPE-RDF是一个用于从企业RDF知识图谱生成特定模式NL-SPARQL基准的三阶段流水线，通过反向查询、类别平衡模板生成、检索增强提示等技术，确保100%解析和执行有效性。


<details>
  <summary>Details</summary>
Motivation: 企业使用RDF知识图谱和SPARQL通过自然语言接口暴露运营数据，但现有的KGQA基准无法反映专有模式、前缀或查询分布，缺乏针对企业特定环境的评估基准。

Method: 采用三阶段流水线：1) 反向查询从现有SPARQL查询中提取模式；2) 类别平衡模板生成；3) 检索增强提示、去重和执行验证与修复。使用公司-位置数据切片（5,000家公司）实例化。

Result: 生成了包含450个问题-SPARQL对的平衡基准，覆盖9个类别。修复后达到100%解析和执行有效性，修复前各阶段有效性为96.5%-100%。提供了实体多样性指标、模板覆盖分析和成本分解。

Conclusion: PIPE-RDF能够为企业特定RDF知识图谱生成高质量的自然语言-SPARQL基准，支持现实环境中的模型评估和系统规划，相关代码和结构化工件已开源。

Abstract: Enterprises rely on RDF knowledge graphs and SPARQL to expose operational data through natural language interfaces, yet public KGQA benchmarks do not reflect proprietary schemas, prefixes, or query distributions. We present PIPE-RDF, a three-phase pipeline that constructs schema-specific NL-SPARQL benchmarks using reverse querying, category-balanced template generation, retrieval-augmented prompting, deduplication, and execution-based validation with repair. We instantiate PIPE-RDF on a fixed-schema company-location slice (5,000 companies) derived from public RDF data and generate a balanced benchmark of 450 question-SPARQL pairs across nine categories. The pipeline achieves 100% parse and execution validity after repair, with pre-repair validity rates of 96.5%-100% across phases. We report entity diversity metrics, template coverage analysis, and cost breakdowns to support deployment planning. We release structured artifacts (CSV/JSONL, logs, figures) and operational metrics to support model evaluation and system planning in real-world settings. Code is available at https://github.com/suraj-ranganath/PIPE-RDF.

</details>


### [19] [Should I Hide My Duck in the Lake?](https://arxiv.org/abs/2602.18775)
*Jonas Dann,Gustavo Alonso*

Main category: cs.DB

TL;DR: 提出在云环境中使用SmartNIC来卸载数据解码和过滤操作，以解决数据湖查询中远程存储扫描和解码的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 数据湖查询中，从远程存储扫描数据占用了大量执行时间，其中仅解码操作就占TPC-H查询运行时长的46%。传统架构中CPU需要处理原始数据的解码和过滤，造成性能瓶颈。

Method: 提出一种面向云环境的数据处理SmartNIC架构，将其部署在计算节点的网络数据路径上，用于卸载数据解码和推送下推操作符，直接在网络层面处理预过滤数据。

Result: 通过DuckDB的实验评估表明，使用SmartNIC直接处理预过滤数据后，即使使用更小的CPU也能达到传统设置的查询吞吐量，有效隐藏了查询原始文件的成本。

Conclusion: SmartNIC技术能够有效解决数据湖查询中的解码瓶颈问题，通过在网络数据路径上卸载数据处理任务，可以显著提升查询性能并降低CPU需求。

Abstract: Data lakes spend a significant fraction of query execution time on scanning data from remote storage. Decoding alone accounts for 46% of runtime when running TPC-H directly on Parquet files. To address this bottleneck, we propose a vision for a data processing SmartNIC for the cloud that sits on the network datapath of compute nodes to offload decoding and pushed-down operators, effectively hiding the cost of querying raw files. Our experimental estimations with DuckDB suggest that by operating directly on pre-filtered data as delivered by a SmartNIC, significantly smaller CPUs can still match query throughput of traditional setups.

</details>


### [20] [S$^3$GND: An Effective Learning-Based Approach for Subgraph Similarity Search Under Generalized Neighbor Difference Semantics (Technical Report)](https://arxiv.org/abs/2602.19167)
*Qi Wen,Xiang Lian,Nan Zhang,Yutong Ye,Mingsong Chen*

Main category: cs.DB

TL;DR: 提出基于广义邻居差异（GND）语义的子图相似性搜索问题（S³GND），并设计基于超图神经网络的学习方法进行高效求解。


<details>
  <summary>Details</summary>
Motivation: 现有子图相似性搜索方法主要关注各种图相似性度量，但缺乏同时考虑顶点关键词集关系和边权重差异的语义。实际应用中如蛋白质发现、社交网络分析和推荐系统需要更全面的相似性度量。

Method: 1) 提出广义邻居差异（GND）语义；2) 从数据图构建关键词超图，训练超图神经网络（HGNN）获取高质量关键词嵌入表示；3) 设计关键词嵌入MBR、顶点级ND下界和图级GND下界剪枝策略；4) 构建树状索引机制；5) 开发高效的S³GND查询处理算法。

Result: 在真实和合成图上进行了广泛实验，验证了所提S³GND方法的有效性和效率。

Conclusion: 提出的GND语义能够更全面地捕捉图相似性，基于学习的方法结合有效的剪枝策略和索引机制，能够高效解决大规模图上的子图相似性搜索问题。

Abstract: Subgraph similarity search over large-scale graphs is a fundamental task that retrieves subgraphs similar to a given query graph from a data graph, and it plays a crucial role in real applications such as protein discovery, social network analysis, and recommendation systems. While prior works on subgraph similarity search studied various graph similarity metrics, in this paper, we propose a novel graph similarity semantic, \textit{generalized neighbor difference} (GND), that accounts for both the keyword-set relationships between vertices and edge-weight differences. We formulate the problem of \textit{subgraph similarity search under the generalized neighbor difference semantics} (S$^3$GND), which retrieves those subgraphs similar to a query graph $q$ under GND semantics. To efficiently tackle the S$^3$GND problem, we propose an effective learning-based approach, which constructs a keyword hypergraph from the data graph, and trains a \textit{hypergraph neural network} (HGNN) model to obtain high-quality keyword embedding representations. We design effective pruning strategies, \textit{keyword embedding MBR}, \textit{vertex-Level ND lower bound}, and \textit{graph-level GND lower bound pruning}, to rule out false alarms of candidate vertices/subgraphs, and devise a tree-based indexing mechanism to facilitate efficient S$^3$GND query answering. We develop an efficient S$^3$GND query-processing algorithm that traverses the index, applies pruning strategies, and returns actual S$^3$GND answers. Finally, we conduct extensive experiments to verify the effectiveness and efficiency of our proposed S$^3$GND approach over both real and synthetic graphs.

</details>


### [21] [The Human Factor in Data Cleaning: Exploring Preferences and Biases](https://arxiv.org/abs/2602.19368)
*Hazim AbdElazim,Shadman Islam,Mostafa Milani*

Main category: cs.DB

TL;DR: 研究发现数据清洗中存在多种系统性认知偏见，包括框架效应、锚定调整偏差、代表性启发式等，这些偏见在技术经验丰富的参与者中依然存在，表明是普遍认知倾向而非缺乏专业知识。


<details>
  <summary>Details</summary>
Motivation: 数据清洗通常被视为技术预处理步骤，但实际上高度依赖人工判断。本研究旨在通过受控调查实验，探索数据清洗任务中是否存在系统性认知偏见机制。

Method: 采用受控调查研究方法，参与者执行错误检测、数据修复与插补、实体匹配等任务，基于已知语义有效性的普查启发式场景。分析不同表面格式、专家线索、属性组合等条件下的决策偏差。

Result: 发现多种系统性认知偏见：框架效应（表面格式差异增加误报）、锚定调整偏差（专家线索过度影响决策）、代表性启发式（非典型但有效组合被误判）、实体匹配中表面相似性导致高误报率。数据修复中参与者偏好保留缺失值而非插补（遗漏偏差）。自动化对齐切换未超过保守误差阈值。

Conclusion: 数据清洗中的偏见反映了普遍认知倾向而非缺乏专业知识，这些发现支持开发人机协同清洗系统：清晰分离表示与语义、非规定性呈现专家/算法建议、支持对非典型但有效案例的反思评估。

Abstract: Data cleaning is often framed as a technical preprocessing step, yet in practice it relies heavily on human judgment. We report results from a controlled survey study in which participants performed error detection, data repair and imputation, and entity matching tasks on census-inspired scenarios with known semantic validity. We find systematic evidence for several cognitive bias mechanisms in data cleaning. Framing effects arise when surface-level formatting differences (e.g., capitalization or numeric presentation) increase false-positive error flags despite unchanged semantics. Anchoring and adjustment bias appears when expert cues shift participant decisions beyond parity, consistent with salience and availability effects. We also observe the representativeness heuristic: atypical but valid attribute combinations are frequently flagged as erroneous, and in entity matching tasks, surface similarity produces a substantial false-positive rate with high confidence. In data repair, participants show a robust preference for leaving values missing rather than imputing plausible values, consistent with omission bias. In contrast, automation-aligned switching under strong contradiction does not exceed a conservative rare-error tolerance threshold at the population level, indicating that deference to automated recommendations is limited in this setting. Across scenarios, bias patterns persist among technically experienced participants and across diverse workflow practices, suggesting that bias in data cleaning reflects general cognitive tendencies rather than lack of expertise. These findings motivate human-in-the-loop cleaning systems that clearly separate representation from semantics, present expert or algorithmic recommendations non-prescriptively, and support reflective evaluation of atypical but valid cases.

</details>


### [22] [Breaking the Barriers of Database-Agnostic Transactions](https://arxiv.org/abs/2602.19440)
*Toshihiro Suzuki,Hiroyuki Yamada*

Main category: cs.DB

TL;DR: 提出名为Atomicity Unit (AU)的新概念，用于解决联邦事务管理中的性能优化和模式迁移问题，通过在数据库抽象中引入原子性范围知识来实现操作下推和元数据分离。


<details>
  <summary>Details</summary>
Motivation: 现有的基于数据库抽象的联邦事务管理方法面临两个主要挑战：1）数据库抽象隐藏了底层数据库细节，使得性能优化困难；2）需要将应用数据和事务元数据放在同一记录中，导致需要对现有数据库进行模式迁移才能运行联邦事务。

Method: 提出Atomicity Unit (AU)概念，AU使联邦事务管理器能够利用关于原子性操作范围的知识，积极地将数据库操作下推到底层数据库，充分利用数据库性能。同时，AU支持高效地将事务元数据与应用数据分离。

Result: 在开源数据库无关联邦事务管理器ScalarDB中实现了AU，评估结果显示，带有AU的ScalarDB实现了显著更好的性能，并支持高效的元数据分离，无需模式迁移或显著性能下降。

Conclusion: AU概念有效解决了联邦事务管理中的性能优化和模式迁移问题，通过引入原子性范围知识实现操作下推和元数据分离，为现有数据库运行联邦事务提供了实用解决方案。

Abstract: Federated transaction management has long been used as a method to virtually integrate multiple databases from a transactional perspective, ensuring consistency across the databases. Modern approaches manage transactions on top of a database abstraction to achieve database agnosticism; however, these approaches face several challenges. First, managing transactions on top of a database abstraction makes performance optimization difficult because the abstraction hides away the details of underlying databases, such as database-specific capabilities. Additionally, it requires that application data and the associated transaction metadata be colocated in the same record to allow for efficient updates, necessitating a schema migration to run federated transactions on top of existing databases. This paper introduces a new concept in such database abstraction called Atomicity Unit (AU) to address these challenges. AU enables federated transaction management to aggressively pushdown database operations by making use of the knowledge about the scope within which they can perform operations atomically, fully harnessing the performance of the databases. Moreover, AU enables efficient separation of transaction metadata from application data, allowing federated transactions to run on existing databases without requiring a schema migration or significant performance degradation. In this paper, we describe AU, how AU addresses the challenges, and its implementation within ScalarDB, an open-sourced database-agnostic federated transaction manager. We also present evaluation results demonstrating that ScalarDB with AU achieves significantly better performance and efficient metadata separation.

</details>


### [23] [FuzzySQL: Uncovering Hidden Vulnerabilities in DBMS Special Features with LLM-Driven Fuzzing](https://arxiv.org/abs/2602.19490)
*Yongxin Chen,Zhiyuan Jiang,Chao Zhang,Haoran Xu,Shenglin Xu,Jianping Tang,Zheming Li,Peidai Xie,Yongjun Wang*

Main category: cs.DB

TL;DR: FuzzySQL是一个基于LLM的自适应模糊测试框架，专门用于发现DBMS特殊功能中的漏洞，通过语法引导生成和逻辑转换渐进变异技术，在多个数据库系统中发现了37个漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统数据库模糊测试主要关注语法正确性和通用SQL结构，而忽略了DBMS的系统级模式、程序化构造和高级进程命令等关键但晦涩的功能。这些功能在边缘情况下执行可能导致严重崩溃或安全问题。

Method: FuzzySQL结合了语法引导的SQL生成和逻辑转换渐进变异技术，通过否定条件和重构执行逻辑来探索替代控制路径。同时采用混合错误修复管道，统一基于规则的修补和LLM驱动的语义修复。

Result: 在MySQL、MariaDB、SQLite、PostgreSQL和Clickhouse等多个DBMS中发现了37个漏洞，其中7个与未充分测试的DBMS特殊功能相关。29个案例已确认，获得9个CVE标识，14个已被厂商修复。

Conclusion: 研究结果突显了传统模糊测试在语义功能覆盖方面的局限性，并展示了基于LLM的模糊测试在发现复杂数据库系统中深度隐藏漏洞方面的潜力。

Abstract: Traditional database fuzzing techniques primarily focus on syntactic correctness and general SQL structures, leaving critical yet obscure DBMS features, such as system-level modes (e.g., GTID), programmatic constructs (e.g., PROCEDURE), advanced process commands (e.g., KILL), largely underexplored. Although rarely triggered by typical inputs, these features can lead to severe crashes or security issues when executed under edge-case conditions. In this paper, we present FuzzySQL, a novel LLM-powered adaptive fuzzing framework designed to uncover subtle vulnerabilities in DBMS special features. FuzzySQL combines grammar-guided SQL generation with logic-shifting progressive mutation, a novel technique that explores alternative control paths by negating conditions and restructuring execution logic, synthesizing structurally and semantically diverse test cases. To further ensure deeper execution coverage of the back end, FuzzySQL employs a hybrid error repair pipeline that unifies rule-based patching with LLM-driven semantic repair, enabling automatic correction of syntactic and context-sensitive failures. We evaluate FuzzySQL across multiple DBMSs, including MySQL, MariaDB, SQLite, PostgreSQL and Clickhouse, uncovering 37 vulnerabilities, 7 of which are tied to under-tested DBMS special features. As of this writing, 29 cases have been confirmed with 9 assigned CVE identifiers, 14 already fixed by vendors, and additional vulnerabilities scheduled to be patched in upcoming releases. Our results highlight the limitations of conventional fuzzers in semantic feature coverage and demonstrate the potential of LLM-based fuzzing to discover deeply hidden bugs in complex database systems.

</details>


### [24] [The Climate Change Knowledge Graph: Supporting Climate Services](https://arxiv.org/abs/2602.19786)
*Miguel Ceriani,Fiorela Ciroku,Alessandro Russo,Massimiliano Schembri,Fai Fung,Neha Mittal,Vito Trianni,Andrea Giovanni Nuzzolese*

Main category: cs.DB

TL;DR: 该论文提出了一个气候变化知识图谱，用于整合和管理气候模型数据，以支持复杂查询和气候决策。


<details>
  <summary>Details</summary>
Motivation: 当前气候模型数据分散，研究人员依赖传统搜索接口和API，需要手动整合不同来源的元数据和词汇表，缺乏统一、可互操作的数据框架来支持复杂的气候变化分析。

Method: 开发了一个气候变化知识图谱，通过整合多样化的气候模拟数据源，构建了统一的知识图谱和底层本体，支持对气候模型、模拟、变量、时空域和粒度等元素的复杂查询。

Result: 创建了一个开放访问的气候变化知识图谱资源，提供了全面的数据框架，增强了气候数据的探索能力，为应对气候变化问题提供了更明智的决策支持。

Conclusion: 气候变化知识图谱通过整合分散的气候模拟数据，提供了一个可互操作的统一框架，显著改善了气候数据的可访问性和分析能力，有助于更有效地应对气候变化挑战。

Abstract: Climate change impacts a broad spectrum of human resources and activities, necessitating the use of climate models to project long-term effects and inform mitigation and adaptation strategies. These models generate multiple datasets by running simulations across various scenarios and configurations, thereby covering a range of potential future outcomes. Currently, researchers rely on traditional search interfaces and APIs to retrieve such datasets, often piecing together information from metadata and community vocabularies. The Climate Change Knowledge Graph is designed to address these challenges by integrating diverse data sources related to climate simulations into a coherent and interoperable knowledge graph. This innovative resource allows for executing complex queries involving climate models, simulations, variables, spatio-temporal domains, and granularities. Developed with input from domain experts, the knowledge graph and its underlying ontology are published with open access license and provide a comprehensive framework that enhances the exploration of climate data, facilitating more informed decision-making in addressing climate change issues.

</details>


### [25] [Semantic Caching for OLAP via LLM-Based Query Canonicalization (Extended Version)](https://arxiv.org/abs/2602.19811)
*Laurent Bindschaedler*

Main category: cs.DB

TL;DR: 提出一个基于OLAP意图签名的安全中间件缓存，通过规范化SQL和自然语言查询到统一的意图空间，实现高命中率且零误命中


<details>
  <summary>Details</summary>
Motivation: 分析工作负载存在大量语义重复，但现有生产缓存通常基于SQL表面形式（文本或AST）进行键值存储，导致跨BI工具、笔记本和自然语言接口的复用碎片化

Method: 引入安全优先的中间件缓存，将SQL和自然语言查询规范化为统一的OLAP意图签名，捕获度量、分组级别、过滤条件和时间窗口。复用需要严格模式验证下的精确意图匹配和置信度门控的自然语言接受；通过两种正确性保持的推导（上卷、下钻）扩展覆盖范围

Result: 在TPC-DS、SSB和NYC TLC（共1,395个查询）测试中，实现82%命中率，相比文本缓存（28%）和AST缓存（56%）显著提升，且零误命中；推导机制在分层查询上将命中率翻倍

Conclusion: OLAP意图签名方法能有效解决分析工作负载中的语义重复问题，通过规范化表示和正确性保持的推导，在保证安全性的同时大幅提升缓存复用效率

Abstract: Analytical workloads exhibit substantial semantic repetition, yet most production caches key entries by SQL surface form (text or AST), fragmenting reuse across BI tools, notebooks, and NL interfaces. We introduce a safety-first middleware cache for dashboard-style OLAP over star schemas that canonicalizes both SQL and NL into a unified key space -- the OLAP Intent Signature -- capturing measures, grouping levels, filters, and time windows. Reuse requires exact intent matches under strict schema validation and confidence-gated NL acceptance; two correctness-preserving derivations (roll-up, filter-down) extend coverage without approximate matching. Across TPC-DS, SSB, and NYC TLC (1,395 queries), we achieve 82% hit rate versus 28% (text) and 56% (AST) with zero false hits; derivations double hit rate on hierarchical queries.

</details>


### [26] [A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT](https://arxiv.org/abs/2602.19990)
*Monica Marconi Sciarroni,Emanuele Storti*

Main category: cs.DB

TL;DR: 提出一个基于知识图谱的上下文感知语义平台，用于管理工业物联网数据流，支持异构数据源统一、可组合处理管道和动态基于角色的访问控制。


<details>
  <summary>Details</summary>
Motivation: 工业物联网生态系统产生大量异构、高速数据流，需要可互操作、安全且上下文感知的管理。现有流管理架构主要依赖语法集成机制，在复杂的工业5.0场景中灵活性、可维护性和可解释性有限。

Method: 提出一个上下文感知语义平台，通过知识图谱统一异构IoT/IoE数据源，形式化表示设备、流、代理、转换管道、角色和权限。平台结合Apache Kafka和Apache Flink进行实时处理，使用SPARQL和SWRL进行上下文相关流发现和推理。

Result: 实验评估表明，结合语义模型、上下文感知推理和分布式流处理能够有效实现工业5.0环境的可互操作数据工作流。

Conclusion: 该平台通过语义集成和上下文感知推理解决了工业物联网数据流管理的互操作性和灵活性挑战，为工业5.0环境提供了有效的解决方案。

Abstract: Industrial IoT ecosystems bring together sensors, machines and smart devices operating collaboratively across industrial environments. These systems generate large volumes of heterogeneous, high-velocity data streams that require interoperable, secure and contextually aware management. Most of the current stream management architectures, however, still rely on syntactic integration mechanisms, which result in limited flexibility, maintainability and interpretability in complex Industry 5.0 scenarios. This work proposes a context-aware semantic platform for data stream management that unifies heterogeneous IoT/IoE data sources through a Knowledge Graph enabling formal representation of devices, streams, agents, transformation pipelines, roles and rights. The model supports flexible data gathering, composable stream processing pipelines, and dynamic role-based data access based on agents' contexts, relying on Apache Kafka and Apache Flink for real-time processing, while SPARQL and SWRL-based reasoning provide context-dependent stream discovery. Experimental evaluations demonstrate the effectiveness of combining semantic models, context-aware reasoning and distributed stream processing to enable interoperable data workflows for Industry 5.0 environments.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [27] [Validated Code Translation for Projects with External Libraries](https://arxiv.org/abs/2602.18534)
*Hanliang Zhang,Arindam Sharma,Cristina David,Meng Wang,Brandon Paulsen,Daniel Kroening,Wenjia Ye,Taro Sekiyama*

Main category: cs.SE

TL;DR: 提出一个Go到Rust的代码翻译验证框架，解决外部依赖库API映射和语义等价验证问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM在程序翻译时，特别是处理外部库依赖时存在严重问题：会幻觉出不存在的目标API、无法生成正确的导入语句，且在处理库定义的不透明类型时难以验证语义等价性

Method: 结合(i)检索机制将Go库API映射到Rust API，(ii)跨语言验证流水线，通过仅从公共库API合成适配器来建立语言互操作性，然后验证I/O等价性

Result: 在6个具有非平凡外部依赖的真实Go仓库上评估，显著提高了编译和等价成功率（在依赖最重的情况下达到100%，平均约2倍提升）

Conclusion: 该框架能够实现经过验证的翻译，处理库定义的不透明类型，有效解决了Go到Rust迁移中的外部依赖问题

Abstract: Large Language Models (LLMs) have shown promise for program translation, particularly for migrating systems code to memory-safe languages such as Rust. However, existing approaches struggle when source programs depend on external libraries: LLMs frequently hallucinate non-existent target APIs and fail to generate call-enabling imports; moreover, validating semantic equivalence is challenging when the code manipulates opaque, library-defined types. We present a translation and validation framework for translating Go projects with external dependencies to Rust. Our approach combines (i) a retrieval mechanism that maps Go library APIs to Rust APIs, and (ii) a cross-language validation pipeline that establishes language interoperability in the presence of opaque library types by synthesising adapters exclusively from public library APIs, prior to validating I/O equivalence. We evaluate our system on six real-world Go repositories with non-trivial external dependencies. Our approach significantly increases both the compilation and equivalence success rate (up to 100% in the most dependency-heavy case; approx. 2x on average) by enabling validated translation that manipulate opaque, library-defined types.

</details>


### [28] [Runtime-Augmented LLMs for Crash Detection and Diagnosis in ML Notebooks](https://arxiv.org/abs/2602.18537)
*Yiran Wang,José Antonio Hernández López,Ulf Nilsson,Dániel Varró*

Main category: cs.SE

TL;DR: CRANE-LLM使用LLM结合运行时信息来预测和诊断Jupyter notebook中机器学习代码的崩溃问题，相比纯静态分析提升7-10个百分点的准确率。


<details>
  <summary>Details</summary>
Motivation: 机器学习Jupyter notebook在开发过程中容易崩溃，严重影响开发效率。目前缺乏系统性的崩溃检测和诊断方法，需要一种能在执行前预测崩溃并分析原因的工具。

Method: CRANE-LLM将大语言模型与从notebook内核状态提取的结构化运行时信息相结合。给定已执行的单元格和目标单元格，系统结合静态代码上下文和运行时信息（对象类型、张量形状、数据属性等）来预测目标单元格是否会崩溃（检测）并解释根本原因（诊断）。

Result: 在JunoBench基准测试（222个ML notebook，包含111对崩溃/非崩溃notebook）上，运行时信息将三个先进LLM（Gemini、Qwen、GPT-5）的崩溃检测和诊断准确率提升了7-10个百分点，F1分数提升8-11分。诊断任务的提升幅度更大，改进效果因ML库、崩溃原因和LLM而异。

Conclusion: CRANE-LLM证明了将LLM与结构化运行时信息结合能有效提升ML notebook崩溃检测和诊断能力，为开发更可靠的机器学习开发工具提供了新方向。

Abstract: Jupyter notebooks are widely used for machine learning (ML) development due to their support for interactive and iterative experimentation. However, ML notebooks are highly prone to bugs, with crashes being among the most disruptive. Despite their practical importance, systematic methods for crash detection and diagnosis in ML notebooks remain largely unexplored. We present CRANE-LLM, a novel approach that augments large language models (LLMs) with structured runtime information extracted from the notebook kernel state to detect and diagnose crashes before executing a target cell. Given previously executed cells and a target cell, CRANE-LLM combines static code context with runtime information, including object types, tensor shapes, and data attributes, to predict whether the target cell will crash (detection) and explain the underlying cause (diagnosis). We evaluate CRANE-LLM on JunoBench, a benchmark of 222 ML notebooks comprising 111 pairs of crashing and corresponding non-crashing notebooks across multiple ML libraries and crash root causes. Across three state-of-the-art LLMs (Gemini, Qwen, and GPT-5), runtime information improves crash detection and diagnosis by 7-10 percentage points in accuracy and 8-11 in F1-score, with larger gains for diagnosis. Improvements vary across ML libraries, crash causes, and LLMs, and depends on the integration of complementary categories of runtime information.

</details>


### [29] [LAPIS: Lightweight API Specification for Intelligent Systems](https://arxiv.org/abs/2602.18541)
*Daniel Garcia*

Main category: cs.SE

TL;DR: LAPIS是一种专为LLM优化的轻量级API规范格式，相比OpenAPI平均减少85.5%的token使用量，同时保留API推理所需的语义信息。


<details>
  <summary>Details</summary>
Motivation: OpenAPI作为API描述标准，原本是为文档工具和代码生成器设计的，当作为LLM上下文使用时会产生大量token开销，影响LLM的效率和性能。

Method: 提出LAPIS格式，通过领域特定的结构创新（如集中化错误定义、webhook触发条件、结构化速率限制描述等），在保留必要语义信息的同时最小化token使用。提供从OpenAPI 3.x自动转换的工具。

Result: 在GitHub、Twilio、DigitalOcean等五个真实生产API规范上评估，相比OpenAPI YAML平均减少85.5%的token使用，相比OpenAPI JSON减少88.6%（使用cl100k_base分词器测量）。

Conclusion: LAPIS是一种专为LLM消费优化的轻量级API规范格式，能显著减少token开销，同时支持OpenAPI无法表示或冗余表示的重要API信息，已作为CC BY 4.0开放规范发布。

Abstract: Large Language Models (LLMs) increasingly serve as consumers of API specifications, whether for code generation, autonomous agent interaction, or API-assisted reasoning. The de facto standard for API description, OpenAPI, was designed for documentation tools and code generators, resulting in substantial token overhead when used as LLM context.
  We present LAPIS (Lightweight API Specification for Intelligent Systems), a domain-specific format optimized for LLM consumption that preserves the semantic information necessary for API reasoning while minimizing token usage. Through empirical evaluation against five real-world production API specifications including GitHub (1,080 endpoints), Twilio (197 endpoints), DigitalOcean (545 endpoints), Petstore, and HTTPBin we demonstrate an average token reduction of 85.5% compared to OpenAPI YAML and 88.6% compared to OpenAPI JSON, measured with the cl100k_base tokenizer. LAPIS introduces domain-specific structural innovations, including centralized error definitions, webhook trigger conditions, structured rate limit descriptions, and operation flow declarations information that OpenAPI either duplicates redundantly or cannot represent at all.
  The format is fully convertible from OpenAPI 3.x via an automated converter, requires no special parser for LLM consumption, and is released as an open specification under CC BY 4.0.

</details>


### [30] [Programmable Property-Based Testing](https://arxiv.org/abs/2602.18545)
*Alperen Keles,Justine Frank,Ceren Mert,Harrison Goldstein,Leonidas Lampropoulos*

Main category: cs.SE

TL;DR: 提出基于延迟绑定抽象语法的深层属性语言，将属性与执行器解耦，实现更灵活可编程的测试框架


<details>
  <summary>Details</summary>
Motivation: 现有属性测试框架采用浅层嵌入式DSL，属性定义与测试方式紧密耦合，用户受限于框架作者预设的配置选项，缺乏灵活性

Method: 提出延迟绑定抽象语法（deferred binding abstract syntax）的深层嵌入式语言，将属性具体化为数据结构，与属性执行器解耦，在Rocq和Racket中实现

Result: 实现了更灵活可编程的测试方法，能够快速原型化多种属性执行器，解锁特定领域的测试改进

Conclusion: 通过深层嵌入式语言和属性与执行器的解耦，显著提升了属性测试框架的灵活性和可编程性

Abstract: Property-based testing (PBT) is a popular technique for establishing confidence in software, where users write properties -- i.e., executable specifications -- that can be checked many times in a loop by a testing framework. In modern PBT frameworks, properties are usually written in shallowly embedded domain-specific languages, and their definition is tightly coupled to the way they are tested. Such frameworks often provide convenient configuration options to customize aspects of the testing process, but users are limited to precisely what library authors had the prescience to allow for when developing the framework; if they want more flexibility, they may need to write a new framework from scratch.
  We propose a new, deeper language for properties based on a mixed embedding that we call deferred binding abstract syntax, which reifies properties as a data structure and decouples them from the property runners that execute them. We implement this language in Rocq and Racket, leveraging the power of dependent and dynamic types, respectively. Finally, we showcase the flexibility of this new approach by rapidly prototyping a variety of property runners, highlighting domain-specific testing improvements that can be unlocked by more programmable testing.

</details>


### [31] [1D-Bench: A Benchmark for Iterative UI Code Generation with Visual Feedback in Real-World](https://arxiv.org/abs/2602.18548)
*Qiao Xu,Yipeng Yu,Chengxiao Feng,Xu Liu*

Main category: cs.SE

TL;DR: 1D-Bench是一个基于真实电商工作流的设计到代码转换基准测试，要求生成可执行的React代码库，并支持多轮迭代编辑，测试模型对中间表示缺陷的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前设计到代码转换领域缺乏一致的评估标准，难以比较不同方法的进展。现有方法在数据集、工具链和评估协议上存在不一致性，阻碍了该领域的系统性发展。

Method: 1D-Bench提供参考渲染和可能包含提取错误的中间表示作为输入，要求模型生成具有明确组件层次结构的可执行React代码库。基准测试包含多轮迭代设置，模型根据执行反馈进行组件级编辑。

Result: 实验表明，迭代编辑通常能提高最终性能，增加渲染成功率并改善视觉相似度。但基于合成修复轨迹和强化学习的后训练方法效果有限且不稳定，可能源于稀疏的终端奖励和高方差的文件级更新。

Conclusion: 1D-Bench为设计到代码转换提供了标准化的评估框架，强调对中间表示缺陷的鲁棒性和迭代改进能力。虽然迭代编辑能提升性能，但更先进的训练方法仍需进一步研究以解决当前局限性。

Abstract: Design-to-code translates high-fidelity UI designs into executable front-end implementations, but progress remains hard to compare due to inconsistent datasets, toolchains, and evaluation protocols. We introduce 1D-Bench, a benchmark grounded in real e-commerce workflows, where each instance provides a reference rendering and an exported intermediate representation that may contain extraction errors. 1D is short for one day, representing the efficient completion of design-to-code tasks in less than one day. Models take both as input, using the intermediate representation as structural cues while being evaluated against the reference rendering, which tests robustness to intermediate representation defects rather than literal adherence.
  1D-Bench requires generating an executable React codebase under a fixed toolchain with an explicit component hierarchy, and defines a multi-round setting in which models iteratively apply component-level edits using execution feedback. Experiments on commercial and open-weight multimodal models show that iterative editing generally improves final performance by increasing rendering success and often improving visual similarity. We further conduct a pilot study on post-training with synthetic repair trajectories and reinforcement learning based editing, and observe limited and unstable gains that may stem from sparse terminal rewards and high-variance file-level updates.

</details>


### [32] [Debug2Fix: Supercharging Coding Agents with Interactive Debugging Capabilities](https://arxiv.org/abs/2602.18571)
*Spandan Garg,Yufan Huang*

Main category: cs.SE

TL;DR: Debug2Fix框架通过集成交互式调试器提升代码代理的bug修复能力，使较弱模型能达到较强模型性能


<details>
  <summary>Details</summary>
Motivation: 当前代码代理主要依赖静态分析或试错式测试修复循环，缺乏开发者调试时常用的运行时信息访问能力。尽管调试器在现代IDE中普遍存在，但尚未被整合到代码代理中。

Method: 提出Debug2Fix框架，通过子代理架构将交互式调试作为软件工程代理的核心组件，为Java和Python集成调试器。

Result: 在GitBug-Java和SWE-Bench-Live基准测试中，相比基线获得>20%的性能提升。使用该框架，GPT-5和Claude Haiku 4.5等较弱模型能达到或超过Claude Sonnet 4.5等较强模型性能。

Conclusion: 更好的工具设计与切换到更昂贵模型同样重要。子代理架构和调试器集成对提升代码代理的bug修复能力至关重要。

Abstract: While significant progress has been made in automating various aspects of software development through coding agents, there is still significant room for improvement in their bug fixing capabilities. Debugging and investigation of runtime behavior remains largely a manual, developer-driven process. Popular coding agents typically rely on either static analysis of the code or iterative test-fix cycles, which is akin to trial and error debugging. We posit that there is a wealth of rich runtime information that developers routinely access while debugging code, which agents are currently deprived of due to design limitations. Despite how prevalent debuggers are in modern IDEs and command-line tools, they have surprisingly not made their way into coding agents. In this work, we introduce Debug2Fix, a novel framework that incorporates interactive debugging as a core component of a software engineering agent via a subagent architecture. We incorporate debuggers for Java and Python into our agent framework and evaluate against GitBug-Java and SWE-Bench-Live and achieve >20% improvement in performance compared to the baseline for certain models. Furthermore, using our framework, we're able to make weaker models like GPT-5 and Claude Haiku 4.5 match or exceed the performances of stronger models like Claude Sonnet 4.5, showing that better tool design is often just as important as switching to a more expensive model. Finally, we conduct systematic ablations demonstrating the importance of both the subagent architecture and debugger integration.

</details>


### [33] [Refactoring for Novices in Java: An Eye Tracking Study on the Extract vs. Inline Methods](https://arxiv.org/abs/2602.18579)
*José Aldo Silva da Costa,Rohit Gheyi,José Júnior Silva da Costa,Márcio Ribeiro,Rodrigo Bonifácio,Hyggo Almeida,Ana Carla Bibiano,Alessandro Garcia*

Main category: cs.SE

TL;DR: 研究通过眼动追踪比较内联方法与提取方法重构对Java新手的影响，发现效果取决于任务难度：复杂任务中提取方法能提升性能、减少视觉努力，但简单任务中提取方法反而损害性能、增加导航负担


<details>
  <summary>Details</summary>
Motivation: 之前基于静态指标的研究未能清晰展示内联方法与提取方法重构的差异，且对代码理解和导航的人为因素探索不足，需要动态方法（眼动追踪）来研究这两种重构实践对代码理解的影响

Method: 采用眼动追踪动态方法，32名Java新手参与控制实验，每人完成8个简单任务（4个内联版本，4个提取版本），分析视觉努力和阅读行为（注视时长、次数、回视、重访），辅以58名新手的补充调查和简短访谈

Result: 效果取决于任务难度：复杂任务中方法提取提升性能、减少视觉努力（时间减少达78.8%，回视减少84.6%）；简单任务中提取方法损害性能（时间增加达166.9%，回视增加200%），即使有有意义的方法名，新手仍频繁在调用点和提取方法间切换，增加导航和认知负荷

Conclusion: 教育者应谨慎对待新手的过早模块化，眼动追踪可作为静态指标的有用补充；虽然新手偏好提取方法以提高可读性和重用性，但这种偏好并不总是与实测性能相匹配

Abstract: Developers often extract methods to improve readability, understanding, and reuse, while inlining keeps logic in one block. Prior work based on static metrics has not shown clear differences between these practices, and the human side of comprehension and navigation remains underexplored. We investigate Inline Method vs. Extract Method refactorings using a dynamic approach: eye tracking while participants read and solve tasks. We analyze key code areas and compare visual effort and reading behavior (fixation duration and count, regressions, revisits), alongside time and attempts. We ran a controlled experiment with 32 Java novices, followed by short interviews. Each participant solved eight simple tasks across four programs presented in an inlined version and four in an extracted version. We also surveyed 58 additional novices for complementary quantitative and qualitative data. Results show that effects depend on task difficulty. In two tasks, method extraction improved performance and reduced visual effort, with time decreasing by up to 78.8% and regressions by 84.6%. For simpler tasks (e.g., square area), extraction hurt performance: time increased by up to 166.9% and regressions by 200%. Even with meaningful method names, novices often switched back and forth between call sites and extracted methods, increasing navigation and cognitive load. Preferences frequently favored extraction for readability and reuse, but did not always match measured performance. These findings suggest educators should be cautious about premature modularization for novices and highlight eye tracking as a useful complement to static metrics.

</details>


### [34] [Modeling and Recovering Hierarchical Structural Architectures of ROS 2 Systems from Code and Launch Configurations using LLM-based Agents](https://arxiv.org/abs/2602.18644)
*Mohamed Benchat,Dominique Briechle,Raj Chanchad,Mitbhai Chauhan,Meet Chavda,Ruidi He,Dhruv Jajadiya,Dhruv Kapadiya,Nidhiben Kaswala,Daniel Osterholz,Andreas Rausch,Meng Zhang*

Main category: cs.SE

TL;DR: 提出基于UML的ROS~2系统层次化结构建模概念和蓝图引导的自动化恢复流程，从代码和配置中重建架构模型，解决ROS~2中子系统结构隐式编码在启动文件中的问题。


<details>
  <summary>Details</summary>
Motivation: ROS~2系统的子系统结构通常隐式编码在分布式配置工件（特别是启动文件）中，这使得层次化结构分解难以捕获和维护。现有ROS~2建模方法覆盖节点级实体和连接，但没有将层次化结构（分解）作为独立于启动工件的首要架构视图。

Method: 提出（1）基于UML的ROS~2系统层次化结构架构建模概念；（2）蓝图引导的自动化恢复流程，结合确定性提取和基于LLM的代理，从代码和配置工件中重建模型。ROS~2架构蓝图（节点、主题、接口、启动诱导的连接）编码为结构契约，以约束合成并实现确定性验证。

Result: 在三个ROS~2仓库（包括工业级代码子集）上评估。结果显示在不同抽象级别上具有高精度，但子系统级召回率随仓库复杂性下降，这是由于隐式启动语义，使得高层恢复成为剩余挑战。

Conclusion: 提出的方法能够有效恢复ROS~2系统的层次化架构模型，解决了结构隐式编码的问题，但高层子系统恢复仍面临挑战，需要进一步处理隐式启动语义。

Abstract: Model-Driven Engineering (MDE) relies on explicit architecture models to document and evolve systems across abstraction levels. For ROS~2, subsystem structure is often encoded implicitly in distributed configuration artifacts -- most notably launch files -- making hierarchical structural decomposition hard to capture and maintain. Existing ROS~2 modeling approaches cover node-level entities and wiring, but do not make hierarchical structural (de-)composition a first-class architectural view independent of launch artifacts.
  We contribute (1) a UML-based modeling concept for hierarchical structural architectures of ROS~2 systems and (2) a blueprint-guided automated recovery pipeline that reconstructs such models from code and configuration artifacts by combining deterministic extraction with LLM-based agents. The ROS~2 architectural blueprint (nodes, topics, interfaces, launch-induced wiring) is encoded as structural contracts to constrain synthesis and enable deterministic validation, improving reliability.
  We evaluate the approach on three ROS~2 repositories, including an industrial-scale code subset. Results show high precision across abstraction levels, while subsystem-level recall drops with repository complexity due to implicit launch semantics, making high-level recovery the remaining challenge.

</details>


### [35] [Automatic, Expressive, and Scalable Fuzzing with Stitching](https://arxiv.org/abs/2602.18689)
*Harrison Green,Fraser Brown,Claire Le Goues*

Main category: cs.SE

TL;DR: STITCH提出了一种动态组装API使用约束的fuzzing技术，通过静态类型系统和动态类型状态跟踪，实现丰富的语义约束表达，显著提升了代码覆盖率和bug发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有fuzzing技术在扩展性方面存在挑战：自动化harness生成在合成时固定API序列，限制了测试的行为范围；而动态探索新序列的方法缺乏表达真实世界使用约束的能力，导致API误用产生的误报。

Method: 提出stitching技术，将API使用约束编码为片段，由fuzzer在运行时动态组装。采用静态类型系统管理对象在块间的流动，同时使用动态检查的外部类型状态跟踪跨块的任意元数据，从而表达丰富的语义约束，如对象状态依赖和跨函数前置条件。

Result: 在33个基准测试中，STITCH在21个上实现了最高代码覆盖率，发现了30个真实bug，而其他所有工具合计仅发现10个。精度显著更高（70% vs 次优LLM工具的12%）。在1365个开源项目中自动部署，发现了102个项目中的131个新bug，其中73个已修复。

Conclusion: STITCH通过动态组装API约束片段的技术，有效解决了fuzzing扩展性问题，能够表达丰富的语义约束，显著提升了代码覆盖率和bug发现能力，在实际项目中验证了其有效性。

Abstract: Fuzzing is a powerful technique for finding bugs in software libraries, but scaling it remains difficult. Automated harness generation commits to fixed API sequences at synthesis time, limiting the behaviors each harness can test. Approaches that instead explore new sequences dynamically lack the expressiveness to model real-world usage constraints leading to false positives from straightforward API misuse.
  We propose stitching, a technique that encodes API usage constraints in pieces that a fuzzer dynamically assembles at runtime. A static type system governs how objects flow between blocks, while a dynamically-checked extrinsic typestate tracks arbitrary metadata across blocks, enabling specifications to express rich semantic constraints such as object state dependencies and cross-function preconditions. This allows a single specification to describe an open-ended space of valid API interactions that the fuzzer explores guided by coverage feedback.
  We implement stitching in STITCH, using LLMs to automatically configure projects for fuzzing, synthesize a specification, triage crashes, and repair the specification itself. We evaluated STITCH against four state-of-the-art tools on 33 benchmarks, where it achieved the highest code coverage on 21 and found 30 true-positive bugs compared to 10 by all other tools combined, with substantially higher precision (70% vs. 12% for the next-best LLM-based tool). Deployed automatically on 1365 widely used open-source projects, STITCH discovered 131 new bugs across 102 projects, 73 of which have already been patched.

</details>


### [36] [Efficient Dynamic Test Case Generation for Path-Based Coverage Criteria](https://arxiv.org/abs/2602.18768)
*Jakub Zelek,Jakub Ruszil,Adam Roman,Artur Polański*

Main category: cs.SE

TL;DR: 提出一种新颖的测试用例生成方法，支持四种白盒路径覆盖准则，采用流式增量生成，内存高效且可扩展到大图


<details>
  <summary>Details</summary>
Motivation: 现有测试用例生成方法需要预先计算整个测试套件，效率低下且内存消耗大，缺乏灵活性

Method: 基于改进的Johnson算法，增量生成测试用例；利用控制流图的强连通分量新理论刻画prime paths；允许限制测试路径中的循环数量

Result: 在执行时间和内存消耗上优于现有技术，提供更灵活高效的测试工具，显著减少测试设计开销

Conclusion: 该方法实现了流式测试用例生成，支持四种覆盖准则，具有内存高效、可扩展和灵活控制的特点，是测试设计的重要进步

Abstract: We present a novel approach to test-case generation that satisfies four white-box, path-based coverage criteria: Prime Path, Simple Cycle, Simple Path, and Edge-Acyclic Path. Our method builds on a modified version of Johnson algorithm and enables test cases to be generated incrementally and on demand, rather than requiring the entire test suite to be computed upfront. This streaming capability represents a substantial advancement over existing approaches, as it allows testers to begin executing and refining tests immediately, thereby significantly improving the efficiency of test design. Our solution is inherently memory efficient, as it does not store all discovered coverage items; instead, it retains only the minimal set of paths required to generate subsequent coverage items on the fly. As a result, the approach scales to arbitrarily large graphs. In addition, the algorithm gives testers explicit control over the size of the generated test suite by allowing them to restrict the number of cycles permitted in a test path. The approach is grounded in new theoretical insights, most notably a novel characterization of prime paths in terms of the strongly connected components of control-flow graphs. We complement these theoretical contributions with a practical implementation and a comprehensive empirical evaluation. The results demonstrate that our method not only outperforms existing techniques in terms of execution time and memory consumption, but also provides testers with a more flexible and efficient tool for achieving high coverage while substantially reducing test design overhead.

</details>


### [37] [Operational Robustness of LLMs on Code Generation](https://arxiv.org/abs/2602.18800)
*Debalina Ghosh Paul,Hong Zhu,Ian Bayley*

Main category: cs.SE

TL;DR: 提出一种评估LLM代码生成鲁棒性的场景域分析方法，通过寻找导致错误输出的最小自然语言描述变化来评估模型对任务描述变化的敏感性。


<details>
  <summary>Details</summary>
Motivation: LLM在软件开发中广泛用于代码生成，需要评估其鲁棒性。现有评估方法不适合代码生成，因为自然语言描述输入空间是离散的，需要专门的方法来评估LLM对编码任务描述变化的敏感性。

Method: 提出场景域分析方法，旨在找到导致LLM产生错误输出的自然语言描述的最小预期变化。该方法通过理论证明和实验验证，评估LLM在不同编码任务描述变化下的鲁棒性。

Result: 成功评估了四个先进LLM（Gemini-pro、Codex、Llamma2、Falcon 7B）的鲁棒性并进行了排名。发现更复杂的任务和更高级的主题（如多线程和数据结构）鲁棒性更低。

Conclusion: 场景域分析是评估LLM代码生成鲁棒性的有效方法，能够量化模型对任务描述变化的敏感性，为选择更可靠的代码生成模型提供依据，并揭示了任务复杂度和主题对鲁棒性的影响。

Abstract: It is now common practice in software development for large language models (LLMs) to be used to generate program code. It is desirable to evaluate the robustness of LLMs for this usage. This paper is concerned in particular with how sensitive LLMs are to variations in descriptions of the coding tasks. However, existing techniques for evaluating this robustness are unsuitable for code generation because the input data space of natural language descriptions is discrete. To address this problem, we propose a robustness evaluation method called scenario domain analysis, which aims to find the expected minimal change in the natural language descriptions of coding tasks that would cause the LLMs to produce incorrect outputs. We have formally proved the theoretical properties of the method and also conducted extensive experiments to evaluate the robustness of four state-of-the-art art LLMs: Gemini-pro, Codex, Llamma2 and Falcon 7B, and have found that we are able to rank these with confidence from best to worst. Moreover, we have also studied how robustness varies in different scenarios, including the variations with the topic of the coding task and with the complexity of its sample solution, and found that robustness is lower for more complex tasks and also lower for more advanced topics, such as multi-threading and data structures.

</details>


### [38] [From Docs to Descriptions: Smell-Aware Evaluation of MCP Server Descriptions](https://arxiv.org/abs/2602.18914)
*Peiran Wang,Ying Li,Yuqiang Sun,Chengwei Liu,Yang Liu,Yuan Tian*

Main category: cs.SE

TL;DR: 该研究首次系统分析了MCP工具描述中的质量问题，提出了四维质量标准，发现描述缺陷普遍存在且显著影响LLM工具选择，标准合规描述能大幅提升选择概率。


<details>
  <summary>Details</summary>
Motivation: MCP已成为连接LLM代理与外部工具的事实标准，但其工具描述依赖自由文本且约束松散，导致描述常误报或遗漏关键语义，增加试错集成、降低代理性能并引入安全风险。

Method: 基于软件/API文档实践和代理工具使用需求，提出包含准确性、功能性、信息完整性和简洁性的四维质量标准，涵盖18个具体缺陷类别。在10,831个MCP服务器数据集上进行大规模实证研究，并通过受控突变实验评估缺陷影响。

Result: 发现描述缺陷普遍存在（如73%重复工具名，数千个存在参数语义错误或缺少返回描述），反映"代码优先，描述最后"模式。功能性和准确性缺陷对LLM工具选择影响最大（分别+11.6%和+8.8%）。在功能等效服务器竞争中，标准合规描述达到72%选择概率（比20%基线提升260%）。

Conclusion: MCP工具描述存在系统性质量问题，提出的质量标准能有效识别缺陷并指导修复，显著提升LLM工具选择效果。发布标注数据集和标准以支持未来可靠安全的MCP生态系统建设。

Abstract: The Model Context Protocol (MCP) has rapidly become a de facto standard for connecting LLM-based agents with external tools via reusable MCP servers. In practice, however, server selection and onboarding rely heavily on free-text tool descriptions that are intentionally loosely constrained. Although this flexibility largely ensures the scalability of MCP servers, it also creates a reliability gap that descriptions often misrepresent or omit key semantics, increasing trial-and-error integration, degrading agent behavior, and potentially introducing security risks. To this end, we present the first systematic study of description smells in MCP tool descriptions and their impact on usability. Specifically, we synthesize software/API documentation practices and agentic tool-use requirements into a four-dimensional quality standard: accuracy, functionality, information completeness, and conciseness, covering 18 specific smell categories. Using this standard, we conducted a large-scale empirical study on a well-constructed dataset of 10,831 MCP servers. We find that description smells are pervasive (e.g., 73% repeated tool names, thousands with incorrect parameter semantics or missing return descriptions), reflecting a "code-first, description-last" pattern. Through a controlled mutation-based study, we show these smells significantly affect LLM tool selection, with functionality and accuracy having the largest effects (+11.6% and +8.8%, p < 0.001). In competitive settings with functionally equivalent servers, standard-compliant descriptions reach 72% selection probability (260% over a 20% baseline), demonstrating that smell-guided remediation yields substantial practical benefits. We release our labeled dataset and standards to support future work on reliable and secure MCP ecosystems.

</details>


### [39] [Narrowing the Complexity Gap in the Evaluation of Large Language Models](https://arxiv.org/abs/2602.18928)
*Yang Chen,Shuyang Liu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: GeneBench通过多目标优化自动增加编程问题的复杂性，使其更接近真实世界复杂度，评估13个LLM在四个基准上性能平均下降35.2%，揭示了LLM在真实编程场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM编程评估基准过于简化，可能导致高估其实际编程能力。真实世界代码复杂度评估至关重要，但构建真实基准耗时且易受数据污染影响。

Method: GeneBench采用多目标优化技术，自动增加编程问题的复杂性，同时保持代码可读性接近真实世界程序，可将任何编程基准转化为更复杂的版本。

Result: 在四个广泛使用的编程基准上评估13个LLM（包括两个推理LLM），所有编程任务性能显著下降（14.9%-60.5%，平均35.2%），即使few-shot提示或微调也难以改善。

Conclusion: GeneBench能有效评估LLM在真实世界复杂度下的编程能力，无需昂贵构建真实基准，揭示了LLM在实际编程场景中的显著局限性。

Abstract: Evaluating Large Language Models (LLMs) with respect to real-world code complexity is essential. Otherwise, there is a risk of overestimating LLMs' programming abilities based on simplistic benchmarks, only to be disappointed when using them in real-world settings. Recently, researchers explored the construction of more realistic benchmarks by mining or augmenting open-source repositories. Such solutions are usually task-specific. Data quality control from real-world projects can also be time-consuming and error-prone. More importantly, evaluating LLMs on fixed benchmark problems is subject to data contamination and overfitting. We propose GeneBench, an automated technique to add real-world complexities to any programming benchmark. GeneBench leverages a multi-objective optimization to increase the complexity of programming problems while maintaining the readability of code similar to real-world programs. Transforming four widely-used programming benchmarks using GeneBench and evaluating 13 LLMs (including two reasoning LLMs) on them shows a notable performance drop across all programming tasks (14.9%-60.5%, avg=35.2%), demonstrating LLMs' struggle under real-world complexities. The struggle persists even when LLMs are few-shot prompted or fine-tuned with examples from different versions of GeneBench, demonstrating the challenging nature of the problems. Finally, we show that the performance of the studied LLMs in bug repair is similar under GeneBench and SWE-Bench. This, along with the consistent reproduction of performance drop of all studied LLMs across four tasks under different versions of GeneBench, makes the technique suitable to evaluate LLMs without costly construction of real-world benchmarks.

</details>


### [40] [A Systematic Evaluation of Environmental Flakiness in JavaScript Tests](https://arxiv.org/abs/2602.19098)
*Negar Hashemi,Amjed Tahir,August Shi,Shawn Rasheed,Rachel Blagojevic*

Main category: cs.SE

TL;DR: 研究JavaScript测试中环境因素导致的测试不稳定性（flakiness），识别了操作系统、Node.js版本和浏览器三个关键环境因素，并开发了js-env-sanitizer工具来缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 测试不稳定性是工业界的重要问题，影响测试效率和产品质量。现有研究对动态语言（特别是JavaScript）中环境因素导致的测试不稳定性关注不足，需要系统评估环境因素对JavaScript测试不稳定性的影响。

Method: 1. 在多种环境配置下执行测试套件，确定环境变化是否导致不稳定行为；2. 操纵三个环境因素：操作系统、Node.js版本和浏览器；3. 开发轻量级缓解工具js-env-sanitizer，通过跳过并报告环境相关的不稳定测试（而非失败）来清理环境。

Result: 识别了65个环境不稳定项目：28个与操作系统问题相关，5个与Node.js版本兼容性相关，16个与操作系统和Node.js组合问题相关，17个与浏览器兼容性相关。js-env-sanitizer工具实现了高准确性，性能开销小，支持Jest、Mocha和Vitest三个流行的JavaScript测试框架。

Conclusion: 环境因素是JavaScript测试不稳定的重要原因，js-env-sanitizer提供了一种有效的轻量级缓解方案，允许CI构建继续/成功运行而无需重新运行整个测试套件，提高了测试效率和可靠性。

Abstract: Test flakiness is a significant issue in industry, affecting test efficiency and product quality. While extensive research has examined the impact of flaky tests, many root causes remain unexplored, particularly in the context of dynamic languages such as JavaScript. In this paper, we conduct a systematic evaluation of the impact of environmental factors on test flakiness in JavaScript. We first executed test suites across multiple environmental configurations to determine whether changes in the environment could lead to flaky behavior. We selected three environmental factors to manipulate: the operating system, the Node.js version, and the browser. We identified a total of 65 environmental flaky projects, with 28 related to operating system issues, five to Node.js version compatibility, 16 to a combination of operating system and Node.js issues, and 17 related to browser compatibility. To address environmental flakiness, we developed a lightweight mitigation approach, js-env-sanitizer, that can sanitize environmental-related flaky tests by skipping and reporting them (rather than failing), allowing CI builds to continue/succeed without rerunning entire test suites. The tool achieves high accuracy with minimal performance or configuration overhead, and currently supports three popular JavaScript testing frameworks (Jest, Mocha, and Vitest)

</details>


### [41] [Gecko: A Simulation Environment with Stateful Feedback for Refining Agent Tool Calls](https://arxiv.org/abs/2602.19218)
*Zeyu Zhang,Guohao Li,Zhenchang Xing,Alexandros Apostolopoulos,Yu Lin Lee,Liang Zheng*

Main category: cs.SE

TL;DR: Gecko是一个模拟工具响应的环境，通过规则和LLMs结合检查工具调用有效性、合成合理响应并评估任务完成情况，提出的GATS方法能显著提升各种LLM的工具调用性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统依赖LLMs规划和生成工具调用，但这些调用容易出错，且使用真实工具进行迭代优化既昂贵又不安全，需要更好的解决方案来改进工具调用质量。

Method: 提出Gecko环境，结合规则和LLMs模拟工具响应：检查工具调用有效性（包括参数和工具名）、合成符合输出模式的合理响应、评估任务目标完成情况，形成GATS测试时扩展方法。

Result: 在BFCLv3和τ²-bench基准测试中，GATS方法持续提升了GPT-4o、GPT-5和Gemini-3.0-pro等多种LLM的工具调用性能。

Conclusion: Gecko环境通过模拟工具响应为LLM工具调用提供有效反馈，GATS方法简单但能显著提升性能，未来可进一步探索其工作机制和扩展可能性。

Abstract: The ability to use tools is fundamental for large language model (LLM) agents. Given a task, existing systems use LLMs to plan and generate tool calls, which are executed by real-world tools to complete the task. However, tool calls are prone to errors because they are derived merely from LLM intrinsic capabilities. What is more, while it is useful to let LLMs iteratively refine the tool-call sequence using execution results from real tools, this process can be expensive and lead to unsafe results. To improve LLM tool calls and address issues caused by using real tools for refinement, we introduce Gecko, a comprehensive environment that simulates tool responses using a combination of rules and LLMs. Specifically, Gecko checks the validity of tool calls including input arguments and tool names, synthesizes reasonable responses that adhere to the output schema, and assesses whether all task objectives have been achieved. These three types of feedback provided by Gecko allow LLMs to refine their tool calls, forming a simple yet effective test-time scaling method named GATS. On BFCLv3 and $τ^2$-bench, GATS consistently improves the tool calling performance of various LLMs including GPT-4o, GPT-5, and Gemini-3.0-pro. We further discuss working mechanisms of our method and share future possibilities.

</details>


### [42] [ComUICoder: Component-based Reusable UI Code Generation for Complex Websites via Semantic Segmentation and Element-wise Feedback](https://arxiv.org/abs/2602.19276)
*Jingyu Xiao,Jiantong Qin,Shuoqi Li,Man Ho Lam,Yuxuan Wan,Jen-tse Huang,Yintong Huo,Michael R. Lyu*

Main category: cs.SE

TL;DR: ComUICoder：基于组件的UI代码生成框架，通过语义感知分割、代码重用和细粒度优化，解决MLLM在复杂多页面网站生成中的碎片化、冗余和不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在UI转代码任务中，面对长而复杂的网站时存在三个主要问题：碎片化分割、重复组件的冗余代码生成、以及频繁的UI不一致性。这些问题限制了MLLM在实际复杂网站场景中的应用效果。

Method: 提出ComUICoder框架，包含三个核心组件：1）混合语义感知块分割，用于准确检测UI语义连贯的块；2）视觉感知的基于图的块合并，在页面内和跨页面合并结构相似的组件以实现可重用实现；3）基于优先级的元素级反馈，优化生成代码并减少元素级不一致性。

Result: 在ComUIBench基准测试中，ComUICoder显著提高了复杂多页面网站的整体生成质量和代码可重用性。实验证明该框架能有效解决现有MLLM在复杂UI代码生成中的局限性。

Conclusion: ComUICoder通过组件化的方法，结合语义分割、代码重用和细粒度优化，为复杂多页面网站的UI代码生成提供了有效的解决方案，显著提升了生成质量和可重用性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance on the UI-to-code task, which aims to generate UI code from design mock-ups. However, when applied to long and complex websites, they often struggle with fragmented segmentation, redundant code generation for repetitive components, and frequent UI inconsistencies. To systematically investigate and address these challenges, we introduce ComUIBench, a new multi-page complex webpage benchmark with component annotations, designed to evaluate MLLMs' ability to generate reusable UI code in realistic website scenarios. Building upon this benchmark, we propose ComUICoder, a component-based UI code generation framework that emphasizes semantic-aware segmentation, code reuse, and fine-grained refinement. Specifically, ComUICoder incorporates (1) Hybrid Semantic-aware Block Segmentation for accurate UI semantic coherent block detection, (2) Visual-aware Graph-based Block Merge to consolidate structurally similar components within and across webpages for reusable implementation, and (3) Priority-based Element-wise Feedback to refine generated code and reduce element-level inconsistencies. Extensive experiments demonstrate that ComUICoder significantly improves overall generation quality and code reusability on complex multipage websites. Our datasets and code are publicly available at https://github.com/WebPAI/ComUICoder.

</details>


### [43] [Towards Automated Page Object Generation for Web Testing using Large Language Models](https://arxiv.org/abs/2602.19294)
*Betül Karagöz,Filippo Ricca,Matteo Biagiola,Andrea Stocco*

Main category: cs.SE

TL;DR: LLMs (GPT-4o和DeepSeek Coder)能够自动生成网页测试的Page Objects，准确率32.6%-54.0%，元素识别率超过70%，为自动化测试提供新方向。


<details>
  <summary>Details</summary>
Motivation: Page Objects是提高自动化端到端网页测试可维护性和可扩展性的设计模式，但创建和维护POs主要是手动、劳动密集型活动，自动化解决方案的实际采用有限。LLMs在这些任务中的潜力尚未充分探索。

Method: 使用GPT-4o和DeepSeek Coder等LLMs自动生成网页测试的Page Objects，在五个网页应用的现有基准上进行评估，这些应用有手动编写的POs作为基准。评估指标包括准确率（正确识别的基准元素比例）和元素识别率（正确识别或标记修改的元素比例）。

Result: LLMs能够生成语法正确且功能有用的POs，准确率在32.6%到54.0%之间，在大多数情况下元素识别率超过70%。

Conclusion: 这是首次系统评估LLMs在自动PO生成方面的优势和开放挑战，为将LLMs集成到实际测试工作流程中提供了进一步研究方向。

Abstract: Page Objects (POs) are a widely adopted design pattern for improving the maintainability and scalability of automated end-to-end web tests. However, creating and maintaining POs is still largely a manual, labor-intensive activity, while automated solutions have seen limited practical adoption. In this context, the potential of Large Language Models (LLMs) for these tasks has remained largely unexplored. This paper presents an empirical study on the feasibility of using LLMs, specifically GPT-4o and DeepSeek Coder, to automatically generate POs for web testing. We evaluate the generated artifacts on an existing benchmark of five web applications for which manually written POs are available (the ground truth), focusing on accuracy (i.e., the proportion of ground truth elements correctly identified) and element recognition rate (i.e., the proportion of ground truth elements correctly identified or marked for modification). Our results show that LLMs can generate syntactically correct and functionally useful POs with accuracy values ranging from 32.6% to 54.0% and element recognition rate exceeding 70% in most cases. Our study contributes the first systematic evaluation of LLMs strengths and open challenges for automated PO generation, and provides directions for further research on integrating LLMs into practical testing workflows.

</details>


### [44] [Designing and Implementing a Comprehensive Research Software Engineer Career Ladder: A Case Study from Princeton University](https://arxiv.org/abs/2602.19353)
*Ian A. Cosden,Elizabeth Holtz,Joel U. Bretheim*

Main category: cs.SE

TL;DR: 普林斯顿大学为应对研究软件工程师（RSE）需求激增，设计并实施了涵盖从初级到首席级别的综合职业阶梯，包含团队领导和管理双轨路径。


<details>
  <summary>Details</summary>
Motivation: 研究软件工程师在计算研究中日益重要，但高等教育机构对新科技角色的职业路径缺乏结构化设计。普林斯顿大学面临RSE需求激增和快速招聘的挑战，需要制定既能标准化又能灵活适应个体差异的职业发展框架。

Method: 通过案例研究方法，设计综合RSE职业阶梯（从助理到首席级别），建立平行团队领导和管理双轨路径。采用指导原则、能力框架、人力资源对齐、外部顾问参与，并映射到标准职位层级框架和市场基准。

Result: 实施后取得了积极成果：提高了招聘效率，明确了晋升路径，获得了员工积极反馈。职业阶梯设计成功平衡了标准化需求与职位独特性。

Conclusion: 该案例研究表明，通过精心设计的职业阶梯框架，高等教育机构能够有效应对RSE角色的快速增长需求，为技术专长、学术影响和领导力发展提供结构化支持，同时保持职位灵活性。

Abstract: Research Software Engineers (RSEs) have become indispensable to computational research and scholarship. The fast rise of RSEs in higher education and the trend of universities to be slow creating or adopting models for new technology roles means a lack of structured career pathways that recognize technical mastery, scholarly impact, and leadership growth. In response to an immense demand for RSEs at Princeton University, and dedicated funding to grow the RSE group at least two-fold, Princeton was forced to strategize how to cohesively define job descriptions to match the rapid hiring of RSE positions but with enough flexibility to recognize the unique nature of each individual position. This case study describes our design and implementation of a comprehensive RSE career ladder spanning Associate through Principal levels, with parallel team-lead and managerial tracks. We outline the guiding principles, competency framework, Human Resources (HR) alignment, and implementation process, including engagement with external consultants and mapping to a standard job leveling framework utilizing market benchmarks. We share early lessons learned and outcomes including improved hiring efficiency, clearer promotion pathways, and positive reception among staff.

</details>


### [45] [Compliance Management for Federated Data Processing](https://arxiv.org/abs/2602.19360)
*Natallia Kokash,Adam Belloum,Paola Grosso*

Main category: cs.SE

TL;DR: 提出一个结合策略即代码、工作流编排和LLM辅助合规管理的联邦数据处理框架，解决异构访问策略、法规要求和跨组织工作流管理的复杂性。


<details>
  <summary>Details</summary>
Motivation: 联邦数据处理(FDP)虽然为敏感数据的协作分析提供了有前景的方法，但在实际应用中仍面临诸多挑战：异构访问策略管理复杂、法规要求多样、跨组织长期工作流难以协调，这些因素限制了FDP在现实世界的采用。

Method: 提出一个合规感知的FDP框架，整合三个关键技术：1) 策略即代码(将法律和组织要求转化为机器可执行的策略)，2) 工作流编排(管理跨组织的长期数据处理流程)，3) LLM辅助合规管理(利用大语言模型协助合规性管理)。

Result: 通过实现的原型系统，展示了如何将法律和组织要求收集并转化为FDP网络中机器可执行的策略，验证了框架的可行性。

Conclusion: 该框架通过整合策略即代码、工作流编排和LLM辅助合规管理，有效解决了FDP在实际部署中的合规性和管理复杂性挑战，为促进联邦数据处理的现实应用提供了可行方案。

Abstract: Federated data processing (FDP) offers a promising approach for enabling collaborative analysis of sensitive data without centralizing raw datasets. However, real-world adoption remains limited due to the complexity of managing heterogeneous access policies, regulatory requirements, and long-running workflows across organizational boundaries. In this paper, we present a framework for compliance-aware FDP that integrates policy-as-code, workflow orchestration, and large language model (LLM)-assisted compliance management. Through the implemented prototype, we show how legal and organizational requirements can be collected and translated into machine-actionable policies in FDP networks.

</details>


### [46] [On the Variability of Source Code in Maven Package Rebuilds](https://arxiv.org/abs/2602.19383)
*Jens Dietrich,Behnaz Hassanshahi*

Main category: cs.SE

TL;DR: 研究验证开源软件包重建时是否使用相同源代码，发现Google和Oracle的替代构建中28个流行包的85个版本存在源代码不一致问题，主要原因是构建时生成代码的扩展机制难以复现。


<details>
  <summary>Details</summary>
Motivation: 开源软件包重建已成为提升软件供应链安全的常见工业实践，但需要验证替代构建是否真正使用相同的源代码，以确保安全改进的有效性。

Method: 比较Maven Central发布的源代码与Google Assured Open Source和Oracle Build-from-Source项目的替代构建源代码，分析28个流行包的85个版本的非等价源代码情况。

Result: 发现替代构建与原始构建存在源代码不一致问题，主要原因是构建时生成代码的扩展机制（如代码生成工具）难以复现，导致重建时无法完全复制原始构建过程。

Conclusion: 开源软件包重建面临源代码不一致的挑战，需要制定策略解决构建时生成代码的复现问题，以确保替代构建真正使用相同源代码并实现预期的安全改进。

Abstract: Rebuilding packages from open source is a common practice to improve the security of software supply chains, and is now done at an industrial scale. The basic principle is to acquire the source code used to build a package published in a repository such as Maven Central (for Java), rebuild the package independently with hardened security, and publish it in some alternative repository. In this paper we test the assumption that the same source code is being used by those alternative builds. To study this, we compare the sources released with packages on Maven Central, with the sources associated with independently built packages from Google's Assured Open Source and Oracle's Build-from-Source projects. We study non-equivalent sources for alternative builds of 28 popular packages with 85 releases. We investigate the causes of non-equivalence, and find that the main cause is build extensions that generate code at build time, which are difficult to reproduce. We suggest strategies to address this issue.

</details>


### [47] [Multi-CoLoR: Context-Aware Localization and Reasoning across Multi-Language Codebases](https://arxiv.org/abs/2602.19407)
*Indira Vats,Sanjukta De,Subhayan Roy,Saurabh Bodhe,Lejin Varghese,Max Kiehn,Yonas Bedasso,Marsha Chechik*

Main category: cs.SE

TL;DR: Multi-CoLoR是一个用于多语言代码库上下文感知定位和推理的框架，通过集成组织知识检索和图推理来改进代码定位性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码生成方面表现良好，但在复杂多语言代码库中定位相关代码方面存在困难。现有方法要么局限于单语言基准测试，要么通过浅层文本相似性跨语言检索代码，要么假设没有先验上下文。

Method: Multi-CoLoR采用两阶段方法：1) 相似问题上下文(SIC)模块检索语义和组织相关的历史问题以缩小搜索空间；2) 代码图遍历代理(扩展版LocAgent)在C++和QML代码库中进行结构推理。

Result: 在真实企业数据集上的评估显示，SIC减少了搜索空间并提高了定位准确性，图推理在Python之外的多语言代码库中有效泛化。Multi-CoLoR在AMD代码库上提高了Acc@5指标，同时减少了工具调用。

Conclusion: Multi-CoLoR通过结合组织知识检索和图推理，有效解决了多语言代码库中的代码定位问题，超越了现有的词法和图基线方法。

Abstract: Large language models demonstrate strong capabilities in code generation but struggle to navigate complex, multi-language repositories to locate relevant code. Effective code localization requires understanding both organizational context (e.g., historical issue-fix patterns) and structural relationships within heterogeneous codebases. Existing methods either (i) focus narrowly on single-language benchmarks, (ii) retrieve code across languages via shallow textual similarity, or (iii) assume no prior context. We present Multi-CoLoR, a framework for Context-aware Localization and Reasoning across Multi-Language codebases, which integrates organizational knowledge retrieval with graph-based reasoning to traverse complex software ecosystems. Multi-CoLoR operates in two stages: (i) a similar issue context (SIC) module retrieves semantically and organizationally related historical issues to prune the search space, and (ii) a code graph traversal agent (an extended version of LocAgent, a state-of-the-art localization framework) performs structural reasoning within C++ and QML codebases. Evaluations on a real-world enterprise dataset show that incorporating SIC reduces the search space and improves localization accuracy, and graph-based reasoning generalizes effectively beyond Python-only repositories. Combined, Multi-CoLoR improves Acc@5 over both lexical and graph-based baselines while reducing tool calls on an AMD codebase.

</details>


### [48] [When AI Teammates Meet Code Review: Collaboration Signals Shaping the Integration of Agent-Authored Pull Requests](https://arxiv.org/abs/2602.19441)
*Costain Nachuma,Minhaz Zibran*

Main category: cs.SE

TL;DR: 研究分析GitHub上AI代理提交的PR如何融入人类评审流程，发现评审者参与度是成功合并的最强预测因素，而大改动和破坏协作的行为会降低合并概率


<details>
  <summary>Details</summary>
Motivation: 随着自主编码代理在GitHub上提交PR的增多，需要了解这些AI贡献如何融入人类驱动的评审工作流，以及影响集成结果的因素

Method: 使用公共AIDev数据集进行大规模实证研究，采用逻辑回归分析（含仓库聚类标准误），并辅以定性分析，考察集成结果、解决速度和评审时协作信号

Result: 评审者参与度与成功集成最强相关；大改动和强制推送等破坏协调的行为降低合并概率；迭代强度在考虑协作信号后解释力有限；成功集成需要AI代理参与可操作的评审循环并满足评审者期望

Conclusion: AI代理PR的有效集成不仅取决于代码质量，还需要与现有评审和协调实践保持一致，强调协作流程对齐的重要性

Abstract: Autonomous coding agents increasingly contribute to software development by submitting pull requests on GitHub; yet, little is known about how these contributions integrate into human-driven review workflows. We present a large empirical study of agent-authored pull requests using the public AIDev dataset, examining integration outcomes, resolution speed, and review-time collaboration signals. Using logistic regression with repository-clustered standard errors, we find that reviewer engagement has the strongest correlation with successful integration, whereas larger change sizes and coordination-disrupting actions, such as force pushes, are associated with a lower likelihood of merging. In contrast, iteration intensity alone provides limited explanatory power once collaboration signals are considered. A qualitative analysis further shows that successful integration occurs when agents engage in actionable review loops that converge toward reviewer expectations. Overall, our results highlight that the effective integration of agent-authored pull requests depends not only on code quality but also on alignment with established review and coordination practices.

</details>


### [49] ["Write in English, Nobody Understands Your Language Here": A Study of Non-English Trends in Open-Source Repositories](https://arxiv.org/abs/2602.19446)
*Masudul Hasan Masud Bhuiyan,Manish Kumar Bala Kumar,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: 该研究分析了2015-2025年间GitHub上91.4亿个问题和讨论，发现开源软件社区正变得更加多语言化，特别是韩语、中文和俄语的使用增长显著，但语言多样性也带来了英语使用规范与母语表达之间的张力。


<details>
  <summary>Details</summary>
Motivation: 随着全球参与度提高和Unicode等标准对非拉丁文字的支持，开源软件社区正逐渐变得更加多语言化。研究旨在探究开源软件多语言化的程度及其影响。

Method: 分析2015-2025年间GitHub上的91.4亿个问题、拉取请求和讨论，以及62,500个仓库，涵盖5种编程语言和30种自然语言。通过六个研究问题追踪沟通、代码和文档中语言使用的变化。

Result: 多语言参与度稳步增长，特别是韩语、中文和俄语。这种增长不仅体现在问题和讨论中，还出现在代码注释、字符串字面量和文档文件中。但非英语或多语言项目往往获得较少关注和参与，表明语言既是资源也是障碍。

Conclusion: 开源软件正变得更加多语言化，反映了更大的包容性和语言多样性，但也带来了语言张力。母语表达与英语使用规范之间的冲突，以及非英语项目的可见度问题，表明语言继续塑造着谁被听到、谁贡献以及开放协作如何展开。

Abstract: The open-source software (OSS) community has historically been dominated by English as the primary language for code, documentation, and developer interactions. However, with growing global participation and better support for non-Latin scripts through standards like Unicode, OSS is gradually becoming more multilingual. This study investigates the extent to which OSS is becoming more multilingual, analyzing 9.14 billion GitHub issues, pull requests, and discussions, and 62,500 repositories across five programming languages and 30 natural languages, covering the period from 2015 to 2025. We examine six research questions to track changes in language use across communication, code, and documentation. We find that multilingual participation has steadily increased, especially in Korean, Chinese, and Russian. This growth appears not only in issues and discussions but also in code comments, string literals, and documentation files. While this shift reflects greater inclusivity and language diversity in OSS, it also creates language tension. The ability to express oneself in a native language can clash with shared norms around English use, especially in collaborative settings. Non-English or multilingual projects tend to receive less visibility and participation, suggesting that language remains both a resource and a barrier, shaping who gets heard, who contributes, and how open collaboration unfolds.

</details>


### [50] [Workflow-Level Design Principles for Trustworthy GenAI in Automotive System Engineering](https://arxiv.org/abs/2602.19614)
*Chih-Hong Cheng,Brian Hsuan-Cheng Liao,Adam Molin,Hasan Esen*

Main category: cs.SE

TL;DR: 提出工作流级设计原则，用于在安全关键汽车工程中可信地集成生成式AI，通过需求差异识别、SysML v2架构更新和回归测试的端到端管道来验证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在安全关键系统工程中的应用受到可信度、可追溯性以及与现有验证实践对齐的限制。需要建立可信的生成式AI集成方法来解决这些问题。

Method: 1) 采用分节分解、多样性采样和轻量级NLP完整性检查，而非整体提示；2) 将需求差异传播到SysML v2模型中，通过编译和静态分析验证更新；3) 通过从规范变量到架构端口和状态的显式映射生成测试用例，确保可追溯的回归测试。

Result: 分节分解方法比整体提示提高了完整性和正确性；实现了从需求差异识别到架构更新和回归测试的端到端可信管道；为安全关键汽车工程中的生成式AI应用提供了实际保障。

Conclusion: 提出了工作流级设计原则，成功演示了在汽车工程中可信集成生成式AI的端到端管道，解决了大型规范中的关键变更识别、架构更新验证和可追溯回归测试等问题。

Abstract: The adoption of large language models in safety-critical system engineering is constrained by trustworthiness, traceability, and alignment with established verification practices. We propose workflow-level design principles for trustworthy GenAI integration and demonstrate them in an end-to-end automotive pipeline, from requirement delta identification to SysML v2 architecture update and re-testing. First, we show that monolithic ("big-bang") prompting misses critical changes in large specifications, while section-wise decomposition with diversity sampling and lightweight NLP sanity checks improves completeness and correctness. Then, we propagate requirement deltas into SysML v2 models and validate updates via compilation and static analysis. Additionally, we ensure traceable regression testing by generating test cases through explicit mappings from specification variables to architectural ports and states, providing practical safeguards for GenAI used in safety-critical automotive engineering.

</details>


### [51] [Towards Understanding Views on Combining Videos and Gamification in Software Engineering Training](https://arxiv.org/abs/2602.19628)
*Pasan Peiris,Matthias Galster,Antonija Mitrovic,Sanna Malinen,Raul Vincent Lumapas,Jay Holland*

Main category: cs.SE

TL;DR: 研究软件工程学生和行业从业者对视频培训游戏化的看法，发现两者对视频培训和游戏化结合持相似积极态度


<details>
  <summary>Details</summary>
Motivation: 被动观看培训视频导致学习效果肤浅，而游戏化能增加参与度，需要了解软件工程领域相关人员对视频培训游戏化的看法

Method: 对软件工程学生和行业从业者进行问卷调查，收集他们对视频培训和游戏化结合的看法

Result: 学生和专业人士对视频培训有相似看法，都支持将游戏化与视频培训相结合

Conclusion: 研究结果可为软件工程师的游戏化培训方案设计提供参考

Abstract: Watching training videos passively leads to superficial learning. Adding gamification can increase engagement. We study how software engineering students and industry practitioners view gamifying video-based training. We conducted a survey with students and professionals. Students and professionals share similar perceptions toward video-based training in general and support combining gamification and video-based training. Our findings can inform the design of gamified training solutions for software engineers.

</details>


### [52] [Carbon-Aware Governance Gates: An Architecture for Sustainable GenAI Development](https://arxiv.org/abs/2602.19718)
*Mateen A. Abbasi,Tommi J. Mikkonen,Petri J. Ihantola,Muhammad Waseem,Pekka Abrahamsson,Niko K. Mäkitalo*

Main category: cs.SE

TL;DR: 提出Carbon-Aware Governance Gates (CAGG)架构，将碳预算、能源溯源和可持续性验证编排集成到AI辅助开发治理中，以降低GenAI开发过程中的碳足迹。


<details>
  <summary>Details</summary>
Motivation: GenAI在软件开发中的快速采用增加了计算需求，从而提高了开发活动的碳足迹。同时，组织在GenAI辅助开发中嵌入治理机制以支持信任、透明度和问责制，但这些治理机制引入了额外的计算工作负载（如重复推理、再生周期和扩展验证管道），进一步增加了能源使用和碳足迹。

Method: 提出Carbon-Aware Governance Gates (CAGG)架构扩展，包含三个组件：(i) 能源和碳溯源账本，(ii) 碳预算管理器，(iii) 绿色验证编排器，通过治理策略和可重用设计模式进行实施。

Result: CAGG架构将碳预算、能源溯源和可持续性验证编排嵌入到人-AI治理层中，旨在减少GenAI辅助开发过程中的能源消耗和碳足迹。

Conclusion: 通过将可持续性考虑整合到GenAI辅助开发的治理机制中，CAGG架构有助于在保持治理效益的同时降低环境影响，实现更环保的AI辅助软件开发。

Abstract: The rapid adoption of Generative AI (GenAI) in the software development life cycle (SDLC) increases computational demand, which can raise the carbon footprint of development activities. At the same time, organizations are increasingly embedding governance mechanisms into GenAI-assisted development to support trust, transparency, and accountability. However, these governance mechanisms introduce additional computational workloads, including repeated inference, regeneration cycles, and expanded validation pipelines, increasing energy use and the carbon footprint of GenAI-assisted development. This paper proposes Carbon-Aware Governance Gates (CAGG), an architectural extension that embeds carbon budgets, energy provenance, and sustainability-aware validation orchestration into human-AI governance layers. CAGG comprises three components: (i) an Energy and Carbon Provenance Ledger, (ii) a Carbon Budget Manager, and (iii) a Green Validation Orchestrator, operationalized through governance policies and reusable design patterns.

</details>


### [53] [MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2602.19843)
*Jin Jia,Zhiling Deng,Zhuangbin Chen,Yingqi Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: MAS-FIRE：一个用于多智能体系统的故障注入与可靠性评估框架，通过定义15种故障类型和三种注入机制，揭示多智能体系统的容错行为层次，发现更强的基座模型不一定提升鲁棒性，而架构拓扑对系统可靠性有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的多智能体系统被越来越多地部署于复杂任务，确保其可靠性成为紧迫挑战。由于多智能体系统通过非结构化的自然语言而非刚性协议进行协调，容易产生语义故障（如幻觉、指令误解、推理漂移），这些故障会无声传播而不引发运行时异常。现有的评估方法仅测量端到端任务成功率，对故障如何产生或智能体如何有效恢复提供有限洞察。

Method: 提出MAS-FIRE框架，定义了涵盖智能体内认知错误和智能体间协调失败的15种故障类型分类法，并通过三种非侵入式机制注入故障：提示修改、响应重写和消息路由操作。将该框架应用于三种代表性的多智能体系统架构，分析其容错行为。

Result: 揭示了多智能体系统的容错行为可组织为四个层次：机制层、规则层、提示层和推理层。发现更强的基座模型并不均匀提升鲁棒性。架构拓扑同样起决定性作用，迭代闭环设计能消除超过40%导致线性工作流灾难性崩溃的故障。

Conclusion: MAS-FIRE提供了过程级可观测性和可操作指导，能够系统性地改进多智能体系统的可靠性。该框架填补了多智能体系统故障诊断和评估的空白，为构建更可靠的多智能体系统提供了方法论基础。

Abstract: As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.

</details>
