{"id": "2512.09277", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09277", "abs": "https://arxiv.org/abs/2512.09277", "authors": ["Yanpeng Yu", "Haiyue Ma", "Krish Agarwal", "Nicolai Oswald", "Qijing Huang", "Hugo Linsenmaier", "Chunhui Mei", "Ritchie Zhao", "Ritika Borkar", "Bita Darvish Rouhani", "David Nellans", "Ronny Krashinsky", "Anurag Khandelwal"], "title": "Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens", "comment": null, "summary": "Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.\n  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.", "AI": {"tldr": "METRO\u662f\u4e00\u79cd\u9488\u5bf9\u5185\u5b58\u53d7\u9650\u573a\u666f\u7684MoE\u6a21\u578b\u4e13\u5bb6\u5e76\u884c\u8def\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861GPU\u4e0a\u7684\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\u6765\u4f18\u5316\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u89e3\u7801\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u4e13\u5bb6\u5e76\u884c\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u5404GPU\u5904\u7406\u7684\u4ee4\u724c\u6570\u91cf\u6765\u89e3\u51b3\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u4f46\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u670d\u52a1\u573a\u666f\uff08\u7279\u522b\u662f\u89e3\u7801\u9636\u6bb5\uff09\u4e2d\uff0c\u8fd9\u79cd\u5e73\u8861\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u589e\u52a0\u4e86\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\uff0c\u52a0\u5267\u4e86\u5185\u5b58\u538b\u529b\u3002", "method": "\u63d0\u51faMETRO\uff08Minimum Expert Token ROuting\uff09\u7b97\u6cd5\uff1a1\uff09\u5e73\u8861\u5404GPU\u7684\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\uff1b2\uff09\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7b97\u6cd5\u6548\u7387\u548cGPU\u5e76\u884c\u5904\u7406\u80fd\u529b\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u7531\u8d28\u91cf\uff1b3\uff09\u91c7\u7528\u65b0\u9896\u7684allGather\u65b9\u6848\u6536\u96c6\u5168\u5c40top-k\u4fe1\u606f\uff0c\u76f8\u6bd4\u4f20\u7edfallToAll\u5f00\u9500\u66f4\u5c0f\u3002", "result": "\u5728\u771f\u5b9e\u7cfb\u7edf\uff08vLLM over 8 A100 GPUs\uff09\u548c\u4e13\u6709\u6a21\u62df\u5668\uff088-16 B200 GPUs\uff09\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4EPLB\uff1a1\uff09\u89e3\u7801\u5ef6\u8fdf\u964d\u4f4e11-22%\uff1b2\uff09Qwen3\u548cDeepSeek-V3\u670d\u52a1\u7684\u603b\u4ee4\u724c\u541e\u5410\u91cf\u63d0\u9ad83-21%\uff1b3\uff09\u901a\u8fc7\u5ef6\u8fdf\u4e0e\u541e\u5410\u91cf\u7684\u6743\u8861\uff0c\u5728\u56fa\u5b9a\u89e3\u7801SLO\u4e0b\u89e3\u7801\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53474.11\u500d\u3002", "conclusion": "\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u670d\u52a1\u573a\u666f\u4e2d\uff0c\u5e73\u8861\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\u800c\u975e\u4ee4\u724c\u6570\u91cf\u662f\u66f4\u6709\u6548\u7684\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\uff0cMETRO\u7b97\u6cd5\u901a\u8fc7\u8fd9\u4e00\u521b\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u5e76\u884cMoE\u670d\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u89e3\u7801\u9636\u6bb5\u3002"}}
{"id": "2512.09309", "categories": ["cs.DC", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09309", "abs": "https://arxiv.org/abs/2512.09309", "authors": ["Zihao Ding", "Mufeng Zhu", "Zhongze Tang", "Sheng Wei", "Yao Liu"], "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge", "comment": "16 pages, 7 figures. Published in the Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing (SEC '25), Dec 3-6, 2025, Washington, D.C., USA", "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5206\u5e03\u5f0f\u5206\u5c42\u5378\u8f7d\u6846\u67b6\uff0c\u7528\u4e8eVision Transformers\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u6570\u636e\u5206\u5272\u5230\u591a\u4e2a\u72ec\u7acb\u4e91\u670d\u52a1\u5668\u6765\u4fdd\u62a4\u9690\u79c1\uff0c\u9632\u6b62\u5355\u670d\u52a1\u5668\u91cd\u5efa\u5b8c\u6574\u56fe\u50cf\u3002", "motivation": "\u89c6\u89c9\u667a\u80fd\u5de5\u5177\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u8d85\u51fa\u79fb\u52a8\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u80fd\u529b\u3002\u4f20\u7edf\u4e91\u5378\u8f7d\u65b9\u6848\u5728\u4f20\u8f93\u548c\u670d\u52a1\u5668\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u6f0f\u6d1e\uff0c\u9700\u8981\u8bbe\u8ba1\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u672c\u5730\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u624b\u673a\u6216Nvidia Jetson\uff09\u4f5c\u4e3a\u8fb9\u7f18\u534f\u8c03\u5668\uff0c\u5c06\u7528\u6237\u89c6\u89c9\u6570\u636e\u5206\u5272\u6210\u5c0f\u90e8\u5206\uff0c\u5206\u53d1\u5230\u591a\u4e2a\u72ec\u7acb\u4e91\u670d\u52a1\u5668\u3002\u6700\u7ec8\u6570\u636e\u5408\u5e76\u548c\u805a\u5408\u8ba1\u7b97\u4ec5\u5728\u7528\u6237\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\uff0c\u786e\u4fdd\u6ca1\u6709\u5355\u4e2a\u5916\u90e8\u670d\u52a1\u5668\u62e5\u6709\u5b8c\u6574\u56fe\u50cf\u3002", "result": "\u4ee5Segment Anything Model (SAM)\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u6846\u67b6\u663e\u8457\u589e\u5f3a\u4e86\u5185\u5bb9\u9690\u79c1\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u57fa\u51c6\u7684\u5206\u5272\u6027\u80fd\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u5185\u5bb9\u91cd\u5efa\u548c\u7528\u6237\u6570\u636e\u66b4\u9732\u7684\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bbe\u8ba1\u9632\u6b62\u5355\u670d\u52a1\u5668\u6570\u636e\u91cd\u5efa\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u548c\u8ba1\u7b97\u6027\u80fd\u3002"}}
{"id": "2512.09331", "categories": ["cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09331", "abs": "https://arxiv.org/abs/2512.09331", "authors": ["Nam Anh Dang", "Ben Landrum", "Ken Birman"], "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN", "comment": "12 pages, 14 figures, submitted to VLDB 2026", "summary": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.", "AI": {"tldr": "BatANN\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u67e5\u8be2\u72b6\u6001\u5b8c\u6574\u4f20\u8f93\u5230\u6570\u636e\u6240\u5728\u673a\u5668\u6267\u884c\uff0c\u5728\u4fdd\u6301\u5355\u673a\u56fe\u5bf9\u6570\u641c\u7d22\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u8fd1\u7ebf\u6027\u541e\u5410\u6269\u5c55\u3002", "motivation": "\u968f\u7740\u6570\u636e\u96c6\u6269\u5c55\u5230\u6570\u5341\u4ebf\u5411\u91cf\uff0c\u78c1\u76d8\u5411\u91cf\u641c\u7d22\u6210\u4e3a\u5b9e\u7528\u65b9\u6848\uff0c\u4f46\u672a\u6765\u9700\u8981\u5904\u7406\u5355\u4e2a\u670d\u52a1\u5668\u65e0\u6cd5\u5bb9\u7eb3\u7684\u8d85\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6838\u5fc3\u521b\u65b0\u662f\u5f53\u8bbf\u95ee\u5b58\u50a8\u5728\u53e6\u4e00\u53f0\u673a\u5668\u4e0a\u7684\u90bb\u57df\u65f6\uff0c\u5c06\u67e5\u8be2\u7684\u5b8c\u6574\u72b6\u6001\u53d1\u9001\u5230\u8be5\u673a\u5668\u7ee7\u7eed\u6267\u884c\uff0c\u4ee5\u63d0\u9ad8\u5c40\u90e8\u6027\u3002\u7cfb\u7edf\u57fa\u4e8e\u6807\u51c6TCP\uff0c\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7ef4\u62a4\u5355\u4e00\u5168\u5c40\u56fe\u3002", "result": "\u57281\u4ebf\u548c10\u4ebf\u70b9\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u752810\u53f0\u670d\u52a1\u5668\u8fbe\u52300.95\u53ec\u56de\u7387\u65f6\uff0cBatANN\u5206\u522b\u8fbe\u5230\u57fa\u7ebf\u65b9\u6cd56.21-6.49\u500d\u548c2.5-5.10\u500d\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u5e73\u5747\u5ef6\u8fdf\u4f4e\u4e8e6\u6beb\u79d2\u3002", "conclusion": "BatANN\u662f\u9996\u4e2a\u5f00\u6e90\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u641c\u7d22\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u64cd\u4f5c\u5355\u4e00\u5168\u5c40\u56fe\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6570\u641c\u7d22\u6548\u7387\u548c\u8fd1\u7ebf\u6027\u541e\u5410\u6269\u5c55\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2512.09472", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09472", "abs": "https://arxiv.org/abs/2512.09472", "authors": ["Chiheng Lou", "Sheng Qi", "Rui Kang", "Yong Zhang", "Chen Sun", "Pengcheng Wang", "Bingyang Liu", "Xuanzhe Liu", "Xin Jin"], "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving", "comment": null, "summary": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.\n  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.", "AI": {"tldr": "WarmServe\u662f\u4e00\u4e2a\u591aLLM\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u901a\u7528GPU\u5de5\u4f5c\u5668\u548c\u9884\u77e5\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9884\u9884\u70ed\u6280\u672f\uff0c\u663e\u8457\u6539\u5584\u9996\u6b21\u4ee4\u724c\u65f6\u95f4(TTFT)\uff0c\u540c\u65f6\u63d0\u9ad8GPU\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u591aLLM\u670d\u52a1\u7cfb\u7edf\u5728\u63d0\u9ad8GPU\u5229\u7528\u7387\u7684\u540c\u65f6\u727a\u7272\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662f\u9996\u6b21\u4ee4\u724c\u65f6\u95f4(TTFT)\u3002\u6839\u672c\u539f\u56e0\u662f\u5b83\u4eec\u7f3a\u4e4f\u5bf9\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u7684\u8ba4\u77e5\uff0c\u800c\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5177\u6709\u9ad8\u5ea6\u5468\u671f\u6027\u548c\u957f\u671f\u53ef\u9884\u6d4b\u6027\u3002", "method": "\u63d0\u51fa\u901a\u7528GPU\u5de5\u4f5c\u5668\u5b9e\u73b0\u4e00\u5bf9\u591aGPU\u9884\u9884\u70ed\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1WarmServe\u7cfb\u7edf\uff1a1)\u91c7\u7528\u9a71\u9010\u611f\u77e5\u6a21\u578b\u653e\u7f6e\u7b56\u7565\u51cf\u5c11\u96c6\u7fa4\u7ea7\u9884\u9884\u70ed\u5e72\u6270\uff1b2)\u901a\u8fc7\u4e3b\u52a8\u9884\u9884\u70ed\u63d0\u524d\u51c6\u5907\u901a\u7528GPU\u5de5\u4f5c\u5668\uff1b3)\u4f7f\u7528\u96f6\u5f00\u9500\u5185\u5b58\u5207\u6362\u673a\u5236\u7ba1\u7406GPU\u5185\u5b58\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\uff0cWarmServe\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6269\u7f29\u7cfb\u7edf\u5c06TTFT\u63d0\u5347\u9ad8\u8fbe50.8\u500d\uff0c\u540c\u65f6\u76f8\u6bd4GPU\u5171\u4eab\u7cfb\u7edf\u80fd\u591f\u670d\u52a1\u591a\u8fbe2.5\u500d\u7684\u8bf7\u6c42\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5de5\u4f5c\u8d1f\u8f7d\u53ef\u9884\u6d4b\u6027\u548c\u901a\u7528GPU\u5de5\u4f5c\u5668\uff0cWarmServe\u5728\u4fdd\u6301\u9ad8GPU\u5229\u7528\u7387\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u591aLLM\u670d\u52a1\u7684\u63a8\u7406\u6027\u80fd\uff0c\u7279\u522b\u662fTTFT\u6307\u6807\u3002"}}
{"id": "2512.09006", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09006", "abs": "https://arxiv.org/abs/2512.09006", "authors": ["Dyna Soumhane Ouchebara", "St\u00e9phane Dupont"], "title": "Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning", "comment": "20 pages, Accepted at ESORICS 2025", "summary": "The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4f7f\u7528Llama-3.1 8B\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u5fae\u8c03\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u53d1\u73b0\u5fae\u8c03\u5bf9\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4e2d\u63d0\u51fa\u7684\u53cc\u91cd\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u52a0\u901f\uff0c\u8f6f\u4ef6\u6f0f\u6d1e\u6570\u91cf\u6301\u7eed\u589e\u52a0\uff0c\u81ea\u52a8\u5316\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f53\u524d\u6027\u80fd\u6700\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5e94\u7528\u5404\u79cd\u5148\u8fdb\u6280\u672f\u63d0\u5347\u5176\u6548\u679c\u3002", "method": "\u4f7f\u7528Llama-3.1 8B\u5f00\u6e90\u6a21\u578b\uff0c\u4eceBigVul\u548cPrimeVul\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u6e90\u4ee3\u7801\u6837\u672c\u3002\u63a2\u7d22\u4e86\u591a\u79cd\u5fae\u8c03\u8bbe\u7f6e\uff08\u5305\u62ec\u63d0\u51fa\u7684\u53cc\u91cd\u5fae\u8c03\u65b9\u6cd5\u548c\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff09\u548c\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u4ee5\u53ca\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4f5c\u4e3a\u793a\u4f8b\u9009\u62e9\u6280\u672f\u3002", "result": "\u5fae\u8c03\u5bf9\u89e3\u51b3\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff1b\u63d0\u51fa\u7684\u53cc\u91cd\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u826f\u597d\uff1bLlama\u6a21\u578b\u5728\u6f0f\u6d1e\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff1b\u63d0\u793a\u5de5\u7a0b\u6548\u679c\u4e0d\u4f73\uff1b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4f5c\u4e3a\u793a\u4f8b\u9009\u62e9\u6280\u672f\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\u3002", "conclusion": "\u7814\u7a76\u90e8\u5206\u56de\u7b54\u4e86\u7814\u7a76\u95ee\u9898\uff0c\u4f46\u4ecd\u6709\u8bb8\u591a\u95ee\u9898\u6709\u5f85\u89e3\u51b3\uff0c\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u591a\u4e2a\u65b9\u5411\u3002\u5927\u8bed\u8a00\u6a21\u578b\u7279\u522b\u662f\u7ecf\u8fc7\u9002\u5f53\u5fae\u8c03\u7684Llama\u6a21\u578b\u5728\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.09622", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09622", "abs": "https://arxiv.org/abs/2512.09622", "authors": ["Xiao Yan", "Tiezheng Nie", "Boyang Fang", "Derong Shen", "Kou Yue", "Yu Ge"], "title": "CUBE: A Cardinality Estimator Based on Neural CDF", "comment": "13 pages", "summary": "Modern database optimizer relies on cardinality estimator, whose accuracy directly affects the optimizer's ability to choose an optimal execution plan. Recent work on data-driven methods has leveraged probabilistic models to achieve higher estimation accuracy, but these approaches cannot guarantee low inference latency at the same time and neglect scalability. As data dimensionality grows, optimization time can even exceed actual query execution time. Furthermore, inference with probabilistic models by sampling or integration procedures unpredictable estimation result and violate stability, which brings unstable performance with query execution and make database tuning hard for database users. In this paper, we propose a novel approach to cardinality estimation based on cumulative distribution function(CDF), which calculates range query cardinality without sampling or integration, ensuring accurate and predictable estimation results. With inference acceleration by merging calculations, we can achieve fast and nearly constant inference speed while maintaining high accuracy, even as dimensionality increases, which is over 10x faster than current state-of-the-art data-driven cardinality estimator. This demonstrates its excellent dimensional scalability, making it well-suited for real-world database applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7d2f\u79ef\u5206\u5e03\u51fd\u6570(CDF)\u7684\u65b0\u578b\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u65e0\u9700\u91c7\u6837\u6216\u79ef\u5206\u5373\u53ef\u8ba1\u7b97\u8303\u56f4\u67e5\u8be2\u57fa\u6570\uff0c\u5b9e\u73b0\u51c6\u786e\u3001\u53ef\u9884\u6d4b\u7684\u4f30\u8ba1\u7ed3\u679c\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5feb10\u500d\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6982\u7387\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548c\u53ef\u6269\u5c55\u6027\uff0c\u968f\u7740\u6570\u636e\u7ef4\u5ea6\u589e\u52a0\uff0c\u4f18\u5316\u65f6\u95f4\u53ef\u80fd\u8d85\u8fc7\u5b9e\u9645\u67e5\u8be2\u6267\u884c\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u91c7\u6837\u6216\u79ef\u5206\u7684\u6982\u7387\u6a21\u578b\u63a8\u7406\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u4f30\u8ba1\u7ed3\u679c\uff0c\u8fdd\u53cd\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u67e5\u8be2\u6267\u884c\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u7ed9\u6570\u636e\u5e93\u8c03\u4f18\u5e26\u6765\u56f0\u96be\u3002", "method": "\u57fa\u4e8e\u7d2f\u79ef\u5206\u5e03\u51fd\u6570(CDF)\u7684\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u5e76\u8ba1\u7b97\u5b9e\u73b0\u63a8\u7406\u52a0\u901f\uff0c\u65e0\u9700\u91c7\u6837\u6216\u79ef\u5206\u5373\u53ef\u8ba1\u7b97\u8303\u56f4\u67e5\u8be2\u57fa\u6570\uff0c\u786e\u4fdd\u4f30\u8ba1\u7ed3\u679c\u51c6\u786e\u4e14\u53ef\u9884\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u63a5\u8fd1\u6052\u5b9a\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\uff0c\u5373\u4f7f\u7ef4\u5ea6\u589e\u52a0\u4e5f\u80fd\u4fdd\u6301\u6027\u80fd\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6570\u636e\u9a71\u52a8\u57fa\u6570\u4f30\u8ba1\u5668\u5feb10\u500d\u4ee5\u4e0a\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u7ef4\u5ea6\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u57fa\u4e8eCDF\u7684\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u9002\u5408\u5b9e\u9645\u6570\u636e\u5e93\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u6570\u636e\u5e93\u4f18\u5316\u5668\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u6570\u4f30\u8ba1\u65b9\u6848\u3002"}}
{"id": "2512.09502", "categories": ["cs.DC", "cs.NE", "physics.comp-ph", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.09502", "abs": "https://arxiv.org/abs/2512.09502", "authors": ["Bruno Golosio", "Gianmarco Tiddia", "Jos\u00e9 Villamar", "Luca Pontisso", "Luca Sergi", "Francesco Simula", "Pooja Babu", "Elena Pastorelli", "Abigail Morrison", "Markus Diesmann", "Alessandro Lonardo", "Pier Stanislao Paolucci", "Johanna Senk"], "title": "Scalable Construction of Spiking Neural Networks using up to thousands of GPUs", "comment": null, "summary": "Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\\mathcal{O}(10^{10})$ neurons, each forming $\\mathcal{O}(10^{3})$--$\\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u591aGPU\u96c6\u7fa4\u548c\u672a\u6765\u767e\u4ebf\u4ebf\u6b21\u8d85\u7ea7\u8ba1\u7b97\u673a\u7684MPI\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df", "motivation": "\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u5728\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u8981\u9ad8\u6548\u7ba1\u7406\u901a\u4fe1\u548c\u5185\u5b58\u3002\u4eba\u8111\u76ae\u5c42\u5305\u542b\u7ea610^10\u4e2a\u795e\u7ecf\u5143\uff0c\u6bcf\u4e2a\u5f62\u621010^3-10^4\u4e2a\u7a81\u89e6\uff0c\u6a21\u62df\u8fd9\u79cd\u590d\u6742\u7cfb\u7edf\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6d88\u606f\u4f20\u9012\u63a5\u53e3(MPI)\u5f00\u53d1\u65b0\u9896\u7684\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u8fdb\u7a0b\u6784\u5efa\u672c\u5730\u8fde\u63a5\u6027\uff0c\u5e76\u51c6\u5907\u6570\u636e\u7ed3\u6784\u4ee5\u5728\u72b6\u6001\u4f20\u64ad\u671f\u95f4\u5b9e\u73b0\u96c6\u7fa4\u5185\u9ad8\u6548\u8109\u51b2\u4ea4\u6362\u3002\u6f14\u793a\u4e86\u70b9\u5bf9\u70b9\u548c\u96c6\u4f53\u901a\u4fe1\u4e24\u79cd\u76ae\u5c42\u6a21\u578b\u7684\u6269\u5c55\u6027\u80fd\u3002", "result": "\u5c55\u793a\u4e86\u4e24\u79cd\u76ae\u5c42\u6a21\u578b\u4f7f\u7528\u70b9\u5bf9\u70b9\u548c\u96c6\u4f53\u901a\u4fe1\u7684\u6269\u5c55\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591aGPU\u96c6\u7fa4\u548c\u672a\u6765\u767e\u4ebf\u4ebf\u6b21\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7f51\u7edc\u6784\u5efa\u548c\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2512.09108", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09108", "abs": "https://arxiv.org/abs/2512.09108", "authors": ["Paul Brookes", "Vardan Voskanyan", "Rafail Giavrimis", "Matthew Truscott", "Mina Ilieva", "Chrystalla Pavlou", "Alexandru Staicu", "Manal Adham", "Will Evers- Hood", "Jingzhi Gong", "Kejia Zhang", "Matvey Fedoseev", "Vishal Sharma", "Roman Bauer", "Zheng Wang", "Hema Nair", "Wei Jie", "Tianhua Xu", "Aurora Constantin", "Leslie Kanthan", "Michail Basios"], "title": "Evolving Excellence: Automated Optimization of LLM-based Agents", "comment": null, "summary": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.\n  We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.\n  We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.", "AI": {"tldr": "ARTEMIS\u662f\u4e00\u4e2a\u65e0\u9700\u4ee3\u7801\u7684\u8fdb\u5316\u4f18\u5316\u5e73\u53f0\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u7684\u9057\u4f20\u7b97\u5b50\u8054\u5408\u4f18\u5316AI\u4ee3\u7406\u914d\u7f6e\uff0c\u5728\u591a\u4e2a\u4ee3\u7406\u7cfb\u7edf\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u4ee3\u7406\u7cfb\u7edf\u5728\u81ea\u52a8\u5316\u590d\u6742\u5de5\u4f5c\u6d41\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u7531\u4e8e\u63d0\u793a\u8bcd\u3001\u5de5\u5177\u63cf\u8ff0\u548c\u53c2\u6570\u7b49\u914d\u7f6e\u9700\u8981\u6570\u5468\u624b\u52a8\u8c03\u4f18\uff0c\u5f80\u5f80\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u590d\u6742\uff0c\u8981\u4e48\u5b64\u7acb\u5904\u7406\u7ec4\u4ef6\uff0c\u5ffd\u7565\u4e86\u5173\u952e\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "ARTEMIS\u662f\u4e00\u4e2a\u65e0\u9700\u4ee3\u7801\u7684\u8fdb\u5316\u4f18\u5316\u5e73\u53f0\uff0c\u4f7f\u7528\u8bed\u4e49\u611f\u77e5\u7684\u9057\u4f20\u7b97\u5b50\u8054\u5408\u4f18\u5316\u4ee3\u7406\u914d\u7f6e\u3002\u7ed9\u5b9a\u57fa\u51c6\u811a\u672c\u548c\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u540e\uff0c\u5b83\u80fd\u81ea\u52a8\u53d1\u73b0\u53ef\u914d\u7f6e\u7ec4\u4ef6\uff0c\u4ece\u6267\u884c\u65e5\u5fd7\u4e2d\u63d0\u53d6\u6027\u80fd\u4fe1\u53f7\uff0c\u5e76\u5728\u65e0\u9700\u67b6\u6784\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\u8fdb\u5316\u914d\u7f6e\u3002", "result": "\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u4ee3\u7406\u7cfb\u7edf\u4e0a\u8bc4\u4f30\uff1a1) ALE\u4ee3\u7406\u5728AtCoder Heuristic\u7ade\u8d5b\u4e2d\u63a5\u53d7\u7387\u63d0\u534713.6%\uff1b2) Mini-SWE\u4ee3\u7406\u5728SWE-Perf\u4ee3\u7801\u4f18\u5316\u4e0a\u83b7\u5f9710.1%\u6027\u80fd\u63d0\u5347\uff1b3) CrewAI\u4ee3\u7406\u5728Math Odyssey\u6570\u5b66\u63a8\u7406\u4e0a\u51cf\u5c1136.9%\u7684token\u4f7f\u7528\u91cf\uff1b4) \u57fa\u4e8eQwen2.5-7B\u5c0f\u6a21\u578b\u7684MathTales-Teacher\u4ee3\u7406\u5728GSM8K\u6570\u5b66\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u63d0\u534722%\u3002", "conclusion": "ARTEMIS\u80fd\u591f\u6709\u6548\u4f18\u5316\u57fa\u4e8e\u5546\u4e1a\u548c\u672c\u5730\u6a21\u578b\u7684AI\u4ee3\u7406\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u4f18\u6216\u67b6\u6784\u4fee\u6539\uff0c\u4e3aAI\u4ee3\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09695", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09695", "abs": "https://arxiv.org/abs/2512.09695", "authors": ["Hyunjoon Kim", "Chaerim Lim", "Hyeonjun An", "Rathijit Sen", "Kwanghyun Park"], "title": "Exqutor: Extended Query Optimizer for Vector-augmented Analytical Queries", "comment": null, "summary": "Vector similarity search is becoming increasingly important for data science pipelines, particularly in Retrieval-Augmented Generation (RAG), where it enhances large language model inference by enabling efficient retrieval of relevant external knowledge. As RAG expands with table-augmented generation to incorporate structured data, workloads integrating table and vector search are becoming more prevalent. However, efficiently executing such queries remains challenging due to inaccurate cardinality estimation for vector search components, leading to suboptimal query plans. In this paper, we propose Exqutor, an extended query optimizer for vector-augmented analytical queries. Exqutor is a pluggable cardinality estimation framework designed to address this issue, leveraging exact cardinality query optimization techniques to enhance estimation accuracy when vector indexes (e.g., HNSW, IVF) are available. In scenarios lacking these indexes, we employ a sampling-based approach with adaptive sampling size adjustment, dynamically tuning the sample size to balance estimation accuracy and sampling overhead. This allows Exqutor to efficiently approximate vector search cardinalities while minimizing computational costs. We integrate our framework into pgvector, VBASE, and DuckDB, demonstrating performance improvements of up to four orders of magnitude on vector-augmented analytical queries.", "AI": {"tldr": "Exqutor\u662f\u4e00\u4e2a\u53ef\u63d2\u62d4\u7684\u57fa\u6570\u4f30\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5411\u91cf\u589e\u5f3a\u5206\u6790\u67e5\u8be2\uff0c\u901a\u8fc7\u7cbe\u786e\u57fa\u6570\u67e5\u8be2\u4f18\u5316\u548c\u81ea\u9002\u5e94\u91c7\u6837\u6280\u672f\u63d0\u9ad8\u5411\u91cf\u641c\u7d22\u57fa\u6570\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u5728pgvector\u3001VBASE\u548cDuckDB\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe4\u4e2a\u6570\u91cf\u7ea7\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u968f\u7740\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6269\u5c55\u5230\u8868\u683c\u589e\u5f3a\u751f\u6210\u4ee5\u6574\u5408\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7ed3\u5408\u8868\u683c\u548c\u5411\u91cf\u641c\u7d22\u7684\u5de5\u4f5c\u8d1f\u8f7d\u65e5\u76ca\u666e\u904d\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5411\u91cf\u641c\u7d22\u7ec4\u4ef6\u7684\u57fa\u6570\u4f30\u8ba1\u4e0d\u51c6\u786e\u5bfc\u81f4\u67e5\u8be2\u8ba1\u5212\u6b21\u4f18\uff0c\u9ad8\u6548\u6267\u884c\u6b64\u7c7b\u67e5\u8be2\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faExqutor\u6269\u5c55\u67e5\u8be2\u4f18\u5316\u5668\uff0c\u91c7\u7528\u53ef\u63d2\u62d4\u7684\u57fa\u6570\u4f30\u8ba1\u6846\u67b6\uff1a\u5f53\u5411\u91cf\u7d22\u5f15\uff08\u5982HNSW\u3001IVF\uff09\u53ef\u7528\u65f6\uff0c\u5229\u7528\u7cbe\u786e\u57fa\u6570\u67e5\u8be2\u4f18\u5316\u6280\u672f\uff1b\u5f53\u7f3a\u4e4f\u7d22\u5f15\u65f6\uff0c\u91c7\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u5927\u5c0f\u8c03\u6574\u52a8\u6001\u5e73\u8861\u4f30\u8ba1\u7cbe\u5ea6\u548c\u91c7\u6837\u5f00\u9500\u3002", "result": "\u5c06Exqutor\u96c6\u6210\u5230pgvector\u3001VBASE\u548cDuckDB\u4e2d\uff0c\u5728\u5411\u91cf\u589e\u5f3a\u5206\u6790\u67e5\u8be2\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe\u56db\u4e2a\u6570\u91cf\u7ea7\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "Exqutor\u901a\u8fc7\u63d0\u9ad8\u5411\u91cf\u641c\u7d22\u57fa\u6570\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5411\u91cf\u589e\u5f3a\u5206\u6790\u67e5\u8be2\u7684\u4f18\u5316\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u6027\u80fd\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u7ba1\u9053\u4e2d\u7684RAG\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u67e5\u8be2\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09568", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09568", "abs": "https://arxiv.org/abs/2512.09568", "authors": ["Zhi Zhao", "Hang Xiao", "Wei Rang"], "title": "PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing", "comment": "24 pages,5 figures", "summary": "Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e15\u7d2f\u6258\u7684\u6df7\u5408\u9cb8\u9c7c-\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5(PHWSOA)\uff0c\u7528\u4e8e\u4e91\u4efb\u52a1\u8c03\u5ea6\uff0c\u540c\u65f6\u4f18\u5316\u5b8c\u5de5\u65f6\u95f4\u3001\u865a\u62df\u673a\u8d1f\u8f7d\u5747\u8861\u548c\u7ecf\u6d4e\u6210\u672c\u4e09\u4e2a\u76ee\u6807\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u4e91\u4efb\u52a1\u8c03\u5ea6\u65b9\u6848\u5927\u591a\u53ea\u4f18\u5316\u5355\u4e00\u6216\u6709\u9650\u6307\u6807\uff08\u5982\u6267\u884c\u65f6\u95f4\u6216\u8d44\u6e90\u5229\u7528\u7387\uff09\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u5b8c\u5de5\u65f6\u95f4\u3001\u8d1f\u8f7d\u5747\u8861\u548c\u7ecf\u6d4e\u6210\u672c\u7b49\u591a\u4e2a\u5173\u952e\u76ee\u6807\u3002", "method": "\u63d0\u51faPHWSOA\u7b97\u6cd5\uff0c\u878d\u5408\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5(WOA)\u548c\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5(SOA)\u7684\u4f18\u52bf\uff0c\u91c7\u7528\u5e15\u7d2f\u6258\u652f\u914d\u539f\u5219\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\u3002\u5173\u952e\u6539\u8fdb\u5305\u62ec\uff1aHalton\u5e8f\u5217\u521d\u59cb\u5316\u589e\u5f3a\u79cd\u7fa4\u591a\u6837\u6027\u3001\u5e15\u7d2f\u6258\u5f15\u5bfc\u7684\u53d8\u5f02\u673a\u5236\u9632\u6b62\u65e9\u719f\u6536\u655b\u3001\u5e76\u884c\u5904\u7406\u52a0\u901f\u6536\u655b\u3001\u52a8\u6001\u865a\u62df\u673a\u8d1f\u8f7d\u91cd\u5206\u914d\u673a\u5236\u6539\u5584\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u5728CloudSim\u6a21\u62df\u5668\u4e0a\u4f7f\u7528NASA-iPSC\u548cHPC2N\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u5b9e\u9a8c\uff0cPHWSOA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08WOA\u3001GA\u3001PEWOA\u3001GCWOA\uff09\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1a\u5b8c\u5de5\u65f6\u95f4\u6700\u591a\u51cf\u5c1172.1%\uff0c\u865a\u62df\u673a\u8d1f\u8f7d\u5747\u8861\u63d0\u534736.8%\uff0c\u6210\u672c\u8282\u7ea623.5%\u3002", "conclusion": "PHWSOA\u7b97\u6cd5\u901a\u8fc7\u878d\u5408WOA\u548cSOA\u7684\u4f18\u52bf\uff0c\u7ed3\u5408\u5e15\u7d2f\u6258\u591a\u76ee\u6807\u4f18\u5316\u548c\u591a\u79cd\u6539\u8fdb\u673a\u5236\uff0c\u5728\u4e91\u4efb\u52a1\u8c03\u5ea6\u4e2d\u5b9e\u73b0\u4e86\u5168\u9762\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u5b9e\u9645\u4e91\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09196", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09196", "abs": "https://arxiv.org/abs/2512.09196", "authors": ["Haonan Li", "Keyu Man", "Partha Kanuparthy", "Hanning Chen", "Wei Sun", "Sreen Tallam", "Chenguang Zhu", "Kevin Zhu", "Zhiyun Qian"], "title": "TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization", "comment": null, "summary": "High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.", "AI": {"tldr": "TritonForge\u662f\u4e00\u4e2a\u57fa\u4e8e\u6027\u80fd\u5256\u6790\u7684\u81ea\u52a8\u5316Triton GPU\u5185\u6838\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5185\u6838\u5206\u6790\u3001\u8fd0\u884c\u65f6\u5256\u6790\u548c\u8fed\u4ee3\u4ee3\u7801\u8f6c\u6362\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe5\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1Triton DSL\u7b80\u5316\u4e86GPU\u5185\u6838\u5f00\u53d1\uff0c\u4f46\u8981\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6027\u80fd\u4ecd\u9700\u6df1\u5165\u4e86\u89e3GPU\u67b6\u6784\u548c\u5e95\u5c42\u6027\u80fd\u6743\u8861\uff0c\u8fd9\u662f\u4e00\u4e2a\u52b3\u52a8\u5bc6\u96c6\u578b\u4efb\u52a1\u3002\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u964d\u4f4e\u4f18\u5316\u95e8\u69db\u3002", "method": "TritonForge\u91c7\u7528\u5256\u6790\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u96c6\u6210\u5185\u6838\u5206\u6790\u3001\u8fd0\u884c\u65f6\u5256\u6790\u548c\u8fed\u4ee3\u4ee3\u7801\u8f6c\u6362\u3002\u7cfb\u7edf\u5229\u7528\u5256\u6790\u7ed3\u679c\u7684\u6570\u636e\u9a71\u52a8\u53cd\u9988\uff0c\u8bc6\u522b\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u51fa\u9488\u5bf9\u6027\u4ee3\u7801\u4fee\u6539\u5e76\u81ea\u52a8\u8bc4\u4f30\u5176\u5f71\u54cd\u3002\u539f\u578b\u7cfb\u7edf\u4f7f\u7528LLM\u8f85\u52a9\u4ee3\u7801\u63a8\u7406\u548c\u8f6c\u6362\uff0c\u4f46\u6846\u67b6\u4fdd\u6301\u6a21\u5757\u5316\u548c\u6a21\u578b\u65e0\u5173\u6027\u3002", "result": "\u5728\u4e0d\u540c\u5185\u6838\u7c7b\u578b\u548cGPU\u67b6\u6784\u4e0a\uff0cTritonForge\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u5b9e\u73b0\u4e86\u9ad8\u8fbe5\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e73\u5747\u6210\u529f\u7387\u4e3a1.76\u500d\uff08\u537376%\u7684\u6027\u80fd\u6539\u8fdb\uff09\u3002", "conclusion": "TritonForge\u4e3a\u81ea\u52a8\u5316GPU\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5256\u6790\u5f15\u5bfc\u65b9\u6cd5\u5728\u964d\u4f4eGPU\u5185\u6838\u4f18\u5316\u95e8\u69db\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.09762", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09762", "abs": "https://arxiv.org/abs/2512.09762", "authors": ["Jonathan Edwards", "Tomas Petricek"], "title": "Baseline: Operation-Based Evolution and Versioning of Data", "comment": "Submitted to The Art, Science, and Engineering of Programming", "summary": "Baseline is a platform for richly structured data supporting change in multiple dimensions: mutation over time, collaboration across space, and evolution through design changes. It is built upon Operational Differencing, a new technique for managing data in terms of high-level operations that include refactorings and schema changes. We use operational differencing to construct an operation-based form of version control on data structures used in programming languages and relational databases.\n  This approach to data version control does fine-grained diffing and merging despite intervening structural transformations like schema changes. It offers users a simplified conceptual model of version control for ad hoc usage: There is no repo; Branching is just copying. The informaton maintained in a repo can be synthesized more precisely from the append-only histories of branches. Branches can be flexibly shared as is commonly done with document files, except with the added benefit of diffing and merging.\n  We conjecture that queries can be operationalized into a sequence of schema and data operations. We develop that idea on a query language fragment containing selects and joins.\n  Operationalized queries are represented as a future timeline that is speculatively executed as a branch off of the present state, returning a value from its hypothetical future. Operationalized queries get rewritten to accommodate schema change \"for free\" by the machinery of operational differencing.\n  Altogether we develop solutions to four of the eight challenge problems of schema evolution identified in a recent paper.", "AI": {"tldr": "Baseline\u662f\u4e00\u4e2a\u652f\u6301\u591a\u7ef4\u5ea6\u53d8\u66f4\u7684\u6570\u636e\u5e73\u53f0\uff0c\u91c7\u7528\u64cd\u4f5c\u5dee\u5206\u6280\u672f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u7248\u672c\u63a7\u5236\uff0c\u80fd\u591f\u5904\u7406\u6a21\u5f0f\u53d8\u66f4\u7b49\u7ed3\u6784\u8f6c\u6362\uff0c\u5e76\u53ef\u5c06\u67e5\u8be2\u64cd\u4f5c\u5316\u4e3a\u65f6\u95f4\u7ebf\u8fdb\u884c\u63a8\u6d4b\u6267\u884c\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u7248\u672c\u63a7\u5236\u96be\u4ee5\u5904\u7406\u6a21\u5f0f\u53d8\u66f4\u7b49\u7ed3\u6784\u8f6c\u6362\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u652f\u6301\u65f6\u95f4\u7ef4\u5ea6\u3001\u534f\u4f5c\u7ef4\u5ea6\u548c\u8bbe\u8ba1\u7ef4\u5ea6\u53d8\u66f4\u7684\u6570\u636e\u7ba1\u7406\u5e73\u53f0\u3002", "method": "\u57fa\u4e8e\u64cd\u4f5c\u5dee\u5206\u6280\u672f\uff0c\u5c06\u6570\u636e\u7ba1\u7406\u4e3a\u5305\u542b\u91cd\u6784\u548c\u6a21\u5f0f\u53d8\u66f4\u7684\u9ad8\u7ea7\u64cd\u4f5c\u5e8f\u5217\uff0c\u5b9e\u73b0\u64cd\u4f5c\u5f0f\u7248\u672c\u63a7\u5236\uff0c\u5e76\u5c06\u67e5\u8be2\u64cd\u4f5c\u5316\u4e3a\u65f6\u95f4\u7ebf\u8fdb\u884c\u63a8\u6d4b\u6267\u884c\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u591f\u5904\u7406\u6a21\u5f0f\u53d8\u66f4\u7684\u7ec6\u7c92\u5ea6\u5dee\u5f02\u6bd4\u8f83\u548c\u5408\u5e76\u673a\u5236\uff0c\u7b80\u5316\u4e86\u7248\u672c\u63a7\u5236\u6982\u5ff5\u6a21\u578b\uff08\u65e0\u4ed3\u5e93\u3001\u5206\u652f\u5373\u590d\u5236\uff09\uff0c\u5e76\u89e3\u51b3\u4e86\u6700\u8fd1\u4e00\u7bc7\u8bba\u6587\u4e2d\u63d0\u51fa\u7684\u516b\u4e2a\u6a21\u5f0f\u6f14\u5316\u6311\u6218\u95ee\u9898\u4e2d\u7684\u56db\u4e2a\u3002", "conclusion": "\u64cd\u4f5c\u5dee\u5206\u6280\u672f\u4e3a\u6570\u636e\u7248\u672c\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u7ed3\u6784\u8f6c\u6362\uff0c\u7b80\u5316\u7528\u6237\u6982\u5ff5\u6a21\u578b\uff0c\u5e76\u4e3a\u67e5\u8be2\u91cd\u5199\u4ee5\u9002\u5e94\u6a21\u5f0f\u53d8\u66f4\u63d0\u4f9b\u4e86\"\u514d\u8d39\"\u652f\u6301\u3002"}}
{"id": "2512.09664", "categories": ["cs.DC", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09664", "abs": "https://arxiv.org/abs/2512.09664", "authors": ["Antonio Terpin", "Alan Bonomi", "Francesco Banelli", "Raffaello D'Andrea"], "title": "SynthPix: A lightspeed PIV images generator", "comment": "Code: https://github.com/antonioterpin/synthpix", "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.", "AI": {"tldr": "SynthPix\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u5b9e\u73b0\u7684\u9ad8\u6027\u80fd\u5e76\u884c\u5408\u6210\u56fe\u50cf\u751f\u6210\u5668\uff0c\u4e13\u95e8\u7528\u4e8e\u7c92\u5b50\u56fe\u50cf\u6d4b\u901f\uff08PIV\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u5b9e\u73b0\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u5f00\u53d1SynthPix\u7684\u4e3b\u8981\u52a8\u673a\u662f\u4e3a\u4e86\u652f\u6301\u6570\u636e\u5bc6\u96c6\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6d41\u573a\u4f30\u8ba1\u4e2d\u7684\u8bad\u7ec3\uff0c\u5e76\u7f29\u77ed\u5feb\u901f\u6d41\u573a\u4f30\u8ba1\u65b9\u6cd5\u7684\u5f00\u53d1\u8fed\u4ee3\u65f6\u95f4\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5b9e\u65f6PIV\u53cd\u9988\u7684\u4e3b\u52a8\u6d41\u4f53\u63a7\u5236\u7814\u7a76\u4e2d\u3002", "method": "SynthPix\u91c7\u7528JAX\u6846\u67b6\u5b9e\u73b0\uff0c\u4e13\u6ce8\u4e8e\u5728\u52a0\u901f\u5668\uff08\u5982GPU\uff09\u4e0a\u7684\u6027\u80fd\u548c\u5e76\u884c\u5316\u3002\u5b83\u652f\u6301\u4e0e\u73b0\u6709\u5de5\u5177\u76f8\u540c\u7684\u914d\u7f6e\u53c2\u6570\uff0c\u4f46\u901a\u8fc7\u5e76\u884c\u5316\u67b6\u6784\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u56fe\u50cf\u751f\u6210\u6548\u7387\u3002", "result": "SynthPix\u5728\u6bcf\u79d2\u56fe\u50cf\u5bf9\u751f\u6210\u541e\u5410\u91cf\u65b9\u9762\u6bd4\u73b0\u6709\u5de5\u5177\u9ad8\u51fa\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86PIV\u5408\u6210\u56fe\u50cf\u7684\u751f\u6210\u6548\u7387\u3002", "conclusion": "SynthPix\u5bf9\u6d41\u4f53\u52a8\u529b\u5b66\u793e\u533a\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u80fd\u591f\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u8bad\u7ec3\u548c\u5feb\u901f\u6d41\u573a\u4f30\u8ba1\u65b9\u6cd5\u7684\u5f00\u53d1\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5b9e\u65f6PIV\u53cd\u9988\u7684\u4e3b\u52a8\u6d41\u4f53\u63a7\u5236\u7814\u7a76\u4e2d\u3002"}}
{"id": "2512.09216", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09216", "abs": "https://arxiv.org/abs/2512.09216", "authors": ["Guangzong Cai", "Zengyang Li", "Peng Liang", "Ran Mo", "Hui Liu", "Yutao Ma"], "title": "Bug Priority Change Prediction: An Exploratory Study on Apache Software", "comment": "Preprint accepted for publication in ACM Transactions on Software Engineering and Methodology (TOSEM), 2025", "summary": "Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7f3a\u9677\u4fee\u590d\u6f14\u5316\u7279\u5f81\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u7684\u4e24\u9636\u6bb5\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728Apache\u9879\u76ee\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u7f3a\u9677\u4f18\u5148\u7ea7\u5728\u4fee\u590d\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u53d8\u5316\uff0c\u4f46\u4eba\u5de5\u8bc4\u4f30\u4f9d\u8d56\u4e3b\u89c2\u5224\u65ad\u4e14\u7e41\u7410\uff0c\u5bb9\u6613\u5bfc\u81f4\u9519\u8bef\u53d8\u66f4\uff0c\u5f71\u54cd\u53ca\u65f6\u4fee\u590d\u3002\u76ee\u524d\u7f3a\u4e4f\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u7684\u7814\u7a76\u3002", "method": "\u5c06\u7f3a\u9677\u751f\u547d\u5468\u671f\u5206\u4e3a\u62a5\u544a\u9636\u6bb5\u548c\u4fee\u590d\u9636\u6bb5\uff0c\u4e3a\u6bcf\u4e2a\u9636\u6bb5\u6784\u5efa\u9884\u6d4b\u6a21\u578b\u3002\u4f7f\u7528\u7f3a\u9677\u4fee\u590d\u6f14\u5316\u7279\u5f81\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u572832\u4e2aApache\u9879\u76ee\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u62a5\u544a\u9636\u6bb5\u6a21\u578bF1-score\u8fbe0.798\uff0c\u4fee\u590d\u9636\u6bb5\u6a21\u578bF1-weighted\u4e3a0.712\uff0cF1-macro\u4e3a0.613\u3002\u8de8\u9879\u76ee\u9002\u7528\u6027\u5206\u6790\u663e\u793a\u6a21\u578b\u6027\u80fd\u5728\u4e0d\u540c\u9879\u76ee\u95f4\u6709\u8f83\u5927\u5dee\u5f02\uff0c\u4f46\u603b\u4f53\u8868\u73b0\u826f\u597d\uff0c\u5404\u4f18\u5148\u7ea7\u6c34\u5e73\u7684\u9884\u6d4b\u6027\u80fd\u76f8\u5bf9\u4e00\u81f4\u4e14\u8f83\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u7f3a\u9677\u4fee\u590d\u6f14\u5316\u7279\u5f81\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u7f3a\u9677\u7ba1\u7406\u63d0\u4f9b\u4e86\u81ea\u52a8\u5316\u652f\u6301\u5de5\u5177\u3002"}}
{"id": "2512.09836", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09836", "abs": "https://arxiv.org/abs/2512.09836", "authors": ["Bernhard St\u00f6ckl", "Maximilian E. Sch\u00fcle"], "title": "Fast Factorized Learning: Powered by In-Memory Database Systems", "comment": null, "summary": "Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u56e0\u5b50\u5316\u5b66\u4e60\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u76f8\u6bd4\u975e\u56e0\u5b50\u5316\u5b66\u4e60\u6027\u80fd\u63d0\u534770%\uff0c\u76f8\u6bd4\u78c1\u76d8\u6570\u636e\u5e93\u7cfb\u7edf\u63d0\u5347100\u500d", "motivation": "\u5148\u524d\u7814\u7a76\u5728\u4f20\u7edf\u78c1\u76d8\u6570\u636e\u5e93\u7cfb\u7edf\u4e0a\u63a2\u7d22\u4e86\u56e0\u5b50\u5316\u5b66\u4e60\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u516c\u5f00\u4ee3\u7801\uff0c\u65e0\u6cd5\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u4e0a\u590d\u73b0\u5b9e\u9a8c\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5b9e\u73b0\u56e0\u5b50\u5316\u5b66\u4e60\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5b9e\u73b0\u4e86\u56e0\u5b50\u5316\u5b66\u4e60\u7684\u5f00\u6e90\u5b9e\u73b0\uff0c\u4f7f\u7528PostgreSQL\u4f5c\u4e3a\u78c1\u76d8\u6570\u636e\u5e93\u7cfb\u7edf\u548cHyPer\u4f5c\u4e3a\u5185\u5b58\u5f15\u64ce\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u7ebf\u6027\u56de\u5f52\u5728\u56e0\u5b50\u5316\u8fde\u63a5\u4e0a\u7684\u5b66\u4e60\u6027\u80fd\u3002", "result": "\u56e0\u5b50\u5316\u5b66\u4e60\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u4e0a\u76f8\u6bd4\u975e\u56e0\u5b50\u5316\u5b66\u4e60\u6027\u80fd\u63d0\u534770%\uff0c\u76f8\u6bd4\u78c1\u76d8\u6570\u636e\u5e93\u7cfb\u7edf\u63d0\u5347100\u500d\u3002\u73b0\u4ee3\u6570\u636e\u5e93\u5f15\u64ce\u53ef\u4ee5\u901a\u8fc7\u5728\u6570\u636e\u63d0\u53d6\u524d\u9884\u8ba1\u7b97\u805a\u5408\u6765\u52a0\u901f\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u3002", "conclusion": "\u73b0\u4ee3\u6570\u636e\u5e93\u5f15\u64ce\u53ef\u4ee5\u901a\u8fc7\u9884\u8ba1\u7b97\u805a\u5408\u6765\u52a0\u901f\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6d41\u7a0b\uff0c\u56e0\u5b50\u5316\u5b66\u4e60\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2512.09685", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09685", "abs": "https://arxiv.org/abs/2512.09685", "authors": ["Zeyu Zhang", "Haiying Shen"], "title": "Straggler Tolerant and Resilient DL Training on Homogeneous GPUs", "comment": null, "summary": "Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.", "AI": {"tldr": "STAR\u7cfb\u7edf\u901a\u8fc7\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\u548c\u8d44\u6e90\u91cd\u5206\u914d\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684straggler\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002", "motivation": "\u5c3d\u7ba1GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u5f88\u6d41\u884c\uff0c\u4f46straggler\u95ee\u9898\u7684\u666e\u904d\u6027\u3001\u539f\u56e0\u3001\u5f71\u54cd\u4ee5\u53ca\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u53d1\u73b0straggler\u5e7f\u6cdb\u5b58\u5728\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u540c\u6b65\u8f6c\u5f02\u6b65SGD\uff09\u53ef\u80fd\u65e0\u6cd5\u6539\u5584\u8bad\u7ec3\u65f6\u95f4\u751a\u81f3\u4ea7\u751f\u66f4\u591astraggler\u3002", "method": "\u63d0\u51faSTAR\u7cfb\u7edf\uff0c\u5305\u542b\uff1a1\uff09\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\uff0c\u5c06worker\u5206\u7ec4\u8fdb\u884c\u53c2\u6570\u66f4\u65b0\uff1b2\uff09\u542f\u53d1\u5f0f\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9009\u62e9\u6700\u4f18\u540c\u6b65\u6a21\u5f0f\u4ee5\u6700\u5c0f\u5316\u8bad\u7ec3\u65f6\u95f4\uff1b3\uff09\u8d44\u6e90\u91cd\u5206\u914d\u652f\u6301\u6240\u9009\u6a21\u5f0f\u540c\u65f6\u51cf\u5c11\u5bf9\u5171\u5b58\u4f5c\u4e1a\u7684\u5f71\u54cd\uff1b4\uff09\u4e3b\u52a8\u9884\u9632straggler\uff0c\u907f\u514dCPU\u548c\u5e26\u5bbd\u8fc7\u8f7d\u3002", "result": "\u5728AWS\u4e0a\u7684trace\u9a71\u52a8\u8bc4\u4f30\u663e\u793a\uff0cSTAR\u5728PS\u67b6\u6784\u4e2d\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f448-84%\uff0c\u5728all-reduce\u67b6\u6784\u4e2d\u964d\u4f4e51-70%\uff0c\u540c\u65f6\u4fdd\u6301\u540c\u6b65SGD\u7684\u6536\u655b\u7cbe\u5ea6\u3002", "conclusion": "STAR\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u540c\u6b65\u6a21\u5f0f\u548c\u8d44\u6e90\u7ba1\u7406\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684straggler\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.09543", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09543", "abs": "https://arxiv.org/abs/2512.09543", "authors": ["Arihant Tripathy", "Ch Pavan Harshit", "Karthik Vaidhyanathan"], "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs", "comment": "8 pages, 5 figures, 1 table. Accepted to AGENT 2026 (ICSE 2026 workshop)", "summary": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u57fa\u4e8eSLM\u7684\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\u5728\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5f53\u524d\u6846\u67b6\u67b6\u6784\u662f\u80fd\u8017\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u4f46SLM\u7684\u6709\u9650\u63a8\u7406\u80fd\u529b\u5bfc\u81f4\u80fd\u8017\u6d6a\u8d39\uff0c\u9700\u8981\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u6765\u7ba1\u7406SLM\u5f31\u70b9\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u4ee3\u7406\u4f9d\u8d56\u5927\u578b\u4e13\u6709\u6a21\u578b\uff0c\u96be\u4ee5\u672c\u5730\u90e8\u7f72\uff0c\u4fc3\u4f7f\u4eba\u4eec\u5173\u6ce8\u5c0f\u8bed\u8a00\u6a21\u578b\u3002\u4f46SLM\u5728\u590d\u6742\u4ee3\u7406\u6846\u67b6\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u5316\u95ee\u9898\u89e3\u51b3\u4efb\u52a1\u4e2d\uff0c\u4ecd\u4e0d\u6e05\u695a\u3002", "method": "\u5728\u56fa\u5b9a\u786c\u4ef6\u4e0a\uff0c\u5bf9\u56db\u79cd\u9886\u5148\u7684\u4ee3\u7406\u6846\u67b6\uff08SWE-Agent\u3001OpenHands\u3001Mini SWE Agent\u3001AutoCodeRover\uff09\u4f7f\u7528\u4e24\u79cdSLM\uff08Gemma-3 4B\u3001Qwen-3 1.7B\uff09\uff0c\u5728SWE-bench Verified Mini\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u63a7\u5236\u8bc4\u4f30\uff0c\u6d4b\u91cf150\u6b21\u8fd0\u884c\u7684\u80fd\u91cf\u3001\u6301\u7eed\u65f6\u95f4\u3001\u4ee4\u724c\u4f7f\u7528\u548c\u5185\u5b58\u6d88\u8017\u3002", "result": "\u6846\u67b6\u67b6\u6784\u662f\u80fd\u8017\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff1a\u80fd\u8017\u6700\u9ad8\u7684AutoCodeRover\uff08Gemma\uff09\u6bd4\u6700\u4f4e\u7684OpenHands\uff08Gemma\uff09\u591a\u6d88\u80179.4\u500d\u80fd\u91cf\u3002\u4f46\u4efb\u52a1\u89e3\u51b3\u7387\u63a5\u8fd1\u96f6\uff0c\u8868\u660e\u5f53\u524d\u6846\u67b6\u4e0eSLM\u914d\u5bf9\u65f6\uff0c\u5927\u91cf\u80fd\u91cf\u6d6a\u8d39\u5728\u65e0\u751f\u4ea7\u529b\u7684\u63a8\u7406\u5faa\u73af\u4e0a\u3002SLM\u7684\u6709\u9650\u63a8\u7406\u662f\u6210\u529f\u74f6\u9888\uff0c\u800c\u6846\u67b6\u8bbe\u8ba1\u662f\u6548\u7387\u74f6\u9888\u3002", "conclusion": "\u5f53\u524d\u4e3a\u5f3a\u5927LLM\u8bbe\u8ba1\u7684\u4ee3\u7406\u6846\u67b6\u65e0\u6cd5\u4e0eSLM\u9ad8\u6548\u534f\u4f5c\u3002\u6846\u67b6\u67b6\u6784\u662f\u80fd\u8017\u4e3b\u56e0\uff0c\u4f46\u80fd\u91cf\u56e0SLM\u6709\u9650\u63a8\u7406\u800c\u6d6a\u8d39\u3002\u53ef\u884c\u7684\u4f4e\u80fd\u8017\u89e3\u51b3\u65b9\u6848\u9700\u8981\u4ece\u88ab\u52a8\u7f16\u6392\u8f6c\u5411\u4e3b\u52a8\u7ba1\u7406SLM\u5f31\u70b9\u7684\u67b6\u6784\u3002"}}
{"id": "2512.09710", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09710", "abs": "https://arxiv.org/abs/2512.09710", "authors": ["Hagit Attiya", "Panagiota Fatourou", "Eleftherios Kosmas", "Yuanhao Wei"], "title": "Recoverable Lock-Free Locks", "comment": null, "summary": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u540c\u65f6\u5b9e\u73b0\u65e0\u9501\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u5c06\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u8f6c\u6362\u4e3a\u53ef\u6062\u590d\u7684\u65e0\u9501\u5b9e\u73b0", "motivation": "\u73b0\u6709\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u5728\u65e0\u9501\u6027\u548c\u53ef\u6062\u590d\u6027\u4e4b\u95f4\u505a\u51fa\u6743\u8861\uff0c\u7f3a\u4e4f\u80fd\u540c\u65f6\u63d0\u4f9b\u8fd9\u4e24\u79cd\u7279\u6027\u7684\u8f6c\u6362\u65b9\u6cd5", "method": "\u4ece\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u51fa\u53d1\uff0c\u63d0\u4f9b\u5bf9\u9501\u83b7\u53d6\u548c\u9501\u91ca\u653e\u64cd\u4f5c\u7684\u53ef\u6062\u590d\u3001\u65e0\u9501\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u5d4c\u5957\u9501\u4ee5\u786e\u4fdd\u901a\u7528\u6027", "result": "\u5b9e\u73b0\u4e86\u9996\u4e2a\u540c\u65f6\u5f15\u5165\u65e0\u9501\u6027\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\uff0c\u5728\u4e0d\u5f71\u54cd\u539f\u59cb\u57fa\u4e8e\u9501\u5b9e\u73b0\u6b63\u786e\u6027\u7684\u524d\u63d0\u4e0b\u786e\u4fdd\u53ef\u6062\u590d\u6027", "conclusion": "\u8be5\u8f6c\u6362\u65b9\u6cd5\u586b\u8865\u4e86\u540c\u65f6\u5b9e\u73b0\u65e0\u9501\u548c\u53ef\u6062\u590d\u6027\u7684\u6280\u672f\u7a7a\u767d\uff0c\u4e3a\u5e76\u53d1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.09562", "categories": ["cs.SE", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.09562", "abs": "https://arxiv.org/abs/2512.09562", "authors": ["Radoslaw Klimek", "Jakub Blazowski"], "title": "Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values", "comment": "This manuscript has been submitted to Rank A/A* conference", "summary": "Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.", "AI": {"tldr": "\u5c06\u5de5\u4f5c\u6d41\u6316\u6398\u8f6c\u5316\u4e3a\u903b\u8f91\u89c4\u8303\uff0c\u7ed3\u5408Shapley\u503c\u91cf\u5316\u5143\u7d20\u8d21\u732e\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u5206\u6790", "motivation": "\u4f20\u7edf\u5de5\u4f5c\u6d41\u6316\u6398\u751f\u6210\u5c42\u6b21\u5316\u8fc7\u7a0b\u6811\uff0c\u4f46\u65e0\u6cd5\u89e3\u91ca\u6a21\u578b\u4e3a\u4f55\u6ee1\u8db3\u6216\u8fdd\u53cd\u903b\u8f91\u5c5e\u6027\uff0c\u4ee5\u53ca\u5404\u5143\u7d20\u5982\u4f55\u5f71\u54cd\u6574\u4f53\u884c\u4e3a", "method": "\u5c06\u6316\u6398\u7684\u5de5\u4f5c\u6d41\u8f6c\u5316\u4e3a\u903b\u8f91\u89c4\u8303\uff0c\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u5206\u6790\u53ef\u6ee1\u8db3\u6027\u3001\u6d3b\u6027\u548c\u5b89\u5168\u6027\u7b49\u5c5e\u6027\uff0c\u5e76\u5e94\u7528\u5408\u4f5c\u535a\u5f08\u8bba\u7684Shapley\u503c\u91cf\u5316\u5de5\u4f5c\u6d41\u5143\u7d20\u7684\u8d21\u732e", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u8bc6\u522b\u5173\u952e\u8282\u70b9\u3001\u63ed\u793a\u5197\u4f59\u3001\u66b4\u9732\u6709\u5bb3\u7ed3\u6784", "conclusion": "\u4e3a\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\u6709\u76f4\u63a5\u610f\u4e49\uff0c\u652f\u6301\u5408\u89c4\u68c0\u67e5\u3001\u6d41\u7a0b\u4f18\u5316\u3001\u5197\u4f59\u51cf\u5c11\u548c\u4e0b\u4e00\u4ee3\u8fc7\u7a0b\u6316\u6398\u5de5\u5177\u8bbe\u8ba1"}}
{"id": "2512.09596", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09596", "abs": "https://arxiv.org/abs/2512.09596", "authors": ["Arkadiusz Ry\u015b", "Lucas Lima", "Joeri Exelmans", "Dennis Janssens", "Hans Vangheluwe"], "title": "Model management to support systems engineering workflows using ontology-based knowledge graphs", "comment": null, "summary": "System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber- Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u672c\u4f53\u7684\u6846\u67b6\u6765\u7ba1\u7406CPS\u5de5\u4f5c\u6d41\u6267\u884c\u4ea7\u751f\u7684\u5efa\u6a21\u5de5\u4ef6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5b58\u50a8\u548c\u63a8\u7406\u7cfb\u7edf\u5de5\u7a0b\u6570\u636e", "motivation": "\u7cfb\u7edf\u5de5\u7a0b\u4ece\u6587\u6863\u4e2d\u5fc3\u8f6c\u5411\u6a21\u578b\u9a71\u52a8\uff0cCPS\u6d89\u53ca\u591a\u9886\u57df\u4e13\u5bb6\u4f7f\u7528\u4e0d\u540c\u5f62\u5f0f\u5316\u65b9\u6cd5\u6267\u884c\u590d\u6742\u5de5\u4f5c\u6d41\uff0c\u9700\u8981\u6709\u6548\u7ba1\u7406\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u4ea7\u751f\u7684\u5efa\u6a21\u5de5\u4ef6\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u3001\u53ef\u590d\u5236\u6027\u548c\u6570\u636e\u63a8\u7406", "method": "\u4f7f\u7528OML\uff08\u672c\u4f53\u5efa\u6a21\u8bed\u8a00\uff09\u5b9a\u4e49\u5de5\u4f5c\u6d41\u6982\u5ff5\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u5de5\u4ef6\u7684\u672c\u4f53\uff0c\u6784\u5efa\u5305\u542b\u7cfb\u7edf\u5de5\u7a0b\u6570\u636e\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u5f00\u53d1\u652f\u6301\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u3001\u6267\u884c\u3001\u5de5\u4ef6\u5b58\u50a8\u3001\u7248\u672c\u63a7\u5236\u3001\u67e5\u8be2\u548c\u63a8\u7406\u7684\u5de5\u5177\u96c6", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u4f20\u52a8\u7cfb\u7edf\u667a\u80fd\u4f20\u611f\u5668\u7cfb\u7edf\u5f00\u53d1\u573a\u666f\u4e2d\u5e94\u7528\u8be5\u6846\u67b6\uff0c\u7ed3\u679c\u663e\u793a\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5b58\u50a8\u548c\u7248\u672c\u63a7\u5236\u7b49\u57fa\u672c\u56f0\u96be\uff0c\u8fd8\u51cf\u5c11\u4e86\u8bbf\u95ee\u76f8\u5173\u4fe1\u606f\u7684\u65f6\u95f4\uff0c\u5e76\u80fd\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63a8\u7406\u51fa\u65b0\u77e5\u8bc6", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u672c\u4f53\u7684\u6846\u67b6\u80fd\u6709\u6548\u7ba1\u7406CPS\u5de5\u4f5c\u6d41\u6267\u884c\u4ea7\u751f\u7684\u5efa\u6a21\u5de5\u4ef6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u652f\u6301\u7cfb\u7edf\u5de5\u7a0b\u6570\u636e\u7684\u5b58\u50a8\u3001\u7248\u672c\u63a7\u5236\u3001\u67e5\u8be2\u548c\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u5f00\u53d1\u6548\u7387"}}
{"id": "2512.09627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09627", "abs": "https://arxiv.org/abs/2512.09627", "authors": ["Jingwei Ye", "Zhi Wang", "Chenbin Su", "Jieshuai Yang", "Jiayi Ding", "Chunbo Liu", "Ge Chu"], "title": "LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection", "comment": null, "summary": "Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.", "AI": {"tldr": "LogICL\uff1a\u4e00\u4e2a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8de8\u57df\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u901a\u8fc7\u63a8\u7406\u611f\u77e5\u7684\u6f14\u793a\u9009\u62e9\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u9700\u8981\u5927\u91cf\u8d44\u6e90\u548c\u6807\u6ce8\u6570\u636e\uff0c\u5728\u76ee\u6807\u57df\u65e5\u5fd7\u7a00\u7f3a\u65f6\u5b58\u5728\u51b7\u542f\u52a8\u95ee\u9898\u3002\u73b0\u6709\u8de8\u57df\u65b9\u6cd5\u4f9d\u8d56\u8868\u9762\u8bcd\u6c47\u76f8\u4f3c\u6027\uff0c\u96be\u4ee5\u6355\u6349\u7ed3\u6784\u5dee\u5f02\u4e0b\u7684\u6f5c\u5728\u8bed\u4e49\u7b49\u4ef7\u6027\u3002", "method": "\u63d0\u51faLogICL\u6846\u67b6\uff1a1\uff09\u8bad\u7ec3\u65f6\u6784\u5efadelta\u77e9\u9635\u8861\u91cf\u6f14\u793a\u5bf9\u96f6\u6837\u672c\u63a8\u7406\u7684\u6548\u7528\uff1b2\uff09\u4f7f\u7528\u591a\u76ee\u6807\u635f\u5931\u4f18\u5316\u7f16\u7801\u5668\uff08ICL\u5f15\u5bfc\u9879\u3001\u6700\u5927\u5747\u503c\u5dee\u5f02\u3001\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\uff09\uff1b3\uff09\u63a8\u7406\u65f6\u7f16\u7801\u5668\u68c0\u7d22\u63a8\u7406\u611f\u77e5\u7684\u6f14\u793a\uff0c\u652f\u6301\u51bb\u7ed3LLM\u7684\u601d\u7ef4\u94fe\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6709\u6548\u6355\u6349\u6f5c\u5728\u8bed\u4e49\u7b49\u4ef7\u6027\uff0c\u8d85\u8d8a\u8868\u9762\u8bcd\u6c47\u76f8\u4f3c\u6027\uff0c\u5b9e\u73b0\u5feb\u901f\u90e8\u7f72\u3002", "conclusion": "LogICL\u901a\u8fc7\u5c06LLM\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u4e86\u8de8\u57df\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u68c0\u6d4b\u3002"}}
{"id": "2512.09679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09679", "abs": "https://arxiv.org/abs/2512.09679", "authors": ["Naizhu Jin", "Zhong Li", "Guang Yang", "Tian Zhang", "Qingkai Zeng"], "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis", "comment": null, "summary": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u7ed3\u6784\u5316CoT\u65b9\u6cd5\u5e73\u5747\u63d0\u53475-12%\u7684Pass@1\uff0c\u4e14\u6bd4\u53cd\u601d\u63a8\u7406\u4f7f\u7528\u66f4\u5c11token\uff0c\u6548\u679c\u53d6\u51b3\u4e8e\u8bed\u8a00\u7c7b\u578b\u7cfb\u7edf\u548c\u6a21\u578b\u5bb9\u91cf\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u751f\u6210\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u601d\u7ef4\u94fe\u63d0\u793a\u5982\u4f55\u5e2e\u52a9\u4ee3\u7801\u751f\u6210\u7684\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5b9e\u8bc1\u548c\u4fe1\u606f\u8bba\u7814\u7a76\u6765\u7406\u89e3CoT\u5728\u795e\u7ecf\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u4e92\u4fe1\u606fI(Y;C|X)\u4f5c\u4e3a\u6982\u5ff5\u6846\u67b6\uff0c\u8bc4\u4f30\u4e94\u79cd\u8303\u5f0f\uff08\u96f6\u6837\u672c\u3001\u96f6\u6837\u672cCoT\u3001\u81ea\u89c4\u5212\u3001\u7ed3\u6784\u5316CoT\u3001\u63a8\u7406CoT\uff09\uff0c\u5728\u516d\u4e2aPython\u57fa\u51c6\u3001\u4e00\u4e2a\u5305\u542b12\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u4ee5\u53ca6\u4e2a7B\u5230480B\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5916\u90e8\u5f15\u5bfc\u7684CoT\u6301\u7eed\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\uff0c\u7ed3\u6784\u5316\u65b9\u6cd5\u5e73\u5747\u63d0\u5347Pass@1 5-12%\uff0c\u540c\u65f6\u6bd4\u53cd\u601d\u63a8\u7406\u4f7f\u7528\u66f4\u5c11token\uff1bCoT\u6548\u679c\u53d6\u51b3\u4e8e\u8bed\u8a00\u7c7b\u578b\u7cfb\u7edf\u548c\u6a21\u578b\u5bb9\u91cf\uff1b\u63a8\u7406\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316CoT\u6bd4\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u51c6\u786e\u7387\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8e\u6a21\u578b\u5bb9\u91cf\u3001\u8bed\u8a00\u7279\u6027\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u9009\u62e9CoT\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316CoT\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86CoT\u6548\u679c\u4e0e\u8bed\u8a00\u7c7b\u578b\u7cfb\u7edf\u548c\u6a21\u578b\u89c4\u6a21\u7684\u5173\u7cfb\u3002"}}
{"id": "2512.09775", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09775", "abs": "https://arxiv.org/abs/2512.09775", "authors": ["Vladimir Balditsyn", "Philippe Lalanda", "German Vega", "St\u00e9phanie Chollet"], "title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition", "comment": null, "summary": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6280\u672f\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u9886\u57df\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u4e0d\u540c\uff0c\u5b83\u4eec\u901a\u8fc7\u8bad\u7ec3\u800c\u975e\u624b\u52a8\u7f16\u7801\u5b9e\u73b0\uff0c\u5bfc\u81f4\u5176\u8fd0\u884c\u8fb9\u754c\u4e0d\u786e\u5b9a\u4e14\u65e0\u6cd5\u4fdd\u8bc1\u7edd\u5bf9\u65e0\u9519\u8bef\u3002\u5f53\u524d\u7f3a\u4e4f\u91cf\u5316ML\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u91cf\u5316ML\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u548c\u8054\u5408\u4f7f\u7528\u4e00\u7ec4\u9009\u5b9a\u7684\u6280\u672f\u6765\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u5728\u8fd0\u884c\u65f6\u7684\u76f8\u5173\u6027\u3002\u5728\u9ad8\u5ea6\u5f02\u6784\u548c\u52a8\u6001\u53d8\u5316\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u9886\u57df\u5e94\u7528\u8fd9\u4e9b\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u76f8\u5173\u6027\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30ML\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e3a\u9886\u57df\u4e13\u5bb6\u63d0\u4f9b\u8be6\u7ec6\u7684\u652f\u6301\u548c\u5e2e\u52a9\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5728ML\u7cfb\u7edf\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u53ef\u9760\u9884\u6d4b\u7684\u9886\u57df\u5982\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u4e2d\uff0c\u80fd\u591f\u5e2e\u52a9\u4e13\u5bb6\u66f4\u597d\u5730\u7406\u89e3\u548c\u4fe1\u4efbML\u7cfb\u7edf\u7684\u8f93\u51fa\u3002"}}
