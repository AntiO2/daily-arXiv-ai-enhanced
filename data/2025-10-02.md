<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 28]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: 提出了PBFD和PDFD两种基于图论建模的形式化软件开发方法，通过分层有向图和CSP确保结构正确性，并引入TLE编码方案实现常数时间更新。实证显示PBFD比Salesforce OmniScript快20倍，查询性能比传统关系模型快7-8倍。


<details>
  <summary>Details</summary>
Motivation: 解决形式化方法与实际开发实践之间的长期差距，通过图论建模强制执行结构正确性，为工业级全栈软件开发提供可扩展的解决方案。

Method: 使用分层有向图和统一状态机、CSP进行形式化建模，提出基于位掩码的TLE三级封装编码方案，确保有界精化终止和结构完整性。

Result: PBFD在8年企业部署中验证，开发速度比Salesforce OmniScript快20倍以上，查询性能比传统关系模型快7-8倍。两种方法都有开源MVP实现。

Conclusion: PBFD和PDFD建立了可复现、透明的框架，将形式化验证集成到实际软件开发中，所有形式化规范、MVP和数据集都已公开。

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary
Depth-First Development (PDFD), two formally defined and verified methodologies
for scalable, industrial-grade full-stack software engineering. These
approaches bridge a longstanding gap between formal methods and real-world
development practice by enforcing structural correctness through
graph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD
operate over layered directed graphs and are formalized using unified state
machines and Communicating Sequential Processes (CSP) to ensure critical
properties, including bounded-refinement termination and structural
completeness. To coordinate hierarchical data at scale, we propose Three-Level
Encapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers
provably constant-time updates. TLE's formal guarantees underpin PBFD's
industrial-scale performance and scalability. PBFD was empirically validated
through an eight-year enterprise deployment, demonstrating over 20x faster
development than Salesforce OmniScript and 7-8x faster query performance
compared to conventional relational models. Additionally, both methodologies
are supported by open-source MVPs, with PDFD's implementation conclusively
demonstrating its correctness-first design principles. Together, PBFD and PDFD
establish a reproducible, transparent framework that integrates formal
verification into practical software development. All formal specifications,
MVPs, and datasets are publicly available to foster academic research and
industrial-grade adoption.

</details>


### [2] [Semantic Zoom and Mini-Maps for Software Cities](https://arxiv.org/abs/2510.00003)
*Malte Hansen,Jens Bamberg,Noe Baumann,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 本文提出了两种解决3D软件城市可视化可扩展性挑战的方法：语义缩放和迷你地图，并在ExplorViz工具中实现，通过用户研究表明这些方法对大型软件景观特别有用。


<details>
  <summary>Details</summary>
Motivation: 随着可视化中显示数据量的增加，可视化本身可能变得难以理解。软件可视化工具需要解决视觉可扩展性挑战，特别是在3D软件城市环境中。

Method: 1. 语义缩放：根据虚拟相机与视觉对象的距离改变软件景观的图形表示；2. 迷你地图：在可视化中添加二维俯视投影的迷你地图。这些方法在基于Web的ExplorViz软件可视化工具中实现。

Result: 通过两个独立的用户研究评估，结果表明语义缩放和迷你地图都是有用的补充，特别适用于大型软件景观和协作软件探索。研究显示实现方法具有良好的可用性。

Conclusion: 语义缩放和迷你地图是解决3D软件城市视觉可扩展性问题的有效方法，但实施中仍存在一些需要改进的不足之处。

Abstract: Software visualization tools can facilitate program comprehension by
providing visual metaphors, or abstractions that reduce the amount of textual
data that needs to be processed mentally. One way they do this is by enabling
developers to build an internal representation of the visualized software and
its architecture. However, as the amount of displayed data in the visualization
increases, the visualization itself can become more difficult to comprehend.
The ability to display small and large amounts of data in visualizations is
called visual scalability.
  In this paper, we present two approaches to address the challenge of visual
scalability in 3D software cities. First, we present an approach to semantic
zoom, in which the graphical representation of the software landscape changes
based on the virtual camera's distance from visual objects. Second, we augment
the visualization with a miniature two-dimensional top-view projection called
mini-map. We demonstrate our approach using an open-source implementation in
our software visualization tool ExplorViz. ExplorViz is web-based and uses the
3D city metaphor, focusing on live trace visualization.
  We evaluated our approaches in two separate user studies. The results
indicate that semantic zoom and the mini-map are both useful additions. User
feedback indicates that semantic zoom and mini-maps are especially useful for
large software landscapes and collaborative software exploration. The studies
indicate a good usability of our implemented approaches. However, some
shortcomings in our implementations have also been discovered, to be addressed
in future work.
  Video URL: https://youtu.be/LYtUeWvizjU

</details>


### [3] [HTML Structure Exploration in 3D Software Cities](https://arxiv.org/abs/2510.00004)
*Malte Hansen,David Moreno-Lumbreras,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: 在ExplorViz软件可视化工具中嵌入Web视图，通过3D可视化展示HTML的DOM结构，便于与应用程序交互和探索网页内容


<details>
  <summary>Details</summary>
Motivation: 大型软件系统通常提供Web界面，但现有软件可视化工具往往忽略这些界面。需要一种方法将Web界面集成到软件可视化中，便于理解和探索软件行为

Method: 在ExplorViz工具中添加嵌入式Web视图，为检测的应用程序提供3D可视化，并在同源上下文中通过三维表示可视化HTML的DOM结构

Result: 通过初步用户研究评估了可视化方法，获得了关于潜在用例、优势和不足的见解

Conclusion: 基于研究结果提出了进一步研究方向，支持Web界面的可视化探索，并探索软件城市和HTML结构组合可视化的用例

Abstract: Software visualization, which uses data from dynamic program analysis, can
help to explore and understand the behavior of software systems. It is common
that large software systems offer a web interface for user interaction.
Usually, available web interfaces are not regarded in software visualization
tools. This paper introduces additions to the web-based live tracing software
visualization tool ExplorViz: We add an embedded web view for instrumented
applications in the 3D visualization to ease interaction with the given
applications and enable the exploration of the thereby displayed HTML content.
Namely, the Document Object Model (DOM) is visualized via a three-dimensional
representation of the HTML structure in same-origin contexts.
  Our visualization approach is evaluated in a preliminary user study. The
study results give insights into the potential use cases, benefits, and
shortcomings of our implemented approach. Based on our study results, we
propose directions for further research to support the visual exploration of
web interfaces and explore use cases for the combined visualization of software
cities and HTML structure.
  Video URL: https://youtu.be/wBWKlbvzOOE

</details>


### [4] [VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs](https://arxiv.org/abs/2510.00031)
*Shun-ichiro Hayashi,Koki Morita,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.SE

TL;DR: VibeCodeHPC是一个基于多智能体LLM的HPC程序自动调优系统，通过角色分配和迭代提示优化来生成高质量代码。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自动调优高性能计算程序的系统，通过多智能体协作提高代码生成质量和效率。

Method: 采用四角色多智能体配置（项目经理、系统工程师、程序员、持续交付），结合动态智能体部署和活动监控功能，实现迭代式提示优化。

Result: 在CPU到GPU代码转换案例中，多智能体配置相比单智能体在单位时间内生成更高质量的代码，并能更有效地识别需求违规和其他问题。

Conclusion: VibeCodeHPC的多智能体架构和动态部署监控机制显著提升了HPC程序调优的效率和代码质量。

Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on
multi-agent LLMs for code generation. VibeCodeHPC tunes programs through
multi-agent role allocation and iterative prompt refinement. We describe the
system configuration with four roles: Project Manager (PM), System Engineer
(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent
deployment and activity monitoring functions to facilitate effective
multi-agent collaboration. In our case study, we convert and optimize CPU-based
matrix-matrix multiplication code written in C to GPU code using CUDA. The
multi-agent configuration of VibeCodeHPC achieved higher-quality code
generation per unit time compared to a solo-agent configuration. Additionally,
the dynamic agent deployment and activity monitoring capabilities facilitated
more effective identification of requirement violations and other issues.

</details>


### [5] [A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0](https://arxiv.org/abs/2510.00092)
*Shufeng Chen,Mariat James Elizebeth,Robab Aghazadeh Chakherlou,Xingyu Zhao,Eric Barbier,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: 本文提出了一个基于Assurance 2.0范式的分解框架，用于识别完整的安全论证集合并测量相应证据，并在端到端AI自动驾驶车辆开发中进行了案例研究。


<details>
  <summary>Details</summary>
Motivation: 解决Assurance 2.0在置信度测量、残余怀疑管理、自动化支持以及实际处理反驳论点和确认偏见方面的局限性。

Method: 采用三层分解策略：顶层将SDV开发分为需求工程、验证与确认、部署后三个阶段；每个阶段按产品开发生命周期进一步分解；使用改进的5M1E模型（人、机、法、料、测、环）进行多维度分析。

Result: 建立了一个支持安全声明、证据和潜在反驳论点细粒度可追溯性的多维分解框架。

Conclusion: 该框架增强了安全论证的全面性和可追溯性，为复杂自适应系统的保证提供了结构化方法。

Abstract: Assurance 2.0 is a modern framework developed to address the assurance
challenges of increasingly complex, adaptive, and autonomous systems. Building
on the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable
assurance theories and explicit counterarguments (defeaters) to enhance rigor,
transparency, and adaptability. It supports continuous, incremental assurance,
enabling innovation without compromising safety. However, limitations persist
in confidence measurement, residual doubt management, automation support, and
the practical handling of defeaters and confirmation bias. This paper presents
\textcolor{black}{a set of decomposition frameworks to identify a complete set
of safety arguments and measure their corresponding evidence.} Grounded in the
Assurance 2.0 paradigm, the framework is instantiated through a structured
template and employs a three-tiered decomposition strategy. \textcolor{black}{A
case study regarding the application of the decomposition framework in the
end-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also
presented in this paper.} At the top level, the SDV development is divided into
three critical phases: Requirements Engineering (RE), Verification and
Validation (VnV), and Post-Deployment (PD). Each phase is further decomposed
according to its Product Development Lifecycle (PDLC). To ensure comprehensive
coverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,
Method, Material, Measurement, and Environment). Originally developed for
manufacturing quality control, the 5M1E model is reinterpreted and contextually
mapped to the assurance domain. This enables a multi-dimensional decomposition
that supports fine-grained traceability of safety claims, evidence, and
potential defeaters.

</details>


### [6] [Container Orchestration Patterns for Optimizing Resource Use](https://arxiv.org/abs/2510.00197)
*Diogo Maia,Filipe Correia,André Restivo,Paulo Queiroz*

Main category: cs.SE

TL;DR: 该论文提出了三个服务编排资源优化模式：抢占式调度、服务平衡和垃圾收集，旨在解决服务编排中的挑战并促进最佳实践的应用。


<details>
  <summary>Details</summary>
Motivation: 服务编排在基于服务的架构中具有挑战性，特别是对于新手而言。现有的编排技术资源缺乏清晰度和标准化，使得最佳实践难以实施，限制了其在软件行业中的采用。

Method: 通过分析现有文献和工具，识别常见的编排实践，并基于发现定义了三个关键的编排资源优化模式。

Result: 定义了三个编排资源优化模式：抢占式调度（为高优先级服务分配足够资源）、服务平衡（重新构建节点以优化资源使用）和垃圾收集（创建清理机制以优化系统资源使用）。

Conclusion: 这些模式作为改进编排实践和促进在基于服务的架构中更广泛采用的基础要素。

Abstract: Service-based architectures provide substantial benefits, yet service
orchestration remains a challenge, particularly for newcomers. While various
resources on orchestration techniques exist, they often lack clarity and
standardization, making best practices difficult to implement and limiting
their adoption within the software industry.
  To address this gap, we analyzed existing literature and tools to identify
common orchestration practices. Based on our findings, we define three key
orchestration resource optimization patterns: {\sc Preemptive Scheduling}, {\sc
Service Balancing}, and {\sc Garbage Collection}. {\sc Preemptive Scheduling}
allows the allocation of sufficient resources for services of higher priority
in stressful situations, while {\sc Service Balancing} enables a restructuring
of the nodes to allow better resource usage. To end, {\sc Garbage Collection}
creates cleanup mechanisms to better understand the system's resource usage and
optimize it. These patterns serve as foundational elements for improving
orchestration practices and fostering broader adoption in service-based
architectures.

</details>


### [7] [Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?](https://arxiv.org/abs/2510.00324)
*Lucas Roberts,Denisa Roberts*

Main category: cs.SE

TL;DR: 本研究探讨使用大语言模型进行代码检索和标注生成，比较不同检索表示方法、编程语言和LLM对人类标注的影响，发现检索器和编程语言存在亲和性，并提出使用转译器扩展代码搜索基准数据集的方法。


<details>
  <summary>Details</summary>
Motivation: 代码搜索在信息检索中很重要，但受限于人工标注的高成本。需要编程语言专业知识和软件工程领域知识，使得代码标注比一般文本问答系统更困难。

Method: 使用LLMs检索函数级代码并生成标注，比较稀疏表示与语义表示、不同编程语言（C、Java、Javascript、Go、Python）和不同LLM的影响，分析人类标注与AI相关性判断的一致性。

Result: 发现所选检索器和编程语言存在亲和性，可以改善人类与AI相关性判断的一致性；不同编程语言在表示方法（稀疏vs语义）上存在差异，影响人类与AI判断的一致性；使用转译器构建的基准数据集中，人类-AI一致性基本匹配人类-人类一致性。

Conclusion: 可以利用检索器和编程语言的亲和性来改进代码搜索性能；转译器可用于扩展代码搜索基准数据集；人类-AI相关性判断一致性接近人类-人类一致性水平。

Abstract: Code search is an important information retrieval application. Benefits of
better code search include faster new developer on-boarding, reduced software
maintenance, and ease of understanding for large repositories. Despite
improvements in search algorithms and search benchmarks, the domain of code
search has lagged behind. One reason is the high cost of human annotation for
code queries and answers. While humans may annotate search results in general
text QA systems, code annotations require specialized knowledge of a
programming language (PL), as well as domain specific software engineering
knowledge. In this work we study the use of Large Language Models (LLMs) to
retrieve code at the level of functions and to generate annotations for code
search results. We compare the impact of the retriever representation (sparse
vs. semantic), programming language, and LLM by comparing human annotations
across several popular languages (C, Java, Javascript, Go, and Python). We
focus on repositories that implement common data structures likely to be
implemented in any PLs. For the same human annotations, we compare several
LLM-as-a-Judge models to evaluate programming language and other affinities
between LLMs. We find that the chosen retriever and PL exhibit affinities that
can be leveraged to improve alignment of human and AI relevance determinations,
with significant performance implications. We also find differences in
representation (sparse vs. semantic) across PLs that impact alignment of human
and AI relevance determinations. We propose using transpilers to bootstrap
scalable code search benchmark datasets in other PLs and in a case study
demonstrate that human-AI relevance agreement rates largely match the (worst
case) human-human agreement under study. The application code used in this work
is available at \href{https://github.com/rlucas7/code-searcher/}{this github
repo}.

</details>


### [8] [Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook - a Grey Literature Review](https://arxiv.org/abs/2510.00328)
*Ahmed Fawz,Amjed Tahir,Kelly Blincoe*

Main category: cs.SE

TL;DR: 本文系统研究了"氛围编程"现象，即用户通过直觉和试错依赖AI代码生成工具而不理解底层代码。研究发现存在速度-质量悖论，虽然加速了原型开发但牺牲了可靠性和可维护性。


<details>
  <summary>Details</summary>
Motivation: 尽管AI代码生成工具被广泛采用，但尚无研究系统调查用户为何进行氛围编程、他们在过程中的体验以及如何进行质量保证。

Method: 对101个从业者来源进行系统灰色文献综述，提取了518个关于氛围编程实践、挑战和限制的第一手行为描述。

Result: 揭示了速度-质量权衡悖论：氛围编程者被速度和可访问性驱动，经常体验到"即时成功和流畅"，但大多数人认为生成的代码快速但有缺陷。质量保证实践常被忽视，许多用户跳过测试或依赖工具输出。

Conclusion: 氛围编程降低了门槛并加速了原型开发，但以可靠性和可维护性为代价。这创造了一类新的易受攻击的软件开发人员，他们能够构建产品但无法调试问题。这些发现对工具设计者和开发团队具有重要意义。

Abstract: AI code generation tools are transforming software development, especially
for novice and non-software developers, by enabling them to write code and
build applications faster and with little to no human intervention. Vibe coding
is the practice where users rely on AI code generation tools through intuition
and trial-and-error without necessarily understanding the underlying code.
Despite widespread adoption, no research has systematically investigated why
users engage in vibe coding, what they experience while doing so, and how they
approach quality assurance (QA) and perceive the quality of the AI-generated
code. To this end, we conduct a systematic grey literature review of 101
practitioner sources, extracting 518 firsthand behavioral accounts about vibe
coding practices, challenges, and limitations. Our analysis reveals a
speed-quality trade-off paradox, where vibe coders are motivated by speed and
accessibility, often experiencing rapid ``instant success and flow'', yet most
perceive the resulting code as fast but flawed. QA practices are frequently
overlooked, with many skipping testing, relying on the models' or tools'
outputs without modification, or delegating checks back to the AI code
generation tools. This creates a new class of vulnerable software developers,
particularly those who build a product but are unable to debug it when issues
arise. We argue that vibe coding lowers barriers and accelerates prototyping,
but at the cost of reliability and maintainability. These insights carry
implications for tool designers and software development teams. Understanding
how vibe coding is practiced today is crucial for guiding its responsible use
and preventing a broader QA crisis in AI-assisted development.

</details>


### [9] [Beyond Pass/Fail: The Story of Learning-Based Testing](https://arxiv.org/abs/2510.00450)
*Sheikh Md. Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: 基于学习的测试（LBT）结合学习和测试过程，通过主动学习推断被测系统模型，仅需少量初始测试用例即可扩展至大型复杂程序，确保全面测试。


<details>
  <summary>Details</summary>
Motivation: LBT旨在通过融合学习和测试过程，实现测试和行为充分性，为大型复杂程序提供可扩展的测试方法。

Method: 使用主动学习推断被测系统模型，逐步生成测试用例进行测试，涵盖不同程序类型的各种LBT实现。

Result: LBT在过程和反应式程序测试中显示出有效性，现有工具和库展示了该概念的演进和工业应用潜力。

Conclusion: LBT是一种有前景的软件测试技术，具有未充分利用的潜力，可为实践者和研究社区带来显著益处。

Abstract: Learning-Based Testing (LBT) merges learning and testing processes to achieve
both testing and behavioral adequacy. LBT utilizes active learning to infer the
model of the System Under Test (SUT), enabling scalability for large and
complex programs by requiring only a minimal set of initial test cases. The
core principle of LBT is that the SUT's behavior can be thoroughly inferred by
progressively generating test cases and subjecting the SUT to testing, thereby
ensuring comprehensive testing. Despite being in its early stages, LBT has a
solid foundation of theoretical research demonstrating its efficacy in testing
both procedural and reactive programs. This paper provides a systematic
literature review of various LBT implementations across different program types
and evaluates the current state of research in this field. We explore diverse
theoretical frameworks, existing tools, and libraries within the LBT domain to
illustrate the concept's evolution and current research status. Additionally,
we examine case studies involving the application of LBT tools in industrial
settings, highlighting their potential and effectiveness in commercial software
testing. This systematic literature review aims to offer researchers a
comprehensive perspective on the inception and development of LBT, presenting
it as a promising technique in software testing. By unveiling LBT's
underutilized potential, this paper seeks to significantly benefit the
practitioners and research community.

</details>


### [10] [Analyzing Latent Concepts in Code Language Models](https://arxiv.org/abs/2510.00476)
*Arushi Sharma,Vedant Pungliya,Christopher J. Quinn,Ali Jannesari*

Main category: cs.SE

TL;DR: 提出了Code Concept Analysis (CoCoA)框架，通过聚类上下文标记嵌入来揭示代码语言模型中的词汇、句法和语义概念结构，结合静态分析和LLM进行概念标注，提升模型解释性。


<details>
  <summary>Details</summary>
Motivation: 解释代码语言模型的内部行为对于需要信任、透明度和语义鲁棒性的应用至关重要，但目前仍是一个挑战。

Method: 提出CoCoA全局后验可解释性框架，聚类上下文标记嵌入形成人类可解释的概念组；采用混合标注流程结合静态分析和提示工程LLM进行概念标注；与局部归因方法结合生成概念基础解释。

Result: CoCoA发现的概念在语义保持扰动下保持稳定（平均CSI=0.288），随微调可预测演化；在用户研究中，概念增强解释在编程语言分类任务中比基于积分梯度的标记级归因提高37个百分点的人类中心可解释性。

Conclusion: CoCoA框架能够有效揭示代码语言模型中的潜在概念结构，提供稳定且可解释的概念表示，显著提升模型解释性和人类理解能力。

Abstract: Interpreting the internal behavior of large language models trained on code
remains a critical challenge, particularly for applications demanding trust,
transparency, and semantic robustness. We propose Code Concept Analysis
(CoCoA): a global post-hoc interpretability framework that uncovers emergent
lexical, syntactic, and semantic structures in a code language model's
representation space by clustering contextualized token embeddings into
human-interpretable concept groups. We propose a hybrid annotation pipeline
that combines static analysis tool-based syntactic alignment with
prompt-engineered large language models (LLMs), enabling scalable labeling of
latent concepts across abstraction levels. We analyse the distribution of
concepts across layers and across three finetuning tasks. Emergent concept
clusters can help identify unexpected latent interactions and be used to
identify trends and biases within the model's learned representations. We
further integrate LCA with local attribution methods to produce
concept-grounded explanations, improving the coherence and interpretability of
token-level saliency. Empirical evaluations across multiple models and tasks
show that LCA discovers concepts that remain stable under semantic-preserving
perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve
predictably with fine-tuning. In a user study, concept-augmented explanations
disambiguate token roles. In a user study on the programming-language
classification task, concept-augmented explanations disambiguated token roles
and improved human-centric explainability by 37 percentage points compared with
token-level attributions using Integrated Gradients.

</details>


### [11] [CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling](https://arxiv.org/abs/2510.00501)
*Kaixin Wang,Tianlin Li,Xiaoyu Zhang,Aishan Liu,Xianglong Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,and Bin Shi*

Main category: cs.SE

TL;DR: CodeChemist是一个无需重新训练模型就能提升低资源编程语言代码生成性能的测试时扩展框架，通过从高资源语言生成测试用例实现功能知识迁移。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型在不同编程语言上的性能不一致，低资源语言由于训练数据有限表现较差，需要一种有效的方法来提升低资源语言的代码生成能力。

Method: 首先生成并执行高资源语言的代码来创建包含功能知识的测试用例，然后使用多温度对冲采样生成低资源语言的代码片段，根据测试用例通过率选择最佳代码。

Result: 大量实验表明，CodeChemist优于现有的测试时扩展方法，显著提升了低资源编程语言的代码生成性能。

Conclusion: CodeChemist提供了一种高效的方法，通过测试用例驱动的功能知识迁移，成功解决了低资源编程语言代码生成性能不足的问题。

Abstract: Code Large Language Models (CodeLLMs) are increasingly used in code
generation tasks across a wide range of applications. However, their
performance is often inconsistent across different programming languages (PLs),
with low-resource PLs suffering the most due to limited training data. In this
paper, we present CodeChemist, a novel and efficient framework for test-time
scaling that enables functional knowledge transfer from high-resource to
low-resource PLs using generated test cases. CodeChemist first generates and
executes code in high-resource PLs to create test cases that encapsulate
functional knowledge. It then uses multi-temperature hedged sampling to
generate code snippets in the low-resource PL and selects the best one based on
the pass rate of the test cases. Our extensive experiments show that
CodeChemist outperforms existing test-time scaling approaches, boosting the
performance of code generation for low-resource PLs without requiring any model
retraining.

</details>


### [12] [Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems](https://arxiv.org/abs/2510.00519)
*Hadiza Umar Yusuf,Khouloud Gaaloul*

Main category: cs.SE

TL;DR: 本文研究了AI驱动的CPS与传统控制模型在架构上的差异及其对系统验证的影响


<details>
  <summary>Details</summary>
Motivation: 尽管AI集成提升了CPS的适应性，但对其架构、操作复杂性和验证实践的影响仍存在理解空白

Method: 通过调查Simulink中设计的AI驱动与传统控制模型的架构差异

Result: 揭示了两种模型在系统验证方面的不同影响

Conclusion: 需要更深入理解AI集成对CPS架构和验证实践的影响

Abstract: In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion
occurs where digital technology meets the physical world. This synergy has been
significantly transformed by the integration of artificial intelligence (AI), a
move that dramatically enhances system adaptability and introduces a layer of
complexity that impacts CPS control optimization and reliability. Despite
advancements in AI integration, a significant gap remains in understanding how
this shift affects CPS architecture, operational complexity, and verification
practices. The extended abstract addresses this gap by investigating
architectural distinctions between AI-driven and traditional control models
designed in Simulink and their respective implications for system verification.

</details>


### [13] [LSPFuzz: Hunting Bugs in Language Servers](https://arxiv.org/abs/2510.00532)
*Hengcheng Zhu,Songqiang Chen,Valerio Terragni,Lili Wei,Jiarong Wu,Yepang Liu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: LSPFuzz是一个针对语言服务器协议(LSP)的灰盒混合模糊测试工具，通过两阶段变异策略系统性地测试LSP服务器，发现了51个真实漏洞。


<details>
  <summary>Details</summary>
Motivation: LSP协议已被广泛采用，但缺乏专门的测试技术。LSP服务器崩溃会禁用所有代码智能功能，漏洞还会带来安全风险。

Method: 采用两阶段变异管道：语法感知的源代码变异，然后上下文感知的编辑器操作分发。

Result: 在四个广泛使用的LSP服务器上测试，发现51个漏洞，其中42个被确认，26个已修复，2个获得CVE编号。

Conclusion: LSPFuzz提升了LSP服务器的质量保证，为未来研究提供了实用工具和基础见解。

Abstract: The Language Server Protocol (LSP) has revolutionized the integration of code
intelligence in modern software development. There are approximately 300 LSP
server implementations for various languages and 50 editors offering LSP
integration. However, the reliability of LSP servers is a growing concern, as
crashes can disable all code intelligence features and significantly impact
productivity, while vulnerabilities can put developers at risk even when
editing untrusted source code. Despite the widespread adoption of LSP, no
existing techniques specifically target LSP server testing. To bridge this gap,
we present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.
Our key insight is that effective LSP server testing requires holistic mutation
of source code and editor operations, as bugs often manifest from their
combinations. To satisfy the sophisticated constraints of LSP and effectively
explore the input space, we employ a two-stage mutation pipeline: syntax-aware
mutations to source code, followed by context-aware dispatching of editor
operations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz
demonstrated superior performance compared to baseline fuzzers, and uncovered
previously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,
42 have been confirmed, 26 have been fixed by developers, and two have been
assigned CVE numbers. Our work advances the quality assurance of LSP servers,
providing both a practical tool and foundational insights for future research
in this domain.

</details>


### [14] [AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation](https://arxiv.org/abs/2510.00591)
*Liyi Cai,Yijie Ren,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: 提出了AI驱动的自进化软件概念，通过多智能体架构实现软件自主解释用户需求、生成验证代码并集成新功能，展示了真正自动化软件开发的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前AI在软件开发中主要作为人类开发者的助手，仍然依赖人工干预。研究旨在探索AI能否超越助手角色，成为软件核心组件，实现真正的软件自动化。

Method: 采用多智能体架构构建轻量级原型，系统能够自主解释用户需求、生成和验证代码、集成新功能，实现软件的持续进化。

Result: 在多个代表性场景的案例研究表明，原型能够可靠地构建和重用功能，为更复杂应用的扩展提供了早期证据。

Conclusion: AI驱动的自进化软件展示了真正自动化软件开发的可行性，为未来软件工程的发展指明了方向。

Abstract: Software automation has long been a central goal of software engineering,
striving for software development that proceeds without human intervention.
Recent efforts have leveraged Artificial Intelligence (AI) to advance software
automation with notable progress. However, current AI functions primarily as
assistants to human developers, leaving software development still dependent on
explicit human intervention. This raises a fundamental question: Can AI move
beyond its role as an assistant to become a core component of software, thereby
enabling genuine software automation? To investigate this vision, we introduce
AI-Driven Self-Evolving Software, a new form of software that evolves
continuously through direct interaction with users. We demonstrate the
feasibility of this idea with a lightweight prototype built on a multi-agent
architecture that autonomously interprets user requirements, generates and
validates code, and integrates new functionalities. Case studies across
multiple representative scenarios show that the prototype can reliably
construct and reuse functionality, providing early evidence that such software
systems can scale to more sophisticated applications and pave the way toward
truly automated software development. We make code and cases in this work
publicly available at https://anonymous.4open.science/r/live-software.

</details>


### [15] [PyTrim: A Practical Tool for Reducing Python Dependency Bloat](https://arxiv.org/abs/2510.00674)
*Konstantinos Karakatsanis,Georgios Alexopoulos,Ioannis Karyotakis,Foivos Timotheos Proestakis,Evangelos Talos,Panos Louridas,Dimitris Mitropoulos*

Main category: cs.SE

TL;DR: PYTRIM是一个端到端系统，用于自动检测和移除Python项目中的未使用依赖项，包括源代码和配置文件中的导入和包声明。


<details>
  <summary>Details</summary>
Motivation: Python项目中的依赖膨胀问题增加了维护成本和安全风险，现有工具只能检测未使用依赖但无法自动移除，需要人工操作。

Method: PYTRIM采用模块化设计，可与任何依赖检测工具集成，并包含新颖的动态分析组件以提高检测召回率，自动移除各种文件类型中的未使用导入和包声明。

Result: 在37个真实合并请求的数据集上，PYTRIM达到98.3%的准确率；在971个开源包中，成功识别并修剪了39个包的膨胀依赖，其中6个修剪请求已被接受合并。

Conclusion: PYTRIM能有效自动化Python依赖修剪过程，具有高准确性和实际应用价值，已作为开源项目发布。

Abstract: Dependency bloat is a persistent challenge in Python projects, which
increases maintenance costs and security risks. While numerous tools exist for
detecting unused dependencies in Python, removing these dependencies across the
source code and configuration files of a project requires manual effort and
expertise.
  To tackle this challenge we introduce PYTRIM, an end-to-end system to
automate this process. PYTRIM eliminates unused imports and package
declarations across a variety of file types, including Python source and
configuration files such as requirements.txt and setup.py. PYTRIM's modular
design makes it agnostic to the source of dependency bloat information,
enabling integration with any detection tool. Beyond its contribution when it
comes to automation, PYTRIM also incorporates a novel dynamic analysis
component that improves dependency detection recall.
  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset
of 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%
accuracy in replicating human-made changes. To show its practical impact, we
run PYTRIM on 971 open-source packages, identifying and trimming bloated
dependencies in 39 of them. For each case, we submit a corresponding pull
request, 6 of which have already been accepted and merged. PYTRIM is available
as an open-source project, encouraging community contributions and further
development.
  Video demonstration: https://youtu.be/LqTEdOUbJRI
  Code repository: https://github.com/TrimTeam/PyTrim

</details>


### [16] [TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies](https://arxiv.org/abs/2510.00680)
*Hang Cui,Jingjing Li,Haotian Si,Quan Zhou,Changhua Pei,Gaogang Xie,Dan Pei*

Main category: cs.SE

TL;DR: TShape是一个用于工业时间序列异常检测的新框架，通过补丁式双注意力机制和多尺度卷积来检测复杂的形状异常，在五个基准测试中平均F1分数提升10%。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以检测表现为复杂形状偏差的形状异常，这些异常对人类专家显而易见但对机器学习算法具有挑战性。

Method: 引入补丁式双注意力机制与多尺度卷积，通过平衡局部细粒度形状特征与全局上下文依赖来建模复杂子序列变化。

Result: 在五个多样化基准测试中，TShape优于现有最先进模型，异常检测平均F1分数提升10%。消融研究和注意力可视化证实了各组件的重要贡献。

Conclusion: TShape对时间序列数据中复杂形状异常具有鲁棒性和适应性，各组件均对性能提升有重要贡献。

Abstract: Time series anomaly detection (TSAD) is critical for maintaining the
reliability of modern IT infrastructures, where complex anomalies frequently
arise in highly dynamic environments. In this paper, we present TShape, a novel
framework designed to address the challenges in industrial time series anomaly
detection. Existing methods often struggle to detect shapelet anomalies that
manifest as complex shape deviations, which appear obvious to human experts but
prove challenging for machine learning algorithms. TShape introduces a
patch-wise dual attention mechanism with multi-scale convolution to model
intricate sub-sequence variations by balancing local, fine-grained shape
features with global contextual dependencies. Our extensive evaluation on five
diverse benchmarks demonstrates that TShape outperforms existing
state-of-the-art models, achieving an average 10\% F1 score improvement in
anomaly detection. Additionally, ablation studies and attention visualizations
confirm the essential contributions of each component, highlighting the
robustness and adaptability of TShape to complex shapelet shapes in time series
data.

</details>


### [17] [Maven-Lockfile: High Integrity Rebuild of Past Java Releases](https://arxiv.org/abs/2510.00730)
*Larissa Schmid,Elias Lundell,Yogya Gamage,Benoit Baudry,Martin Monperrus*

Main category: cs.SE

TL;DR: Maven-Lockfile为Java生态系统中的Maven包管理器提供了锁文件支持，能够生成和更新锁文件，捕获所有直接和传递依赖及其校验和，确保构建的高完整性和可重现性。


<details>
  <summary>Details</summary>
Motivation: 现代软件项目依赖大量第三方库，这使可重现和安全构建变得复杂。Maven作为Java生态系统中最重要的包管理器之一，缺乏对锁文件的本地支持，无法确保依赖版本的固定和完整性验证。

Method: 开发了Maven-Lockfile工具来生成和更新锁文件，支持从历史版本重建项目。锁文件捕获所有直接和传递依赖及其校验和。

Result: 评估显示Maven-Lockfile能够重现历史提交的构建，并能检测到被篡改的构件。通过最小配置即可为Java项目提供现代构建完整性和可重现性。

Conclusion: Maven-Lockfile为Java项目提供了构建完整性和可重现性的现代解决方案，并为Java软件供应链安全的未来研究奠定了基础。

Abstract: Modern software projects depend on many third-party libraries, complicating
reproducible and secure builds. Several package managers address this with the
generation of a lockfile that freezes dependency versions and can be used to
verify the integrity of dependencies. Yet, Maven, one of the most important
package managers in the Java ecosystem, lacks native support for a lockfile. We
present Maven-Lockfile to generate and update lockfiles, with support for
rebuilding projects from past versions. Our lockfiles capture all direct and
transitive dependencies with their checksums, enabling high integrity builds.
Our evaluation shows that Maven-Lockfile can reproduce builds from historical
commits and is able to detect tampered artifacts. With minimal configuration,
Maven-Lockfile equips Java projects with modern build integrity and build
reproducibility, and fosters future research on software supply chain security
in Java.

</details>


### [18] [AI Where It Matters: Where, Why, and How Developers Want AI Support in Daily Work](https://arxiv.org/abs/2510.00762)
*Rudrajit Choudhuri,Carmen Badea,Christian Bird,Jenna Butler,Rob DeLine,Brian Houck*

Main category: cs.SE

TL;DR: 该研究通过860名开发者的大规模混合方法研究，揭示了开发者对AI支持的需求模式和负责任AI优先事项，基于任务评估预测AI采用情况。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑软件工作，但缺乏关于开发者最需要AI支持的领域以及如何负责任设计的明确指导。

Method: 使用认知评估理论，通过大规模混合方法研究(N=860)分析开发者的任务感知与AI采用模式之间的关系。

Result: 研究发现：核心工作(编码、测试)有强烈使用需求；减少繁琐工作(文档、运维)需求高；身份和关系相关工作(指导)有明显限制。负责任AI优先级因情境而异。

Conclusion: 研究结果为在开发者关心的领域提供AI支持提供了具体、情境化的指导。

Abstract: Generative AI is reshaping software work, yet we lack clear guidance on where
developers most need and want support, and how to design it responsibly. We
report a large-scale, mixed-methods study of N=860 developers that examines
where, why, and how they seek or limit AI help, providing the first task-aware,
empirically validated mapping from developers' perceptions of their tasks to AI
adoption patterns and responsible AI priorities. Using cognitive appraisal
theory, we show that task evaluations predict openness to and use of AI,
revealing distinct patterns: strong current use and a desire for improvement in
core work (e.g., coding, testing); high demand to reduce toil (e.g.,
documentation, operations); and clear limits for identity- and
relationship-centric work (e.g., mentoring). Priorities for responsible AI
support vary by context: reliability and security for systems-facing tasks;
transparency, alignment, and steerability to maintain control; and fairness and
inclusiveness for human-facing work. Our results offer concrete, contextual
guidance for delivering AI where it matters to developers and their work.

</details>


### [19] [Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning](https://arxiv.org/abs/2510.00881)
*Patrizio Migliarini,Mashal Afzal Memon,Marco Autili,Paola Inverardi*

Main category: cs.SE

TL;DR: 提出自动化框架评估16个LLM在零样本设置下的伦理推理能力，使用30个真实伦理场景，结果显示LLM在理论一致性和道德可接受性方面表现良好，支持其在SE工具中作为伦理推理引擎的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地集成到软件工程工具中，需要评估其在伦理推理方面的能力，特别是在不确定性和伦理重要情境下的判断能力。

Method: 使用30个真实伦理场景，在零样本设置下测试16个LLM，要求模型识别最适用的伦理理论、评估道德可接受性并解释推理过程，与专家选择进行比较。

Result: LLM平均理论一致性率为73.3%，道德可接受性二元一致率为86.7%，在伦理模糊案例中存在可解释的分歧，定性分析显示模型间存在强概念收敛。

Conclusion: LLM展现出足够的解释稳定性和理论一致推理能力，支持其作为SE流程中可扩展、可审计和自适应的伦理推理引擎的可行性。

Abstract: Large Language Models (LLMs) are increasingly integrated into software
engineering (SE) tools for tasks that extend beyond code synthesis, including
judgment under uncertainty and reasoning in ethically significant contexts. We
present a fully automated framework for assessing ethical reasoning
capabilities across 16 LLMs in a zero-shot setting, using 30 real-world
ethically charged scenarios. Each model is prompted to identify the most
applicable ethical theory to an action, assess its moral acceptability, and
explain the reasoning behind their choice. Responses are compared against
expert ethicists' choices using inter-model agreement metrics. Our results show
that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary
Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable
divergences concentrated in ethically ambiguous cases. A qualitative analysis
of free-text explanations reveals strong conceptual convergence across models
despite surface-level lexical diversity. These findings support the potential
viability of LLMs as ethical inference engines within SE pipelines, enabling
scalable, auditable, and adaptive integration of user-aligned ethical
reasoning. Our focus is the Ethical Interpreter component of a broader
profiling pipeline: we evaluate whether current LLMs exhibit sufficient
interpretive stability and theory-consistent reasoning to support automated
profiling.

</details>


### [20] [On Effective Semantic Translation for Code: A Study Based on Pseudocode](https://arxiv.org/abs/2510.00920)
*Songqiang Chen,Congying Xu,Jingyi Chen,Jialun Cao,Jiarong Wu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: 该论文提出基于伪代码的代码翻译方法，通过先将程序意图和逻辑转换为伪代码，再实现为目标编程语言，来提升大语言模型在代码翻译中的准确性。


<details>
  <summary>Details</summary>
Motivation: 直接代码到代码的翻译方法在处理复杂程序时存在挑战，受人类语义翻译启发，探索通过中间步骤（伪代码）来指导LLMs完成翻译任务。

Method: 采用基于伪代码的代码翻译方法，首先将程序解释为伪代码表示意图和逻辑，然后在目标编程语言中实现。通过对9,690个翻译任务在6种编程语言和5个流行LLMs上的实验，比较直接翻译和伪代码翻译方法。

Result: 基于伪代码的翻译能有效补充直接翻译，特别是在从灵活语言到严格语言的翻译或处理低资源Rust时效果显著。该方法能解耦复杂程序的翻译过程，减少原始程序实现细节的干扰。

Conclusion: 建议结合两种方法的互补优势来提升代码翻译准确性。基于伪代码的翻译存在伪代码不正确、不完整或模糊等局限性，但总体上能有效增强翻译效果。

Abstract: Large language models (LLMs) show great potential in code translation.
However, accurate translation remains challenging when using the commonly
adopted direct code-to-code translation approach, which converts a program into
the target programming language (PL) in a single step. Inspired by the success
of incorporating intermediate steps to guide LLMs in resolving challenging
tasks, we explore pseudocode-based code translation, which emulates the human
semantic translation by first interpreting the program's intent and logic into
pseudocode and then implementing it in the target PL. We find that
pseudocode-based translation helps translate programs that direct translation
struggles to handle. Nonetheless, the effectiveness, advantages, and
limitations of this approach remain underexplored. To bridge this gap, we
present an empirical study on pseudocode-based code translation, aiming to
investigate its effectiveness in enhancing the direct translation approach,
illuminate its effective usage, and identify limitations hindering its
potential benefits. By comparing direct and pseudocode-based translation
approaches on 9,690 translation tasks across six PLs with five popular LLMs, we
demonstrate that pseudocode-based translation can effectively complement direct
translation, particularly when translating from flexible to rigid PLs or
dealing with low-resource Rust. Based on these findings, we suggest adopting
strategies that combine the complementary strengths of both approaches to
enhance code translation accuracy. We also reveal the advantages of
pseudocode-based translation in disentangling translations of complicated
programs and mitigating distractions from detailed implementations in original
programs, as well as its limitations due to incorrect, incomplete, or ambiguous
pseudocode.

</details>


### [21] [ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions](https://arxiv.org/abs/2510.00946)
*Shiza Andleeb,Brandon Kantorski,Jeffrey C. Carver*

Main category: cs.SE

TL;DR: 研究ChatGPT对CS1课程中学生编程表现的影响，发现使用ChatGPT能提高代码质量和效率，但对概念理解的影响不一致。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT等大语言模型在编程教学中的普及，需要了解其对代码质量、概念理解、完成时间和学生认知的实际影响。

Method: 采用平衡设计的准实验研究，学生在两个C语言编程作业中交替使用ChatGPT和非ChatGPT条件，通过多维评分标准、概念后测调查和任务完成时间进行评估。

Result: 使用ChatGPT的学生代码质量评分显著更高，完成任务时间更短；概念理解方面，函数主题表现较差，结构主题表现较好；学生对ChatGPT体验积极，但担心准确性和长期技能发展。

Conclusion: ChatGPT能提升新手程序员的代码质量和效率，但未必能一致改善概念理解，建议结构化整合和补充教学策略以培养独立解决问题的能力。

Abstract: Background: Large language models (LLMs) such as ChatGPT are increasingly
used in introductory programming courses to provide real-time code generation,
debugging, and explanations. While these tools can boost productivity and code
quality, concerns remain about over-reliance and potential impacts on
conceptual learning. Objective: To investigate how ChatGPT access affects code
quality, conceptual understanding, task completion times, and student
perceptions in a CS1 course. Methods: We conducted a counterbalanced,
quasi-experimental study in which students alternated between ChatGPT and
non-ChatGPT conditions across two programming assignments in C (functions and
structures). We evaluated their code submissions using multidimensional
rubrics, conceptual post-surveys, and task completion time. Results: Students
who had access to ChatGPT produced significantly higher rubric scores for code
quality and completed tasks in less time compared to those without access.
However, gains in conceptual understanding were mixed, lower for the functions
topic but higher for the structures topic. Students reported positive
experiences with ChatGPT, citing its value for debugging and practice, while
expressing concerns about accuracy and long-term skill development.
Conclusions: ChatGPT can enhance code quality and efficiency for novice
programmers, but may not uniformly improve conceptual understanding. Structured
integration and complementary instructional strategies are recommended to
foster independent problem-solving skills.

</details>


### [22] [Enhancing Software Testing Education: Understanding Where Students Struggle](https://arxiv.org/abs/2510.00957)
*Shiza Andleeb,Teo Mendoza,Lucas Cordova,Gursimran Walia,Jeffrey C. Carver*

Main category: cs.SE

TL;DR: 分析学生在软件测试课程中的常见概念误解，发现决策覆盖率和异常处理是主要挑战，学生常做无效的表面修改


<details>
  <summary>Details</summary>
Motivation: 虽然自动化反馈工具广泛使用，但学生仍难以掌握测试概念，不清楚哪些概念最常被误解以及这些误解如何影响测试套件修订

Method: 在高级软件测试课程中使用自动化反馈工具，分析两个作业的学生提交，识别普遍概念差距和无效修改模式

Result: 决策覆盖率和异常处理是持续挑战，学生最常做表面或方法级别的修改，这些修改未能提高代码覆盖率

Conclusion: 研究结果为教育者、研究人员和工具设计者提供了可操作的见解，通过识别导致测试结果差的概念，可以改进反馈系统，针对性解决持续存在的误解

Abstract: Effective software testing is critical for producing reliable and secure
software, yet many computer science students struggle to master the
foundational concepts required to construct comprehensive test suites. While
automated feedback tools are widely used to support student learning, it
remains unclear which testing concepts are most frequently misunderstood and
how these misunderstandings are reflected in students' test suite revisions.
This study examines the specific testing concepts that lead students to make
ineffective changes, those that fail to improve code coverage, during test
suite development. Leveraging an automated feedback tool in a senior-level
software testing course, we analyzed student submissions from two assignments
to identify prevalent conceptual gaps and patterns of unproductive
modification. Our results reveal that decision coverage and exception handling
are persistent challenges, and that students most often make superficial or
method-level changes that do not enhance coverage. These findings provide
actionable insights for educators, researchers, and tool designers. By
pinpointing the concepts that most often contribute to poor testing outcomes,
we can refine feedback systems, target instruction to address persistent
misconceptions, and more effectively support students in developing robust,
maintainable test suites.

</details>


### [23] [Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework](https://arxiv.org/abs/2510.01002)
*Chengran Yang,Ting Zhang,Jinfeng Jiang,Xin Zhou,Haoye Tian,Jieke Shi,Junkai Chen,Yikun Li,Eng Lieh Ouh,Lwin Khin Shar,David Lo*

Main category: cs.SE

TL;DR: SeCuRepair是一个语义对齐、课程驱动和推理增强的漏洞修复框架，通过"推理-编辑"范式和语义感知的强化学习，显著提升了自动化漏洞修复的泛化能力和复杂修复性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的自动化漏洞修复方法在真实场景中泛化能力不足，存在跨仓库泛化能力有限、无法捕获长距离依赖关系、过度依赖表面词汇模式等三个根本性弱点。

Method: 采用"推理-编辑"范式，要求模型在生成补丁前明确阐述修复逻辑；使用语义感知的强化学习，奖励与标准补丁在语法和语义上对齐的补丁；实施难度感知的课程学习，从简单修复逐步过渡到复杂的多块协调编辑。

Result: 在BigVul和PrimeVul_AVR数据集上的严格仓库级分割评估中，SeCuRepair显著优于所有基线方法，在CodeBLEU指标上分别比最佳基线高出34.52%和31.52%。

Conclusion: SeCuRepair通过显式推理、语义对齐和渐进式学习有效解决了现有AVR方法的局限性，每个组件都对最终性能有重要贡献。

Abstract: Current learning-based Automated Vulnerability Repair (AVR) approaches, while
promising, often fail to generalize effectively in real-world scenarios. Our
diagnostic analysis reveals three fundamental weaknesses in state-of-the-art
AVR approaches: (1) limited cross-repository generalization, with performance
drops on unseen codebases; (2) inability to capture long-range dependencies,
causing a performance degradation on complex, multi-hunk repairs; and (3)
over-reliance on superficial lexical patterns, leading to significant
performance drops on vulnerabilities with minor syntactic variations like
variable renaming.
  To address these limitations, we propose SeCuRepair, a semantics-aligned,
curriculum-driven, and reasoning-enhanced framework for vulnerability repair.
At its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model
to articulate why and how a vulnerability should be fixed before generating the
patch. This explicit reasoning enforces a genuine understanding of repair logic
rather than superficial memorization of lexical patterns. SeCuRepair also moves
beyond traditional supervised fine-tuning and employs semantics-aware
reinforcement learning, rewarding patches for their syntactic and semantic
alignment with the oracle patch rather than mere token overlap. Complementing
this, a difficulty-aware curriculum progressively trains the model, starting
with simple fixes and advancing to complex, multi-hunk coordinated edits.
  We evaluate SeCuRepair on strict, repository-level splits of BigVul and newly
crafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all
baselines, surpassing the best-performing baselines by 34.52% on BigVul and
31.52% on PrimeVul\textsubscript{AVR} in terms of CodeBLEU, respectively.
Comprehensive ablation studies further confirm that each component of our
framework contributes to its final performance.

</details>


### [24] [Improving Code Localization with Repository Memory](https://arxiv.org/abs/2510.01003)
*Boshi Wang,Weijian Xu,Yunsheng Li,Mei Gao,Yujia Xie,Huan Sun,Dongdong Chen*

Main category: cs.SE

TL;DR: 该研究通过利用代码库的提交历史来增强语言代理的长期记忆能力，从而改进代码定位任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理代码库任务时通常从零开始，忽略了人类开发者自然积累的长期存储库记忆，如关键模块功能和bug类型与修复位置的关联。

Method: 引入工具使代理能够从非参数化记忆中检索信息，包括近期历史提交、关联问题以及通过提交模式识别的活跃代码部分的功能摘要。

Result: 实验表明，这种记忆增强显著提升了最先进的定位框架LocAgent在SWE-bench-verified和SWE-bench-live基准测试上的性能。

Conclusion: 该研究为开发能够积累和利用过去经验进行长期任务的代理做出了贡献，更接近模拟人类开发者的专业知识。

Abstract: Code localization is a fundamental challenge in repository-level software
engineering tasks such as bug fixing. While existing methods equip language
agents with comprehensive tools/interfaces to fetch information from the
repository, they overlook the critical aspect of memory, where each instance is
typically handled from scratch assuming no prior repository knowledge. In
contrast, human developers naturally build long-term repository memory, such as
the functionality of key modules and associations between various bug types and
their likely fix locations. In this work, we augment language agents with such
memory by leveraging a repository's commit history - a rich yet underutilized
resource that chronicles the codebase's evolution. We introduce tools that
allow the agent to retrieve from a non-parametric memory encompassing recent
historical commits and linked issues, as well as functionality summaries of
actively evolving parts of the codebase identified via commit patterns. We
demonstrate that augmenting such a memory can significantly improve LocAgent, a
state-of-the-art localization framework, on both SWE-bench-verified and the
more recent SWE-bench-live benchmarks. Our research contributes towards
developing agents that can accumulate and leverage past experience for
long-horizon tasks, more closely emulating the expertise of human developers.

</details>


### [25] [GenIA-E2ETest: A Generative AI-Based Approach for End-to-End Test Automation](https://arxiv.org/abs/2510.01024)
*Elvis Júnior,Alan Valejo,Jorge Valverde-Rebaza,Vânia de Oliveira Neves*

Main category: cs.SE

TL;DR: GenIA-E2ETest利用生成式AI从自然语言描述自动生成可执行的端到端测试脚本，在Web应用中验证了其有效性，平均元素指标达77%，执行精度82%，执行召回率85%，手动修改率仅10%。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的测试生成方案主要关注单元测试，未能解决端到端测试的挑战，而手动端到端测试耗时且易出错。

Method: 提出GenIA-E2ETest方法，利用生成式AI从自然语言描述自动生成可执行的端到端测试脚本。

Result: 在两个Web应用上评估显示：元素指标平均77%，执行精度82%，执行召回率85%，手动修改率仅10%，在典型Web场景中表现稳定。

Conclusion: GenIA-E2ETest是实用有效的端到端测试自动化解决方案，能显著减少手动工作量并扩大自动化测试的可及性，尽管对上下文相关导航和动态内容仍有一定敏感性。

Abstract: Software testing is essential to ensure system quality, but it remains
time-consuming and error-prone when performed manually. Although recent
advances in Large Language Models (LLMs) have enabled automated test
generation, most existing solutions focus on unit testing and do not address
the challenges of end-to-end (E2E) testing, which validates complete
application workflows from user input to final system response. This paper
introduces GenIA-E2ETest, which leverages generative AI to generate executable
E2E test scripts from natural language descriptions automatically. We evaluated
the approach on two web applications, assessing completeness, correctness,
adaptation effort, and robustness. Results were encouraging: the scripts
achieved an average of 77% for both element metrics, 82% for precision of
execution, 85% for execution recall, required minimal manual adjustments
(average manual modification rate of 10%), and showed consistent performance in
typical web scenarios. Although some sensitivity to context-dependent
navigation and dynamic content was observed, the findings suggest that
GenIA-E2ETest is a practical and effective solution to accelerate E2E test
automation from natural language, reducing manual effort and broadening access
to automated testing.

</details>


### [26] [CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code](https://arxiv.org/abs/2510.01077)
*Daniele Bifolco,Guido Annicchiarico,Pierluigi Barbiero,Massimiliano Di Penta,Fiorella Zampetti*

Main category: cs.SE

TL;DR: CodeGenLink是一个GitHub CoPilot扩展，用于链接LLM生成代码与相似代码来源，并提供许可证信息


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成代码缺乏可信度和可能违反版权/许可证的问题，因为缺少代码来源信息

Method: 结合LLM的网页搜索功能检索候选链接，然后对生成代码和检索代码进行相似性分析

Result: 初步结果显示CodeGenLink能有效通过相似性分析过滤无关链接，并在可用时提供许可证信息

Conclusion: CodeGenLink能够帮助开发者识别LLM生成代码的潜在来源和许可证信息

Abstract: Large Language Models (LLMs) are widely used in software development tasks
nowadays. Unlike reusing code taken from the Web, for LLMs' generated code,
developers are concerned about its lack of trustworthiness and possible
copyright or licensing violations, due to the lack of code provenance
information. This paper proposes CodeGenLink, a GitHub CoPilot extension for
Visual Studio Code aimed at (i) suggesting links containing code very similar
to automatically generated code, and (ii) whenever possible, indicating the
license of the likely origin of the code. CodeGenLink retrieves candidate links
by combining LLMs with their web search features and then performs similarity
analysis between the generated and retrieved code. Preliminary results show
that CodeGenLink effectively filters unrelated links via similarity analysis
and provides licensing information when available. Tool URL:
https://github.com/danielebifolco/CodeGenLink Tool Video:
https://youtu.be/M6nqjBf9_pw

</details>


### [27] [Developers' Perspectives on Software Licensing: Current Practices, Challenges, and Tools](https://arxiv.org/abs/2510.01096)
*Nathan Wintersgill,Trevor Stalnaker,Daniel Otten,Laura A. Heymann,Oscar Chaparro,Massimiliano Di Penta,Daniel M. German,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 该研究通过调查和访谈分析了开发者处理开源许可证合规的实践、挑战和工具使用情况，提出了15个关键发现和改进建议。


<details>
  <summary>Details</summary>
Motivation: 现代软件大量使用开源组件，许可证不合规会带来财务、法律和声誉风险，需要了解开发者如何处理许可证合规任务及其面临的挑战。

Method: 由软件工程和法律研究人员组成的联合团队，对58名软件开发者进行问卷调查，并进行了7次后续访谈。

Result: 研究得出了15个关于当前实践状态的关键发现，揭示了开发者在许可证合规方面的方法、挑战和工具使用情况。

Conclusion: 研究结果对许可证工具提出了可操作的建议，并为未来研究提供了方向，强调需要更好的工具支持来帮助开发者处理许可证合规问题。

Abstract: Most modern software products incorporate open-source components, requiring
development teams to maintain compliance with each component's licenses.
Noncompliance can lead to significant financial, legal, and reputational
repercussions. While some organizations may seek advice from legal
practitioners to assist with licensing tasks, developers still play a key role
in such a process. To this end, it is essential to understand how developers
approach license compliance tasks, the challenges they encounter, and the tools
that they use. This work studies these aspects of software licensing practices
through a study - conducted by a joint team of software engineering and legal
researchers - consisting of a survey with 58 software developers and seven
follow-up interviews. The study resulted in 15 key findings regarding the
current state of practice. We discuss the implications of our findings and
offer directions for future research as well as actionable recommendations for
licensing tools.

</details>


### [28] [When Shared Worlds Break: Demystifying Defects in Multi-User Extended Reality Software Systems](https://arxiv.org/abs/2510.01182)
*Shuqing Li,Chenran Zhang,Binchang Li,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 首个大规模多用户XR系统缺陷实证研究，分析了2649个真实bug报告，揭示了同步不一致和化身异常是最常见症状，网络/同步逻辑缺陷是主要根源，34%以上bug会导致严重破坏共享体验的后果。


<details>
  <summary>Details</summary>
Motivation: 多用户XR系统引入了独特的软件缺陷，影响用户体验，但相关研究仍不足。理解这些缺陷对于提升系统可靠性至关重要。

Method: 通过定性分析使用迭代开放编码，从开发者论坛、GitHub仓库和主流XR应用商店的应用评论中分析2649个真实bug报告，开发了包含症状表现、根源起源和后果严重性三个维度的综合分类法。

Result: 发现同步不一致和化身相关异常是最普遍症状，网络/同步逻辑缺陷和会话管理缺陷是主要根源。超过34%的bug会导致系统崩溃、持续断开连接和完全交互中断等严重后果，破坏共享体验。

Conclusion: 多用户XR系统在分布式系统、实时3D交互和沉浸式体验的交叉点面临独特挑战，需要专门的测试、调试和质量保证方法。基于研究发现为开发者、平台供应商和研究人员提供了可行建议。

Abstract: Multi-user Extended Reality (XR) systems enable transformative shared
experiences but introduce unique software defects that compromise user
experience. Understanding software defects in multi-user XR systems is crucial
for enhancing system reliability, yet remains underexplored. To fill the gap,
this paper presents the first large-scale empirical study of multi-user XR
defects, analyzing 2,649 real-world bug reports from diverse sources, including
developer forums, GitHub repositories, and app reviews on mainstream XR app
stores. Through rigorous qualitative analysis using iterative open coding, we
develop a comprehensive taxonomy that classifies multi-user XR bugs along three
dimensions: Symptom Manifestation, Root Cause Origin, and Consequence Severity.
Our findings reveal that synchronization inconsistencies and avatar-related
anomalies are the most prevalent symptoms, while network/synchronization logic
defects and session management flaws emerge as dominant root causes.
Critically, over 34% of analyzed bugs lead to severe consequences that
fundamentally break the shared experience, including system crashes, persistent
disconnections, and complete interaction breakdowns, etc. We also identify
concerning privacy and health implications unique to multi-user XR contexts.
Based on our findings of defect analysis, we provide actionable recommendations
for developers, platform vendors, and researchers. Our results demonstrate that
multi-user XR systems face distinct challenges at the intersection of
distributed systems, real-time 3D interaction, and immersive experiences,
necessitating specialized approaches to testing, debugging, and quality
assurance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [29] [Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training](https://arxiv.org/abs/2510.00183)
*Ween Yang,Jason Liu,Suli Wang,Xinyuan Song,Lynn Ai,Eric Yang,Tianyu Shi*

Main category: cs.DC

TL;DR: Lattica是一个去中心化的跨NAT通信框架，专门为分布式AI系统设计，通过NAT穿越、基于CRDT的去中心化数据存储和基于DHT的内容发现层，构建了可独立运行的完整协议栈。


<details>
  <summary>Details</summary>
Motivation: 分布式AI工作负载从集中式数据中心扩展到异构和无许可环境，面临NAT和防火墙的通信限制，现有解决方案要么专为数据中心设计，要么将机器学习逻辑与网络代码紧密耦合。

Method: 集成三个核心组件：1）NAT穿越机制建立全局可寻址的P2P网络；2）基于CRDT的去中心化数据存储确保可验证的最终一致性状态复制；3）基于DHT的内容发现层和优化的RPC协议实现高效模型同步。

Result: 构建了一个完整协议栈，支持主权、弹性和可扩展的AI系统，能够独立于集中式中介运行。

Conclusion: Lattica为边缘智能、协作强化学习等大规模分布式机器学习场景提供了直接适用的解决方案。

Abstract: The rapid expansion of distributed Artificial Intelligence (AI) workloads
beyond centralized data centers creates a demand for new communication
substrates. These substrates must operate reliably in heterogeneous and
permissionless environments, where Network Address Translators (NATs) and
firewalls impose significant constraints. Existing solutions, however, are
either designed for controlled data center deployments or implemented as
monolithic systems that tightly couple machine learning logic with networking
code. To address these limitations, we present Lattica, a decentralized
cross-NAT communication framework designed to support distributed AI systems.
Lattica integrates three core components. First, it employs a robust suite of
NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh.
Second, it provides a decentralized data store based on Conflict-free
Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent
state replication. Third, it incorporates a content discovery layer that
leverages distributed hash tables (DHTs) together with an optimized RPC
protocol for efficient model synchronization. By integrating these components,
Lattica delivers a complete protocol stack for sovereign, resilient, and
scalable AI systems that operate independently of centralized intermediaries.
It is directly applicable to edge intelligence, collaborative reinforcement
learning, and other large-scale distributed machine learning scenarios.

</details>


### [30] [FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.00207)
*Yunqi Gao,Bing Hu,Mahdi Boloursaz Mashhadi,A-Long Jin,Yanfeng Zhang,Pei Xiao,Rahim Tafazolli,Merouane Debbah*

Main category: cs.DC

TL;DR: FlowMoE是一个用于调度混合专家模型训练中多类型任务管道的可扩展框架，通过统一流水线和基于张量分块的优先级调度机制，显著提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注MoE层内的专家计算和all-to-all通信调度，而忽略了多头注意力计算、门控和all-reduce通信等其他关键操作，导致训练效率不高。

Method: 1. 构建统一流水线来一致调度MHA计算、门控、专家计算和A2A通信；2. 引入基于张量分块的优先级调度机制，将all-reduce通信与所有计算任务重叠。

Result: 在675个典型MoE层和4个真实世界MoE模型上的实验表明，FlowMoE相比最先进的MoE训练框架，训练时间减少13%-57%，能耗降低10%-39%，内存使用减少7%-32%。

Conclusion: FlowMoE是一个高效、通用的MoE训练框架，通过全面调度多类型任务管道，显著提升了训练效率和资源利用率。

Abstract: The parameter size of modern large language models (LLMs) can be scaled up
via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid
excessive increase of the computational costs. To further improve training
efficiency, pipelining computation and communication has become a promising
solution for distributed MoE training. However, existing work primarily focuses
on scheduling tasks within the MoE layer, such as expert computing and
all-to-all (A2A) communication, while neglecting other key operations including
multi-head attention (MHA) computing, gating, and all-reduce communication. In
this paper, we propose FlowMoE, a scalable framework for scheduling multi-type
task pipelines. First, FlowMoE constructs a unified pipeline to consistently
scheduling MHA computing, gating, expert computing, and A2A communication.
Second, FlowMoE introduces a tensor chunk-based priority scheduling mechanism
to overlap the all-reduce communication with all computing tasks. We implement
FlowMoE as an adaptive and generic framework atop PyTorch. Extensive
experiments with 675 typical MoE layers and four real-world MoE models across
two GPU clusters demonstrate that our proposed FlowMoE framework outperforms
state-of-the-art MoE training frameworks, reducing training time by 13%-57%,
energy consumption by 10%-39%, and memory usage by 7%-32%.

</details>


### [31] [BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains](https://arxiv.org/abs/2510.00306)
*Wenyang Jia,Jingjing Wang,Kai Lei*

Main category: cs.DC

TL;DR: BlockSDN-VC是一个基于SDN控制器的交易广播协议，通过集中化坐标计算和转发控制，显著降低区块链传播延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代区块链需要快速可靠的传播来平衡安全性和吞吐量，但现有的虚拟坐标方法依赖缓慢的迭代更新，导致节点不同步。

Method: 将坐标计算和转发控制集中到SDN控制器中，实现全局一致性、最小路径拉伸，并能快速响应节点变化或拥塞。

Result: 在分布式模拟中，中位延迟降低62%，收敛速度提升4倍，控制平面开销低于3%；在实际区块链环境中，确认交易吞吐量提升17%。

Conclusion: BlockSDN-VC无需修改现有客户端即可显著提升区块链性能，在对抗性工作负载下表现优异。

Abstract: Modern blockchains need fast, reliable propagation to balance security and
throughput. Virtual-coordinate methods speed dissemination but rely on slow
iterative updates, leaving nodes out of sync. We present BlockSDN-VC, a
transaction-broadcast protocol that centralises coordinate computation and
forwarding control in an SDN controller, delivering global consistency, minimal
path stretch and rapid response to churn or congestion. In geo-distributed
simulations, BlockSDN-VC cuts median latency by up to 62% and accelerates
convergence fourfold over state-of-the-art schemes with under 3% control-plane
overhead. In a real blockchain environment, BlockSDN-VC boosts
confirmed-transaction throughput by 17% under adversarial workloads, requiring
no modifications to existing clients.

</details>


### [32] [ThirstyFLOPS: Water Footprint Modeling and Analysis Toward Sustainable HPC Systems](https://arxiv.org/abs/2510.00471)
*Yankai Jiang,Raghavendra Kanakagiri,Rohan Basu Roy,Devesh Tiwari*

Main category: cs.DC

TL;DR: 提出了ThirstyFLOPS框架，用于分析高性能计算系统的水足迹，结合区域特定指标评估水资源消耗，为HPC系统规划提供可持续性指导。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统依赖水冷技术和能源消耗，水资源足迹研究相对不足，与碳排放关注度形成对比，需要全面评估HPC的水资源影响。

Method: 开发ThirstyFLOPS框架，整合区域特定指标（WUE、PUE、EWF），使用真实世界数据量化水消耗，分析四个代表性HPC系统案例。

Result: 通过分析Marconi、Fugaku、Polaris和Frontier系统，揭示了区域水资源短缺和核能策略对HPC可持续性的影响。

Conclusion: 该研究推动了水资源意识的环境友好计算基础设施发展，为HPC系统规划和管理提供了重要参考。

Abstract: High-performance computing (HPC) systems are becoming increasingly
water-intensive due to their reliance on water-based cooling and the energy
used in power generation. However, the water footprint of HPC remains
relatively underexplored-especially in contrast to the growing focus on carbon
emissions. In this paper, we present ThirstyFLOPS - a comprehensive water
footprint analysis framework for HPC systems. Our approach incorporates
region-specific metrics, including Water Usage Effectiveness, Power Usage
Effectiveness, and Energy Water Factor, to quantify water consumption using
real-world data. Using four representative HPC systems - Marconi, Fugaku,
Polaris, and Frontier - as examples, we provide implications for HPC system
planning and management. We explore the impact of regional water scarcity and
nuclear-based energy strategies on HPC sustainability. Our findings aim to
advance the development of water-aware, environmentally responsible computing
infrastructures.

</details>


### [33] [Towards Efficient VM Placement: A Two-Stage ACO-PSO Approach for Green Cloud Infrastructure](https://arxiv.org/abs/2510.00541)
*Ali M. Baydoun,Ahmed S. Zekri*

Main category: cs.DC

TL;DR: 提出了一种混合ACO-PSO算法(HAPSO)，用于绿色云数据中心中节能的虚拟机放置和迁移，通过两阶段优化显著降低了能耗和SLA违规。


<details>
  <summary>Details</summary>
Motivation: 数据中心能耗日益增长，需要可持续的资源管理方法来解决虚拟机放置和迁移的节能问题。

Method: 第一阶段使用蚁群优化(ACO)进行节能的初始虚拟机放置，确保全局可行性；第二阶段使用离散粒子群优化(PSO)通过迁移过载或低利用率主机上的虚拟机来优化分配。

Result: 在CloudSimPlus中的仿真显示，HAPSO相比经典启发式算法(BFD、FFD)、统一蚁群系统(UACS)和仅使用ACO的方法表现更优，能耗降低达25%，SLA违规减少18%。

Conclusion: 两阶段生物启发式混合方法能有效应对云资源管理的动态和多目标特性，在保持成本和碳排放稳定的同时实现显著节能效果。

Abstract: Datacenters consume a growing share of energy, prompting the need for
sustainable resource management. This paper presents a Hybrid ACO-PSO (HAPSO)
algorithm for energy-aware virtual machine (VM) placement and migration in
green cloud datacenters. In the first stage, Ant Colony Optimization (ACO)
performs energy-efficient initial placement across physical hosts, ensuring
global feasibility. In the second stage, a discrete Particle Swarm Optimization
(PSO) refines allocations by migrating VMs from overloaded or underutilized
hosts. HAPSO introduces several innovations: sequential hybridization of
metaheuristics, system-informed particle initialization using ACO output,
heuristic-guided discretization for constraint handling, and a multi-objective
fitness function that minimizes active servers and resource wastage.
Implemented in CloudSimPlus, extensive simulations demonstrate that HAPSO
consistently outperforms classical heuristics (BFD, FFD), Unified Ant Colony
System (UACS), and ACO-only. Notably, HAPSO achieves up to 25% lower energy
consumption and 18% fewer SLA violations compared to UACS at large-scale
workloads, while sustaining stable cost and carbon emissions. These results
highlight the effectiveness of two-stage bio-inspired hybridization in
addressing the dynamic and multi-objective nature of cloud resource management.

</details>


### [34] [ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training](https://arxiv.org/abs/2510.00606)
*Xueze Kang,Guangyu Xiang,Yuxin Wang,Hao Zhang,Yuchu Fang,Yuhang Zhou,Zhenheng Tang,Youhui Lv,Eliran Maman,Mark Wasserman,Alon Zameret,Zhipeng Bian,Shushu Chen,Zhiyou Yu,Jin Wang,Xiaoyu Wu,Yang Zheng,Chen Tian,Xiaowen Chu*

Main category: cs.DC

TL;DR: ElasWave是一个弹性原生的LLM训练系统，通过多维调度实现故障容错，在96个NPU上相比现有方法提升吞吐量1.35-1.60倍，通信恢复时间缩短82倍，迁移恢复时间减少51%，收敛偏差降低78%。


<details>
  <summary>Details</summary>
Motivation: 大规模LLM预训练需要处理频繁的硬件故障和弹性需求，现有系统无法同时保证参数一致性、低恢复时间、高吞吐量和计算一致性。

Method: 采用多维调度（图、数据流、频率、随机数生成），实现微批处理工作负载的动态调整，通过异步参数迁移和ZeRO分片交错，使用DVFS吸收流水线气泡，动态通信器支持原地通信组编辑。

Result: 在96个NPU上测试，吞吐量比ReCycle提升1.35倍，比TorchFT提升1.60倍；通信恢复在1秒内完成；迁移恢复时间最多减少51%；收敛偏差降低约78%。

Conclusion: ElasWave成功实现了弹性训练系统的四个关键目标，为大规模LLM训练提供了高效的故障容错和弹性扩展解决方案。

Abstract: Large-scale LLM pretraining today spans $10^{5}$--$10^{6}$ accelerators,
making failures commonplace and elasticity no longer optional. We posit that an
elastic-native training system must simultaneously ensure (i) Parameter
Consistency, (ii) low Mean Time to Recovery (MTTR), (iii) high post-change
Throughput, and (iv) Computation Consistency. This objective set not has never
been jointly attained by prior work. To achieve these goals, we present
ElasWave, which provides per-step fault tolerance via multi-dimensional
scheduling across Graph, Dataflow, Frequency, and Random Number Generation.
ElasWave resizes and reshards micro-batch workloads while preserving the global
batch size and gradient scale; it performs online pipeline resharding with
asynchronous parameter migration, interleaving ZeRO partitions so recovery
reduces to disjoint rank-to-rank transfers. It further uses DVFS to absorb
pipeline bubbles and reshards RNG to keep consistent computations. A dynamic
communicator enables in-place communication group edits, while per-step
in-memory snapshots support online verification and redistribution. We
evaluated ElasWave on 96 NPUs and benchmarked against state-of-the-art
baselines: throughput improves by $1.35\times$ over ReCycle and $1.60\times$
over TorchFT; communicator recovery completes within one second (up to
$82\times/3.6\times$ faster than full/partial rebuilds); migration MTTR drops
by as much as $51\%$; and convergence deviation is reduced by approximately
$78\%$.

</details>


### [35] [Net-Zero 6G from Earth to Orbit: Sustainable Design of Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2510.00678)
*Muhammad Ali Jamshed,Malik Muhammad Saad,Muhammad Ahmed Mohsin,Dongkyun Kim,Octavia A. Dobre,Halim Yanikomeroglu,Lina Mohjazi*

Main category: cs.DC

TL;DR: 该论文首次全面概述了在集成地面网络和非地面网络系统中实现净零能耗目标的设计挑战，提出了支持此类网络能源需求的关键使能技术，并利用AI提供可适应不同部署场景的能效解决方案。


<details>
  <summary>Details</summary>
Motivation: 地面网络和非地面网络的集成为弥合数字鸿沟和实现6G及更高代的真正无处不在连接至关重要，但这种集成由于系统特性和操作环境的多样性带来了显著的能源挑战。

Method: 论文概述了实现净零能耗目标的设计挑战，提出了一套关键使能技术，并通过AI用例分析提供了跨不同部署场景的可适应能效解决方案。

Result: 论文提供了集成TN和NTN系统实现净零能耗的全面框架，包括设计挑战分析、关键使能技术识别以及AI驱动的能效提升用例。

Conclusion: 论文强调了集成TN和NTN系统可持续演进的有前景研究方向，为实现净零能耗目标提供了指导性框架。

Abstract: The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks
(NTN) plays a crucial role in bridging the digital divide and enabling Sixth
Generation (6G) and beyond to achieve truly ubiquitous connectivity. However,
combining TN and NTN introduces significant energy challenges due to the
diverse characteristics and operational environments of these systems. In this
paper, we present for the first time a comprehensive overview of the design
challenges associated with achieving Net-Zero energy targets in integrated TN
and NTN systems. We outline a set of key enabling technologies that can support
the energy demands of such networks while aligning with Net-Zero objectives. To
enhance the Energy Efficiency (EE) of integrated TN and NTN systems, we provide
a use case analysis that leverages Artificial Intelligence (AI) to deliver
adaptable solutions across diverse deployment scenarios. Finally, we highlight
promising research directions that can guide the sustainable evolution of
integrated TN and NTN.

</details>


### [36] [Decentralized and Self-adaptive Core Maintenance on Temporal Graphs](https://arxiv.org/abs/2510.00758)
*Davide Rucci,Emanuele Carlini,Patrizio Dazzi,Hanna Kavalionak,Matteo Mordacchini*

Main category: cs.DC

TL;DR: 提出了一种新颖的分布式增量算法，用于计算时序网络的核分解，通过利用先前计算的核值来减少节点激活和消息交换量，在精度损失最小的情况下实现可扩展性。


<details>
  <summary>Details</summary>
Motivation: 图基问题在理解网络拓扑和发现同质及时序数据中的相似模式方面起着核心作用。这些模式可以通过分析节点形成的社区来揭示，而时序k核可以有效地建模这些社区。分布式解决方案利用网络节点本地通信和协调的能力，以可扩展、自适应和及时的方式解决复杂问题。

Method: 开发了一种分布式增量算法，利用先前计算的核值来减少网络随时间变化时的节点激活和消息交换量。该方法通过本地通信和协调实现可扩展性。

Result: 在具有不同动态水平的大型真实世界网络上的实验评估表明，与最先进的方法相比，该解决方案在活动节点数量、通信开销和收敛速度方面具有更高的效率。

Conclusion: 所提出的分布式增量算法能够有效计算时序网络的核分解，在保持高精度的同时显著提高了可扩展性和效率，特别适用于动态网络环境。

Abstract: Key graph-based problems play a central role in understanding network
topology and uncovering patterns of similarity in homogeneous and temporal
data. Such patterns can be revealed by analyzing communities formed by nodes,
which in turn can be effectively modeled through temporal $k$-cores. This paper
introduces a novel decentralized and incremental algorithm for computing the
core decomposition of temporal networks. Decentralized solutions leverage the
ability of network nodes to communicate and coordinate locally, addressing
complex problems in a scalable, adaptive, and timely manner. By leveraging
previously computed coreness values, our approach significantly reduces the
activation of nodes and the volume of message exchanges when the network
changes over time. This enables scalability with only a minimal trade-off in
precision. Experimental evaluations on large real-world networks under varying
levels of dynamism demonstrate the efficiency of our solution compared to a
state-of-the-art approach, particularly in terms of active nodes, communication
overhead, and convergence speed.

</details>


### [37] [CGSim: A Simulation Framework for Large Scale Distributed Computing Environment](https://arxiv.org/abs/2510.00822)
*Sairam Sri Vatsavai,Raees Khan,Kuan-Chieh Hsu,Ozgur O. Kilic,Paul Nilsson,Tatiana Korchuganova,David K. Park,Sankha Dutta,Yihui Ren,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Verena Ingrid Martinez,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: CGSim是一个用于大规模分布式计算环境的新型仿真框架，解决了现有仿真器在可扩展性、算法灵活性、实时监控和机器学习数据集生成方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模分布式计算基础设施仿真工具存在可扩展性有限、算法固化、缺乏实时监控以及无法生成适合现代机器学习方法的数据集等问题，需要开发更先进的仿真框架。

Method: 基于经过验证的SimGrid仿真框架构建，提供高级抽象来建模异构网格环境，同时保持准确性和可扩展性。关键特性包括模块化插件机制、交互式实时可视化仪表板以及自动生成适合AI辅助性能建模的事件级数据集。

Result: 使用生产级ATLAS PanDA工作负载进行综合评估，显示在WLCG计算站点上实现了显著的校准精度改进。可扩展性实验显示多站点仿真具有接近线性的扩展性，分布式工作负载相比单站点执行实现了6倍的性能提升。

Conclusion: 该框架使研究人员能够在商品硬件上以实用时间预算约束模拟具有数百个站点和数千个并发作业的WLCG规模基础设施。

Abstract: Large-scale distributed computing infrastructures such as the Worldwide LHC
Computing Grid (WLCG) require comprehensive simulation tools for evaluating
performance, testing new algorithms, and optimizing resource allocation
strategies. However, existing simulators suffer from limited scalability,
hardwired algorithms, lack of real-time monitoring, and inability to generate
datasets suitable for modern machine learning approaches. We present CGSim, a
simulation framework for large-scale distributed computing environments that
addresses these limitations. Built upon the validated SimGrid simulation
framework, CGSim provides high-level abstractions for modeling heterogeneous
grid environments while maintaining accuracy and scalability. Key features
include a modular plugin mechanism for testing custom workflow scheduling and
data movement policies, interactive real-time visualization dashboards, and
automatic generation of event-level datasets suitable for AI-assisted
performance modeling. We demonstrate CGSim's capabilities through a
comprehensive evaluation using production ATLAS PanDA workloads, showing
significant calibration accuracy improvements across WLCG computing sites.
Scalability experiments show near-linear scaling for multi-site simulations,
with distributed workloads achieving 6x better performance compared to
single-site execution. The framework enables researchers to simulate WLCG-scale
infrastructures with hundreds of sites and thousands of concurrent jobs within
practical time budget constraints on commodity hardware.

</details>


### [38] [Data Management System Analysis for Distributed Computing Workloads](https://arxiv.org/abs/2510.00828)
*Kuan-Chieh Hsu,Sairam Sri Vatsavai,Ozgur O. Kilic,Tatiana Korchuganova,Paul Nilsson,Sankha Dutta,Yihui Ren,David K. Park,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Raees Khan,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Verena Ingrid Martinez Outschoorn,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: ATLAS实验的PanDA工作流系统和Rucio数据管理系统在协同运行时存在系统性低效问题，本文通过开发元数据匹配算法连接两个系统，识别异常传输模式，并提出优化策略来改善资源利用和减少不必要的数据移动。


<details>
  <summary>Details</summary>
Motivation: ATLAS实验的PanDA工作流系统和Rucio数据管理系统各自高度优化，但在全球规模协同运行时暴露系统性低效，包括资源利用不足、冗余传输和错误分布改变等问题，缺乏共享性能感知和协调策略。

Method: 开发元数据匹配算法，在文件级别连接PanDA作业和Rucio数据集，获得数据访问和移动的完整细粒度视图，识别违反PanDA数据中心作业分配原则的异常传输模式。

Result: 识别出空间和时间上不平衡的传输活动，发现异常传输模式，并提出了缓解这些模式的策略。

Conclusion: 通过更紧密的PanDA-Rucio协调可以改善资源利用率、减少不必要的数据移动，并增强整体系统弹性，为两个系统的协同优化指明了路径。

Abstract: Large-scale international collaborations such as ATLAS rely on globally
distributed workflows and data management to process, move, and store vast
volumes of data. ATLAS's Production and Distributed Analysis (PanDA) workflow
system and the Rucio data management system are each highly optimized for their
respective design goals. However, operating them together at global scale
exposes systemic inefficiencies, including underutilized resources, redundant
or unnecessary transfers, and altered error distributions. Moreover, PanDA and
Rucio currently lack shared performance awareness and coordinated, adaptive
strategies.
  This work charts a path toward co-optimizing the two systems by diagnosing
data-management pitfalls and prioritizing end-to-end improvements. With the
observation of spatially and temporally imbalanced transfer activities, we
develop a metadata-matching algorithm that links PanDA jobs and Rucio datasets
at the file level, yielding a complete, fine-grained view of data access and
movement. Using this linkage, we identify anomalous transfer patterns that
violate PanDA's data-centric job-allocation principle. We then outline
mitigation strategies for these patterns and highlight opportunities for
tighter PanDA-Rucio coordination to improve resource utilization, reduce
unnecessary data movement, and enhance overall system resilience.

</details>


### [39] [Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead](https://arxiv.org/abs/2510.00833)
*Thanh Linh Nguyen,Marcela Tuler de Oliveira,An Braeken,Aaron Yi Ding,Quoc-Viet Pham*

Main category: cs.DC

TL;DR: 本文提出了veriFUL框架，用于解决联邦学习中数据遗忘的可验证性问题，确保客户数据影响被可靠移除。


<details>
  <summary>Details</summary>
Motivation: 当前的联邦学习遗忘机制缺乏可靠验证方法，客户无法确认其数据影响是否真正被移除，这违背了隐私法规的"被遗忘权"要求。

Method: 提出veriFUL参考框架，形式化验证实体、目标、方法和指标，整合现有工作并贡献新见解、概念和度量标准。

Result: 建立了可验证联邦遗忘的框架基础，明确了验证在联邦学习生命周期中的关键地位。

Conclusion: 可验证遗忘是联邦学习在高度监管和数据敏感应用中的关键需求，veriFUL为此领域提供了系统化解决方案和发展方向。

Abstract: Federated unlearning (FUL) enables removing the data influence from the model
trained across distributed clients, upholding the right to be forgotten as
mandated by privacy regulations. FUL facilitates a value exchange where clients
gain privacy-preserving control over their data contributions, while service
providers leverage decentralized computing and data freshness. However, this
entire proposition is undermined because clients have no reliable way to verify
that their data influence has been provably removed, as current metrics and
simple notifications offer insufficient assurance. We envision unlearning
verification becoming a pivotal and trust-by-design part of the FUL life-cycle
development, essential for highly regulated and data-sensitive services and
applications like healthcare. This article introduces veriFUL, a reference
framework for verifiable FUL that formalizes verification entities, goals,
approaches, and metrics. Specifically, we consolidate existing efforts and
contribute new insights, concepts, and metrics to this domain. Finally, we
highlight research challenges and identify potential applications and
developments for verifiable FUL and veriFUL.

</details>


### [40] [An Efficient, Reliable and Observable Collective Communication Library in Large-scale GPU Training Clusters](https://arxiv.org/abs/2510.00991)
*Ziteng Chen,Xiaohe Hu,Menghao Zhang,Yanmin Jia,Yan Zhang,Mingjun Zhang,Da Liu,Fangzheng Jiao,Jun Chen,He Liu,Aohan Zeng,Shuaixing Duan,Ruya Gu,Yang Jing,Bowen Han,Jiahao Cao,Wei Chen,Wenqi Xie,Jinlong Hou,Yuan Cheng,Bohua Xu,Mingwei Xu,Chunming Hu*

Main category: cs.DC

TL;DR: ICCL是一个针对大规模LLM训练的高效、可靠、可观测的集合通信库，解决了NCCL在生产环境中遇到的P2P通信效率低、RNIC端口故障容忍度差和通信异常观测不足等问题。


<details>
  <summary>Details</summary>
Motivation: 在大规模GPU训练集群中使用NCCL时面临三个主要挑战：1）P2P通信效率低且复杂，2）对频繁RNIC端口故障容忍度差，3）对瞬时集合通信异常的可观测性不足。

Method: ICCL将P2P通信从GPU内核卸载到CPU线程以减少SM消耗，消除与实际通信过程无关的冗余内存拷贝；引入主备QP机制容忍NIC端口故障；设计基于窗口的监控器在微秒级别观测网络异常。

Result: 开源ICCL并在生产训练集群中部署数月，相比NCCL，ICCL在P2P吞吐量/延迟上分别提升23.4%/28.5%，训练吞吐量提升6.02%。

Conclusion: ICCL成功解决了NCCL在大规模LLM训练中的关键问题，提供了高效、可靠、可观测的集合通信解决方案，并分享了在大规模集群中的运营经验。

Abstract: Large-scale LLM training requires collective communication libraries to
exchange data among distributed GPUs. As a company dedicated to building and
operating large-scale GPU training clusters, we encounter several challenges
when using NCCL in production, including 1) limited efficiency with costly and
cumbersome P2P communication, 2) poor tolerance to frequent RNIC port failures,
and 3) insufficient observability of transient collective communication
anomalies. To address these issues, we propose ICCL, an efficient, reliable,
and observable collective communication library in large-scale GPU training
clusters. ICCL offloads the P2P communication from GPU kernels to CPU threads
for minimal SM consumption, and removes the redundant memory copies irrelevant
to the actual communicating process. ICCL also introduces a primary-backup QP
mechanism to tolerate frequent NIC port failures, and designs a window-based
monitor to observe network anomalies at O(us) level. We open-source ICCL and
deploy it in production training clusters for several months, with results
showing that compared to NCCL, ICCL achieves a 23.4%/28.5% improvement in P2P
throughput/latency as well as a 6.02% increase in training throughput. We also
share the operating experience of ICCL in large-scale clusters, hoping to give
the communities more insights on production-level collective communication
libraries in LLM training.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [41] [AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents](https://arxiv.org/abs/2510.00039)
*Hossein Sholehrasa,Amirhossein Ghanaatian,Doina Caragea,Lisa A. Tell,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.DB

TL;DR: AutoPK是一个两阶段框架，用于从复杂科学表格中准确提取药代动力学数据，通过LLM识别参数变体并重构标准化表格，显著提升精确度和召回率。


<details>
  <summary>Details</summary>
Motivation: 药代动力学数据通常嵌入在结构复杂、术语不一致的表格中，给自动化数据提取和标准化带来巨大挑战。

Method: 第一阶段使用LLM识别和提取PK参数变体，结合混合相似度度量和LLM验证；第二阶段过滤相关行，将表格转换为键值文本格式，再用LLM重构标准化表格。

Result: 在605个真实PK表格数据集上，AutoPK显著优于直接使用LLM的基线方法。使用LLaMA 3.1-70B时，半衰期和清除率的F1分数分别达到0.92和0.91。开源模型Gemma 3-27B在多个PK参数上甚至优于GPT-4o Mini。

Conclusion: AutoPK能够实现可扩展且高置信度的PK数据提取，适用于兽医药理学、药物安全监测和公共卫生决策等关键应用，同时解决了表格结构异质性和术语不一致的问题。

Abstract: Pharmacokinetics (PK) plays a critical role in drug development and
regulatory decision-making for human and veterinary medicine, directly
affecting public health through drug safety and efficacy assessments. However,
PK data are often embedded in complex, heterogeneous tables with variable
structures and inconsistent terminologies, posing significant challenges for
automated PK data retrieval and standardization. AutoPK, a novel two-stage
framework for accurate and scalable extraction of PK data from complex
scientific tables. In the first stage, AutoPK identifies and extracts PK
parameter variants using large language models (LLMs), a hybrid similarity
metric, and LLM-based validation. The second stage filters relevant rows,
converts the table into a key-value text format, and uses an LLM to reconstruct
a standardized table. Evaluated on a real-world dataset of 605 PK tables,
including captions and footnotes, AutoPK shows significant improvements in
precision and recall over direct LLM baselines. For instance, AutoPK with LLaMA
3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance
parameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and
0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with
AutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's
hallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled
open-source models like Gemma 3-27B to outperform commercial systems such as
GPT-4o Mini on several PK parameters. AutoPK enables scalable and
high-confidence PK data extraction, making it well-suited for critical
applications in veterinary pharmacology, drug safety monitoring, and public
health decision-making, while addressing heterogeneous table structures and
terminology and demonstrating generalizability across key PK parameters. Code
and data: https://github.com/hosseinsholehrasa/AutoPK

</details>


### [42] [Data Quality Taxonomy for Data Monetization](https://arxiv.org/abs/2510.00089)
*Eduardo Vyhmeister,Bastien Pietropoli,Andrea Visentin*

Main category: cs.DB

TL;DR: 提出了一个用于数据货币化背景下数据质量评估的综合分类法，通过系统文献综述开发，将100多个指标和KPI组织到平衡计分卡框架的四个子集群中。


<details>
  <summary>Details</summary>
Motivation: 弥合细粒度技术评估与高层决策之间的差距，为数据质量管理与可持续价值创造提供可扩展的、基于证据的参考框架。

Method: 通过系统文献综述开发分类法，将指标组织到平衡计分卡框架的四个子集群（基础、情境、分辨率和专业）中。

Result: 创建了一个综合分类法，将数据质量作为战略连接器定位在平衡计分卡的四个视角中，展示了质量指标如何支撑估值准确性、客户信任、运营效率和创新能力。

Conclusion: 该整体方法提供了一个可扩展的、基于证据的参考框架，使从业者、数据管理者和战略家能够将数据质量管理与可持续价值创造对齐。

Abstract: This chapter presents a comprehensive taxonomy for assessing data quality in
the context of data monetisation, developed through a systematic literature
review. Organising over one hundred metrics and Key Performance Indicators
(KPIs) into four subclusters (Fundamental, Contextual, Resolution, and
Specialised) within the Balanced Scorecard (BSC) framework, the taxonomy
integrates both universal and domain-specific quality dimensions. By
positioning data quality as a strategic connector across the BSC's Financial,
Customer, Internal Processes, and Learning & Growth perspectives, it
demonstrates how quality metrics underpin valuation accuracy, customer trust,
operational efficiency, and innovation capacity. The framework's interconnected
"metrics layer" ensures that improvements in one dimension cascade into others,
maximising strategic impact. This holistic approach bridges the gap between
granular technical assessment and high-level decision-making, offering
practitioners, data stewards, and strategists a scalable, evidence-based
reference for aligning data quality management with sustainable value creation.

</details>


### [43] [EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases](https://arxiv.org/abs/2510.00549)
*Kwanhyung Lee,Sungsoo Hong,Joonhyung Park,Jeonghyeop Lim,Juhwan Choi,Donghwee Yoon,Eunho Yang*

Main category: cs.DB

TL;DR: EMR-AGENT是一个基于智能体的框架，使用语言模型驱动的交互来自动化提取和标准化临床数据，替代了传统的手工规则编写流程。


<details>
  <summary>Details</summary>
Motivation: 当前临床预测模型依赖从电子病历中提取结构化数据，但这一过程仍由硬编码、特定于数据库的流水线主导，限制了可扩展性、可重复性和跨机构泛化能力。

Method: 通过模块化智能体迭代观察查询结果并推理模式和文档，使用SQL不仅用于数据检索，还作为数据库观察和决策工具，自动化队列选择、特征提取和代码映射。

Result: 在三个EMR数据库（MIMIC-III、eICU、SICdb）上的基准测试显示，该方法在这些数据库上表现出强大的性能和泛化能力。

Conclusion: 该方法证明了自动化先前被认为需要专家驱动设计的过程的可行性，代码将公开发布。

Abstract: Machine learning models for clinical prediction rely on structured data
extracted from Electronic Medical Records (EMRs), yet this process remains
dominated by hardcoded, database-specific pipelines for cohort definition,
feature selection, and code mapping. These manual efforts limit scalability,
reproducibility, and cross-institutional generalization. To address this, we
introduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an
agent-based framework that replaces manual rule writing with dynamic, language
model-driven interaction to extract and standardize structured clinical data.
Our framework automates cohort selection, feature extraction, and code mapping
through interactive querying of databases. Our modular agents iteratively
observe query results and reason over schema and documentation, using SQL not
just for data retrieval but also as a tool for database observation and
decision making. This eliminates the need for hand-crafted, schema-specific
logic. To enable rigorous evaluation, we develop a benchmarking codebase for
three EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen
schema settings. Our results demonstrate strong performance and generalization
across these databases, highlighting the feasibility of automating a process
previously thought to require expert-driven design. The code will be released
publicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a
demonstration, please visit our anonymous demo page:
https://anonymoususer-max600.github.io/EMR_AGENT/

</details>
