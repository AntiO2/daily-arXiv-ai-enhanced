{"id": "2510.06387", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06387", "abs": "https://arxiv.org/abs/2510.06387", "authors": ["Raaghav Ravishankar", "Sandeep Kulkarni", "Sathya Peri", "Gokarna Sharma"], "title": "DiLi: A Lock-Free Asynchronously Distributable Linked List", "comment": null, "summary": "Modern databases use dynamic search structures that store a huge amount of\ndata, and often serve them using multi-threaded algorithms to support the\never-increasing throughput needs. When this throughput need exceeds the\ncapacity of the machine hosting the structure, one either needs to replace the\nunderlying hardware (an option that is typically not viable and introduces a\nlong down time) or make the data structure distributed. Static partitioning of\nthe data structure for distribution is not desirable, as it is prone to uneven\nload distribution over time, and having to change the partitioning scheme later\nwill require downtime.\n  Since a distributed data structure, inherently, relies on communication\nsupport from the network stack and operating systems, we introduce the notion\nof conditional lock-freedom that extends the notion of lock-free computation\nwith reasonable assumptions about communication between processes. We present\nDiLi, a conditional lock-free, linearizable, and distributable linked list that\ncan be asynchronously and dynamically (1) partitioned into multiple sublists\nand (2) load balanced by distributing sublists across multiple machines. DiLi\ncontains primitives for these that also maintain the lock-free property of the\nunderlying search structure that supports find, remove, and insert of a key as\nthe client operations.\n  Searching for an item in DiLi is by a novel traversal that involves a binary\nsearch on the partitioning scheme, and then a linear traversal on a limitable\nnumber of linked nodes. As a result, we are able to empirically show that DiLi\nperforms as well as the state-of-the-art lock-free concurrent search structures\nthat are based off of a linked list when executed on a single-machine. We also\nshow that the throughput of DiLi scales linearly with the number of machines\nthat host it."}
{"id": "2510.06396", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06396", "abs": "https://arxiv.org/abs/2510.06396", "authors": ["Aymen Alsaadi", "Jonathan Ash", "Mikhail Titov", "Matteo Turilli", "Andre Merzky", "Shantenu Jha", "Sagar Khare"], "title": "Adaptive Protein Design Protocols and Middleware", "comment": "N/A", "summary": "Computational protein design is experiencing a transformation driven by\nAI/ML. However, the range of potential protein sequences and structures is\nastronomically vast, even for moderately sized proteins. Hence, achieving\nconvergence between generated and predicted structures demands substantial\ncomputational resources for sampling. The Integrated Machine-learning for\nProtein Structures at Scale (IMPRESS) offers methods and advanced computing\nsystems for coupling AI to high-performance computing tasks, enabling the\nability to evaluate the effectiveness of protein designs as they are developed,\nas well as the models and simulations used to generate data and train models.\nThis paper introduces IMPRESS and demonstrates the development and\nimplementation of an adaptive protein design protocol and its supporting\ncomputing infrastructure. This leads to increased consistency in the quality of\nprotein design and enhanced throughput of protein design due to dynamic\nresource allocation and asynchronous workload execution."}
{"id": "2510.06404", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06404", "abs": "https://arxiv.org/abs/2510.06404", "authors": ["Raaghav Ravishankar", "Sandeep Kulkarni", "Nitin H Vaidya"], "title": "MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases", "comment": null, "summary": "We focus on the problem of checkpointing in fully replicated weakly\nconsistent distributed databases, which we refer to as Distributed Transaction\nConsistent Snapshot (DTCS). A typical example of such a system is a main-memory\ndatabase that provides strong eventual consistency. This problem is important\nand challenging for several reasons: (1) eventual consistency often creates\nanomalies that the users do not anticipate. Hence, frequent checkpoints to\nascertain desired invariants is highly beneficial in their use, and (2)\ntraditional checkpoints lead to significant overhead and/or inconsistencies. By\nshowing that the traditional checkpoint leads to inconsistencies or excessive\noverhead, we define the notion of size-minimal checkpointing for fully\nreplicated databases. We present an algorithm for checkpointing with minimal\ncheckpointing overhead (only O(n) new messages and addition of a single counter\nfor existing messages). It also provides a significant benefit over existing\ncheckpointing algorithms for distributed systems and main-memory databases.\n  A key benefit of DTCS is that it summarizes the computation by a sequence of\nsnapshots that are strongly consistent even though the underlying computation\nis weakly consistent. In essence, when anomalies arise in an eventually\nconsistent system, DTCS enables one to concentrate solely on the snapshots\nsurrounding the time point of the anomaly."}
{"id": "2510.06675", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.06675", "abs": "https://arxiv.org/abs/2510.06675", "authors": ["Xu Bai", "Muhammed Tawfiqul Islam", "Rajkumar Buyya", "Adel N. Toosi"], "title": "REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum", "comment": "10 pages, 10 figures", "summary": "Cloud computing, despite its advantages in scalability, may not always fully\nsatisfy the low-latency demands of emerging latency-sensitive pervasive\napplications. The cloud-edge continuum addresses this by integrating the\nresponsiveness of edge resources with cloud scalability. Microservice\nArchitecture (MSA) characterized by modular, loosely coupled services, aligns\neffectively with this continuum. However, the heterogeneous and dynamic\ncomputing resource poses significant challenges to the optimal placement of\nmicroservices. We propose REACH, a novel rescheduling algorithm that\ndynamically adapts microservice placement in real time using reinforcement\nlearning to react to fluctuating resource availability, and performance\nvariations across distributed infrastructures. Extensive experiments on a\nreal-world testbed demonstrate that REACH reduces average end-to-end latency by\n7.9%, 10%, and 8% across three benchmark MSA applications, while effectively\nmitigating latency fluctuations and spikes."}
{"id": "2510.06343", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.06343", "abs": "https://arxiv.org/abs/2510.06343", "authors": ["Fikret Mert GÃ¼ltekin", "Oscar Lilja", "Ranim Khojah", "Rebekka Wohlrab", "Marvin Damschen", "Mazen Mohamad"], "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems", "comment": "Accepted at Autonomous Agents in Software Engineering (AgenticSE)\n  Workshop, co-located with ASE 2025", "summary": "In safety-critical software systems, cybersecurity activities become\nessential, with risk assessment being one of the most critical. In many\nsoftware teams, cybersecurity experts are either entirely absent or represented\nby only a small number of specialists. As a result, the workload for these\nexperts becomes high, and software engineers would need to conduct\ncybersecurity activities themselves. This creates a need for a tool to support\ncybersecurity experts and engineers in evaluating vulnerabilities and threats\nduring the risk assessment process. This paper explores the potential of\nleveraging locally hosted large language models (LLMs) with retrieval-augmented\ngeneration to support cybersecurity risk assessment in the forestry domain\nwhile complying with data protection and privacy requirements that limit\nexternal data sharing. We performed a design science study involving 12 experts\nin interviews, interactive sessions, and a survey within a large-scale project.\nThe results demonstrate that LLMs can assist cybersecurity experts by\ngenerating initial risk assessments, identifying threats, and providing\nredundancy checks. The results also highlight the necessity for human oversight\nto ensure accuracy and compliance. Despite trust concerns, experts were willing\nto utilize LLMs in specific evaluation and assistance roles, rather than solely\nrelying on their generative capabilities. This study provides insights that\nencourage the use of LLM-based agents to support the risk assessment process of\ncyber-physical systems in safety-critical domains."}
{"id": "2510.06414", "categories": ["cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06414", "abs": "https://arxiv.org/abs/2510.06414", "authors": ["Abdur Rehman Anwar Qureshi", "Adrian Rebmann", "Timotheus Kampik", "Matthias Weidlich", "Mathias Weske"], "title": "Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation", "comment": null, "summary": "Business process management is increasingly practiced using data-driven\napproaches. Still, classical imperative process models, which are typically\nformalized using Petri nets, are not straightforwardly applicable to the\nrelational databases that contain much of the available structured process\nexecution data. This creates a gap between the traditional world of process\nmodeling and recent developments around data-driven process analysis,\nultimately leading to the under-utilization of often readily available process\nmodels. In this paper, we close this gap by providing an approach for\ntranslating imperative models into relaxed process data queries, specifically\nSQL queries executable on relational databases, for conformance checking. Our\nresults show the continued relevance of imperative process models to\ndata-driven process management, as well as the importance of behavioral\nfootprints and other declarative approaches for integrating model-based and\ndata-driven process management."}
{"id": "2510.06882", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.06882", "abs": "https://arxiv.org/abs/2510.06882", "authors": ["Boris Sedlak", "Philipp Raith", "Andrea Morichetta", "VÃ­ctor Casamayor Pujol", "Schahram Dustdar"], "title": "Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices", "comment": null, "summary": "Edge devices have limited resources, which inevitably leads to situations\nwhere stream processing services cannot satisfy their needs. While existing\nautoscaling mechanisms focus entirely on resource scaling, Edge devices require\nalternative ways to sustain the Service Level Objectives (SLOs) of competing\nservices. To address these issues, we introduce a Multi-dimensional Autoscaling\nPlatform (MUDAP) that supports fine-grained vertical scaling across both\nservice- and resource-level dimensions. MUDAP supports service-specific scaling\ntailored to available parameters, e.g., scale data quality or model size for a\nparticular service. To optimize the execution across services, we present a\nscaling agent based on Regression Analysis of Structural Knowledge (RASK). The\nRASK agent efficiently explores the solution space and learns a continuous\nregression model of the processing environment for inferring optimal scaling\nactions. We compared our approach with two autoscalers, the Kubernetes VPA and\na reinforcement learning agent, for scaling up to 9 services on a single Edge\ndevice. Our results showed that RASK can infer an accurate regression model in\nmerely 20 iterations (i.e., observe 200s of processing). By increasingly adding\nelasticity dimensions, RASK sustained the highest request load with 28% less\nSLO violations, compared to baselines."}
{"id": "2510.06363", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06363", "abs": "https://arxiv.org/abs/2510.06363", "authors": ["Ololade Babatunde", "Tomisin Ayodabo", "Raqibul Raqibul"], "title": "Improving Assignment Submission in Higher Education through a Git-Enabled System: An Iterative Case Study", "comment": null, "summary": "This study addresses challenges in traditional assignment submission methods\nused in higher education by introducing and evaluating a customized Git-based\nsubmission system. Employing iterative software development and user-centered\ndesign methodologies, the system was integrated within a real-world university\nenvironment. Empirical evaluation, including usability testing and student\nfeedback, indicated significant improvements in assignment tracking,\ncollaboration, and submission efficiency. Students reported positive\nexperiences using distributed version control workflows, highlighting improved\nlearning outcomes and reduced administrative burden. Challenges related to\ninitial adoption and student learning curves were identified and mitigated\nthrough iterative improvements. The proposed system contributes practical\ninsights for integrating distributed version control into educational settings,\nenhancing both instructor oversight and student engagement in software\nengineering and related disciplines. Based on our results, the research showed\nthat 85% of instructors found the git based system easier to use, with 84% of\nstudents preferring it over traditional methods, as it provides a 38% reduction\nin time taken for submission and review, while also leading to a 48% reduction\nin storage requirements."}
{"id": "2510.06663", "categories": ["cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06663", "abs": "https://arxiv.org/abs/2510.06663", "authors": ["Qiuyang Mang", "Runyuan He", "Suyang Zhong", "Xiaoxuan Liu", "Huanchen Zhang", "Alvin Cheung"], "title": "Automated Discovery of Test Oracles for Database Management Systems Using LLMs", "comment": null, "summary": "Since 2020, automated testing for Database Management Systems (DBMSs) has\nflourished, uncovering hundreds of bugs in widely-used systems. A cornerstone\nof these techniques is test oracle, which typically implements a mechanism to\ngenerate equivalent query pairs, thereby identifying bugs by checking the\nconsistency between their results. However, while applying these oracles can be\nautomated, their design remains a fundamentally manual endeavor. This paper\nexplores the use of large language models (LLMs) to automate the discovery and\ninstantiation of test oracles, addressing a long-standing bottleneck towards\nfully automated DBMS testing. Although LLMs demonstrate impressive creativity,\nthey are prone to hallucinations that can produce numerous false positive bug\nreports. Furthermore, their significant monetary cost and latency mean that LLM\ninvocations should be limited to ensure that bug detection is efficient and\neconomical.\n  To this end, we introduce Argus, a novel framework built upon the core\nconcept of the Constrained Abstract Query - a SQL skeleton containing\nplaceholders and their associated instantiation conditions (e.g., requiring a\nplaceholder to be filled by a boolean column). Argus uses LLMs to generate\npairs of these skeletons that are asserted to be semantically equivalent. This\nequivalence is then formally proven using a SQL equivalence solver to ensure\nsoundness. Finally, the placeholders within the verified skeletons are\ninstantiated with concrete, reusable SQL snippets that are also synthesized by\nLLMs to efficiently produce complex test cases. We implemented Argus and\nevaluated it on five extensively tested DBMSs, discovering 40 previously\nunknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already\nfixed by the developers."}
{"id": "2510.06902", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2510.06902", "abs": "https://arxiv.org/abs/2510.06902", "authors": ["Ayesha Afzal", "Anna Kahler", "Georg Hager", "Gerhard Wellein"], "title": "GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs", "comment": "12 pages", "summary": "Molecular dynamics simulations are essential tools in computational\nbiophysics, but their performance depend heavily on hardware choices and\nconfiguration. In this work, we presents a comprehensive performance analysis\nof four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six\nrepresentative GROMACS biomolecular workloads alongside two synthetic\nbenchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We\ninvestigate how performance scales with GPU graphics clock frequency and how\nworkloads respond to power capping. The two synthetic benchmarks define the\nextremes of frequency scaling: Pi Solver shows ideal compute scalability, while\nSTREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance\nin context. Our results reveal distinct frequency scaling behaviors: Smaller\nGROMACS systems exhibit strong frequency sensitivity, while larger systems\nsaturate quickly, becoming increasingly memory bound. Under power capping,\nperformance remains stable until architecture- and workload-specific thresholds\nare reached, with high-end GPUs like the A100 maintaining near-maximum\nperformance even under reduced power budgets. Our findings provide practical\nguidance for selecting GPU hardware and optimizing GROMACS performance for\nlarge-scale MD workflows under power constraints."}
{"id": "2510.06483", "categories": ["cs.SE", "D.2.1; D.2.2; K.4.2; D.2.13"], "pdf": "https://arxiv.org/pdf/2510.06483", "abs": "https://arxiv.org/abs/2510.06483", "authors": ["Judith Michael", "Lukas Netz", "Bernhard Rumpe", "Ingo MÃ¼ller", "John Grundy", "Shavindra Wickramathilaka", "Hourieh Khalajzadeh"], "title": "Addressing Visual Impairments with Model-Driven Engineering: A Systematic Literature Review", "comment": "41 pages", "summary": "Software applications often pose barriers for users with accessibility needs,\ne.g., visual impairments. Model-driven engineering (MDE), with its systematic\nnature of code derivation, offers systematic methods to integrate accessibility\nconcerns into software development while reducing manual effort. This paper\npresents a systematic literature review on how MDE addresses accessibility for\nvision impairments. From 447 initially identified papers, 30 primary studies\nmet the inclusion criteria. About two-thirds reference the Web Content\nAccessibility Guidelines (WCAG), yet their project-specific adaptions and\nend-user validations hinder wider adoption in MDE. The analyzed studies model\nuser interface structures, interaction and navigation, user capabilities,\nrequirements, and context information. However, only few specify concrete\nmodeling techniques on how to incorporate accessibility needs or demonstrate\nfully functional systems. Insufficient details on MDE methods, i.e.,\ntransformation rules or code templates, hinder the reuse, generalizability, and\nreproducibility. Furthermore, limited involvement of affected users and limited\ndeveloper expertise in accessibility contribute to weak empirical validation.\nOverall, the findings indicate that current MDE research insufficiently\nsupports vision-related accessibility. Our paper concludes with a research\nagenda outlining how support for vision impairments can be more effectively\nembedded in MDE processes."}
{"id": "2510.06980", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.06980", "abs": "https://arxiv.org/abs/2510.06980", "authors": ["Xinyi Gao", "Jingxi Zhang", "Lijian Chen", "Tong Chen", "Lizhen Cui", "Hongzhi Yin"], "title": "Relational Database Distillation: From Structured Tables to Condensed Graph Data", "comment": null, "summary": "Relational databases (RDBs) underpin the majority of global data management\nsystems, where information is structured into multiple interdependent tables.\nTo effectively use the knowledge within RDBs for predictive tasks, recent\nadvances leverage graph representation learning to capture complex inter-table\nrelations as multi-hop dependencies. Despite achieving state-of-the-art\nperformance, these methods remain hindered by the prohibitive storage overhead\nand excessive training time, due to the massive scale of the database and the\ncomputational burden of intensive message passing across interconnected tables.\nTo alleviate these concerns, we propose and study the problem of Relational\nDatabase Distillation (RDD). Specifically, we aim to distill large-scale RDBs\ninto compact heterogeneous graphs while retaining the predictive power (i.e.,\nutility) required for training graph-based models. Multi-modal column\ninformation is preserved through node features, and primary-foreign key\nrelations are encoded via heterogeneous edges, thereby maintaining both data\nfidelity and relational structure. To ensure adaptability across diverse\ndownstream tasks without engaging the traditional, inefficient bi-level\ndistillation framework, we further design a kernel ridge regression-guided\nobjective with pseudo-labels, which produces quality features for the distilled\ngraph. Extensive experiments on multiple real-world RDBs demonstrate that our\nsolution substantially reduces the data size while maintaining competitive\nperformance on classification and regression tasks, creating an effective\npathway for scalable learning with RDBs."}
{"id": "2510.06998", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2510.06998", "abs": "https://arxiv.org/abs/2510.06998", "authors": ["Martin Wilhelm", "Franz Freitag", "Max Tzschoppe", "Thilo Pionteck"], "title": "Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic", "comment": "To be published on NorCAS 2025", "summary": "Heterogeneous computing systems, which combine general-purpose processors\nwith specialized accelerators, are increasingly important for optimizing the\nperformance of modern applications. A central challenge is to decide which\nparts of an application should be executed on which accelerator or, more\ngenerally, how to map the tasks of an application to available devices.\nPredicting the impact of a change in a task mapping on the overall makespan is\nnon-trivial. While there are very capable simulators, these generally require a\nfull implementation of the tasks in question, which is particularly\ntime-intensive for programmable logic. A promising alternative is to use a\npurely analytical function, which allows for very fast predictions, but\nabstracts significantly from reality. Bridging the gap between theory and\npractice poses a significant challenge to algorithm developers. This paper aims\nto aid in the development of rapid makespan prediction algorithms by providing\na highly flexible evaluation framework for heterogeneous systems consisting of\nCPUs, GPUs and FPGAs, which is capable of collecting real-world makespan\nresults based on abstract task graph descriptions. We analyze to what extent\nactual makespans can be predicted by existing analytical approaches.\nFurthermore, we present common challenges that arise from high-level\ncharacteristics such as data transfer overhead and device congestion in\nheterogeneous systems."}
{"id": "2510.06606", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06606", "abs": "https://arxiv.org/abs/2510.06606", "authors": ["Uswat Yusuf", "Genevieve Caumartin", "Diego Elias Costa"], "title": "Beyond More Context: How Granularity and Order Drive Code Completion Quality", "comment": null, "summary": "Context plays an important role in the quality of code completion, as Large\nLanguage Models (LLMs) require sufficient and relevant information to assist\ndevelopers in code generation tasks. However, composing a relevant context for\ncode completion poses challenges in large repositories: First, the limited\ncontext length of LLMs makes it impractical to include all repository files.\nSecond, the quality of generated code is highly sensitive to noisy or\nirrelevant context. In this paper, we present our approach for the ASE 2025\nContext Collection Challenge. The challenge entails outperforming JetBrains\nbaselines by designing effective retrieval and context collection strategies.\nWe develop and evaluate a series of experiments that involve retrieval\nstrategies at both the file and chunk levels. We focus our initial experiments\non examining the impact of context size and file ordering on LLM performance.\nOur results show that the amount and order of context can significantly\ninfluence the performance of the models. We introduce chunk-based retrieval\nusing static analysis, achieving a 6% improvement over our best file-retrieval\nstrategy and a 16% improvement over the no-context baseline for Python in the\ninitial phase of the competition. Our results highlight the importance of\nretrieval granularity, ordering and hybrid strategies in developing effective\ncontext collection pipelines for real-world development scenarios."}
{"id": "2510.07062", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.07062", "abs": "https://arxiv.org/abs/2510.07062", "authors": ["Hadar Rotschield", "Liat Peterfreund"], "title": "On the Expressiveness of Languages for Querying Property Graphs in Relational Databases", "comment": null, "summary": "SQL/PGQ is the emerging ISO standard for querying property graphs defined as\nviews over relational data. We formalize its expressive power across three\nfragments: the read-only core, the read-write extension, and an extended\nvariant with richer view definitions. Our results show that graph creation\nplays a central role in determining the expressiveness. The read-only fragment\nis strictly weaker than the read-write fragment, and the latter is still below\nthe complexity class NL. Extending view definitions with arbitrary arity\nidentifiers closes this gap: the extended fragment captures exactly NL. This\nyields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL\nqueries. On ordered structures the hierarchy collapses: once arity-2\nidentifiers are allowed, higher arities add no power, mirroring the classical\ntransitive-closure collapse and underscoring the central role of view\nconstruction in property graph querying."}
{"id": "2510.06708", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06708", "abs": "https://arxiv.org/abs/2510.06708", "authors": ["Aleksi Huotala", "Miikka Kuutila", "Olli-Pekka Turtio", "Mika MÃ¤ntylÃ¤"], "title": "AISysRev -- LLM-based Tool for Title-abstract Screening", "comment": "4 pages", "summary": "Systematic reviews are a standard practice for summarizing the state of\nevidence in software engineering. Conducting systematic reviews is laborious,\nespecially during the screening or study selection phase, where the number of\npapers can be overwhelming. During this phase, papers are assessed against\ninclusion and exclusion criteria based on their titles and abstracts. Recent\nresearch has demonstrated that large language models (LLMs) can perform\ntitle-abstract screening at a level comparable to that of a master's student.\nWhile LLMs cannot be fully trusted, they can help, for example, in Rapid\nReviews, which try to expedite the review process. Building on recent research,\nwe developed AiSysRev, an LLM-based screening tool implemented as a web\napplication running in a Docker container. The tool accepts a CSV file\ncontaining paper titles and abstracts. Users specify inclusion and exclusion\ncriteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev\nsupports both zero-shot and few-shot screening, and also allows for manual\nscreening through interfaces that display LLM results as guidance for human\nreviewers.We conducted a trial study with 137 papers using the tool. Our\nfindings indicate that papers can be classified into four categories: Easy\nIncludes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary\ncases, where LLMs are prone to errors, highlight the need for human\nintervention. While LLMs do not replace human judgment in systematic reviews,\nthey can significantly reduce the burden of assessing large volumes of\nscientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool:\nhttps://github.com/EvoTestOps/AISysRev"}
{"id": "2510.06718", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.06718", "abs": "https://arxiv.org/abs/2510.06718", "authors": ["Ranim Khojah", "Mazen Mohamad", "Linda Erlenhov", "Francisco Gomes de Oliveira Neto", "Philipp Leitner"], "title": "LLM Company Policies and Policy Implications in Software Organizations", "comment": "Accepted at IEEE Software Special Issue on AIware in the Foundation\n  Models Era", "summary": "The risks associated with adopting large language model (LLM) chatbots in\nsoftware organizations highlight the need for clear policies. We examine how 11\ncompanies create these policies and the factors that influence them, aiming to\nhelp managers safely integrate chatbots into development workflows."}
{"id": "2510.06844", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06844", "abs": "https://arxiv.org/abs/2510.06844", "authors": ["Nicole Hoess", "Carlos Paradis", "Rick Kazman", "Wolfgang Mauerer"], "title": "Oops!... I did it again. Conclusion (In-)Stability in Quantitative Empirical Software Engineering: A Large-Scale Analysis", "comment": null, "summary": "Context: Mining software repositories is a popular means to gain insights\ninto a software project's evolution, monitor project health, support decisions\nand derive best practices. Tools supporting the mining process are commonly\napplied by researchers and practitioners, but their limitations and agreement\nare often not well understood.\n  Objective: This study investigates some threats to validity in complex tool\npipelines for evolutionary software analyses and evaluates the tools' agreement\nin terms of data, study outcomes and conclusions for the same research\nquestions.\n  Method: We conduct a lightweight literature review to select three studies on\ncollaboration and coordination, software maintenance and software quality from\nhigh-ranked venues, which we formally replicate with four independent,\nsystematically selected mining tools to quantitatively and qualitatively\ncompare the extracted data, analysis results and conclusions.\n  Results: We find that numerous technical details in tool design and\nimplementation accumulate along the complex mining pipelines and can cause\nsubstantial differences in the extracted baseline data, its derivatives,\nsubsequent results of statistical analyses and, under specific circumstances,\nconclusions.\n  Conclusions: Users must carefully choose tools and evaluate their limitations\nto assess the scope of validity in an adequate way. Reusing tools is\nrecommended. Researchers and tool authors can promote reusability and help\nreducing uncertainties by reproduction packages and comparative studies\nfollowing our approach."}
{"id": "2510.06984", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06984", "abs": "https://arxiv.org/abs/2510.06984", "authors": ["Masanari Kondo", "Mahmoud Alfadel", "Shane McIntosh", "Yasutaka Kamei", "Naoyasu Ubayashi"], "title": "An empirical study on declined proposals: why are these proposals declined?", "comment": null, "summary": "Design-level decisions in open-source software (OSS) projects are often made\nthrough structured mechanisms such as proposals, which require substantial\ncommunity discussion and review. Despite their importance, the proposal process\nis resource-intensive and often leads to contributor frustration, especially\nwhen proposals are declined without clear feedback. Yet, the reasons behind\nproposal rejection remain poorly understood, limiting opportunities to\nstreamline the process or guide contributors effectively. This study\ninvestigates the characteristics and outcomes of proposals in the Go\nprogramming language to understand why proposals are declined and how such\noutcomes might be anticipated. We conduct a mixed-method empirical study on\n1,091 proposals submitted to the Go project. We quantify proposal outcomes,\nbuild a taxonomy of decline reasons, and evaluate large language models (LLMs)\nfor predicting these outcomes. We find that proposals are more often declined\nthan accepted, and resolution typically takes over a month. Only 14.7% of\ndeclined proposals are ever resubmitted. Through qualitative coding, we\nidentify nine key reasons for proposal decline, such as duplication, limited\nuse cases, or violations of project principles. This taxonomy can help\ncontributors address issues in advance, e.g., checking for existing\nalternatives can reduce redundancy. We also demonstrate that GPT-based models\ncan predict decline decisions early in the discussion (F1 score = 0.71 with\npartial comments), offering a practical tool for prioritizing review effort.\nOur findings reveal inefficiencies in the proposal process and highlight\nactionable opportunities for improving both contributor experience and reviewer\nworkload by enabling early triage and guiding contributors to strengthen their\nproposals using a structured understanding of past decline reasons."}
{"id": "2510.06989", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06989", "abs": "https://arxiv.org/abs/2510.06989", "authors": ["Pengyue Yang", "Haolin Jin", "Qingwen Zeng", "Jiawen Wen", "Harry Rao", "Huaming Chen"], "title": "Human-aligned AI Model Cards with Weighted Hierarchy Architecture", "comment": "10 pages, 5 figures. Submitted to ICSE SEIP 2026 (Software\n  Engineering in Practice)", "summary": "The proliferation of Large Language Models (LLMs) has led to a burgeoning\necosystem of specialized, domain-specific models. While this rapid growth\naccelerates innovation, it has simultaneously created significant challenges in\nmodel discovery and adoption. Users struggle to navigate this landscape due to\ninconsistent, incomplete, and imbalanced documentation across platforms.\nExisting documentation frameworks, such as Model Cards and FactSheets, attempt\nto standardize reporting but are often static, predominantly qualitative, and\nlack the quantitative mechanisms needed for rigorous cross-model comparison.\nThis gap exacerbates model underutilization and hinders responsible adoption.\nTo address these shortcomings, we introduce the Comprehensive Responsible AI\nModel Card Framework (CRAI-MCF), a novel approach that transitions from static\ndisclosures to actionable, human-aligned documentation. Grounded in Value\nSensitive Design (VSD), CRAI-MCF is built upon an empirical analysis of 240\nopen-source projects, distilling 217 parameters into an eight-module,\nvalue-aligned architecture. Our framework introduces a quantitative sufficiency\ncriterion to operationalize evaluation and enables rigorous cross-model\ncomparison under a unified scheme. By balancing technical, ethical, and\noperational dimensions, CRAI-MCF empowers practitioners to efficiently assess,\nselect, and adopt LLMs with greater confidence and operational integrity."}
{"id": "2510.07070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07070", "abs": "https://arxiv.org/abs/2510.07070", "authors": ["Gopi Krishnan Rajbahadur", "Keheliya Gallaba", "Elyas Rashno", "Arthit Suriyawongkul", "Karen Bennet", "Kate Stewart", "Ahmed E. Hassan"], "title": "Building an Open AIBOM Standard in the Wild", "comment": null, "summary": "Modern software engineering increasingly relies on open, community-driven\nstandards, yet how such standards are created in fast-evolving domains like\nAI-powered systems remains underexplored. This paper presents a detailed\nexperience report on the development of the AI Bill of Materials AIBOM\nspecification, an extension of the ISO/IEC 5962:2021 Software Package Data\nExchange (SPDX) software bill of materials (SBOM) standard, which captures AI\ncomponents such as datasets and iterative training artifacts. Framed through\nthe lens of Action Research (AR), we document a global, multi-stakeholder\neffort involving over 90 contributors and structured AR cycles. The resulting\nspecification was validated through four complementary approaches: alignment\nwith major regulations and ethical standards (e.g., EU AI Act and IEEE 7000\nstandards), systematic mapping to six industry use cases, semi-structured\npractitioner interviews, and an industrial case study. Beyond delivering a\nvalidated artefact, our paper documents the process of building the AIBOM\nspecification in the wild, and reflects on how it aligns with the AR cycle, and\ndistills lessons that can inform future standardization efforts in the software\nengineering community."}
{"id": "2510.07189", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.07189", "abs": "https://arxiv.org/abs/2510.07189", "authors": ["Junjie Li", "Fazle Rabbi", "Bo Yang", "Song Wang", "Jinqiu Yang"], "title": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe", "comment": null, "summary": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively."}
{"id": "2510.06396", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06396", "abs": "https://arxiv.org/abs/2510.06396", "authors": ["Aymen Alsaadi", "Jonathan Ash", "Mikhail Titov", "Matteo Turilli", "Andre Merzky", "Shantenu Jha", "Sagar Khare"], "title": "Adaptive Protein Design Protocols and Middleware", "comment": "N/A", "summary": "Computational protein design is experiencing a transformation driven by\nAI/ML. However, the range of potential protein sequences and structures is\nastronomically vast, even for moderately sized proteins. Hence, achieving\nconvergence between generated and predicted structures demands substantial\ncomputational resources for sampling. The Integrated Machine-learning for\nProtein Structures at Scale (IMPRESS) offers methods and advanced computing\nsystems for coupling AI to high-performance computing tasks, enabling the\nability to evaluate the effectiveness of protein designs as they are developed,\nas well as the models and simulations used to generate data and train models.\nThis paper introduces IMPRESS and demonstrates the development and\nimplementation of an adaptive protein design protocol and its supporting\ncomputing infrastructure. This leads to increased consistency in the quality of\nprotein design and enhanced throughput of protein design due to dynamic\nresource allocation and asynchronous workload execution."}
{"id": "2510.06414", "categories": ["cs.DB", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06414", "abs": "https://arxiv.org/abs/2510.06414", "authors": ["Abdur Rehman Anwar Qureshi", "Adrian Rebmann", "Timotheus Kampik", "Matthias Weidlich", "Mathias Weske"], "title": "Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation", "comment": null, "summary": "Business process management is increasingly practiced using data-driven\napproaches. Still, classical imperative process models, which are typically\nformalized using Petri nets, are not straightforwardly applicable to the\nrelational databases that contain much of the available structured process\nexecution data. This creates a gap between the traditional world of process\nmodeling and recent developments around data-driven process analysis,\nultimately leading to the under-utilization of often readily available process\nmodels. In this paper, we close this gap by providing an approach for\ntranslating imperative models into relaxed process data queries, specifically\nSQL queries executable on relational databases, for conformance checking. Our\nresults show the continued relevance of imperative process models to\ndata-driven process management, as well as the importance of behavioral\nfootprints and other declarative approaches for integrating model-based and\ndata-driven process management."}
{"id": "2510.06663", "categories": ["cs.DB", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06663", "abs": "https://arxiv.org/abs/2510.06663", "authors": ["Qiuyang Mang", "Runyuan He", "Suyang Zhong", "Xiaoxuan Liu", "Huanchen Zhang", "Alvin Cheung"], "title": "Automated Discovery of Test Oracles for Database Management Systems Using LLMs", "comment": null, "summary": "Since 2020, automated testing for Database Management Systems (DBMSs) has\nflourished, uncovering hundreds of bugs in widely-used systems. A cornerstone\nof these techniques is test oracle, which typically implements a mechanism to\ngenerate equivalent query pairs, thereby identifying bugs by checking the\nconsistency between their results. However, while applying these oracles can be\nautomated, their design remains a fundamentally manual endeavor. This paper\nexplores the use of large language models (LLMs) to automate the discovery and\ninstantiation of test oracles, addressing a long-standing bottleneck towards\nfully automated DBMS testing. Although LLMs demonstrate impressive creativity,\nthey are prone to hallucinations that can produce numerous false positive bug\nreports. Furthermore, their significant monetary cost and latency mean that LLM\ninvocations should be limited to ensure that bug detection is efficient and\neconomical.\n  To this end, we introduce Argus, a novel framework built upon the core\nconcept of the Constrained Abstract Query - a SQL skeleton containing\nplaceholders and their associated instantiation conditions (e.g., requiring a\nplaceholder to be filled by a boolean column). Argus uses LLMs to generate\npairs of these skeletons that are asserted to be semantically equivalent. This\nequivalence is then formally proven using a SQL equivalence solver to ensure\nsoundness. Finally, the placeholders within the verified skeletons are\ninstantiated with concrete, reusable SQL snippets that are also synthesized by\nLLMs to efficiently produce complex test cases. We implemented Argus and\nevaluated it on five extensively tested DBMSs, discovering 40 previously\nunknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already\nfixed by the developers."}
