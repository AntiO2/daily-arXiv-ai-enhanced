<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation](https://arxiv.org/abs/2510.18893)
*Sergey Pugachev*

Main category: cs.DC

TL;DR: CodeCRDT是一种基于观察驱动的协调模式，使用CRDT技术实现无锁、无冲突的并行代码生成，避免了传统多智能体LLM系统中昂贵的显式消息传递协调成本。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体LLM系统由于昂贵的协调成本无法实现并行加速，需要一种更高效的协调机制来提升性能。

Method: 采用观察驱动协调模式，通过监控具有可观察更新和确定性收敛的共享状态来实现协调，使用冲突无关复制数据类型（CRDTs）确保强最终一致性。

Result: 在600次试验中，某些任务实现21.1%加速，其他任务出现39.4%减速，但100%收敛且无合并失败，语义冲突率5-10%。

Conclusion: 研究形式化了随机LLM智能体的观察驱动协调，揭示了语义冲突率和质量-性能权衡，并基于任务结构提供了并行协调成功与失败的实证特征。

Abstract: Multi-agent LLM systems fail to realize parallel speedups due to costly
coordination. We present CodeCRDT, an observation-driven coordination pattern
where agents coordinate by monitoring a shared state with observable updates
and deterministic convergence, rather than explicit message passing. Using
Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free,
conflict-free concurrent code generation with strong eventual consistency.
Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits
and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on
others, and 100% convergence with zero merge failures. The study formalizes
observation-driven coordination for stochastic LLM agents, revealing semantic
conflict rates (5-10%) and quality-performance tradeoffs, and provides
empirical characterization of when parallel coordination succeeds versus fails
based on task structure.

</details>


### [2] [AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators](https://arxiv.org/abs/2510.18897)
*Jacopo Tagliabue*

Main category: cs.DC

TL;DR: 使用LLM生成随机代码与确定性验证相结合的方法，通过生成-验证循环自动设计分布式系统调度策略，在保持可解释性的同时探索大规模设计空间。


<details>
  <summary>Details</summary>
Motivation: 探索AI驱动的分布式系统策略设计，结合大语言模型的随机代码生成能力和领域特定模拟器的确定性验证，实现自动化策略优化。

Method: 采用迭代的生成-验证循环：LLM生成Python策略代码，模拟器在标准化跟踪上进行评估，结构化反馈指导后续生成，使用Bauplan FaaS运行时和Eudoxia模拟器作为案例研究。

Result: 在多模型上实现了吞吐量改进的初步结果，证明了该方法的有效性。

Conclusion: 该方法在保持可解释性的同时实现了大规模设计空间的定向搜索，并指出AI在扩展该方法论方面将发挥关键作用，特别是帮助引导新模拟器的开发。

Abstract: We explore AI-driven distributed-systems policy design by combining
stochastic code generation from large language models (LLMs) with deterministic
verification in a domain-specific simulator. Using a Function-as-a-Service
runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we
frame scheduler design as an iterative generate-and-verify loop: an LLM
proposes a Python policy, the simulator evaluates it on standardized traces,
and structured feedback steers subsequent generations. This setup preserves
interpretability while enabling targeted search over a large design space. We
detail the system architecture and report preliminary results on throughput
improvements across multiple models. Beyond early gains, we discuss the limits
of the current setup and outline next steps; in particular, we conjecture that
AI will be crucial for scaling this methodology by helping to bootstrap new
simulators.

</details>


### [3] [Comparative analysis of large data processing in Apache Spark using Java, Python and Scala](https://arxiv.org/abs/2510.19012)
*Ivan Borodii,Illia Fedorovych,Halyna Osukhivska,Diana Velychko,Roman Butsii*

Main category: cs.DC

TL;DR: 该研究比较了在Apache Spark平台上使用Java、Python和Scala处理大数据集的性能差异。结果显示，对于小数据集Python表现最佳，而对于复杂操作和大数据集，Scala和Java更高效。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单个处理阶段，缺乏对完整ETL工作流程在不同编程语言下的全面比较，特别是在使用Apache Iceberg的情况下。

Method: 通过执行从CSV文件下载数据、转换并加载到Apache Iceberg分析表等操作，比较三种编程语言在Spark平台上的性能表现。

Result: 处理5MB CSV文件时Python最快(6.71秒)，处理1.6GB文件时三种语言性能相近(Python 46.34秒最快)，复杂操作时Scala最优(374.42秒)。

Conclusion: 编程语言显著影响Spark数据处理效率：Python在小数据处理上有优势，Scala和Java在复杂操作和大数据处理上更高效。

Abstract: During the study, the results of a comparative analysis of the process of
handling large datasets using the Apache Spark platform in Java, Python, and
Scala programming languages were obtained. Although prior works have focused on
individual stages, comprehensive comparisons of full ETL workflows across
programming languages using Apache Iceberg remain limited. The analysis was
performed by executing several operations, including downloading data from CSV
files, transforming and loading it into an Apache Iceberg analytical table. It
was found that the performance of the Spark algorithm varies significantly
depending on the amount of data and the programming language used. When
processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71
seconds, which is superior to Scala's score of 9.13 seconds and Java's time of
9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming
languages demonstrated similar results: the fastest performance was showed in
Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56
seconds, respectively. When performing a more complex operation that involved
combining two CSV files into a single dataset for further loading into an
Apache Iceberg table, Scala demonstrated the highest performance, at 374.42
seconds. Java processing was completed in 379.8 seconds, while Python was the
least efficient, with a runtime of 398.32 seconds. It follows that the
programming language significantly affects the efficiency of data processing by
the Apache Spark algorithm, with Scala and Java being more productive for
processing large amounts of data and complex operations, while Python
demonstrates an advantage in working with small amounts of data. The results
obtained can be useful for optimizing data handling processes depending on
specific performance requirements and the amount of information being
processed.

</details>


### [4] [On the Randomized Locality of Matching Problems in Regular Graphs](https://arxiv.org/abs/2510.19151)
*Seri Khoury,Manish Purohit,Aaron Schild,Joshua Wang*

Main category: cs.DC

TL;DR: 该论文研究了正则图中匹配问题的局部性，展示了(1+ε)-近似匹配是真正局部的问题，而最大匹配则需要依赖节点数或度数。同时揭示了最大匹配的节点平均复杂度和最坏情况复杂度之间的强分离。


<details>
  <summary>Details</summary>
Motivation: 研究分布式对称性破坏问题中的局部性，特别是在正则图这一重要基准族中，理解节点需要探索的邻域半径来获得全局解决方案的部分。

Method: 开发随机化算法，使用基于鞅的新颖分析方法来分析Luby的40年历史算法，证明在Δ-正则图的线图上应用一轮Luby算法会产生几乎Δ/2-正则的图。

Result: (1+ε)-近似匹配在正则图中是真正局部的，仅依赖于ε；当Δ≥poly(1/ε)时，这种依赖是对数级的。最大匹配的节点平均复杂度仅为O(1)，与最坏情况复杂度形成强分离。

Conclusion: 在正则图中，近似匹配具有真正局部性，而最大匹配的局部性需要依赖图参数。节点平均复杂度和最坏情况复杂度之间存在显著差异，这为分布式对称性破坏问题的分类提供了重要见解。

Abstract: The main goal in distributed symmetry-breaking is to understand the locality
of problems; i.e., the radius of the neighborhood that a node needs to explore
in order to arrive at its part of a global solution. In this work, we study the
locality of matching problems in the family of regular graphs, which is one of
the main benchmarks for establishing lower bounds on the locality of
symmetry-breaking problems, as well as for obtaining classification results.
For approximate matching, we develop randomized algorithms to show that $(1 +
\epsilon)$-approximate matching in regular graphs is truly local; i.e., the
locality depends only on $\epsilon$ and is independent of all other graph
parameters. Furthermore, as long as the degree $\Delta$ is not very small
(namely, as long as $\Delta \geq \text{poly}(1/\epsilon)$), this dependence is
only logarithmic in $1/\epsilon$. This stands in sharp contrast to maximal
matching in regular graphs which requires some dependence on the number of
nodes $n$ or the degree $\Delta$. We show matching lower bounds for both
results. For maximal matching, our techniques further allow us to establish a
strong separation between the node-averaged complexity and worst-case
complexity of maximal matching in regular graphs, by showing that the former is
only $O(1)$. Central to our main technical contribution is a novel
martingale-based analysis for the $\approx 40$-year-old algorithm by Luby. In
particular, our analysis shows that applying one round of Luby's algorithm on
the line graph of a $\Delta$-regular graph results in an almost
$\Delta/2$-regular graph.

</details>


### [5] [RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs](https://arxiv.org/abs/2510.19225)
*Yongji Wu,Xueshen Liu,Haizhong Zheng,Juncheng Gu,Beidi Chen,Z. Morley Mao,Arvind Krishnamurthy,Ion Stoica*

Main category: cs.DC

TL;DR: RLBoost是一个利用抢占式GPU资源进行成本高效强化学习训练的系统解决方案，通过混合架构和三项关键技术，显著提升训练吞吐量和成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有RL框架无法有效解决强化学习中rollout和训练阶段资源需求差异的矛盾，同时抢占式GPU资源提供了显著的成本节约机会但未被充分利用。

Method: 采用混合架构，包含三项关键技术：自适应rollout卸载、基于拉取的权重传输、令牌级响应收集和迁移。

Result: 实验显示RLBoost将训练吞吐量提升1.51x-1.97x，成本效率提高28%-49%。

Conclusion: RLBoost通过有效利用抢占式GPU资源，成功解决了RL训练中的资源利用效率问题，实现了显著的性能提升和成本节约。

Abstract: Reinforcement learning (RL) has become essential for unlocking advanced
reasoning capabilities in large language models (LLMs). RL workflows involve
interleaving rollout and training stages with fundamentally different resource
requirements. Rollout typically dominates overall execution time, yet scales
efficiently through multiple independent instances. In contrast, training
requires tightly-coupled GPUs with full-mesh communication. Existing RL
frameworks fall into two categories: co-located and disaggregated
architectures. Co-located ones fail to address this resource tension by forcing
both stages to share the same GPUs. Disaggregated architectures, without
modifications of well-established RL algorithms, suffer from resource
under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances
on public clouds and spare capacity in production clusters, present significant
cost-saving opportunities for accelerating RL workflows, if efficiently
harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient
RL training that harvests preemptible GPU resources. Our key insight is that
rollout's stateless and embarrassingly parallel nature aligns perfectly with
preemptible and often fragmented resources. To efficiently utilize these
resources despite frequent and unpredictable availability changes, RLBoost
adopts a hybrid architecture with three key techniques: (1) adaptive rollout
offload to dynamically adjust workloads on the reserved (on-demand) cluster,
(2) pull-based weight transfer that quickly provisions newly available
instances, and (3) token-level response collection and migration for efficient
preemption handling and continuous load balancing. Extensive experiments show
RLBoost increases training throughput by 1.51x-1.97x while improving cost
efficiency by 28%-49% compared to using only on-demand GPU resources.

</details>


### [6] [RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.19262)
*Heng Xu,Zhiwei Yu,Chengze Du,Ying Zhou,Letian Li,Haojie Wang,Weiqiang Cheng,Jialong Li*

Main category: cs.DC

TL;DR: RailS是一个分布式负载均衡框架，通过利用Rail拓扑的对称性，将全局协调转化为本地调度，使用LPT喷洒调度器主动平衡流量，显著提升MoE训练中的all-to-all通信性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型训练中的稀疏且高度不平衡的all-to-all通信主导了迭代时间，传统负载均衡方法无法充分利用Rail架构的确定性拓扑，导致多NIC带宽利用不足。

Method: 利用Rail拓扑的对称性证明均匀发送确保均匀接收，每个节点独立执行LPT喷洒调度器使用本地信息主动平衡流量，激活N个并行rail进行细粒度、拓扑感知的多路径传输。

Result: 在合成和真实MoE工作负载中，RailS将总线带宽提升20%-78%，完成时间减少17%-78%。对于Mixtral工作负载，迭代时间缩短18%-40%，实现接近最优的负载均衡。

Conclusion: RailS能够充分利用分布式训练中的架构并行性，显著改善MoE训练中的通信性能。

Abstract: Training Mixture-of-Experts (MoE) models introduces sparse and highly
imbalanced all-to-all communication that dominates iteration time. Conventional
load-balancing methods fail to exploit the deterministic topology of Rail
architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a
distributed load-balancing framework that minimizes all-to-all completion time
in MoE training. RailS leverages the Rail topology's symmetry to prove that
uniform sending ensures uniform receiving, transforming global coordination
into local scheduling. Each node independently executes a Longest Processing
Time First (LPT) spraying scheduler to proactively balance traffic using local
information. RailS activates N parallel rails for fine-grained, topology-aware
multipath transmission. Across synthetic and real-world MoE workloads, RailS
improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For
Mixtral workloads, it shortens iteration time by 18%--40% and achieves
near-optimal load balance, fully exploiting architectural parallelism in
distributed training.

</details>


### [7] [FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems](https://arxiv.org/abs/2510.19301)
*Ziheng Deng,Xue Liu,Jiantong Jiang,Yankai Li,Qingxu Deng,Xiaochun Yang*

Main category: cs.DC

TL;DR: FLASH Viterbi是一种快速、轻量级、自适应且硬件友好的Viterbi解码算子，通过非递归分治策略、剪枝和并行化技术提高时间和内存效率，适用于资源受限的数据系统。


<details>
  <summary>Details</summary>
Motivation: 随着结构化序列推理工作负载迁移到资源受限的边缘平台，标准Viterbi解码存在内存密集和计算不灵活的问题，现有方法通常在解码时间和空间效率之间进行权衡，但往往带来显著的运行时开销且缺乏适应性。

Method: 提出FLASH Viterbi算法，结合非递归分治策略与剪枝和并行化技术；进一步提出FLASH-BS Viterbi，基于内存高效数据结构的动态波束搜索变体；为两种算法开发基于FPGA的硬件加速器。

Result: 实验表明，所提算法在解码时间和内存效率方面始终优于现有基线方法，同时保持适应性和硬件友好特性，在边缘设备上实现高吞吐量和低资源使用。

Conclusion: FLASH Viterbi算法通过创新的分治策略和硬件优化，成功解决了标准Viterbi解码在边缘计算环境中的资源效率问题，为现代数据系统提供了高效的自适应解码解决方案。

Abstract: The Viterbi algorithm is a key operator for structured sequence inference in
modern data systems, with applications in trajectory analysis, online
recommendation, and speech recognition. As these workloads increasingly migrate
to resource-constrained edge platforms, standard Viterbi decoding remains
memory-intensive and computationally inflexible. Existing methods typically
trade decoding time for space efficiency, but often incur significant runtime
overhead and lack adaptability to various system constraints. This paper
presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly
Viterbi decoding operator that enhances adaptability and resource efficiency.
FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning
and parallelization techniques to enhance both time and memory efficiency,
making it well-suited for resource-constrained data systems.To further decouple
space complexity from the hidden state space size, we present FLASH-BS Viterbi,
a dynamic beam search variant built on a memory-efficient data structure. Both
proposed algorithms exhibit strong adaptivity to diverse deployment scenarios
by dynamically tuning internal parameters.To ensure practical deployment on
edge devices, we also develop FPGA-based hardware accelerators for both
algorithms, demonstrating high throughput and low resource usage. Extensive
experiments show that our algorithms consistently outperform existing baselines
in both decoding time and memory efficiency, while preserving adaptability and
hardware-friendly characteristics essential for modern data systems. All codes
are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.

</details>


### [8] [HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission](https://arxiv.org/abs/2510.19470)
*Weihao Yang,Hao Huang,Donglei Wu,Ningke Li,Yanqi Pan,Qiyang Zheng,Wen Xia,Shiyi Li,Qiang Wang*

Main category: cs.DC

TL;DR: HybridEP是一个针对混合专家模型在受限带宽下优化的专家并行框架，通过动态调整专家空间布局来减少通信开销，在受限带宽下比现有系统快5.6倍。


<details>
  <summary>Details</summary>
Motivation: 随着混合专家模型规模快速增长，单数据中心训练已无法满足需求，转向跨数据中心训练。但在低带宽环境下，专家并行面临严重的可扩展性问题，现有重叠通信和计算的优化方法效果有限。

Method: 提出HybridEP框架，通过流式模型确定最优传输比例，结合基于域的分区构建混合模式与GPU级通信拓扑的映射，以及参数高效迁移来减少专家传输开销并扩大域规模。

Result: 实验结果显示HybridEP在受限带宽下比现有最先进的MoE训练系统快5.6倍，在大规模模拟中，在1000个数据中心下不同带宽下实现1.45倍加速。

Conclusion: HybridEP是一个更通用的专家并行框架，具有更好的可扩展性，能够有效解决跨数据中心专家并行在低带宽环境下的可扩展性瓶颈。

Abstract: Mixture-of-Experts (MoE) has become a popular architecture for scaling large
models. However, the rapidly growing scale outpaces model training on a single
DC, driving a shift toward a more flexible, cross-DC training paradigm. Under
this, Expert Parallelism (EP) of MoE faces significant scalability issues due
to the limited cross-DC bandwidth. Specifically, existing EP optimizations
attempt to overlap data communication and computation, which has little benefit
in low-bandwidth scenarios due to a much longer data communication time.
Therefore, the trends of cross-DC EP scaling is fast becoming a critical
roadblock to the continued growth of MoE models.
  To address this, we propose HybridEP, a modeling-guided framework to optimize
EP under constrained bandwidth. Our key idea is to dynamically transform the
spatial placement of experts to reduce data communication traffic and
frequency, thereby minimizing EP's communication overheads. However, it is
non-trivial to find the optimal solution because it complicates the original
communication pattern by mixing data and expert communication. We therefore
build a stream-based model to determine the optimal transmission ratio. Guided
by this, we incorporate two techniques: (1) domain-based partition to construct
the mapping between hybrid patterns and specific communication topology at GPU
level, and (2) parameter-efficient migration to further refine this topology by
reducing expert transmission overhead and enlarging the domain size. Combining
all these designs, HybridEP can be considered as a more general EP with better
scalability. Experimental results show that HybridEP outperforms existing
state-of-the-art MoE training systems by up to 5.6x under constrained
bandwidth. We further compare HybridEP and EP on large-scale simulations.
HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.

</details>


### [9] [Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud](https://arxiv.org/abs/2510.19617)
*Eric Ding*

Main category: cs.DC

TL;DR: Propius是一个用于协作机器学习的可扩展资源管理系统，通过控制平面和数据平面解决多租户资源管理问题，显著提升了资源利用率、吞吐量和作业完成时间。


<details>
  <summary>Details</summary>
Motivation: 协作机器学习面临数据隐私、通信开销和模型异构性挑战，现有系统多为特定用例构建，缺乏可扩展和可重用的基础设施。随着协作ML规模扩大，需要高效的多租户资源管理系统。

Method: 提出Propius系统，包含控制平面和数据平面。控制平面支持多协作ML作业间的资源共享和多种资源分配策略，数据平面提升模型共享和结果收集的可扩展性。

Result: 评估显示Propius在资源利用率（最高1.88倍）、吞吐量（最高2.76倍）和作业完成时间（最高1.26倍）方面优于现有资源管理技术。

Conclusion: Propius通过创新的系统架构有效解决了协作机器学习中的资源管理挑战，为大规模协作ML提供了高效的基础设施支持。

Abstract: Collaborative Machine Learning is a paradigm in the field of distributed
machine learning, designed to address the challenges of data privacy,
communication overhead, and model heterogeneity. There have been significant
advancements in optimization and communication algorithm design and ML hardware
that enables fair, efficient and secure collaborative ML training. However,
less emphasis is put on collaborative ML infrastructure development. Developers
and researchers often build server-client systems for a specific collaborative
ML use case, which is not scalable and reusable. As the scale of collaborative
ML grows, the need for a scalable, efficient, and ideally multi-tenant resource
management system becomes more pressing. We propose a novel system, Propius,
that can adapt to the heterogeneity of client machines, and efficiently manage
and control the computation flow between ML jobs and edge resources in a
scalable fashion. Propius is comprised of a control plane and a data plane. The
control plane enables efficient resource sharing among multiple collaborative
ML jobs and supports various resource sharing policies, while the data plane
improves the scalability of collaborative ML model sharing and result
collection. Evaluations show that Propius outperforms existing resource
management techniques and frameworks in terms of resource utilization (up to
$1.88\times$), throughput (up to $2.76$), and job completion time (up to
$1.26\times$).

</details>


### [10] [Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond](https://arxiv.org/abs/2510.19805)
*Carl-Johan Fauvelle Munck af Rosensch"old,Feras M. Awaysheh,Ahmad Awad*

Main category: cs.DC

TL;DR: 对Redis替代品Valkey、KeyDB和Garnet在Kubernetes环境下的性能评估，展示了吞吐量、延迟、资源效率和迁移复杂度等方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对现代内存键值数据库的实验评估，需要填补这一空白，评估新兴工具在可扩展性、兼容性和可持续性方面的表现。

Method: 在Kubernetes部署中使用真实工作负载对Valkey、KeyDB和Garnet进行基准测试，系统评估性能指标。

Result: 结果显示被测试的数据系统之间存在明确的权衡关系，不同系统在性能、兼容性和长期可行性方面各有优劣。

Conclusion: 研究提供了新兴内存键值存储的全面性能和可行性评估，强调了性能、兼容性和项目成熟度之间的权衡关系。

Abstract: In-memory key-value datastores have become indispensable building blocks of
modern cloud-native infrastructures, yet their evolution faces scalability,
compatibility, and sustainability constraints. The current literature lacks an
experimental evaluation of state-of-the-art tools in the domain. This study
addressed this timely gap by benchmarking Redis alternatives and systematically
evaluating Valkey, KeyDB, and Garnet under realistic workloads within
Kubernetes deployments. The results demonstrate clear trade-offs among the
benchmarked data systems. Our study presents a comprehensive performance and
viability assessment of the emerging in-memory key-value stores. Metrics
include throughput, tail latency, CPU and memory efficiency, and migration
complexity. We highlight trade-offs between performance, compatibility, and
long-term viability, including project maturity, community support, and
sustained development.

</details>


### [11] [Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation](https://arxiv.org/abs/2510.19689)
*Guilin Zhang,Wulan Guo,Ziqi Tan,Srinivas Vippagunta,Suchitra Raman,Shreeshankar Chatterjee,Ju Lin,Shang Liu,Mary Schladenhauffen,Jeffrey Luo,Hailong Jiang*

Main category: cs.DC

TL;DR: 提出面向生产环境的BDaaS蓝图，集成单节点无服务器GPU运行时与TabNet，在受监管环境中实现高吞吐、低延迟、低成本且可解释的表格数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统分布式框架如Spark和Flink在大规模批处理或流分析中有效，但存在协调复杂性和审计开销，不适合中等规模、延迟敏感的推理场景。云提供商提供无服务器GPU和TabNet等可解释模型，为受监管环境部署提供了新机遇。

Method: 设计生产导向的BDaaS蓝图，集成单节点无服务器GPU运行时与TabNet模型，利用GPU加速提高吞吐量，无服务器弹性降低成本，特征掩码可解释性满足IL4/FIPS合规要求。

Result: 在HR、Adult和BLS数据集上的基准测试显示，GPU流水线相比Spark基线实现高达4.5倍吞吐量提升、98倍延迟降低和90%每千次推理成本降低，合规机制仅增加约5.7毫秒延迟。

Conclusion: 该研究提供了合规感知基准、可复现的Helm打包蓝图和决策框架，证明了在受监管企业和政府环境中实现安全、可解释且经济高效的无服务器GPU分析的实用性。

Abstract: Industrial and government organizations increasingly depend on data-driven
analytics for workforce, finance, and regulated decision processes, where
timeliness, cost efficiency, and compliance are critical. Distributed
frameworks such as Spark and Flink remain effective for massive-scale batch or
streaming analytics but introduce coordination complexity and auditing
overheads that misalign with moderate-scale, latency-sensitive inference.
Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet
enable interpretable tabular ML, motivating new deployment blueprints for
regulated environments. In this paper, we present a production-oriented Big
Data as a Service (BDaaS) blueprint that integrates a single-node serverless
GPU runtime with TabNet. The design leverages GPU acceleration for throughput,
serverless elasticity for cost reduction, and feature-mask interpretability for
IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets,
comparing our approach against Spark and CPU baselines. Our results show that
GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90%
lower cost per 1K inferences compared to Spark baselines, while compliance
mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains
stable under peak load, ensuring reliable auditability. Taken together, these
findings provide a compliance-aware benchmark, a reproducible Helm-packaged
blueprint, and a decision framework that demonstrate the practicality of
secure, interpretable, and cost-efficient serverless GPU analytics for
regulated enterprise and government settings.

</details>


### [12] [CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing](https://arxiv.org/abs/2510.19725)
*Jingfan Meng,Tianji Yang,Jun Xu*

Main category: cs.DC

TL;DR: 本文开发了一种新的集合交集协议SetX，通过压缩感知技术和多轮通信，显著降低了通信成本，比传统的集合协调协议SetR效率高8-10倍。


<details>
  <summary>Details</summary>
Motivation: 现有的集合交集解决方案通常复用集合协调协议，但存在误解认为两者成本相当。实际上集合交集本质上比集合协调更便宜，需要专门的协议来优化性能。

Method: 使用多轮通信协议：Alice发送A的压缩感知草图给Bob，Bob识别其唯一元素。如果A不是B的子集，Bob将无法解码的残差发回给Alice解码她的唯一元素。双方通过集合成员过滤器反复更新直到达成交集共识。

Result: 在真实数据集上的实验表明，该SetX协议相比基于IBLT的SetR协议，通信成本降低了8到10倍。

Conclusion: 集合交集问题SetX在通信成本上比集合协调问题SetR有本质优势，本文提出的多轮协议突破了SetR的信息论下界，实现了显著的性能提升。

Abstract: In the set reconciliation (\textsf{SetR}) problem, two parties Alice and Bob,
holding sets $\mathsf{A}$ and $\mathsf{B}$, communicate to learn the symmetric
difference $\mathsf{A} \Delta \mathsf{B}$. In this work, we study a related but
under-explored problem: set intersection (\textsf{SetX})~\cite{Ozisik2019},
where both parties learn $\mathsf{A} \cap \mathsf{B}$ instead. However,
existing solutions typically reuse \textsf{SetR} protocols due to the absence
of dedicated \textsf{SetX} protocols and the misconception that \textsf{SetR}
and \textsf{SetX} have comparable costs. Observing that \textsf{SetX} is
fundamentally cheaper than \textsf{SetR}, we developed a multi-round
\textsf{SetX} protocol that outperforms the information-theoretic lower bound
of \textsf{SetR} problem. In our \textsf{SetX} protocol, Alice sends Bob a
compressed sensing (CS) sketch of $\mathsf{A}$ to help Bob identify his unique
elements (those in $\mathsf{B \setminus A}$). This solves the \textsf{SetX}
problem, if $\mathsf{A} \subseteq \mathsf{B}$. Otherwise, Bob sends a CS sketch
of the residue (a set of elements he cannot decode) back to Alice for her to
decode her unique elements (those in $\mathsf{A \setminus B}$). As such, Alice
and Bob communicate back and forth %with a set membership filter (SMF) of
estimated $\mathsf{B \setminus A}$. Alice updates $\mathsf{A}$ and
communication repeats until both parties agrees on $\mathsf{A} \cap
\mathsf{B}$. On real world datasets, experiments show that our $\mathsf{SetX}$
protocol reduces the communication cost by 8 to 10 times compared to the
IBLT-based $\mathsf{SetR}$ protocol.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [13] [FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains](https://arxiv.org/abs/2510.19025)
*Hamed Jelodar,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.DB

TL;DR: FlexiDataGen是一个基于大语言模型的动态语义数据集生成框架，用于解决敏感领域中数据稀缺、获取成本高和隐私限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 在医疗、生物医学研究和网络安全等领域，数据获取成本高、标注数据有限、关键事件罕见或敏感，这些问题阻碍了机器学习模型的开发和泛化能力。

Method: 框架包含四个核心组件：句法语义分析、检索增强生成、动态元素注入、以及带语义验证的迭代改写，确保生成高质量、领域相关的数据。

Result: 实验结果表明，FlexiDataGen有效缓解了数据短缺和标注瓶颈，支持可扩展且准确的机器学习模型开发。

Conclusion: FlexiDataGen为敏感领域提供了一种自适应、动态生成语义连贯且语言多样数据集的有效解决方案。

Abstract: Dataset availability and quality remain critical challenges in machine
learning, especially in domains where data are scarce, expensive to acquire, or
constrained by privacy regulations. Fields such as healthcare, biomedical
research, and cybersecurity frequently encounter high data acquisition costs,
limited access to annotated data, and the rarity or sensitivity of key events.
These issues-collectively referred to as the dataset challenge-hinder the
development of accurate and generalizable machine learning models in such
high-stakes domains. To address this, we introduce FlexiDataGen, an adaptive
large language model (LLM) framework designed for dynamic semantic dataset
generation in sensitive domains. FlexiDataGen autonomously synthesizes rich,
semantically coherent, and linguistically diverse datasets tailored to
specialized fields. The framework integrates four core components: (1)
syntactic-semantic analysis, (2) retrieval-augmented generation, (3) dynamic
element injection, and (4) iterative paraphrasing with semantic validation.
Together, these components ensure the generation of high-quality,
domain-relevant data. Experimental results show that FlexiDataGen effectively
alleviates data shortages and annotation bottlenecks, enabling scalable and
accurate machine learning model development.

</details>


### [14] [Fine-Grained Dichotomies for Conjunctive Queries with Minimum or Maximum](https://arxiv.org/abs/2510.19197)
*Nofar Carmeli,Nikolaos Tziavelis*

Main category: cs.DB

TL;DR: 本文研究了在最小/最大值排序下直接访问合取查询(CQ)答案的细粒度复杂性，并探索了相关任务，包括排序枚举、计数、枚举、直接访问和谓词消除。


<details>
  <summary>Details</summary>
Motivation: 研究合取查询在最小/最大值排序下的直接访问复杂性，以及相关任务的效率边界，为数据库查询优化提供理论基础。

Method: 使用细粒度复杂性分析工具，对自连接无关的合取查询建立完整的二分法分类，识别可在准线性预处理时间后实现常数或对数时间输出的情况。

Result: 为每个任务建立了完整的二分法，精确识别了能够在接近理想时间（准线性预处理时间后常数或对数时间输出）内解决的问题情况。

Conclusion: 研究为合取查询在最小/最大值排序下的各种任务提供了完整的复杂性分类，为数据库查询优化实践提供了理论指导。

Abstract: We investigate the fine-grained complexity of direct access to Conjunctive
Query (CQ) answers according to their position, ordered by the minimum (or
maximum) value between attributes. We further use the tools we develop to
explore a wealth of related tasks. We consider the task of ranked enumeration
under min/max orders, as well as tasks concerning CQs with predicates of the
form x <= min X , where X is a set of variables and x is a single variable:
counting, enumeration, direct access, and predicate elimination (i.e.,
transforming the pair of query and database to an equivalent pair without
min-predicates). For each task, we establish a complete dichotomy for
self-join-free CQs, precisely identifying the cases that are solvable in
near-ideal time, i.e., (quasi)linear preprocessing time followed by constant or
logarithmic time per output.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [15] [CosmoCore Affective Dream-Replay Reinforcement Learning for Code Generation](https://arxiv.org/abs/2510.18895)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore是一种受神经科学启发的强化学习架构，通过整合情感信号来增强大语言模型的代码生成能力，显著减少幻觉代码并加速自我修正。


<details>
  <summary>Details</summary>
Motivation: 受人类和动物学习过程中因错误感到尴尬而快速纠正的启发，类似于训练小狗在受到一次责备后避免重复错误，旨在通过情感信号提升代码生成质量。

Method: 使用轻量级多层感知机为代码生成轨迹标记情感价值和惊喜度，将高负价值（尴尬）的代码片段优先在Dream Queue中进行五倍重放，同时修剪低惊喜度的成功案例以防止过度自信和缓冲区膨胀。

Result: 在HumanEval和BigCodeBench等代码生成基准测试中，CosmoCore将幻觉代码（如语法错误或逻辑错误）减少了48%，并将自我修正速度提高了45%。

Conclusion: 该框架扩展了基于人类反馈的强化学习，为代码助手提供了更情感感知的能力，适用于IDE和数据管道等场景。

Abstract: We introduce CosmoCore, a neuroscience-inspired reinforcement learning (RL)
architecture that integrates affective signals to enhance code generation in
large language models (LLMs). Motivated by human and animal learning where
embarrassment from mistakes drives rapid correction, as observed in training a
puppy to avoid repeating errors after a single scolding CosmoCore tags code
generation trajectories with valence and surprise using a lightweight
multi-layer perceptron (MLP). High-negative valence (cringe) episodes, such as
buggy code outputs, are prioritized in a Dream Queue for five-fold replay
during off-policy updates, while low-surprise successes are pruned to prevent
overconfidence and buffer bloat. Evaluated on code generation benchmarks like
HumanEval and BigCodeBench, alongside simulations with a custom data pipeline
environment, CosmoCore reduces hallucinated code (e.g., syntax errors or
logical bugs) by 48\% and accelerates self-correction by 45\%. Local
experiments using Hugging Face models in a PySpark environment validate these
gains, with code snippets provided for replication. Ablations confirm valence
tagging boosts curiosity in exploration, and pruning mitigates inefficiency.
This framework extends RL from human feedback (RLHF) for more emotionally aware
code assistants, with applications in IDEs and data pipelines. Code and the
custom mini-world simulation are released.

</details>


### [16] [A Survey on Feedback Types in Automated Programming Assessment Systems](https://arxiv.org/abs/2510.18923)
*Eduard Frankford,Tobias Antensteiner,Michael Vierhauser,Clemens Sauerwein,Vivien Wallner,Iris Groher,Reinhold Plösch,Ruth Breu*

Main category: cs.SE

TL;DR: 该研究比较了三种编程自动评估系统的反馈机制：编译器反馈、单元测试反馈和基于大语言模型的AI反馈。研究发现虽然学生认为单元测试反馈最有帮助，但AI反馈能带来更好的学习表现。


<details>
  <summary>Details</summary>
Motivation: 随着编程课程需求的增加，需要更有效的自动评估系统。传统APAS主要依赖预定义单元测试，反馈范围有限。大语言模型的出现为提升反馈质量和个性化提供了新机会。

Method: 在两所大学对200多名学生进行大规模研究，比较编译器反馈、标准单元测试反馈和先进的基于LLM的反馈在感知质量和学生表现方面的影响。

Result: 学生评价单元测试反馈最有帮助，但AI生成的反馈能显著提高学生表现。

Conclusion: 建议结合单元测试和AI驱动的指导来优化自动反馈机制，改善编程教育的学习成果。

Abstract: With the recent rapid increase in digitization across all major industries,
acquiring programming skills has increased the demand for introductory
programming courses. This has further resulted in universities integrating
programming courses into a wide range of curricula, including not only
technical studies but also business and management fields of study.
  Consequently, additional resources are needed for teaching, grading, and
tutoring students with diverse educational backgrounds and skills. As part of
this, Automated Programming Assessment Systems (APASs) have emerged, providing
scalable and high-quality assessment systems with efficient evaluation and
instant feedback. Commonly, APASs heavily rely on predefined unit tests for
generating feedback, often limiting the scope and level of detail of feedback
that can be provided to students. With the rise of Large Language Models (LLMs)
in recent years, new opportunities have emerged as these technologies can
enhance feedback quality and personalization.
  To investigate how different feedback mechanisms in APASs are perceived by
students, and how effective they are in supporting problem-solving, we have
conducted a large-scale study with over 200 students from two different
universities. Specifically, we compare baseline Compiler Feedback, standard
Unit Test Feedback, and advanced LLM-based Feedback regarding perceived quality
and impact on student performance.
  Results indicate that while students rate unit test feedback as the most
helpful, AI-generated feedback leads to significantly better performances.
These findings suggest combining unit tests and AI-driven guidance to optimize
automated feedback mechanisms and improve learning outcomes in programming
education.

</details>


### [17] [Extending Resource Constrained Project Scheduling to Mega-Projects with Model-Based Systems Engineering & Hetero-functional Graph Theory](https://arxiv.org/abs/2510.19035)
*Amirreza Hosseini,Amro M. Farid*

Main category: cs.SE

TL;DR: 该论文将资源受限项目调度问题（RCPSP）与基于模型的系统工程（MBSE）和异构功能图论（HFGT）相结合，构建了从活动节点网络到SysML活动图再到操作数网络的转换流程，并专门化了异构功能网络最小成本流（HFNMCF）公式，证明RCPSP可作为更广泛模型的特殊情况。


<details>
  <summary>Details</summary>
Motivation: 虽然RCPSP是项目管理的核心问题，但它与基于模型的系统工程文献脱节，限制了其在复杂系统设计和管理中的集成。

Method: 构建从活动节点网络到SysML活动图再到操作数网络的转换流程，专门化异构功能网络最小成本流（HFNMCF）公式应用于RCPSP背景。

Result: 在包含可再生和不可再生操作数的示例实例中，专门的HFNMCF产生类似调度，同时提供项目状态的明确解释，支持更丰富的监控和控制。

Conclusion: 该框架保留了经典RCPSP的优势，同时适应了大型复杂巨型项目中遇到的实际约束和企业级决策过程。

Abstract: Within the project management context, project scheduling serves as an
indispensable component, functioning as a fundamental tool for planning,
monitoring, controlling, and managing projects more broadly. Although the
resource-constrained project scheduling problem (RCPSP) lies at the core of
project management activities, it remains largely disconnected from the broader
literature on model-based systems engineering (MBSE), thereby limiting its
integration into the design and management of complex systems. The original
contribution of this paper is twofold. First, the paper seeks to reconcile the
RCPSP with the broader literature and vocabulary of model-based systems
engineering and hetero-functional graph theory (HFGT). A concrete translation
pipeline from an activity-on-node network to a SysML activity diagram, and then
to an operand net is constructed. Using this representation, it specializes the
hetero-functional network minimum-cost flow (HFNMCF) formulation to the RCPSP
context as a systematic means of HFGT for quantitative analysis and proves that
the RCPSP is recoverable as a special case of a broader model. Secondly, on an
illustrative instance with renewable and non-renewable operands, the
specialized HFNMCF, while producing similar schedules, yields explicit
explanations of the project states that enable richer monitoring and control.
Overall, the framework preserves the strengths of the classical RCPSP while
accommodating real-world constraints and enterprise-level decision processes
encountered in large, complex megaprojects.

</details>


### [18] [Docker-based CI/CD for Rocq/OCaml projects](https://arxiv.org/abs/2510.19089)
*Érik Martin-Dorel*

Main category: cs.SE

TL;DR: 本文介绍了三个紧密相关的软件项目：docker-coq、docker-coq-action和docker-keeper，旨在通过Docker-based CI/CD促进Coq/OCaml项目的开发，并为未来维护者提供设计文档。


<details>
  <summary>Details</summary>
Motivation: 为Coq（现称Rocq）和OCaml项目提供基于Docker的CI/CD解决方案，同时为这些DevOps工具的未来维护者记录需求和设计选择。

Method: 开发了三个相互关联的DevOps工具：docker-coq（Docker镜像）、docker-coq-action（GitHub Action）和docker-keeper（自动化工具），采用Docker容器化技术实现持续集成和持续部署。

Result: 成功创建了一套完整的Docker-based CI/CD工具链，为Coq/OCaml项目提供了标准化的开发和测试环境。

Conclusion: 这三个项目为Coq/OCaml生态系统提供了实用的DevOps基础设施，既满足了当前用户需求，也为未来的维护和扩展奠定了基础。

Abstract: This paper presents three closely-related software projects, namely:
docker-coq, docker-coq-action, and docker-keeper. It aims at two objectives:
provide a high-level description of the available features -- to foster the use
of a Docker-based CI/CD for Rocq (formerly known as Coq) or OCaml projects --
and document the underlying requirements and the main design choices of these
three DevOps tools -- to help their future maintainers.

</details>


### [19] [Automated Concern Extraction from Textual Requirements of Cyber-Physical Systems: A Multi-solution Study](https://arxiv.org/abs/2510.19237)
*Dongming Jin,Zhi Jin,Xiaohong Chen,Zheng Fang,Linyu Li,Shengxin Zhao,Chuihui Wang,Hongbin Xiao*

Main category: cs.SE

TL;DR: 提出了ReqEBench，一个包含2,721个真实CPS需求的基准数据集，用于评估自动化需求关注点提取方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化需求关注点提取解决方案缺乏公平和全面的基准来评估其有效性，特别是在CPS领域。

Method: 构建包含12个真实世界CPS系统的2,721个需求的基准数据集，涵盖多个应用领域，并经过严格的标注过程。

Result: 使用ReqEBench评估三种自动化需求关注点提取方案，发现GPT-4在实体关注点提取中的最高F1分数仅为0.24。

Conclusion: ReqEBench将促进自动化需求关注点提取的评估和发展，通过分析失败案例为改进LLM能力提供了思路。

Abstract: Cyber-physical systems (CPSs) are characterized by a deep integration of the
information space and the physical world, which makes the extraction of
requirements concerns more challenging. Some automated solutions for
requirements concern extraction have been proposed to alleviate the burden on
requirements engineers. However, evaluating the effectiveness of these
solutions, which relies on fair and comprehensive benchmarks, remains an open
question. To address this gap, we propose ReqEBench, a new CPSs requirements
concern extraction benchmark, which contains 2,721 requirements from 12
real-world CPSs. ReqEBench offers four advantages. It aligns with real-world
CPSs requirements in multiple dimensions, e.g., scale and complexity. It covers
comprehensive concerns related to CPSs requirements. It undergoes a rigorous
annotation process. It covers multiple application domains of CPSs, e.g.,
aerospace and healthcare. We conducted a comparative study on three types of
automated requirements concern extraction solutions and revealed their
performance in real-world CPSs using our ReqEBench. We found that the highest
F1 score of GPT-4 is only 0.24 in entity concern extraction. We further analyze
failure cases of popular LLM-based solutions, summarize their shortcomings, and
provide ideas for improving their capabilities. We believe ReqEBench will
facilitate the evaluation and development of automated requirements concern
extraction.

</details>


### [20] [A General Solution for the Implementation of CI/CD in Embedded Linux Development](https://arxiv.org/abs/2510.19240)
*Behnam Agahi,Hamed Farbeh*

Main category: cs.SE

TL;DR: 本研究基于Yocto项目设计并实现了一个用于Linux操作系统开发、构建和测试的集成化、可复现的基础设施，采用三层架构确保版本同步、可扩展性和可复现性，并通过CI/CD流水线、缓存优化和模拟器测试验证了系统的功能性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式系统在各行业的广泛应用，对自动化平台的需求日益增长，需要能够快速开发和部署定制化Linux操作系统的集成化解决方案。

Method: 采用Yocto项目的三层架构（主仓库、自定义层和协调清单层），开发了三个示例项目，实现了基于GitLab CI和Docker的CI/CD流水线，使用本地缓存服务器优化构建时间，并通过QEMU模拟器进行六种启动测试场景验证。

Result: 提出的设计不仅确保了可复现性，还显著减少了构建时间，系统功能性和稳定性得到验证，能够扩展到实时Linux版本等高级应用的持续部署。

Conclusion: 该基础设施为嵌入式系统的工业和科研项目提供了一个稳定、可扩展的模型，实现了快速可靠的开发周期，未来可扩展自动化测试、系统监控、分布式构建等优化。

Abstract: With the growing use of embedded systems in various industries, the need for
automated platforms for the development and deployment of customized
Linux-based operating systems has become more important. This research was
conducted with the aim of designing and implementing an integrated and
reproducible infrastructure for the development, building, and testing of a
Linux-based operating system using the Yocto Project. The proposed structure
was implemented based on a three-layer architecture consisting of the main
Yocto repositories, a custom layer (meta-custom), and a coordinating manifest
layer to ensure version synchronization, scalability, and reproducibility.
Three sample projects, including libhelloworld, helloworld, and the kernel
module hello mod, were developed and integrated into the build process.
Continuous Integration and Continuous Deployment pipelines were implemented
with GitLab CI and combined with an isolated Docker environment to automate and
streamline the build and testing workflows. Using a local cache server
containing hashserv, downloads and sstate cache significantly reduced the build
time. The functionality and stability of the system were verified through six
boot test scenarios in the QEMU simulator. The results show that the proposed
design not only ensures reproducibility but also can be extended to advanced
applications such as continuous deployment of real-time Linux versions. Future
recommendations include expanding automated tests, implementing system
monitoring with Prometheus and Grafana, using distributed builds, optimizing
with Docker multi-stage builds, and enabling continuous deployment of real-time
Linux changes to provide a stable and scalable model for industrial and
research projects in embedded systems with a rapid and reliable development
cycle.

</details>


### [21] [Trace: Securing Smart Contract Repository Against Access Control Vulnerability](https://arxiv.org/abs/2510.19254)
*Chong Chen,Jiachi Chen,Lingfeng Bao,David Lo,Yanlin Wang,Zhenyu Shan,Ting Chen,Guangqiang Yin,Jianxing Yu,Zibin Zheng*

Main category: cs.SE

TL;DR: TRACE是一个使用LLM检测不可编译智能合约仓库中访问控制漏洞的工具，通过补全代码片段、构建函数调用图和控制流图来识别敏感函数中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 智能合约中的访问控制漏洞已造成数十亿美元损失，现有工具需要可编译合约才能分析，无法处理复杂的不可编译仓库，而第三方开发者经常复用这些仓库中的代码，带来安全风险。

Method: 使用LLM定位敏感函数，补全代码片段为可编译合约，从AST构建函数调用图，使用CFG作为节点信息，分析敏感函数节点检测访问控制漏洞。

Result: 在开源CVE数据集中检测出15个中的14个漏洞，在5000个最新链上合约中达到89.2%准确率，在83个真实仓库中达到87.0%准确率，均显著优于现有最佳工具。

Conclusion: TRACE能有效检测不可编译智能合约仓库中的访问控制漏洞，在多个测试场景下性能远超现有工具，为智能合约安全提供了新的解决方案。

Abstract: Smart contract vulnerabilities, particularly improper Access Control that
allows unauthorized execution of restricted functions, have caused billions of
dollars in losses. GitHub hosts numerous smart contract repositories containing
source code, documentation, and configuration files-these serve as intermediate
development artifacts that must be compiled and packaged before deployment.
Third-party developers often reference, reuse, or fork code from these
repositories during custom development. However, if the referenced code
contains vulnerabilities, it can introduce significant security risks. Existing
tools for detecting smart contract vulnerabilities are limited in their ability
to handle complex repositories, as they typically require the target contract
to be compilable to generate an abstract representation for further analysis.
This paper presents TRACE, a tool designed to secure non-compilable smart
contract repositories against access control vulnerabilities. TRACE employs
LLMs to locate sensitive functions involving critical operations (e.g.,
transfer) within the contract and subsequently completes function snippets into
a fully compilable contract. TRACE constructs a function call graph from the
abstract syntax tree (AST) of the completed contract. It uses the control flow
graph (CFG) of each function as node information. The nodes of the sensitive
functions are then analyzed to detect Access Control vulnerabilities.
Experimental results demonstrate that TRACE outperforms state-of-the-art tools
on an open-sourced CVE dataset, detecting 14 out of 15 CVEs. In addition, it
achieves 89.2% precision on 5,000 recent on-chain contracts, far exceeding the
best existing tool at 76.9%. On 83 real-world repositories, TRACE achieves
87.0% precision, significantly surpassing DeepSeek-R1's 14.3%.

</details>


### [22] [From Specification to Service: Accelerating API-First Development Using Multi-Agent Systems](https://arxiv.org/abs/2510.19274)
*Saurabh Chauhan,Zeeshan Rasheed,Malik Abdul Sami,Kai-Kristian Kemell,Muhammad Waseem,Zheying Zhang,Jussi Rasku,Mika Saari,Pekka Abrahamsson*

Main category: cs.SE

TL;DR: 使用基于大语言模型的多智能体系统自动化RESTful微服务的API优先开发，通过OpenAPI规范生成、代码生成和基于日志分析的反馈循环来减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 推进RESTful Web服务的API优先开发自动化，并测试基于LLM的多智能体系统在支持API优先开发方法方面的能力。

Method: 构建一个系统，利用LLM智能体创建OpenAPI规范，从中生成服务器代码，并通过分析执行日志和错误消息的反馈循环来优化代码。

Result: 使用PRAB基准测试表明，当OpenAPI规范保持小而专注时，LLM能够生成符合规范的完整功能代码和业务逻辑。

Conclusion: 基于LLM的多智能体系统可以有效支持API优先开发方法，特别是在保持OpenAPI规范简洁的情况下，能够生成功能完整的代码。

Abstract: This paper presents a system that uses Large Language Models (LLMs)-based
agents to automate the API-first development of RESTful microservices. This
system helps to create an OpenAPI specification, generate server code from it,
and refine the code through a feedback loop that analyzes execution logs and
error messages. The integration of log analysis enables the LLM to detect and
address issues efficiently, reducing the number of iterations required to
produce functional and robust services. This study's main goal is to advance
API-first development automation for RESTful web services and test the
capability of LLM-based multi-agent systems in supporting the API-first
development approach. To test the proposed system's potential, we utilized the
PRAB benchmark. The results indicate that if we keep the OpenAPI specification
small and focused, LLMs are capable of generating complete functional code with
business logic that aligns to the specification. The code for the system is
publicly available at https://github.com/sirbh/code-gen

</details>


### [23] [An Empirical Study of Bitwise Operators Intuitiveness through Performance Metrics](https://arxiv.org/abs/2510.19281)
*Shubham Joshi*

Main category: cs.SE

TL;DR: 该研究调查了不同编程背景人员对位运算符的可读性和理解能力，发现某些运算符（如OR、NOT、左移）在任务完成时间上具有统计显著性差异。


<details>
  <summary>Details</summary>
Motivation: 研究位运算符在编程中的可读性和理解性，检验不同编程经验人员在使用位运算符时的表现差异。

Method: 采用被试内实验设计，让23名不同编程背景的参与者（从无编程经验到博士生）在JavaScript程序中完成任务，记录任务完成时间和准确率。

Result: 运算符是预测响应时间的因素之一，具有小但显著的影响（R²=0.032，F(1,494)=16.5，p<.001）。OR、NOT和左移运算符在任务完成时间上显示出统计显著性。

Conclusion: 虽然位运算符的复杂性通常不会导致更长的任务完成时间，但某些运算符被发现不够直观，需要进一步研究和可能的重新设计以提高可理解性。

Abstract: Objectives: This study aims to investigate the readability and
understandability of bitwise operators in programming, with the main hypothesis
that there will be a difference in the performance metrics (response time and
error rate) between participants exposed to various bitwise operators related
questions and those who are not.
  Participants: Participants in this human research study include people
without programming background, novice programmers, and university students
with varying programming experience (from freshmen to PhD level). There were 23
participants for this study.
  Study Methods: This study uses an Within-Subjects Experimental Design to
assess how people with diverse programming backgrounds understand and use
bitwise operators. Participants complete tasks in JavaScript program, and their
task completion time and accuracy of the tasks are recorded for analysis.
  Findings: The results indicate that operators can be one of the factors
predicting response time, with a small but significant effect, with R-squared
0.032, (1, 494) = 16.5, p < .001. Additionally, some operators like OR, NOT,
and Left Shift showed statistical significance in task completion times
compared to other operators.
  Conclusions: While the complexity of bitwise operators did not generally
result in longer task completion times, certain operators were found to be less
intuitive, suggesting the need for further investigation and potential redesign
for improved understandability.

</details>


### [24] [Bytecode-centric Detection of Known-to-be-vulnerable Dependencies in Java Projects](https://arxiv.org/abs/2510.19393)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Jaralyzer是一个基于字节码分析的Java依赖扫描器，能够有效检测修改过的开源依赖中的安全漏洞，性能优于现有扫描工具。


<details>
  <summary>Details</summary>
Motivation: 现代Java项目中71%的代码来自开源依赖，这带来了严重的安全风险。现有依赖扫描器在处理重新编译、重新打包等常见依赖修改时存在局限性。

Method: 开发了Jaralyzer工具，直接分析依赖的字节码，不依赖元数据或源代码的可用性。

Result: 在56个流行OSS组件上的评估显示，Jaralyzer在检测修改依赖中的漏洞方面优于其他扫描器，是唯一能识别所有类型修改中漏洞的工具。在未修改依赖上，比Eclipse Steady多检测28个真实漏洞，减少29个误报。

Conclusion: Jaralyzer通过字节码分析有效解决了依赖修改带来的漏洞检测挑战，显著提升了依赖扫描的准确性和覆盖范围。

Abstract: On average, 71% of the code in typical Java projects comes from open-source
software (OSS) dependencies, making OSS dependencies the dominant component of
modern software code bases. This high degree of OSS reliance comes with a
considerable security risk of adding known security vulnerabilities to a code
base. To remedy this risk, researchers and companies have developed various
dependency scanners, which try to identify inclusions of known-to-be-vulnerable
OSS dependencies. However, there are still challenges that modern dependency
scanners do not overcome, especially when it comes to dependency modifications,
such as re-compilations, re-bundlings or re-packagings, which are common in the
Java ecosystem. To overcome these challenges, we present Jaralyzer, a
bytecode-centric dependency scanner for Java. Jaralyzer does not rely on the
metadata or the source code of the included OSS dependencies being available
but directly analyzes a dependency's bytecode. Our evaluation across 56 popular
OSS components demonstrates that Jaralyzer outperforms other popular dependency
scanners in detecting vulnerabilities within modified dependencies. It is the
only scanner capable of identifying vulnerabilities across all the above
mentioned types of modifications. But even when applied to unmodified
dependencies, Jaralyzer outperforms the current state-of-the-art code-centric
scanner Eclipse Steady by detecting 28 more true vulnerabilities and yielding
29 fewer false warnings.

</details>


### [25] [AutoMT: A Multi-Agent LLM Framework for Automated Metamorphic Testing of Autonomous Driving Systems](https://arxiv.org/abs/2510.19438)
*Linfeng Liang,Chenkai Tan,Yao Deng,Yingfeng Cai,T. Y Chen,Xi Zheng*

Main category: cs.SE

TL;DR: AutoMT是一个基于大语言模型的多智能体蜕变测试框架，用于自动驾驶系统测试，能够从交通规则中自动提取蜕变关系并生成有效的后续测试用例。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统是安全关键系统，现有蜕变测试方法依赖大量人工工作且缺乏自动化，需要更高效的自动化测试方法。

Method: 使用LLMs从Gherkin语法的交通规则中提取蜕变关系，通过视觉语言智能体分析场景，搜索智能体从RAG数据库中检索合适的蜕变关系，通过计算机视觉生成后续测试用例。

Result: AutoMT在后续用例生成中实现了比最佳基线（手动专家定义的蜕变关系）高5倍的测试多样性，检测到20.55%更多的行为违规。

Conclusion: AutoMT能够自动提取多样化的蜕变关系，补充真实世界数据集，发现现场测试和数据收集中经常遗漏的边界情况，其模块化架构支持工业流水线集成。

Abstract: Autonomous Driving Systems (ADS) are safety-critical, where failures can be
severe. While Metamorphic Testing (MT) is effective for fault detection in ADS,
existing methods rely heavily on manual effort and lack automation. We present
AutoMT, a multi-agent MT framework powered by Large Language Models (LLMs) that
automates the extraction of Metamorphic Relations (MRs) from local traffic
rules and the generation of valid follow-up test cases. AutoMT leverages LLMs
to extract MRs from traffic rules in Gherkin syntax using a predefined
ontology. A vision-language agent analyzes scenarios, and a search agent
retrieves suitable MRs from a RAG-based database to generate follow-up cases
via computer vision. Experiments show that AutoMT achieves up to 5 x higher
test diversity in follow-up case generation compared to the best baseline
(manual expert-defined MRs) in terms of validation rate, and detects up to
20.55% more behavioral violations. While manual MT relies on a fixed set of
predefined rules, AutoMT automatically extracts diverse metamorphic relations
that augment real-world datasets and help uncover corner cases often missed
during in-field testing and data collection. Its modular architecture
separating MR extraction, filtering, and test generation supports integration
into industrial pipelines and potentially enables simulation-based testing to
systematically cover underrepresented or safety-critical scenarios.

</details>


### [26] [Mapping and Evolving Interoperability Testing in European Energy Systems: The int:net Perspective](https://arxiv.org/abs/2510.19460)
*Thomas I. Strasser,Edmund Widl,Carlos Ayon Mac Gregor,Mirko Ginocchi,Rene Kuchenbuch*

Main category: cs.SE

TL;DR: 本文分析了欧洲30个互操作性测试设施，提供了测试基础设施、方法和参考测试案例的分类清单，并为未来测试环境的开发引入了蓝图。


<details>
  <summary>Details</summary>
Motivation: 欧洲能源转型需要高度互操作性，但目前缺乏对互操作性测试的专门关注和结构化概述。

Method: 通过对30个欧洲测试设施进行结构化调查，分析测试基础设施、应用方法和参考测试案例。

Result: 建立了欧洲互操作性测试设施的分类清单，并提出了未来测试环境开发的蓝图。

Conclusion: 研究结果有助于建立协调的欧洲互操作性测试生态系统，支持合作、创新和能源转型目标的实现。

Abstract: The ongoing transformation of the European energy landscape, driven by the
integration of renewable energy sources, digital technologies, and
decentralized systems, requires a high degree of interoperability across
diverse components and systems. Ensuring that these elements can exchange
information and operate together reliably is essential for achieving a secure,
flexible, and efficient energy supply infrastructure. While several initiatives
have contributed to the development of smart grid testing infrastructures, they
do not provide a dedicated or comprehensive focus on interoperability testing.
A structured and harmonized overview of interoperability testing capabilities
across Europe is therefore still missing. This work therefore presents a novel
contribution by analyzing the European interoperability testing facility
landscape through a structured survey of 30 facilities. It provides a
categorized inventory of testing infrastructures, applied methodologies, and
reference test cases, and introduces a blueprint for the development of future
testing environments. The findings contribute to the establishment of a
coordinated European ecosystem for interoperability testing, supporting
collaboration, innovation, and alignment with the goals of the energy
transition.

</details>


### [27] [A Goal-Driven Survey on Root Cause Analysis](https://arxiv.org/abs/2510.19593)
*Aoyang Fang,Haowen Yang,Haoze Dong,Qisheng Lu,Junjielong Xu,Pinjia He*

Main category: cs.SE

TL;DR: 本文提出了一个目标驱动的框架，对2014-2025年间135篇云事故管理中的根因分析论文进行分类，强调按目标而非数据类型进行组织的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有RCA调查通常按数据类型分类，忽视了不同研究目标的根本差异，导致目标不同的研究被混为一谈，掩盖了领域的真实进展和差距。

Method: 提出了一个目标驱动的分类框架，基于研究目标对RCA论文进行系统分类和整合，涵盖不同的任务制定方式。

Result: 成功分类了135篇RCA论文，识别了不同研究目标下的工作，并讨论了所有RCA论文的终极目标作为统一框架。

Conclusion: 目标驱动的分类方法能更清晰地展示RCA领域进展，指出了开放挑战和未来研究方向，为研究者和实践者提供了更好的指导。

Abstract: Root Cause Analysis (RCA) is a crucial aspect of incident management in
large-scale cloud services. While the term root cause analysis or RCA has been
widely used, different studies formulate the task differently. This is because
the term "RCA" implicitly covers tasks with distinct underlying goals. For
instance, the goal of localizing a faulty service for rapid triage is
fundamentally different from identifying a specific functional bug for a
definitive fix. However, previous surveys have largely overlooked these
goal-based distinctions, conventionally categorizing papers by input data types
(e.g., metric-based vs. trace-based methods). This leads to the grouping of
works with disparate objectives, thereby obscuring the true progress and gaps
in the field. Meanwhile, the typical audience of an RCA survey is either laymen
who want to know the goals and big picture of the task or RCA researchers who
want to figure out past research under the same task formulation. Thus, an RCA
survey that organizes the related papers according to their goals is in high
demand. To this end, this paper presents a goal-driven framework that
effectively categorizes and integrates 135 papers on RCA in the context of
cloud incident management based on their diverse goals, spanning the period
from 2014 to 2025. In addition to the goal-driven categorization, it discusses
the ultimate goal of all RCA papers as an umbrella covering different RCA
formulations. Moreover, the paper discusses open challenges and future
directions in RCA.

</details>


### [28] [Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1](https://arxiv.org/abs/2510.19600)
*Qianli Ma,Siyu Wang,Yilin Chen,Yinhao Tang,Yixiang Yang,Chang Guo,Bingjie Gao,Zhening Xing,Yanan Sun,Zhipeng Zhang*

Main category: cs.SE

TL;DR: AutoPage是一个多智能体系统，通过分层协作流程自动将学术论文转换为交互式项目网页，解决了传统手动创建网页的繁琐问题。


<details>
  <summary>Details</summary>
Motivation: 研究人员在创建项目网页以展示研究成果时面临手动、重复性工作的困扰，现有自动化工具无法处理网页的动态交互特性。

Method: 采用从粗到细的管道方法，包括叙事规划、多模态内容生成和交互式渲染，使用专门的"检查器"智能体验证内容准确性，并设置可选的人工检查点。

Result: AutoPage能在15分钟内以低于0.1美元的成本生成高质量、视觉吸引力强的网页，并构建了首个相关基准测试PageBench。

Conclusion: AutoPage将系统从单纯工具转变为强大的协作助手，有效解决了论文到网页转换的自动化挑战。

Abstract: In the quest for scientific progress, communicating research is as vital as
the discovery itself. Yet, researchers are often sidetracked by the manual,
repetitive chore of building project webpages to make their dense papers
accessible. While automation has tackled static slides and posters, the
dynamic, interactive nature of webpages has remained an unaddressed challenge.
To bridge this gap, we reframe the problem, arguing that the solution lies not
in a single command, but in a collaborative, hierarchical process. We introduce
$\textbf{AutoPage}$, a novel multi-agent system that embodies this philosophy.
AutoPage deconstructs paper-to-page creation into a coarse-to-fine pipeline
from narrative planning to multimodal content generation and interactive
rendering. To combat AI hallucination, dedicated "Checker" agents verify each
step against the source paper, while optional human checkpoints ensure the
final product aligns perfectly with the author's vision, transforming the
system from a mere tool into a powerful collaborative assistant. To rigorously
validate our approach, we also construct $\textbf{PageBench}$, the first
benchmark for this new task. Experiments show AutoPage not only generates
high-quality, visually appealing pages but does so with remarkable efficiency
in under 15 minutes for less than \$0.1. Code and dataset will be released at
$\href{https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$.

</details>


### [29] [FidelityGPT: Correcting Decompilation Distortions with Retrieval Augmented Generation](https://arxiv.org/abs/2510.19615)
*Zhiping Zhou,Xiaohong Li,Ruitao Feng,Yao Zhang,Yuekang Li,Wenbu Feng,Yunqian Wang,Yuqing Li*

Main category: cs.SE

TL;DR: FidelityGPT是一个通过系统检测和校正语义失真来增强反编译代码准确性和可读性的框架，在二进制相似性基准测试中实现了94%的修复率和64%的正确修复率。


<details>
  <summary>Details</summary>
Motivation: 现有反编译方法存在保真度问题，导致反编译输出的可读性和语义准确性下降，特别是在复杂的闭源二进制文件中缺乏鲁棒的检测和校正机制。

Method: FidelityGPT引入了针对闭源环境的失真感知提示模板，集成了检索增强生成(RAG)与动态语义强度算法来定位失真行并从数据库中检索语义相似的代码，还使用变量依赖算法分析冗余变量并将其依赖关系集成到提示上下文中。

Result: 在620个函数对的评估中，FidelityGPT实现了平均89%的检测准确率和83%的精确率，相比最先进的DeGPT(修复率83%，正确修复率37%)，达到了94%的修复率和64%的正确修复率。

Conclusion: FidelityGPT在准确性和可读性方面取得了显著提升，展示了其在基于LLM的反编译和逆向工程领域的潜力。

Abstract: Decompilation converts machine code into human-readable form, enabling
analysis and debugging without source code. However, fidelity issues often
degrade the readability and semantic accuracy of decompiled output. Existing
methods, such as variable renaming or structural simplification, provide
partial improvements but lack robust detection and correction, particularly for
complex closed-source binaries. We present FidelityGPT, a framework that
enhances decompiled code accuracy and readability by systematically detecting
and correcting semantic distortions. FidelityGPT introduces distortion-aware
prompt templates tailored to closed-source settings and integrates
Retrieval-Augmented Generation (RAG) with a dynamic semantic intensity
algorithm to locate distorted lines and retrieve semantically similar code from
a database. A variable dependency algorithm further mitigates long-context
limitations by analyzing redundant variables and integrating their dependencies
into the prompt context. Evaluated on 620 function pairs from a binary
similarity benchmark, FidelityGPT achieved an average detection accuracy of 89%
and a precision of 83%. Compared to the state-of-the-art DeGPT (Fix Rate 83%,
Corrected Fix Rate 37%), FidelityGPT attained 94% FR and 64% CFR, demonstrating
significant gains in accuracy and readability. These results highlight its
potential to advance LLM-based decompilation and reverse engineering.

</details>


### [30] [Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary](https://arxiv.org/abs/2510.19692)
*Rashina Hoda*

Main category: cs.SE

TL;DR: 本文提出将智能体软件工程的范围从代码扩展到全流程，基于SE基础和新兴框架，提出指导原则和词汇表设计建议，为智能体SE建立坚实基础。


<details>
  <summary>Details</summary>
Motivation: 智能体AI正在引发软件工程的范式转变，但当前研究主要关注代码相关活动，需要从社会技术角度考虑实际应用问题。

Method: 通过扩展智能体SE的范围至全流程，基于SE基础和演化以及新兴框架，提出初步价值观和原则，并分享词汇表设计指导。

Result: 提出了智能体SE的扩展愿景、指导原则和词汇表设计建议，为社区协作和未来发展奠定基础。

Conclusion: 这些想法旨在促进社区合作，引导SE社区为智能体SE建立坚实基础，使其不仅是必然趋势，而且是长期可期和理想的发展方向。

Abstract: Agentic AI is poised to usher in a seismic paradigm shift in Software
Engineering (SE). As technologists rush head-along to make agentic AI a
reality, SE researchers are driven to establish agentic SE as a research area.
While early visions of agentic SE are primarily focused on code-related
activities, early empirical evidence calls for a consideration of a range of
socio-technical concerns to make it work in practice. This paper contributes to
the emerging community vision by: (a) recommending an expansion of its scope
beyond code, toward a 'whole of process' vision, grounding it in SE foundations
and evolution and emerging agentic SE frameworks, (b) proposing a preliminary
set of values and principles to guide efforts, and (c) sharing guidance on
designing/using well-defined vocabulary for agentic SE. It is hoped that these
ideas will encourage community collaborations and steer the SE community
towards laying strong foundations of agentic SE so its not only inevitable but
also deliberate and desirable in the long run.

</details>


### [31] [Review of Tools for Zero-Code LLM Based Application Development](https://arxiv.org/abs/2510.19747)
*Priyaranjan Pattnayak,Hussain Bohra*

Main category: cs.SE

TL;DR: 本文调查了基于大型语言模型的零代码开发平台，分析了各类平台的特点、分类和核心功能，比较了它们的优缺点，并讨论了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，零代码开发平台正在改变软件创建方式。本文旨在系统性地调查和分析这些平台，帮助理解当前技术现状和未来趋势。

Method: 采用广泛的调查方法，根据界面风格、后端集成、输出类型和可扩展性等关键维度对平台进行分类。分析了专门的LLM应用构建器和集成LLM功能的通用无代码平台。

Result: 提出了一个分类法，按界面类型、支持的LLM后端、输出类型和可扩展程度对平台进行归类。详细比较了各平台的优缺点，讨论了与传统和低代码开发方法的权衡。

Conclusion: 基于LLM的零代码平台大大降低了创建AI应用的障碍，但在灵活性和可靠性方面仍面临挑战。该领域正在快速发展，为非程序员创建复杂软件提供了令人兴奋的机会。

Abstract: Large Language Models (LLMs) are transforming software creation by enabling
zero code development platforms. Our survey reviews recent platforms that let
users build applications without writing code, by leveraging LLMs as the brains
of the development process. We adopt a broad survey methodology, categorizing
platforms based on key dimensions such as interface style, backend integration,
output type, and extensibility. We analyze both dedicated LLM based app
builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and
general no code platforms (e.g., Bubble, Glide) that integrate LLM
capabilities. We present a taxonomy categorizing these platforms by their
interface (conversational, visual, etc.), supported LLM backends, output type
(chatbot, full application, workflow), and degree of extensibility. Core
features such as autonomous agents, memory management, workflow orchestration,
and API integrations are in scope of the survey. We provide a detailed
comparison, highlighting each platform's strengths and limitations. Trade offs
(customizability, scalability, vendor lock-in) are discussed in comparison with
traditional and low code development approaches. Finally, we outline future
directions, including multimodal interfaces, on device LLMs, and improved
orchestration for democratizing app creation with AI. Our findings indicate
that while zero code LLM platforms greatly reduce the barrier to creating AI
powered applications, they still face challenges in flexibility and
reliability. Overall, the landscape is rapidly evolving, offering exciting
opportunities to empower non programmers to create sophisticated software.

</details>


### [32] [BOSQTGEN: Breaking the Sound Barrier in Test Generation](https://arxiv.org/abs/2510.19777)
*S M Sadrul Islam Asif,James Chen,Earl T. Barr,Mark Marron*

Main category: cs.SE

TL;DR: BOSQTGEN是一种新颖的黑盒API测试生成方法，通过分解API规范为原语、使用LLM建议连贯层次结构，并应用组合测试来高效采样，解决了多语言系统、源代码不可访问等挑战。


<details>
  <summary>Details</summary>
Motivation: 现代软件越来越多地通过组合API构建，但API契约不足会导致期望不匹配和故障。现有测试生成技术面临多语言系统、源代码不可访问、成本可靠性权衡以及生成结构化输入困难等挑战。

Method: 将API规范分解为原语，使用LLM建议连贯层次结构，应用组合测试来高效采样这些值，覆盖关键交互同时避免随机采样的冗余。

Result: 在RESTful基准测试中平均达到82%的代码覆盖率，通常比先前最先进系统提高20%或更多，接近手工编写测试套件的水平。

Conclusion: BOSQTGEN提供完全API驱动的测试生成方法，使开发人员能够自动创建高质量测试用例进行验证或测试驱动开发。

Abstract: Modern software is increasingly built by composing APIs, elevating the API
contract to a critical role. Inadequate contracts, however, lead to mismatched
expectations and failures, creating a pressing need for robust conformance
testing. Current test generation techniques are hindered by key challenges:
polyglot systems, source code inaccessibility, a cost-reliability trade-off,
and, most critically, the difficulty of generating structured inputs.
  We introduce BOSQTGEN, a novel black-box methodology and tool for API test
generation. BOSQTGEN utilizes a novel approach for decomposing API
specifications into primitives, using LLMs to suggest coherent strata for them,
and employing combinatorial testing to efficiently sample over these values.
This approach ensures coverage of critical interactions while avoiding the
redundancy of random sampling.
  The resulting BOSQTGEN system achieves an average of 82% code coverage on
RESTful benchmarks, often a 20% or more increase over prior state-of-the-art
systems and nearing parity with hand-written test suites. Providing a fully
API-driven approach to test generation, enables developers to automatically
create high-quality test cases for validation or test-driven development.

</details>
