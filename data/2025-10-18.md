<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.DC](#cs.DC) [Total: 13]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Towards a Multimodal Stream Processing System](https://arxiv.org/abs/2510.14631)
*Uélison Jean Lopes dos Santos,Alessandro Ferri,Szilard Nistor,Riccardo Tommasini,Carsten Binnig,Manisha Luthra*

Main category: cs.DB

TL;DR: 提出了将多模态大语言模型作为一流操作符嵌入流处理系统的新一代多模态流处理系统，通过多级优化实现实时多模态查询处理。


<details>
  <summary>Details</summary>
Motivation: 现有工作将MLLMs集成到数据库中处理多模态查询，但流处理系统由于严格的延迟和吞吐量要求需要根本不同的方法。

Method: 在逻辑、物理和语义查询转换等多个层面提出新颖优化，减少模型负载以提高吞吐量同时保持准确性。

Result: 原型系统通过优化将性能提升了一个数量级以上。

Conclusion: 讨论了构建可扩展高效多模态流处理系统的研究路线图和开放研究挑战。

Abstract: In this paper, we present a vision for a new generation of multimodal
streaming systems that embed MLLMs as first-class operators, enabling real-time
query processing across multiple modalities. Achieving this is non-trivial:
while recent work has integrated MLLMs into databases for multimodal queries,
streaming systems require fundamentally different approaches due to their
strict latency and throughput requirements. Our approach proposes novel
optimizations at all levels, including logical, physical, and semantic query
transformations that reduce model load to improve throughput while preserving
accuracy. We demonstrate this with \system{}, a prototype leveraging such
optimizations to improve performance by more than an order of magnitude.
Moreover, we discuss a research roadmap that outlines open research challenges
for building a scalable and efficient multimodal stream processing systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering](https://arxiv.org/abs/2510.13857)
*Qiang Xu,Xiangyu Wen,Changran Xu,Zeju Li,Jianyuan Zhong*

Main category: cs.SE

TL;DR: 本文提出ArbiterOS架构，通过治理优先的范式来解决LLM代理在关键应用中存在的脆弱性和不可预测性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型开启了代理时代，但原型到生产的过渡受到"工艺危机"的阻碍，导致代理在关键应用中脆弱、不可预测且不可信。这种危机源于用传统软件工程的确定性思维模式来命令概率性处理器的根本范式不匹配。

Method: 引入治理优先的代理工程原则，体现在名为ArbiterOS的正式架构中。

Result: 提出了解决代理工程危机的新范式，但没有提供具体的实验结果。

Conclusion: 需要从传统的确定性软件工程范式转向治理优先的代理工程范式，以构建可信赖的自主系统。

Abstract: The advent of powerful Large Language Models (LLMs) has ushered in an ``Age
of the Agent,'' enabling autonomous systems to tackle complex goals. However,
the transition from prototype to production is hindered by a pervasive ``crisis
of craft,'' resulting in agents that are brittle, unpredictable, and ultimately
untrustworthy in mission-critical applications. This paper argues this crisis
stems from a fundamental paradigm mismatch -- attempting to command inherently
probabilistic processors with the deterministic mental models of traditional
software engineering. To solve this crisis, we introduce a governance-first
paradigm for principled agent engineering, embodied in a formal architecture we
call ArbiterOS.

</details>


### [3] [Benchmarking Correctness and Security in Multi-Turn Code Generation](https://arxiv.org/abs/2510.13859)
*Ruchit Rawal,Jeffrey Yang Fan Chiang,Chihao Shen,Jeffery Siyuan Tian,Aastha Mahajan,Tom Goldstein,Yizheng Chen*

Main category: cs.SE

TL;DR: MT-Sec是首个系统评估多轮编码场景中正确性和安全性的基准，发现从单轮到多轮设置中"正确且安全"的输出下降20-27%，即使在最先进模型中也是如此。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常局限于单轮任务，无法反映真实世界开发的迭代性质，需要评估多轮编码场景中的正确性和安全性。

Method: 使用合成数据管道将现有单轮任务转换为语义对齐的多轮交互序列，重用原始测试套件，同时建模真实世界编码过程的复杂性。

Result: 评估了32个开源和闭源模型及三个智能体框架，在多轮设置中"正确且安全"输出下降20-27%；在多轮代码差异生成中表现更差，功能错误和不安全输出率增加；智能体框架在单轮代码生成中表现提升，但在多轮评估中效果不佳。

Conclusion: 需要能够联合评估多轮真实世界编码工作流中正确性和安全性的基准。

Abstract: AI coding assistants powered by large language models (LLMs) have transformed
software development, significantly boosting productivity. While existing
benchmarks evaluate the correctness and security of LLM-generated code, they
are typically limited to single-turn tasks that do not reflect the iterative
nature of real-world development. We introduce MT-Sec, the first benchmark to
systematically evaluate both correctness and security in multi-turn coding
scenarios. We construct this using a synthetic data pipeline that transforms
existing single-turn tasks into semantically aligned multi-turn interaction
sequences, allowing reuse of original test suites while modeling the complexity
of real-world coding processes. We evaluate 32 open- and closed-source models,
and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in
"correct and secure" outputs from single-turn to multi-turn settings -- even
among state-of-the-art models. Beyond full-program generation, we also evaluate
models on multi-turn code-diff generation -- an unexplored yet practically
relevant setting -- and find that models perform worse here, with increased
rates of functionally incorrect and insecure outputs. Finally, we find that
while agent scaffoldings boost single-turn code generation performance, they
are not quite as effective in multi-turn evaluations. Together, these findings
highlight the need for benchmarks that jointly evaluate correctness and
security in multi-turn, real-world coding workflows.

</details>


### [4] [A11YN: aligning LLMs for accessible web UI code generation](https://arxiv.org/abs/2510.13914)
*Janghan Yoon,Jaegwan Cho,Junhyeok Kim,Jiwan Chung,Jaehyun Jeon,Youngjae Yu*

Main category: cs.SE

TL;DR: A11yn是一种对齐代码生成LLM的方法，能可靠生成符合可访问性标准的网页UI，通过优化基于WCAG违规惩罚的奖励函数，将不可访问率降低了60%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成网页界面时会复制训练数据中的可访问性缺陷，导致界面无法满足多样化用户需求，需要系统性地解决可访问性问题。

Method: 提出A11yn方法，优化基于WCAG违规惩罚的奖励函数，惩罚程度根据可访问性测试引擎识别的违规严重程度进行缩放。构建UIReq-6.8K数据集用于训练，并引入RealUIReq-300基准进行评估。

Result: A11yn显著优于基线方法，将不可访问率比基础模型降低了60%，同时保持了生成UI的语义保真度和视觉质量。

Conclusion: 研究表明可访问性可以在LLM中系统优化，证明了为可访问性对齐代码生成的可行性。

Abstract: Large language models (LLMs) have recently demonstrated strong capabilities
in generating functional and aesthetic web interfaces directly from
instructions. However, these models often replicate accessibility flaws from
their training data, resulting in interfaces that exclude users with diverse
needs and contexts. To address this gap, we introduce A11yn, the first method
that aligns code-generating LLMs to reliably produce accessibility-compliant
web UIs. A11yn optimizes a novel reward function that penalizes violations of
the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the
severity of each violation as identified by an accessibility testing engine. To
support training, we construct UIReq-6.8K, a dataset of 6,800 diverse
instructions for web UI generation. For evaluation, we introduce RealUIReq-300,
a benchmark of 300 real-world web UI requests grounded and manually curated
from public web pages, spanning a broad range of use cases. Empirical results
show that A11yn significantly outperforms strong baselines, lowering the
Inaccessibility Rate by 60% over the base model while preserving semantic
fidelity and visual quality of generated UIs. These findings demonstrate that
accessibility can be systematically optimized within LLMs, showing the
feasibility of aligning code generation for accessibility.

</details>


### [5] [Signature in Code Backdoor Detection, how far are we?](https://arxiv.org/abs/2510.13992)
*Quoc Hung Le,Thanh Le-Cong,Bach Le,Bowen Xu*

Main category: cs.SE

TL;DR: 重新评估基于谱签名防御方法在代码模型后门攻击中的适用性，发现传统设置通常不是最优的，并提出了新的代理指标来更准确评估防御性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件开发中广泛应用，后门攻击成为重大威胁。虽然谱签名方法在神经网络后门检测中已有研究，但近期研究表明这些方法对代码模型可能不够有效，需要重新评估其适用性。

Method: 系统评估谱签名防御在不同攻击场景和防御配置下的有效性，分析关键因素的不同设置对性能的影响。

Result: 发现代码后门检测中广泛使用的谱签名设置通常不是最优的，并发现了一个新的代理指标，可以在防御后无需模型重新训练的情况下更准确地估计谱签名的实际性能。

Conclusion: 谱签名防御在代码模型后门检测中需要优化配置，提出的新代理指标能够有效评估防御性能，为改进后门检测方法提供了重要见解。

Abstract: As Large Language Models (LLMs) become increasingly integrated into software
development workflows, they also become prime targets for adversarial attacks.
Among these, backdoor attacks are a significant threat, allowing attackers to
manipulate model outputs through hidden triggers embedded in training data.
Detecting such backdoors remains a challenge, and one promising approach is the
use of Spectral Signature defense methods that identify poisoned data by
analyzing feature representations through eigenvectors. While some prior works
have explored Spectral Signatures for backdoor detection in neural networks,
recent studies suggest that these methods may not be optimally effective for
code models. In this paper, we revisit the applicability of Spectral
Signature-based defenses in the context of backdoor attacks on code models. We
systematically evaluate their effectiveness under various attack scenarios and
defense configurations, analyzing their strengths and limitations. We found
that the widely used setting of Spectral Signature in code backdoor detection
is often suboptimal. Hence, we explored the impact of different settings of the
key factors. We discovered a new proxy metric that can more accurately estimate
the actual performance of Spectral Signature without model retraining after the
defense.

</details>


### [6] [One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery](https://arxiv.org/abs/2510.14036)
*Qiushi Wu,Yue Xiao,Dhilung Kirat,Kevin Eykholt,Jiyong Jang,Douglas Lee Schales*

Main category: cs.SE

TL;DR: BugStone是一个基于LLVM和大型语言模型的程序分析系统，能够利用已修复的bug实例识别代码中重复出现的错误模式，在Linux内核中发现了大量新的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 大型程序中存在重复出现的错误模式（RPBs），这些bug在程序的不同部分甚至不同程序中反复出现，但往往被遗漏。手动发现和修复每个实例非常耗时，而且bug报告可能扩大攻击面。

Method: 利用LLVM和大型语言模型分析程序，通过已修复的bug实例识别一致的错误模式（如特定API误用），然后在整个程序中搜索类似模式来识别潜在漏洞。

Result: 从135个独特RPBs开始，BugStone在Linux内核中识别了超过22K个新潜在问题，手动分析400个发现确认246个有效。在包含1.9K安全bug的数据集上，BugStone达到92.2%精确度和79.1%成对准确度。

Conclusion: RPBs在软件中广泛存在且严重影响安全性，BugStone系统能够有效识别这些重复错误模式，显著提高bug发现效率。

Abstract: Fixing bugs in large programs is a challenging task that demands substantial
time and effort. Once a bug is found, it is reported to the project
maintainers, who work with the reporter to fix it and eventually close the
issue. However, across the program, there are often similar code segments,
which may also contain the bug, but were missed during discovery. Finding and
fixing each recurring bug instance individually is labor intensive. Even more
concerning, bug reports can inadvertently widen the attack surface as they
provide attackers with an exploitable pattern that may be unresolved in other
parts of the program.
  In this paper, we explore these Recurring Pattern Bugs (RPBs) that appear
repeatedly across various code segments of a program or even in different
programs, stemming from a same root cause, but are unresolved. Our
investigation reveals that RPBs are widespread and can significantly compromise
the security of software programs. This paper introduces BugStone, a program
analysis system empowered by LLVM and a Large Language Model (LLM). The key
observation is that many RPBs have one patched instance, which can be leveraged
to identify a consistent error pattern, such as a specific API misuse. By
examining the entire program for this pattern, it is possible to identify
similar sections of code that may be vulnerable. Starting with 135 unique RPBs,
BugStone identified more than 22K new potential issues in the Linux kernel.
Manual analysis of 400 of these findings confirmed that 246 were valid. We also
created a dataset from over 1.9K security bugs reported by 23 recent top-tier
conference works. We manually annotate the dataset, identify 80 recurring
patterns and 850 corresponding fixes. Even with a cost-efficient model choice,
BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.

</details>


### [7] [David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation](https://arxiv.org/abs/2510.14115)
*Philipp Bauerfeind,Amir Salarpour,David Fernandez,Pedram MohajerAnsari,Johannes Reschke,Mert D. Pesé*

Main category: cs.SE

TL;DR: NL2Scenic是一个用于评估自然语言到Scenic代码生成的数据集和框架，包含146个NL/Scenic对和30个测试用例，评估了13个模型，发现GPT-4o表现最佳，而开源模型Qwen2.5Coder-14B在本地硬件上能达到GPT-4o 88%的专家评分。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言到Scenic代码生成中数据稀缺、可复现性差和评估指标不一致的问题，为自动驾驶场景编程提供标准化评估基准。

Method: 构建包含146个NL/Scenic对的数据集，开发Example Retriever和14种提示变体，使用文本指标（BLEU、ChrF、EDIT-SIM、CrystalBLEU）和执行指标（编译和生成）评估13个模型，并与专家研究对比。

Result: EDIT-SIM与人类判断相关性最好，提出的EDIT-COMP指标提高了排名保真度。检索增强提示能显著提升小模型性能，模型规模超过中等规模后收益递减。

Conclusion: NL2Scenic和EDIT-COMP为Scenic代码生成提供了标准化评估基础，表明中等规模的开源模型是自动驾驶场景编程的实用且经济高效的选择。

Abstract: Scenario simulation is central to testing autonomous driving systems. Scenic,
a domain-specific language (DSL) for CARLA, enables precise and reproducible
scenarios, but NL-to-Scenic generation with large language models (LLMs)
suffers from scarce data, limited reproducibility, and inconsistent metrics. We
introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a
difficulty-stratified 30-case test split, an Example Retriever, and 14
prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four
proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine
open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using
text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics
(compilation and generation), and compare them with an expert study (n=11).
EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of
EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking
fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88
percent of its expert score on local hardware. Retrieval-augmented prompting,
Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and
scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder
outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a
standardized, reproducible basis for evaluating Scenic code generation and
indicate that mid-size open-source models are practical, cost-effective options
for autonomous-driving scenario programming.

</details>


### [8] [Caruca: Effective and Efficient Specification Mining for Opaque Software Components](https://arxiv.org/abs/2510.14279)
*Evangelos Lamprou,Seong-Heon Jung,Mayank Keoliya,Lukas Lazarek,Konstantinos Kallas,Michael Greenberg,Nikos Vasilakis*

Main category: cs.SE

TL;DR: Caruca是一个自动为不透明命令挖掘规范的系统，它使用大语言模型将命令文档转换为结构化调用语法，通过执行命令-环境对来提取关键属性，如并行性和文件系统前后条件。


<details>
  <summary>Details</summary>
Motivation: 现有系统需要手动创建命令规范，这个过程繁琐且容易出错，限制了这些系统的实用性。

Method: 首先使用大语言模型将命令文档翻译为结构化调用语法，然后探索语法有效的命令调用和执行环境空间，通过系统调用和文件系统级别的拦截来提取命令属性。

Result: 在60个GNU Coreutils、POSIX和第三方命令上测试，除一个案例外，Caruca都能生成正确的规范，完全消除了手动工作，目前为一个最先进的静态分析工具提供完整规范。

Conclusion: Caruca能够自动生成命令规范，显著减少了手动工作量，提高了规范依赖系统的实用性。

Abstract: A wealth of state-of-the-art systems demonstrate impressive improvements in
performance, security, and reliability on programs composed of opaque
components, such as Unix shell commands. To reason about commands, these
systems require partial specifications. However, creating such specifications
is a manual, laborious, and error-prone process, limiting the practicality of
these systems. This paper presents Caruca, a system for automatic specification
mining for opaque commands. To overcome the challenge of language diversity
across commands, Caruca first instruments a large language model to translate a
command's user-facing documentation into a structured invocation syntax. Using
this representation, Caruca explores the space of syntactically valid command
invocations and execution environments. Caruca concretely executes each
command-environment pair, interposing at the system-call and filesystem level
to extract key command properties such as parallelizability and filesystem pre-
and post-conditions. These properties can be exported in multiple specification
formats and are immediately usable by existing systems. Applying Caruca across
60 GNU Coreutils, POSIX, and third-party commands across several
specification-dependent systems shows that Caruca generates correct
specifications for all but one case, completely eliminating manual effort from
the process and currently powering the full specifications for a
state-of-the-art static analysis tool.

</details>


### [9] [A Hybrid, Knowledge-Guided Evolutionary Framework for Personalized Compiler Auto-Tuning](https://arxiv.org/abs/2510.14292)
*Haolin Pan,Hongbin Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一种混合知识引导进化框架，通过离线分析构建编译知识库，在线阶段使用知识增强的遗传算法来寻找个性化优化序列，相比-Oz基线平均减少11.0% LLVM IR指令。


<details>
  <summary>Details</summary>
Motivation: 编译器优化标志如-O3和-Oz采用一刀切方法，无法充分发挥程序性能潜力。寻找最优编译通道序列是NP难问题，需要更智能的个性化优化方法。

Method: 混合知识引导进化框架：离线阶段构建包含通道行为向量、通道分组、协同通道图和原型序列的知识库；在线阶段使用知识增强的遗传算法进行语义感知重组和针对性变异。

Result: 在7个公共数据集上，相比高度优化的opt -Oz基线，平均额外减少11.0%的LLVM IR指令，展现了发现个性化高性能优化序列的最先进能力。

Conclusion: 该框架通过结合离线知识提取和在线进化搜索，有效解决了编译器通道自动调优问题，显著提升了程序优化效果。

Abstract: Compiler pass auto-tuning is critical for enhancing software performance, yet
finding the optimal pass sequence for a specific program is an NP-hard problem.
Traditional, general-purpose optimization flags like -O3 and -Oz adopt a
one-size-fits-all approach, often failing to unlock a program's full
performance potential. To address this challenge, we propose a novel Hybrid,
Knowledge-Guided Evolutionary Framework. This framework intelligently guides
online, personalized optimization using knowledge extracted from a large-scale
offline analysis phase. During the offline stage, we construct a comprehensive
compilation knowledge base composed of four key components: (1) Pass Behavioral
Vectors to quantitatively capture the effectiveness of each optimization; (2)
Pass Groups derived from clustering these vectors based on behavior similarity;
(3) a Synergy Pass Graph to model beneficial sequential interactions; and (4) a
library of Prototype Pass Sequences evolved for distinct program types. In the
online stage, a bespoke genetic algorithm leverages this rich knowledge base
through specially designed, knowledge-infused genetic operators. These
operators transform the search by performing semantically-aware recombination
and targeted, restorative mutations. On a suite of seven public datasets, our
framework achieves an average of 11.0% additional LLVM IR instruction reduction
over the highly-optimized opt -Oz baseline, demonstrating its state-of-the-art
capability in discovering personalized, high-performance optimization
sequences.

</details>


### [10] [A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments](https://arxiv.org/abs/2510.14339)
*Jialu Zhang,Jialiang Gu,Wangmeiyu Zhang,José Pablo Cambronero,John Kolesar,Ruzica Piskac,Daming Li,Hanyuan Shi*

Main category: cs.SE

TL;DR: 本文首次对在线编程中的TLE错误进行大规模实证研究，揭示了TLE错误的多种根本原因，并开发了首个专门修复TLE错误的自动化工具Nettle。


<details>
  <summary>Details</summary>
Motivation: 在线编程平台上的TLE错误难以解决，错误信息缺乏诊断价值，平台支持有限，现有调试工具帮助不大，导致许多用户在多次TLE失败后放弃提交。

Method: 手动分析1000个Codeforces的TLE提交，分类根本原因并追踪用户修复尝试；开发Nettle工具，将LLM与编译器生成的针对性自动反馈和测试用例相结合。

Result: Nettle在1000个真实案例中达到98.5%的修复率，远超最强LLM基线，所有修复都通过了Nettle-Eval和平台官方检查器。

Conclusion: TLE错误不仅源于算法效率问题，还包括无限循环、数据结构使用不当和I/O效率问题；Nettle是首个专门修复TLE错误的可靠自动化工具。

Abstract: Online programming platforms such as Codeforces and LeetCode attract millions
of users seeking to learn to program or refine their skills for industry
interviews. A major challenge for these users is the Time Limit Exceeded (TLE)
error, triggered when a program exceeds the execution time bound. Although
designed as a performance safeguard, TLE errors are difficult to resolve: error
messages provide no diagnostic insight, platform support is minimal, and
existing debugging tools offer little help. As a result, many users abandon
their submissions after repeated TLE failures.
  This paper presents the first large-scale empirical study of TLE errors in
online programming. We manually analyzed 1000 Codeforces submissions with TLE
errors, classified their root causes, and traced how users attempted to fix
them. Our analysis shows that TLE errors often arise not only from inefficient
algorithms but also from infinite loops, improper data structure use, and
inefficient I/O, challenging the conventional view that TLEs are purely
performance issues.
  Guided by these findings, we introduce Nettle, the first automated repair
tool specifically designed for TLE errors, and Nettle-Eval, the first framework
for evaluating TLE repairs. Integrating LLMs with targeted automated feedback
generated by the compiler and test cases, Nettle produces small, correct code
edits that eliminate TLEs while preserving functionality. Evaluated on the same
1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the
strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the
platform's official checker, confirming the reliability of our framework.

</details>


### [11] [PathFix: Automated Program Repair with Expected Path](https://arxiv.org/abs/2510.14341)
*Xu He,Shu Wang,Kun Sun*

Main category: cs.SE

TL;DR: PathFix是一种新的自动程序修复方法，利用从正确执行路径中提取的路径敏感约束来修复错误代码，通过结合大语言模型提升修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有APR方法因难以生成精确规范而面临两个主要挑战：生成过多可能的补丁候选和过拟合部分测试用例。

Method: PathFix通过四个步骤操作：1)追踪错误路径；2)分析期望路径；3)生成和评估补丁；4)验证补丁正确性。同时集成大语言模型以提升性能。

Result: 实验结果显示PathFix优于现有解决方案，特别是在处理复杂程序结构（如循环和递归）方面表现突出。

Conclusion: PathFix通过路径敏感约束和LLM集成，有效解决了APR中的过拟合和可扩展性问题，提升了程序修复的准确性和效率。

Abstract: Automated program repair (APR) techniques are effective in fixing inevitable
defects in software, enhancing development efficiency and software robustness.
However, due to the difficulty of generating precise specifications, existing
APR methods face two main challenges: generating too many plausible patch
candidates and overfitting them to partial test cases. To tackle these
challenges, we introduce a new APR method named PathFix, which leverages
path-sensitive constraints extracted from correct execution paths to generate
patches for repairing buggy code. It is based on one observation: if a buggy
program is repairable, at least one expected path is supposed to replace the
fault path in the patched program. PathFix operates in four main steps. First,
it traces fault paths reaching the fault output in the buggy program. Second,
it derives expected paths by analyzing the desired correct output on the
control flow graph, where an expected path defines how a feasible patch leads
to the correct execution. Third, PathFix generates and evaluates patches by
solving state constraints along the expected path. Fourth, we validate the
correctness of the generated patch. To further enhance repair performance and
mitigate scalability issues introduced by path-sensitive analysis, we integrate
a large language model (LLM) into our framework. Experimental results show that
PathFix outperforms existing solutions, particularly in handling complex
program structures such as loops and recursion.

</details>


### [12] [Towards Automated Governance: A DSL for Human-Agent Collaboration in Software Projects](https://arxiv.org/abs/2510.14465)
*Adem Ait,Gwendal Jouneaux,Javier Luis Cánovas Izquierdo,Jordi Cabot*

Main category: cs.SE

TL;DR: 提出一种新的领域特定语言（DSL），用于定义和执行涉及多样化利益相关者（包括AI代理）的软件项目治理政策。


<details>
  <summary>Details</summary>
Motivation: 软件开发的利益相关者日益多样化，包括来自不同背景的人类贡献者和AI代理，这在开源软件项目中带来了独特的治理挑战，因为明确的政策往往缺失或不清晰。

Method: 设计一种领域特定语言（DSL），能够定义和执行丰富的治理政策，支持系统涉及多样化利益相关者（包括代理）。

Result: 该DSL为实现更强大、适应性更强且最终自动化的治理提供了途径。

Conclusion: 这种DSL为软件项目（特别是开源项目）中更有效的协作铺平了道路。

Abstract: The stakeholders involved in software development are becoming increasingly
diverse, with both human contributors from varied backgrounds and AI-powered
agents collaborating together in the process. This situation presents unique
governance challenges, particularly in Open-Source Software (OSS) projects,
where explicit policies are often lacking or unclear. This paper presents the
vision and foundational concepts for a novel Domain-Specific Language (DSL)
designed to define and enforce rich governance policies in systems involving
diverse stakeholders, including agents. This DSL offers a pathway towards more
robust, adaptable, and ultimately automated governance, paving the way for more
effective collaboration in software projects, especially OSS ones.

</details>


### [13] [E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task](https://arxiv.org/abs/2510.14509)
*Jingyao Liu,Chen Huang,Zhizhao Guan,Wenqiang Lei,Yang Deng*

Main category: cs.SE

TL;DR: E2EDev是一个端到端软件开发基准，包含细粒度用户需求、BDD测试场景和自动化测试流水线，通过人机协同多智能体标注框架减少标注工作量，评估显示现有E2ESD框架在解决这些任务时仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有端到端软件开发(E2ESD)解决方案在有效解决实际任务方面存在困难，需要更有效和成本效益高的方法。

Method: 提出E2EDev基准，包含用户需求、BDD测试场景和自动化测试流水线，采用人机协同多智能体标注框架(HITL-MAA)来确保质量并减少标注工作量。

Result: 通过评估各种E2ESD框架和LLM骨干网络，分析显示这些框架在有效解决任务方面持续挣扎。

Conclusion: 需要更有效和成本效益高的E2ESD解决方案，E2EDev基准和代码库已公开可用。

Abstract: E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple
BDD test scenarios with corresponding Python step implementations for each
requirement}, and (iii) a fully automated testing pipeline built on the Behave
framework. To ensure its quality while reducing the annotation effort, E2EDev
leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework
(HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with
E2EDev}, our analysis reveals a persistent struggle to effectively solve these
tasks, underscoring the critical need for more effective and cost-efficient
E2ESD solutions. Our codebase and benchmark are publicly available at
https://github.com/SCUNLP/E2EDev.

</details>


### [14] [Software Testing Education and Industry Needs - Report from the ENACTEST EU Project](https://arxiv.org/abs/2510.14625)
*Mehrdad Saadatmand,Abbas Khan,Beatriz Marin,Ana C. R Paiva,Nele Van Asch,Graham Moran,Felix Cammaerts,Monique Snoeck,Alexandra Mendes*

Main category: cs.SE

TL;DR: 本研究调查了工业界对软件测试能力的需求，识别了当前测试教育中的知识差距，并通过焦点小组、访谈和范围综述发现AI测试、安全测试和软技能等领域存在显著知识缺口。


<details>
  <summary>Details</summary>
Motivation: 软件开发的不断演变要求测试人员持续适应新工具和实践，本研究旨在了解行业对测试能力的具体需求，并识别当前教育与实际需求之间的差距。

Method: 采用焦点小组讨论、专业人士访谈和范围综述的方法，研究工具由ENACTEST项目联盟共同设计，通过多次迭代完善，确保全面覆盖行业需求和教育差距。

Result: 研究发现工业界在专业培训方法、培训挑战、培训质量评估、学术教育与行业需求的知识差距、未来测试教育趋势以及公司内部知识传递方法等方面存在显著问题，范围综述确认了AI测试、安全测试和软技能等领域的知识缺口。

Conclusion: 软件测试教育需要更好地适应行业需求，特别是在新兴技术领域如AI测试和安全测试，以及软技能培养方面需要加强，以缩小学术教育与实际工作需求之间的差距。

Abstract: The evolving landscape of software development demands that software testers
continuously adapt to new tools, practices, and acquire new skills. This study
investigates software testing competency needs in industry, identifies
knowledge gaps in current testing education, and highlights competencies and
gaps not addressed in academic literature. This is done by conducting two focus
group sessions and interviews with professionals across diverse domains,
including railway industry, healthcare, and software consulting and performing
a curated small-scale scoping review. The study instrument, co-designed by
members of the ENACTEST project consortium, was developed collaboratively and
refined through multiple iterations to ensure comprehensive coverage of
industry needs and educational gaps. In particular, by performing a thematic
qualitative analysis, we report our findings and observations regarding:
professional training methods, challenges in offering training in industry,
different ways of evaluating the quality of training, identified knowledge gaps
with respect to academic education and industry needs, future needs and trends
in testing education, and knowledge transfer methods within companies. Finally,
the scoping review results confirm knowledge gaps in areas such as AI testing,
security testing and soft skills.

</details>


### [15] [ATGen: Adversarial Reinforcement Learning for Test Case Generation](https://arxiv.org/abs/2510.14635)
*Qingyao Li,Xinyi Dai,Weiwen Liu,Xiangyang Li,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.SE

TL;DR: ATGen是一个通过对抗性强化学习训练测试用例生成的框架，它让测试生成器与对抗性代码生成器相互竞争，动态生成越来越难的bug来突破静态数据集的难度上限。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态数据集的测试生成方法存在"固定难度上限"问题，无法发现训练范围之外的新颖或更复杂bug。

Method: 采用对抗性强化学习框架，让测试生成器与对抗性代码生成器相互竞争，测试生成器通过RL优化同时最大化"输出准确性"和"攻击成功率"。

Result: 实验表明ATGen显著优于现有最先进方法，既能作为更有效的Best-of-N推理过滤器，也能作为更高质量的奖励源来训练代码生成模型。

Conclusion: ATGen建立了一个新的动态范式，用于提高LLM生成代码的可靠性。

Abstract: Large Language Models (LLMs) excel at code generation, yet their outputs
often contain subtle bugs, for which effective test cases are a critical
bottleneck. Existing test generation methods, whether based on prompting or
supervised fine-tuning, rely on static datasets. This imposes a
``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover
novel or more complex bugs beyond their training scope. To overcome this, we
introduce ATGen, a framework that trains a test case generator via adversarial
reinforcement learning. ATGen pits a test generator against an adversarial code
generator that continuously crafts harder bugs to evade the current policy.
This dynamic loop creates a curriculum of increasing difficulty challenging
current policy. The test generator is optimized via Reinforcement Learning (RL)
to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to
learn a progressively stronger policy that breaks the fixed-difficulty ceiling
of static training. Extensive experiments demonstrate that ATGen significantly
outperforms state-of-the-art baselines. We further validate its practical
utility, showing it serves as both a more effective filter for Best-of-N
inference and a higher-quality reward source for training code generation
models. Our work establishes a new, dynamic paradigm for improving the
reliability of LLM-generated code.

</details>


### [16] [Requirement Identification for Traffic Simulations in Driving Simulators](https://arxiv.org/abs/2510.14653)
*Sven Tarlowski,Lutz Eckstein*

Main category: cs.SE

TL;DR: 提出一种系统识别交通仿真需求的方法论，通过分阶段子目标来确保仿真真实性，支持汽车开发和测试。


<details>
  <summary>Details</summary>
Motivation: 解决确保交通仿真真实性的挑战，提高实验结果的可靠性和参与者参与度。

Method: 基于各研究阶段的子目标，采用结构化方法推导微观层面、智能体模型和视觉表示的具体技术需求。

Result: 建立了研究目标与交通仿真设计之间的清晰联系，提高了仿真的保真度。

Conclusion: 该方法论能够支持稳健的汽车开发和测试，通过系统化的需求识别确保交通仿真的真实性。

Abstract: This paper addresses the challenge of ensuring realistic traffic conditions
by proposing a methodology that systematically identifies traffic simulation
requirements. Using a structured approach based on sub-goals in each study
phase, specific technical needs are derived for microscopic levels, agent
models, and visual representation. The methodology aims to maintain a high
degree of fidelity, enhancing both the validity of experimental outcomes and
participant engagement. By providing a clear link between study objectives and
traffic simulation design, this approach supports robust automotive development
and testing.

</details>


### [17] [LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?](https://arxiv.org/abs/2510.14700)
*Bin Liu,Yanjie Zhao,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: 本文对最先进的LLM代理在自动化Web漏洞复现方面进行了首次全面评估，发现虽然LLM代理在简单漏洞上表现尚可，但在需要复杂环境配置的漏洞上表现不佳，存在显著的能力差距。


<details>
  <summary>Details</summary>
Motivation: LLM代理在软件工程和网络安全任务中表现出色，但在自动化Web漏洞复现这一关键但未充分探索的应用领域仍面临挑战，需要系统评估其实际能力。

Method: 系统评估了20个来自不同领域的LLM代理在16个维度上的表现，包括技术能力、环境适应性和用户体验因素，并在3个代表性Web漏洞上进行测试，然后选择表现最佳的3个代理在包含80个真实CVE的基准数据集上进行深入评估。

Result: LLM代理在简单的基于库的漏洞上取得合理成功，但在需要多组件环境的复杂基于服务的漏洞上持续失败。复杂环境配置和认证障碍导致代理能执行漏洞利用代码但无法触发实际漏洞，性能在认证信息不完整时下降超过33%。

Conclusion: 当前LLM代理能力与可靠的自动化漏洞复现需求之间存在显著差距，需要在环境适应和自主问题解决能力方面取得进展。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities
in software engineering and cybersecurity tasks, including code generation,
vulnerability discovery, and automated testing. One critical but underexplored
application is automated web vulnerability reproduction, which transforms
vulnerability reports into working exploits. Although recent advances suggest
promising potential, challenges remain in applying LLM agents to real-world web
vulnerability reproduction scenarios. In this paper, we present the first
comprehensive evaluation of state-of-the-art LLM agents for automated web
vulnerability reproduction. We systematically assess 20 agents from software
engineering, cybersecurity, and general domains across 16 dimensions, including
technical capabilities, environment adaptability, and user experience factors,
on 3 representative web vulnerabilities. Based on the results, we select three
top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation
on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types
and 6 web technologies. Our results reveal that while LLM agents achieve
reasonable success on simple library-based vulnerabilities, they consistently
fail on complex service-based vulnerabilities requiring multi-component
environments. Complex environment configurations and authentication barriers
create a gap where agents can execute exploit code but fail to trigger actual
vulnerabilities. We observe high sensitivity to input guidance, with
performance degrading by over 33% under incomplete authentication information.
Our findings highlight the significant gap between current LLM agent
capabilities and the demands of reliable automated vulnerability reproduction,
emphasizing the need for advances in environmental adaptation and autonomous
problem-solving capabilities.

</details>


### [18] [Leveraging Code Cohesion Analysis to Identify Source Code Supply Chain Attacks](https://arxiv.org/abs/2510.14778)
*Maor Reuben,Ido Mendel,Or Feldman,Moshe Kravchik,Mordehai Guri,Rami Puzis*

Main category: cs.SE

TL;DR: 提出一种基于名称预测的内聚度（NPC）指标的无监督方法，通过量化源代码中的内聚度破坏来检测恶意代码注入。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击通过向合法项目中注入恶意代码严重威胁软件安全，这类攻击虽然罕见但破坏性极大。自动检测虚假代码注入具有挑战性，因为需要理解插入代码及其上下文的意图。

Method: 使用基于名称预测的内聚度（NPC）指标分析函数内聚度变化，比较恶意代码引入时与自然内聚度波动的差异。分析了369个开源C++仓库中的54,707个函数。

Result: 代码注入会降低内聚度，并使命名模式转向更短、描述性更差的名称。在极端不平衡测试集下，NPC方法在1:1,000比例下达到36.41%的Precision@100，在1:10,000比例下达到12.47%。

Conclusion: 自动内聚度测量，特别是基于名称预测的内聚度，可能有助于识别供应链攻击，提高源代码完整性。

Abstract: Supply chain attacks significantly threaten software security with malicious
code injections within legitimate projects. Such attacks are very rare but may
have a devastating impact. Detecting spurious code injections using automated
tools is further complicated as it often requires deciphering the intention of
both the inserted code and its context. In this study, we propose an
unsupervised approach for highlighting spurious code injections by quantifying
cohesion disruptions in the source code. Using a name-prediction-based cohesion
(NPC) metric, we analyze how function cohesion changes when malicious code is
introduced compared to natural cohesion fluctuations. An analysis of 54,707
functions over 369 open-source C++ repositories reveals that code injection
reduces cohesion and shifts naming patterns toward shorter, less descriptive
names compared to genuine function updates. Considering the sporadic nature of
real supply-chain attacks, we evaluate the proposed method with extreme
test-set imbalance and show that monitoring high-cohesion functions with NPC
can effectively detect functions with injected code, achieving a Precision@100
of 36.41% at a 1:1,000 ratio and 12.47% at 1:10,000. These results suggest that
automated cohesion measurements, in general, and name-prediction-based
cohesion, in particular, may help identify supply chain attacks, improving
source code integrity.

</details>


### [19] [Instruction Set Migration at Warehouse Scale](https://arxiv.org/abs/2510.14928)
*Eric Christopher,Kevin Crossan,Wolff Dobson,Chris Kennelly,Drew Lewis,Kun Lin,Martin Maas,Parthasarathy Ranganathan,Emma Rapati,Brian Yang*

Main category: cs.SE

TL;DR: 该论文分析了从x86到Arm架构的大规模代码迁移挑战，发现现代ISA迁移主要依赖重新编译而非二进制翻译，并展示了AI在自动化迁移任务中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为ISA迁移的主要挑战是二进制翻译，但现代开源生态系统使得重新编译成为可能，这带来了新的多方面挑战。

Method: 通过分析Google从x86到Arm的40,000个代码提交的大规模迁移，构建了ISA迁移任务的分类法，并展示了自动化方法和AI的应用。

Result: 识别了ISA迁移中的关键任务，展示了Google如何自动化这些步骤，并证明了AI在自动处理这些任务中的重要作用。

Conclusion: 现代ISA迁移面临与二进制翻译不同的新挑战，AI可以显著帮助自动化迁移过程，但仍有一些挑战性任务需要进一步研究。

Abstract: Migrating codebases from one instruction set architecture (ISA) to another is
a major engineering challenge. A recent example is the adoption of Arm (in
addition to x86) across the major Cloud hyperscalers. Yet, this problem has
seen limited attention by the academic community. Most work has focused on
static and dynamic binary translation, and the traditional conventional wisdom
has been that this is the primary challenge.
  In this paper, we show that this is no longer the case. Modern ISA migrations
can often build on a robust open-source ecosystem, making it possible to
recompile all relevant software from scratch. This introduces a new and
multifaceted set of challenges, which are different from binary translation.
  By analyzing a large-scale migration from x86 to Arm at Google, spanning
almost 40,000 code commits, we derive a taxonomy of tasks involved in ISA
migration. We show how Google automated many of the steps involved, and
demonstrate how AI can play a major role in automatically addressing these
tasks. We identify tasks that remain challenging and highlight research
challenges that warrant further attention.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [20] [Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management](https://arxiv.org/abs/2510.14024)
*Thanh Son Phung,Douglas Thain*

Main category: cs.DC

TL;DR: 提出了一种名为"Pervasive Context Management"的技术，通过将LLM初始化上下文与实际推理分离并保留在GPU中，显著减少了科学计算中集成LLM应用的长等待时间和高启动成本。


<details>
  <summary>Details</summary>
Motivation: 当前HPC集群设计无法有效支持集成轻量级LLM的高通量科学应用，要么面临静态批处理队列的长等待时间，要么在资源抢占时重复支付昂贵的LLM启动成本。

Method: 采用"上下文解耦"策略，将LLM初始化上下文与实际LLM推理分离，并在GPU中保留上下文直到不再需要，实现"Pervasive Context Management"技术。

Result: 应用该技术后，事实验证应用的执行时间减少了72.1%（从3小时降至48分钟），使用相同GPU数量；在集群32.8%的GPU上实现机会性扩展，进一步将执行时间降至13分钟。

Conclusion: Pervasive Context Management技术有效解决了HPC集群中LLM应用的资源管理问题，显著提升了执行效率并实现了更好的资源利用率。

Abstract: The rise of Generative AI introduces a new class of HPC workloads that
integrates lightweight LLMs with traditional high-throughput applications to
accelerate scientific discovery. The current design of HPC clusters is
inadequate to support this new class however, either incurring long wait times
on static batch queues or repeatedly paying expensive LLM startup costs upon
resource preemption. To circumvent both the long queues and high startup costs,
we propose to "decouple" the LLM initialization context from the actual LLM
inferences, and retain the context in GPUs until it is no longer needed, a
technique we term "Pervasive Context Management". We transform a fact
verification application to enable this technique, allowing it to reduce its
execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of
GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and
further reduce the execution time to 13 minutes.

</details>


### [21] [Anonymized Network Sensing using C++26 std::execution on GPUs](https://arxiv.org/abs/2510.14050)
*Michael Mandulak,Sayan Ghosh,S M Ferdous,Mahantesh Halappanavar,George Slota*

Main category: cs.DC

TL;DR: 本文探讨了使用C++26 Senders模型在密集GPU系统上开发网络感知图挑战的实践方面，相比串行GraphBLAS基准实现了高达55倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着网络数据包数据日益庞大，基于GPU的并行方法成为网络分析的主流，但仍面临主机-设备内存管理和复杂工作负载移植等启动挑战。

Method: 采用C++26 Senders异步数据操作链模型，将GPU作为一等执行资源，通过表达性和标准化的异步语义开发多GPU应用工作负载。

Result: 基于商品库的实现相比参考串行GraphBLAS基准，在8个NVIDIA A100 GPU上实现了高达55倍的性能改进。

Conclusion: 采用通用且高效的程序模型不一定会影响关键路径性能，相比低级别专有供应商编程模型仍能获得显著性能提升。

Abstract: Large-scale network sensing plays a vital role in network traffic analysis
and characterization. As network packet data grows increasingly large, parallel
methods have become mainstream for network analytics. While effective,
GPU-based implementations still face start-up challenges in host-device memory
management and porting complex workloads on devices, among others. To mitigate
these challenges, composable frameworks have emerged using modern C++
programming language, for efficiently deploying analytics tasks on GPUs.
Specifically, the recent C++26 Senders model of asynchronous data operation
chaining provides a simple interface for bulk pushing tasks to varied device
execution contexts.
  Considering the prominence of contemporary dense-GPU platforms and
vendor-leveraged software libraries, such a programming model consider GPUs as
first-class execution resources (compared to traditional host-centric
programming models), allowing convenient development of multi-GPU application
workloads via expressive and standardized asynchronous semantics. In this
paper, we discuss practical aspects of developing the Anonymized Network
Sensing Graph Challenge on dense-GPU systems using the recently proposed C++26
Senders model. Adopting a generic and productive programming model does not
necessarily impact the critical-path performance (as compared to low-level
proprietary vendor-based programming models): our commodity library-based
implementation achieves up to 55x performance improvements on 8x NVIDIA A100
GPUs as compared to the reference serial GraphBLAS baseline.

</details>


### [22] [Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving](https://arxiv.org/abs/2510.14126)
*Nikos Pagonas,Yeounoh Chung,Kostis Kaffes,Arvind Krishnamurthy*

Main category: cs.DC

TL;DR: Cortex是一个专为智能体工作负载设计的原型工作流感知服务平台，通过阶段隔离策略提升性能和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 解决智能体工作流中各阶段在计算和内存方面的相互干扰问题，提高KV缓存利用率、吞吐量和性能可预测性。

Method: 采用阶段隔离原则，为智能体工作流的每个不同阶段配置专用资源池，定制化资源分配和调度策略。

Result: 通过阶段隔离策略减轻了阶段间干扰，实现了更好的KV缓存利用、更高的吞吐量和更可预测的性能。

Conclusion: Cortex为更先进的智能体原生服务范式奠定了基础，包括可塑资源管理、工作流分支的推测执行以及智能体状态的共享多级缓存。

Abstract: We introduce Cortex, a prototype workflow-aware serving platform designed for
agentic workloads. The core principle of Cortex is stage isolation: it
provisions dedicated resource pools for each distinct stage of an agentic
workflow. This simple yet powerful strategy mitigates inter-stage interference
in compute and memory, leading to better KV cache utilization, higher
throughput, and more predictable performance. By customizing resource
allocation and scheduling within each distinct stage of agentic workflows,
Cortex lays the groundwork for more advanced, agent-native serving paradigms,
including malleable resource management, speculative execution of workflow
branches, and a shared, multi-tiered cache for "agentic state."

</details>


### [23] [Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction](https://arxiv.org/abs/2510.14147)
*Gabriel Raulet,Dmitriy Morozov,Aydin Buluc,Katherine Yelick*

Main category: cs.DC

TL;DR: 提出了一种可扩展的分布式内存算法，使用覆盖树计算一般度量空间中的固定半径近邻图，在真实世界高维数据集上实现了显著的并行加速。


<details>
  <summary>Details</summary>
Motivation: 随着计算能力和数据获取方法的进步，大型科学数据集需要可扩展的近邻图计算解决方案。现有并行近邻算法主要关注欧几里得空间和近似解，但许多应用需要精确解和非欧几里得度量。

Method: 开发了共享内存的覆盖树构建算法，并引入两种分布式内存算法：简单的点分区策略和空间分区策略，在节点上利用覆盖树算法。

Result: 在各种真实和合成数据集上展示了并行扩展性。在100万点的高维数据集上，使用1024核时比现有技术快678.34倍（平均每个顶点70个邻居），使用4096核时快1590.99倍（平均每个顶点500个邻居）。

Conclusion: 该算法为一般度量空间中的固定半径近邻图计算提供了高效的可扩展解决方案，特别适用于需要精确解和非欧几里得度量的应用场景。

Abstract: Computing fixed-radius near-neighbor graphs is an important first step for
many data analysis algorithms. Near-neighbor graphs connect points that are
close under some metric, endowing point clouds with a combinatorial structure.
As computing power and data acquisition methods advance, diverse sources of
large scientific datasets would greatly benefit from scalable solutions to this
common subroutine for downstream analysis. Prior work on parallel nearest
neighbors has made great progress in problems like k-nearest and approximate
nearest neighbor search problems, with particular attention on Euclidean
spaces. Yet many applications need exact solutions and non-Euclidean metrics.
This paper presents a scalable sparsity-aware distributed memory algorithm
using cover trees to compute near-neighbor graphs in general metric spaces. We
provide a shared-memory algorithm for cover tree construction and demonstrate
its competitiveness with state-of-the-art fixed-radius search data structures.
We then introduce two distributed-memory algorithms for the near-neighbor graph
problem, a simple point-partitioning strategy and a spatial-partitioning
strategy, which leverage the cover tree algorithm on each node. Our algorithms
exhibit parallel scaling across a variety of real and synthetic datasets for
both traditional and non-traditional metrics. On real world high dimensional
datasets with one million points, we achieve speedups up to 678.34x over the
state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on
average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per
vertex (on average).

</details>


### [24] [Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability](https://arxiv.org/abs/2510.14151)
*Saeed Moradi,Koosha Esmaeilzadeh Khorasani,Sara Rouhani*

Main category: cs.DC

TL;DR: 提出了一个区块链无关的框架，用于实现许可链和无许可链之间的互操作性，通过加密技术确保安全数据交换，轻量级架构简化实现，集成Clover和Dandelion++协议增强交易匿名性。


<details>
  <summary>Details</summary>
Motivation: 互操作性对于将区块链从孤立网络转变为协作生态系统至关重要。虽然公链互操作性取得了进展，但许可链和无许可链之间的桥接由于访问控制、架构和安全要求的差异面临独特挑战。

Method: 采用区块链无关框架，利用加密技术确保安全数据交换，轻量级架构设计简化实现和维护，集成Clover和Dandelion++协议增强交易匿名性。

Result: 性能评估通过测量转发时间、吞吐量、可用性及其共谋影响，证明了该框架在异构区块链生态系统中实现安全高效互操作性的有效性。

Conclusion: 该框架成功解决了许可链和无许可链之间的互操作性挑战，通过安全的数据交换机制和轻量级设计实现了高效的跨链协作。

Abstract: Interoperability is essential for transforming blockchains from isolated
networks into collaborative ecosystems, unlocking their full potential. While
significant progress has been made in public blockchain interoperability,
bridging permissioned and permissionless blockchains poses unique challenges
due to differences in access control, architectures, and security requirements.
This paper introduces a blockchain-agnostic framework to enable
interoperability between permissioned and permissionless networks. Leveraging
cryptographic techniques, the framework ensures secure data exchanges. Its
lightweight architectural design simplifies implementation and maintenance,
while the integration of Clover and Dandelion++ protocols enhances transaction
anonymity. Performance evaluations demonstrate the framework's effectiveness in
achieving secure and efficient interoperability by measuring the forwarding
time, the throughput, the availability, and their collusion impact of the
system across heterogeneous blockchain ecosystems.

</details>


### [25] [Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs](https://arxiv.org/abs/2510.14186)
*Pengkun Ren,Hai Dong,Nasrin Sohrabi,Zahir Tari,Pengcheng Zhang*

Main category: cs.DC

TL;DR: AUTIG是一个高性能、可插拔的顺序公平服务，通过非对称架构解决BFT共识中的顺序公平问题。它使用增量图维护和可验证证明，避免了传统对称协议中每个副本都需要重新计算排序的开销。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序公平共识协议（如Themis）要求每个副本重新运行领导者的昂贵排序计算进行验证，这种对称冗余范式效率低下。AUTIG旨在打破这种对称性，提高性能。

Method: 采用非对称架构：领导者维护持久化的未确认交易增量图（UTIG），分摊多轮图构建成本，并为每个提案生成结构化公平性证明；跟随者无需维护历史状态即可验证证明。

Result: 实验表明，AUTIG在保持gamma-batch-order-fairness的同时，实现了更高的吞吐量和更低的端到端延迟。

Conclusion: AUTIG通过非对称架构和可验证证明机制，有效解决了传统顺序公平共识协议的性能瓶颈，为区块链系统提供了更高效的抗价值提取攻击解决方案。

Abstract: Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on
transaction ordering despite malicious actors, but unconstrained ordering power
enables sophisticated value extraction attacks like front running and sandwich
attacks - a critical threat to blockchain systems. Order-fair consensus curbs
adversarial value extraction by constraining how leaders may order
transactions. While state-of-the-art protocols such as Themis attain strong
guarantees through graph-based ordering, they ask every replica to re-run the
leader's expensive ordering computation for validation - an inherently
symmetric and redundant paradigm. We present AUTIG, a high-performance,
pluggable order-fairness service that breaks this symmetry. Our key insight is
that verifying a fair order does not require re-computing it. Instead,
verification can be reduced to a stateless audit of succinct, verifiable
assertions about the ordering graph's properties. AUTIG realizes this via an
asymmetric architecture: the leader maintains a persistent
Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction
across rounds and emits a structured proof of fairness with each proposal;
followers validate the proof without maintaining historical state. AUTIG
introduces three critical innovations: (i) incremental graph maintenance driven
by threshold-crossing events and state changes; (ii) a decoupled pipeline that
overlaps leader-side collection/update/extraction with follower-side stateless
verification; and (iii) a proof design covering all internal pairs in the
finalized prefix plus a frontier completeness check to rule out hidden external
dependencies. We implement AUTIG and evaluate it against symmetric graph-based
baselines under partial synchrony. Experiments show higher throughput and lower
end-to-end latency while preserving gamma-batch-order-fairness.

</details>


### [26] [FairBatching: Fairness-Aware Batch Formation for LLM Inference](https://arxiv.org/abs/2510.14392)
*Hongtao Lyu,Boyue Liu,Mingyu Wu,Haibo Chen*

Main category: cs.DC

TL;DR: FairBatching是一个新型LLM推理调度器，通过公平分配prefill和decode任务的计算资源，解决了现有调度器中的计算不公平问题，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理调度器在最小化首token延迟和维持高token生成率之间存在矛盾，Sarathi提出的无停顿批处理调度器虽然防止了解码停顿，但引入了显著的计算不公平性，过度优先解码任务导致资源利用不足和prefill排队延迟。

Method: 提出FairBatching调度器，包含自适应批容量确定机制、公平动态批形成算法和新型负载估计方法，动态调整计算预算，打破解码优先范式，实现全局公平。

Result: 在真实trace上评估，FairBatching将TTFT尾延迟降低达2.29倍，稳健维持TPOT SLO，单节点容量提升20.0%，集群级容量提升54.3%。

Conclusion: FairBatching通过公平资源分配和自适应调度策略，有效解决了LLM推理中的计算不公平问题，显著提升了系统服务质量和服务容量。

Abstract: Large language model (LLM) inference systems face a fundamental tension
between minimizing Time-to-First-Token (TTFT) latency for new requests and
maintaining a high, steady token generation rate (low Time-Per-Output-Token, or
TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by
Sarathi, while effective at preventing decode stalls, introduce significant
computational unfairness. They prioritize decode tasks excessively,
simultaneously leading to underutilized decode slack and unnecessary prefill
queuing delays, which collectively degrade the system's overall quality of
service (QoS).
  This work identifies the root cause of this unfairness: the non-monotonic
nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid
decode-prioritizing policy that fails to adapt to dynamic workload bursts. We
therefore propose FairBatching, a novel LLM inference scheduler that enforces
fair resource allocation between prefill and decode tasks. It features an
adaptive batch capacity determination mechanism, which dynamically adjusts the
computational budget to improve the GPU utilization without triggering SLO
violations. Its fair and dynamic batch formation algorithm breaks away from the
decode-prioritizing paradigm, allowing computation resources to be reclaimed
from bursting decode tasks to serve prefill surges, achieving global fairness.
Furthermore, FairBatching provides a novel load estimation method, enabling
more effective coordination with upper-level schedulers. Implemented and
evaluated on realistic traces, FairBatching significantly reduces TTFT tail
latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall
20.0% improvement in single-node capacity and 54.3% improvement in
cluster-level capacity.

</details>


### [27] [ScalePool: Hybrid XLink-CXL Fabric for Composable Resource Disaggregation in Unified Scale-up Domains](https://arxiv.org/abs/2510.14580)
*Hyein Woo,Miryeong Kwon,Jiseon Kim,Eunjee Na,Hanjin Choi,Seonghyeon Jang,Myoungsoo Jung*

Main category: cs.DC

TL;DR: ScalePool提出了一种新型集群架构，通过统一硬件互连技术连接大量加速器，采用XLink-CXL混合结构实现低延迟通信和可扩展内存共享，显著提升LLM训练性能和内存密集型工作负载效率。


<details>
  <summary>Details</summary>
Motivation: 传统长距离网络互连在加速器集群中存在延迟高、互操作性差的问题，需要一种能够统一硬件互连、支持异构集群操作和可组合资源解聚的架构。

Method: 采用XLink-CXL混合结构：XLink用于集群内低延迟加速器通信，分层CXL交换结构用于可扩展的集群间内存共享。引入显式内存分层：第1层结合加速器本地内存与CXL/XLink，第2层通过CXL互连专用内存节点实现内存池化。

Result: 相比传统RDMA环境，ScalePool平均加速LLM训练1.22倍，最高达1.84倍；第2层内存解聚策略为内存密集型工作负载降低延迟最高4.5倍。

Conclusion: ScalePool通过统一的硬件互连架构有效解决了加速器集群的互操作性和性能瓶颈，为大规模AI训练提供了高效解决方案。

Abstract: This paper proposes ScalePool, a novel cluster architecture designed to
interconnect numerous accelerators using unified hardware interconnects rather
than traditional long-distance networking. ScalePool integrates
Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified
XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for
intra-cluster, low-latency accelerator communication, while using hierarchical
CXL-based switching fabrics for scalable and coherent inter-cluster memory
sharing. By abstracting interfaces through CXL, ScalePool structurally resolves
interoperability constraints, enabling heterogeneous cluster operation and
composable resource disaggregation. In addition, ScalePool introduces explicit
memory tiering: the latency-critical tier-1 combines accelerator-local memory
with coherence-centric CXL and XLink, whereas the highcapacity tier-2 employs
dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable
and efficient memory pooling. Evaluation results show that ScalePool
accelerates LLM training by 1.22x on average and up to 1.84x compared to
conventional RDMA-based environments. Furthermore, the proposed tier-2 memory
disaggregation strategy reduces latency by up to 4.5x for memory-intensive
workloads.

</details>


### [28] [JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization](https://arxiv.org/abs/2510.14599)
*Michal Konopa,Jan Fesl,Ladislav Ber ánek*

Main category: cs.DC

TL;DR: JASDA是一个基于SJA概念的新型去中心化调度范式，通过双向迭代交互将拍卖理论和在线优化与GPU工作负载的时间粒度相结合，为MIG-enabled GPU提供可扩展的资源管理。


<details>
  <summary>Details</summary>
Motivation: MIG-enabled GPU上工作负载的复杂性和时间可变性日益增加，挑战了传统集中式调度的可扩展性。

Method: JASDA扩展SJA从集中式调度模型转向完全去中心化的协商过程，作业主动生成和评分可行子作业，调度器执行策略驱动的清理过程。

Result: 这种双向迭代交互将反馈、校准和概率安全性直接嵌入调度循环，实现自适应和透明的决策制定。

Conclusion: JASDA为市场感知和公平驱动的资源管理提供了可扩展基础，将理论调度模型与现代MIG-enabled环境中的实际部署相连接。

Abstract: The increasing complexity and temporal variability of workloads on
MIG-enabled GPUs challenge the scalability of traditional centralized
scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel
paradigm that extends SJA from a largely centralized scheduling model toward a
fully decentralized negotiation process. In JASDA, jobs actively generate and
score feasible subjobs in response to scheduler-announced execution windows,
while the scheduler performs policy-driven clearing that balances utilization,
fairness, and temporal responsiveness. This bidirectional, iterative
interaction embeds feedback, calibration, and probabilistic safety directly
into the scheduling loop, enabling adaptive and transparent decision-making. By
coupling principles from auction theory and online optimization with the
temporal granularity of GPU workloads, JASDA provides a scalable foundation for
market-aware and fairness-driven resource management-bridging theoretical
scheduling models with practical deployment in modern MIG-enabled environments
relevant to Artificial Intelligence and Agriculture 4.0.

</details>


### [29] [MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems](https://arxiv.org/abs/2510.14622)
*Miryeong Kwon,Donghyun Gouk,Hyein Woo,Junhee Kim,Jinwoo Baek,Kyungkuk Nam,Sangyoon Ji,Jiseon Kim,Hanyeoreum Bae,Junhyeok Jang,Hyunwoo You,Junseok Moon,Myoungsoo Jung*

Main category: cs.DC

TL;DR: MPI-over-CXL是一种利用CXL技术的新型MPI通信范式，通过共享内存直接访问替代传统数据拷贝方法，显著降低通信延迟和内存带宽使用。


<details>
  <summary>Details</summary>
Motivation: 传统MPI实现依赖显式内存拷贝操作，导致冗余数据移动和缓冲区管理的开销，严重影响涉及密集处理器间通信的HPC工作负载性能。

Method: 利用CXL提供跨多主机的缓存一致性共享内存，将共享内存区域直接映射到MPI进程的虚拟地址空间，实现基于指针的高效通信，消除冗余拷贝操作。

Result: 使用代表性基准测试进行评估，相比传统MPI系统显示出显著的性能提升。

Conclusion: MPI-over-CXL有潜力在大规模HPC环境中提高效率和可扩展性。

Abstract: MPI implementations commonly rely on explicit memory-copy operations,
incurring overhead from redundant data movement and buffer management. This
overhead notably impacts HPC workloads involving intensive inter-processor
communication. In response, we introduce MPI-over-CXL, a novel MPI
communication paradigm leveraging CXL, which provides cache-coherent shared
memory across multiple hosts. MPI-over-CXL replaces traditional data-copy
methods with direct shared memory access, significantly reducing communication
latency and memory bandwidth usage. By mapping shared memory regions directly
into the virtual address spaces of MPI processes, our design enables efficient
pointer-based communication, eliminating redundant copying operations. To
validate this approach, we implement a comprehensive hardware and software
environment, including a custom CXL 3.2 controller, FPGA-based multi-host
emulation, and dedicated software stack. Our evaluations using representative
benchmarks demonstrate substantial performance improvements over conventional
MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and
scalability in large-scale HPC environments.

</details>


### [30] [xLLM Technical Report](https://arxiv.org/abs/2510.14686)
*Tongxuan Liu,Tao Peng,Peijun Yang,Xiaoyang Zhao,Xiusheng Lu,Weizhe Huang,Zirui Liu,Xiaoyu Chen,Zhiwei Liang,Jun Xiong,Donghe Jin,Minchao Zhang,Jinrong Guo,Yingxu Deng,Xu Zhang,Xianzhe Dong,Siqi Wang,Siyu Wu,Yu Wu,Zihan Tang,Yuting Zeng,Yanshu Wang,Jinguang Liu,Meng Kang,Menxin Li,Yunlong Wang,Yiming Liu,Xiaolong Ma,Yifan Wang,Yichen Zhang,Jinrun Yin,Keyang Zheng,Jiawei Yin,Jun Zhang,Ziyue Wang,Xiaobo Lin,Liangyu Liu,Liwei Lan,Yang Liu,Chunhua Peng,Han Liu,Songcheng Ren,Xuezhu Wang,Yunheng Shen,Yi Wang,Guyue Liu,Hui Chen,Tong Yang,Hailong Yang,Jing Li,Guiguang Ding,Ke Zhang*

Main category: cs.DC

TL;DR: xLLM是一个高效的大语言模型推理框架，采用解耦的服务-引擎架构，通过智能调度、分布式KV缓存管理和算法优化，在相同硬件条件下实现比MindIE和vLLM-Ascend更高的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 为了解决企业级大规模LLM服务中的性能瓶颈和资源利用效率问题，需要设计一个能够充分利用AI加速器的高性能推理框架。

Method: 构建解耦的服务-引擎架构：服务层包含智能调度模块支持多模态请求处理，采用动态Prefill-Decode解耦策略和Encode-Prefill-Decode策略；引擎层通过多层级执行管道优化、自适应图模式和xTensor内存管理来饱和计算资源。

Result: 在相同TPOT约束下，xLLM在Qwen系列模型上的吞吐量达到MindIE的1.7倍和vLLM-Ascend的2.2倍，在Deepseek系列模型上平均吞吐量为MindIE的1.7倍。

Conclusion: xLLM通过创新的架构设计和深度优化，显著提升了LLM推理的性能和资源效率，为企业级大规模服务提供了高效的解决方案。

Abstract: We introduce xLLM, an intelligent and efficient Large Language Model (LLM)
inference framework designed for high-performance, large-scale enterprise-grade
serving, with deep optimizations for diverse AI accelerators. To address these
challenges, xLLM builds a novel decoupled service-engine architecture. At the
service layer, xLLM-Service features an intelligent scheduling module that
efficiently processes multimodal requests and co-locates online and offline
tasks through unified elastic scheduling to maximize cluster utilization. This
module also relies on a workload-adaptive dynamic Prefill-Decode (PD)
disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation
policy designed for multimodal inputs. Furthermore, it incorporates a
distributed architecture to provide global KV Cache management and robust
fault-tolerant capabilities for high availability. At the engine layer,
xLLM-Engine co-optimizes system and algorithm designs to fully saturate
computing resources. This is achieved through comprehensive multi-layer
execution pipeline optimizations, an adaptive graph mode and an xTensor memory
management. xLLM-Engine also further integrates algorithmic enhancements such
as optimized speculative decoding and dynamic EPLB, collectively serving to
substantially boost throughput and inference efficiency. Extensive evaluations
demonstrate that xLLM delivers significantly superior performance and resource
efficiency. Under identical TPOT constraints, xLLM achieves throughput up to
1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while
maintaining an average throughput of 1.7x that of MindIE with Deepseek-series
models. xLLM framework is publicly available at
https://github.com/jd-opensource/xllm and
https://github.com/jd-opensource/xllm-service.

</details>


### [31] [Deadlock-free routing for Full-mesh networks without using Virtual Channels](https://arxiv.org/abs/2510.14730)
*Alejandro Cano,Cristóbal Camarero,Carmen Martínez,Ramón Beivide*

Main category: cs.DC

TL;DR: 提出TERA路由算法，在Full-mesh网络中通过嵌入物理子网络提供无死锁的非最小路径，无需虚拟通道，显著提升性能并减少缓冲区需求。


<details>
  <summary>Details</summary>
Motivation: 传统高基数低直径网络依赖虚拟通道避免死锁，但虚拟通道带来面积、功耗和设计复杂度开销，限制了交换机可扩展性。

Method: 提出TERA算法，在Full-mesh网络中嵌入物理子网络，提供无死锁的非最小路径，完全避免使用虚拟通道。

Result: 在Full-mesh网络中，TERA比链路排序路由算法在对抗性流量下性能提升80%，应用内核中提升100%；相比VC方法减少50%缓冲区需求；在2D-HyperX中比最先进算法性能提升32%。

Conclusion: TERA算法通过物理子网络嵌入实现了无虚拟通道的死锁避免，在保持性能的同时显著降低了硬件开销。

Abstract: High-radix, low-diameter networks like HyperX and Dragonfly use a Full-mesh
core, and rely on multiple virtual channels (VCs) to avoid packet deadlocks in
adaptive routing. However, VCs introduce significant overhead in the switch in
terms of area, power, and design complexity, limiting the switch scalability.
This paper starts by revisiting VC-less routing through link ordering schemes
in Full-mesh networks, which offer implementation simplicity but suffer from
performance degradation under adversarial traffic. Thus, to overcome these
challenges, we propose TERA (Topology-Embedded Routing Algorithm), a novel
routing algorithm which employs an embedded physical subnetwork to provide
deadlock-free non-minimal paths without using VCs.
  In a Full-mesh network, TERA outperforms link ordering routing algorithms by
80% when dealing with adversarial traffic, and up to 100% in application
kernels. Furthermore, compared to other VC-based approaches, it reduces buffer
requirements by 50%, while maintaining comparable latency and throughput.
Lastly, early results from a 2D-HyperX evaluation show that TERA outperforms
state-of-the-art algorithms that use the same number of VCs, achieving
performance improvements of up to 32%.

</details>


### [32] [Balls and Bins and the Infinite Process with Random Deletions](https://arxiv.org/abs/2510.14798)
*Petra Berenbrink,Tom Friedetzky,Peter Kling,Lars Nagel*

Main category: cs.DC

TL;DR: 该论文分析了具有删除操作的无限球-箱过程，证明了在任意时间点，超过平均值的球数为O(n)，差异度为O(log(n))且存在匹配下界，过载为loglog(n)+O(1)。对于良好的插入概率序列，差异度也可限制在loglog(n)+O(1)。


<details>
  <summary>Details</summary>
Motivation: 研究具有删除操作的球-箱过程中的负载均衡问题，分析最大负载与平均负载之间的差异度和过载，为动态负载分配系统提供理论保证。

Method: 采用分层归纳法（layered induction）和详细的势能分析，通过概率耦合简化设置以获得恢复性质，避免复杂的条件分析。

Result: 证明在任意时间点：超过平均值的球数为O(n)；差异度为O(log(n))且存在匹配下界；过载为loglog(n)+O(1)；对于良好插入序列，差异度可降至loglog(n)+O(1)。

Conclusion: 该研究为具有删除操作的动态负载分配系统提供了严格的理论界限，证明了即使在删除操作下，系统仍能保持良好的负载均衡性能。

Abstract: We consider an infinite balls-into-bins process with deletions where in each
discrete step $t$ a coin is tossed as to whether, with probability $\beta(t)
\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places
the ball in the lower loaded of two bins sampled uniformly at random) or, with
remaining probability $1-\beta(t)$, a ball is deleted from a non-empty bin
chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total
load at time $t$. We are interested in bounding the discrepancy $x_{\max}(t) -
m(t)/n$ (current maximum load relative to current average) and the overload
$x_{\max}(t) - m_{\max}(t)/n$ (current maximum load relative to highest average
observed so far).
  We prove that at an arbitrarily chosen time $t$ the total number of balls
above the average is $O(n)$ and that the discrepancy is $ O(\log(n))$. For the
discrepancy, we provide a matching lower bound. Furthermore we prove that at an
arbitrarily chosen time $t$ the overload is $\log\log(n)+O(1)$. For "good"
insertion probability sequences (in which the average load of time intervals
with polynomial length increases in expectation) we show that even the
discrepancy is bounded by $\log\log(n)+O(1)$.
  One of our main analytical tools is a layered induction, as per [ABKU99].
Since our model allows for rather more general scenarios than what was
previously considered, the formal analysis requires some extra ingredients as
well, in particular a detailed potential analysis. Furthermore, we simplify the
setup by applying probabilistic couplings to obtain certain "recovery"
properties, which eliminate much of the need for intricate and careful
conditioning elsewhere in the analysis.

</details>
