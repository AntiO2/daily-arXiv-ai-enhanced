<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 15]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 该论文扩展了GCS算法的形式化模型，通过将单向测量范式改为双向测量范式，在实现接近的假设下运行，移除了许多先前为证明性能而施加的限制。


<details>
  <summary>Details</summary>
Motivation: 扩展GCS算法的形式化模型，使其在实现接近的假设下运行，移除先前为证明性能而施加的限制，创建更现实的部署模型。

Method: 将单向测量范式替换为双向测量范式，放宽单位链路长度要求，提供频率源的形式化模型，对算法估计误差的不同组成部分进行细粒度区分。

Result: 将不确定性对算法估计误差的贡献从延迟量级显著降低到每个链路延迟的10%到0.1%，并给出了GCS局部和全局偏差的匹配上界。

Conclusion: 通过改变测量范式，在保持GCS核心行为的同时，显著提高了算法的实用性和性能，为实际部署提供了更现实的模型。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [2] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Gaia是一个GPU即服务模型和架构，通过动态执行模式识别和运行时评估，为异构环境中的无服务器AI工作负载提供SLO感知、成本高效的硬件加速。


<details>
  <summary>Details</summary>
Motivation: 当前平台在管理硬件加速方面存在困难，静态用户-设备分配无法在变化负载或放置下确保SLO合规，一次性动态选择往往导致次优或成本低效的配置。

Method: Gaia结合了轻量级执行模式标识器（在部署时检查函数代码并发出四种执行模式之一）和动态函数运行时（持续重新评估用户定义的SLO以在CPU和GPU后端之间进行升级或降级）。

Result: 评估显示Gaia能够无缝选择最适合工作负载的硬件加速，将端到端延迟降低高达95%。

Conclusion: Gaia能够在异构环境中为无服务器AI实现SLO感知、成本高效的加速。

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [3] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: TT-Edge是一个软硬件协同设计框架，通过在边缘AI处理器上专门优化张量训练分解(TTD)的计算流程，实现了1.7倍加速和40.2%能耗降低，同时硬件开销最小。


<details>
  <summary>Details</summary>
Motivation: 分布式学习在资源受限边缘设备上的需求增长，需要高效的设备端模型压缩。TTD虽然提供高压缩比和低精度损失，但其重复的SVD和矩阵乘法在低功耗处理器上会产生显著的延迟和能耗开销。

Method: 将SVD分解为双对角化和对角化两个阶段，将计算密集型任务卸载到专门的TTD引擎。该引擎与现有的GEMM加速器紧密集成，减少频繁的矩阵向量传输。采用轻量级设计，重用GEMM资源并使用共享浮点单元。

Result: 在RISC-V边缘AI处理器上压缩ResNet-32模型时，相比仅使用GEMM的基线，实现1.7倍加速，总能耗降低40.2%，总功耗仅增加4%，硬件开销最小。

Conclusion: TT-Edge有效解决了边缘环境中基于TTD压缩的延迟和能耗瓶颈问题，通过软硬件协同设计实现了高效能效比的模型压缩。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [4] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: VOLT是一个轻量级编译器工具链，旨在解决开源GPU架构中SIMT功能执行和性能优化的挑战，通过分层设计支持多抽象级别的代码生成和优化。


<details>
  <summary>Details</summary>
Motivation: 开源GPU架构需要复杂的编译器框架来执行现有GPU程序并优化性能，但这一技术复杂性在开源硬件开发成本中常常被低估。

Method: 采用分层设计，在中端集中处理SIMT相关分析和优化，支持多种前端语言和开源GPU硬件，确保可扩展性以适应不断发展的GPU架构。

Result: 通过ISA扩展和主机运行时API的案例研究，展示了VOLT能够支持扩展，并实现跨前端和新兴开源GPU变体的重用。

Conclusion: VOLT提供了一个可扩展的编译器框架，能够有效支持开源GPU架构的SIMT执行和优化需求，为开源GPU生态系统的发展提供了重要工具支持。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [5] [What happens when nanochat meets DiLoCo?](https://arxiv.org/abs/2511.13761)
*Alexander Acker,Soeren Becker,Sasho Nedelkoski,Dominik Scheinert,Odej Kao,Philipp Wiesner*

Main category: cs.DC

TL;DR: 论文研究了在通信受限的分布式环境中使用DiLoCo算法进行LLM训练，发现虽然预训练收敛稳定，但异步更新会导致表示漂移，影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索在通信受限的分布式环境中LLM训练引入的模型权衡，这些权衡在现有研究中尚未充分探索。

Method: 使用nanochat项目作为基线，实现DiLoCo算法作为轻量级包装器，在多个本地步骤后进行同步，与标准数据并行(DDP)设置进行比较。

Result: DiLoCo在预训练中实现稳定收敛和竞争性损失，但在中期训练和SFT后产生更差的MMLU、GSM8K和HumanEval分数。使用DiLoCo预训练权重后切换到DDP也无法恢复性能。

Conclusion: 异步更新会导致不可逆的表示漂移，损害下游任务的对齐能力，这揭示了分布式训练中的重要权衡。

Abstract: Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.
  We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.
  DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.

</details>


### [6] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: ADP是一个完全在GPU上运行的框架，通过自动动态精度和指数跨度容量估计器，使用低精度单元模拟双精度矩阵乘法，在保持FP64精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现代GPU硬件转向低精度格式（如FP16、FP8、FP4），传统FP64流水线吞吐量较低，需要利用低精度单元来模拟双精度精度以支持高性能科学计算。

Method: 提出自动动态精度（ADP）框架，核心是指数跨度容量（ESC）估计器，结合异常处理、运行时启发式和原生FP64回退机制，以及无符号整数切片方案改进Ozaki分解。

Result: 在55位尾数设置下，相比原生FP64 GEMM，在NVIDIA Blackwell GB200上实现2.3倍加速，在RTX Pro 6000 Blackwell Server Edition上实现13.2倍加速，运行时间开销低于10%。

Conclusion: 低精度加速器可以作为高保真、高性能科学计算工作负载的实用、生产就绪基础，ADP框架证明了这一可行性。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [7] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 本文提出了语义多路复用新概念，将多任务压缩表示合并为单一语义表示，突破了传统比特级并行传输的限制，显著提升了多任务并发处理能力。


<details>
  <summary>Details</summary>
Motivation: 移动设备需要在无线边缘并行执行多个计算任务，但现有通信系统仅支持比特级并行传输，这从根本上限制了可并发处理的任务数量。

Method: 提出语义多路复用方法，将多任务相关的压缩表示合并为单一语义表示，在语义层扩展有效自由度，而不违反香农容量规则。

Result: 实验表明，在4×4信道上从2个任务增加到8个任务时，图像分类准确率下降不到4%。相比基线方法，延迟降低8倍，能耗降低25倍，通信负载降低54倍。

Conclusion: 语义多路复用能够在语义层联合处理多个任务，同时保持足够的任务准确性，显著提升了多任务处理的效率和性能。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [8] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: MPI派生数据类型(DDTs)的性能在不同MPI实现中表现不一，没有统一的性能优势，建议根据具体MPI实现和通信模式进行性能分析


<details>
  <summary>Details</summary>
Motivation: 评估MPI派生数据类型在实际应用中的性能表现，澄清关于DDTs性能的争议，提供跨实现的性能对比

Method: 使用三个2D应用（Jacobi CFD求解器、康威生命游戏、基于格点的图像重建），每个应用分别实现手动打包版本和DDT版本，在四个主流MPI实现(MPICH、Open MPI、Intel MPI、MVAPICH2)上测试强扩展和弱扩展

Result: 结果混合：DDTs在某些情况下最快（如图像重建在Intel MPI和MPICH上），但在其他情况下最慢（如Open MPI和MVAPICH2上的相同代码）。CFD求解器中手动打包版本普遍优于DDTs，而生命游戏中性能排名因MPI库而异

Conclusion: 没有单一策略在所有程序、通信语义和MPI实现中占优，DDTs的性能可移植性无法保证，建议在实际使用的MPI实现和通信模式下同时分析DDT和手动打包设计的性能

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [9] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: ParallelKittens (PK) 是一个简化的 CUDA 框架，通过八个核心原语和统一编程模板，系统性地指导开发最优的多 GPU 内核，显著提升异构工作负载在不同加速器上的性能。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模扩大和硬件计算吞吐量提升超过互连带宽改进，GPU 间通信已成为现代 AI 工作负载的主要瓶颈。现有系统通过计算-通信重叠来缓解，但往往无法在异构工作负载和新加速器上达到理论峰值性能。

Method: PK 扩展了 ThunderKittens 框架，通过八个核心原语和统一编程模板体现多 GPU 内核设计原则，基于对数据传输机制、资源调度和设计开销等影响多 GPU 性能因素的综合分析。

Result: 在 Hopper 和 Blackwell 架构上验证，仅用不到 50 行设备代码，PK 实现了数据并行和 tensor 并行工作负载最高 2.33 倍加速，序列并行工作负载 4.08 倍加速，专家并行工作负载 1.22 倍加速。

Conclusion: PK 证明了通过少量简单、可重用的原则可以系统性地指导设计最优的多 GPU 内核，显著简化了重叠多 GPU 内核的开发过程。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [10] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: FailSafe是一个容错的张量并行(TP)服务系统，通过循环KVCache放置、混合注意力和细粒度负载感知路由等技术，在GPU故障时维持高性能LLM推理，实现2倍吞吐量提升和两个数量级的恢复延迟降低。


<details>
  <summary>Details</summary>
Motivation: 传统张量并行(TP)在LLM推理中存在脆弱性：单个GPU故障会导致执行中断、触发昂贵的KVCache重计算，并造成长期的计算和内存不平衡。

Method: 采用循环KVCache放置实现均匀内存利用，混合注意力结合张量和数据并行注意力消除慢节点，细粒度负载感知路由动态平衡请求，以及主动KVCache备份和按需权重恢复。

Result: 在8xH100 DGX系统上，FailSafe相比标准容错方法实现高达2倍的吞吐量提升和两个数量级的恢复延迟降低，即使最多三个GPU故障也能维持高吞吐量和均衡利用。

Conclusion: FailSafe展示了在动态不可靠硬件条件下实现鲁棒高效LLM服务的能力，为大规模LLM部署提供了可靠的容错解决方案。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [11] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 10Cache是一个资源感知的张量缓存和迁移系统，通过智能协调GPU、CPU和NVMe层级的内存使用来加速大语言模型训练，解决了现有方法的高张量迁移延迟和设备内存利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 云端训练大语言模型面临GPU内存瓶颈，现有GPU内存卸载方法存在高张量迁移延迟和设备内存利用率低的问题，导致训练时间增加和云成本上升。

Method: 10Cache通过分析张量执行顺序构建预取策略，基于张量大小分布在固定内存中分配内存缓冲区，并重用内存缓冲区以减少分配开销。

Result: 在多样化LLM工作负载中，10Cache实现了高达2倍的训练加速，GPU缓存命中率提升高达86.6倍，CPU/GPU内存利用率分别提高2.15倍和1.33倍。

Conclusion: 10Cache是优化云端LLM训练吞吐量和资源效率的实用且可扩展的解决方案。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [12] [Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks](https://arxiv.org/abs/2511.14450)
*Mulei Ma,Minrui Xu,Zihan Chen,Yang Yang,Tony Q. S. Quek*

Main category: cs.DC

TL;DR: Hyperion是一个用于多层级网络中流水线LLM推理的两阶段框架，通过联合优化模型分区和任务调度来最小化端到端延迟。


<details>
  <summary>Details</summary>
Motivation: LLM在边缘、雾和云层部署时面临GPU内存限制、异构计算和可变带宽的挑战，需要平衡模型分区和请求调度来降低延迟。

Method: 采用分层两阶段方法：第一阶段通过二分搜索和动态规划进行离线分区；第二阶段使用自适应实时任务调度算法进行在线调度。

Result: 在Phi-3-medium模型上，相比GPipe和HEFT基线，Hyperion分别将端到端延迟降低了52.1%和31.2%，并在长序列生成中保持44.5%的延迟优势。

Conclusion: Hyperion能够有效平衡多层级网络中的计算和内存资源，显著降低LLM推理延迟，且无需模型重训练，运行时开销可忽略。

Abstract: Large Language Models (LLMs) are increasingly executed across edge, fog, and cloud tiers where limited GPU memory, heterogeneous compute, and variable inter-tier bandwidth jointly constrain deployment and motivate model partitioning and request scheduling. In this setting, achieving low end-to-end latency is governed not only by where a model is deployed (inter-tier model partitioning) but also by how incoming requests are scheduled (intra-tier task scheduling) across heterogeneous nodes. These two problems are tightly coupled, as a suboptimal scheduler can negate the benefits of a good partition, and vice versa. In this paper, we propose Hyperion, a hierarchical two-stage framework that jointly optimizes partitioning and scheduling to minimize end-to-end latency for pipelined LLM inference in multi-tier networks, balancing compute and memory across tiers while introducing negligible runtime overhead and requiring no model retraining. Motivated by the observation that partition choices evolve on slower timescales than request arrivals, Stage 1 performs offline, inter-tier partitioning via a Binary Search with Dynamic Programming (BSDP) procedure to produce balanced stage times under tier capacity and memory constraints; to adapt to time-varying load, Stage 2 performs online, intra-tier scheduling with a lightweight Adaptive Real-time Task Scheduling (ARTS) algorithm that maps each request to the best available node using real-time estimates of queue length and effective capacity. Experimental results on multi-tier inference tasks demonstrate that Hyperion significantly reduces end-to-end latency by up to 52.1\% and 31.2\%, with the Phi-3-medium model, compared to the GPipe and HEFT baselines, respectively. Furthermore, Hyperion shows superior scalability in long-sequence generation, maintaining a 44.5\% lower latency than GPipe and achieving higher GPU utilization.

</details>


### [13] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 本文分析了跨组织联邦学习中参与者故障对模型质量的影响，重点关注故障时机、数据分布和评估方法等因素。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在跨组织场景中需要可靠系统，但参与者可能因各种原因故障。目前对跨组织联邦学习中参与者故障影响的研究较少，需要深入分析。

Method: 通过广泛研究分析跨组织联邦学习中参与者故障对模型质量的影响，重点关注故障时机、数据分布和评估方法等关键因素。

Result: 研究表明，在高数据偏斜情况下评估结果过于乐观，掩盖了真实影响；故障时机对训练模型质量有显著影响。

Conclusion: 研究结果为构建鲁棒联邦学习系统的研究人员和软件架构师提供了重要见解。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [14] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: Hapax Locks是一种新颖的锁算法，具有恒定时间到达和解锁路径、FIFO准入顺序、空间效率高、在竞争情况下产生较少一致性流量等优点。


<details>
  <summary>Details</summary>
Motivation: 开发一种性能与最先进锁算法相当，但对运行时环境约束更少、更易于集成到现有系统中的锁算法。

Method: 提出Hapax Locks算法，该算法在线程间不转移指针所有权，简化了实现并减少了环境依赖。

Result: Hapax Locks在延迟和可扩展性方面与最佳锁算法性能相当，同时更易于集成到现有系统或API中。

Conclusion: Hapax Locks是一种简单高效、易于集成的锁算法，特别适合在现有系统中使用。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [15] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: Seer是一个在线上下文学习系统，通过利用共享相同提示的请求在输出长度和生成模式上的相似性，解决了同步强化学习系统中的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有的同步强化学习系统在rollout阶段面临严重的性能瓶颈，存在显著的长尾延迟和资源利用率低的问题，主要由于工作负载不平衡导致。

Method: Seer引入了三项关键技术：分割rollout实现动态负载均衡、上下文感知调度、以及自适应分组推测解码，这些机制共同减少了长尾延迟并提高了资源效率。

Result: 在生产级强化学习工作负载上的评估表明，与最先进的同步强化学习系统相比，Seer将端到端rollout吞吐量提高了74%到97%，并将长尾延迟降低了75%到93%。

Conclusion: Seer系统显著加速了强化学习训练迭代，通过利用请求间的相似性有效解决了同步强化学习系统中的性能瓶颈问题。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [16] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 该研究比较了指令提示、示例提示和组合提示在控制语言模型代码生成风格方面的效果，发现在两轮协议中，组合提示在初始压缩和扩展纪律方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的代码往往过于冗长，与人类基准存在差异。研究旨在探索不同提示机制在保持功能准确性的同时，能否维持风格约束。

Method: 采用配对两轮协议，在四种提示条件下让模型首先生成Python任务解决方案，然后在通用改进指令下修订代码，保持用户任务不变（N=160对程序）。

Result: 组合提示产生最强的初始压缩和最大的扩展纪律；指令提示显示大的初始效果和中等扩展纪律；示例提示显示适度的初始效果但无扩展纪律。

Conclusion: 初始提示有效性和扩展纪律是提示设计的两个独立方面，组合方法在两轮工作流程中提供最稳定的风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [17] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 该研究通过定性访谈探讨计算机科学学生如何在软件开发项目中策略性和道德地使用ChatGPT，揭示了学生学习模式从传统转向AI辅助的转变，以及学生对AI使用伦理问题的认知。


<details>
  <summary>Details</summary>
Motivation: ChatGPT在计算机科学教育中的使用日益增多，但现有研究多依赖调查问卷，缺乏对学生使用策略和伦理意识的深入分析。本研究旨在通过定性研究填补这一空白。

Method: 采用半结构化访谈方法，对英国一所院校的计算机科学学生进行定性调查，分析他们在软件开发项目中使用ChatGPT的策略和伦理考量。

Result: 研究发现学生的学习模式从传统的"独立思考-手动编码-迭代调试"转变为"AI辅助构思-交互式编程-协作优化"。学生倾向于将ChatGPT的贡献控制在30%左右，并评估其输出以避免过度依赖，但只有少数人会深入分析AI生成的代码。

Conclusion: 学生拒绝未经授权的使用，强调隐私泄露和技能退化等风险，并呼吁教师制定明确的使用指南。研究强调了为支持负责任和教学合理使用此类工具提供明确指导的必要性。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [18] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: LoCoBench-Agent是一个专门评估LLM智能体在真实长上下文软件工程工作流程中的框架，通过多轮交互、工具使用和错误恢复等维度系统评估智能体能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准如LoCoBench主要关注单轮代码理解评估，无法捕捉真实编码智能体所需的多轮交互、工具使用模式和自适应推理能力。

Method: 将LoCoBench的8000个场景扩展到交互式智能体环境，提供8种专用工具，在10K到1M token的上下文长度范围内评估智能体性能，使用9个跨理解和效率维度的指标。

Result: 评估发现：(1)智能体表现出显著的长上下文鲁棒性；(2)理解与效率存在负相关的权衡；(3)不同模型的对话效率差异显著，策略性工具使用模式区分高性能智能体。

Conclusion: LoCoBench-Agent作为首个面向软件工程的长上下文LLM智能体基准，为测量智能体能力、识别性能差距和推进大规模自主软件开发建立了严谨基础。

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [19] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuard通过将代码视为图结构并使用选择性图探索来找到最相关的上下文，解决了LLM修复不稳定测试时的上下文问题，在工业环境中修复了47.6%的可重现不稳定测试，其中51.8%的修复被开发者接受。


<details>
  <summary>Details</summary>
Motivation: 不稳定的测试会浪费开发者时间并减慢发布周期。现有方法如FlakyDoctor在工业环境中失败，因为存在上下文问题：提供太少上下文（缺少关键生产代码）或太多上下文（用无关信息淹没LLM）。

Method: FlakyGuard将代码视为图结构，并使用选择性图探索来仅找到最相关的上下文。

Result: 在工业存储库的真实世界不稳定测试评估中，FlakyGuard修复了47.6%的可重现不稳定测试，其中51.8%的修复被开发者接受。比最先进方法在修复成功率上至少高出22%。开发者调查确认100%认为FlakyGuard的根本原因解释有用。

Conclusion: FlakyGuard通过选择性图探索有效解决了LLM修复不稳定测试时的上下文问题，在工业环境中取得了显著效果。

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [20] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 该论文研究了如何在代码库持续演化的环境中保持代码搜索模型的新鲜度，比较了三种更新策略：完全刷新、上下文学习和增量微调，发现增量微调结合新旧代码混合训练能提供最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现代代码库持续演化导致训练模型快速过时，需要研究如何在保持对旧代码记忆的同时适应新代码变化。

Method: 将代码新鲜度建模为领域漂移问题，比较三种更新策略：(A)完全重新训练；(B)上下文学习注入最近变更；(C)增量微调结合新旧代码混合训练以防止灾难性遗忘。

Result: 在Flask、SQLAlchemy、Pandas和Poetry等项目中，增量微调结合新旧代码混合训练在混合数据集上表现最佳，上下文学习在无法训练时提供最快的新代码提升，完全重新训练在追求最大新代码准确率时仍是上限。

Conclusion: 增量微调是保持代码搜索模型新鲜度的有效方法，能够平衡新旧代码的保留和适应，而不同策略适用于不同场景需求。

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [21] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: LogPurge是一个成本感知、规则增强的日志净化框架，能够自动从受污染的日志序列中选择足够的正常子集来训练异常检测模型，解决了获取干净日志数据的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 现代日志异常检测方法需要基于干净、无异常的日志序列训练深度学习模型，但获取这样的干净日志数据需要昂贵且繁琐的人工标注，现有的自动清理方法未能充分整合日志的特定特征和实际语义。

Method: 采用两阶段过滤算法：第一阶段使用大语言模型(LLM)去除聚类异常模式并增强系统规则以改进LLM对系统日志的理解；第二阶段使用分治策略将剩余污染区域分解为更小的子问题，每个子问题通过第一阶段程序有效净化。

Result: 在两个公共数据集和一个工业数据集上的实验表明，该方法平均移除了98.74%的异常，同时保留了82.39%的正常样本。与最新的无监督日志样本选择算法相比，在公共数据集上F1分数分别提高了35.7%和84.11%，在私有数据集上F1分数提高了149.72%。

Conclusion: LogPurge框架在自动净化受污染日志数据方面表现出显著效果，能够有效支持日志异常检测模型的训练，解决了实际应用中获取干净训练数据的挑战。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [22] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 提出一个经过实证验证的Scrum敏捷框架，专门用于DO-178C合规的安全关键航空航天软件，相比传统瀑布模型显著提升了开发效率和缺陷处理速度，同时保持完全合规。


<details>
  <summary>Details</summary>
Motivation: 航空航天系统日益复杂，需要在敏捷性和严格的安全认证要求之间取得平衡，传统开发方法难以满足现代航空航天软件的开发需求。

Method: 定制化Scrum框架，包括多学科产品所有权模型、双重验收标准、独立测试和文档团队、认证联络员等关键增强功能，并通过两个可比项目进行实证评估。

Result: 相比瀑布模型，总需求工作量减少76%，缺陷检测速度提升75%，缺陷解决速度提升78%，缺陷密度降低50%以上，同时保持DO-178C设计保证等级A的完全合规。

Conclusion: 敏捷实践与监管合规可以共存，但需要严格的定制化和与认证机构的积极合作，未来可通过自动化、CI/CD等进一步优化，该框架可推广到其他安全关键行业。

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [23] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: KTester是一个集成项目特定知识和测试领域知识的LLM测试生成框架，通过静态分析提取项目结构和使用知识，采用测试领域知识引导的测试用例设计与测试方法生成分离策略，结合多视角提示，显著提升测试生成的质量和可维护性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的自动化单元测试生成方法在真实项目中往往难以生成既正确又可维护的测试用例，需要更好地整合项目特定知识和测试领域知识。

Method: KTester框架通过静态分析提取项目结构和使用知识，采用测试领域知识引导的测试用例设计与测试方法生成分离策略，结合多视角提示技术，引导LLM考虑多样化的测试启发式方法，并遵循结构化模板生成测试。

Result: 在多个开源项目上的评估显示，KTester在六个关键指标上显著优于现有方法，执行通过率提升5.69%，行覆盖率提升8.83%，同时需要更少时间和生成更少的测试用例。人工评估也确认KTester生成的测试在正确性、可读性和可维护性方面评分更高。

Conclusion: KTester证明了知识驱动框架在提升LLM测试生成质量方面的有效性，通过整合项目特定知识和测试领域知识，能够生成更正确、可读和可维护的测试用例。

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [24] [How Does Cognitive Capability and Personality Influence Problem-Solving in Coding Interview Puzzles?](https://arxiv.org/abs/2511.14367)
*Dulaji Hidellaarachchi,Sebastian Baltes,John Grundy*

Main category: cs.SE

TL;DR: 本研究探讨认知能力和人格特质如何共同影响软件问题解决能力，发现认知能力（语法推理准确性）与问题解决表现正相关，尽责性和开放性人格特质对软件问题解决有积极影响，而神经质则有轻微负面影响。


<details>
  <summary>Details</summary>
Motivation: 软件工程是深度认知活动，受个体差异影响。本研究旨在探索认知能力和人格特质如何共同影响软件问题解决能力，为教育和行业实践提供指导。

Method: 对80名参与者（40名从业者，40名学生）进行认知能力测试（Baddeley语法推理测试）、人格评估（IPIP NEO 50测试）和9个问题解决任务（6个编码问题，3个逻辑推理问题）。

Result: 从业者在语法推理准确性和任务表现上略优于学生。认知能力与问题解决表现正相关，尽责性人格特质与问题解决和推理准确性相关性最强，开放性特质也有积极影响，神经质则与准确性和表现呈负相关。

Conclusion: 尽责性和开放性人格特质与认知能力共同支持软件问题解决，而负面情绪可能影响时间压力下的精确性。研究对教育课程设计和行业招聘实践具有实际意义。

Abstract: Software engineering is a deeply cognitive activity shaped by individual differences that extend beyond technical skill. This study investigates how cognitive capability and personality traits jointly relate to software problem solving among 80 participants (40 software practitioners, 40 software engineering students). Cognitive capability was measured using Baddeleys three minute grammatical reasoning test, while personality was assessed using the IPIP NEO 50 test. Participants further completed nine interview style problem solving questions. Six questions were related to coding and three were related to logical reasoning. Descriptive and correlational analyses show that practitioners achieved slightly higher grammatical reasoning accuracy and overall task performance than students. Grammatical-reasoning accuracy correlated positively with problem solving performance, indicating that stronger cognitive capability is associated with better performance in coding and logical tasks. Personality performance links were systematic. We identified that the conscientiousness trait correlated most strongly with problem solving and with reasoning accuracy, while the openness to experience trait was positively related to both outcomes. Neuroticism showed small, negative associations with accuracy and performance. Taken together, our results suggest that conscientiousness and openness to experience characteristics complement reasoning accuracy to support software problem solving, whereas elevated negative affect may hinder precision under time pressure. Our findings suggest practical implications for education and industry such as integrating structured reasoning tasks in curricula, and considering personality cognition in recruitment and role allocation. We highlight directions for future research such as longitudinal and task diverse replications with larger samples.

</details>


### [25] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: 本文提出将运行时验证(RV)与大语言模型(LLMs)进行共生集成，RV可作为LLM驱动自主系统的安全护栏，而LLMs可扩展RV的能力，帮助处理规范捕获、预期推理和不确定性。


<details>
  <summary>Details</summary>
Motivation: 确保具有学习组件和开放环境的自主系统的安全性和可信赖性具有挑战性。形式化方法提供强保证但依赖完整模型和静态假设，而LLMs擅长模式识别但缺乏形式化保证且容易出错。

Method: 提出RV和LLMs的共生集成方法：RV作为LLM驱动自主系统的安全护栏，LLMs辅助RV进行规范捕获、支持预期推理和处理不确定性。

Result: 这种相互增强的方法不同于现有的调查和路线图，为构建可信赖的自主系统提供了新的研究方向。

Conclusion: RV和LLMs的共生集成有望实现更可靠的自主系统，需要进一步研究相关挑战和认证影响。

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [26] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究探讨了在软件工程定性研究中集成大语言模型到主题分析的方法论影响，发现LLMs可作为辅助工具但不能替代人类解释性分析，需要关注偏见、上下文丢失等风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地用于辅助软件工程定性研究，但其在解释性过程（如主题分析）中的方法论影响仍未被充分探索，需要研究其严谨性、透明度和研究者能动性等问题。

Method: 通过反思性研讨会，引导25名ISERN研究人员进行结构化讨论，使用彩色编码画布记录对LLM辅助开放式编码、主题生成和主题审查的感知机会、局限性和建议。

Result: 参与者认识到潜在的效率和可扩展性收益，但强调了与偏见、上下文丢失、可重复性以及LLMs快速演变相关的风险，同时强调了提示素养和持续人工监督的必要性。

Conclusion: 研究发现LLMs可以作为支持但不能替代解释性分析的工具，该研究有助于社区持续反思如何负责任地使用LLMs增强软件工程中的定性研究。

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [27] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: FHIRconnect是一个用于openEHR和HL7 FHIR之间双向数据交换的领域特定语言和开源转换引擎，通过三层架构实现65%的映射复用，成功映射了24个国际原型到15个FHIR配置文件。


<details>
  <summary>Details</summary>
Motivation: 解决openEHR和HL7 FHIR之间由于数据建模方法根本差异和缺乏标准化转换机制而导致的医疗互操作性挑战。

Method: 开发了FHIRconnect领域特定语言和开源转换引擎，采用三层架构，利用国际原型基础支持本地定制化，实现标准化双向数据交换。

Result: 成功映射24个国际原型到15个FHIR配置文件，覆盖七个临床领域，建立了社区驱动的映射标准化技术基础。

Conclusion: FHIRconnect减少了自定义ETL解决方案的依赖，推进了基于开放标准的医疗IT系统的语法和语义互操作性。

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [28] [From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 本文提出了一种基于智能AI代理的工作流，通过专门的LLM代理协作将Fortran内核转换为可移植的Kokkos C++程序，实现了在异构GPU架构上的性能可移植性。


<details>
  <summary>Details</summary>
Motivation: 随着HPC向异构GPU加速架构转变，许多加速器缺乏原生Fortran绑定，需要将传统Fortran代码现代化以实现可移植性。虽然Kokkos框架提供了性能可移植性，但手动移植需要大量专业知识和时间。

Method: 使用专门的LLM代理协作工作流，包括翻译、验证、编译、运行、测试、调试和优化Fortran内核为可移植的Kokkos C++程序。

Result: 该流水线成功现代化了一系列基准内核，在硬件分区上生成了性能可移植的Kokkos代码。付费OpenAI模型仅需几美元就能执行工作流，生成的优化代码超越了Fortran基线，而开源模型通常无法生成功能代码。

Conclusion: 这项工作证明了智能AI代理在Fortran到Kokkos转换中的可行性，为自主现代化传统科学应用程序提供了途径，使其能够在不同超级计算机上可移植且高效运行。

Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [29] [SQL-to-Text Generation with Weighted-AST Few-Shot Prompting](https://arxiv.org/abs/2511.13907)
*Sriom Chakrabarti,Chuangtao Ma,Arijit Khan,Sebastian Link*

Main category: cs.DB

TL;DR: 提出Weighted-AST检索提示方法，通过基于加权抽象语法树的相似度检索相关示例作为few-shot提示，确保SQL到文本生成的语义准确性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 解决现有SQL-to-Text生成方法在保持SQL查询精确语义方面的不足，特别是在存在多种正确表述时语义保真度不够的问题。

Method: 提出Weighted-AST检索提示架构，整合结构化查询表示和LLM提示，使用基于加权AST的相似度度量检索语义相关示例作为few-shot提示。

Result: 在Spider、SParC和CoSQL三个基准数据集上，执行准确率提升高达+17.24%，在精确匹配和语义保真度方面表现优异，同时保持竞争力的运行时性能。

Conclusion: Weighted-AST提示是一种可扩展且有效的方法，能够从结构化数据库查询中生成自然语言解释，确保语义准确性和流畅性。

Abstract: SQL-to-Text generation aims at translating structured SQL queries into natural language descriptions, thereby facilitating comprehension of complex database operations for non-technical users. Although large language models (LLMs) have recently demonstrated promising results, current methods often fail to maintain the exact semantics of SQL queries, particularly when there are multiple possible correct phrasings. To address this problem, our work proposes Weighted-AST retrieval with prompting, an architecture that integrates structural query representations and LLM prompting. This method retrieves semantically relevant examples as few-shot prompts using a similarity metric based on an Abstract Syntax Tree (AST) with learned feature weights. Our structure-aware prompting technique ensures that generated descriptions are both fluent and faithful to the original query logic. Numerous experiments on three benchmark datasets - Spider, SParC, and CoSQL show that our method outperforms the current baselines by up to +17.24% in execution Accuracy (EX), performs superior in Exact Match (EM) and provides more consistent semantic fidelity when evaluated by humans, all while preserving competitive runtime performance. These results demonstrate that Weighted-AST prompting is a scalable and effective method for deriving natural language explanations from structured database queries.

</details>


### [30] [Fast Verification of Strong Database Isolation (Extended Version)](https://arxiv.org/abs/2511.14067)
*Zhiheng Cai,Si Liu,Hengfeng Wei,Yuxing Chen,Anqun Pan*

Main category: cs.DB

TL;DR: VeriStrong是一个用于验证数据库强隔离保证（如可串行化和快照隔离）的高效验证器，通过新颖的超多图形式化方法处理事务依赖关系，并针对数据库工作负载特性优化SMT求解。


<details>
  <summary>Details</summary>
Motivation: 验证数据库是否遵守其声称的强隔离保证至关重要，但在黑盒设置下具有挑战性，因为只能观察系统行为且存在不确定的事务依赖关系。

Method: 提出超多图形式化方法，紧凑地捕捉数据库执行中的确定和不确定事务依赖；开发可验证可串行化和快照隔离的完备编码；针对数据库工作负载特性定制SMT求解。

Result: 在多样化基准测试中，VeriStrong不仅显著优于现有最先进验证器，还能扩展到大型通用工作负载，同时保持高精度的隔离异常检测能力。

Conclusion: VeriStrong通过新颖的形式化方法和针对性的优化，实现了对数据库强隔离保证的高效、可扩展验证。

Abstract: Strong isolation guarantees, such as serializability and snapshot isolation, are essential for maintaining data consistency and integrity in modern databases. Verifying whether a database upholds its claimed guarantees is increasingly critical, as these guarantees form a contract between the vendor and its users. However, this task is challenging, particularly in black-box settings, where only observable system behavior is available and often involves uncertain dependencies between transactions.
  In this paper, we present VeriStrong, a fast verifier for strong database isolation. At its core is a novel formalism called hyper-polygraphs, which compactly captures both certain and uncertain transactional dependencies in database executions. Leveraging this formalism, we develop sound and complete encodings for verifying both serializability and snapshot isolation. To achieve high efficiency, VeriStrong tailors SMT solving to the characteristics of database workloads, in contrast to prior general-purpose approaches. Our extensive evaluation across diverse benchmarks shows that VeriStrong not only significantly outperforms state-of-the-art verifiers on the workloads they support, but also scales to large, general workloads beyond their reach, while maintaining high accuracy in detecting isolation anomalies.

</details>


### [31] [Chipmink: Efficient Delta Identification for Massive Object Graph](https://arxiv.org/abs/2511.14162)
*Supawit Chockchowwat,Sumay Thakurdesai,Zhaoheng Li,Matthew Krafczyk,Yongjoo Park*

Main category: cs.DB

TL;DR: Chipmink是一个基于图的对象存储系统，通过动态分区对象到pod中来识别脏对象，实现高效的部分持久化，相比现有方法显著减少存储空间和提升持久化速度。


<details>
  <summary>Details</summary>
Motivation: 现代数据科学工具中的对象持久化机制（如Pickle、Dill）依赖完整快照，会冗余存储未更改对象，导致时间和存储效率低下。数据科学系统缺乏类似DBMS的集中式缓冲区管理器来跟踪脏对象。

Method: 提出基于图的对象存储Chipmink，通过动态将对象分区到适当的子组（称为pod）来最小化预期持久化成本，这些pod有效隔离脏对象，实现高效的部分持久化。

Result: Chipmink支持依赖共享内存、GPU和远程对象的库，在真实笔记本和脚本中比最佳基线方法实现高达36.5倍的存储空间减少和12.4倍的持久化速度提升。

Conclusion: Chipmink作为集中式缓冲区管理器，通过动态对象分区和脏对象隔离，为数据科学系统提供了高效的持久化解决方案。

Abstract: Ranging from batch scripts to computational notebooks, modern data science tools rely on massive and evolving object graphs that represent structured data, models, plots, and more. Persisting these objects is critical, not only to enhance system robustness against unexpected failures but also to support continuous, non-linear data exploration via versioning. Existing object persistence mechanisms (e.g., Pickle, Dill) rely on complete snapshotting, often redundantly storing unchanged objects during execution and exploration, resulting in significant inefficiency in both time and storage. Unlike DBMSs, data science systems lack centralized buffer managers that track dirty objects. Worse, object states span various locations such as memory heaps, shared memory, GPUs, and remote machines, making dirty object identification fundamentally more challenging. In this work, we propose a graph-based object store, named Chipmink, that acts like the centralized buffer manager. Unlike static pages in DBMSs, persistence units in Chipmink are dynamically induced by partitioning objects into appropriate subgroups (called pods), minimizing expected persistence costs based on object sizes and reference structure. These pods effectively isolate dirty objects, enabling efficient partial persistence. Our experiments show that Chipmink is general, supporting libraries that rely on shared memory, GPUs, and remote objects. Moreover, Chipmink achieves up to 36.5x smaller storage sizes and 12.4x faster persistence than the best baselines in real-world notebooks and scripts.

</details>


### [32] [Gradient-Based Join Ordering](https://arxiv.org/abs/2511.14482)
*Tim Schwabe,Maribel Acosta*

Main category: cs.DB

TL;DR: 本文提出了一种基于梯度的连接排序方法，通过连续松弛将查询计划表示为软邻接矩阵，结合Gumbel-Softmax参数化和可微约束，使用图神经网络作为成本模型进行梯度搜索。


<details>
  <summary>Details</summary>
Motivation: 传统连接排序方法作为离散组合搜索问题存在计算复杂度高和可扩展性有限的问题，需要更高效的解决方案。

Method: 将查询计划连续松弛为软邻接矩阵，使用Gumbel-Softmax参数化和可微约束确保计划有效性，结合图神经网络成本模型进行梯度搜索。

Result: 在两个图数据集上，该方法能找到与传统离散局部搜索方法相当甚至更低成本的计划，且运行时间随查询大小线性增长。

Conclusion: 这是迈向基于梯度的连接排序的第一步，有望在未来实现更有效和高效的查询优化器。

Abstract: Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.

</details>


### [33] [Overview and Prospects of Using Integer Surrogate Keys for Data Warehouse Performance Optimization](https://arxiv.org/abs/2511.14502)
*Sviatoslav Stumpf,Vladislav Povyshev*

Main category: cs.DB

TL;DR: 使用整数型日期时间标签替代标准DATE和TIMESTAMP类型，可减少存储30-60%，提升查询性能25-40%，吞吐量最高提升8倍


<details>
  <summary>Details</summary>
Motivation: 优化数据仓库和时间序列性能，解决标准日期时间类型存储效率低、查询性能差的问题

Method: 提出32位和64位整数格式，开发索引、聚合、压缩和批处理算法

Result: 存储需求减少30-60%，查询执行速度提升25-40%，吞吐量最高提升8倍

Conclusion: 整数型日期时间标签在金融、电信、物联网和科学研究等实际应用中表现出高效性和通用性

Abstract: The aim of this paper is to examine and demonstrate how integer-based datetime labels (integer surrogate keys for time) can optimize data-warehouse and time-series performance, proposing practical formats and algorithms and validating their efficiency on real-world workloads. It is shown that replacing standard DATE and TIMESTAMP types with 32- and 64-bit integer formats reduces storage requirements by 30-60 percent and speeds up query execution by 25-40 percent. The paper presents indexing, aggregation, compression, and batching algorithms demonstrating up to an eightfold increase in throughput. Practical examples from finance, telecommunications, IoT, and scientific research confirm the efficiency and versatility of the proposed approach.

</details>


### [34] [Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems](https://arxiv.org/abs/2511.14629)
*Anadi Shakya,Primal Pappachan,David Maier,Roberto Yus,Sharad Mehrotra,Johann-Christoph Freytag*

Main category: cs.DB

TL;DR: Sieve是一个关系型数据库中间件，通过查询重写和缓存机制优化细粒度访问控制策略执行，在200-1200条策略下提升性能2-10倍，缓存机制在动态工作负载下进一步改善性能6-22%。


<details>
  <summary>Details</summary>
Motivation: 智能技术普及和GDPR、CPRA等隐私法规发展增加了对细粒度访问控制策略管理的需求，现有方法无法扩展到数千条策略，导致查询性能下降和系统效率降低。

Method: 结合查询重写和缓存机制：使用保护表达式重写查询来分组和过滤策略，有效利用数据库索引；集成具有有效替换策略和刷新机制的缓存机制以适应动态工作负载。

Result: 在两个数据库管理系统上的实验显示，Sieve可扩展到大型数据集和策略库，保持低查询延迟和系统负载，在200-1200条策略的工作负载下将策略评估性能提高2-10倍。缓存扩展在动态工作负载下进一步将查询性能提高6-22%，特别是对于较大的缓存大小。

Conclusion: Sieve适用于智能环境中的实时访问控制，支持高效、可扩展的用户偏好和隐私策略管理。

Abstract: The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies.

</details>
