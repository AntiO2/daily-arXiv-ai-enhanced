<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Process Analytics -- Data-driven Business Process Management](https://arxiv.org/abs/2512.20703)
*Matthias Stierle,Karsten Kraume,Martin Matzner*

Main category: cs.SE

TL;DR: 本文提出"流程分析"概念，将数据驱动的流程分析从技术层面扩展到社会技术视角，强调组织与利益相关者的整合。


<details>
  <summary>Details</summary>
Motivation: 当前流程挖掘研究过于关注技术层面，忽视了人类和组织因素，导致对流程分析多面性的认识不足。需要从信息系统研究的社会技术视角出发，重新审视数据驱动的流程分析。

Method: 采用归纳与演绎相结合的方法，概念化"流程分析"术语及其多个维度，并通过大型企业实施数据驱动流程分析和自动化的真实案例研究进行对比讨论。

Result: 提出了一个结合分析过程、组织及其利益相关者的新视角，概念化了流程分析的多维度框架，并通过案例研究验证了该框架的实际应用价值。

Conclusion: 流程分析提供了一个更全面的数据驱动流程分析视角，超越了单纯的技术导向，强调了社会技术整合的重要性，为实际应用提供了理论框架。

Abstract: Data-driven analysis of business processes has a long tradition in research. However, recently the term of process mining is mostly used when referring to data-driven process analysis. As a consequence, awareness for the many facets of process analysis is decreasing. In particular, while an increasing focus is put onto technical aspects of the analysis, human and organisational concerns remain under the radar. Following the socio-technical perspective of information systems research, we propose a new perspective onto data-driven process analysis that combines the process of analysis with the organisation and its stakeholders. This paper conceptualises the term process analytics and its various dimensions by following both an inductive and deductive approach. The results are discussed by contrasting them to a real-life case study from a large company implementing data-driven process analysis and automation.

</details>


### [2] [One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents](https://arxiv.org/abs/2512.20957)
*Zhaoxi Zhang,Yitong Duan,Yanzhi Zhang,Yiming Xu,Jiyan He,Yunfang Wu*

Main category: cs.SE

TL;DR: RepoNavigator：一个配备执行感知工具的LLM智能体，通过强化学习训练，用于大型开源软件仓库中的问题定位，性能超越更大规模的基线模型。


<details>
  <summary>Details</summary>
Motivation: 在大型开源软件仓库中定位需要修改的文件和函数具有挑战性，因为规模庞大且结构复杂。现有的基于LLM的方法通常将其视为仓库级检索任务，依赖多个辅助工具，但忽视了代码执行逻辑并使模型控制复杂化。

Method: 提出RepoNavigator，一个配备单一执行感知工具（跳转到被调用符号的定义）的LLM智能体。这种统一设计反映了代码执行的实际流程，同时简化了工具操作。通过强化学习端到端训练，直接从预训练模型开始，无需任何闭源蒸馏。

Result: RL训练的RepoNavigator实现了最先进的性能：7B模型优于14B基线，14B模型超越32B竞争对手，32B模型甚至超过了Claude-3.7等闭源模型。

Conclusion: 将单一、结构基础的工具与强化学习训练相结合，为仓库级问题定位提供了高效且可扩展的解决方案。

Abstract: Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.

</details>


### [3] [Artificial or Just Artful? Do LLMs Bend the Rules in Programming?](https://arxiv.org/abs/2512.21028)
*Oussama Ben Sghaier,Kevin Delcourt,Houari Sahraoui*

Main category: cs.SE

TL;DR: 研究发现大语言模型在代码生成时会利用测试用例作为上下文信号，即使被明确禁止使用，正确率仍能显著提升，揭示了预训练目标与对齐约束之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLMs在代码生成中如何处理预训练目标（利用所有可用信号最大化成功率）与对齐约束（可能限制某些信号使用）之间的冲突，特别是在代理AI环境中，模型可能利用本用于验证的测试用例作为生成信号。

Method: 使用BigCodeBench（Hard）数据集，设计五种提示条件来操纵测试用例的可见性，并施加明确或隐式的使用限制。评估了五个LLM（四个开源和一个闭源）在正确性、代码相似性、程序大小和代码变动等方面的表现，并分析跨模型一致性以识别重复出现的适应策略。

Result: 测试可见性显著改变模型性能，某些模型的正确率几乎翻倍，而明确限制或部分暴露只能部分缓解这种效应。除了原始性能外，识别出四种重复出现的适应策略，其中测试驱动精炼最为常见。

Conclusion: 研究揭示了LLMs在暴露于与明确指令冲突的上下文信号时如何调整其行为，为理解模型如何协调预训练目标与对齐约束提供了有用见解，强调了在评估LLM代码生成能力时考虑上下文信号影响的重要性。

Abstract: Large Language Models (LLMs) are widely used for automated code generation, yet their apparent successes often mask a tension between pretraining objectives and alignment choices. While pretraining encourages models to exploit all available signals to maximize success, alignment, whether through fine-tuning or prompting, may restrict their use. This conflict is especially salient in agentic AI settings, for instance when an agent has access to unit tests that, although intended for validation, act as strong contextual signals that can be leveraged regardless of explicit prohibitions. In this paper, we investigate how LLMs adapt their code generation strategies when exposed to test cases under different prompting conditions. Using the BigCodeBench (Hard) dataset, we design five prompting conditions that manipulate test visibility and impose explicit or implicit restrictions on their use. We evaluate five LLMs (four open-source and one closed-source) across correctness, code similarity, program size, and code churn, and analyze cross-model consistency to identify recurring adaptation strategies. Our results show that test visibility dramatically alters performance, correctness nearly doubles for some models, while explicit restrictions or partial exposure only partially mitigate this effect. Beyond raw performance, we identify four recurring adaptation strategies, with test-driven refinement emerging as the most frequent. These results highlight how LLMs adapt their behavior when exposed to contextual signals that conflict with explicit instructions, providing useful insight into how models reconcile pretraining objectives with alignment constraints.

</details>


### [4] [Assessing the Software Security Comprehension of Large Language Models](https://arxiv.org/abs/2512.21238)
*Mohammed Latif Siddiq,Natalie Sekerak,Antonio Karam,Maria Leal,Arvin Islam-Gomes,Joanna C. S. Santos*

Main category: cs.SE

TL;DR: 该研究系统评估了5个主流LLM在软件安全领域的认知能力，发现它们在低阶认知任务表现良好，但在需要推理、架构评估和系统创建的高阶任务上表现显著下降。


<details>
  <summary>Details</summary>
Motivation: LLM在软件开发中应用日益广泛，但其软件安全专业知识的水平尚不明确。本研究旨在系统评估主流LLM在软件安全领域的认知理解能力，了解它们在不同认知层次上的表现差异。

Method: 使用布鲁姆分类法作为框架，评估六个认知维度：记忆、理解、应用、分析、评估和创造。方法整合了多样化数据集，包括精心设计的选择题、易受攻击代码片段(SALLM)、软件安全入门课程评估、真实案例研究(XBOW)以及安全软件工程课程的基于项目的创造任务。

Result: LLM在低阶认知任务（如回忆事实和识别已知漏洞）上表现良好，但在需要推理、架构评估和创建安全系统的高阶任务上表现显著下降。研究引入了软件安全知识边界概念，识别模型能保持可靠性能的最高认知层次，并发现了LLM在布鲁姆各层次上表现出的51种重复误解模式。

Conclusion: 虽然LLM在基础软件安全知识方面有一定能力，但在需要深度推理和创造性安全设计的高阶认知任务上存在明显局限。研究提出的知识边界和误解模式分析为理解LLM在软件安全领域的实际能力提供了系统框架，对实际应用中的风险识别有重要价值。

Abstract: Large language models (LLMs) are increasingly used in software development, but their level of software security expertise remains unclear. This work systematically evaluates the security comprehension of five leading LLMs: GPT-4o-Mini, GPT-5-Mini, Gemini-2.5-Flash, Llama-3.1, and Qwen-2.5, using Blooms Taxonomy as a framework. We assess six cognitive dimensions: remembering, understanding, applying, analyzing, evaluating, and creating. Our methodology integrates diverse datasets, including curated multiple-choice questions, vulnerable code snippets (SALLM), course assessments from an Introduction to Software Security course, real-world case studies (XBOW), and project-based creation tasks from a Secure Software Engineering course. Results show that while LLMs perform well on lower-level cognitive tasks such as recalling facts and identifying known vulnerabilities, their performance degrades significantly on higher-order tasks that require reasoning, architectural evaluation, and secure system creation. Beyond reporting aggregate accuracy, we introduce a software security knowledge boundary that identifies the highest cognitive level at which a model consistently maintains reliable performance. In addition, we identify 51 recurring misconception patterns exhibited by LLMs across Blooms levels.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [RHAPSODY: Execution of Hybrid AI-HPC Workflows at Scale](https://arxiv.org/abs/2512.20795)
*Aymen Alsaadi,Mason Hooten,Mariya Goliyad,Andre Merzky,Andrew Shao,Mikhail Titov,Tianle Wang,Yian Chen,Maria Kalantzi,Kent Lee,Andrew Park,Indira Pimpalkhare,Nick Radcliffe,Colin Wahl,Pete Mendygral,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RHAPSODY是一个多运行时中间件，支持在HPC平台上并发执行异构的AI-HPC混合工作流，通过统一抽象协调现有运行时系统，实现模拟、推理和智能体工作流的共存。


<details>
  <summary>Details</summary>
Motivation: 混合AI-HPC工作流结合了大规模模拟、训练、高吞吐量推理和紧密耦合的智能体驱动控制，对运行时系统提出了异构且往往冲突的要求。现有系统通常只能满足部分需求，限制了支持新兴AI-HPC应用的能力。

Method: 提出RHAPSODY多运行时中间件，通过任务、服务、资源和执行策略的统一抽象，协调和组合现有运行时系统（如Dragon和vLLM），而不是替换它们，使模拟代码、推理服务和智能体工作流能在单个作业分配中共存。

Result: 在多个HPC系统上的评估显示：RHAPSODY引入的运行时开销极小；能持续扩展支持日益增长的异构性；为高吞吐量推理工作流实现近线性扩展；在智能体工作流中实现数据和控制的AI-HPC任务高效耦合。

Conclusion: RHAPSODY通过协调现有运行时系统，有效解决了混合AI-HPC工作流的异构需求，为领导级HPC平台上的新兴AI-HPC应用提供了可扩展的解决方案。

Abstract: Hybrid AI-HPC workflows combine large-scale simulation, training, high-throughput inference, and tightly coupled, agent-driven control within a single execution campaign. These workflows impose heterogeneous and often conflicting requirements on runtime systems, spanning MPI executables, persistent AI services, fine-grained tasks, and low-latency AI-HPC coupling. Existing systems typically address only subsets of these requirements, limiting their ability to support emerging AI-HPC applications at scale. We present RHAPSODY, a multi-runtime middleware that enables concurrent execution of heterogeneous AI-HPC workloads through uniform abstractions for tasks, services, resources, and execution policies. Rather than replacing existing runtimes, RHAPSODY composes and coordinates them, allowing simulation codes, inference services, and agentic workflows to coexist within a single job allocation on leadership-class HPC platforms. We evaluate RHAPSODY with Dragon and vLLM on multiple HPC systems using representative heterogeneous, inference-at-scale, and tightly coupled AI-HPC workflows. Our results show that RHAPSODY introduces minimal runtime overhead, sustains increasing heterogeneity at scale, achieves near-linear scaling for high-throughput inference workloads, and data- and control-efficient coupling between AI and HPC tasks in agentic workflows.

</details>


### [6] [Stochastic well-structured transition systems](https://arxiv.org/abs/2512.20939)
*James Aspnes*

Main category: cs.DC

TL;DR: 论文扩展了良结构转移系统，引入概率调度规则，定义了一类新的随机良结构转移系统，涵盖多种分布式模型。证明了这类系统中相位时钟的实现要么停止要么过快计时，且终止计算在期望多项式时间内完成或失败。通过添加全序或等价关系，系统可计算BPP语言，否则只能计算BPL中的对称语言。


<details>
  <summary>Details</summary>
Motivation: 现有良结构转移系统缺乏概率调度机制，无法充分建模种群协议、化学反应网络等实际分布式系统中的随机行为。需要扩展理论框架以包含概率调度，并研究其计算能力。

Method: 扩展良结构转移系统，引入概率调度规则，定义随机良结构转移系统。分析相位时钟实现的时间特性，研究终止计算的期望时间。通过添加全序或等价关系代理，分析系统的计算能力。

Result: 证明相位时钟实现要么停止要么在期望多项式步骤后过快计时；终止计算在期望多项式时间内完成或失败。添加全序或等价关系的系统可计算BPP语言，未增强的系统只能计算BPL中的对称语言。

Conclusion: 随机良结构转移系统为多种分布式模型提供了统一的理论框架。系统计算能力取决于是否添加全序或等价关系：增强系统达到BPP计算能力，未增强系统限于BPL中的对称语言。

Abstract: Extending well-structured transition systems to incorporate a probabilistic scheduling rule, we define a new class of stochastic well-structured transition systems that includes population protocols, chemical reaction networks, and many common gossip models; as well as augmentations of these systems by an oracle that exposes a total order on agents as in population protocols in the comparison model or an equivalence relation as in population protocols with unordered data.
  We show that any implementation of a phase clock in these systems either stops or ticks too fast after polynomially many expected steps, and that any terminating computation in these systems finishes or fails in expected polynomial time. This latter property allows an exact characterization of the computational power of many stochastic well-structured transition systems augmented with a total order or equivalence relation on agents, showing that these compute exactly the languages in BPP, while the corresponding unaugmented systems compute just the symmetric languages in BPL.

</details>


### [7] [Diving into 3D Parallelism with Heterogeneous Spot Instance GPUs: Design and Implications](https://arxiv.org/abs/2512.20953)
*Yuxiao Wang,Yuedong Xu,Qingyang Duan,Yuxuan Liu,Lei Jiao,Yinghao Yu,Jun Wu*

Main category: cs.DC

TL;DR: AutoHet：一种在异构GPU环境中自动优化3D并行训练的系统，通过智能负载均衡和高效恢复策略，相比现有系统实现最高1.79倍训练加速和4.38倍恢复加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型快速发展和新GPU产品不断发布，分布式训练在异构GPU环境中的需求显著增加。现有系统难以有效处理异构环境中的3D并行挑战，包括对称张量并行需求、非对称流水线并行中的梯度同步效率问题，以及内存利用与计算效率之间的权衡。

Method: 提出AutoHet系统：1）支持非对称3D并行结构和细粒度工作负载分配；2）建立理论模型，将设备分组和负载均衡构建为最小化每次迭代训练时间的优化问题；3）针对抢占式实例设计高效恢复策略，优先从本地节点恢复训练状态，仅从云存储下载缺失检查点。

Result: 在三个大规模模型和三种不同GPU类型的组合上进行评估，AutoHet相比现有DNN训练系统（Megatron-LM和Whale）实现了最高1.79倍的训练吞吐量加速，相比抢占式实例基线实现了4.38倍的恢复速度加速。

Conclusion: AutoHet系统有效解决了异构GPU环境中3D并行训练的挑战，通过自动优化并行计划和智能负载均衡，显著提升了训练效率和恢复速度，为大规模模型在异构环境中的高效训练提供了实用解决方案。

Abstract: The rapid growth of large language models (LLMs) and the continuous release of new GPU products have significantly increased the demand for distributed training across heterogeneous GPU environments. In this paper, we present a comprehensive analysis of the challenges involved in implementing 3D parallelism in such environments, addressing critical issues such as the need for symmetric tensor parallelism, efficient gradient synchronization in asymmetric pipeline parallelism, and the trade-offs between memory utilization and computational efficiency. Building upon these insights, we introduce AutoHet, a novel system that automatically identifies the optimal parallelism plan for distributed training on heterogeneous GPUs. AutoHet supports asymmetric 3D parallelism structures and facilitates fine-grained workload distribution. We propose a theoretical model that frames the device grouping and load balancing as an optimization problem to minimize per-iteration training time, thus effectively balancing computing power and memory usage across GPUs with diverse capabilities. To enable elastic training upon spot instance preemption, AutoHet presents an efficient recovery strategy that prioritizes to retrieve training states from local nodes, and only downloads the missing checkpoints from the cloud storage. Our extensive evaluation, conducted on three large-scale models and utilizing combinations of three different GPU types, demonstrates that AutoHet outperforms existing DNN training systems, achieving up to a 1.79$\times$ speedup in training throughput compared with Megatron-LM and Whale, and a 4.38$\times$ speedup of recovery speed compared to a spot instance baseline.

</details>


### [8] [Deadline-Aware Online Scheduling for LLM Fine-Tuning with Spot Market Predictions](https://arxiv.org/abs/2512.20967)
*Linggao Kong,Yuedong Xu,Lei Jiao,Chuan Xu*

Main category: cs.DC

TL;DR: 提出一个基于混合竞价和按需实例的在线调度框架，通过预测竞价实例价格和可用性来优化大模型微调成本，包含预测算法、无预测算法和策略选择算法，在预测误差降低时性能提升，策略选择算法具有√T的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型规模增大，微调成本急剧上升。GPU竞价实例虽然成本较低，但其价格和可用性的波动性使得满足截止时间的调度变得困难。需要利用混合实例（竞价+按需）来平衡成本与可靠性。

Method: 1) 证明竞价市场价格和可用性的可预测性；2) 建立整数规划问题捕获混合实例在价格和可用性动态下的使用；3) 提出基于预测的在线分配算法（基于承诺水平控制）；4) 当预测不准确时，提出无预测的补充算法；5) 开发策略选择算法，从参数化算法池中学习最佳策略。

Result: 理论证明：预测算法在预测误差降低时获得更紧的性能界，策略选择算法具有O(√T)的遗憾界。实验表明：该框架能自适应选择最佳策略，适应竞价市场动态和预测质量变化，比基线方法提升效用达54.8%。

Conclusion: 通过混合竞价和按需实例，结合预测和在线学习，可以有效降低大模型微调成本。该框架能自适应市场动态，在预测准确时利用预测优势，在预测不准确时仍能保持稳健性能。

Abstract: As foundation models grow in size, fine-tuning them becomes increasingly expensive. While GPU spot instances offer a low-cost alternative to on-demand resources, their volatile prices and availability make deadline-aware scheduling particularly challenging. We tackle this difficulty by using a mix of spot and on-demand instances. Distinctively, we show the predictability of prices and availability in a spot instance market, the power of prediction in enabling cost-efficient scheduling and its sensitivity to estimation errors. An integer programming problem is formulated to capture the use of mixed instances under both the price and availability dynamics. We propose an online allocation algorithm with prediction based on the committed horizon control approach that leverages a \emph{commitment level} to enforce the partial sequence of decisions. When this prediction becomes inaccurate, we further present a complementary online algorithm without predictions. An online policy selection algorithm is developed that learns the best policy from a pool constructed by varying the parameters of both algorithms. We prove that the prediction-based algorithm achieves tighter performance bounds as prediction error decreases, while the policy selection algorithm possesses a regret bound of $\mathcal{O}(\sqrt{T})$. Experimental results demonstrate that our online framework can adaptively select the best policy under varying spot market dynamics and prediction quality, consistently outperforming baselines and improving utility by up to 54.8\%.

</details>


### [9] [Mesh-Attention: A New Communication-Efficient Distributed Attention with Improved Data Locality](https://arxiv.org/abs/2512.20968)
*Sirui Chen,Jingji Chen,Siqi Zhu,Ziheng Jiang,Yanghua Peng,Xuehai Qian*

Main category: cs.DC

TL;DR: Mesh-Attention是一种新的分布式注意力算法，通过二维计算块分配降低通信计算比，相比Ring-Attention在256个GPU上实现平均2.9倍加速和79%通信量减少。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的Ring-Attention方法在扩展LLM上下文窗口时存在可扩展性限制，主要问题是通信流量过大。需要设计更高效的分布式注意力算法来降低通信开销。

Method: 提出Mesh-Attention算法，采用新的基于矩阵的模型重新设计分布式注意力空间。将二维计算块分配给每个GPU（而非一维行或列），通过调整块形状优化通信计算比。使用贪心算法在限制条件下高效搜索调度空间，确保GPU间高效通信。

Result: 在256个GPU上实现最高3.4倍加速（平均2.9倍），通信量减少最高85.4%（平均79.0%）。理论分析显示Mesh-Attention具有更低的通信复杂度和良好的可扩展性。

Conclusion: Mesh-Attention通过二维计算块分配和优化的通信调度，显著降低了分布式注意力的通信开销，在大规模部署中表现出优越的可扩展性和性能优势。

Abstract: Distributed attention is a fundamental problem for scaling context window for Large Language Models (LLMs). The state-of-the-art method, Ring-Attention, suffers from scalability limitations due to its excessive communication traffic. This paper proposes a new distributed attention algorithm, Mesh-Attention, by rethinking the design space of distributed attention with a new matrix-based model. Our method assigns a two-dimensional tile -- rather than one-dimensional row or column -- of computation blocks to each GPU to achieve higher efficiency through lower communication-computation (CommCom) ratio. The general approach covers Ring-Attention as a special case, and allows the tuning of CommCom ratio with different tile shapes. Importantly, we propose a greedy algorithm that can efficiently search the scheduling space within the tile with restrictions that ensure efficient communication among GPUs. The theoretical analysis shows that Mesh-Attention leads to a much lower communication complexity and exhibits good scalability comparing to other current algorithms.
  Our extensive experiment results show that Mesh-Attention can achieve up to 3.4x speedup (2.9x on average) and reduce the communication volume by up to 85.4% (79.0% on average) on 256 GPUs. Our scalability results further demonstrate that Mesh-Attention sustains superior performance as the system scales, substantially reducing overhead in large-scale deployments. The results convincingly confirm the advantage of Mesh-Attention.

</details>


### [10] [ESCHER: Efficient and Scalable Hypergraph Evolution Representation with Application to Triad Counting](https://arxiv.org/abs/2512.21009)
*S. M. Shovan,Arindam Khanda,Sanjukta Bhowmick,Sajal K. Das*

Main category: cs.DC

TL;DR: ESCHER：一个GPU中心的并行数据结构，用于高效管理大规模动态超图，并设计了超图三元组计数更新框架，显著提升了计算性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂网络通常包含超越成对关系的高阶交互，这些交互需要用超图建模。分析超图特性（如三元组计数）对于揭示传统图无法捕捉的复杂群体交互模式至关重要。然而，大规模动态超图由于缺乏专门的软件包和数据结构，其分析仍未被充分探索。

Method: 提出了ESCHER（高效可扩展超图演化表示），这是一个GPU中心的并行数据结构，专门设计用于高效管理大规模超图动态。同时设计了一个超图三元组计数更新框架，该框架最小化冗余计算，并充分利用ESCHER的动态操作能力。

Result: 在大型真实世界和合成数据集上的实验表明，该方法在超边基、关联顶点基和时间三元组等多种超图三元组计数类别中均优于现有最先进方法，分别实现了高达104.5倍、473.7倍和112.5倍的加速。

Conclusion: ESCHER及其配套的超图三元组计数更新框架为大规模动态超图的高效分析提供了有效的解决方案，显著提升了计算性能，填补了该领域的技术空白。

Abstract: Higher-order interactions beyond pairwise relationships in large complex networks are often modeled as hypergraphs. Analyzing hypergraph properties such as triad counts is essential, as hypergraphs can reveal intricate group interaction patterns that conventional graphs fail to capture. In real-world scenarios, these networks are often large and dynamic, introducing significant computational challenges. Due to the absence of specialized software packages and data structures, the analysis of large dynamic hypergraphs remains largely unexplored. Motivated by this gap, we propose ESCHER, a GPU-centric parallel data structure for Efficient and Scalable Hypergraph Evolution Representation, designed to manage large scale hypergraph dynamics efficiently. We also design a hypergraph triad-count update framework that minimizes redundant computation while fully leveraging the capabilities of ESCHER for dynamic operations. We validate the efficacy of our approach across multiple categories of hypergraph triad counting, including hyperedge-based, incident-vertex-based, and temporal triads. Empirical results on both large real-world and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art methods, achieving speedups of up to 104.5x, 473.7x, and 112.5x for hyperedge-based, incident-vertex-based, and temporal triad types, respectively.

</details>
