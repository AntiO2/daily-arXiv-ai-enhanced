{"id": "2511.10063", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.10063", "abs": "https://arxiv.org/abs/2511.10063", "authors": ["Yiwen Wang", "Vivek Shah", "Marcos Antonio Vaz Salles", "Claudia Bauzer Medeiros", "Julio Cesar Dos Reis", "Yongluan Zhou"], "title": "Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management", "comment": null, "summary": "Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79fb\u52a8actor\u7684\u5206\u5e03\u5f0f\u53cd\u5e94\u5f0f\u79fb\u52a8\u5bf9\u8c61\u6570\u636e\u7ba1\u7406\u5e73\u53f0M-AODBs\u53ca\u5176\u5b9e\u73b0Dolphin\uff0c\u901a\u8fc7\u5728actor\u6a21\u578b\u4e2d\u589e\u5f3a\u53cd\u5e94\u611f\u77e5\u3001\u79fb\u52a8\u548c\u7a7a\u95f4\u67e5\u8be2\u80fd\u529b\uff0c\u4e3a\u53cd\u5e94\u5f0f\u79fb\u52a8\u5bf9\u8c61\u5e94\u7528\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u79fb\u52a8\u5bf9\u8c61\u573a\u666f\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65f6\u7a7a\u6570\u636e\u7ba1\u7406\uff0c\u800c\u53cd\u5e94\u5f0f\u884c\u4e3a\u901a\u5e38\u9700\u8981\u590d\u6742\u7684\u7ec8\u7aef\u7528\u6237\u5b9e\u73b0\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u8ba1\u7b97\u8981\u6c42\u548c\u53ef\u6269\u5c55\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u79fb\u52a8actor\u62bd\u8c61\u6982\u5ff5\uff0c\u5728actor\u6a21\u578b\u4e2d\u589e\u5f3a\u53cd\u5e94\u611f\u77e5\u3001\u79fb\u52a8\u548c\u7a7a\u95f4\u67e5\u8be2\u80fd\u529b\uff1b\u57fa\u4e8e\u6b64\u6784\u5efaM-AODBs\u5e73\u53f0\uff0c\u5e76\u5728Microsoft Orleans\u5206\u5e03\u5f0f\u865a\u62dfactor\u6846\u67b6\u4e0a\u5b9e\u73b0Dolphin\u7cfb\u7edf\u3002", "result": "\u5728\u771f\u5b9e\u53cd\u5e94\u5f0f\u79fb\u52a8\u5bf9\u8c61\u573a\u666f\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0cDolphin\u5728\u591a\u673a\u5668\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u63d0\u4f9b\u63a5\u8fd1\u5b9e\u65f6\u7684\u53cd\u5e94\u5ef6\u8fdf\u3002", "conclusion": "\u79fb\u52a8actor\u62bd\u8c61\u548cM-AODBs\u5e73\u53f0\u80fd\u591f\u6709\u6548\u51cf\u8f7b\u5f00\u53d1\u8005\u5728\u5e73\u8861\u6027\u80fd\u4e0e\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8d1f\u62c5\uff0c\u4e3a\u53cd\u5e94\u5f0f\u79fb\u52a8\u5bf9\u8c61\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.09766", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.09766", "abs": "https://arxiv.org/abs/2511.09766", "authors": ["Michael Dang'ana", "Yuqiu Zhang", "Hans-Arno Jacobsen"], "title": "Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation", "comment": "14 pages, 22 figures, 2 tables", "summary": "Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.\n  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\\%$ at p95 and $47\\%$ at p99, demonstrates a $4\\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86Ksurf\u65b9\u6cd5\u5728\u57fa\u4e8e\u4f30\u8ba1\u7684\u8d44\u6e90\u7f16\u6392\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u9ad8\u53ef\u53d8\u6027\u4e91\u73af\u5883\u4e0b\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u76ee\u6807\u51fd\u6570\u6a21\u578b\u7684\u5e94\u7528\u3002", "motivation": "\u4e91\u6570\u636e\u4e2d\u5fc3\u4e2d\u5bb9\u5668\u57fa\u7840\u8bbe\u65bd\u7684\u8d44\u6e90\u7f16\u6392\u548c\u914d\u7f6e\u53c2\u6570\u641c\u7d22\u9762\u4e34\u5de8\u5927\u914d\u7f6e\u7a7a\u95f4\u548c\u4e91\u4e0d\u786e\u5b9a\u6027\u7684\u6311\u6218\u3002\u4e91\u73af\u5883\u4e2d\u865a\u62df\u673a\u6570\u91cf\u7684\u53d8\u5316\u5f15\u5165\u4e86\u5de5\u4f5c\u8d1f\u8f7d\u548c\u8d44\u6e90\u6307\u6807\u7684\u53d8\u5f02\u6027\uff0c\u4f7f\u5f97\u7f16\u6392\u51b3\u7b56\u56e0\u975e\u7ebf\u6027\u589e\u52a0\u548c\u566a\u58f0\u800c\u53d8\u5f97\u4e0d\u51c6\u786e\u3002", "method": "\u4f7f\u7528Ksurf\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a\u76ee\u6807\u51fd\u6570\u6a21\u578b\uff0c\u7ed3\u5408Drone\u7f16\u6392\u5668\uff0c\u5728\u9ad8\u5ea6\u53ef\u53d8\u7684\u4e91\u573a\u666f\u4e2d\u8fdb\u884c\u8d44\u6e90\u4f30\u8ba1\u3002Ksurf\u662f\u4e00\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u5dee\u6700\u5c0f\u5316\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u53ef\u53d8\u6027\u4e91\u6570\u636e\u3002", "result": "Ksurf\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u4f4e\u7684\u5ef6\u8fdf\u65b9\u5dee\uff1ap95\u964d\u4f4e41%\uff0cp99\u964d\u4f4e47%\uff1bCPU\u4f7f\u7528\u7387\u964d\u4f4e4%\uff1b\u4e3b\u8282\u70b9\u5185\u5b58\u4f7f\u7528\u51cf\u5c117MB\uff1b\u5728VarBench Kubernetes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5de5\u4f5cpod\u6570\u91cf\u51cf\u5c117%\uff0c\u5e26\u6765\u6210\u672c\u8282\u7ea6\u3002", "conclusion": "Ksurf\u5728\u9ad8\u53ef\u53d8\u6027\u4e91\u73af\u5883\u4e0b\u80fd\u591f\u6709\u6548\u4f18\u5316\u8d44\u6e90\u4f30\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u65b9\u5dee\u548c\u8d44\u6e90\u4f7f\u7528\uff0c\u5b9e\u73b0\u6210\u672c\u8282\u7ea6\u3002"}}
{"id": "2511.10418", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.10418", "abs": "https://arxiv.org/abs/2511.10418", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Mark Birkin", "Man Luo"], "title": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.", "AI": {"tldr": "CityVerse\u662f\u9996\u4e2a\u7edf\u4e00\u5e73\u53f0\uff0c\u6574\u5408\u591a\u6e90\u57ce\u5e02\u6570\u636e\u3001\u57fa\u4e8e\u80fd\u529b\u7684\u4efb\u52a1\u5206\u7c7b\u548c\u52a8\u6001\u6a21\u62df\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u57ce\u5e02\u8ba1\u7b97\u4efb\u52a1\u4e2d\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u7edf\u4e00\u5e73\u53f0\u8fdb\u884c\u4e00\u81f4\u7684\u591a\u6e90\u6570\u636e\u8bbf\u95ee\uff0c\u4ee5\u53ca\u4efb\u52a1\u5b9a\u4e49\u788e\u7247\u5316\u963b\u788d\u516c\u5e73\u6bd4\u8f83\u3002", "method": "1) \u57fa\u4e8e\u5750\u6807\u7684\u6570\u636eAPI\u7edf\u4e00\u5341\u7c7b\u57ce\u5e02\u6570\u636e\uff1b2) \u4efb\u52a1API\u5c0643\u4e2a\u57ce\u5e02\u8ba1\u7b97\u4efb\u52a1\u7ec4\u7ec7\u4e3a\u56db\u7ea7\u8ba4\u77e5\u5c42\u6b21\uff1b3) \u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u524d\u7aef\u652f\u6301\u5b9e\u65f6\u6570\u636e\u68c0\u7d22\u548c\u591a\u5c42\u663e\u793a\u3002", "result": "\u901a\u8fc7\u5bf9\u4e3b\u6d41LLM\u5728\u4ee3\u8868\u6027\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u5e73\u53f0\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u652f\u6301\u53ef\u91cd\u590d\u548c\u7cfb\u7edf\u8bc4\u4f30\u7684\u80fd\u529b\u3002", "conclusion": "CityVerse\u4e3a\u63a8\u8fdbLLM\u548c\u591a\u4efb\u52a1\u65b9\u6cd5\u5728\u57ce\u5e02\u8ba1\u7b97\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u91cd\u7528\u7684\u57fa\u7840\u5e73\u53f0\u3002"}}
{"id": "2511.09776", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09776", "abs": "https://arxiv.org/abs/2511.09776", "authors": ["Ramesh Adhikari", "Costas Busch", "Pavan Poudel"], "title": "A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond", "comment": "14 pages, 3 figures, accepted for the proceedings at The 27th International Symposium on Stabilization, Safety, and Security of Distributed Systems", "summary": "Transaction scheduling is crucial to efficiently allocate shared resources in a conflict-free manner in distributed systems. We investigate the efficient scheduling of transactions in a network of fog-cloud computing model, where transactions and their associated shared objects can move within the network. The schedule may require objects to move to transaction nodes, or the transactions to move to the object nodes. Moreover, the schedule may determine intermediate nodes where both objects and transactions meet. Our goal is to minimize the total combined cost of the schedule. We focus on networks of constant doubling dimension, which appear frequently in practice. We consider a batch problem where an arbitrary set of nodes has transactions that need to be scheduled. First, we consider a single shared object required by all the transactions and present a scheduling algorithm that gives an $O(\\log n \\cdot \\log D)$ approximation of the optimal schedule, where $n$ is the number of nodes and $D$ is the diameter of the network. Later, we consider transactions accessing multiple shared objects (at most $k$ objects per transaction) and provide a scheduling algorithm that gives an $O(k \\cdot \\log n \\cdot \\log D)$ approximation. We also provide a fully distributed version of the scheduling algorithms where the nodes do not need global knowledge of transactions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u96fe-\u4e91\u8ba1\u7b97\u7f51\u7edc\u4e2d\u4e8b\u52a1\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5728\u5e38\u6570\u500d\u7ef4\u5ea6\u7684\u7f51\u7edc\u4e2d\u6700\u5c0f\u5316\u8c03\u5ea6\u603b\u6210\u672c\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5305\u62ec\u5355\u5bf9\u8c61\u548c\u591a\u5bf9\u8c61\u8bbf\u95ee\u573a\u666f\u7684\u5206\u5e03\u5f0f\u8c03\u5ea6\u65b9\u6848\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u4e8b\u52a1\u8c03\u5ea6\u5bf9\u4e8e\u9ad8\u6548\u5206\u914d\u5171\u4eab\u8d44\u6e90\u81f3\u5173\u91cd\u8981\u3002\u7279\u522b\u662f\u5728\u96fe-\u4e91\u8ba1\u7b97\u6a21\u578b\u4e2d\uff0c\u4e8b\u52a1\u548c\u5171\u4eab\u5bf9\u8c61\u53ef\u4ee5\u5728\u7f51\u7edc\u4e2d\u79fb\u52a8\uff0c\u9700\u8981\u8bbe\u8ba1\u6709\u6548\u7684\u8c03\u5ea6\u7b56\u7565\u6765\u6700\u5c0f\u5316\u603b\u6210\u672c\u3002", "method": "\u9488\u5bf9\u5e38\u6570\u500d\u7ef4\u5ea6\u7f51\u7edc\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u8c03\u5ea6\u7b97\u6cd5\uff1a\u5355\u5bf9\u8c61\u8bbf\u95ee\u573a\u666f\u7684O(log n \u00b7 log D)\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u4ee5\u53ca\u591a\u5bf9\u8c61\u8bbf\u95ee\uff08\u6700\u591ak\u4e2a\u5bf9\u8c61\uff09\u7684O(k \u00b7 log n \u00b7 log D)\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u5168\u5206\u5e03\u5f0f\u5b9e\u73b0\u3002", "result": "\u7b97\u6cd5\u5728\u5355\u5bf9\u8c61\u8bbf\u95ee\u65f6\u8fbe\u5230O(log n \u00b7 log D)\u8fd1\u4f3c\u6bd4\uff0c\u5728\u591a\u5bf9\u8c61\u8bbf\u95ee\u65f6\u8fbe\u5230O(k \u00b7 log n \u00b7 log D)\u8fd1\u4f3c\u6bd4\uff0c\u5176\u4e2dn\u4e3a\u8282\u70b9\u6570\uff0cD\u4e3a\u7f51\u7edc\u76f4\u5f84\uff0ck\u4e3a\u6bcf\u4e2a\u4e8b\u52a1\u8bbf\u95ee\u7684\u6700\u5927\u5bf9\u8c61\u6570\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8c03\u5ea6\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u5177\u6709\u826f\u597d\u7684\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u4e2d\u5e38\u89c1\u7684\u5e38\u6570\u500d\u7ef4\u5ea6\u7f51\u7edc\uff0c\u4e14\u652f\u6301\u5206\u5e03\u5f0f\u5b9e\u73b0\uff0c\u4e0d\u9700\u8981\u5168\u5c40\u4e8b\u52a1\u77e5\u8bc6\u3002"}}
{"id": "2511.09837", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.09837", "abs": "https://arxiv.org/abs/2511.09837", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Shangchao Su", "Ziqing Yin", "Zhiyan Cui", "Hongfeng Sun", "Baoguo He", "Yueqiang Chen", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Lijun Ma", "Qiang Huang", "Ting Liu", "Chong Wang", "Can Wei"], "title": "MoFa: A Unified Performance Modeling Framework for LLM Pretraining", "comment": null, "summary": "The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.", "AI": {"tldr": "MoFa\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u9884\u8bad\u7ec3\u6027\u80fd\u5efa\u6a21\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u591a\u7ef4\u4f18\u5316\u7279\u5f81\u548c\u5bb9\u9519\u673a\u5236\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5927\u89c4\u6a21LLM\u9884\u8bad\u7ec3\u6027\u80fd\u5e76\u63d0\u4f9b\u4f18\u5316\u6307\u5bfc\u3002", "motivation": "\u968f\u7740LLM\u53c2\u6570\u89c4\u6a21\u4ece\u6570\u5341\u4ebf\u589e\u957f\u5230\u6570\u4e07\u4ebf\uff0c\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u6210\u4e3a\u5fc5\u8981\uff0c\u4f46\u6df7\u5408\u5e76\u884c\u5316\u7b56\u7565\u7684\u5e9e\u5927\u7ec4\u5408\u7a7a\u95f4\u5e26\u6765\u4e86\u663e\u8457\u7684\u4f18\u5316\u6311\u6218\u3002\u4f20\u7edf\u624b\u52a8\u8c03\u4f18\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u6027\u80fd\u5efa\u6a21\u65b9\u6cd5\u65e0\u6cd5\u5168\u9762\u8003\u8651\u4f18\u5316\u7279\u5f81\u548c\u5bb9\u9519\u673a\u5236\u7684\u5f00\u9500\u3002", "method": "\u63d0\u51faMoFa\u6846\u67b6\uff0c\u5305\u542b\u589e\u5f3a\u7684\u6210\u672c\u6a21\u578b\u6765\u51c6\u786e\u6355\u6349\u5173\u952e\u4f18\u5316\u6548\u679c\uff0c\u5e76\u57fa\u4e8e\u5386\u53f2\u96c6\u7fa4\u53ef\u9760\u6027\u6570\u636e\u96c6\u6210\u5bb9\u9519\u6a21\u578b\u3002\u5f00\u53d1\u4e86\u57fa\u4e8eMoFa\u7684\u8c03\u4f18\u7cfb\u7edf\u6765\u63a2\u7d22\u6700\u4f18\u9884\u8bad\u7ec3\u6027\u80fd\u548c\u6f5c\u5728\u74f6\u9888\u3002", "result": "\u5e7f\u6cdb\u7684\u5efa\u6a21\u8bc4\u4f30\u8868\u660eMoFa\u5728\u5404\u79cd\u573a\u666f\u4e0b\u90fd\u80fd\u5b9e\u73b0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u8c03\u4f18\u5b9e\u9a8c\u7cfb\u7edf\u63ed\u793a\u4e86\u4e0d\u540c\u914d\u7f6e\u4e0b\u5f71\u54cd\u9884\u8bad\u7ec3\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "MoFa\u4e3aLLM\u9884\u8bad\u7ec3\u7cfb\u7edf\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5148\u9a8c\u6307\u5bfc\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2511.09861", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.09861", "abs": "https://arxiv.org/abs/2511.09861", "authors": ["Marco Kurzynski", "Shaizeen Aga", "Di Wu"], "title": "Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs", "comment": null, "summary": "GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.", "AI": {"tldr": "GPU\u7cfb\u7edf\u5728\u6570\u636e\u4e2d\u5fc3\u5b58\u5728\u6027\u80fd\u53d8\u5f02\u95ee\u9898\uff0cLit Silicon\u6548\u5e94\u63ed\u793a\u4e86\u70ed\u4e0d\u5e73\u8861\u5bfc\u81f4GPU\u8282\u70b9\u7ea7\u6027\u80fd\u4e0b\u964d\uff0c\u901a\u8fc7\u68c0\u6d4b\u548c\u7f13\u89e3\u6280\u672f\u53ef\u5b9e\u73b06%\u6027\u80fd\u63d0\u5347\u548c4%\u529f\u8017\u4f18\u5316\u3002", "motivation": "GPU\u7cfb\u7edf\u5728\u73b0\u4ee3\u6570\u636e\u4e2d\u5fc3\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b58\u5728\u8282\u70b9\u548c\u96c6\u7fa4\u7ea7\u522b\u7684\u6027\u80fd\u53d8\u5f02\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u9ad8\u6027\u80fd\u8ba1\u7b97\u548cAI\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff09\u7684\u6548\u7387\u3002", "method": "\u5206\u6790\u4e86\u591aGPU\u8282\u70b9\u8fd0\u884cLLM\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u70ed\u8bf1\u5bfc\u7684\u62d6\u5c3e\u6548\u5e94\u4e0e\u5e76\u53d1\u8ba1\u7b97\u901a\u4fe1\u6280\u672f\u76f8\u5173\uff0c\u63d0\u51fa\u4e86Lit Silicon\u6548\u5e94\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u68c0\u6d4b\u548c\u7f13\u89e3\u6280\u672f\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u7535\u6e90\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5728\u4e24\u4e2aAMD Instinct MI300X GPU\u7cfb\u7edf\u548c\u4e24\u4e2aLLM\u8bad\u7ec3\u6846\u67b6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u53ef\u5b9e\u73b0\u9ad8\u8fbe6%\u7684\u6027\u80fd\u63d0\u5347\u548c4%\u7684\u529f\u8017\u4f18\u5316\uff0c\u4e3a\u6570\u636e\u4e2d\u5fc3\u8282\u7701\u6570\u4ebf\u7f8e\u5143\u6210\u672c\u3002", "conclusion": "Lit Silicon\u6548\u5e94\u662fGPU\u7cfb\u7edf\u6027\u80fd\u53d8\u5f02\u7684\u91cd\u8981\u6839\u6e90\uff0c\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u51e0\u4e4e\u96f6\u6210\u672c\uff0c\u53ef\u4f5c\u4e3a\u65b0\u7684\u8282\u70b9\u7ea7\u7535\u6e90\u7ba1\u7406\u5c42\u8f7b\u677e\u90e8\u7f72\u5230\u6570\u636e\u4e2d\u5fc3\u4e2d\u3002"}}
{"id": "2511.09794", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.09794", "abs": "https://arxiv.org/abs/2511.09794", "authors": ["Wasique Islam Shafin", "Md Nakhla Rafi", "Zhenhao Li", "Tse-Hsun Chen"], "title": "Evaluating Software Process Models for Multi-Agent Class-Level Code Generation", "comment": null, "summary": "Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\\% for GPT-4o-mini and -39.8\\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53LLM\u5de5\u4f5c\u6d41\u901a\u8fc7\u7011\u5e03\u5f0f\u5f00\u53d1\u6d41\u7a0b\uff08\u9700\u6c42\u3001\u8bbe\u8ba1\u3001\u5b9e\u73b0\u3001\u6d4b\u8bd5\uff09\u91cd\u7ec4\u800c\u975e\u589e\u5f3a\u6a21\u578b\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u53ef\u7ef4\u62a4\u6027\u548c\u529f\u80fd\u6b63\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u667a\u80fd\u4f53\u51fd\u6570\u7ea7\u4ee3\u7801\u751f\u6210\uff0c\u7f3a\u4e4f\u5bf9\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u6d41\u7a0b\u7ed3\u6784\u548c\u89d2\u8272\u4e13\u4e1a\u5316\u5982\u4f55\u5f71\u54cd\u7c7b\u7ea7\u4ee3\u7801\u751f\u6210\u7684\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u4e09\u4e2aLLM\uff08GPT-4o-mini\u3001DeepSeek-Chat\u3001Claude-3.5-Haiku\uff09\u5728ClassEval\u57fa\u51c6\u7684100\u4e2aPython\u4efb\u52a1\u4e0a\u6a21\u62df\u7011\u5e03\u5f0f\u5f00\u53d1\u5468\u671f\u3002", "result": "\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4ea7\u751f\u66f4\u6e05\u6670\u53ef\u7ef4\u62a4\u7684\u4ee3\u7801\uff0c\u4f46\u901a\u5e38\u964d\u4f4e\u529f\u80fd\u6b63\u786e\u6027\uff08GPT-4o-mini -37.8%\uff0cDeepSeek-Chat -39.8%\uff09\uff0cClaude-3.5-Haiku\u4f8b\u5916\uff08+9.5%\uff09\u3002\u6d41\u7a0b\u7ea6\u675f\u6539\u53d8\u5931\u8d25\u7279\u5f81\uff1a\u7ed3\u6784\u95ee\u9898\u51cf\u5c11\uff0c\u8bed\u4e49\u548c\u9a8c\u8bc1\u9519\u8bef\u589e\u52a0\u3002\u6d4b\u8bd5\u9636\u6bb5\u5f71\u54cd\u6700\u5927\u3002", "conclusion": "\u8f6f\u4ef6\u6d41\u7a0b\u7ed3\u6784\u4ece\u6839\u672c\u4e0a\u6539\u53d8LLM\u7684\u63a8\u7406\u3001\u534f\u4f5c\u548c\u5931\u8d25\u65b9\u5f0f\uff0c\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53\u4ee3\u7801\u751f\u6210\u4e2d\u4e25\u683c\u5de5\u4f5c\u6d41\u7eaa\u5f8b\u4e0e\u7075\u6d3b\u95ee\u9898\u89e3\u51b3\u4e4b\u95f4\u7684\u5185\u5728\u6743\u8861\u3002"}}
{"id": "2511.09956", "categories": ["cs.DC", "cs.OS", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.09956", "abs": "https://arxiv.org/abs/2511.09956", "authors": ["Mani Tofigh", "Edward Guo", "Weiwei Jia", "Xiaoning Ding", "Jianchen Shan"], "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction", "comment": null, "summary": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.", "AI": {"tldr": "CacheX\u901a\u8fc7\u5728\u865a\u62df\u673a\u5185\u4f7f\u7528\u9a71\u9010\u96c6\u63a2\u6d4b\u7f13\u5b58\u62bd\u8c61\uff0c\u65e0\u9700\u786c\u4ef6\u6216\u7ba1\u7406\u7a0b\u5e8f\u652f\u6301\uff0c\u5b9e\u73b0\u4e86LLC\u7ade\u4e89\u611f\u77e5\u4efb\u52a1\u8c03\u5ea6\u548c\u865a\u62df\u989c\u8272\u611f\u77e5\u9875\u9762\u7f13\u5b58\u7ba1\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u516c\u5171\u4e91\u865a\u62df\u673a\u4e2d\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7f13\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u5728\u516c\u5171\u4e91\u865a\u62df\u673a\u4e2d\uff0c\u57fa\u4e8e\u7f13\u5b58\u7684\u4f18\u5316\u5f80\u5f80\u65e0\u6548\uff0c\u56e0\u4e3a\u865a\u62df\u673a\u65e0\u6cd5\u4e86\u89e3\u548c\u63a7\u5236\u5206\u914d\u7684\u7f13\u5b58\u3002CPU\u7f13\u5b58\u53ef\u80fd\u5728\u865a\u62df\u673a\u4e4b\u95f4\u5206\u533a\u6216\u5171\u4eab\uff0c\u4f46\u865a\u62df\u673a\u4e0d\u77e5\u9053\u7f13\u5b58\u5206\u914d\u7ec6\u8282\uff0c\u4e5f\u65e0\u6cd5\u901a\u8fc7\u9875\u9762\u653e\u7f6e\u7b56\u7565\u5f71\u54cd\u7f13\u5b58\u4f7f\u7528\u3002", "method": "\u63d0\u51faCacheX\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u7528\u9a71\u9010\u96c6\u5728\u865a\u62df\u673a\u5185\u63a2\u6d4b\u51c6\u786e\u7ec6\u7c92\u5ea6\u7684\u7f13\u5b58\u62bd\u8c61\uff0c\u65e0\u9700\u786c\u4ef6\u6216\u7ba1\u7406\u7a0b\u5e8f\u652f\u6301\u3002\u5c55\u793a\u4e86\u4e24\u79cd\u65b0\u6280\u672f\uff1aLLC\u7ade\u4e89\u611f\u77e5\u4efb\u52a1\u8c03\u5ea6\u548c\u865a\u62df\u989c\u8272\u611f\u77e5\u9875\u9762\u7f13\u5b58\u7ba1\u7406\u3002", "result": "\u5728x86 Linux\u5185\u6838\u4e2d\u5b9e\u73b0\u7684CacheX\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u80fd\u6709\u6548\u63d0\u9ad8\u516c\u5171\u4e91\u865a\u62df\u673a\u4e2d\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7f13\u5b58\u5229\u7528\u7387\u3002", "conclusion": "CacheX\u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u4e91\u865a\u62df\u673a\u4e2d\u6709\u6548\u63a2\u6d4b\u548c\u7ba1\u7406\u7f13\u5b58\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7f13\u5b58\u4f18\u5316\u5728\u4e91\u73af\u5883\u4e2d\u5931\u6548\u7684\u95ee\u9898\u3002"}}
{"id": "2511.09964", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.09964", "abs": "https://arxiv.org/abs/2511.09964", "authors": ["Noah van der Vleuten", "Anthony Flores", "Shray Mathur", "Max Rakitin", "Thomas Hopkins", "Kevin G. Yager", "Esther H. R. Tsai"], "title": "EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines", "comment": null, "summary": "Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.", "AI": {"tldr": "EnvTrace\u662f\u4e00\u79cd\u57fa\u4e8e\u4eff\u771f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u6267\u884c\u8f68\u8ff9\u6765\u8bc4\u4f30\u8bed\u4e49\u4ee3\u7801\u7b49\u4ef7\u6027\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4eea\u5668\u63a7\u5236\u65b9\u9762\u7684\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5149\u675f\u7ebf\u63a7\u5236\u903b\u8f91\u6570\u5b57\u5b6a\u751f\u8fdb\u884c\u6f14\u793a\uff0c\u8bc4\u4f30\u4e8630\u591a\u4e2aLLM\uff0c\u663e\u793a\u8bb8\u591a\u9876\u7ea7\u6a21\u578b\u5728\u5feb\u901f\u63a7\u5236\u4ee3\u7801\u751f\u6210\u65b9\u9762\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4eea\u5668\u63a7\u5236\u65b9\u9762\u7684\u80fd\u529b\u9700\u8981\u8d85\u8d8a\u6807\u51c6\u9759\u6001\u7b97\u6cd5\u57fa\u51c6\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u7269\u7406\u7cfb\u7edf\u7684\u884c\u4e3a\u65e0\u6cd5\u4ec5\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\u5b8c\u5168\u6355\u83b7\u3002", "method": "\u5f15\u5165EnvTrace\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u4eff\u771f\u7684\u6267\u884c\u8f68\u8ff9\u8bc4\u4f30\u6765\u8bc4\u4f30\u8bed\u4e49\u4ee3\u7801\u7b49\u4ef7\u6027\uff0c\u901a\u8fc7\u5149\u675f\u7ebf\u63a7\u5236\u903b\u8f91\u6570\u5b57\u5b6a\u751f\u8fdb\u884c\u6f14\u793a\uff0c\u5e76\u4f7f\u7528\u8f68\u8ff9\u5bf9\u9f50\u4e3a30\u591a\u4e2aLLM\u751f\u6210\u591a\u65b9\u9762\u7684\u529f\u80fd\u6b63\u786e\u6027\u8bc4\u5206\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8bb8\u591a\u9876\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5feb\u901f\u63a7\u5236\u4ee3\u7801\u751f\u6210\u65b9\u9762\u80fd\u591f\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u6027\u80fd\u3002", "conclusion": "\u8fd9\u662f\u5b9e\u73b0LLM\u548c\u6570\u5b57\u5b6a\u751f\u5171\u751f\u534f\u4f5c\u613f\u666f\u7684\u7b2c\u4e00\u6b65\uff1aLLM\u63d0\u4f9b\u76f4\u89c2\u63a7\u5236\u548c\u667a\u80fd\u7f16\u6392\uff0c\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u5b89\u5168\u9ad8\u4fdd\u771f\u73af\u5883\uff0c\u4e3a\u81ea\u4e3b\u5177\u8eabAI\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2511.10146", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2511.10146", "abs": "https://arxiv.org/abs/2511.10146", "authors": ["Jaime Sebastian Burbano", "Arnova Abdullah", "Eldiyar Zhantileuov", "Mohan Liyanage", "Rolf Schuster"], "title": "Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach", "comment": null, "summary": "Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8fb9\u7f18\u670d\u52a1\u5668\u9009\u62e9\u65b9\u6cd5MO-HAN\uff0c\u901a\u8fc7\u878d\u5408\u5ef6\u8fdf\u9884\u6d4b\u3001\u81ea\u9002\u5e94\u53ef\u9760\u6027\u548c\u57fa\u4e8e\u6ede\u540e\u7684\u5207\u6362\u673a\u5236\uff0c\u5728\u964d\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u51cf\u5c1150%\u7684\u5207\u6362\u6b21\u6570\u3002", "motivation": "\u591a\u670d\u52a1\u5668\u67b6\u6784\u4e2d\u7684\u52a8\u6001\u7f51\u7edc\u62e5\u585e\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u5d4c\u5165\u5f0f\u5e94\u7528\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u670d\u52a1\u5668\u9009\u62e9\u65b9\u6cd5\u6765\u786e\u4fdd\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u88ab\u52a8\u6d4b\u91cf\uff08\u5230\u8fbe\u7387\u3001\u5229\u7528\u7387\u3001\u8d1f\u8f7d\u5927\u5c0f\uff09\u548c\u6307\u6570\u8c03\u5236\u6709\u7406\u5ef6\u8fdf\u6a21\u578b\uff0c\u8ba1\u7b97\u5e73\u8861\u9884\u6d4b\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u7684\u5f97\u5206\uff0c\u4ec5\u5728\u9884\u671f\u589e\u76ca\u6709\u610f\u4e49\u65f6\u8fdb\u884c\u5207\u6362\u3002", "result": "MO-HAN\u5728\u964d\u4f4e\u5e73\u5747\u548c\u5c3e\u90e8\u5ef6\u8fdf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u9759\u6001\u548c\u516c\u5e73\u5206\u5e03\u57fa\u7ebf\uff0c\u540c\u65f6\u76f8\u6bd4\u7eaf\u673a\u4f1a\u9009\u62e9\u51cf\u5c11\u4e86\u8fd150%\u7684\u5207\u6362\u6b21\u6570\u3002", "conclusion": "MO-HAN\u65e0\u9700\u4fb5\u5165\u5f0f\u68c0\u6d4b\u6216\u91cd\u578b\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\uff0c\u5b9e\u73b0\u4e86\u5ef6\u8fdf\u964d\u4f4e\u548c\u5207\u6362\u4f18\u5316\u7684\u53cc\u91cd\u76ee\u6807\u3002"}}
{"id": "2511.10049", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10049", "abs": "https://arxiv.org/abs/2511.10049", "authors": ["Divyanshu Saxena", "Rishikesh Maurya", "Xiaoxuan Ou", "Gagan Somashekar", "Shachee Mishra Gupta", "Arun Iyer", "Yu Kang", "Chetan Bansal", "Aditya Akella", "Saravan Rajmohan"], "title": "Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents", "comment": "5 pages", "summary": "The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u6301\u7eed\u6f14\u8fdb\u7684\u4f01\u4e1a\u7ea7AI\u4ee3\u7406\u7684\u57fa\u51c6\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u534a\u7ed3\u6784\u5316\u6587\u6863\u548cLLM\u751f\u6210\u57fa\u51c6\uff0c\u5b9e\u73b0\u53ef\u7ef4\u62a4\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u56fa\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6ee1\u8db3\u4f01\u4e1a\u7ea7AI\u4ee3\u7406\u7684\u8bc4\u4f30\u9700\u6c42\uff0c\u56e0\u4e3a\u4f01\u4e1a\u670d\u52a1\u548c\u8981\u6c42\u6301\u7eed\u6f14\u8fdb\uff0c\u4e14\u771f\u5b9e\u6848\u4f8b\u7a00\u758f\u3002", "method": "\u4f7f\u7528\u534a\u7ed3\u6784\u5316\u6587\u6863\u8868\u8fbe\u9ad8\u5c42\u610f\u56fe\uff0c\u5229\u7528\u6700\u5148\u8fdb\u7684LLM\u4ece\u5c11\u91cf\u6587\u6863\u751f\u6210\u57fa\u51c6\uff0c\u652f\u6301\u57fa\u51c6\u968f\u9700\u6c42\u53d8\u5316\u800c\u6f14\u8fdb\u3002", "result": "\u5728\u5927\u578b\u516c\u5171\u4f01\u4e1a\u7684\u670d\u52a1\u8fc1\u79fb\u6848\u4f8b\u7814\u7a76\u4e2d\u6210\u529f\u5e94\u7528\uff0c\u5b9e\u73b0\u4e86\u53ef\u7ef4\u62a4\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u80fd\u591f\u5feb\u901f\u53cd\u9988\u4ee3\u7406\u6027\u80fd\u5e76\u4fc3\u8fdb\u9488\u5bf9\u6027\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u6301\u7eed\u6f14\u8fdb\u7684\u4f01\u4e1a\u7ea7AI\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.10180", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.10180", "abs": "https://arxiv.org/abs/2511.10180", "authors": ["Tao Tang", "Youfu Jiang", "Yingbo Cui", "Jianbin Fang", "Peng Zhang", "Lin Peng", "Chun Huang"], "title": "Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms", "comment": "14pages", "summary": "Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u7a00\u758f\u77e9\u9635\u91cd\u6392\u5e8f\u7b97\u6cd5\u9009\u62e9\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u77e9\u9635\u7279\u5f81\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u91cd\u6392\u5e8f\u7b97\u6cd5\uff0c\u76f8\u6bd4\u5355\u4e00AMD\u7b97\u6cd5\u51cf\u5c1155.37%\u6c42\u89e3\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u77e9\u9635\u6392\u5e8f\u7b97\u6cd5\u9009\u62e9\u4f9d\u8d56\u66b4\u529b\u641c\u7d22\u6216\u7ecf\u9a8c\u77e5\u8bc6\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7a00\u758f\u77e9\u9635\u7ed3\u6784\uff0c\u9700\u8981\u667a\u80fd\u5316\u7684\u81ea\u52a8\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u76d1\u7763\u5b66\u4e60\u6a21\u578b\uff0c\u5b66\u4e60\u77e9\u9635\u7279\u5f81\u4e0e\u5e38\u7528\u91cd\u6392\u5e8f\u7b97\u6cd5\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u5b9e\u73b0\u7a00\u758f\u77e9\u9635\u91cd\u6392\u5e8f\u7b97\u6cd5\u7684\u81ea\u52a8\u667a\u80fd\u9009\u62e9\u3002", "result": "\u5728Florida\u7a00\u758f\u77e9\u9635\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u5404\u79cd\u77e9\u9635\u7684\u6700\u4f18\u91cd\u6392\u5e8f\u7b97\u6cd5\uff0c\u76f8\u6bd4\u5355\u4e00AMD\u7b97\u6cd5\u51cf\u5c1155.37%\u6c42\u89e3\u65f6\u95f4\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u4e3a1.45\u3002", "conclusion": "\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u7a00\u758f\u77e9\u9635\u91cd\u6392\u5e8f\u7b97\u6cd5\u9009\u62e9\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u6c42\u89e3\u6548\u7387\uff0c\u5b9e\u73b0\u7b97\u6cd5\u9009\u62e9\u7684\u81ea\u52a8\u5316\u548c\u667a\u80fd\u5316\u3002"}}
{"id": "2511.10271", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10271", "abs": "https://arxiv.org/abs/2511.10271", "authors": ["Xin Sun", "Daniel St\u00e5hl", "Kristian Sandahl", "Christoph Kessler"], "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics", "comment": null, "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u751f\u6210\u4ee3\u7801\u7684\u975e\u529f\u80fd\u6027\u8d28\u91cf\uff0c\u53d1\u73b0\u5b66\u672f\u5173\u6ce8\u70b9\u4e0e\u884c\u4e1a\u4f18\u5148\u7ea7\u5b58\u5728\u9519\u914d\uff0c\u4e14LLM\u5728\u4e0d\u540c\u8d28\u91cf\u7ef4\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u751f\u6210\u4ee3\u7801\u7684\u529f\u80fd\u6b63\u786e\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u975e\u529f\u80fd\u6027\u8d28\u91cf\uff08\u5982\u5b89\u5168\u6027\u3001\u53ef\u7ef4\u62a4\u6027\u3001\u6027\u80fd\u6548\u7387\uff09\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u548c\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u4e92\u8865\u7814\u7a76\u65b9\u6cd5\uff1a\u5bf9108\u7bc7\u8bba\u6587\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\u3001\u4e0e\u884c\u4e1a\u4ece\u4e1a\u8005\u7684\u5de5\u4f5c\u574a\u3001\u4f7f\u7528\u4e09\u4e2aLLM\u4fee\u590d\u771f\u5b9e\u8f6f\u4ef6\u95ee\u9898\u7684\u5b9e\u8bc1\u5206\u6790\u3002", "result": "\u5b66\u672f\u754c\u4e3b\u8981\u5173\u6ce8\u5b89\u5168\u6027\u548c\u6027\u80fd\u6548\u7387\uff0c\u800c\u884c\u4e1a\u66f4\u91cd\u89c6\u53ef\u7ef4\u62a4\u6027\u548c\u53ef\u8bfb\u6027\uff1bLLM\u751f\u6210\u7684\u6b63\u786e\u8865\u4e01\u5728\u4e0d\u540c\u8d28\u91cf\u7ef4\u5ea6\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4f18\u5316\u4e00\u4e2a\u7ef4\u5ea6\u5f80\u5f80\u727a\u7272\u5176\u4ed6\u7ef4\u5ea6\u3002", "conclusion": "\u9700\u8981\u5728LLM\u4ee3\u7801\u751f\u6210\u6d41\u7a0b\u4e2d\u96c6\u6210\u8d28\u91cf\u4fdd\u8bc1\u673a\u5236\uff0c\u786e\u4fdd\u751f\u6210\u7684\u4ee3\u7801\u4e0d\u4ec5\u901a\u8fc7\u6d4b\u8bd5\uff0c\u800c\u4e14\u771f\u6b63\u5177\u5907\u8d28\u91cf\u3002"}}
{"id": "2511.10258", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10258", "abs": "https://arxiv.org/abs/2511.10258", "authors": ["Leszek Sliwko", "Vladimir Getov"], "title": "Workload Schedulers -- Genesis, Algorithms and Differences", "comment": null, "summary": "This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u73b0\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\uff0c\u63cf\u8ff0\u4e86\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u7a0b\u8c03\u5ea6\u5668\u3001\u96c6\u7fa4\u7cfb\u7edf\u4f5c\u4e1a\u8c03\u5ea6\u5668\u548c\u5927\u6570\u636e\u8c03\u5ea6\u5668\u4e09\u7c7b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u4ece\u65e9\u671f\u91c7\u7528\u5230\u73b0\u4ee3\u5b9e\u73b0\u7684\u6f14\u53d8\u8fc7\u7a0b\u3002", "motivation": "\u4e3a\u73b0\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u63d0\u4f9b\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u7406\u89e3\u4e0d\u540c\u7c7b\u578b\u8c03\u5ea6\u5668\u7684\u6f14\u53d8\u5386\u7a0b\u548c\u7b97\u6cd5\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u63cf\u8ff0\u4e09\u7c7b\u8c03\u5ea6\u5668\uff08\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u7a0b\u8c03\u5ea6\u5668\u3001\u96c6\u7fa4\u7cfb\u7edf\u4f5c\u4e1a\u8c03\u5ea6\u5668\u3001\u5927\u6570\u636e\u8c03\u5ea6\u5668\uff09\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u5206\u6790\u5b83\u4eec\u7684\u4f7f\u7528\u60c5\u51b5\u548c\u7b97\u6cd5\u7279\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u73b0\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u5668\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u8c03\u5ea6\u5668\u4e4b\u95f4\u7684\u5dee\u5f02\u53ca\u5176\u6309\u65f6\u95f4\u987a\u5e8f\u7684\u53d1\u5c55\u8f68\u8ff9\u3002", "conclusion": "\u672c\u5730\u7cfb\u7edf\u548c\u5206\u5e03\u5f0f\u7cfb\u7edf\u5728\u8c03\u5ea6\u7b56\u7565\u8bbe\u8ba1\u4e0a\u5b58\u5728\u76f8\u4f3c\u7684\u5173\u6ce8\u70b9\uff0c\u8c03\u5ea6\u5668\u7684\u53d1\u5c55\u5448\u73b0\u51fa\u4ece\u7b80\u5355\u5230\u590d\u6742\u3001\u4ece\u672c\u5730\u5230\u5206\u5e03\u5f0f\u7684\u6f14\u8fdb\u8d8b\u52bf\u3002"}}
{"id": "2511.10323", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10323", "abs": "https://arxiv.org/abs/2511.10323", "authors": ["D\u00e1vid K\u00f3sz\u00f3", "Tam\u00e1s Aladics", "Rudolf Ferenc", "P\u00e9ter Heged\u0171s"], "title": "A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports", "comment": "Under publication to Nature Scientific Data journal", "summary": "Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6536\u96c6\u548c\u5206\u7c7b\u9759\u6001\u4ee3\u7801\u5206\u6790\u8b66\u544a\u7684\u65b0\u65b9\u6cd5\uff0c\u521b\u5efa\u4e86\u5305\u542b\u8d85\u8fc7100\u4e07\u6761Java\u6e90\u4ee3\u7801\u8b66\u544a\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6NASCAR\uff0c\u7528\u4e8e\u533a\u5206\u53ef\u64cd\u4f5c\u548c\u4e0d\u53ef\u64cd\u4f5c\u7684\u8b66\u544a\u3002", "motivation": "\u9759\u6001\u4ee3\u7801\u5206\u6790\u5de5\u5177\u4f1a\u4ea7\u751f\u5927\u91cf\u8b66\u544a\uff0c\u5176\u4e2d\u8bb8\u591a\u662f\u4e0d\u53ef\u64cd\u4f5c\u7684\uff0c\u5bfc\u81f4\u5f00\u53d1\u8005\u4ea7\u751f'\u8b66\u544a\u75b2\u52b3'\uff0c\u53ef\u80fd\u5ffd\u7565\u5173\u952e\u95ee\u9898\uff0c\u5f71\u54cd\u751f\u4ea7\u529b\u548c\u4ee3\u7801\u8d28\u91cf\u3002\u76ee\u524d\u7f3a\u4e4f\u8db3\u591f\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6539\u8fdbSCA\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u6536\u96c6\u548c\u5206\u7c7bSCA\u8b66\u544a\uff0c\u6709\u6548\u533a\u5206\u53ef\u64cd\u4f5c\u548c\u4e0d\u53ef\u64cd\u4f5c\u7684\u8b66\u544a\uff0c\u5e76\u57fa\u4e8e\u6b64\u65b9\u6cd5\u751f\u6210\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "result": "\u521b\u5efa\u4e86\u540d\u4e3aNASCAR\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7100\u4e07\u6761Java\u6e90\u4ee3\u7801\u8b66\u544a\u6761\u76ee\uff0c\u5e76\u5c06\u6570\u636e\u96c6\u548c\u751f\u6210\u5de5\u5177\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86SCA\u8b66\u544a\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u6539\u8fdb\u9759\u6001\u4ee3\u7801\u5206\u6790\u5de5\u5177\u7684\u51c6\u786e\u6027\u548c\u53ef\u7528\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u51cf\u8f7b\u8b66\u544a\u75b2\u52b3\u95ee\u9898\u3002"}}
{"id": "2511.10442", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.10442", "abs": "https://arxiv.org/abs/2511.10442", "authors": ["Aarush Agarwal", "Raymond He", "Jan Kieseler", "Matteo Cremonesi", "Shah Rukh Qasim"], "title": "FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing", "comment": null, "summary": "We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.", "AI": {"tldr": "FastGraph\u662f\u4e00\u79cd\u4e13\u4e3a\u4f4e\u7ef4\u7a7a\u95f4\uff082-10\u7ef4\uff09\u56fe\u6784\u5efa\u4f18\u5316\u7684GPU\u52a0\u901fk\u8fd1\u90bb\u7b97\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u901f20-40\u500d\u4e14\u51e0\u4e4e\u65e0\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u4e3a\u9ad8\u6027\u80fd\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u56fe\u6784\u5efa\u8fc7\u7a0b\u63d0\u4f9bGPU\u4f18\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "method": "\u91c7\u7528GPU\u9a7b\u7559\u7684\u7bb1\u5206\u533a\u65b9\u6cd5\uff0c\u652f\u6301\u5b8c\u6574\u68af\u5ea6\u6d41\u548c\u81ea\u9002\u5e94\u53c2\u6570\u8c03\u4f18\uff0c\u63d0\u5347\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u3002", "result": "\u5728\u7ef4\u5ea6\u5c0f\u4e8e10\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4FAISS\u3001ANNOY\u548cSCANN\u7b49\u5148\u8fdb\u5e93\u5feb20-40\u500d\uff0c\u5185\u5b58\u5f00\u9500\u51e0\u4e4e\u4e3a\u96f6\u3002", "conclusion": "FastGraph\u663e\u8457\u63d0\u5347\u4e86GNN\u5de5\u4f5c\u6d41\u7a0b\u7684\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u80fd\u7269\u7406\u7c92\u5b50\u805a\u7c7b\u3001\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u548c\u56fe\u805a\u7c7b\u7b49\u8ba1\u7b97\u5bc6\u96c6\u578b\u4f4e\u7ef4\u5e94\u7528\u3002"}}
{"id": "2511.10326", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.10326", "abs": "https://arxiv.org/abs/2511.10326", "authors": ["Shuangyu Lyu", "Chuan Luo", "Ruizhi Shi", "Wei Wu", "Chanjuan Liu", "Chunming Hu"], "title": "Towards Comprehensive Sampling of SMT Solutions", "comment": null, "summary": "This work focuses on effectively generating diverse solutions for satisfiability modulo theories (SMT) formulas, targeting the theories of bit-vectors, arrays, and uninterpreted functions, which is a critical task in software and hardware testing. Generating diverse SMT solutions helps uncover faults and detect safety violations during the verification and testing process, resulting in the SMT sampling problem, i.e., constructing a small number of solutions while achieving comprehensive coverage of the constraint space. While high coverage is crucial for exploring system behaviors, reducing the number of solutions is of great importance, as excessive solutions increase testing time and resource usage, undermining efficiency. In this work, we introduce PanSampler, a novel SMT sampler that achieves high coverage with a small number of solutions. It incorporates three novel techniques, i.e., diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring function and post-sampling optimization technology, enhancing its practical performance. It iteratively samples solutions, evaluates candidates, and employs local search to refine solutions, ensuring high coverage with a small number of samples. Extensive experiments on practical benchmarks demonstrate that PanSampler exhibits a significantly stronger capability to reach high target coverage, while requiring fewer solutions than current samplers to achieve the same coverage level. Furthermore, our empirical evaluation on practical subjects, which are collected from real-world software systems, shows that PanSampler achieves higher fault detection capability and reduces the number of required test cases from 32.6\\% to 76.4\\% to reach the same fault detection effectiveness, leading to a substantial improvement in testing efficiency. PanSampler advances SMT sampling, reducing the cost of software testing and hardware verification.", "AI": {"tldr": "PanSampler\u662f\u4e00\u4e2a\u65b0\u9896\u7684SMT\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u591a\u6837\u6027\u611f\u77e5\u7b97\u6cd5\u3001AST\u5f15\u5bfc\u8bc4\u5206\u51fd\u6570\u548c\u540e\u91c7\u6837\u4f18\u5316\u6280\u672f\uff0c\u7528\u5c11\u91cf\u89e3\u5b9e\u73b0\u9ad8\u8986\u76d6\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u8f6f\u4ef6\u6d4b\u8bd5\u548c\u786c\u4ef6\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u5728\u8f6f\u4ef6\u548c\u786c\u4ef6\u6d4b\u8bd5\u4e2d\uff0c\u751f\u6210\u591a\u6837\u5316\u7684SMT\u89e3\u5bf9\u4e8e\u53d1\u73b0\u6545\u969c\u548c\u5b89\u5168\u8fdd\u89c4\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u89e3\u624d\u80fd\u8fbe\u5230\u9ad8\u8986\u76d6\u7387\uff0c\u589e\u52a0\u4e86\u6d4b\u8bd5\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u3002", "method": "\u7ed3\u5408\u4e09\u79cd\u65b0\u6280\u672f\uff1a\u591a\u6837\u6027\u611f\u77e5SMT\u7b97\u6cd5\u3001AST\u5f15\u5bfc\u8bc4\u5206\u51fd\u6570\u548c\u540e\u91c7\u6837\u4f18\u5316\u6280\u672f\u3002\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u3001\u5019\u9009\u8bc4\u4f30\u548c\u5c40\u90e8\u641c\u7d22\u6765\u4f18\u5316\u89e3\uff0c\u786e\u4fdd\u7528\u5c11\u91cf\u6837\u672c\u8fbe\u5230\u9ad8\u8986\u76d6\u7387\u3002", "result": "\u5728\u5b9e\u8df5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPanSampler\u6bd4\u73b0\u6709\u91c7\u6837\u5668\u7528\u66f4\u5c11\u7684\u89e3\u8fbe\u5230\u76f8\u540c\u8986\u76d6\u7387\u6c34\u5e73\u3002\u5728\u5b9e\u9645\u8f6f\u4ef6\u7cfb\u7edf\u6d4b\u8bd5\u4e2d\uff0c\u5c06\u6240\u9700\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u51cf\u5c1132.6%\u523076.4%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7684\u6545\u969c\u68c0\u6d4b\u6548\u679c\u3002", "conclusion": "PanSampler\u663e\u8457\u63a8\u8fdb\u4e86SMT\u91c7\u6837\u6280\u672f\uff0c\u964d\u4f4e\u4e86\u8f6f\u4ef6\u6d4b\u8bd5\u548c\u786c\u4ef6\u9a8c\u8bc1\u7684\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\u3002"}}
{"id": "2511.10480", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10480", "abs": "https://arxiv.org/abs/2511.10480", "authors": ["Changhai Man", "Joongun Park", "Hanjiang Wu", "Huan Xu", "Srinivas Sridharan", "Tushar Krishna"], "title": "Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs", "comment": null, "summary": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.", "AI": {"tldr": "STAGE\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u6267\u884c\u8f68\u8ff9\u7684\u6846\u67b6\uff0c\u80fd\u591f\u51c6\u786e\u5efa\u6a21\u5927\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u652f\u6301\u5168\u9762\u7684\u5e76\u884c\u5316\u7b56\u7565\uff0c\u53ef\u6269\u5c55\u523032K GPU\u89c4\u6a21\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21AI\u8bad\u7ec3\u548c\u63a8\u7406\u7cfb\u7edf\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4ece\u771f\u5b9e\u7cfb\u7edf\u6536\u96c6\u6267\u884c\u8f68\u8ff9\uff0c\u4f46\u5927\u578b\u57fa\u7840\u8bbe\u65bd\u4e3b\u8981\u9650\u4e8e\u4e3b\u8981\u4e91\u63d0\u4f9b\u5546\uff0c\u4e14\u73b0\u6709\u5e73\u53f0\u7684\u8f68\u8ff9\u96be\u4ee5\u9002\u5e94\u672a\u6765\u66f4\u5927\u89c4\u6a21\u7cfb\u7edf\u914d\u7f6e\u7684\u7814\u7a76\u3002", "method": "\u5f15\u5165\u7b26\u53f7\u5f20\u91cf\u56fe\u751f\u6210\u5668(STAGE)\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u9ad8\u4fdd\u771f\u6267\u884c\u8f68\u8ff9\u6765\u5efa\u6a21LLM\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u652f\u6301\u5168\u9762\u7684\u5e76\u884c\u5316\u7b56\u7565\uff0c\u5141\u8bb8\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u5e7f\u6cdb\u7684LLM\u67b6\u6784\u548c\u7cfb\u7edf\u914d\u7f6e\u3002", "result": "STAGE\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5408\u6210\u8de8\u8d8a32K GPU\u7684\u9ad8\u4fdd\u771fLLM\u8f68\u8ff9\uff0c\u540c\u65f6\u5728\u8ba1\u7b97\u3001\u5185\u5b58\u548c\u901a\u4fe1\u65b9\u9762\u4fdd\u6301\u5f20\u91cf\u7ea7\u7cbe\u5ea6\u3002", "conclusion": "STAGE\u5c06\u516c\u5f00\u53ef\u7528\uff0c\u4ee5\u4fc3\u8fdb\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
