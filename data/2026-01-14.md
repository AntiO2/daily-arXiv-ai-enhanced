<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Where to Split? A Pareto-Front Analysis of DNN Partitioning for Edge Inference](https://arxiv.org/abs/2601.08025)
*Adiba Masud,Nicholas Foley,Pragathi Durga Rajarajan,Palden Lama*

Main category: cs.DC

TL;DR: ParetoPipe：一个基于帕累托前沿分析的多目标优化框架，用于在边缘设备上平衡DNN分区推理的延迟与吞吐量权衡。


<details>
  <summary>Details</summary>
Motivation: 现有DNN分区研究主要关注单目标优化（如最小化延迟或最大化吞吐量），但在实际边缘部署场景中，需要在延迟和吞吐量之间进行复杂权衡，且网络条件变化进一步加剧了这一挑战。

Method: 提出ParetoPipe框架，采用帕累托前沿分析来系统识别平衡延迟和吞吐量的最优分区策略。框架包含双通信后端（PyTorch RPC和自定义轻量级实现），并在树莓派和GPU边缘服务器组成的异构测试平台上进行基准测试。

Result: 通过帕累托最优点分析揭示了不同网络条件下的延迟-吞吐量权衡关系，并提供了灵活的开源工具链支持分布式推理和基准测试。

Conclusion: 将DNN分区重新定义为多目标优化问题，通过帕累托前沿分析能够系统平衡延迟和吞吐量的竞争目标，为实际边缘部署提供了更实用的解决方案。

Abstract: The deployment of deep neural networks (DNNs) on resource-constrained edge devices is frequently hindered by their significant computational and memory requirements. While partitioning and distributing a DNN across multiple devices is a well-established strategy to mitigate this challenge, prior research has largely focused on single-objective optimization, such as minimizing latency or maximizing throughput. This paper challenges that view by reframing DNN partitioning as a multi-objective optimization problem. We argue that in real-world scenarios, a complex trade-off between latency and throughput exists, which is further complicated by network variability. To address this, we introduce ParetoPipe, an open-source framework that leverages Pareto front analysis to systematically identify optimal partitioning strategies that balance these competing objectives.
  Our contributions are threefold: we benchmark pipeline partitioned inference on a heterogeneous testbed of Raspberry Pis and a GPU-equipped edge server; we identify Pareto-optimal points to analyze the latency-throughput trade-off under varying network conditions; and we release a flexible, open-source framework to facilitate distributed inference and benchmarking. This toolchain features dual communication backends, PyTorch RPC and a custom lightweight implementation, to minimize overhead and support broad experimentation.

</details>


### [2] [Hierarchical Precision and Recursion for Accelerating Symmetric Linear Solves on MXUs](https://arxiv.org/abs/2601.08082)
*Vicki Carrica,Rabab Alomairy,Evelyne Ringoot,Alan Edelman*

Main category: cs.DC

TL;DR: 提出一种针对矩阵处理单元（MXU）的便携式混合精度对称线性求解器，通过递归分解和混合精度策略，在NVIDIA H200和AMD MI300X上实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 对称线性求解在科学计算和工程应用中至关重要，但传统Cholesky分解及其相关操作（TRSM、SYRK）在矩阵处理单元上的性能仍有提升空间，需要兼顾计算效率和数值稳定性。

Method: 采用嵌套递归算法，将Cholesky分解递归分解为TRSM和SYRK子问题；设计自定义递归数据结构，对大型非对角块使用FP16低精度计算，对角块保持高精度以确保数值稳定性；在Julia中实现，利用数组编程和多分派特性。

Result: 在NVIDIA H200上：递归FP64 SYRK比cuBLAS快14倍；混合精度SYRK比全精度基线快27倍，TRSM快5倍；Cholesky整体比cuSOLVER FP64快5倍，精度比纯FP16高100倍，同时保持88%的峰值加速。在AMD MI300X上观察到类似的性能和精度趋势。

Conclusion: 该方法实现了跨GPU平台的便携式高性能对称线性求解，通过混合精度策略在保持数值稳定性的同时显著提升计算性能，为科学计算和机器学习应用提供了有效的解决方案。

Abstract: Symmetric linear solves are fundamental to a wide range of scientific and engineering applications, from climate modeling and structural analysis to machine learning and optimization. These workloads often rely on Cholesky (POTRF) decomposition and its supporting operations, triangular solves (TRSM) and symmetric rank-k updates (SYRK), which together form the computational core for solving symmetric positive-definite systems. To accelerate these kernels, we present a portable, mixed-precision solver designed for Matrix Processing Units (MXUs), including NVIDIA Tensor Cores (H200) and AMD Matrix Cores (MI300X). Our algorithm builds on a nested recursive formulation in which Cholesky exposes parallelism through recursive decomposition of its TRSM and SYRK sub-problems. This structure yields a hierarchical recursion that maximizes GEMM throughput while enabling fine-grained control over numerical precision. We introduce a custom recursive data structure that assigns low-precision FP16 arithmetic to large off-diagonal blocks, while preserving high precision on diagonal blocks to ensure numerical stability. The solver is implemented in Julia, leveraging array programming, multiple dispatch, and dynamic type inference to enable seamless expression of mixed-precision computation. This design provides a high-level, hardware-agnostic interface while efficiently interfacing with low-level vendor libraries for backend portability. On H200, our recursive FP64 SYRK achieves a 14x speedup over cuBLAS, while mixed-precision delivers up to 27x speedup in SYRK and 5x in TRSM over full-precision baselines. This results in a 5x overall speedup for Cholesky versus cuSOLVER FP64, with 100x better accuracy than pure FP16 while retaining 88% of its peak speedup. Comparable performance and accuracy trends are observed on MI300X, demonstrating broad applicability across GPUs.

</details>


### [3] [Matrix-PIC: Harnessing Matrix Outer-product for High-Performance Particle-in-Cell Simulations](https://arxiv.org/abs/2601.08277)
*Yizhuo Rao,Xingjian Cui,Jiabin Xie,Shangzhi Pang,Guangnan Feng,Jinhui Wei,Zhiguang Chen,Yutong Lu*

Main category: cs.DC

TL;DR: MatrixPIC：首个针对现代CPU中MPU-VPU混合架构的PIC模拟协同设计，通过矩阵化沉积算法、混合执行流水线和增量排序，显著提升粒子-网格交互性能。


<details>
  <summary>Details</summary>
Motivation: 传统多核CPU上PIC模拟的粒子-网格交互中，细粒度原子更新成为主要瓶颈。现代CPU集成了专门支持矩阵外积运算的MPU，为克服这一限制提供了新机会。

Method: 1) 提出块矩阵形式的电流沉积算法，自然映射到MPU外积原语；2) 设计混合执行流水线，结合MPU高密度累积与VPU数据准备和控制流；3) 基于间隙填充内存数组的O(1)摊销增量排序器，保持数据局部性。

Result: 在下一代HPC平台上，MatrixPIC在激光尾波场加速模拟中实现总运行时间2.63倍加速，三阶沉积核心内核比基线加速8.7倍，比最佳手动优化VPU实现快2.0倍，达到理论CPU峰值性能的83.08%，比数据中心GPU上高度优化的CUDA内核高近2.8倍。

Conclusion: MatrixPIC证明了面向矩阵的协同设计在加速新兴CPU架构上PIC模拟的有效性，为利用现代CPU中专用矩阵处理单元提供了新途径。

Abstract: Particle-in-Cell (PIC) simulations spend most of their execution time on particle--grid interactions, where fine-grained atomic updates become a major bottleneck on traditional many-core CPUs. Recent CPU architectures integrate specialized Matrix Processing Units (MPUs) that efficiently support matrix outer-product operations, offering new opportunities to overcome this limitation. Leveraging this architectural shift, this work focuses on redesigning the current deposition step of PIC simulations under a matrix-centric execution model.
  We present MatrixPIC, the first holistic co-design of the deposition kernel, data layout, and incremental particle sorting tailored to the hybrid MPU--VPU SIMD model on modern CPUs. MatrixPIC introduces: (i)~a block-matrix formulation of the current deposition algorithm that maps naturally to MPU outer-product primitives; (ii)~a hybrid execution pipeline that combines MPU-based high-density accumulation with VPU-based data preparation and control flow; and (iii)~an $O(1)$-amortized incremental sorter based on a gapped packed-memory array to preserve data locality for efficient MPU execution.
  Evaluated on a next-generation HPC platform, MatrixPIC achieves significant performance gains. In Laser-Wakefield Acceleration (LWFA) simulations, it delivers up to $2.63\times$ speedup in total runtime. For third-order deposition, the core kernel is accelerated by $8.7\times$ over the baseline and $2.0\times$ over the best hand-optimized VPU implementation. Moreover, MatrixPIC reaches $83.08\%$ of theoretical CPU peak performance, nearly $2.8\times$ higher than a highly optimized CUDA kernel on a data center GPU. These results demonstrate the effectiveness of matrix-oriented co-design for accelerating PIC simulations on emerging CPU architectures.

</details>


### [4] [Shifting the Sweet Spot: High-Performance Matrix-Free Method for High-Order Elasticity](https://arxiv.org/abs/2601.08374)
*Dali Chang,Chong Zhang,Kaiqi Zhang,Mingguan Yang,Huiyuan Li,Weiqiang Kong*

Main category: cs.DC

TL;DR: 针对弹性力学高阶有限元分析，本文提出并实现了一种高度优化的矩阵自由算子，通过多级优化策略将性能最佳点从低阶p≈2提升到高阶p≥6区域，在主流CPU架构上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有矩阵自由方法未能充分利用现代CPU架构和张量积单元的特殊结构，导致其性能最佳点异常地停留在低阶p≈2，严重限制了高阶方法的潜力。

Method: 在MFEM框架内设计并实现高度优化的矩阵自由算子，与几何多重网格预处理器深度集成。采用多级优化策略：1）用基于张量分解的O(p^4)算法替换原始O(p^6)通用算法；2）利用Voigt对称性减少弹性问题的冗余计算；3）采用宏核融合增强数据局部性并突破内存带宽瓶颈。

Result: 在主流x86和ARM架构上的广泛实验表明，该方法成功将性能最佳点转移到高阶区域p≥6。与MFEM基线相比，优化后的核心算子实现了7x到83x的加速，在完整求解过程中实现了3.6x到16.8x的端到端性能提升。

Conclusion: 本文为在主流CPU硬件上进行大规模高阶弹性模拟提供了一条经过验证的高效实践路径，通过深度优化成功解决了矩阵自由方法在高阶区域性能受限的问题。

Abstract: In high-order finite element analysis for elasticity, matrix-free (PA) methods are a key technology for overcoming the memory bottleneck of traditional Full Assembly (FA). However, existing implementations fail to fully exploit the special structure of modern CPU architectures and tensor-product elements, causing their performance "sweet spot" to anomalously remain at the low order of $p \approx 2$, which severely limits the potential of high-order methods. To address this challenge, we design and implement a highly optimized PA operator within the MFEM framework, deeply integrated with a Geometric Multigrid (GMG) preconditioner. Our multi-level optimization strategy includes replacing the original $O(p^6)$ generic algorithm with an efficient $O(p^4)$ one based on tensor factorization, exploiting Voigt symmetry to reduce redundant computations for the elasticity problem, and employing macro-kernel fusion to enhance data locality and break the memory bandwidth bottleneck. Extensive experiments on mainstream x86 and ARM architectures demonstrate that our method successfully shifts the performance "sweet spot" to the higher-order region of $p \ge 6$. Compared to the MFEM baseline, the optimized core operator (kernel) achieves speedups of 7x to 83x, which translates to a 3.6x to 16.8x end-to-end performance improvement in the complete solution process. This paper provides a validated and efficient practical path for conducting large-scale, high-order elasticity simulations on mainstream CPU hardware.

</details>


### [5] [MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm](https://arxiv.org/abs/2601.08800)
*Bowen Zhou,Jinrui Jia,Wenhao He,Yong Zhang,Fang Dong*

Main category: cs.DC

TL;DR: MixServe是一个用于高效部署MoE模型的自动分布式服务系统，采用基于融合AR-A2A通信算法的TP-EP混合并行策略，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型参数庞大，只能在多GPU/多节点系统中部署，通信成为主要瓶颈。现有TP方法节点间效率低，EP方法负载不均衡，需要更好的并行策略。

Method: 提出MixServe系统：1) 评估不同并行策略的通信开销，自动选择最优策略；2) 提出基于融合AR-A2A通信算法的TP-EP混合并行，重叠节点内AR通信和节点间A2A通信。

Result: 在DeepSeek-R1和Qwen3模型上实验显示：首token时间加速1.08-3.80倍，token间延迟加速1.03-1.66倍，吞吐量提升5.2%-50.3%。

Conclusion: MixServe通过自动选择并行策略和融合通信算法，有效解决了MoE模型分布式服务中的通信瓶颈问题，显著提升了推理性能。

Abstract: The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node & multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.
  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [6] [CSQL: Mapping Documents into Causal Databases](https://arxiv.org/abs/2601.08109)
*Sridhar Mahadevan*

Main category: cs.DB

TL;DR: CSQL系统将非结构化文本文档自动转换为可SQL查询的因果数据库，支持因果干预和结构化因果查询，超越传统关联检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有系统（如RAG或知识图谱）主要支持关联检索，无法回答"为什么"这类因果问题。需要一种能够从文档集合中提取因果信息并进行因果分析的系统。

Method: 基于DEMOCRITUS系统，从文档中提取数千个局部因果模型，构建因果数据库。支持将RAG/IE编译的因果语料库大规模整合，如TCC经济学论文数据集。

Result: 成功构建包含265,656个因果声明实例的因果数据库，涵盖45,319篇论文、44年数据和1,575个报告方法字符串。支持文档级和语料库级的因果查询与纵向分析。

Conclusion: CSQL作为从非结构化文档到因果数据库的编译器，配备有原则的查询代数，可广泛应用于商业、人文和科学等多个领域，实现真正的因果分析而不仅仅是关联检索。

Abstract: We describe a novel system, CSQL, which automatically converts a collection of unstructured text documents into an SQL-queryable causal database (CDB). A CDB differs from a traditional DB: it is designed to answer "why'' questions via causal interventions and structured causal queries. CSQL builds on our earlier system, DEMOCRITUS, which converts documents into thousands of local causal models derived from causal discourse. Unlike RAG-based systems or knowledge-graph based approaches, CSQL supports causal analysis over document collections rather than purely associative retrieval. For example, given an article on the origins of human bipedal walking, CSQL enables queries such as: "What are the strongest causal influences on bipedalism?'' or "Which variables act as causal hubs with the largest downstream influence?'' Beyond single-document case studies, we show that CSQL can also ingest RAG/IE-compiled causal corpora at scale by compiling the Testing Causal Claims (TCC) dataset of economics papers into a causal database containing 265,656 claim instances spanning 45,319 papers, 44 years, and 1,575 reported method strings, thereby enabling corpus-level causal queries and longitudinal analyses in CSQL. Viewed abstractly, CSQL functions as a compiler from unstructured documents into a causal database equipped with a principled algebra of queries, and can be applied broadly across many domains ranging from business, humanities, and science.

</details>


### [7] [SVFusion: A CPU-GPU Co-Processing Architecture for Large-Scale Real-Time Vector Search](https://arxiv.org/abs/2601.08528)
*Yuchen Peng,Dingyu Yang,Zhongle Xie,Ji Sun,Lidan Shou,Ke Chen,Gang Chen*

Main category: cs.DB

TL;DR: SVFusion是一个GPU-CPU-磁盘协同框架，用于实时向量搜索，通过层次化索引架构和负载感知缓存机制，在保证高召回率的同时显著提升查询吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有CPU方案支持更新但吞吐量低，GPU加速系统性能高但面临动态更新和GPU内存限制的挑战，导致大规模实时向量搜索存在性能差距。

Method: 采用GPU-CPU-磁盘协同框架，包含层次化向量索引架构、CPU-GPU协同处理、负载感知向量缓存机制、CUDA多流优化、自适应资源管理和并发控制。

Result: 相比基线方法，平均吞吐量提升20.9倍，延迟降低1.3-50.7倍，同时在大规模数据集和各种流式工作负载下保持高召回率。

Conclusion: SVFusion成功解决了大规模实时向量搜索中性能与更新能力的平衡问题，为信息检索和推荐系统等应用提供了高效解决方案。

Abstract: Approximate Nearest Neighbor Search (ANNS) underpins modern applications such as information retrieval and recommendation. With the rapid growth of vector data, efficient indexing for real-time vector search has become rudimentary. Existing CPU-based solutions support updates but suffer from low throughput, while GPU-accelerated systems deliver high performance but face challenges with dynamic updates and limited GPU memory, resulting in a critical performance gap for continuous, large-scale vector search requiring both accuracy and speed. In this paper, we present SVFusion, a GPU-CPU-disk collaborative framework for real-time vector search that bridges sophisticated GPU computation with online updates. SVFusion leverages a hierarchical vector index architecture that employs CPU-GPU co-processing, along with a workload-aware vector caching mechanism to maximize the efficiency of limited GPU memory. It further enhances performance through real-time coordination with CUDA multi-stream optimization and adaptive resource management, along with concurrency control that ensures data consistency under interleaved queries and updates. Empirical results demonstrate that SVFusion achieves significant improvements in query latency and throughput, exhibiting a 20.9x higher throughput on average and 1.3x to 50.7x lower latency compared to baseline methods, while maintaining high recall for large-scale datasets under various streaming workloads.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [SECite: Analyzing and Summarizing Citations in Software Engineering Literature](https://arxiv.org/abs/2601.07939)
*Shireesh Reddy Pyreddy,Khaja Valli Pathan,Hasan Masum,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: SECite：通过引文情感分析评估学术影响力的新方法，结合NLP和生成式AI自动分析引文情感并生成总结


<details>
  <summary>Details</summary>
Motivation: 传统文献综述仅反映作者自我陈述的视角，无法全面了解论文的实际贡献和局限性。通过分析其他研究者如何讨论和引用论文，可以获得更深入、更实用的理解。

Method: 开发半自动化流程提取9篇研究论文的引文，应用先进的NLP技术和无监督机器学习对引文语句进行正面/负面情感分类，并使用生成式AI基于聚类引文组和全文生成情感特定的总结。

Result: 揭示了学术社区对这些工作的感知模式，突出了外部引文反馈与作者自我陈述之间的契合与分歧点。

Conclusion: 通过整合引文情感分析和基于LLM的总结，本研究为评估学术贡献提供了一个全面的框架。

Abstract: Identifying the strengths and limitations of a research paper is a core component of any literature review. However, traditional summaries reflect only the authors' self-presented perspective. Analyzing how other researchers discuss and cite the paper can offer a deeper, more practical understanding of its contributions and shortcomings. In this research, we introduce SECite, a novel approach for evaluating scholarly impact through sentiment analysis of citation contexts. We develop a semi-automated pipeline to extract citations referencing nine research papers and apply advanced natural language processing (NLP) techniques with unsupervised machine learning to classify these citation statements as positive or negative. Beyond sentiment classification, we use generative AI to produce sentiment-specific summaries that capture the strengths and limitations of each target paper, derived both from clustered citation groups and from the full text. Our findings reveal meaningful patterns in how the academic community perceives these works, highlighting areas of alignment and divergence between external citation feedback and the authors' own presentation. By integrating citation sentiment analysis with LLM-based summarization, this study provides a comprehensive framework for assessing scholarly contributions.

</details>


### [9] [Towards Verifiably Safe Tool Use for LLM Agents](https://arxiv.org/abs/2601.08012)
*Aarya Doshi,Yining Hong,Congying Xu,Eunsuk Kang,Alexandros Kapravelos,Christian Kästner*

Main category: cs.SE

TL;DR: 提出结合STPA安全分析和增强MCP框架的方法，为LLM智能体提供形式化安全保障，从临时可靠性修复转向主动防护


<details>
  <summary>Details</summary>
Motivation: LLM智能体通过工具调用扩展能力，但可能引发敏感数据泄露、关键记录覆盖等风险，现有方法无法保证系统安全，需要形式化保障机制

Method: 1) 应用系统理论过程分析(STPA)识别智能体工作流中的危险，推导安全要求，形式化为数据流和工具序列的可执行规范；2) 引入能力增强的模型上下文协议(MCP)框架，要求对能力、机密性和信任级别进行结构化标注

Result: 该方法旨在将LLM智能体安全从临时可靠性修复转向具有形式化保证的主动防护，减少对用户确认的依赖，使自主性成为有意识的设计选择

Conclusion: 通过STPA安全分析和增强MCP框架的结合，为LLM智能体提供形式化安全保障，实现从可靠性到安全性的转变，支持企业级应用

Abstract: Large language model (LLM)-based AI agents extend LLM capabilities by enabling access to tools such as data sources, APIs, search engines, code sandboxes, and even other agents. While this empowers agents to perform complex tasks, LLMs may invoke unintended tool interactions and introduce risks, such as leaking sensitive data or overwriting critical records, which are unacceptable in enterprise contexts. Current approaches to mitigate these risks, such as model-based safeguards, enhance agents' reliability but cannot guarantee system safety. Methods like information flow control (IFC) and temporal constraints aim to provide guarantees but often require extensive human annotation. We propose a process that starts with applying System-Theoretic Process Analysis (STPA) to identify hazards in agent workflows, derive safety requirements, and formalize them as enforceable specifications on data flows and tool sequences. To enable this, we introduce a capability-enhanced Model Context Protocol (MCP) framework that requires structured labels on capabilities, confidentiality, and trust level. Together, these contributions aim to shift LLM-based agent safety from ad hoc reliability fixes to proactive guardrails with formal guarantees, while reducing dependence on user confirmation and making autonomy a deliberate design choice.

</details>


### [10] [Automating API Documentation from Crowdsourced Knowledge](https://arxiv.org/abs/2601.08036)
*Bonan Kou,Zijie Zhou,Muhao Chen,Tianyi Zhang*

Main category: cs.SE

TL;DR: AutoDoc：利用Stack Overflow讨论和GPT-4o自动生成更准确、更全面的API文档，通过专门组件处理LLM幻觉和冗余问题


<details>
  <summary>Details</summary>
Motivation: 官方API文档经常存在过时和不完整的问题，需要从开发者社区（如Stack Overflow）中提取知识来补充和完善文档

Method: 1. 使用微调的密集检索模型从Stack Overflow帖子中识别7类API知识；2. 用GPT-4o总结知识为简洁文本；3. 设计专门组件处理LLM幻觉和内容冗余

Result: 在48个不同流行度的API上评估，AutoDoc生成的文档比基线准确度高77.7%，重复率低9.5%，包含34.4%官方文档未覆盖的知识；用户研究显示所有参与者都认为AutoDoc文档更全面、简洁、有帮助

Conclusion: 通过精心设计对抗LLM幻觉和信息冗余，利用LLMs生成API文档是可行的，即使较小的开源模型也能达到可比效果

Abstract: API documentation is crucial for developers to learn and use APIs. However, it is known that many official API documents are obsolete and incomplete. To address this challenge, we propose a new approach called AutoDoc that generates API documents with API knowledge extracted from online discussions on Stack Overflow (SO). AutoDoc leverages a fine-tuned dense retrieval model to identify seven types of API knowledge from SO posts. Then, it uses GPT-4o to summarize the API knowledge in these posts into concise text. Meanwhile, we designed two specific components to handle LLM hallucination and redundancy in generated content. We evaluated AutoDoc against five comparison baselines on 48 APIs of different popularity levels. Our results indicate that the API documents generated by AutoDoc are up to 77.7% more accurate, 9.5% less duplicated, and contain 34.4% knowledge uncovered by the official documents. We also measured the sensitivity of AutoDoc to the choice of different LLMs. We found that while larger LLMs produce higher-quality API documents, AutoDoc enables smaller open-source models (e.g., Mistral-7B-v0.3) to achieve comparable results. Finally, we conducted a user study to evaluate the usefulness of the API documents generated by AutoDoc. All participants found API documents generated by AutoDoc to be more comprehensive, concise, and helpful than the comparison baselines. This highlights the feasibility of utilizing LLMs for API documentation with careful design to counter LLM hallucination and information redundancy.

</details>


### [11] [Cognitive Biases in LLM-Assisted Software Development](https://arxiv.org/abs/2601.08045)
*Xinyi Zhou,Zeinadsadat Saghi,Sadra Sabouri,Rahul Pandita,Mollie McGuire,Souti Chattopadhyay*

Main category: cs.SE

TL;DR: 该研究首次全面调查了LLM辅助开发中的认知偏见，发现48.8%的程序员行为存在偏见，其中56.4%与开发者-LLM交互相关，并提出了15个偏见分类和缓解建议。


<details>
  <summary>Details</summary>
Motivation: LLM在软件开发中的广泛应用正在将编程从解决方案生成活动转变为解决方案评估活动，这种转变可能放大现有决策偏见或创造全新偏见。研究旨在了解认知偏见如何在新兴AI协作开发中显现并影响决策。

Method: 采用混合方法：首先对14名学生和专业开发者进行观察研究，然后对另外22名开发者进行问卷调查。通过定性比较传统非LLM工作流中影响开发者的偏见类别，系统分析了90个特定于开发者-LLM交互的认知偏见。

Result: 研究发现48.8%的程序员行为存在偏见，其中56.4%的偏见行为与开发者-LLM交互相关。LLM相关行为更可能与新型偏见相关。研究开发了由认知心理学家验证的15个偏见分类法。

Conclusion: LLM辅助开发中存在显著的认知偏见问题，需要开发工具和实践来缓解。研究为开发者和LLM工具构建者提供了具体建议，以改善人机协作编程中的决策质量。

Abstract: The widespread adoption of Large Language Models (LLMs) in software development is transforming programming from a solution-generative to a solution-evaluative activity. This shift opens a pathway for new cognitive challenges that amplify existing decision-making biases or create entirely novel ones. One such type of challenge stems from cognitive biases, which are thinking patterns that lead people away from logical reasoning and result in sub-optimal decisions. How do cognitive biases manifest and impact decision-making in emerging AI-collaborative development? This paper presents the first comprehensive study of cognitive biases in LLM-assisted development. We employ a mixed-methods approach, combining observational studies with 14 student and professional developers, followed by surveys with 22 additional developers. We qualitatively compare categories of biases affecting developers against the traditional non-LLM workflows. Our findings suggest that LLM-related actions are more likely to be associated with novel biases. Through a systematic analysis of 90 cognitive biases specific to developer-LLM interactions, we develop a taxonomy of 15 bias categories validated by cognitive psychologists. We found that 48.8% of total programmer actions are biased, and developer-LLM interactions account for 56.4% of these biased actions. We discuss how these bias categories manifest, present tools and practices for developers, and recommendations for LLM tool builders to help mitigate cognitive biases in human-AI programming.

</details>


### [12] [Coverage-Guided Road Selection and Prioritization for Efficient Testing in Autonomous Driving Systems](https://arxiv.org/abs/2601.08609)
*Qurban Ali,Andrea Stocco,Leonardo Mariani,Oliviero Riganelli*

Main category: cs.SE

TL;DR: 提出基于几何和行为多样性的ADAS测试优先级框架，通过聚类和代表性选择减少冗余测试，在保持79%失败场景的同时减少89%测试规模，早期故障检测提升95倍。


<details>
  <summary>Details</summary>
Motivation: ADAS需要大量测试确保安全，但现有道路场景数据集存在大量冗余案例，这些冗余会减慢测试过程而不提高故障检测能力，需要一种能减少冗余同时保持测试多样性的方法。

Method: 提出新颖的测试优先级框架：1) 基于ADAS驾驶行为的几何和动态特征对道路场景进行聚类；2) 从每个聚类中选择代表性案例以保证覆盖；3) 基于几何复杂度、驾驶难度和历史失败率对道路进行优先级排序，确保最关键和最具挑战性的测试优先执行。

Result: 在OPENCAT数据集和Udacity自动驾驶模拟器上使用两个ADAS模型进行评估：平均减少89%测试套件规模，同时保留79%的失败道路场景；优先级策略相比随机基线将早期故障检测提升高达95倍。

Conclusion: 该测试优先级框架能有效减少ADAS测试冗余，显著提高测试效率，在保持故障检测能力的同时大幅降低测试成本，为自动驾驶系统的安全验证提供了实用解决方案。

Abstract: Autonomous Driving Assistance Systems (ADAS) rely on extensive testing to ensure safety and reliability, yet road scenario datasets often contain redundant cases that slow down the testing process without improving fault detection. To address this issue, we present a novel test prioritization framework that reduces redundancy while preserving geometric and behavioral diversity. Road scenarios are clustered based on geometric and dynamic features of the ADAS driving behavior, from which representative cases are selected to guarantee coverage. Roads are finally prioritized based on geometric complexity, driving difficulty, and historical failures, ensuring that the most critical and challenging tests are executed first. We evaluate our framework on the OPENCAT dataset and the Udacity self-driving car simulator using two ADAS models. On average, our approach achieves an 89% reduction in test suite size while retaining an average of 79% of failed road scenarios. The prioritization strategy improves early failure detection by up to 95x compared to random baselines.

</details>


### [13] [LLMs in Code Vulnerability Analysis: A Proof of Concept](https://arxiv.org/abs/2601.08691)
*Shaznin Sultana,Sadia Afreen,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 研究评估了5对代码专用和通用LLM在C/C++漏洞数据集上的表现，发现微调优于零/少样本方法，代码专用模型在复杂任务上表现更好，但现有评估指标存在不足。


<details>
  <summary>Details</summary>
Motivation: 传统软件安全分析方法难以应对现代代码库的规模和复杂性，需要智能自动化来更高效准确地检测、评估和修复漏洞。本研究探索利用代码专用和通用大语言模型来自动化关键软件安全任务。

Method: 评估了5对近期的大语言模型（包括代码专用和通用开源模型），在Big-Vul和Vul-Repair两个C/C++漏洞数据集上进行测试。比较了微调和基于提示的方法。

Result: 微调在所有任务和模型上都优于零样本和少样本方法。代码专用模型在复杂任务的零/少样本设置中表现更佳，而通用模型效果相近。现有评估指标（CodeBLEU、CodeBERTScore、BLEU、ChrF）在衡量修复质量方面存在不足。

Conclusion: 本研究通过探索先进大语言模型在改进漏洞分析和修复方面的潜力，为软件安全社区做出了贡献。

Abstract: Context: Traditional software security analysis methods struggle to keep pace with the scale and complexity of modern codebases, requiring intelligent automation to detect, assess, and remediate vulnerabilities more efficiently and accurately. Objective: This paper explores the incorporation of code-specific and general-purpose Large Language Models (LLMs) to automate critical software security tasks, such as identifying vulnerabilities, predicting severity and access complexity, and generating fixes as a proof of concept. Method: We evaluate five pairs of recent LLMs, including both code-based and general-purpose open-source models, on two recognized C/C++ vulnerability datasets, namely Big-Vul and Vul-Repair. Additionally, we compare fine-tuning and prompt-based approaches. Results: The results show that fine-tuning uniformly outperforms both zero-shot and few-shot approaches across all tasks and models. Notably, code-specialized models excel in zero-shot and few-shot settings on complex tasks, while general-purpose models remain nearly as effective. Discrepancies among CodeBLEU, CodeBERTScore, BLEU, and ChrF highlight the inadequacy of current metrics for measuring repair quality. Conclusions: This study contributes to the software security community by investigating the potential of advanced LLMs to improve vulnerability analysis and remediation.

</details>


### [14] ["Where is My Troubleshooting Procedure?": Studying the Potential of RAG in Assisting Failure Resolution of Large Cyber-Physical System](https://arxiv.org/abs/2601.08706)
*Maria Teresa Rossi,Leonardo Mariani,Oliviero Riganelli,Giuseppe Filomento,Danilo Giannone,Paolo Gavazzo*

Main category: cs.SE

TL;DR: RAG系统能帮助操作员在复杂工业环境中快速检索故障排除程序，但需要交叉验证建议后再执行


<details>
  <summary>Details</summary>
Motivation: 工业环境中操作员需要从大量技术手册中检索故障排除程序，但手册的复杂性和规模会显著减慢关键事故期间的检索速度，影响应急响应

Method: 基于检索增强生成（RAG）开发对话界面工具，分析Fincantieri公司的海军网络物理系统故障排除程序，进行一系列实验评估

Result: RAG能够帮助操作员快速响应故障症状，但需要在执行建议前采取特定措施进行交叉验证

Conclusion: RAG技术可以有效辅助工业操作员检索故障排除程序，提高应急响应能力，但需要建立验证机制确保建议的可靠性

Abstract: In today's complex industrial environments, operators must often navigate through extensive technical manuals to identify troubleshooting procedures that may help react to some observed failure symptoms. These manuals, written in natural language, describe many steps in detail. Unfortunately, the number, magnitude, and articulation of these descriptions can significantly slow down and complicate the retrieval of the correct procedure during critical incidents. Interestingly, Retrieval Augmented Generation (RAG) enables the development of tools based on conversational interfaces that can assist operators in their retrieval tasks, improving their capability to respond to incidents. This paper presents the results of a set of experiments that derive from the analysis of the troubleshooting procedures available in Fincantieri, a large international company developing complex naval cyber-physical systems. Results show that RAG can assist operators in reacting promptly to failure symptoms, although specific measures have to be taken into consideration to cross-validate recommendations before actuating them.

</details>


### [15] [Revisiting "Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion": A Critical Review and Implications on DNN Coverage Testing](https://arxiv.org/abs/2601.08729)
*Jinhan Kim,Nargiz Humbatova,Gunel Jahangirova,Shin Yoo,Paolo Tonella*

Main category: cs.SE

TL;DR: 对ICSE 2023提出的Neural Coverage (NLC)进行批判性评审，指出其在理论假设、设计原则和实证有效性方面的问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 虽然NLC声称满足八个设计需求并展示了强大的实证性能，但作者质疑其理论和实证假设，认为NLC偏离了覆盖率准则的核心原则，且实证研究存在有效性威胁。

Method: 通过理论分析和实证验证相结合的方法：1) 分析NLC是否满足覆盖率准则的核心原则（如单调性和测试套件顺序独立性）；2) 评估NLC对协方差矩阵关键属性的处理；3) 识别实证研究中的有效性威胁；4) 通过实证验证支持论点。

Result: 研究发现：1) NLC偏离了覆盖率准则的核心原则；2) 未能充分考虑协方差矩阵的关键属性；3) 实证研究存在有效性威胁（特别是关于测试套件真实排序的问题）；4) 实证验证支持了这些论点。

Conclusion: 论文提出了对NLC的批判性见解，为未来DNN覆盖率指标的改进提供了建议，并讨论了这些发现对DNN测试领域的影响。

Abstract: We present a critical review of Neural Coverage (NLC), a state-of-the-art DNN coverage criterion by Yuan et al. at ICSE 2023. While NLC proposes to satisfy eight design requirements and demonstrates strong empirical performance, we question some of their theoretical and empirical assumptions. We observe that NLC deviates from core principles of coverage criteria, such as monotonicity and test suite order independence, and could more fully account for key properties of the covariance matrix. Additionally, we note threats to the validity of the empirical study, related to the ground truth ordering of test suites. Through our empirical validation, we substantiate our claims and propose improvements for future DNN coverage metrics. Finally, we conclude by discussing the implications of these insights.

</details>


### [16] [TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback](https://arxiv.org/abs/2601.08734)
*Prithwish Jana,Sam Davidson,Bhavana Bhasker,Andrey Kan,Anoop Deoras,Laurent Callot*

Main category: cs.SE

TL;DR: TerraFormer是一个神经符号框架，结合监督微调和验证器引导的强化学习，用于IaC生成和变异，在多个数据集上显著提升正确性并超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 自动化基础设施即代码(IaC)具有挑战性，大型语言模型(LLM)从自然语言生成配置时经常出错，需要更可靠的解决方案。

Method: 提出TerraFormer神经符号框架，结合监督微调与验证器引导的强化学习，使用形式化验证工具提供语法、可部署性和策略合规性反馈。通过多阶段验证和迭代LLM自校正构建了两个高质量数据集。

Result: 在IaC-Eval上比基础LLM提升15.94%正确性，在TF-Gen和TF-Mutn测试集上分别提升11.65%和19.60%。超越包括Sonnet 3.7、DeepSeek-R1、GPT-4.1等约50倍大的模型，在IaC-Eval排名第三，在最佳实践和安全合规方面表现最佳。

Conclusion: TerraFormer框架通过神经符号方法有效解决了IaC自动化的可靠性问题，在多个评估指标上超越现有大模型，展示了验证器引导强化学习在代码生成任务中的有效性。

Abstract: Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.

</details>


### [17] [Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs](https://arxiv.org/abs/2601.08773)
*Manideep Reddy Chinthareddy*

Main category: cs.SE

TL;DR: 比较三种RAG检索管道在软件工程中的应用：向量检索、LLM生成知识图谱和AST确定性知识图谱，发现AST方法在成本、覆盖率和正确性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统的向量相似性检索在软件工程中能捕捉主题相似性，但在多跳架构推理（如控制器-服务-仓库链、接口驱动连接、继承关系）方面存在不足，需要更有效的检索方法。

Method: 在Java代码库（Shopizer、ThingsBoard、OpenMRS Core）上基准测试三种检索管道：A) 纯向量检索（No-Graph RAG）；B) LLM生成知识图谱RAG（LLM-KB）；C) 确定性AST派生知识图谱RAG（DKB），使用Tree-sitter和双向遍历构建。每个仓库使用15个架构和代码追踪查询，测量索引时间、查询延迟、语料覆盖率、成本和答案正确性。

Result: DKB在几秒内构建图谱，而LLM-KB需要更长时间；LLM-KB存在索引不完整问题（Shopizer中377个文件被跳过）；DKB端到端成本相对较低，LLM-KB成本随仓库规模增加而显著上升；查询延迟方面，No-Graph和DKB相似，LLM-KB较慢且不稳定；在Shopizer问题集上，DKB正确率最高，LLM-KB次之，纯向量检索在架构查询上表现最差且幻觉风险最高。

Conclusion: 确定性AST派生图谱相比LLM提取图谱，在显著降低索引成本的同时，提供了更可靠的覆盖率和多跳推理基础，是软件工程RAG中更有效的检索方法。

Abstract: Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.
  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.

</details>


### [18] [APEX-SWE](https://arxiv.org/abs/2601.08806)
*Abhi Kottamasu,Akul Datta,Aakash Barthwal,Chirag Mahapatra,Ajay Arun,Adarsh Hiremath,Brendan Foody,Bertie Vidgen*

Main category: cs.SE

TL;DR: APEX-SWE是一个评估前沿AI模型能否执行具有经济价值的软件工程工作的基准，包含集成任务和可观测性任务两种新任务类型，Gemini 3 Pro表现最佳（25% Pass@1）。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注狭窄、定义明确的任务，无法反映真实世界的软件工程工作。需要创建能够评估AI模型在实际软件工程环境中执行经济价值工作的基准。

Method: 提出APEX-SWE基准，包含两种任务类型：1) 集成任务（100个），要求跨异构云原语、业务应用和基础设施即代码服务构建端到端系统；2) 可观测性任务（100个），要求使用日志、仪表板等遥测信号以及非结构化上下文调试生产故障。评估了8个前沿模型。

Result: Gemini 3 Pro（Thinking = High）表现最佳，Pass@1得分为25%。分析表明，强性能主要由认知推理能力驱动，即区分假设与已验证事实的能力，结合在行动前解决不确定性的主动性。

Conclusion: APEX-SWE填补了现有AI评估在真实软件工程工作评估方面的空白，开源了评估框架和开发集（50个任务），认知推理能力是AI模型在复杂软件工程任务中成功的关键因素。

Abstract: We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).

</details>
