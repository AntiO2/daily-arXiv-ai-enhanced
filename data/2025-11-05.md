<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 13]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.DB](#cs.DB) [Total: 8]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Detecting Vulnerabilities from Issue Reports for Internet-of-Things](https://arxiv.org/abs/2511.01941)
*Sogol Masoumzadeh*

Main category: cs.SE

TL;DR: 该研究首次探索使用机器学习和大型语言模型检测物联网软件中的漏洞问题报告，提出了两种方法并在21个Eclipse物联网项目中验证性能。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中漏洞问题报告的及时识别至关重要，但目前机器学习和大语言模型在物联网领域的应用尚未探索，而物联网系统的分析速度比非物联网系统更慢。

Method: 提出了两种方法：(1) 结合机器学习、大语言模型和自然语言处理技术检测21个Eclipse物联网项目的漏洞指示问题；(2) 在11,000个GitHub问题上微调预训练的BERT掩码语言模型用于漏洞分类。

Result: 基于BERT NLP特征训练的SVM模型表现最佳，AUC达到0.65；微调的BERT模型准确率为0.26，表明训练时暴露所有数据的重要性。

Conclusion: 该研究为从物联网问题报告中准确检测漏洞奠定了基础，实现了与非物联网系统类似的检测能力。

Abstract: Timely identification of issue reports reflecting software vulnerabilities is
crucial, particularly for Internet-of-Things (IoT) where analysis is slower
than non-IoT systems. While Machine Learning (ML) and Large Language Models
(LLMs) detect vulnerability-indicating issues in non-IoT systems, their IoT use
remains unexplored. We are the first to tackle this problem by proposing two
approaches: (1) combining ML and LLMs with Natural Language Processing (NLP)
techniques to detect vulnerability-indicating issues of 21 Eclipse IoT projects
and (2) fine-tuning a pre-trained BERT Masked Language Model (MLM) on 11,000
GitHub issues for classifying \vul. Our best performance belongs to a Support
Vector Machine (SVM) trained on BERT NLP features, achieving an Area Under the
receiver operator characteristic Curve (AUC) of 0.65. The fine-tuned BERT
achieves 0.26 accuracy, emphasizing the importance of exposing all data during
training. Our contributions set the stage for accurately detecting IoT
vulnerabilities from issue reports, similar to non-IoT systems.

</details>


### [2] [Metamorphic Testing of Large Language Models for Natural Language Processing](https://arxiv.org/abs/2511.02108)
*Steven Cho,Stefano Ruberto,Valerio Terragni*

Main category: cs.SE

TL;DR: 本文对LLMs进行变形测试的全面研究，收集了191个NLP任务的变形关系，实施了36个代表性关系进行实验，运行约56万次测试，揭示了变形测试在LLMs中的能力、机会和局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然LLMs在NLP任务中表现优异，但经常产生错误结果。自动识别这些错误行为对提高LLMs有效性很有用，但缺乏标注数据集。变形测试可以缓解这个oracle问题。

Method: 进行文献综述收集191个NLP任务的变形关系，实现36个代表性关系，使用三个流行LLMs运行约560,000次变形测试。

Result: 实验结果揭示了变形测试在LLMs中的能力和机会，同时也暴露了其局限性。

Conclusion: 变形测试是识别LLMs错误行为的有效方法，无需依赖标注数据集，但存在一定局限性。

Abstract: Using large language models (LLMs) to perform natural language processing
(NLP) tasks has become increasingly pervasive in recent times. The versatile
nature of LLMs makes them applicable to a wide range of such tasks. While the
performance of recent LLMs is generally outstanding, several studies have shown
that they can often produce incorrect results. Automatically identifying these
faulty behaviors is extremely useful for improving the effectiveness of LLMs.
One obstacle to this is the limited availability of labeled datasets, which
necessitates an oracle to determine the correctness of LLM behaviors.
Metamorphic testing (MT) is a popular testing approach that alleviates this
oracle problem. At the core of MT are metamorphic relations (MRs), which define
relationships between the outputs of related inputs. MT can expose faulty
behaviors without the need for explicit oracles (e.g., labeled datasets). This
paper presents the most comprehensive study of MT for LLMs to date. We
conducted a literature review and collected 191 MRs for NLP tasks. We
implemented a representative subset (36 MRs) to conduct a series of experiments
with three popular LLMs, running approximately 560,000 metamorphic tests. The
results shed light on the capabilities and opportunities of MT for LLMs, as
well as its limitations.

</details>


### [3] [Open the Oyster: Empirical Evaluation and Improvement of Code Reasoning Confidence in LLMs](https://arxiv.org/abs/2511.02197)
*Shufan Wang,Xing Hu,Junkai Chen,Zhiyuan Pan,Xin Xia*

Main category: cs.SE

TL;DR: 本文提出了一个针对代码推理任务的LLM置信度分析与增强框架，通过实证研究发现DeepSeek-Reasoner在置信度可靠性方面表现最佳，结合重新评估提示策略和Platt Scaling的混合策略能显著提升模型置信度可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在代码智能领域的广泛应用，其输出在代码推理任务中的可靠性和可控性受到关注，置信度估计成为评估这些特性的有效方法。

Method: 提出了置信度分析与增强框架，对主流LLM在不同任务中的置信度可靠性进行实证研究，评估了提示策略优化和数学校准（如Platt Scaling）等技术的有效性。

Result: DeepSeek-Reasoner在各项任务中表现最佳，在ECE、Brier Score和Performance Score指标上分别比其他模型高出0.680、0.636和13.652。混合策略在上述三个指标上比原始性能提升了0.541、0.628和15.084。

Conclusion: 具有推理能力的模型展现出更优的置信度可靠性，混合策略在增强各种模型置信度可靠性方面最为有效。当前LLM在复杂推理任务中的置信度仍有较大改进空间。

Abstract: With the widespread application of large language models (LLMs) in the field
of code intelligence, increasing attention has been paid to the reliability and
controllability of their outputs in code reasoning tasks. Confidence estimation
serves as an effective and convenient approach for evaluating these aspects.
This paper proposes a confidence analysis and enhancement framework for LLMs
tailored to code reasoning tasks. We conduct a comprehensive empirical study on
the confidence reliability of mainstream LLMs across different tasks, and
further evaluate the effectiveness of techniques such as prompt strategy
optimisation and mathematical calibration (e.g., Platt Scaling) in improving
confidence reliability. Our results show that DeepSeek-Reasoner achieves the
best performance across various tasks, outperforming other models by up to
$0.680$, $0.636$, and $13.652$ in terms of ECE, Brier Score, and Performance
Score, respectively. The hybrid strategy combining the reassess prompt strategy
and Platt Scaling achieves improvements of up to $0.541$, $0.628$, and $15.084$
over the original performance in the aforementioned three metrics. These
results indicate that models with reasoning capabilities demonstrate superior
confidence reliability, and that the hybrid strategy is the most effective in
enhancing the confidence reliability of various models. Meanwhile, we elucidate
the impact of different task complexities, model scales, and strategies on
confidence performance, and highlight that the confidence of current LLMs in
complex reasoning tasks still has considerable room for improvement. This study
not only provides a research foundation and technical reference for the
application of confidence in LLM-assisted software engineering, but also points
the way for future optimisation and engineering deployment of confidence
mechanisms.

</details>


### [4] [LLMs as Judges: Toward The Automatic Review of GSN-compliant Assurance Cases](https://arxiv.org/abs/2511.02203)
*Gerhard Yu,Mithila Sivakumar,Alvine B. Belle,Soude Ghari,Song Wang,Timothy C. Lethbridge*

Main category: cs.SE

TL;DR: 提出了一种基于LLM-as-a-judge范式的保障案例自动审查方法，通过谓词规则形式化审查标准，实验表明DeepSeek-R1和GPT-4.1表现最佳，但仍需人工完善。


<details>
  <summary>Details</summary>
Motivation: 保障案例对关键系统至关重要，但传统人工审查过程冗长易错，需要利用生成式AI和LLM提高效率、一致性和准确性。

Method: 采用LLM-as-a-judge范式，提出新的基于谓词的规则来形式化保障案例审查标准，并针对审查任务定制LLM提示。

Result: 在多个先进LLM上的实验显示，DeepSeek-R1和GPT-4.1表现最优，DeepSeek-R1最终超越GPT-4.1，但LLM审查仍需人工完善。

Conclusion: LLM能够有效支持保障案例审查，DeepSeek-R1表现最佳，但完全自动化审查仍需人类专家的参与和细化。

Abstract: Assurance cases allow verifying the correct implementation of certain
non-functional requirements of mission-critical systems, including their
safety, security, and reliability. They can be used in the specification of
autonomous driving, avionics, air traffic control, and similar systems. They
aim to reduce risks of harm of all kinds including human mortality,
environmental damage, and financial loss. However, assurance cases often tend
to be organized as extensive documents spanning hundreds of pages, making their
creation, review, and maintenance error-prone, time-consuming, and tedious.
Therefore, there is a growing need to leverage (semi-)automated techniques,
such as those powered by generative AI and large language models (LLMs), to
enhance efficiency, consistency, and accuracy across the entire assurance-case
lifecycle. In this paper, we focus on assurance case review, a critical task
that ensures the quality of assurance cases and therefore fosters their
acceptance by regulatory authorities. We propose a novel approach that
leverages the \textit{LLM-as-a-judge} paradigm to automate the review process.
Specifically, we propose new predicate-based rules that formalize
well-established assurance case review criteria, allowing us to craft LLM
prompts tailored to the review task. Our experiments on several
state-of-the-art LLMs (GPT-4o, GPT-4.1, DeepSeek-R1, and Gemini 2.0 Flash) show
that, while most LLMs yield relatively good review capabilities, DeepSeek-R1
and GPT-4.1 demonstrate superior performance, with DeepSeek-R1 ultimately
outperforming GPT-4.1. However, our experimental results also suggest that
human reviewers are still needed to refine the reviews LLMs yield.

</details>


### [5] [SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks](https://arxiv.org/abs/2511.02352)
*Sanket Mhatre,Yasharth Bajpai,Sumit Gulwani,Emerson Murphy-Hill,Gustavo Soares*

Main category: cs.SE

TL;DR: SWE-Sharp-Bench是首个针对C#语言的软件工程基准测试，包含150个实例，填补了C#在AI编码代理评估中的空白。


<details>
  <summary>Details</summary>
Motivation: C#作为排名第5的企业级编程语言，在现有的AI编码代理基准测试（如SWE-Bench）中缺失，需要专门的基准来评估C#上的AI编码能力。

Method: 构建了包含150个实例的SWE-Sharp-Bench基准测试，涵盖17个代码仓库，并开源了整个构建流程。

Result: 跨语言评估显示显著性能差距：Python任务解决率为70%，而C#任务仅为40%。

Conclusion: C#在AI编码代理任务上的表现明显落后于Python，需要更多针对C#的优化和研究。

Abstract: AI coding agents have shown great progress on Python software engineering
benchmarks like SWE-Bench, and for other languages like Java and C in
benchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language
ranking #5 in the TIOBE index -- remains absent from such benchmarks. We
introduce SWE-Sharp-Bench, a reproducible software engineering benchmark for
C\# featuring 150 instances from 17 repositories. Evaluating identical
model-agent configurations across languages reveals a significant performance
gap: while 70% of Python tasks in SWE-Bench Verified are solved, $only 40% of
our C\# tasks are resolved. We open-source SWE-Sharp-Bench and our entire
curation pipeline.

</details>


### [6] [EvoDev: An Iterative Feature-Driven Framework for End-to-End Software Development with LLM-based Agents](https://arxiv.org/abs/2511.02399)
*Junwei Liu,Chen Xu,Chong Wang,Tong Bai,Weitong Chen,Kaseng Wong,Yiling Lou,Xin Peng*

Main category: cs.SE

TL;DR: EvoDev是一个受特性驱动开发启发的迭代软件开发框架，通过将用户需求分解为特性并构建特性映射图来显式建模特性间依赖关系，在Android开发任务中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用线性的瀑布式流程，过度简化了真实世界开发的迭代性质，难以处理复杂的大规模项目。

Method: 将用户需求分解为特性集，构建有向无环图形式的特性映射图，每个节点维护业务逻辑、设计和代码等多级信息，并沿依赖关系传播上下文。

Result: 在Android开发任务中，EvoDev比最佳基线Claude Code性能提升56.8%，在不同基础LLM上单智能体性能提升16.0%-76.6%。

Conclusion: 依赖建模、上下文传播和工作流感知的智能体设计对复杂软件项目至关重要，为设计迭代式LLM驱动开发框架提供了实用见解。

Abstract: Recent advances in large language model agents offer the promise of
automating end-to-end software development from natural language requirements.
However, existing approaches largely adopt linear, waterfall-style pipelines,
which oversimplify the iterative nature of real-world development and struggle
with complex, large-scale projects. To address these limitations, we propose
EvoDev, an iterative software development framework inspired by feature-driven
development. EvoDev decomposes user requirements into a set of user-valued
features and constructs a Feature Map, a directed acyclic graph that explicitly
models dependencies between features. Each node in the feature map maintains
multi-level information, including business logic, design, and code, which is
propagated along dependencies to provide context for subsequent development
iterations. We evaluate EvoDev on challenging Android development tasks and
show that it outperforms the best-performing baseline, Claude Code, by a
substantial margin of 56.8%, while improving single-agent performance by
16.0%-76.6% across different base LLMs, highlighting the importance of
dependency modeling, context propagation, and workflow-aware agent design for
complex software projects. Our work summarizes practical insights for designing
iterative, LLM-driven development frameworks and informs future training of
base LLMs to better support iterative software development.

</details>


### [7] [Who's Who? LLM-assisted Software Traceability with Architecture Entity Recognition](https://arxiv.org/abs/2511.02434)
*Dominik Fuchß,Haoyu Liu,Sophie Corallo,Tobias Hey,Jan Keim,Johannes von Geisau,Anne Koziolek*

Main category: cs.SE

TL;DR: 本文提出了两种基于LLM的方法来自动识别软件架构相关实体并建立架构文档与源代码之间的可追溯性链接，无需手动创建软件架构模型。


<details>
  <summary>Details</summary>
Motivation: 软件架构文档与源代码之间的可追溯性链接恢复需要识别架构相关实体，但手动创建软件架构模型耗时费力。LLM为自动提取架构实体提供了新能力。

Method: 提出两种方法：ExArch从架构文档和源代码中提取组件名称作为简单架构模型；ArTEMiS识别文档中的架构实体并与架构模型实体进行匹配。

Result: ExArch达到与需要手动架构模型的TransArC相当的性能（F1: 0.86 vs 0.87）；ArTEMiS与传统启发式方法SWATTR性能相当（F1: 0.81），且与TransArC集成时可替代SWATTR。两种方法组合优于无需手动架构模型的最佳基线ArDoCode。

Conclusion: LLM能有效识别文本工件中的架构实体，实现自动化架构模型生成和可追溯性链接恢复，使架构-代码可追溯性更加实用和易用。

Abstract: Identifying architecturally relevant entities in textual artifacts is crucial
for Traceability Link Recovery (TLR) between Software Architecture
Documentation (SAD) and source code. While Software Architecture Models (SAMs)
can bridge the semantic gap between these artifacts, their manual creation is
time-consuming. Large Language Models (LLMs) offer new capabilities for
extracting architectural entities from SAD and source code to construct SAMs
automatically or establish direct trace links. This paper presents two
LLM-based approaches: ExArch extracts component names as simple SAMs from SAD
and source code to eliminate the need for manual SAM creation, while ArTEMiS
identifies architectural entities in documentation and matches them with
(manually or automatically generated) SAM entities. Our evaluation compares
against state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC
achieves strong performance (F1: 0.87) but requires manually created SAMs;
ExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS
is on par with the traditional heuristic-based SWATTR (F1: 0.81) and can
successfully replace it when integrated with TransArC. The combination of
ArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.
Our results demonstrate that LLMs can effectively identify architectural
entities in textual artifacts, enabling automated SAM generation and TLR,
making architecture-code traceability more practical and accessible.

</details>


### [8] [When Continuous Delivery Is Not an Option: Practical Paths to Continuous Engineering in Complex Organizations](https://arxiv.org/abs/2511.02445)
*Eriks Klotins,Magnus Ahlgren,Nicolas Martin Vivaldi,Even-Andre Karlsson*

Main category: cs.SE

TL;DR: 研究四个工业案例，分析约束条件下持续软件工程（CSE）的采用情况，提出更新的就绪度模型来指导实践。


<details>
  <summary>Details</summary>
Motivation: 持续软件工程承诺提升效率、质量和响应能力，但复杂产品、遗留系统、组织惯性和监管要求限制了其全面采用。

Method: 应用并扩展CSE行业就绪度模型，通过专家访谈和叙事综合评估四个工业案例的采用现状和潜力。

Result: 识别了组织准备度、跨组织依赖性和客户对持续交付需求有限等共同驱动因素和障碍，提出了更新的就绪度模型。

Conclusion: 虽然全面端到端CSE采用可能不可行，但有意义的内部改进仍然可能且有益，为组织在约束条件下进行CSE转型提供实证指导。

Abstract: Purpose: Continuous Software Engineering (CSE) promises improved efficiency,
quality, and responsiveness in software-intensive organizations. However, fully
adopting CSE is often constrained by complex products, legacy systems,
organizational inertia, and regulatory requirements. In this paper, we examine
four industrial cases from the automation, automotive, retail, and chemical
sectors to explore how such constraints shape CSE adoption in practice.
Methods: We apply and extend a previously proposed CSE Industry Readiness Model
to assess the current and potential levels of adoption in each case. Through
expert interviews and narrative synthesis, we identify common driving forces
and adoption barriers, including organizational preparedness,
cross-organizational dependencies, and limited customer demand for continuous
delivery. Results: Based on our findings, we propose an updated readiness model
that introduces additional levels of internal and external feedback,
distinguishes market- and organization-facing constraints, and better guides
practitioners in setting realistic CSE adoption goals. Conclusions: Our results
highlight that while full end-to-end CSE adoption may not always be feasible,
meaningful internal improvements are still possible and beneficial. This study
provides empirically grounded guidance for organizations navigating partial or
constrained CSE transformations.

</details>


### [9] [Lost in Code Generation: Reimagining the Role of Software Models in AI-driven Software Engineering](https://arxiv.org/abs/2511.02475)
*Jürgen Cito,Dominik Bork*

Main category: cs.SE

TL;DR: 生成式AI降低了软件开发门槛，但也导致原型与工程化软件界限模糊，产生脆弱系统。需要重新思考软件模型的作用，将其作为AI生成代码与人类意图之间的中介。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使"氛围编程"成为可能，但导致软件原型与工程化系统界限模糊，缺乏健壮性、安全性和可维护性。需要重新思考软件模型在AI驱动开发中的角色。

Method: 提出将软件模型作为事后从AI生成代码中恢复的工具，而不是前期蓝图。模型作为人类意图、AI生成和系统长期演进之间的中介。

Result: 通过事后模型恢复，可以恢复对系统的理解、暴露风险并指导改进，为可持续的AI驱动软件工程提供路径。

Conclusion: 软件模型应重新定位为AI生成代码与人类意图之间的中介角色，为可持续的AI驱动软件工程提供支持。

Abstract: Generative AI enables rapid ``vibe coding," where natural language prompts
yield working software systems. While this lowers barriers to software
creation, it also collapses the boundary between prototypes and engineered
software, leading to fragile systems that lack robustness, security, and
maintainability. We argue that this shift motivates a reimagining of software
models. Rather than serving only as upfront blueprints, models can be recovered
post-hoc from AI-generated code to restore comprehension, expose risks, and
guide refinement. In this role, models serve as mediators between human intent,
AI generation, and long-term system evolution, providing a path toward
sustainable AI-driven software engineering.

</details>


### [10] [ReleaseEval: A Benchmark for Evaluating Language Models in Automated Release Note Generation](https://arxiv.org/abs/2511.02713)
*Qianru Meng,Zhaochun Ren,Joost Visser*

Main category: cs.SE

TL;DR: ReleaseEval是一个用于自动发布说明生成的可复现开源基准，包含94,987条发布说明，支持三种不同输入粒度的任务设置，评估显示大语言模型在利用结构化信息方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决自动发布说明生成中数据集限制（缺乏明确许可、可复现性差）和任务设计不完整（主要依赖提交消息而忽略细粒度上下文）的问题。

Method: 构建ReleaseEval基准，包含94,987条发布说明，支持三种任务设置：commit2sum（基于提交消息）、tree2sum（包含提交树结构）、diff2sum（利用代码差异）。

Result: 大语言模型在所有任务中都优于传统基线方法，在tree2sum任务中表现尤为突出，但在diff2sum任务中仍有困难。

Conclusion: 大语言模型擅长利用结构化信息，但在从长代码差异中抽象信息方面仍面临挑战。

Abstract: Automated release note generation addresses the challenge of documenting
frequent software updates, where manual efforts are time-consuming and prone to
human error. Although recent advances in language models further enhance this
process, progress remains hindered by dataset limitations, including the lack
of explicit licensing and limited reproducibility, and incomplete task design
that relies mainly on commit messages for summarization while overlooking
fine-grained contexts such as commit hierarchies and code changes. To fill this
gap, we introduce ReleaseEval, a reproducible and openly licensed benchmark
designed to systematically evaluate language models for automated release note
generation. ReleaseEval comprises 94,987 release notes from 3,369 repositories
across 6 programming languages, and supports three task settings with three
levels of input granularity: (1) commit2sum, which generates release notes from
commit messages; (2) tree2sum, which incorporates commit tree structures; and
(3) diff2sum, which leverages fine-grained code diffs. Both automated and human
evaluations show that large language models consistently outperform traditional
baselines across all tasks, achieving substantial gains on tree2sum, while
still struggling on diff2sum. These findings highlight LLMs' proficiency in
leveraging structured information while revealing challenges in abstracting
from long code diffs.

</details>


### [11] [Investigating the Experience of Autistic Individuals in Software Engineering](https://arxiv.org/abs/2511.02736)
*Madalena Sasportes,Grischa Liebel,Miguel Goulão*

Main category: cs.SE

TL;DR: 本研究分析了自闭症软件工程师在软件工程活动中的经验，特别关注他们的优势，包括逻辑思维、细节关注和超专注编程能力。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）常导致就业困难和心理健康问题，但现有研究主要关注挑战而非优势。本研究旨在探索自闭症个体在软件工程中的优势，如逻辑推理能力和细节关注能力。

Method: 结合社会技术扎根理论，通过半结构化访谈16名自闭症软件工程师和49名受访者（包括5名自闭症参与者）的调查。

Result: 研究发现自闭症软件工程师在逻辑思维、细节关注和编程超专注方面表现出色，喜欢学习新编程语言，偏好书面沟通和远程工作，且对AI系统互动感到舒适。

Conclusion: 研究结果扩展了现有工作，为自闭症软件工程师的优势提供了进一步证据。

Abstract: Context: Autism spectrum disorder (ASD) leads to various issues in the
everyday life of autistic individuals, often resulting in unemployment and
mental health problems. To improve the inclusion of autistic adults, existing
studies have highlighted the strengths these individuals possess in comparison
to non-autistic individuals, e.g., high attention to detail or excellent
logical reasoning skills. If fostered, these strengths could be valuable in
software engineering activities, such for identifying specific kinds of bugs in
code. However, existing work in SE has primarily studied the challenges of
autistic individuals and possible accommodations, with little attention their
strengths. Objective: Our goal is to analyse the experiences of autistic
individuals in software engineering activities, such as code reviews, with a
particular emphasis on strengths. Methods: This study combines Social-Technical
Grounded Theory through semi-structured interviews with 16 autistic software
engineers and a survey with 49 respondents, including 5 autistic participants.
We compare the emerging themes with the theory by Gama et al. on the Effect of
Neurodivergent Cognitive Dysfunctions in Software Engineering Performance.
Results: Our results suggest that autistic software engineers are often skilled
in logical thinking, attention to detail, and hyperfocus in programming; and
they enjoy learning new programming languages and programming-related
technologies. Confirming previous work, they tend to prefer written
communication and remote work. Finally, we report a high comfort level in
interacting with AI-based systems. Conclusions: Our findings extend existing
work by providing further evidence on the strengths of autistic software
engineers.

</details>


### [12] [Formalizing Regression Testing for Agile and Continuous Integration Environments](https://arxiv.org/abs/2511.02810)
*Suddhasvatta Das,Kevin Gary*

Main category: cs.SE

TL;DR: 本文为敏捷开发中的持续回归测试建立了形式化模型，将连续构建视为时间有序链，并定义了回归测试窗口概念，验证了该模型能准确表示现有敏捷回归测试算法。


<details>
  <summary>Details</summary>
Motivation: 现代敏捷开发实践产生连续的软件版本流，需要持续回归测试，而传统回归测试理论假设在交付或维护阶段进行一次测试，无法满足当前需求。

Method: 将连续构建形式化为时间有序链，每个构建包含程序、需求和测试，定义回归测试窗口来捕获有限的测试时间预算，并通过构建元组操作直接表示现有敏捷回归测试算法。

Result: 该形式化模型能够准确表示两个最先进的敏捷回归测试算法，无需辅助假设，并证明了形式化的正确性和完备性。

Conclusion: 提出的形式化模型为敏捷环境下的持续回归测试提供了理论基础，当时间限制设为无穷大且链简化为两个构建时，模型退化为传统的重测全部方法，保持了与经典理论的语义一致性。

Abstract: Software developed using modern agile practices delivers a stream of software
versions that require continuous regression testing rather than testing once
close to the delivery or maintenance phase, as assumed by classical
regression-testing theory. In this work, we formalize the phenomenon of
continuous or near-continuous regression testing using successive builds as a
time-ordered chain, where each build contains the program, requirements, and
the accompanying tests. We also formalize the regression test window between
any two builds, which captures the limited time budget available for regression
testing. As the time limit is set to infinity and the chain is closed to two
builds, the model degenerates to retest-all, thereby preserving semantics for
the classical two-version case. The formalization is validated by directly
representing two state-of-the-art agile regression testing algorithms in terms
of build-tuple operations without requiring auxiliary assumptions, followed by
proof of the soundness and completeness of our formalization.

</details>


### [13] [From Code Changes to Quality Gains: An Empirical Study in Python ML Systems with PyQu](https://arxiv.org/abs/2511.02827)
*Mohamed Almukhtar,Anwar Ghammam,Marouane Kessentini,Hua Ming*

Main category: cs.SE

TL;DR: 该研究开发了PyQu工具，通过分析3400个开源Python机器学习项目的370万次提交，识别出61种能直接提升软件质量的代码变更，并将其分为13个类别。


<details>
  <summary>Details</summary>
Motivation: 在生成式AI代码生成和Python机器学习系统普及的背景下，缺乏理解代码变更如何影响系统质量的工具和方法，现有研究未能建立代码变更与ML系统质量之间的明确关系。

Method: 对3400个开源Python机器学习项目进行大规模实证研究，涵盖370万次提交和2.7万亿行代码，开发PyQu工具利用低级软件指标识别质量提升提交，并进行主题分析。

Result: PyQu工具在识别质量提升提交时平均准确率、精确率和召回率达到0.84，平均F1分数0.85；发现了61种能直接提升软件质量的代码变更，其中41%是现有Python变更检测工具未能识别的新发现。

Conclusion: 该研究为研究人员、从业者、教育工作者和工具开发者提供了重要基础，推动了Python机器学习软件自动化质量评估和最佳实践的探索。

Abstract: In an era shaped by Generative Artificial Intelligence for code generation
and the rising adoption of Python-based Machine Learning systems (MLS),
software quality has emerged as a major concern. As these systems grow in
complexity and importance, a key obstacle lies in understanding exactly how
specific code changes affect overall quality-a shortfall aggravated by the lack
of quality assessment tools and a clear mapping between ML systems code changes
and their quality effects. Although prior work has explored code changes in
MLS, it mostly stops at what the changes are, leaving a gap in our knowledge of
the relationship between code changes and the MLS quality. To address this gap,
we conducted a large-scale empirical study of 3,340 open-source Python ML
projects, encompassing more than 3.7 million commits and 2.7 trillion lines of
code. We introduce PyQu, a novel tool that leverages low level software metrics
to identify quality-enhancing commits with an average accuracy, precision, and
recall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic
analysis, we identified 61 code changes, each demonstrating a direct impact on
enhancing software quality, and we classified them into 13 categories based on
contextual characteristics. 41% of the changes are newly discovered by our
study and have not been identified by state-of-the-art Python changes detection
tools. Our work offers a vital foundation for researchers, practitioners,
educators, and tool developers, advancing the quest for automated quality
assessment and best practices in Python-based ML software.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 这篇综述分析了已部署和正在使用的工作负载调度器解决方案，并提出了一个基于架构和设计的层次化分类体系。特别关注Google的Borg系统。


<details>
  <summary>Details</summary>
Motivation: 现有的分类体系不够完善，本文重点关注影响调度器吞吐量和可扩展性的关键设计因素，以及改进架构的渐进式优化。

Method: 通过分析已部署的工作负载调度器，建立基于架构和设计的层次化分类法，特别深入研究了Google Borg系统。

Result: 提出了一个系统化的分类框架，识别了影响调度器性能的关键设计因素，并展示了架构优化的演进路径。

Conclusion: 该综述为理解工作负载调度器的设计原理和性能优化提供了有价值的分类框架，特别突出了Borg系统在架构设计方面的先进性。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [15] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: 本概念设计报告规划了德国达姆施塔特FAIR研究中心从2028年"首批科学+"阶段到模块化启动版本的计算基础设施，旨在创建联邦化、集中编排的基础设施以满足多样化研究需求。


<details>
  <summary>Details</summary>
Motivation: 为FAIR各研究组提供计算需求支持，建立能够应对未来数据挑战的可扩展、灵活的计算和存储基础设施。

Method: 制定计算和存储基础设施政策，设计包含开放数据、软件和服务政策的FAIR计算模型架构。

Result: 提出了从2028年开始的阶段性实施计划，涵盖"首批科学+"阶段到模块化启动版本的计算基础设施蓝图。

Conclusion: 目标是创建联邦化、集中编排的基础设施，为FAIR的多样化研究路线提供足够可扩展性和灵活性，以应对未来的数据挑战。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [16] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 云计算已成为AI/ML创新的主导力量，但当前的资源成本模型不适合科学计算需求，可能导致科学工作负载在不适合的环境中运行。


<details>
  <summary>Details</summary>
Motivation: 分析云计算成本模型从科学计算驱动到AI/ML主导的转变，探讨如何继续支持科学发现。

Method: 通过历史回顾和趋势分析，讨论云计算成本模型的演变及其对科学计算的影响。

Result: 发现AI/ML需求主导的云计算创新导致资源模型不适合科学计算，可能造成科学工作负载运行环境不匹配。

Conclusion: 需要重新设计云计算成本模型以更好地支持科学发现，避免科学计算被边缘化。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [17] [SPHERE: Spherical partitioning for large-scale routing optimization](https://arxiv.org/abs/2511.01863)
*Robert Fabian Lindermann,Paul-Niklas Ken Kandora,Simon Caspar Zeller,Adrian Asmund Fessler,Steffen Rebennack*

Main category: cs.DC

TL;DR: SPHERE是一种源-目标感知的启发式算法，通过识别s-t重叠区域来分割最短路径问题，提高大规模图的路由效率。


<details>
  <summary>Details</summary>
Motivation: 在大规模加权无向图中，扩展搜索边界会增加精确求解器的时间和内存成本，需要更高效的路径规划方法。

Method: 识别源节点s和目标节点t之间的重叠区域，选择锚点a将问题分割为s→a和a→t两个子问题，递归处理大型子图，无需边界修复。

Result: 在超过百万节点和边的大型网络上，SPHERE比基于Louvain的路由和METIS流水线方法具有更快的运行时间和更小的最优性差距，且运行时间优于Dijkstra算法。

Conclusion: SPHERE是一种独立于下游求解器的有效方法，能够在大规模图中实现高效的最短路径路由，并支持子问题的并行处理。

Abstract: We study shortest-path routing in large weighted, undirected graphs, where
expanding search frontiers raise time and memory costs for exact solvers. We
propose \emph{SPHERE}, a source-target-aware heuristic that identifies an
$s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count.
Selecting an anchor $a$ in this overlap partitions the task into two
subproblems with unchanged problem-topology, $s\to a$ and $a\to t$; if either
remains large, the procedure recurses on its induced subgraph. Because the cut
lies inside the overlap, concatenating the resulting subpaths yields a valid
$s\to t$ route without boundary repair. SPHERE is independent of the downstream
solver (e.g., Dijkstra) and exposes parallelism across subproblems. On large
networks, it achieves faster runtimes and smaller optimality gaps than
Louvain-based routing and a METIS-based pipeline, even on graphs with more than
a million nodes and edges, while also outperforming Dijkstra in runtime.

</details>


### [18] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: EdgeReasoning是一个关于在边缘GPU上部署推理大语言模型的综合性研究，系统量化了不同LLM架构和模型大小的延迟-精度权衡，为边缘推理LLM的最优部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 边缘部署LLM用于推理任务面临严格的延迟约束和有限的计算资源挑战，开发者需要在推理与非推理架构选择、模型大小、token预算分配和测试时扩展策略等多个设计因素之间取得平衡，但目前缺乏关于这些变量最优组合的指导。

Method: 系统评估基于提示和模型微调的技术以减少推理token长度同时保持性能质量；分析不同并行度的测试时扩展方法以在严格延迟预算下最大化精度；通过分析映射可实现的精度-延迟配置的帕累托前沿。

Result: 研究量化了各种LLM架构和模型大小的延迟-精度权衡，评估了减少推理token长度和测试时扩展方法的有效性。

Conclusion: EdgeReasoning为推理LLM在边缘GPU上的最优部署提供了系统性指导，通过映射精度-延迟配置的帕累托前沿，帮助开发者在严格资源约束下做出最优设计决策。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [19] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 本文开发了多核处理器结构分析和效率指标评估模型，包括可靠性、容错性、生存性和灵活性，使用逻辑概率方法分析多功能核心的多核处理器性能。


<details>
  <summary>Details</summary>
Motivation: 研究多核处理器中多功能核心的结构特性和效率指标，以评估其在不同状态下的性能和可靠性。

Method: 采用逻辑概率方法，开发了可靠性、容错性评估模型，最短路径、灵活性和性能条件模型，以及考虑所有可能性能状态的寿命评估模型。

Result: 对双核和四核处理器进行了结构分析，展示了多核处理器效率指标提升的趋势。

Conclusion: 通过逻辑概率模型有效评估了多核处理器的结构效率和性能指标，为多核处理器设计提供了理论支持。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [20] [Learned Cost Model for Placement on Reconfigurable Dataflow Hardware](https://arxiv.org/abs/2511.01872)
*Etash Guha,Tianxiao Jiang,Andrew Deng,Jian Zhang,Muthu Annamalai*

Main category: cs.DC

TL;DR: 提出了一种学习方法来预测数据流图在可重构系统上的吞吐量，比传统手工分析模型准确度提高31%-52%，并能生成5.6%更快的编译图


<details>
  <summary>Details</summary>
Motivation: 将ML模型的数据流图映射到可重构系统很困难，不同映射具有不同吞吐量且资源消耗不同。完全测量吞吐量成本高昂，而现有手工分析模型依赖代理特征或直觉，存在误差

Method: 使用学习方法来预测映射的吞吐量，该方法在移除性能注释后仍保持准确度

Result: 该方法在各种图上的吞吐量预测准确度比现有方法提高31%-52%，使用该方法编译的图速度提高5.6%

Conclusion: 学习方法是预测数据流图在可重构系统上吞吐量的有效方案，显著优于传统手工分析模型

Abstract: Mapping a dataflow-graph of an ML model onto a reconfigurable system is
difficult, as different mappings have different throughputs and consume
resource constraints differently. To solve this, a model to evaluate the
throughput of mappings is necessary as measuring throughput completely is
expensive. Many use a hand-designed analytical model, relying on proxy features
or intuition, introducing error. We provide a Learned Approach that predicts
throughput 31%-52% more accurately over a variety of graphs. In addition, our
approach shows no accuracy degradation after removing performance annotations.
We show that using this approach results in 5.6% faster compiled graphs.

</details>


### [21] [HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing](https://arxiv.org/abs/2511.01881)
*Zhengxin Fang,Hui Ma,Gang Chen,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出了一种名为HGraphScale的新型自动扩缩容方法，通过分层图神经网络捕捉微服务依赖关系和部署方案，在容器云环境中实现高效的资源扩缩容。


<details>
  <summary>Details</summary>
Motivation: 微服务架构在容器云中部署时，复杂的微服务依赖关系和部署方案给资源扩缩容带来了额外挑战，需要更有效的自动扩缩容方法。

Method: 设计分层图神经网络来捕捉微服务依赖关系和部署方案，基于快速变化的用户请求负载做出有效的扩缩容决策。

Result: 在真实用户请求轨迹上的实验表明，HGraphScale在特定VM租赁预算下，最多能减少80.16%的平均响应时间，优于现有最先进的自动扩缩容方法。

Conclusion: HGraphScale通过分层图神经网络有效解决了微服务架构在容器云中的自动扩缩容挑战，显著提升了系统性能。

Abstract: Microservice architecture has become a dominant paradigm in application
development due to its advantages of being lightweight, flexible, and
resilient. Deploying microservice applications in the container-based cloud
enables fine-grained elastic resource allocation. Autoscaling is an effective
approach to dynamically adjust the resource provisioned to containers. However,
the intricate microservice dependencies and the deployment scheme of the
container-based cloud bring extra challenges of resource scaling. This article
proposes a novel autoscaling approach named HGraphScale. In particular,
HGraphScale captures microservice dependencies and the deployment scheme by a
newly designed hierarchical graph neural network, and makes effective scaling
actions for rapidly changing user requests workloads. Extensive experiments
based on real-world traces of user requests are conducted to evaluate the
effectiveness of HGraphScale. The experiment results show that the HGraphScale
outperforms existing state-of-the-art autoscaling approaches by reducing at
most 80.16\% of the average response time under a certain VM rental budget of
application providers.

</details>


### [22] [Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions](https://arxiv.org/abs/2511.01888)
*Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Roadrunner是一个边车shim，通过零拷贝和免序列化数据传输，显著提升WebAssembly无服务器函数的通信性能。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算中函数间数据传输需要序列化和多次内存拷贝，导致上下文切换和内存分配开销，增加了延迟和资源消耗。

Method: 使用边车shim技术，通过映射函数内存和专用虚拟数据管道，绕过序列化和反序列化过程，实现零拷贝数据传输。

Result: 实验结果显示，Roadrunner将函数间通信延迟降低44-89%，减少97%的数据传输序列化开销，吞吐量提升69倍。

Conclusion: Roadrunner通过零拷贝和免序列化数据传输，显著提升了WebAssembly无服务器函数的性能，实现了接近原生延迟的性能表现。

Abstract: Serverless computing provides infrastructure management and elastic
auto-scaling, therefore reducing operational overhead. By design serverless
functions are stateless, which means they typically leverage external remote
services to store and exchange data. Transferring data over a network typically
involves serialization and deserialization. These operations usually require
multiple data copies and transitions between user and kernel space, resulting
in overhead from context switching and memory allocation, contributing
significantly to increased latency and resource consumption. To address these
issues, we present Roadrunner, a sidecar shim that enables near-zero copy and
serialization-free data transfer between WebAssembly-based serverless
functions. Roadrunner reduces the multiple copies between user space and kernel
space by mapping the function memory and moving the data along a dedicated
virtual data hose, bypassing the costly processes of serialization and
deserialization. This approach reduces data movement overhead and context
switching, achieving near-native latency performance for WebAssembly-based
serverless functions. Our experimental results demonstrate that Roadrunner
significantly improves the inter-function communication latency from 44% up to
89%, reducing the serialization overhead in 97% of data transfer, and
increasing throughput by 69 times compared to state-of-the-art
WebAssembly-based serverless functions.

</details>


### [23] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mLR通过记忆化技术优化ADMM-FFT算法，用存储的FFT结果替代重复计算，显著提升计算性能并减少内存消耗，支持大规模层析成像重建。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT算法在层析成像重建中精度高但计算时间和内存消耗过大，限制了其在大规模问题中的应用。

Method: 利用记忆化技术存储重复出现的FFT计算结果，引入变量卸载技术节省CPU内存，并在多GPU节点间扩展ADMM-FFT算法。

Result: 成功在2Kx2Kx2K的最大规模输入问题上运行ADMM-FFT，相比原算法平均性能提升52.8%，最高达65.4%。

Conclusion: mLRA方法有效解决了ADMM-FFT的计算瓶颈，使其能够处理更大规模的层析成像重建问题，同时显著提升计算效率。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [24] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 提出GPoS协议，通过整合地理空间多样性与权益投票权，显著提升PoS区块链的地理去中心化程度，平均改善45%，同时对BFT协议性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 当前主要PoS区块链存在地理集中问题，少数地区主导共识投票权，限制了地理去中心化，影响监管韧性、鲁棒性和公平性。

Method: 提出地理空间感知权益证明(GPoS)，将地理空间多样性纳入权益投票机制，在HotStuff和CometBFT等BFT协议中实现。

Result: 实验评估显示GPoS平均提升45%的地理去中心化程度（基于特征向量中心性的基尼系数），同时对共识性能影响极小。

Conclusion: GPoS能有效改善PoS区块链的地理去中心化，且在实践中几乎不增加性能开销。

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [25] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 本文提出超越传统BSP模型的分布式GPU执行方法，通过细粒度编程模式消除"三大税收"瓶颈，在LLM分布式推理中获得10-20%的端到端延迟提升


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，传统BSP模型在分布式GPU执行中引入显著性能瓶颈，需要新的执行范式来解决效率问题

Method: 利用Iris for Triton库的内核通信原语，设计细粒度编程模式，建立直接的tile级生产者-消费者流水线，用细粒度数据流同步替代全局屏障

Result: 在关键内核（从All-Gather+矩阵乘法到复杂Flash Decode算法）上，相比BSP方法实现了10-20%的端到端延迟加速

Conclusion: 建立了一个更可编程和高效的分布式LLM工作负载执行范式，系统性地消除了三大性能瓶颈

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [26] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 提出了一种算子级自动扩缩容框架，通过细粒度资源分配优化大型生成模型的推理服务效率


<details>
  <summary>Details</summary>
Motivation: 现有解决方案将模型视为整体进行静态配置或模型级扩缩容，导致性能下降或资源利用率低下，无法适应动态推理流量

Method: 基于生成模型内部算子图结构，分析算子的计算和内存特征差异，提出算子级资源分配框架，优化扩缩容、批处理和部署策略

Result: 在生产规模trace上评估，相比现有方法减少40% GPU和35%能耗，或在固定资源下提升1.6倍吞吐量并降低5%能耗

Conclusion: 算子比模型更适合作为大型生成工作负载扩缩容的基本单元，能更有效地平衡服务等级目标和资源效率

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [27] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出了两种新颖的调度算法，用于优化LQCD模拟中相关函数计算的内存使用和数据传输效率，通过重新排序张量收缩操作来提高时间局部性。


<details>
  <summary>Details</summary>
Motivation: LQCD模拟中的相关函数计算涉及大量二进制批量张量收缩操作，每个张量可能占用数百MB内存。在GPU加速器上执行这些收缩操作时，如何优化调度以提高张量重用率和减少数据流量是一个重要挑战。

Method: 提出了两种快速调度算法，利用应用特定特征（如二进制收缩和收缩树内的局部性）来重新排序收缩操作，通过输入/中间张量重用增加时间局部性，以最小化峰值内存为目标。

Result: 调度器实现了高达2.1倍的峰值内存改进，反映为高达4.2倍的驱逐减少，高达1.8倍的数据流量减少，最终使相关函数计算时间加快高达1.9倍。

Conclusion: 所提出的调度算法成功集成到LQCD分析软件套件Redstar中，显著改善了求解时间，证明了通过优化调度策略可以有效提升LQCD模拟的计算效率。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [28] [3D Point Cloud Object Detection on Edge Devices for Split Computing](https://arxiv.org/abs/2511.02293)
*Taisuke Noguchi,Takuya Azumi*

Main category: cs.DC

TL;DR: 该研究利用分割计算技术优化自动驾驶中的3D目标检测，通过在点云处理的不同阶段分割神经网络，显著降低了边缘设备的推理时间和功耗。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的3D目标检测模型复杂度高，导致边缘设备处理时间长、功耗大，需要解决这些性能瓶颈问题。

Method: 采用分割计算技术，在点云体素化后或网络内部进行分割，将部分计算任务分配到云端，减少边缘设备负担。

Result: 在体素化后分割可使推理时间减少70.8%，边缘设备执行时间减少90.0%；在网络内部分割可使推理时间减少57.1%，边缘设备执行时间减少69.5%。

Conclusion: 分割计算能有效降低自动驾驶3D目标检测的计算负担，提高边缘设备性能，同时通过传输中间数据减少数据泄露风险。

Abstract: The field of autonomous driving technology is rapidly advancing, with deep
learning being a key component. Particularly in the field of sensing, 3D point
cloud data collected by LiDAR is utilized to run deep neural network models for
3D object detection. However, these state-of-the-art models are complex,
leading to longer processing times and increased power consumption on edge
devices. The objective of this study is to address these issues by leveraging
Split Computing, a distributed machine learning inference method. Split
Computing aims to lessen the computational burden on edge devices, thereby
reducing processing time and power consumption. Furthermore, it minimizes the
risk of data breaches by only transmitting intermediate data from the deep
neural network model. Experimental results show that splitting after
voxelization reduces the inference time by 70.8% and the edge device execution
time by 90.0%. When splitting within the network, the inference time is reduced
by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.

</details>


### [29] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: FedAttn是一个将联邦学习范式集成到自注意力机制中的分布式LLM推理框架，通过本地自注意力计算和周期性KV矩阵交换聚合，实现隐私保护、通信效率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决边缘部署LLM在协作场景中的隐私漏洞、通信开销和计算瓶颈等挑战。

Method: 将联邦学习范式集成到自注意力机制中，参与者执行本地自注意力计算，定期交换和聚合KV矩阵，基于结构对偶性将联邦优化技术移植到协作LLM推理。

Result: 理论分析了本地自注意力计算和异构令牌相关性如何影响Transformer块间的误差传播动态，揭示了响应质量与通信/计算效率之间的权衡关系。实验验证了理论分析，并展示了通过稀疏注意力和自适应KV聚合的优化机会。

Conclusion: FedAttn为实际边缘部署提供了可扩展性和效率潜力，能够同时实现隐私保护、通信效率和计算效率。

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [30] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文比较了四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）在N体模拟和结构化网格模拟中的性能表现，发现不同框架在Polaris超级计算机上存在显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 随着异构计算架构的兴起，科学计算需要跨硬件平台的高性能计算框架，以实现代码在不同硬件上的高效执行。

Method: 在Polaris超级计算机的单个节点上，使用四个NVIDIA A100 GPU，通过四种性能可移植性框架（Kokkos、OpenMP、RAJA、OCCA）运行N体模拟和结构化网格模拟，采用分布式内存方法和硬件加速。

Result: 不同框架表现出显著性能差异：OCCA在小规模验证问题上执行更快（可能由于JIT编译），但其缺乏优化的归约算法可能限制大规模模拟的可扩展性；OpenMP在结构化网格模拟中表现较差，可能由于节点间数据同步和通信效率低下。

Conclusion: 需要进一步优化以最大化各框架性能，未来工作将重点改进归约算法、数据通信、内存管理，并进行可扩展性研究和全面的统计分析来评估比较框架性能。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [31] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: EPaxos*是一个简化且正确的Egalitarian Paxos变体，通过更简单的故障恢复算法解决了原协议的复杂性和错误问题，并扩展到最优进程数范围。


<details>
  <summary>Details</summary>
Motivation: 解决Egalitarian Paxos协议的复杂性、模糊规范和严重错误问题，提供更简单、正确的无领导者状态机复制协议。

Method: 提出EPaxos*协议，使用简化的故障恢复算法，并扩展到支持n ≥ max{2e+f-1, 2f+1}的最优进程数配置。

Result: 开发了经过严格证明正确的协议变体，在保持原协议优点的同时解决了复杂性和正确性问题。

Conclusion: EPaxos*成功简化了Egalitarian Paxos，提供了正确性保证，并达到了最优的进程数配置范围。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [32] [An Experimental Comparison of Alternative Techniques for Event-Log Augmentation](https://arxiv.org/abs/2511.01896)
*Alessandro Padella,Francesco Vinci,Massimiliano de Leoni*

Main category: cs.DB

TL;DR: 本文评估了七种最先进的事件日志增强技术，发现基于随机转移系统结合资源队列建模的技术能生成更高质量的合成事件日志，且事件日志增强技术相比传统数据增强技术具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 过程挖掘依赖于大型事件日志，但现实中往往缺乏足够数据。事件日志增强技术通过生成模拟真实过程执行的额外轨迹来解决这一限制，但之前缺乏对这些技术有效性的全面比较。

Method: 在八个事件日志上评估七种最先进的增强技术，并与基于随机转移系统的基线技术进行比较，从相似性、预测信息保留、信息损失/增强和计算时间四个维度进行分析。

Result: 结果显示，基于随机转移系统结合资源队列建模的技术在不同标准下能提供更高质量的合成事件日志。事件日志增强技术相比传统数据增强技术具有显著优势，后者无法考虑过程约束。

Conclusion: 事件日志增强技术能有效解决过程挖掘中的数据不足问题，其中基于随机转移系统结合资源队列建模的方法表现最佳，为过程挖掘与机器学习结合提供了可靠的数据支持。

Abstract: Process mining analyzes and improves processes by examining transactional
data stored in event logs, which record sequences of events with timestamps.
However, the effectiveness of process mining, especially when combined with
machine or deep learning, depends on having large event logs. Event log
augmentation addresses this limitation by generating additional traces that
simulate realistic process executions while considering various perspectives
like time, control-flow, workflow, resources, and domain-specific attributes.
Although prior research has explored event-log augmentation techniques, there
has been no comprehensive comparison of their effectiveness. This paper reports
on an evaluation of seven state-of-the-art augmentation techniques across eight
event logs. The results are also compared with those obtained by a baseline
technique based on a stochastic transition system. The comparison has been
carried on analyzing four different aspects: similarity, preservation of
predictive information, information loss/enhancement, and computational times
required. Results show that, considering the different criteria, a technique
based on a stochastic transition system combined with resource queue modeling
would provide higher quality synthetic event logs. Event-log augmentation
techniques are also compared with traditional data-augmentation techniques,
showing that the former provide significant benefits, whereas the latter fail
to consider process constraints.

</details>


### [33] [Towards Defect Phase Diagrams: From Research Data Management to Automated Workflows](https://arxiv.org/abs/2511.01942)
*Khalil Rejiba,Sang-Hyeok Lee,Christina Gasper,Martina Freund,Sandra Korte-Kerzel,Ulrich Kerzel*

Main category: cs.DB

TL;DR: 该论文介绍了一个综合研究数据管理基础设施，用于构建缺陷相图，通过整合异构实验和模拟数据，实现跨研究组和地点的数据可重复使用。


<details>
  <summary>Details</summary>
Motivation: 构建缺陷相图需要系统整合异构的实验和模拟数据，但数据来源和格式的多样性带来了挑战，需要有效的研究数据管理来连接分布式研究活动。

Method: 建立综合RDM基础设施，包括：联合电子实验室笔记本和实验室信息管理系统、易用的大对象数据存储、自动元数据提取、交互式溯源图、自动化报告和分析工作流，核心是openBIS系统及其配套应用。

Result: 该集成方法减少了数据捕获和整理的摩擦，实现了可追踪和可重用的数据集，加速了跨机构的缺陷相图构建。

Conclusion: 综合RDM基础设施成功解决了异构数据集成问题，为材料设计中的缺陷相图构建提供了有效的技术支持。

Abstract: Defect phase diagrams provide a unified description of crystal defect states
for materials design and are central to the scientific objectives of the
Collaborative Research Centre (CRC) 1394. Their construction requires the
systematic integration of heterogeneous experimental and simulation data across
research groups and locations. In this setting, research data management (RDM)
is a key enabler of new scientific insight by linking distributed research
activities and making complex data reproducible and reusable.
  To address the challenge of heterogeneous data sources and formats, a
comprehensive RDM infrastructure has been established that links experiment,
data, and analysis in a seamless workflow. The system combines: (1) a joint
electronic laboratory notebook and laboratory information management system,
(2) easy-to-use large-object data storage, (3) automatic metadata extraction
from heterogeneous and proprietary file formats, (4) interactive provenance
graphs for data exploration and reuse, and (5) automated reporting and analysis
workflows. The two key technological elements are the openBIS electronic
laboratory notebook and laboratory information management system, and a newly
developed companion application that extends openBIS with large-scale data
handling, automated metadata capture, and federated access to distributed
research data.
  This integrated approach reduces friction in data capture and curation,
enabling traceable and reusable datasets that accelerate the construction of
defect phase diagrams across institutions.

</details>


### [34] [InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations](https://arxiv.org/abs/2511.02002)
*Xiangru Jian,Zhengyuan Dong,M. Tamer Özsu*

Main category: cs.DB

TL;DR: InteracSPARQL是一个交互式SPARQL查询生成和优化系统，通过自然语言解释和LLM增强，帮助非专家用户更轻松地构建和优化语义网查询。


<details>
  <summary>Details</summary>
Motivation: SPARQL查询语言语法复杂且需要理解复杂数据结构，对非专家用户具有挑战性，需要更易用的查询接口。

Method: 结合LLM和基于规则的方法，从SPARQL抽象语法树生成结构化解释，然后通过LLM进行语言优化，支持用户交互式反馈和LLM驱动的自我优化。

Result: 在标准基准测试中显著提高了查询准确性、解释清晰度和用户满意度，优于基线方法。

Conclusion: 结合基于规则方法和LLM驱动的优化能够创建更易访问和鲁棒的SPARQL接口。

Abstract: In recent years, querying semantic web data using SPARQL has remained
challenging, especially for non-expert users, due to the language's complex
syntax and the prerequisite of understanding intricate data structures. To
address these challenges, we propose InteracSPARQL, an interactive SPARQL query
generation and refinement system that leverages natural language explanations
(NLEs) to enhance user comprehension and facilitate iterative query refinement.
InteracSPARQL integrates LLMs with a rule-based approach to first produce
structured explanations directly from SPARQL abstract syntax trees (ASTs),
followed by LLM-based linguistic refinements. Users can interactively refine
queries through direct feedback or LLM-driven self-refinement, enabling the
correction of ambiguous or incorrect query components in real time. We evaluate
InteracSPARQL on standard benchmarks, demonstrating significant improvements in
query accuracy, explanation clarity, and overall user satisfaction compared to
baseline approaches. Our experiments further highlight the effectiveness of
combining rule-based methods with LLM-driven refinements to create more
accessible and robust SPARQL interfaces.

</details>


### [35] [Vortex: Hosting ML Inference and Knowledge Retrieval Services With Tight Latency and Throughput Requirements](https://arxiv.org/abs/2511.02062)
*Yuting Yang,Tiancheng Yuan,Jamal Hashim,Thiago Garrett,Jeffrey Qian,Ann Zhang,Yifan Wang,Weijia Song,Ken Birman*

Main category: cs.DB

TL;DR: Vortex是一个面向SLO（服务水平目标）的ML推理服务平台，通过优化批处理机制显著降低延迟并提高稳定性，相比TorchServe和Ray Serve在相同工作负载下能支持更高的请求率。


<details>
  <summary>Details</summary>
Motivation: 随着ML推理和知识检索服务需求的增长，特别是在AI代理集成到终端用户应用中的场景下，需要满足严格的服务水平延迟目标（SLOs）。现有ML服务平台通过批处理优化吞吐量，但面临不可预测的尾部延迟问题。

Method: Vortex采用SLO优先的方法，优化批处理机制以降低延迟并提高稳定性。当RDMA可用时，其优势更加显著。

Result: 在相同任务下，Vortex相比TorchServe和Ray Serve实现了显著更低且更稳定的延迟，在广泛的工作负载范围内，通常能在超过两倍的请求率下达到给定的SLO目标。

Conclusion: Vortex证明了SLO优先方法在ML推理服务中的有效性，能够显著改善延迟性能并支持更高的请求率，特别是在RDMA环境下表现更佳。

Abstract: There is growing interest in deploying ML inference and knowledge retrieval
as services that could support both interactive queries by end users and more
demanding request flows that arise from AIs integrated into a end-user
applications and deployed as agents. Our central premise is that these latter
cases will bring service level latency objectives (SLOs). Existing ML serving
platforms use batching to optimize for high throughput, exposing them to
unpredictable tail latencies. Vortex enables an SLO-first approach. For
identical tasks, Vortex's pipelines achieve significantly lower and more stable
latencies than TorchServe and Ray Serve over a wide range of workloads, often
enabling a given SLO target at more than twice the request rate. When RDMA is
available, the Vortex advantage is even more significant.

</details>


### [36] [Numbering Combinations for Compact Representation of Many-to-Many Relationship Sets](https://arxiv.org/abs/2511.02096)
*Savo Tomovic*

Main category: cs.DB

TL;DR: 提出了一种称为组合关系集的实现方法，用于处理两个实体间的多对多关系，通过组合数系统编码实体，避免物理存储关系表。


<details>
  <summary>Details</summary>
Motivation: 动机源于数据仓库模型中多值维度和桥接表的设计与实现挑战。

Method: 使用组合数系统对实体进行唯一编码，将组合关系信息封装到单个列中，不物理存储关系表，并引入Rank-Join操作到关系代数。

Result: 实现了组合关系集的表示，新列成为候选键，所有关系信息封装到单个列中。

Conclusion: 该方法有效解决了多对多关系的表示问题，避免了传统关系表的物理存储需求。

Abstract: In this paper we propose an approach to implement specific relation-ship set
between two entities called combinatorial relationship set. For the
combinatorial relationship set B between entity sets G and I the mapping
cardinality is many-to-many. Additionally, entities from G can be uniquely
encoded with a pair of values (h, k) generated with the procedure for numbering
combinations of entities from I. The encoding procedure is based on
combinatorial number system that provides a representation of all possible k
-combinations of a set of n elements by a single number. In general
many-to-many relationship sets are represented by a relation or table, while
the combinatorial relationship is not physically stored as separate table.
However, all information is encapsulated into a single column added to G. The
new column is a candidate key in G. Additional operation named Rank-Join to
fundamental relational-algebra is presented to combine information from g and i
associated with a combinatorial relationship set. Motivation for combinatorial
relationship originates from challenges in designing and implementing
multivalued dimensions and bridge tables in data-warehouse models.

</details>


### [37] [Accelerating Graph Similarity Search through Integer Linear Programming](https://arxiv.org/abs/2511.02611)
*Andrea D'Ascenzo,Julian Meffert,Petra Mutzel,Fabrizio Rossi*

Main category: cs.DB

TL;DR: 本文提出了一种基于整数线性规划的新型图相似性搜索算法，通过定义支配性下界和利用阈值参数，显著提升了图编辑距离约束下的图相似性搜索性能。


<details>
  <summary>Details</summary>
Motivation: 图编辑距离(GED)是衡量图相似性的重要指标，但其精确计算是NP难问题，计算复杂度高。图相似性搜索通过设定阈值来缓解这一问题，但现有过滤-验证框架中的过滤步骤仍有改进空间。

Method: 提出基于整数线性规划的下界定义，证明该下界支配现有的分支匹配下界且计算高效。设计了使用下界算法层次结构的新算法，并利用阈值参数构建新颖的整数规划模型。

Result: 在标准测试集上的大量计算实验表明，该方法在大多数测试阈值下显著优于现有最先进算法。

Conclusion: 基于整数线性规划的下界方法能够有效提升图相似性搜索性能，为大规模图相似性计算提供了更高效的解决方案。

Abstract: The Graph Edit Distance (GED) is an important metric for measuring the
similarity between two (labeled) graphs. It is defined as the minimum cost
required to convert one graph into another through a series of (elementary)
edit operations. Its effectiveness in assessing the similarity of large graphs
is limited by the complexity of its exact calculation, which is NP-hard
theoretically and computationally challenging in practice. The latter can be
mitigated by switching to the Graph Similarity Search under GED constraints,
which determines whether the edit distance between two graphs is below a given
threshold. A popular framework for solving Graph Similarity Search under GED
constraints in a graph database for a query graph is the
filter-and-verification framework. Filtering discards unpromising graphs, while
the verification step certifies the similarity between the filtered graphs and
the query graph. To improve the filtering step, we define a lower bound based
on an integer linear programming formulation. We prove that this lower bound
dominates the effective branch match-based lower bound and can also be computed
efficiently. Consequently, we propose a graph similarity search algorithm that
uses a hierarchy of lower bound algorithms and solves a novel integer
programming formulation that exploits the threshold parameter. An extensive
computational experience on a well-assessed test bed shows that our approach
significantly outperforms the state-of-the-art algorithm on most of the
examined thresholds.

</details>


### [38] [EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union Search across Data Lakes](https://arxiv.org/abs/2511.02674)
*Tim Otto*

Main category: cs.DB

TL;DR: EasyTUS是一个利用大语言模型进行高效可扩展表联合搜索的框架，包含表序列化、表表示和向量搜索三个模块化步骤，并引入了TUSBench标准化基准测试环境。


<details>
  <summary>Details</summary>
Motivation: 数据湖虽然便于维护异构数据，但增加了数据发现任务的复杂性，特别是表联合搜索任务需要识别可与给定输入表进行联合的表。

Method: 采用模块化三步法：表序列化进行格式化和采样，表表示利用LLM生成嵌入向量，向量搜索使用近似最近邻索引进行语义匹配。

Result: 实验显示EasyTUS在平均精度上比现有方法提升34.3%，数据准备速度提升79.2倍，查询处理性能提升7.7倍，在元数据缺失场景下仍保持强劲性能。

Conclusion: EasyTUS框架在表联合搜索任务中表现出色，具有高效性、可扩展性和鲁棒性，为数据湖中的数据发现提供了有效解决方案。

Abstract: Data lakes enable easy maintenance of heterogeneous data in its native form.
While this flexibility can accelerate data ingestion, it shifts the complexity
of data preparation and query processing to data discovery tasks. One such task
is Table Union Search (TUS), which identifies tables that can be unioned with a
given input table. In this work, we present EasyTUS, a comprehensive framework
that leverages Large Language Models (LLMs) to perform efficient and scalable
Table Union Search across data lakes. EasyTUS implements the search pipeline as
three modular steps: Table Serialization for consistent formatting and
sampling, Table Representation that utilizes LLMs to generate embeddings, and
Vector Search that leverages approximate nearest neighbor indexing for semantic
matching. To enable reproducible and systematic evaluation, in this paper, we
also introduce TUSBench, a novel standardized benchmarking environment within
the EasyTUS framework. TUSBench supports unified comparisons across approaches
and data lakes, promoting transparency and progress in the field. Our
experiments using TUSBench show that EasyTUS consistently outperforms most of
the state-of the-art approaches, achieving improvements in average of up to
34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,
and up to 7.7x faster query processing performance. Furthermore, EasyTUS
maintains strong performance even in metadata-absent settings, highlighting its
robustness and adaptability across data lakes.

</details>


### [39] [Relational Deep Dive: Error-Aware Queries Over Unstructured Data](https://arxiv.org/abs/2511.02711)
*Daren Chao,Kaiwen Chen,Naiqing Guan,Nick Koudas*

Main category: cs.DB

TL;DR: ReDD框架通过动态发现查询特定模式、填充关系表和使用统计校准的错误检测方法，将非结构化数据提取错误率从30%降至1%以下。


<details>
  <summary>Details</summary>
Motivation: 非结构化数据普遍存在，但分析查询需要结构化表示。现有方法如RAG缺乏模式意识且难以跨文档对齐，导致高错误率。

Method: 采用两阶段流水线：迭代模式发现(ISD)识别最小可连接模式，表格数据填充(TDP)使用基于LLM隐藏状态的轻量级分类器提取和校正数据。核心贡献是SCAPE统计校准错误检测方法和SCAPE-HYB混合方法。

Result: 在多样化数据集上的实验显示，ReDD将数据提取错误率从最高30%降至1%以下，同时保持高模式完整性(100%召回率)和精确度。

Conclusion: ReDD的模块化设计能够精细控制准确性与成本之间的权衡，为高风险分析查询提供了稳健解决方案。

Abstract: Unstructured data is pervasive, but analytical queries demand structured
representations, creating a significant extraction challenge. Existing methods
like RAG lack schema awareness and struggle with cross-document alignment,
leading to high error rates. We propose ReDD (Relational Deep Dive), a
framework that dynamically discovers query-specific schemas, populates
relational tables, and ensures error-aware extraction with provable guarantees.
ReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)
identifies minimal, joinable schemas tailored to each query, and (2) Tabular
Data Population (TDP) extracts and corrects data using lightweight classifiers
trained on LLM hidden states. A main contribution of ReDD is SCAPE, a
statistically calibrated method for error detection with coverage guarantees,
and SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy
and human correction costs. Experiments across diverse datasets demonstrate
ReDD's effectiveness, reducing data extraction errors from up to 30% to below
1% while maintaining high schema completeness (100% recall) and precision.
ReDD's modular design enables fine-grained control over accuracy-cost
trade-offs, making it a robust solution for high-stakes analytical queries over
unstructured corpora.

</details>
