{"id": "2601.20015", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.20015", "abs": "https://arxiv.org/abs/2601.20015", "authors": ["Amani Agrawal", "Tianxin Wang", "Dennis Shasha"], "title": "DBTuneSuite: An Extendible Experimental Suite to Test the Time Performance of Multi-layer Tuning Options on Database Management Systems", "comment": null, "summary": "DBTuneSuite is a suite of experiments on four widely deployed free database systems to test their performance under various query/upsert loads and under various tuning options. The suite provides: (i) scripts to generate data and to install and run tests, making it expandable to other tests and systems; (ii) suggestions of which systems work best for which query types; and (iii) quantitative evidence that tuning options widely used in practice can behave very differently across systems. This paper is most useful for database system engineers, advanced database users and troubleshooters, and students.", "AI": {"tldr": "DBTuneSuite\u662f\u4e00\u4e2a\u9488\u5bf9\u56db\u79cd\u5e7f\u6cdb\u90e8\u7f72\u7684\u514d\u8d39\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u5b9e\u9a8c\u5957\u4ef6\uff0c\u6d4b\u8bd5\u5b83\u4eec\u5728\u5404\u79cd\u67e5\u8be2/\u66f4\u65b0\u8d1f\u8f7d\u548c\u8c03\u4f18\u9009\u9879\u4e0b\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u811a\u672c\u3001\u7cfb\u7edf\u9009\u62e9\u5efa\u8bae\u548c\u8c03\u4f18\u9009\u9879\u7684\u5b9a\u91cf\u5206\u6790\u3002", "motivation": "\u4e3a\u6570\u636e\u5e93\u7cfb\u7edf\u5de5\u7a0b\u5e08\u3001\u9ad8\u7ea7\u7528\u6237\u548c\u6545\u969c\u6392\u9664\u4eba\u5458\u4ee5\u53ca\u5b66\u751f\u63d0\u4f9b\u5b9e\u7528\u7684\u6027\u80fd\u8bc4\u4f30\u5de5\u5177\uff0c\u5e2e\u52a9\u7406\u89e3\u4e0d\u540c\u6570\u636e\u5e93\u7cfb\u7edf\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u4ee5\u53ca\u8c03\u4f18\u9009\u9879\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5b9e\u9a8c\u5957\u4ef6\uff0c\u5305\u542b\u6570\u636e\u751f\u6210\u811a\u672c\u3001\u5b89\u88c5\u548c\u8fd0\u884c\u6d4b\u8bd5\u7684\u811a\u672c\uff0c\u652f\u6301\u6269\u5c55\u5230\u5176\u4ed6\u6d4b\u8bd5\u548c\u7cfb\u7edf\u3002\u5bf9\u56db\u79cd\u5e7f\u6cdb\u90e8\u7f72\u7684\u514d\u8d39\u6570\u636e\u5e93\u7cfb\u7edf\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\uff0c\u6db5\u76d6\u5404\u79cd\u67e5\u8be2/\u66f4\u65b0\u8d1f\u8f7d\u548c\u4e0d\u540c\u7684\u8c03\u4f18\u9009\u9879\u3002", "result": "\u63d0\u4f9b\u4e86\u54ea\u4e9b\u7cfb\u7edf\u6700\u9002\u5408\u7279\u5b9a\u67e5\u8be2\u7c7b\u578b\u7684\u5efa\u8bae\uff0c\u5e76\u7ed9\u51fa\u4e86\u5b9a\u91cf\u8bc1\u636e\u8868\u660e\u5b9e\u8df5\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u8c03\u4f18\u9009\u9879\u5728\u4e0d\u540c\u7cfb\u7edf\u95f4\u8868\u73b0\u5dee\u5f02\u5f88\u5927\u3002\u5957\u4ef6\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u7528\u4e8e\u66f4\u591a\u6d4b\u8bd5\u548c\u7cfb\u7edf\u3002", "conclusion": "DBTuneSuite\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u6570\u636e\u5e93\u6027\u80fd\u8bc4\u4f30\u5de5\u5177\uff0c\u4e3a\u6570\u636e\u5e93\u4e13\u4e1a\u4eba\u5458\u63d0\u4f9b\u4e86\u7cfb\u7edf\u9009\u62e9\u4f9d\u636e\u548c\u8c03\u4f18\u6307\u5bfc\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u5e93\u7cfb\u7edf\u8c03\u4f18\u9009\u9879\u7684\u5dee\u5f02\u6027\uff0c\u5bf9\u5b9e\u9645\u90e8\u7f72\u548c\u6545\u969c\u6392\u9664\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2601.20030", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2601.20030", "abs": "https://arxiv.org/abs/2601.20030", "authors": ["Tyler Griggs", "Soujanya Ponnapalli", "Dev Bali", "Wenjie Ma", "James DeLoye", "Audrey Cheng", "Jaewan Hong", "Natacha Crooks", "Scott Shenker", "Ion Stoica", "Matei Zaharia"], "title": "Delta Fair Sharing: Performance Isolation for Multi-Tenant Storage Systems", "comment": null, "summary": "Modern storage systems, often deployed to support multiple tenants in the cloud, must provide performance isolation. Unfortunately, traditional approaches such as fair sharing do not provide performance isolation for storage systems, because their resources (e.g., write buffers and read caches) exhibit high preemption delays. These delays lead to unacceptable spikes in client tail latencies, as clients may be forced to wait arbitrarily long to receive their fair share of resources.\n  We introduce Delta Fair Sharing, a family of algorithms for sharing resources with high preemption delays. These algorithms satisfy two key properties: $\u03b4$-fairness, which bounds a client's delay in receiving its fair share of resources to $\u03b4$ time units, and $\u03b4$-Pareto-efficiency, which allocates unused resources to clients with unmet demand. Together, these properties capture resource-acquisition delays end-to-end, bound well-behaved clients' tail-latency spikes to $\u03b4$ time units, and ensure high utilization. We implement such algorithms in FAIRDB, an extension of RocksDB. Our evaluation shows that FAIRDB isolates well-behaved clients from high-demand workloads better than state-of-the-art alternatives.", "AI": {"tldr": "Delta Fair Sharing\u7b97\u6cd5\u65cf\u4e3a\u5177\u6709\u9ad8\u62a2\u5360\u5ef6\u8fdf\u7684\u5b58\u50a8\u7cfb\u7edf\u63d0\u4f9b\u6027\u80fd\u9694\u79bb\uff0c\u901a\u8fc7\u03b4-\u516c\u5e73\u6027\u548c\u03b4-\u5e15\u7d2f\u6258\u6548\u7387\u4fdd\u8bc1\u5ba2\u6237\u7aef\u5ef6\u8fdf\u6709\u754c\u4e14\u8d44\u6e90\u9ad8\u5229\u7528\u7387", "motivation": "\u73b0\u4ee3\u4e91\u5b58\u50a8\u7cfb\u7edf\u9700\u8981\u4e3a\u591a\u79df\u6237\u63d0\u4f9b\u6027\u80fd\u9694\u79bb\uff0c\u4f46\u4f20\u7edf\u516c\u5e73\u5171\u4eab\u65b9\u6cd5\u56e0\u8d44\u6e90\uff08\u5982\u5199\u7f13\u51b2\u533a\u548c\u8bfb\u7f13\u5b58\uff09\u7684\u9ad8\u62a2\u5360\u5ef6\u8fdf\u800c\u5931\u8d25\uff0c\u5bfc\u81f4\u5ba2\u6237\u7aef\u5c3e\u90e8\u5ef6\u8fdf\u51fa\u73b0\u4e0d\u53ef\u63a5\u53d7\u7684\u5c16\u5cf0", "method": "\u63d0\u51faDelta Fair Sharing\u7b97\u6cd5\u65cf\uff0c\u6ee1\u8db3\u03b4-\u516c\u5e73\u6027\uff08\u5ba2\u6237\u7aef\u83b7\u5f97\u516c\u5e73\u4efd\u989d\u7684\u5ef6\u8fdf\u4e0d\u8d85\u8fc7\u03b4\u65f6\u95f4\u5355\u4f4d\uff09\u548c\u03b4-\u5e15\u7d2f\u6258\u6548\u7387\uff08\u5c06\u672a\u4f7f\u7528\u8d44\u6e90\u5206\u914d\u7ed9\u9700\u6c42\u672a\u6ee1\u8db3\u7684\u5ba2\u6237\u7aef\uff09\uff0c\u5e76\u5728FAIRDB\uff08RocksDB\u6269\u5c55\uff09\u4e2d\u5b9e\u73b0", "result": "FAIRDB\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6848\uff0c\u80fd\u66f4\u597d\u5730\u5c06\u884c\u4e3a\u826f\u597d\u7684\u5ba2\u6237\u7aef\u4e0e\u9ad8\u9700\u6c42\u5de5\u4f5c\u8d1f\u8f7d\u9694\u79bb\u5f00\u6765", "conclusion": "Delta Fair Sharing\u7b97\u6cd5\u901a\u8fc7\u7aef\u5230\u7aef\u6355\u83b7\u8d44\u6e90\u83b7\u53d6\u5ef6\u8fdf\uff0c\u5c06\u884c\u4e3a\u826f\u597d\u5ba2\u6237\u7aef\u7684\u5c3e\u90e8\u5ef6\u8fdf\u5c16\u5cf0\u9650\u5236\u5728\u03b4\u65f6\u95f4\u5355\u4f4d\u5185\uff0c\u540c\u65f6\u786e\u4fdd\u9ad8\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4e3a\u5177\u6709\u9ad8\u62a2\u5360\u5ef6\u8fdf\u7684\u5b58\u50a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u9694\u79bb\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.20482", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.20482", "abs": "https://arxiv.org/abs/2601.20482", "authors": ["Houming Chen", "Zhe Zhang", "H. V. Jagadish"], "title": "ConStruM: A Structure-Guided LLM Framework for Context-Aware Schema Matching", "comment": "13 pages, 4 figures", "summary": "Column matching is a central task in reconciling schemas for data integration. Column names and descriptions are valuable for this task. LLMs can leverage such natural-language schema metadata. However, in many datasets, correct matching requires additional evidence beyond the column itself. Because it is impractical to provide an LLM with the entire schema metadata needed to capture this evidence, the core challenge becomes to select and organize the most useful contextual information.\n  We present ConStruM, a structure-guided framework for budgeted evidence packing in schema matching. ConStruM constructs a lightweight, reusable structure in which, at query time, it assembles a small context pack emphasizing the most discriminative evidence. ConStruM is designed as an add-on: given a shortlist of candidate targets produced by an upstream matcher, it augments the matcher's final LLM prompt with structured, query-specific evidence so that the final selection is better grounded. For this purpose, we develop a context tree for budgeted multi-level context retrieval and a global similarity hypergraph that surfaces groups of highly similar columns (on both the source and target sides), summarized via group-aware differentiation cues computed online or precomputed offline. Experiments on real datasets show that ConStruM improves matching by providing and organizing the right contextual evidence.", "AI": {"tldr": "ConStruM\u662f\u4e00\u4e2a\u7528\u4e8e\u6a21\u5f0f\u5339\u914d\u4e2d\u9884\u7b97\u5316\u8bc1\u636e\u6253\u5305\u7684\u7ed3\u6784\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u8f7b\u91cf\u7ea7\u53ef\u91cd\u7528\u7ed3\u6784\uff0c\u5728\u67e5\u8be2\u65f6\u7ec4\u88c5\u5f3a\u8c03\u6700\u5177\u533a\u5206\u6027\u8bc1\u636e\u7684\u5c0f\u578b\u4e0a\u4e0b\u6587\u5305\uff0c\u4ee5\u63d0\u5347LLM\u5728\u6a21\u5f0f\u5339\u914d\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u6570\u636e\u96c6\u6210\u4e2d\uff0c\u6a21\u5f0f\u5339\u914d\u662f\u6838\u5fc3\u4efb\u52a1\uff0c\u5217\u540d\u548c\u63cf\u8ff0\u5bf9\u6b64\u5f88\u6709\u4ef7\u503c\u3002LLM\u53ef\u4ee5\u5229\u7528\u8fd9\u4e9b\u81ea\u7136\u8bed\u8a00\u6a21\u5f0f\u5143\u6570\u636e\uff0c\u4f46\u8bb8\u591a\u6570\u636e\u96c6\u9700\u8981\u8d85\u51fa\u5217\u672c\u8eab\u7684\u989d\u5916\u8bc1\u636e\u3002\u7531\u4e8e\u5411LLM\u63d0\u4f9b\u6355\u83b7\u8fd9\u4e9b\u8bc1\u636e\u6240\u9700\u7684\u6574\u4e2a\u6a21\u5f0f\u5143\u6570\u636e\u4e0d\u5207\u5b9e\u9645\uff0c\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u9009\u62e9\u548c\u7ec4\u7ec7\u6700\u6709\u7528\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "ConStruM\u6784\u5efa\u8f7b\u91cf\u7ea7\u53ef\u91cd\u7528\u7ed3\u6784\uff0c\u5728\u67e5\u8be2\u65f6\u7ec4\u88c5\u5c0f\u578b\u4e0a\u4e0b\u6587\u5305\uff0c\u5f3a\u8c03\u6700\u5177\u533a\u5206\u6027\u8bc1\u636e\u3002\u4f5c\u4e3a\u9644\u52a0\u7ec4\u4ef6\uff0c\u7ed9\u5b9a\u4e0a\u6e38\u5339\u914d\u5668\u751f\u6210\u7684\u5019\u9009\u76ee\u6807\u77ed\u5217\u8868\uff0c\u5b83\u7528\u7ed3\u6784\u5316\u3001\u67e5\u8be2\u7279\u5b9a\u7684\u8bc1\u636e\u589e\u5f3a\u5339\u914d\u5668\u7684\u6700\u7ec8LLM\u63d0\u793a\u3002\u5f00\u53d1\u4e86\u7528\u4e8e\u9884\u7b97\u5316\u591a\u7ea7\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u6811\u548c\u5168\u5c40\u76f8\u4f3c\u6027\u8d85\u56fe\uff0c\u901a\u8fc7\u5728\u7ebf\u8ba1\u7b97\u6216\u79bb\u7ebf\u9884\u8ba1\u7b97\u7684\u7ec4\u611f\u77e5\u533a\u5206\u7ebf\u7d22\u6765\u603b\u7ed3\u9ad8\u5ea6\u76f8\u4f3c\u5217\u7ec4\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cConStruM\u901a\u8fc7\u63d0\u4f9b\u548c\u7ec4\u7ec7\u6b63\u786e\u7684\u4e0a\u4e0b\u6587\u8bc1\u636e\u6765\u6539\u8fdb\u6a21\u5f0f\u5339\u914d\u3002", "conclusion": "ConStruM\u662f\u4e00\u4e2a\u6709\u6548\u7684\u7ed3\u6784\u5f15\u5bfc\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u9884\u7b97\u5316\u8bc1\u636e\u6253\u5305\u548c\u4e0a\u4e0b\u6587\u7ec4\u7ec7\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u6a21\u5f0f\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fc7\u8f7d\u7684\u95ee\u9898\u3002"}}
{"id": "2601.20664", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.20664", "abs": "https://arxiv.org/abs/2601.20664", "authors": ["Dimitrios Karapiperis", "Leonidas Akritidis", "Panayiotis Bozanis", "Vassilios Verykios"], "title": "ALER: An Active Learning Hybrid System for Efficient Entity Resolution", "comment": null, "summary": "Entity Resolution (ER) is a critical task for data integration, yet state-of-the-art supervised deep learning models remain impractical for many real-world applications due to their need for massive, expensive-to-obtain labeled datasets. While Active Learning (AL) offers a potential solution to this \"label scarcity\" problem, existing approaches introduce severe scalability bottlenecks. Specifically, they achieve high accuracy but incur prohibitive computational costs by re-training complex models from scratch or solving NP-hard selection problems in every iteration. In this paper, we propose ALER, a novel, semi-supervised pipeline designed to bridge the gap between semantic accuracy and computational scalability. ALER eliminates the training bottleneck by using a frozen bi-encoder architecture to generate static embeddings once and then iteratively training a lightweight classifier on top. To address the memory bottleneck associated with large-scale candidate pools, we first select a representative sample of the data and then use K-Means to partition this sample into semantically coherent chunks, enabling an efficient AL loop. We further propose a hybrid query strategy that combines \"confused\" and \"confident\" pairs to efficiently refine the decision boundary while correcting high-confidence errors.Extensive evaluation demonstrates ALER's superior efficiency, particularly on the large-scale DBLP dataset: it accelerates the training loop by 1.3x while drastically reducing resolution latency by a factor of 3.8 compared to the fastest baseline.", "AI": {"tldr": "ALER\uff1a\u4e00\u79cd\u65b0\u9896\u7684\u534a\u76d1\u7763\u5b9e\u4f53\u89e3\u6790\u7ba1\u9053\uff0c\u901a\u8fc7\u51bb\u7ed3\u7684\u53cc\u7f16\u7801\u5668\u67b6\u6784\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u89e3\u51b3\u4e86\u4f20\u7edf\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u76d1\u7763\u5f0f\u6df1\u5ea6\u5b66\u4e60\u5b9e\u4f53\u89e3\u6790\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u7f13\u89e3\u6807\u6ce8\u7a00\u7f3a\u95ee\u9898\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u5305\u62ec\u91cd\u65b0\u8bad\u7ec3\u590d\u6742\u6a21\u578b\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548cNP-hard\u9009\u62e9\u95ee\u9898\u3002", "method": "ALER\u91c7\u7528\u534a\u76d1\u7763\u7ba1\u9053\uff1a1\uff09\u4f7f\u7528\u51bb\u7ed3\u7684\u53cc\u7f16\u7801\u5668\u67b6\u6784\u751f\u6210\u9759\u6001\u5d4c\u5165\uff0c\u907f\u514d\u91cd\u590d\u8bad\u7ec3\uff1b2\uff09\u5728\u5d4c\u5165\u4e0a\u8fed\u4ee3\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff1b3\uff09\u901a\u8fc7K-Means\u5bf9\u4ee3\u8868\u6027\u6570\u636e\u6837\u672c\u8fdb\u884c\u8bed\u4e49\u5206\u5757\uff0c\u89e3\u51b3\u5927\u89c4\u6a21\u5019\u9009\u6c60\u7684\u5185\u5b58\u74f6\u9888\uff1b4\uff09\u63d0\u51fa\u7ed3\u5408\"\u56f0\u60d1\"\u548c\"\u81ea\u4fe1\"\u5bf9\u7684\u6df7\u5408\u67e5\u8be2\u7b56\u7565\uff0c\u9ad8\u6548\u4f18\u5316\u51b3\u7b56\u8fb9\u754c\u5e76\u7ea0\u6b63\u9ad8\u7f6e\u4fe1\u5ea6\u9519\u8bef\u3002", "result": "\u5728DBLP\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cALER\u8bad\u7ec3\u5faa\u73af\u52a0\u901f1.3\u500d\uff0c\u89e3\u6790\u5ef6\u8fdf\u964d\u4f4e3.8\u500d\uff0c\u76f8\u6bd4\u6700\u5feb\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "conclusion": "ALER\u6210\u529f\u5f25\u5408\u4e86\u8bed\u4e49\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5927\u89c4\u6a21\u5b9e\u4f53\u89e3\u6790\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.19964", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.19964", "abs": "https://arxiv.org/abs/2601.19964", "authors": ["Maxim Tabachnyk", "Xu Shu", "Alexander Fr\u00f6mmgen", "Pavel Sychev", "Vahid Meimand", "Ilia Krets", "Stanislav Pyatykh", "Abner Araujo", "Krist\u00f3f Moln\u00e1r", "Satish Chandra"], "title": "Achieving Productivity Gains with AI-based IDE features: A Journey at Google", "comment": "Accepted for publication at the 3rd International Workshop on Large Language Models For Code (LLM4Code '26 workshop at ICSE '26)", "summary": "We discuss Google's journey in developing and refining two internal AI-based IDE features: code completion and natural-language-driven code transformation (Transform Code). We address challenges in latency, user experience and suggestion quality, all backed by rigorous experimentation. The article serves as an example of how to refine AI developer tools across the user interface, backend, and model layers, to deliver tangible productivity improvements in an enterprise setting.", "AI": {"tldr": "Google\u5206\u4eab\u4e86\u5f00\u53d1AI\u4ee3\u7801\u52a9\u624b\uff08\u4ee3\u7801\u8865\u5168\u548c\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u8f6c\u6362\uff09\u7684\u7ecf\u9a8c\uff0c\u91cd\u70b9\u89e3\u51b3\u5ef6\u8fdf\u3001\u7528\u6237\u4f53\u9a8c\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u901a\u8fc7\u591a\u5c42\u9762\u4f18\u5316\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "motivation": "\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u5f00\u53d1AI\u9a71\u52a8\u7684IDE\u529f\u80fd\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u5ef6\u8fdf\u3001\u7528\u6237\u4f53\u9a8c\u548c\u4ee3\u7801\u5efa\u8bae\u8d28\u91cf\u7b49\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u4f9b\u5b9e\u9645\u7684\u751f\u4ea7\u529b\u63d0\u5347\u3002", "method": "\u901a\u8fc7\u7528\u6237\u754c\u9762\u3001\u540e\u7aef\u548c\u6a21\u578b\u5c42\u7684\u591a\u5c42\u9762\u4f18\u5316\uff0c\u7ed3\u5408\u4e25\u683c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6539\u8fdb\u4ee3\u7801\u8865\u5168\u548c\u81ea\u7136\u8bed\u8a00\u4ee3\u7801\u8f6c\u6362\u529f\u80fd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u80fd\u591f\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u7684AI IDE\u529f\u80fd\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u4f18\u5316\u89e3\u51b3\u4e86\u5ef6\u8fdf\u3001\u7528\u6237\u4f53\u9a8c\u548c\u4ee3\u7801\u8d28\u91cf\u7b49\u5173\u952e\u95ee\u9898\u3002", "conclusion": "Google\u7684\u7ecf\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u8de8\u5c42\u4f18\u5316\u548c\u4e25\u683c\u5b9e\u9a8c\uff0cAI\u5f00\u53d1\u5de5\u5177\u53ef\u4ee5\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u63d0\u4f9b\u5207\u5b9e\u7684\u751f\u4ea7\u529b\u63d0\u5347\uff0c\u4e3a\u7c7b\u4f3c\u5de5\u5177\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u6846\u67b6\u3002"}}
{"id": "2601.20113", "categories": ["cs.DC", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.20113", "abs": "https://arxiv.org/abs/2601.20113", "authors": ["Arshan Khan", "Rohit Deshmukh", "Ben O'Neill"], "title": "A Data-Informed Local Subspaces Method for Error-Bounded Lossy Compression of Large-Scale Scientific Datasets", "comment": "To be submitted to the IEEE Transactions on Parallel and Distributed Systems", "summary": "The growing volume of scientific simulation data presents a significant challenge for storage and transfer. Error-bounded lossy compression has emerged as a critical solution for mitigating these challenges, providing a means to reduce data size while ensuring that reconstructed data remains valid for scientific analysis. In this paper, we present a data-driven scientific data compressor, called Discontinuous Data-informed Local Subspaces (Discontinuous DLS), to improve compression-to-error ratios over data-agnostic compressors. This error-bounded compressor leverages localized spatial and temporal subspaces, informed by the underlying data structure, to enhance compression efficiency and preserve key features. The presented technique is flexible and applicable to a wide range of scientific data, including fluid dynamics, environmental simulations, and other high-dimensional, time-dependent datasets. We describe the core principles of the method and demonstrate its ability to significantly reduce storage requirements without compromising critical data fidelity. The technique is implemented in a distributed computing environment using MPI, and its performance is evaluated against state-of-the-art error-bounded compression methods in terms of compression ratio and reconstruction accuracy. This study highlights discontinuous DLS as a promising approach for large-scale scientific data compression in high-performance computing environments, providing a robust solution for managing the growing data demands of modern scientific simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u79d1\u5b66\u6570\u636e\u538b\u7f29\u65b9\u6cd5Discontinuous DLS\uff0c\u901a\u8fc7\u5c40\u90e8\u65f6\u7a7a\u5b50\u7a7a\u95f4\u63d0\u5347\u538b\u7f29\u6548\u7387\uff0c\u5728\u4fdd\u8bc1\u8bef\u5dee\u8fb9\u754c\u7684\u524d\u63d0\u4e0b\u663e\u8457\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u79d1\u5b66\u6a21\u62df\u6570\u636e\u91cf\u5feb\u901f\u589e\u957f\u7ed9\u5b58\u50a8\u548c\u4f20\u8f93\u5e26\u6765\u5de8\u5927\u6311\u6218\uff0c\u9700\u8981\u8bef\u5dee\u6709\u754c\u7684\u6709\u635f\u538b\u7f29\u6280\u672f\u6765\u51cf\u5c11\u6570\u636e\u89c4\u6a21\uff0c\u540c\u65f6\u786e\u4fdd\u91cd\u5efa\u6570\u636e\u4ecd\u53ef\u7528\u4e8e\u79d1\u5b66\u5206\u6790\u3002", "method": "Discontinuous DLS\u65b9\u6cd5\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u5c40\u90e8\u65f6\u7a7a\u5b50\u7a7a\u95f4\uff0c\u57fa\u4e8e\u5e95\u5c42\u6570\u636e\u7ed3\u6784\u6784\u5efa\u5c40\u90e8\u5316\u6a21\u578b\uff0c\u5728\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u4e2d\u4f7f\u7528MPI\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u73af\u5883\u6a21\u62df\u7b49\u9ad8\u7ef4\u65f6\u53d8\u6570\u636e\u96c6\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u8bef\u5dee\u6709\u754c\u538b\u7f29\u65b9\u6cd5\uff0cDiscontinuous DLS\u5728\u538b\u7f29\u6bd4\u548c\u91cd\u5efa\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u9700\u6c42\u800c\u4e0d\u635f\u5bb3\u5173\u952e\u6570\u636e\u4fdd\u771f\u5ea6\u3002", "conclusion": "Discontinuous DLS\u662f\u5927\u89c4\u6a21\u79d1\u5b66\u6570\u636e\u538b\u7f29\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u7ba1\u7406\u65e5\u76ca\u589e\u957f\u7684\u79d1\u5b66\u6a21\u62df\u6570\u636e\u9700\u6c42\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.20783", "categories": ["cs.DB", "cs.PL"], "pdf": "https://arxiv.org/pdf/2601.20783", "abs": "https://arxiv.org/abs/2601.20783", "authors": ["Naveen Durvasula"], "title": "The Monotone Priority System: Foundations of Contract-Specific Sequencing", "comment": null, "summary": "Modern blockchain applications benefit from the ability to specify sequencing constraints on the transactions that interact with them. This paper proposes a principled and axiomatically justified way of adding sequencing constraints on smart contract function calls that balances expressivity with the tractability of block production. Specifically, we propose a system in which contract developers are allowed to set an integer global priority for each of their calls, so long as that the call's chosen priority is no higher than the priority of any of its referenced calls. Block builders must then simply sequence transactions in priority order (from high to low priority), breaking ties however they would like. We show that this system is the unique system that satisfies five independent axioms.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5168\u5c40\u4f18\u5148\u7ea7\u7684\u667a\u80fd\u5408\u7ea6\u8c03\u7528\u6392\u5e8f\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e94\u4e2a\u516c\u7406\u8bc1\u660e\u5176\u552f\u4e00\u6027", "motivation": "\u73b0\u4ee3\u533a\u5757\u94fe\u5e94\u7528\u9700\u8981\u4e3a\u4ea4\u6613\u8bbe\u7f6e\u6392\u5e8f\u7ea6\u675f\uff0c\u4f46\u9700\u8981\u5728\u8868\u8fbe\u80fd\u529b\u548c\u533a\u5757\u751f\u4ea7\u7684\u53ef\u5904\u7406\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861", "method": "\u5141\u8bb8\u5408\u7ea6\u5f00\u53d1\u8005\u4e3a\u6bcf\u4e2a\u8c03\u7528\u8bbe\u7f6e\u6574\u6570\u5168\u5c40\u4f18\u5148\u7ea7\uff0c\u4f46\u8c03\u7528\u7684\u4f18\u5148\u7ea7\u4e0d\u80fd\u9ad8\u4e8e\u5176\u5f15\u7528\u7684\u4efb\u4f55\u8c03\u7528\u7684\u4f18\u5148\u7ea7\u3002\u533a\u5757\u751f\u4ea7\u8005\u53ea\u9700\u6309\u4f18\u5148\u7ea7\u4ece\u9ad8\u5230\u4f4e\u6392\u5e8f\u4ea4\u6613", "result": "\u8bc1\u660e\u8be5\u7cfb\u7edf\u662f\u6ee1\u8db3\u4e94\u4e2a\u72ec\u7acb\u516c\u7406\u7684\u552f\u4e00\u7cfb\u7edf", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\u7cfb\u7edf\u4e3a\u667a\u80fd\u5408\u7ea6\u8c03\u7528\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u4e14\u516c\u7406\u5316\u7684\u6392\u5e8f\u7ea6\u675f\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u5904\u7406\u6027"}}
{"id": "2601.20103", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20103", "abs": "https://arxiv.org/abs/2601.20103", "authors": ["Darshan Deshpande", "Anand Kannappan", "Rebecca Qian"], "title": "Benchmarking Reward Hack Detection in Code Environments via Contrastive Analysis", "comment": "Dataset: https://huggingface.co/datasets/PatronusAI/trace-dataset", "summary": "Recent advances in reinforcement learning for code generation have made robust environments essential to prevent reward hacking. As LLMs increasingly serve as evaluators in code-based RL, their ability to detect reward hacking remains understudied. In this paper, we propose a novel taxonomy of reward exploits spanning across 54 categories and introduce TRACE (Testing Reward Anomalies in Code Environments), a synthetically curated and human-verified benchmark containing 517 testing trajectories. Unlike prior work that evaluates reward hack detection in isolated classification scenarios, we contrast these evaluations with a more realistic, contrastive anomaly detection setup on TRACE. Our experiments reveal that models capture reward hacks more effectively in contrastive settings than in isolated classification settings, with GPT-5.2 with highest reasoning mode achieving the best detection rate at 63%, up from 45% in isolated settings on TRACE. Building on this insight, we demonstrate that state-of-the-art models struggle significantly more with semantically contextualized reward hacks compared to syntactically contextualized ones. We further conduct qualitative analyses of model behaviors, as well as ablation studies showing that the ratio of benign to hacked trajectories and analysis cluster sizes substantially impact detection performance. We release the benchmark and evaluation harness to enable the community to expand TRACE and evaluate their models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TRACE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u4ee3\u7801\u751f\u6210\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u5bf9\u6bd4\u6027\u5f02\u5e38\u68c0\u6d4b\u8bbe\u7f6e\u4e2d\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0cGPT-5.2\u8fbe\u523063%\u68c0\u6d4b\u7387\u3002", "motivation": "\u968f\u7740\u4ee3\u7801\u751f\u6210\u5f3a\u5316\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u9700\u8981\u5065\u58ee\u7684\u73af\u5883\u6765\u9632\u6b62\u5956\u52b1\u653b\u51fb\u3002LLM\u8d8a\u6765\u8d8a\u591a\u5730\u4f5c\u4e3a\u4ee3\u7801RL\u4e2d\u7684\u8bc4\u4f30\u5668\uff0c\u4f46\u5176\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u6db5\u76d654\u4e2a\u7c7b\u522b\u7684\u5956\u52b1\u653b\u51fb\u5206\u7c7b\u6cd5\uff0c\u5e76\u521b\u5efa\u4e86TRACE\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u542b517\u4e2a\u6d4b\u8bd5\u8f68\u8ff9\uff09\u3002\u5728\u5bf9\u6bd4\u6027\u5f02\u5e38\u68c0\u6d4b\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5b64\u7acb\u5206\u7c7b\u573a\u666f\u3002", "result": "\u6a21\u578b\u5728\u5bf9\u6bd4\u6027\u8bbe\u7f6e\u4e2d\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u7684\u6548\u679c\u66f4\u597d\uff0cGPT-5.2\u5728\u6700\u9ad8\u63a8\u7406\u6a21\u5f0f\u4e0b\u8fbe\u523063%\u68c0\u6d4b\u7387\uff08\u5b64\u7acb\u8bbe\u7f6e\u4e2d\u4e3a45%\uff09\u3002\u6a21\u578b\u5728\u8bed\u4e49\u4e0a\u4e0b\u6587\u5956\u52b1\u653b\u51fb\u4e0a\u7684\u8868\u73b0\u660e\u663e\u5dee\u4e8e\u8bed\u6cd5\u4e0a\u4e0b\u6587\u653b\u51fb\u3002", "conclusion": "TRACE\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86LLM\u68c0\u6d4b\u5956\u52b1\u653b\u51fb\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u8bed\u4e49\u4e0a\u4e0b\u6587\u653b\u51fb\u65b9\u9762\u3002\u5bf9\u6bd4\u6027\u8bc4\u4f30\u8bbe\u7f6e\u66f4\u63a5\u8fd1\u5b9e\u9645\u573a\u666f\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u4ee3\u7801\u751f\u6210RL\u7cfb\u7edf\u7684\u5065\u58ee\u6027\u3002"}}
{"id": "2601.20273", "categories": ["cs.DC", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.20273", "abs": "https://arxiv.org/abs/2601.20273", "authors": ["Jiacheng Yang", "Jun Wu", "Yaoyao Ding", "Zhiying Xu", "Yida Wang", "Gennady Pekhimenko"], "title": "StreamFusion: Scalable Sequence Parallelism for Distributed Inference of Diffusion Transformers on GPUs", "comment": null, "summary": "Diffusion Transformers (DiTs) have gained increasing adoption in high-quality image and video generation. As demand for higher-resolution images and longer videos increases, single-GPU inference becomes inefficient due to increased latency and large activation sizes. Current frameworks employ sequence parallelism (SP) techniques such as Ulysses Attention and Ring Attention to scale inference. However, these implementations have three primary limitations: (1) suboptimal communication patterns for network topologies on modern GPU machines, (2) latency bottlenecks from all-to-all operations in inter-machine communication, and (3) GPU sender-receiver synchronization and computation overheads from using two-sided communication libraries. To address these issues, we present StreamFusion, a topology-aware efficient DiT serving engine. StreamFusion incorporates three key innovations: (1) a topology-aware sequence parallelism technique that accounts for inter- and intra-machine bandwidth differences, (2) Torus Attention, a novel SP technique enabling overlapping of inter-machine all-to-all operations with computation, and (3) a one-sided communication implementation that minimizes GPU sender-receiver synchronization and computation overheads. Our experiments demonstrate that StreamFusion outperforms the state-of-the-art approach by an average of $1.35\\times$ (up to $1.77\\times$).", "AI": {"tldr": "StreamFusion\u662f\u4e00\u4e2a\u9488\u5bf9Diffusion Transformers\u7684\u9ad8\u6548\u670d\u52a1\u5f15\u64ce\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7684\u5e8f\u5217\u5e76\u884c\u3001Torus Attention\u548c\u5355\u8fb9\u901a\u4fe1\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5e76\u884c\u65b9\u6cd5\u5728\u901a\u4fe1\u6a21\u5f0f\u3001\u5ef6\u8fdf\u548c\u540c\u6b65\u5f00\u9500\u65b9\u9762\u7684\u9650\u5236\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe1.77\u500d\u3002", "motivation": "\u968f\u7740\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u751f\u6210\u9700\u6c42\u7684\u589e\u957f\uff0c\u5355GPU\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002\u73b0\u6709\u5e8f\u5217\u5e76\u884c\u6280\u672f\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u5bf9\u73b0\u4ee3GPU\u673a\u5668\u7f51\u7edc\u62d3\u6251\u7684\u901a\u4fe1\u6a21\u5f0f\u4e0d\u4f18\u5316\uff1b2) \u673a\u5668\u95f4all-to-all\u64cd\u4f5c\u9020\u6210\u5ef6\u8fdf\u74f6\u9888\uff1b3) \u4f7f\u7528\u53cc\u8fb9\u901a\u4fe1\u5e93\u5e26\u6765\u7684GPU\u53d1\u9001-\u63a5\u6536\u540c\u6b65\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "method": "StreamFusion\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u8003\u8651\u673a\u5668\u95f4\u548c\u673a\u5668\u5185\u5e26\u5bbd\u5dee\u5f02\u7684\u62d3\u6251\u611f\u77e5\u5e8f\u5217\u5e76\u884c\u6280\u672f\uff1b2) Torus Attention\uff0c\u4e00\u79cd\u65b0\u7684\u5e8f\u5217\u5e76\u884c\u6280\u672f\uff0c\u5141\u8bb8\u673a\u5668\u95f4all-to-all\u64cd\u4f5c\u4e0e\u8ba1\u7b97\u91cd\u53e0\uff1b3) \u6700\u5c0f\u5316GPU\u53d1\u9001-\u63a5\u6536\u540c\u6b65\u548c\u8ba1\u7b97\u5f00\u9500\u7684\u5355\u8fb9\u901a\u4fe1\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cStreamFusion\u5728\u6027\u80fd\u4e0a\u5e73\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd51.35\u500d\uff0c\u6700\u9ad8\u53ef\u8fbe1.77\u500d\u3002", "conclusion": "StreamFusion\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u5e8f\u5217\u5e76\u884c\u6280\u672f\u7684\u901a\u4fe1\u548c\u540c\u6b65\u74f6\u9888\uff0c\u4e3aDiffusion Transformers\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u670d\u52a1\u5f15\u64ce\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u957f\u89c6\u9891\u751f\u6210\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2601.20106", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20106", "abs": "https://arxiv.org/abs/2601.20106", "authors": ["Shamse Tasnim Cynthia", "Joy Krishan Das", "Banani Roy"], "title": "Are We All Using Agents the Same Way? An Empirical Study of Core and Peripheral Developers Use of Coding Agents", "comment": null, "summary": "Autonomous AI agents are transforming software development and redefining how developers collaborate with AI. Prior research shows that the adoption and use of AI-powered tools differ between core and peripheral developers. However, it remains unclear how this dynamic unfolds in the emerging era of autonomous coding agents. In this paper, we present the first empirical study of 9,427 agentic PRs, examining how core and peripheral developers use, review, modify, and verify agent-generated contributions prior to acceptance. Through a mix of qualitative and quantitative analysis, we make four key contributions. First, a subset of peripheral developers use agents more often, delegating tasks evenly across bug fixing, feature addition, documentation, and testing. In contrast, core developers focus more on documentation and testing, yet their agentic PRs are frequently merged into the main/master branch. Second, core developers engage slightly more in review discussions than peripheral developers, and both groups focus on evolvability issues. Third, agentic PRs are less likely to be modified, but when they are, both groups commonly perform refactoring. Finally, peripheral developers are more likely to merge without running CI checks, whereas core developers more consistently require passing verification before acceptance. Our analysis offers a comprehensive view of how developer experience shapes integration offer insights for both peripheral and core developers on how to effectively collaborate with coding agents.", "AI": {"tldr": "\u6838\u5fc3\u4e0e\u5916\u56f4\u5f00\u53d1\u8005\u5728\u4f7f\u7528\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u65f6\u7684\u884c\u4e3a\u5dee\u5f02\u7814\u7a76\uff1a\u5916\u56f4\u5f00\u53d1\u8005\u66f4\u9891\u7e41\u4f7f\u7528\u4ee3\u7406\u4f46\u6838\u5fc3\u5f00\u53d1\u8005\u4ee3\u7406PR\u5408\u5e76\u7387\u66f4\u9ad8\uff0c\u4e24\u8005\u5728\u5ba1\u67e5\u3001\u4fee\u6539\u548c\u9a8c\u8bc1\u65b9\u9762\u5b58\u5728\u4e0d\u540c\u6a21\u5f0f\u3002", "motivation": "\u968f\u7740\u81ea\u4e3bAI\u4ee3\u7406\u6539\u53d8\u8f6f\u4ef6\u5f00\u53d1\u65b9\u5f0f\uff0c\u9700\u8981\u4e86\u89e3\u6838\u5fc3\u4e0e\u5916\u56f4\u5f00\u53d1\u8005\u5982\u4f55\u4e0e\u8fd9\u4e9b\u4ee3\u7406\u534f\u4f5c\u3002\u5148\u524d\u7814\u7a76\u8868\u660eAI\u5de5\u5177\u91c7\u7528\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u65f6\u4ee3\u4e0b\u8fd9\u79cd\u52a8\u6001\u5173\u7cfb\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5bf99,427\u4e2a\u4ee3\u7406\u751f\u6210\u7684PR\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u91c7\u7528\u5b9a\u6027\u4e0e\u5b9a\u91cf\u6df7\u5408\u5206\u6790\u65b9\u6cd5\uff0c\u8003\u5bdf\u6838\u5fc3\u4e0e\u5916\u56f4\u5f00\u53d1\u8005\u5982\u4f55\u4f7f\u7528\u3001\u5ba1\u67e5\u3001\u4fee\u6539\u548c\u9a8c\u8bc1\u4ee3\u7406\u8d21\u732e\u3002", "result": "1) \u5916\u56f4\u5f00\u53d1\u8005\u66f4\u9891\u7e41\u4f7f\u7528\u4ee3\u7406\uff0c\u4efb\u52a1\u5206\u5e03\u5747\u5300\uff1b\u6838\u5fc3\u5f00\u53d1\u8005\u66f4\u5173\u6ce8\u6587\u6863\u548c\u6d4b\u8bd5\uff0c\u4f46\u5176\u4ee3\u7406PR\u5408\u5e76\u7387\u66f4\u9ad8\u30022) \u6838\u5fc3\u5f00\u53d1\u8005\u5ba1\u67e5\u53c2\u4e0e\u5ea6\u7565\u9ad8\uff0c\u4e24\u8005\u90fd\u5173\u6ce8\u53ef\u6f14\u5316\u6027\u95ee\u9898\u30023) \u4ee3\u7406PR\u4fee\u6539\u6982\u7387\u8f83\u4f4e\uff0c\u4fee\u6539\u65f6\u90fd\u8fdb\u884c\u91cd\u6784\u30024) \u5916\u56f4\u5f00\u53d1\u8005\u66f4\u53ef\u80fd\u8df3\u8fc7CI\u68c0\u67e5\u5408\u5e76\uff0c\u6838\u5fc3\u5f00\u53d1\u8005\u66f4\u575a\u6301\u9a8c\u8bc1\u901a\u8fc7\u3002", "conclusion": "\u5f00\u53d1\u8005\u7ecf\u9a8c\u663e\u8457\u5f71\u54cd\u4e0e\u7f16\u7801\u4ee3\u7406\u7684\u534f\u4f5c\u65b9\u5f0f\uff0c\u7814\u7a76\u4e3a\u4e24\u7c7b\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u534f\u4f5c\u7684\u89c1\u89e3\uff0c\u5f3a\u8c03\u6838\u5fc3\u5f00\u53d1\u8005\u66f4\u6ce8\u91cd\u8d28\u91cf\u4fdd\u8bc1\u800c\u5916\u56f4\u5f00\u53d1\u8005\u66f4\u4f9d\u8d56\u4ee3\u7406\u529f\u80fd\u3002"}}
{"id": "2601.20309", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20309", "abs": "https://arxiv.org/abs/2601.20309", "authors": ["Jiahuan Yu", "Mingtao Hu", "Zichao Lin", "Minjia Zhang"], "title": "SuperInfer: SLO-Aware Rotary Scheduling and Memory Management for LLM Inference on Superchips", "comment": "Accepted by MLSys '26", "summary": "Large Language Model (LLM) serving faces a fundamental tension between stringent latency Service Level Objectives (SLOs) and limited GPU memory capacity. When high request rates exhaust the KV cache budget, existing LLM inference systems often suffer severe head-of-line (HOL) blocking. While prior work explored PCIe-based offloading, these approaches cannot sustain responsiveness under high request rates, often failing to meet tight Time-To-First-Token (TTFT) and Time-Between-Tokens (TBT) SLOs. We present SuperInfer, a high-performance LLM inference system designed for emerging Superchips (e.g., NVIDIA GH200) with tightly coupled GPU-CPU architecture via NVLink-C2C. SuperInfer introduces RotaSched, the first proactive, SLO-aware rotary scheduler that rotates requests to maintain responsiveness on Superchips, and DuplexKV, an optimized rotation engine that enables full-duplex transfer over NVLink-C2C. Evaluations on GH200 using various models and datasets show that SuperInfer improves TTFT SLO attainment rates by up to 74.7% while maintaining comparable TBT and throughput compared to state-of-the-art systems, demonstrating that SLO-aware scheduling and memory co-design unlocks the full potential of Superchips for responsive LLM serving.", "AI": {"tldr": "SuperInfer\uff1a\u57fa\u4e8eSuperchips\u7684\u9ad8\u6027\u80fdLLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7RotaSched\u8c03\u5ea6\u5668\u548cDuplexKV\u5f15\u64ce\u89e3\u51b3KV\u7f13\u5b58\u5185\u5b58\u4e0d\u8db3\u65f6\u7684\u5ef6\u8fdfSLO\u95ee\u9898", "motivation": "LLM\u670d\u52a1\u9762\u4e34\u4e25\u683c\u7684\u5ef6\u8fdfSLO\u4e0e\u6709\u9650\u7684GPU\u5185\u5b58\u5bb9\u91cf\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002\u5f53\u9ad8\u8bf7\u6c42\u7387\u8017\u5c3dKV\u7f13\u5b58\u9884\u7b97\u65f6\uff0c\u73b0\u6709\u7cfb\u7edf\u4f1a\u51fa\u73b0\u4e25\u91cd\u7684\u961f\u5934\u963b\u585e\u95ee\u9898\u3002\u867d\u7136\u5148\u524d\u5de5\u4f5c\u63a2\u7d22\u4e86PCIe\u5378\u8f7d\u65b9\u6848\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u5728\u9ad8\u8bf7\u6c42\u7387\u4e0b\u4fdd\u6301\u54cd\u5e94\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u4e25\u683c\u7684TTFT\u548cTBT SLO\u8981\u6c42\u3002", "method": "1. \u9488\u5bf9NVLink-C2C\u7d27\u5bc6\u8026\u5408GPU-CPU\u67b6\u6784\u7684Superchips\u8bbe\u8ba1SuperInfer\u7cfb\u7edf\uff1b2. \u63d0\u51faRotaSched\uff1a\u9996\u4e2a\u4e3b\u52a8\u7684\u3001SLO\u611f\u77e5\u7684\u8f6e\u8f6c\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u8f6e\u8f6c\u8bf7\u6c42\u5728Superchips\u4e0a\u4fdd\u6301\u54cd\u5e94\u6027\uff1b3. \u5f00\u53d1DuplexKV\uff1a\u4f18\u5316\u7684\u8f6e\u8f6c\u5f15\u64ce\uff0c\u652f\u6301NVLink-C2C\u4e0a\u7684\u5168\u53cc\u5de5\u4f20\u8f93\u3002", "result": "\u5728GH200\u4e0a\u4f7f\u7528\u5404\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u663e\u793a\uff0cSuperInfer\u5c06TTFT SLO\u8fbe\u6210\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe74.7%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6700\u5148\u8fdb\u7cfb\u7edf\u76f8\u5f53\u7684TBT\u548c\u541e\u5410\u91cf\u3002", "conclusion": "SLO\u611f\u77e5\u8c03\u5ea6\u548c\u5185\u5b58\u534f\u540c\u8bbe\u8ba1\u91ca\u653e\u4e86Superchips\u5728\u54cd\u5e94\u6027LLM\u670d\u52a1\u4e2d\u7684\u5168\u90e8\u6f5c\u529b\uff0cSuperInfer\u5c55\u793a\u4e86\u5728\u7d27\u5bc6\u8026\u5408GPU-CPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fdLLM\u63a8\u7406\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2601.20109", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20109", "abs": "https://arxiv.org/abs/2601.20109", "authors": ["Shamse Tasnim Cynthia", "Al Muttakin", "Banani Roy"], "title": "Beyond Bug Fixes: An Empirical Investigation of Post-Merge Code Quality Issues in Agent-Generated Pull Requests", "comment": null, "summary": "The increasing adoption of AI coding agents has increased the number of agent-generated pull requests (PRs) merged with little or no human intervention. Although such PRs promise productivity gains, their post-merge code quality remains underexplored, as prior work has largely relied on benchmarks and controlled tasks rather than large-scale post-merge analyses. To address this gap, we analyze 1,210 merged agent-generated bug-fix PRs from Python repositories in the AIDev dataset. Using SonarQube, we perform a differential analysis between base and merged commits to identify code quality issues newly introduced by PR changes. We examine issue frequency, density, severity, and rule-level prevalence across five agents. Our results show that apparent differences in raw issue counts across agents largely disappear after normalizing by code churn, indicating that higher issue counts are primarily driven by larger PRs. Across all agents, code smells dominate, particularly at critical and major severities, while bugs are less frequent but often severe. Overall, our findings show that merge success does not reliably reflect post-merge code quality, highlighting the need for systematic quality checks for agent-generated bug-fix PRs.", "AI": {"tldr": "\u5206\u67901,210\u4e2aAI\u751f\u6210\u7684bug\u4fee\u590dPR\uff0c\u53d1\u73b0\u5408\u5e76\u6210\u529f\u4e0d\u4ee3\u8868\u4ee3\u7801\u8d28\u91cf\u597d\uff0c\u4ee3\u7801\u5f02\u5473\u666e\u904d\u5b58\u5728\uff0c\u95ee\u9898\u6570\u91cf\u4e3b\u8981\u53d7PR\u5927\u5c0f\u5f71\u54cd\u800c\u975eAI\u4ee3\u7406\u5dee\u5f02\u3002", "motivation": "\u968f\u7740AI\u7f16\u7801\u4ee3\u7406\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5927\u91cfAI\u751f\u6210\u7684PR\u5728\u5f88\u5c11\u6216\u6ca1\u6709\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u88ab\u5408\u5e76\u3002\u867d\u7136\u8fd9\u4e9bPR\u627f\u8bfa\u63d0\u9ad8\u751f\u4ea7\u529b\uff0c\u4f46\u5176\u5408\u5e76\u540e\u7684\u4ee3\u7801\u8d28\u91cf\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u56e0\u4e3a\u5148\u524d\u5de5\u4f5c\u4e3b\u8981\u4f9d\u8d56\u57fa\u51c6\u6d4b\u8bd5\u548c\u53d7\u63a7\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5927\u89c4\u6a21\u5408\u5e76\u540e\u5206\u6790\u3002", "method": "\u4f7f\u7528AIDev\u6570\u636e\u96c6\u4e2d\u7684Python\u4ed3\u5e93\uff0c\u5206\u67901,210\u4e2a\u5df2\u5408\u5e76\u7684AI\u751f\u6210bug\u4fee\u590dPR\u3002\u901a\u8fc7SonarQube\u5bf9\u57fa\u7840\u63d0\u4ea4\u548c\u5408\u5e76\u63d0\u4ea4\u8fdb\u884c\u5dee\u5f02\u5206\u6790\uff0c\u8bc6\u522bPR\u53d8\u66f4\u65b0\u5f15\u5165\u7684\u4ee3\u7801\u8d28\u91cf\u95ee\u9898\u3002\u4ece\u95ee\u9898\u9891\u7387\u3001\u5bc6\u5ea6\u3001\u4e25\u91cd\u6027\u548c\u89c4\u5219\u7ea7\u522b\u4e94\u4e2a\u7ef4\u5ea6\u6bd4\u8f83\u4e94\u4e2aAI\u4ee3\u7406\u3002", "result": "\u4e0d\u540cAI\u4ee3\u7406\u7684\u539f\u59cb\u95ee\u9898\u6570\u91cf\u5dee\u5f02\u5728\u6309\u4ee3\u7801\u53d8\u66f4\u91cf\u6807\u51c6\u5316\u540e\u57fa\u672c\u6d88\u5931\uff0c\u8868\u660e\u95ee\u9898\u6570\u91cf\u4e3b\u8981\u7531PR\u5927\u5c0f\u9a71\u52a8\u800c\u975e\u4ee3\u7406\u5dee\u5f02\u3002\u6240\u6709\u4ee3\u7406\u4e2d\uff0c\u4ee3\u7801\u5f02\u5473\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5c24\u5176\u5728\u5173\u952e\u548c\u4e3b\u8981\u4e25\u91cd\u6027\u7ea7\u522b\uff1bbug\u8f83\u5c11\u4f46\u901a\u5e38\u5f88\u4e25\u91cd\u3002", "conclusion": "\u5408\u5e76\u6210\u529f\u4e0d\u80fd\u53ef\u9760\u53cd\u6620\u5408\u5e76\u540e\u7684\u4ee3\u7801\u8d28\u91cf\uff0c\u5f3a\u8c03\u4e86\u5bf9AI\u751f\u6210bug\u4fee\u590dPR\u8fdb\u884c\u7cfb\u7edf\u6027\u8d28\u91cf\u68c0\u67e5\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2601.20389", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20389", "abs": "https://arxiv.org/abs/2601.20389", "authors": ["Xiao Yang", "Yinan Ni", "Yuqi Tang", "Zhimin Qiu", "Chen Wang", "Tingzhou Yuan"], "title": "Graph-Structured Deep Learning Framework for Multi-task Contention Identification with High-dimensional Metrics", "comment": null, "summary": "This study addresses the challenge of accurately identifying multi-task contention types in high-dimensional system environments and proposes a unified contention classification framework that integrates representation transformation, structural modeling, and a task decoupling mechanism. The method first constructs system state representations from high-dimensional metric sequences, applies nonlinear transformations to extract cross-dimensional dynamic features, and integrates multiple source information such as resource utilization, scheduling behavior, and task load variations within a shared representation space. It then introduces a graph-based modeling mechanism to capture latent dependencies among metrics, allowing the model to learn competitive propagation patterns and structural interference across resource links. On this basis, task-specific mapping structures are designed to model the differences among contention types and enhance the classifier's ability to distinguish multiple contention patterns. To achieve stable performance, the method employs an adaptive multi-task loss weighting strategy that balances shared feature learning with task-specific feature extraction and generates final contention predictions through a standardized inference process. Experiments conducted on a public system trace dataset demonstrate advantages in accuracy, recall, precision, and F1, and sensitivity analyses on batch size, training sample scale, and metric dimensionality further confirm the model's stability and applicability. The study shows that structured representations and multi-task classification based on high-dimensional metrics can significantly improve contention pattern recognition and offer a reliable technical approach for performance management in complex computing environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u7ade\u4e89\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u8868\u793a\u53d8\u6362\u3001\u56fe\u7ed3\u6784\u5efa\u6a21\u548c\u4efb\u52a1\u89e3\u8026\u673a\u5236\uff0c\u5728\u9ad8\u7ef4\u7cfb\u7edf\u73af\u5883\u4e2d\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u7ade\u4e89\u7c7b\u578b\u3002", "motivation": "\u5728\u9ad8\u7ef4\u7cfb\u7edf\u73af\u5883\u4e2d\u51c6\u786e\u8bc6\u522b\u591a\u79cd\u4efb\u52a1\u7ade\u4e89\u7c7b\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u5904\u7406\u590d\u6742\u7684\u8de8\u7ef4\u5ea6\u52a8\u6001\u7279\u5f81\u548c\u8d44\u6e90\u4f9d\u8d56\u5173\u7cfb\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u7ade\u4e89\u4f20\u64ad\u6a21\u5f0f\u548c\u7ed3\u6784\u5e72\u6270\u3002", "method": "1) \u4ece\u9ad8\u7ef4\u6307\u6807\u5e8f\u5217\u6784\u5efa\u7cfb\u7edf\u72b6\u6001\u8868\u793a\uff0c\u5e94\u7528\u975e\u7ebf\u6027\u53d8\u6362\u63d0\u53d6\u8de8\u7ef4\u5ea6\u52a8\u6001\u7279\u5f81\uff1b2) \u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u5efa\u6a21\u673a\u5236\u6355\u6349\u6307\u6807\u95f4\u6f5c\u5728\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b66\u4e60\u7ade\u4e89\u4f20\u64ad\u6a21\u5f0f\uff1b3) \u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u6620\u5c04\u7ed3\u6784\u5efa\u6a21\u4e0d\u540c\u7ade\u4e89\u7c7b\u578b\u5dee\u5f02\uff1b4) \u91c7\u7528\u81ea\u9002\u5e94\u591a\u4efb\u52a1\u635f\u5931\u6743\u91cd\u7b56\u7565\u5e73\u8861\u5171\u4eab\u548c\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5728\u516c\u5f00\u7cfb\u7edf\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002\u5bf9\u6279\u91cf\u5927\u5c0f\u3001\u8bad\u7ec3\u6837\u672c\u89c4\u6a21\u548c\u6307\u6807\u7ef4\u5ea6\u7684\u654f\u611f\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u57fa\u4e8e\u9ad8\u7ef4\u6307\u6807\u7684\u7ed3\u6784\u5316\u8868\u793a\u548c\u591a\u4efb\u52a1\u5206\u7c7b\u80fd\u663e\u8457\u6539\u5584\u7ade\u4e89\u6a21\u5f0f\u8bc6\u522b\uff0c\u4e3a\u590d\u6742\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u6027\u80fd\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2601.20112", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20112", "abs": "https://arxiv.org/abs/2601.20112", "authors": ["Maja Vukovic", "Rangeet Pan", "Tin Kam Ho", "Rahul Krishna", "Raju Pavuluri", "Michele Merler"], "title": "Usage, Effects and Requirements for AI Coding Assistants in the Enterprise: An Empirical Study", "comment": "To appear in the 3rd International Workshop on Large Language Models For Code, co-located at ICSE, Rio de Janeiro, Brazil, 2026", "summary": "The rise of large language models (LLMs) has accelerated the development of automated techniques and tools for supporting various software engineering tasks, e.g., program understanding, code generation, software testing, and program repair. As CodeLLMs are being employed toward automating these tasks, one question that arises, especially in enterprise settings, is whether these coding assistants and the code LLMs that power them are ready for real-world projects and enterprise use cases, and how do they impact the existing software engineering process and user experience. In this paper we survey 57 developers from different domains and with varying software engineering skill about their experience with AI coding assistants and CodeLLMs. We also reviewed 35 user surveys on the usage, experience and expectations of professionals and students using AI coding assistants and CodeLLMs. Based on our study findings and analysis of existing surveys, we discuss the requirements for AI-powered coding assistants.", "AI": {"tldr": "\u8c03\u67e5\u663e\u793aAI\u7f16\u7a0b\u52a9\u624b\u548cCodeLLMs\u5728\u73b0\u5b9e\u9879\u76ee\u4e2d\u7684\u4f7f\u7528\u4f53\u9a8c\u3001\u5f71\u54cd\u53ca\u9700\u6c42", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0cAI\u7f16\u7a0b\u52a9\u624b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b83\u4eec\u5728\u771f\u5b9e\u4f01\u4e1a\u9879\u76ee\u4e2d\u7684\u9002\u7528\u6027\u3001\u5bf9\u73b0\u6709\u5f00\u53d1\u6d41\u7a0b\u7684\u5f71\u54cd\u4ee5\u53ca\u7528\u6237\u4f53\u9a8c\u5982\u4f55\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\u3002", "method": "1. \u8c03\u67e5\u4e8657\u540d\u6765\u81ea\u4e0d\u540c\u9886\u57df\u3001\u5177\u6709\u4e0d\u540c\u6280\u80fd\u6c34\u5e73\u7684\u5f00\u53d1\u8005\u5bf9AI\u7f16\u7a0b\u52a9\u624b\u548cCodeLLMs\u7684\u4f7f\u7528\u4f53\u9a8c\uff1b2. \u56de\u987e\u5206\u6790\u4e8635\u4efd\u5173\u4e8e\u4e13\u4e1a\u4eba\u58eb\u548c\u5b66\u751f\u4f7f\u7528AI\u7f16\u7a0b\u52a9\u624b\u7684\u7528\u6237\u8c03\u67e5\u62a5\u544a\u3002", "result": "\u901a\u8fc7\u8c03\u67e5\u548c\u73b0\u6709\u5206\u6790\uff0c\u83b7\u5f97\u4e86\u5173\u4e8eAI\u7f16\u7a0b\u52a9\u624b\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u4f53\u9a8c\u3001\u5f71\u54cd\u548c\u7528\u6237\u671f\u671b\u7684\u5b9e\u8bc1\u6570\u636e\uff0c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u5de5\u5177\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\u548c\u73b0\u6709\u8c03\u67e5\u5206\u6790\uff0c\u8ba8\u8bba\u4e86AI\u9a71\u52a8\u7684\u7f16\u7a0b\u52a9\u624b\u9700\u8981\u6ee1\u8db3\u7684\u5173\u952e\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u5de5\u5177\u5f00\u53d1\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2601.20408", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.20408", "abs": "https://arxiv.org/abs/2601.20408", "authors": ["Nicholas Santavas", "Kareem Eissa", "Patrycja Cieplicka", "Piotr Florek", "Matteo Nulli", "Stefan Vasilev", "Seyyed Hadi Hashemi", "Antonios Gasteratos", "Shahram Khadivi"], "title": "Meeting SLOs, Slashing Hours: Automated Enterprise LLM Optimization with OptiKIT", "comment": "Accepted in MLSys 2026", "summary": "Enterprise LLM deployment faces a critical scalability challenge: organizations must optimize models systematically to scale AI initiatives within constrained compute budgets, yet the specialized expertise required for manual optimization remains a niche and scarce skillset. This challenge is particularly evident in managing GPU utilization across heterogeneous infrastructure while enabling teams with diverse workloads and limited LLM optimization experience to deploy models efficiently.\n  We present OptiKIT, a distributed LLM optimization framework that democratizes model compression and tuning by automating complex optimization workflows for non-expert teams. OptiKIT provides dynamic resource allocation, staged pipeline execution with automatic cleanup, and seamless enterprise integration.\n  In production, it delivers more than 2x GPU throughput improvement while empowering application teams to achieve consistent performance improvements without deep LLM optimization expertise. We share both the platform design and key engineering insights into resource allocation algorithms, pipeline orchestration, and integration patterns that enable large-scale, production-grade democratization of model optimization. Finally, we open-source the system to enable external contributions and broader reproducibility.", "AI": {"tldr": "OptiKIT\u662f\u4e00\u4e2a\u5206\u5e03\u5f0fLLM\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u590d\u6742\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f7f\u975e\u4e13\u4e1a\u56e2\u961f\u80fd\u591f\u8fdb\u884c\u6a21\u578b\u538b\u7f29\u548c\u8c03\u4f18\uff0c\u5728GPU\u53d7\u9650\u73af\u5883\u4e0b\u5b9e\u73b02\u500d\u4ee5\u4e0a\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u4f01\u4e1aLLM\u90e8\u7f72\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\uff1a\u9700\u8981\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u5185\u7cfb\u7edf\u4f18\u5316\u6a21\u578b\uff0c\u4f46\u624b\u52a8\u4f18\u5316\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\u7a00\u7f3a\u4e14\u96be\u4ee5\u83b7\u53d6\u3002\u7279\u522b\u662f\u5728\u5f02\u6784\u57fa\u7840\u8bbe\u65bd\u4e0a\u7ba1\u7406GPU\u5229\u7528\u7387\uff0c\u540c\u65f6\u8ba9\u5177\u6709\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u548c\u6709\u9650LLM\u4f18\u5316\u7ecf\u9a8c\u7684\u56e2\u961f\u9ad8\u6548\u90e8\u7f72\u6a21\u578b\u3002", "method": "OptiKIT\u6846\u67b6\u63d0\u4f9b\u52a8\u6001\u8d44\u6e90\u5206\u914d\u3001\u5206\u9636\u6bb5\u7ba1\u9053\u6267\u884c\uff08\u5e26\u81ea\u52a8\u6e05\u7406\uff09\u548c\u65e0\u7f1d\u4f01\u4e1a\u96c6\u6210\u3002\u5b83\u81ea\u52a8\u5316\u4e86\u590d\u6742\u7684\u4f18\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5305\u62ec\u8d44\u6e90\u5206\u914d\u7b97\u6cd5\u3001\u7ba1\u9053\u7f16\u6392\u548c\u96c6\u6210\u6a21\u5f0f\uff0c\u4f7f\u975e\u4e13\u5bb6\u56e2\u961f\u80fd\u591f\u8fdb\u884c\u6a21\u578b\u538b\u7f29\u548c\u8c03\u4f18\u3002", "result": "\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0cOptiKIT\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684GPU\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4f7f\u5e94\u7528\u56e2\u961f\u80fd\u591f\u5728\u6ca1\u6709\u6df1\u539aLLM\u4f18\u5316\u4e13\u4e1a\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u83b7\u5f97\u4e00\u81f4\u7684\u6027\u80fd\u6539\u8fdb\u3002\u8be5\u7cfb\u7edf\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u5916\u90e8\u8d21\u732e\u548c\u66f4\u5e7f\u6cdb\u7684\u53ef\u590d\u73b0\u6027\u3002", "conclusion": "OptiKIT\u901a\u8fc7\u81ea\u52a8\u5316\u590d\u6742\u4f18\u5316\u6d41\u7a0b\uff0c\u6c11\u4e3b\u5316\u4e86\u6a21\u578b\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u4f01\u4e1aLLM\u90e8\u7f72\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002\u5b83\u4f7f\u975e\u4e13\u4e1a\u56e2\u961f\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u90e8\u7f72\u6a21\u578b\uff0c\u5e76\u5f00\u6e90\u7cfb\u7edf\u4ee5\u4fc3\u8fdb\u793e\u533a\u8d21\u732e\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2601.20147", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20147", "abs": "https://arxiv.org/abs/2601.20147", "authors": ["Saima Afrin", "Zaiyu Cheng", "Tushar Sharma", "Alexander Serebrenik", "Massimiliano Di Penta", "Antonio Mastropaolo"], "title": "Not All Tokens Matter: Data-Centric Optimization for Efficient Code Summarization", "comment": null, "summary": "Instruction-tuned Language Models ILMs have become essential components of modern AI systems, demonstrating exceptional versatility across a wide range of natural language and reasoning tasks. Among their most impactful applications is code generation, where ILMs--commonly referred to as Code Language Models CLMs--have demonstrated remarkable capability. This strength stems from their defining feature: the use of explicit task instructions during fine-tuning, which enables them to bridge natural language and code by translating human intent into executable code. While much of their progress has been driven by advances in scaling laws and training methodologies, one critical aspect remains underexplored--the impact of system prompts on the performance of both general-purpose ILMs and specialized CLMs when instantiated to assist users with code generation activities. In this study, we take a first step toward bridging this gap by systematically evaluating how system prompts of varying instructional detail, along with model scale, prompting strategy, and programming language, affect ILMs and CLMs in code generation tasks. Our evaluation framework, spanning 120 model configurations, reveals that (1) the influence of system prompts increases with model scale; (2) few-shot prompting reduces this effect compared to zero-shot; and (3) programming language matters, with Java showing greater sensitivity to system prompt variations than Python.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u7cfb\u7edf\u63d0\u793a\u5bf9\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\u53d7\u63d0\u793a\u5f71\u54cd\u8d8a\u660e\u663e\uff0c\u5c11\u6837\u672c\u63d0\u793a\u80fd\u51cf\u5c11\u8fd9\u79cd\u5f71\u54cd\uff0c\u4e14\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u5bf9\u63d0\u793a\u7684\u654f\u611f\u5ea6\u4e0d\u540c\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7cfb\u7edf\u63d0\u793a\u5bf9\u5176\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7cfb\u7edf\u63d0\u793a\u7684\u8be6\u7ec6\u7a0b\u5ea6\u3001\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u7b56\u7565\u548c\u7f16\u7a0b\u8bed\u8a00\u7b49\u56e0\u7d20\u5bf9\u4ee3\u7801\u751f\u6210\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d6120\u79cd\u6a21\u578b\u914d\u7f6e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u8be6\u7ec6\u7a0b\u5ea6\u7684\u7cfb\u7edf\u63d0\u793a\u3001\u6a21\u578b\u89c4\u6a21\uff08\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b\uff09\u3001\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672cvs\u5c11\u6837\u672c\uff09\u4ee5\u53ca\u7f16\u7a0b\u8bed\u8a00\uff08Python\u548cJava\uff09\u5bf9\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u548c\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) \u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u589e\u5f3a\uff1b(2) \u4e0e\u96f6\u6837\u672c\u63d0\u793a\u76f8\u6bd4\uff0c\u5c11\u6837\u672c\u63d0\u793a\u80fd\u51cf\u5c11\u7cfb\u7edf\u63d0\u793a\u7684\u5f71\u54cd\uff1b(3) \u7f16\u7a0b\u8bed\u8a00\u5f88\u91cd\u8981\uff0cJava\u6bd4Python\u5bf9\u7cfb\u7edf\u63d0\u793a\u7684\u53d8\u5316\u66f4\u654f\u611f\u3002", "conclusion": "\u7cfb\u7edf\u63d0\u793a\u5bf9\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e14\u8fd9\u79cd\u5f71\u54cd\u53d7\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u7b56\u7565\u548c\u7f16\u7a0b\u8bed\u8a00\u7b49\u56e0\u7d20\u7684\u8c03\u8282\u3002\u8fd9\u4e3a\u4f18\u5316\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u63d0\u793a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2601.20435", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2601.20435", "abs": "https://arxiv.org/abs/2601.20435", "authors": ["Aleix Roca", "Vicen\u00e7 Beltran"], "title": "Rethinking Thread Scheduling under Oversubscription: A User-Space Framework for Coordinating Multi-runtime and Multi-process Workloads", "comment": null, "summary": "The convergence of high-performance computing (HPC) and artificial intelligence (AI) is driving the emergence of increasingly complex parallel applications and workloads. These workloads often combine multiple parallel runtimes within the same application or across co-located jobs, creating scheduling demands that place significant stress on traditional OS schedulers. When oversubscribed (there are more ready threads than cores), OS schedulers rely on periodic preemptions to multiplex cores, often introducing interference that may degrade performance. In this paper, we present: (1) The User-space Scheduling Framework (USF), a novel seamless process scheduling framework completely implemented in user-space. USF enables users to implement their own process scheduling algorithms without requiring special permissions. We evaluate USF with its default cooperative policy, (2) SCHED_COOP, designed to reduce interference by switching threads only upon blocking. This approach mitigates well-known issues such as Lock-Holder Preemption (LHP), Lock-Waiter Preemption (LWP), and scalability collapse. We implement USF and SCHED_COOP by extending the GNU C library with the nOS-V runtime, enabling seamless coordination across multiple runtimes (e.g., OpenMP) without requiring invasive application changes. Evaluations show gains up to 2.4x in oversubscribed multi-process scenarios, including nested BLAS workloads, multi-process PyTorch inference with LLaMA-3, and Molecular Dynamics (MD) simulations.", "AI": {"tldr": "\u63d0\u51fa\u7528\u6237\u7a7a\u95f4\u8c03\u5ea6\u6846\u67b6USF\u548cSCHED_COOP\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u7528\u6237\u7a7a\u95f4\u5b9e\u73b0\u51cf\u5c11\u5e72\u6270\u7684\u8fdb\u7a0b\u8c03\u5ea6\uff0c\u5728\u8fc7\u8f7d\u573a\u666f\u4e0b\u63d0\u5347\u6027\u80fd\u8fbe2.4\u500d", "motivation": "HPC\u4e0eAI\u878d\u5408\u5bfc\u81f4\u590d\u6742\u5e76\u884c\u5e94\u7528\u589e\u591a\uff0c\u591a\u4e2a\u5e76\u884c\u8fd0\u884c\u65f6\u5171\u5b58\u7ed9\u4f20\u7edfOS\u8c03\u5ea6\u5668\u5e26\u6765\u538b\u529b\u3002\u8fc7\u8f7d\u65f6OS\u8c03\u5ea6\u5668\u7684\u5468\u671f\u6027\u62a2\u5360\u4f1a\u5f15\u5165\u5e72\u6270\uff0c\u964d\u4f4e\u6027\u80fd", "method": "\u5f00\u53d1\u7528\u6237\u7a7a\u95f4\u8c03\u5ea6\u6846\u67b6USF\uff0c\u5b8c\u5168\u5728\u7528\u6237\u7a7a\u95f4\u5b9e\u73b0\uff0c\u65e0\u9700\u7279\u6b8a\u6743\u9650\u3002\u5b9e\u73b0SCHED_COOP\u534f\u4f5c\u8c03\u5ea6\u7b56\u7565\uff0c\u4ec5\u5728\u963b\u585e\u65f6\u5207\u6362\u7ebf\u7a0b\uff0c\u51cf\u5c11\u5e72\u6270\u3002\u901a\u8fc7\u6269\u5c55GNU C\u5e93\u548cnOS-V\u8fd0\u884c\u65f6\u5b9e\u73b0\uff0c\u652f\u6301\u591a\u4e2a\u8fd0\u884c\u65f6\u534f\u8c03", "result": "\u5728\u8fc7\u8f7d\u591a\u8fdb\u7a0b\u573a\u666f\u4e0b\u6027\u80fd\u63d0\u5347\u8fbe2.4\u500d\uff0c\u5305\u62ec\u5d4c\u5957BLAS\u5de5\u4f5c\u8d1f\u8f7d\u3001\u591a\u8fdb\u7a0bPyTorch\u63a8\u7406\uff08LLaMA-3\uff09\u548c\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df", "conclusion": "USF\u6846\u67b6\u548cSCHED_COOP\u8c03\u5ea6\u7b56\u7565\u80fd\u6709\u6548\u51cf\u5c11\u8c03\u5ea6\u5e72\u6270\uff0c\u89e3\u51b3LHP\u3001LWP\u548c\u53ef\u6269\u5c55\u6027\u5d29\u6e83\u7b49\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u5e76\u884c\u5e94\u7528\u63d0\u4f9b\u66f4\u597d\u7684\u8c03\u5ea6\u652f\u6301"}}
{"id": "2601.20148", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20148", "abs": "https://arxiv.org/abs/2601.20148", "authors": ["Marcus Emmanuel Barnes", "Taher A. Ghaleb", "Safwat Hassan"], "title": "LogSieve: Task-Aware CI Log Reduction for Sustainable LLM-Based Analysis", "comment": "Preprint. Accepted for presentation at Mining Software Repositories (MSR'26), co-located ICSE 2026. The final version will appear in the ACM Digital Library as part of the MSR'26 conference proceedings", "summary": "Logs are essential for understanding Continuous Integration (CI) behavior, particularly for diagnosing build failures and performance regressions. Yet their growing volume and verbosity make both manual inspection and automated analysis increasingly costly, time-consuming, and environmentally costly. While prior work has explored log compression, anomaly detection, and LLM-based log analysis, most efforts target structured system logs rather than the unstructured, noisy, and verbose logs typical of CI workflows.\n  We present LogSieve, a lightweight, RCA-aware and semantics-preserving log reduction technique that filters low-information lines while retaining content relevant to downstream reasoning. Evaluated on CI logs from 20 open-source Android projects using GitHub Actions, LogSieve achieves an average 42% reduction in lines and 40% reduction in tokens with minimal semantic loss. This pre-inference reduction lowers computational cost and can proportionally reduce energy use (and associated emissions) by decreasing the volume of data processed during LLM inference.\n  Compared with structure-first baselines (LogZip and random-line removal), LogSieve preserves much higher semantic and categorical fidelity (Cosine = 0.93, GPTScore = 0.93, 80% exact-match accuracy). Embedding-based classifiers automate relevance detection with near-human accuracy (97%), enabling scalable and sustainable integration of semantics-aware filtering into CI workflows. LogSieve thus bridges log management and LLM reasoning, offering a practical path toward greener and more interpretable CI automation.", "AI": {"tldr": "LogSieve\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001RCA\u611f\u77e5\u4e14\u8bed\u4e49\u4fdd\u7559\u7684\u65e5\u5fd7\u7f29\u51cf\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9CI\u5de5\u4f5c\u6d41\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u3001\u5608\u6742\u65e5\u5fd7\uff0c\u901a\u8fc7\u8fc7\u6ee4\u4f4e\u4fe1\u606f\u884c\u6765\u51cf\u5c11\u65e5\u5fd7\u91cf\uff0c\u540c\u65f6\u4fdd\u7559\u4e0b\u6e38\u63a8\u7406\u76f8\u5173\u5185\u5bb9\u3002", "motivation": "CI\u65e5\u5fd7\u5bf9\u4e8e\u7406\u89e3\u6301\u7eed\u96c6\u6210\u884c\u4e3a\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u65e5\u5fd7\u91cf\u7684\u589e\u957f\u548c\u5197\u957f\u6027\u4f7f\u5f97\u624b\u52a8\u68c0\u67e5\u548c\u81ea\u52a8\u5316\u5206\u6790\u53d8\u5f97\u6210\u672c\u9ad8\u6602\u3001\u8017\u65f6\u4e14\u73af\u5883\u6210\u672c\u9ad8\u3002\u73b0\u6709\u7814\u7a76\u5927\u591a\u9488\u5bf9\u7ed3\u6784\u5316\u7cfb\u7edf\u65e5\u5fd7\uff0c\u800c\u975eCI\u5de5\u4f5c\u6d41\u4e2d\u5178\u578b\u7684\u975e\u7ed3\u6784\u5316\u3001\u5608\u6742\u3001\u5197\u957f\u65e5\u5fd7\u3002", "method": "LogSieve\u91c7\u7528\u57fa\u4e8e\u5d4c\u5165\u7684\u5206\u7c7b\u5668\u81ea\u52a8\u68c0\u6d4b\u76f8\u5173\u6027\uff0c\u8fc7\u6ee4\u4f4e\u4fe1\u606f\u884c\uff0c\u4fdd\u7559\u4e0e\u6839\u672c\u539f\u56e0\u5206\u6790\u76f8\u5173\u7684\u8bed\u4e49\u5185\u5bb9\u3002\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001RCA\u611f\u77e5\u4e14\u8bed\u4e49\u4fdd\u7559\u7684\u65e5\u5fd7\u7f29\u51cf\u6280\u672f\u3002", "result": "\u572820\u4e2a\u5f00\u6e90Android\u9879\u76ee\u7684GitHub Actions CI\u65e5\u5fd7\u4e0a\u8bc4\u4f30\uff0cLogSieve\u5e73\u5747\u51cf\u5c1142%\u7684\u884c\u6570\u548c40%\u7684token\u6570\uff0c\u8bed\u4e49\u635f\u5931\u6700\u5c0f\u3002\u76f8\u6bd4\u57fa\u4e8e\u7ed3\u6784\u7684\u57fa\u7ebf\u65b9\u6cd5\uff08LogZip\u548c\u968f\u673a\u884c\u79fb\u9664\uff09\uff0cLogSieve\u4fdd\u6301\u4e86\u66f4\u9ad8\u7684\u8bed\u4e49\u548c\u5206\u7c7b\u4fdd\u771f\u5ea6\uff08\u4f59\u5f26\u76f8\u4f3c\u5ea60.93\uff0cGPTScore 0.93\uff0c80%\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\uff09\u3002", "conclusion": "LogSieve\u901a\u8fc7\u5d4c\u5165\u5206\u7c7b\u5668\u5b9e\u73b0\u63a5\u8fd1\u4eba\u7c7b\u51c6\u786e\u7387\uff0897%\uff09\u7684\u76f8\u5173\u6027\u68c0\u6d4b\uff0c\u4e3aCI\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u6301\u7eed\u7684\u8bed\u4e49\u611f\u77e5\u8fc7\u6ee4\u65b9\u6848\uff0c\u8fde\u63a5\u4e86\u65e5\u5fd7\u7ba1\u7406\u548cLLM\u63a8\u7406\uff0c\u4e3a\u66f4\u7eff\u8272\u3001\u66f4\u53ef\u89e3\u91ca\u7684CI\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2601.20595", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.20595", "abs": "https://arxiv.org/abs/2601.20595", "authors": ["Xinwei Qiang", "Yue Guan", "Zhengding Hu", "Yufei Ding", "Adnan Aziz"], "title": "AutoOverlap: Enabling Fine-Grained Overlap of Computation and Communication with Chunk-Based Scheduling", "comment": null, "summary": "Communication has become a first-order bottleneck in large-cale GPU workloads, and existing distributed compilers address it mainly by overlapping whole compute and communication kernels at the stream level. This coarse granularity incurs extra kernel launches, forces device-wide synchronizations at kernel boundaries, and leaves substantial slack when the slowest tile or kernel stretches the communication tail. We present AutoOverlap, a compiler and runtime that enables automatic fine-grained overlap inside a single fused kernel. AutoOverlap introduces a communication chunk abstraction that decouples communication granularity from kernel structure and backend mechanisms, allowing chunk-level plans to be ported from existing distributed compilers, written directly by users, or instantiated from reusable templates. Given a local Triton kernel and a chunk schedule, AutoOverlap performs transformations to align computation with chunk availability. Implemented as a source-to-source compiler on Triton, AutoOverlap delivers an average end-to-end speedup of 1.3$\\times$ and up to 4.7$\\times$ on multi-GPU workloads.", "AI": {"tldr": "AutoOverlap\uff1a\u4e00\u4e2a\u7f16\u8bd1\u5668\u4e0e\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u5355\u4e2a\u878d\u5408\u5185\u6838\u5185\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u901a\u4fe1\u4e0e\u8ba1\u7b97\u91cd\u53e0\uff0c\u89e3\u51b3\u5927\u89c4\u6a21GPU\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u7f16\u8bd1\u5668\u4e3b\u8981\u901a\u8fc7\u6d41\u7ea7\u522b\u91cd\u53e0\u6574\u4e2a\u8ba1\u7b97\u4e0e\u901a\u4fe1\u5185\u6838\u6765\u89e3\u51b3\u901a\u4fe1\u74f6\u9888\uff0c\u4f46\u8fd9\u79cd\u7c97\u7c92\u5ea6\u65b9\u6cd5\u5b58\u5728\u989d\u5916\u5185\u6838\u542f\u52a8\u3001\u8bbe\u5907\u7ea7\u540c\u6b65\u4ee5\u53ca\u901a\u4fe1\u5c3e\u90e8\u7a7a\u95f2\u65f6\u95f4\u7b49\u95ee\u9898\u3002", "method": "\u5f15\u5165\u901a\u4fe1\u5757\u62bd\u8c61\uff0c\u5c06\u901a\u4fe1\u7c92\u5ea6\u4e0e\u5185\u6838\u7ed3\u6784\u548c\u540e\u7aef\u673a\u5236\u89e3\u8026\uff1b\u901a\u8fc7\u6e90\u5230\u6e90\u7f16\u8bd1\u5668\u5728Triton\u4e0a\u5b9e\u73b0\uff0c\u6839\u636e\u5757\u8c03\u5ea6\u5bf9\u672c\u5730Triton\u5185\u6838\u8fdb\u884c\u8f6c\u6362\uff0c\u4f7f\u8ba1\u7b97\u4e0e\u5757\u53ef\u7528\u6027\u5bf9\u9f50\u3002", "result": "\u5728\u591aGPU\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u5b9e\u73b0\u5e73\u57471.3\u500d\u7aef\u5230\u7aef\u52a0\u901f\uff0c\u6700\u9ad8\u53ef\u8fbe4.7\u500d\u52a0\u901f\u3002", "conclusion": "AutoOverlap\u901a\u8fc7\u7ec6\u7c92\u5ea6\u91cd\u53e0\u663e\u8457\u63d0\u5347\u591aGPU\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\uff0c\u5176\u901a\u4fe1\u5757\u62bd\u8c61\u548c\u7f16\u8bd1\u5668\u8f6c\u6362\u65b9\u6cd5\u4e3a\u89e3\u51b3\u901a\u4fe1\u74f6\u9888\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.20158", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20158", "abs": "https://arxiv.org/abs/2601.20158", "authors": ["Laura Baird", "Armin Moin"], "title": "Cascaded Vulnerability Attacks in Software Supply Chains", "comment": "IEEE/ACM International Conference on Software Engineering (ICSE) 2026 Extended Abstract", "summary": "Most of the current software security analysis tools assess vulnerabilities in isolation. However, sophisticated software supply chain security threats often stem from cascaded vulnerability and security weakness chains that span dependent components. Moreover, although the adoption of Software Bills of Materials (SBOMs) has been accelerating, downstream vulnerability findings vary substantially across SBOM generators and analysis tools. We propose a novel approach to SBOM-driven security analysis methods and tools. We model vulnerability relationships over dependency structure rather than treating scanner outputs as independent records. We represent enriched SBOMs as heterogeneous graphs with nodes being the SBOM components and dependencies, the known software vulnerabilities, and the known software security weaknesses. We then train a Heterogeneous Graph Attention Network (HGAT) to predict whether a component is associated with at least one known vulnerability. Since documented multi-vulnerability chains are scarce, we model cascade discovery as a link prediction problem over CVE pairs using a multi-layer perceptron neural network. This way, we produce ranked candidate links that can be composed into multi-step paths. The HGAT component classifier achieves an Accuracy of 91.03% and an F1-score of 74.02%.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSBOM\u7684\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u5206\u6790\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u9884\u6d4b\u7ec4\u4ef6\u6f0f\u6d1e\uff0c\u5e76\u901a\u8fc7\u94fe\u63a5\u9884\u6d4b\u53d1\u73b0\u7ea7\u8054\u6f0f\u6d1e\u94fe", "motivation": "\u5f53\u524d\u8f6f\u4ef6\u5b89\u5168\u5206\u6790\u5de5\u5177\u901a\u5e38\u5b64\u7acb\u8bc4\u4f30\u6f0f\u6d1e\uff0c\u4f46\u590d\u6742\u7684\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5a01\u80c1\u5f80\u5f80\u6765\u81ea\u8de8\u4f9d\u8d56\u7ec4\u4ef6\u7684\u7ea7\u8054\u6f0f\u6d1e\u94fe\u3002\u6b64\u5916\uff0c\u4e0d\u540cSBOM\u751f\u6210\u5668\u548c\u5206\u6790\u5de5\u5177\u7684\u4e0b\u6e38\u6f0f\u6d1e\u53d1\u73b0\u7ed3\u679c\u5dee\u5f02\u5f88\u5927\u3002", "method": "\u5c06\u589e\u5f3a\u7684SBOM\u5efa\u6a21\u4e3a\u5f02\u6784\u56fe\uff0c\u8282\u70b9\u5305\u62ecSBOM\u7ec4\u4ef6\u3001\u4f9d\u8d56\u5173\u7cfb\u3001\u5df2\u77e5\u6f0f\u6d1e\u548c\u5b89\u5168\u5f31\u70b9\u3002\u4f7f\u7528\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(HGAT)\u9884\u6d4b\u7ec4\u4ef6\u662f\u5426\u5173\u8054\u5df2\u77e5\u6f0f\u6d1e\u3002\u5c06\u7ea7\u8054\u53d1\u73b0\u5efa\u6a21\u4e3aCVE\u5bf9\u7684\u94fe\u63a5\u9884\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u5019\u9009\u94fe\u63a5\uff0c\u53ef\u7ec4\u5408\u6210\u591a\u6b65\u8def\u5f84\u3002", "result": "HGAT\u7ec4\u4ef6\u5206\u7c7b\u5668\u8fbe\u523091.03%\u7684\u51c6\u786e\u7387\u548c74.02%\u7684F1\u5206\u6570\uff0c\u80fd\u591f\u6709\u6548\u9884\u6d4b\u7ec4\u4ef6\u6f0f\u6d1e\u5173\u8054\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u5168\u9762\u5730\u5206\u6790\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\uff0c\u901a\u8fc7\u5efa\u6a21\u4f9d\u8d56\u5173\u7cfb\u548c\u6f0f\u6d1e\u94fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5b64\u7acb\u6f0f\u6d1e\u5206\u6790\u7684\u5c40\u9650\u6027\uff0c\u4e3aSBOM\u9a71\u52a8\u7684\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.20655", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.20655", "abs": "https://arxiv.org/abs/2601.20655", "authors": ["June Chen", "Neal Xu", "Gragas Huang", "Bok Zhou", "Stephen Liu"], "title": "OnePiece: A Large-Scale Distributed Inference System with RDMA for Complex AI-Generated Content (AIGC) Workflows", "comment": "12 pages", "summary": "The rapid growth of AI-generated content (AIGC) has enabled high-quality creative production across diverse domains, yet existing systems face critical inefficiencies in throughput, resource utilization, and scalability under concurrent workloads. This paper introduces OnePiece, a large-scale distributed inference system with RDMA optimized for multi-stage AIGC workflows. By decomposing pipelines into fine-grained microservices and leveraging one-sided RDMA communication, OnePiece significantly reduces inter-node latency and CPU overhead while improving GPU utilization. The system incorporates a novel double-ring buffer design to resolve deadlocks in RDMA-aware memory access without CPU involvement. Additionally, a dynamic Node Manager allocates resources elastically across workflow stages in response to real-time load. Experimental results demonstrate that OnePiece reduces GPU resource consumption by 16x in Wan2.1 image-to-video generation compared to monolithic inference pipelines, offering a scalable, fault-tolerant, and efficient solution for production AIGC environments.", "AI": {"tldr": "OnePiece\u662f\u4e00\u4e2a\u57fa\u4e8eRDMA\u4f18\u5316\u7684\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u63a8\u7406\u7cfb\u7edf\uff0c\u4e13\u4e3a\u591a\u9636\u6bb5AIGC\u5de5\u4f5c\u6d41\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5fae\u670d\u52a1\u5206\u89e3\u548c\u5355\u8fb9RDMA\u901a\u4fe1\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548cCPU\u5f00\u9500\uff0c\u63d0\u9ad8GPU\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709AIGC\u7cfb\u7edf\u5728\u5904\u7406\u5e76\u53d1\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u9762\u4e34\u541e\u5410\u91cf\u3001\u8d44\u6e90\u5229\u7528\u7387\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u6d41\u6c34\u7ebf\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u5fae\u670d\u52a1\uff0c\u5229\u7528\u5355\u8fb9RDMA\u901a\u4fe1\u51cf\u5c11\u8282\u70b9\u95f4\u5ef6\u8fdf\u548cCPU\u5f00\u9500\uff1b\u91c7\u7528\u65b0\u9896\u7684\u53cc\u73af\u7f13\u51b2\u533a\u8bbe\u8ba1\u89e3\u51b3RDMA\u611f\u77e5\u5185\u5b58\u8bbf\u95ee\u7684\u6b7b\u9501\u95ee\u9898\uff1b\u901a\u8fc7\u52a8\u6001\u8282\u70b9\u7ba1\u7406\u5668\u6839\u636e\u5b9e\u65f6\u8d1f\u8f7d\u5f39\u6027\u5206\u914d\u8d44\u6e90\u3002", "result": "\u5728Wan2.1\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u5355\u4f53\u63a8\u7406\u6d41\u6c34\u7ebf\uff0cOnePiece\u5c06GPU\u8d44\u6e90\u6d88\u8017\u964d\u4f4e\u4e8616\u500d\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5bb9\u9519\u4e14\u9ad8\u6548\u7684AIGC\u751f\u4ea7\u73af\u5883\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "OnePiece\u901a\u8fc7RDMA\u4f18\u5316\u548c\u5206\u5e03\u5f0f\u67b6\u6784\u8bbe\u8ba1\uff0c\u4e3a\u5927\u89c4\u6a21AIGC\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u8d44\u6e90\u6548\u7387\uff0c\u662f\u751f\u4ea7\u73af\u5883\u4e2d\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u63a8\u7406\u7cfb\u7edf\u3002"}}
{"id": "2601.20160", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20160", "abs": "https://arxiv.org/abs/2601.20160", "authors": ["Lukas Ottenhof", "Daniel Penner", "Abram Hindle", "Thibaud Lutellier"], "title": "How do Agents Refactor: An Empirical Study", "comment": "Accepted for publication in 23rd International Mining Software Repositories Conference (MSR 2026) : 5 pages, 4 tables", "summary": "Software development agents such as Claude Code, GitHub Copilot, Cursor Agent, Devin, and OpenAI Codex are being increasingly integrated into developer workflows. While prior work has evaluated agent capabilities for code completion and task automation, there is little work investigating how these agents perform Java refactoring in practice, the types of changes they make, and their impact on code quality. In this study, we present the first analysis of agentic refactoring pull requests in Java, comparing them to developer refactorings across 86 projects per group. Using RefactoringMiner and DesigniteJava 3.0, we identify refactoring types and detect code smells before and after refactoring commits. Our results show that agent refactorings are dominated by annotation changes (the 5 most common refactoring types done by agents are annotation related), in contrast to the diverse structural improvements typical of developers. Despite these differences in refactoring types, we find Cursor to be the only model to show a statistically significant increase in refactoring smells.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86Java\u9879\u76ee\u4e2dAI\u4ee3\u7406\uff08\u5982Cursor\u3001Claude Code\u7b49\uff09\u4e0e\u5f00\u53d1\u8005\u7684\u91cd\u6784PR\u5dee\u5f02\uff0c\u53d1\u73b0\u4ee3\u7406\u91cd\u6784\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6ce8\u89e3\u4fee\u6539\u800c\u975e\u7ed3\u6784\u6027\u6539\u8fdb\uff0c\u4e14\u53ea\u6709Cursor\u6a21\u578b\u663e\u8457\u589e\u52a0\u4e86\u4ee3\u7801\u574f\u5473\u9053\u3002", "motivation": "\u968f\u7740AI\u7f16\u7a0b\u4ee3\u7406\uff08Claude Code\u3001GitHub Copilot\u7b49\uff09\u5728\u5f00\u53d1\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u8865\u5168\u548c\u4efb\u52a1\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u4ee3\u7406\u5728\u5b9e\u9645Java\u91cd\u6784\u4e2d\u7684\u8868\u73b0\u3001\u6240\u505a\u53d8\u66f4\u7c7b\u578b\u53ca\u5176\u5bf9\u4ee3\u7801\u8d28\u91cf\u5f71\u54cd\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4ee3\u7406\u91cd\u6784\u548c\u5f00\u53d1\u8005\u91cd\u6784\u7684PR\uff0c\u6bcf\u7ec4\u5206\u679086\u4e2a\u9879\u76ee\u3002\u4f7f\u7528RefactoringMiner\u8bc6\u522b\u91cd\u6784\u7c7b\u578b\uff0c\u4f7f\u7528DesigniteJava 3.0\u68c0\u6d4b\u91cd\u6784\u63d0\u4ea4\u524d\u540e\u7684\u4ee3\u7801\u574f\u5473\u9053\uff0c\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u4ee3\u7406\u91cd\u6784\u4e3b\u8981\u7531\u6ce8\u89e3\u53d8\u66f4\u4e3b\u5bfc\uff08\u524d5\u79cd\u5e38\u89c1\u91cd\u6784\u7c7b\u578b\u5747\u4e3a\u6ce8\u89e3\u76f8\u5173\uff09\uff0c\u4e0e\u5f00\u53d1\u8005\u591a\u6837\u5316\u7684\u7ed3\u6784\u6027\u6539\u8fdb\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\u3002\u5c3d\u7ba1\u91cd\u6784\u7c7b\u578b\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u53ea\u6709Cursor\u6a21\u578b\u663e\u793a\u51fa\u7edf\u8ba1\u4e0a\u663e\u8457\u589e\u52a0\u91cd\u6784\u574f\u5473\u9053\u7684\u73b0\u8c61\u3002", "conclusion": "AI\u4ee3\u7406\u5728Java\u91cd\u6784\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f\u4e0e\u5f00\u53d1\u8005\u663e\u8457\u4e0d\u540c\uff0c\u4e3b\u8981\u5173\u6ce8\u6ce8\u89e3\u5c42\u9762\u7684\u4fee\u6539\u800c\u975e\u7ed3\u6784\u6027\u6539\u8fdb\u3002\u867d\u7136\u5927\u591a\u6570\u4ee3\u7406\u672a\u663e\u8457\u964d\u4f4e\u4ee3\u7801\u8d28\u91cf\uff0c\u4f46Cursor\u6a21\u578b\u7684\u8868\u73b0\u9700\u8981\u7279\u522b\u5173\u6ce8\uff0c\u8fd9\u4e3aAI\u7f16\u7a0b\u5de5\u5177\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2601.20764", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.20764", "abs": "https://arxiv.org/abs/2601.20764", "authors": ["Saeed Akbar", "Muhammad Waqas", "Rahmat Ullah"], "title": "Agentic Fog: A Policy-driven Framework for Distributed Intelligence in Fog Computing", "comment": "09 Pages", "summary": "Fog and edge computing require adaptive control schemes that can handle partial observability, severe latency requirements, and dynamically changing workloads. Recent research on Agentic AI (AAI) increasingly integrates reasoning systems powered by Large Language Models; however, these tools are not applicable to infrastructure-level systems due to their high computational cost, stochastic nature, and poor formal analyzability. In this paper, a generic model, Agentic Fog (AF), is presented, in which fog nodes are represented as policy-driven autonomous agents that communicate via p2p interactions based on shared memory and localized coordination. The suggested architecture decomposes a system's goals into abstract policy guidance and formalizes decentralized fog coordination as an exact potential game. The framework is guaranteed to converge and remain stable under asynchronous updates, bounded-rational best-response dynamics, and node failures. Simulations demonstrate that the AF system achieves lower average latency and adapts more efficiently to varying demand than greedy heuristics and integer linear programming under dynamic conditions. The sensitivity analysis also demonstrates the capability to perform optimally under different memory and coordination conditions.", "AI": {"tldr": "\u63d0\u51faAgentic Fog (AF)\u6a21\u578b\uff0c\u5c06\u96fe\u8282\u70b9\u8868\u793a\u4e3a\u57fa\u4e8e\u7b56\u7565\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u5171\u4eab\u5185\u5b58\u548c\u5c40\u90e8\u534f\u8c03\u8fdb\u884cp2p\u901a\u4fe1\uff0c\u5728\u5f02\u6b65\u66f4\u65b0\u548c\u8282\u70b9\u6545\u969c\u4e0b\u4fdd\u8bc1\u6536\u655b\u7a33\u5b9a\uff0c\u76f8\u6bd4\u8d2a\u5a6a\u542f\u53d1\u5f0f\u548c\u6574\u6570\u7ebf\u6027\u89c4\u5212\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u5b9e\u73b0\u66f4\u4f4e\u5ef6\u8fdf\u548c\u66f4\u597d\u9002\u5e94\u6027\u3002", "motivation": "\u96fe\u8ba1\u7b97\u548c\u8fb9\u7f18\u8ba1\u7b97\u9700\u8981\u80fd\u591f\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3001\u4e25\u683c\u5ef6\u8fdf\u8981\u6c42\u548c\u52a8\u6001\u53d8\u5316\u5de5\u4f5c\u8d1f\u8f7d\u7684\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6848\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684Agentic AI\u5de5\u5177\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u968f\u673a\u6027\u5f3a\u548c\u5f62\u5f0f\u5316\u5206\u6790\u80fd\u529b\u5dee\uff0c\u4e0d\u9002\u7528\u4e8e\u57fa\u7840\u8bbe\u65bd\u7ea7\u7cfb\u7edf\u3002", "method": "\u63d0\u51faAgentic Fog (AF)\u901a\u7528\u6a21\u578b\uff1a1) \u96fe\u8282\u70b9\u8868\u793a\u4e3a\u7b56\u7565\u9a71\u52a8\u7684\u81ea\u4e3b\u4ee3\u7406\uff1b2) \u57fa\u4e8e\u5171\u4eab\u5185\u5b58\u548c\u5c40\u90e8\u534f\u8c03\u8fdb\u884cp2p\u901a\u4fe1\uff1b3) \u5c06\u7cfb\u7edf\u76ee\u6807\u5206\u89e3\u4e3a\u62bd\u8c61\u7b56\u7565\u6307\u5bfc\uff1b4) \u5c06\u53bb\u4e2d\u5fc3\u5316\u96fe\u534f\u8c03\u5f62\u5f0f\u5316\u4e3a\u7cbe\u786e\u52bf\u535a\u5f08\uff1b5) \u4fdd\u8bc1\u5728\u5f02\u6b65\u66f4\u65b0\u3001\u6709\u754c\u7406\u6027\u6700\u4f73\u54cd\u5e94\u52a8\u6001\u548c\u8282\u70b9\u6545\u969c\u4e0b\u7684\u6536\u655b\u7a33\u5b9a\u6027\u3002", "result": "\u4eff\u771f\u8868\u660e\uff1a1) AF\u7cfb\u7edf\u76f8\u6bd4\u8d2a\u5a6a\u542f\u53d1\u5f0f\u548c\u6574\u6570\u7ebf\u6027\u89c4\u5212\u5b9e\u73b0\u66f4\u4f4e\u7684\u5e73\u5747\u5ef6\u8fdf\uff1b2) \u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u66f4\u9ad8\u6548\u5730\u9002\u5e94\u53d8\u5316\u9700\u6c42\uff1b3) \u654f\u611f\u6027\u5206\u6790\u663e\u793a\u5728\u4e0d\u540c\u5185\u5b58\u548c\u534f\u8c03\u6761\u4ef6\u4e0b\u90fd\u80fd\u4fdd\u6301\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "AF\u6a21\u578b\u4e3a\u96fe\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f62\u5f0f\u5316\u53ef\u5206\u6790\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u81ea\u4e3b\u534f\u8c03\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709Agentic AI\u5de5\u5177\u5728\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u95ee\u9898\uff0c\u5728\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2601.20171", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20171", "abs": "https://arxiv.org/abs/2601.20171", "authors": ["Kazuma Yamasaki", "Joseph Ayobami Joshua", "Tasha Settewong", "Mahmoud Alfadel", "Kazumasa Shimari", "Kenichi Matsumoto"], "title": "Who Writes the Docs in SE 3.0? Agent vs. Human Documentation Pull Requests", "comment": "Comments: 5 pages, 5 figures. To appear in MSR 2026 Mining Challenge (April 2026). Code available at https://github.com/NAIST-SE/msr2026-docs-prs-replication", "summary": "As software engineering moves toward SE3.0, AI agents are increasingly used to carry out development tasks and contribute changes to software projects. It is therefore important to understand the extent of these contributions and how human developers review and intervene, since these factors shape the risks of delegating work to AI agents. While recent studies have examined how AI agents support software development tasks (e.g., code generation, issue resolution, and PR automation), their role in documentation tasks remains underexplored-even though documentation is widely consumed and shapes how developers understand and use software.\n  Using the AIDev, we analyze 1,997 documentation-related pull requests (PRs) authored by AI agents and human developers, where documentation PRs are those that create or modify project documentation artifacts. We find that AI agents submit substantially more documentation-related PRs than humans in the studied repositories. We further observe that agent-authored documentation edits are typically integrated with little follow-up modification from humans, raising concerns about review practices and the reliability of agent-generated documentation. Overall, while AI agents already contribute substantially to documentation workflows, our results suggest concerns for emerging challenges for documentation quality assurance and human-AI collaboration in SE3.0.", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u6587\u6863\u4efb\u52a1\u4e2d\u8d21\u732e\u4e86\u5927\u91cfPR\uff0c\u4f46\u4eba\u7c7b\u5ba1\u67e5\u4e0d\u8db3\uff0c\u5f15\u53d1\u6587\u6863\u8d28\u91cf\u62c5\u5fe7", "motivation": "\u968f\u7740SE3.0\u65f6\u4ee3AI\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4e86\u89e3\u5176\u5728\u6587\u6863\u4efb\u52a1\u4e2d\u7684\u8d21\u732e\u7a0b\u5ea6\u4ee5\u53ca\u4eba\u7c7b\u5f00\u53d1\u8005\u5982\u4f55\u5ba1\u67e5\u548c\u5e72\u9884\uff0c\u4ee5\u8bc4\u4f30\u59d4\u6258\u5de5\u4f5c\u7ed9AI\u4ee3\u7406\u7684\u98ce\u9669\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5173\u6ce8AI\u5728\u4ee3\u7801\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\uff0c\u4f46\u6587\u6863\u4efb\u52a1\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528AIDev\u5de5\u5177\u5206\u6790\u4e861,997\u4e2a\u6587\u6863\u76f8\u5173\u7684pull requests\uff08PR\uff09\uff0c\u5305\u62ecAI\u4ee3\u7406\u548c\u4eba\u7c7b\u5f00\u53d1\u8005\u521b\u5efa\u7684\u6587\u6863PR\uff08\u521b\u5efa\u6216\u4fee\u6539\u9879\u76ee\u6587\u6863\uff09\u3002\u6bd4\u8f83\u4e86AI\u4ee3\u7406\u548c\u4eba\u7c7b\u5728\u6587\u6863PR\u63d0\u4ea4\u3001\u5ba1\u67e5\u548c\u96c6\u6210\u65b9\u9762\u7684\u5dee\u5f02\u3002", "result": "AI\u4ee3\u7406\u5728\u7814\u7a76\u7684\u4ed3\u5e93\u4e2d\u63d0\u4ea4\u7684\u6587\u6863\u76f8\u5173PR\u663e\u8457\u591a\u4e8e\u4eba\u7c7b\u3002AI\u4ee3\u7406\u7f16\u5199\u7684\u6587\u6863\u7f16\u8f91\u901a\u5e38\u53ea\u9700\u5f88\u5c11\u7684\u4eba\u7c7b\u540e\u7eed\u4fee\u6539\u5c31\u88ab\u96c6\u6210\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8e\u5ba1\u67e5\u5b9e\u8df5\u548cAI\u751f\u6210\u6587\u6863\u53ef\u9760\u6027\u7684\u62c5\u5fe7\u3002", "conclusion": "\u867d\u7136AI\u4ee3\u7406\u5df2\u7ecf\u5728\u6587\u6863\u5de5\u4f5c\u6d41\u4e2d\u505a\u51fa\u91cd\u8981\u8d21\u732e\uff0c\u4f46\u7814\u7a76\u7ed3\u679c\u8868\u660eSE3.0\u65f6\u4ee3\u6587\u6863\u8d28\u91cf\u4fdd\u8bc1\u548c\u4eba\u673a\u534f\u4f5c\u9762\u4e34\u65b0\u5174\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u5ba1\u67e5\u673a\u5236\u4ee5\u786e\u4fdd\u6587\u6863\u53ef\u9760\u6027\u3002"}}
{"id": "2601.20223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20223", "abs": "https://arxiv.org/abs/2601.20223", "authors": ["Aral de Moor", "Yana Hrynevich", "Hleb Badzeika", "Vladyslav Furda", "Marko Kojic", "Artem Savelev", "Kostadin Cvejoski", "Darya Rovdo", "Ekaterina Garanina"], "title": "Control Models for In-IDE Code Completion", "comment": "6 pages; accepted at IDE'26 co-located with ICSE'26", "summary": "We introduce control models for LLM-powered code completion in JetBrains IDEs: ML classifiers which trigger inference and filter the generated suggestions to better align them with users and reduce unnecessary requests. To this end, we evaluate boosting- and transformer-based architectures on an offline dataset of real code completions with n=98 users. We further evaluate the offline classification performance of our boosting-based approach on a range of syntactically diverse languages; and perform an A/B study in a production environment where they improve completion efficiency and quality metrics. With this study, we hope to demonstrate the potential in using auxiliary models for smarter in-IDE integration of LLM-driven features, highlight fruitful future directions, and open problems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8eJetBrains IDE\u4e2dLLM\u4ee3\u7801\u8865\u5168\u7684\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u89e6\u53d1\u63a8\u7406\u5e76\u8fc7\u6ee4\u5efa\u8bae\uff0c\u4ee5\u63d0\u9ad8\u7528\u6237\u5bf9\u9f50\u5ea6\u548c\u51cf\u5c11\u4e0d\u5fc5\u8981\u8bf7\u6c42\u3002", "motivation": "\u5728IDE\u4e2d\u96c6\u6210LLM\u9a71\u52a8\u7684\u4ee3\u7801\u8865\u5168\u529f\u80fd\u65f6\uff0c\u9700\u8981\u66f4\u597d\u5730\u4e0e\u7528\u6237\u610f\u56fe\u5bf9\u9f50\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u5efa\u8bae\u8bf7\u6c42\uff0c\u63d0\u9ad8\u6574\u4f53\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eboosting\u548ctransformer\u7684\u67b6\u6784\uff0c\u5728\u5305\u542b98\u540d\u7528\u6237\u7684\u771f\u5b9e\u4ee3\u7801\u8865\u5168\u79bb\u7ebf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u591a\u79cd\u8bed\u6cd5\u591a\u6837\u5316\u7684\u8bed\u8a00\u4e0a\u6d4b\u8bd5boosting\u65b9\u6cd5\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u6700\u540e\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u8fdb\u884cA/B\u6d4b\u8bd5\u3002", "result": "\u63a7\u5236\u6a21\u578b\u5728\u79bb\u7ebf\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u5728\u751f\u4ea7\u73af\u5883\u7684A/B\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u4ee3\u7801\u8865\u5168\u7684\u6548\u7387\u548c\u8d28\u91cf\u6307\u6807\u3002", "conclusion": "\u8f85\u52a9\u6a21\u578b\u5728IDE\u4e2d\u667a\u80fd\u96c6\u6210LLM\u9a71\u52a8\u529f\u80fd\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u542f\u793a\uff0c\u5e76\u6307\u51fa\u4e86\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002"}}
{"id": "2601.20240", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20240", "abs": "https://arxiv.org/abs/2601.20240", "authors": ["Anthony Peruma", "Truman Choy", "Gerald Lee", "Italo De Oliveira Santos"], "title": "Understanding npm Developers' Practices, Challenges, and Recommendations for Secure Package Development", "comment": "The 19th IEEE/ACM International Conference on Cooperative and Human Aspects of Software Engineering - Research Track", "summary": "Background: The Node Package Manager (npm) ecosystem plays a vital role in modern software development by providing a vast repository of packages and tools that developers can use to implement their software systems. However, recent vulnerabilities in third-party packages have led to serious security breaches, compromising the integrity of applications that depend on them. Objective: This study investigates how npm package developers perceive and handle security in their work. We examined developers' understanding of security risks, the practices and tools they use, the barriers to stronger security measures, and their suggestions for improving the npm ecosystem's security. Method: We conducted an online survey with 75 npm package developers and undertook a mixed-methods approach to analyzing their responses. Results: While developers prioritize security, they perceive their packages as only moderately secure, with concerns about supply chain attacks, dependency vulnerabilities, and malicious code. Only 40% are satisfied with the current npm security tools due to issues such as alert fatigue. Automated methods such as two-factor authentication and npm audit are favored over code reviews. Many drop dependencies due to abandonment or vulnerabilities, and typically respond to vulnerabilities in their packages by quickly releasing patches. Key barriers include time constraints and high false-positive rates. To improve npm security, developers seek better detection tools, clearer documentation, stronger account protections, and more education initiatives. Conclusion: Our findings will benefit npm package contributors and maintainers by highlighting prevalent security challenges and promoting discussions on best practices to strengthen security and trustworthiness within the npm landscape.", "AI": {"tldr": "npm\u5305\u5f00\u53d1\u8005\u8c03\u67e5\u663e\u793a\uff1a\u5f00\u53d1\u8005\u91cd\u89c6\u5b89\u5168\u4f46\u8ba4\u4e3a\u5305\u4ec5\u4e2d\u7b49\u5b89\u5168\uff0c\u4e3b\u8981\u62c5\u5fe7\u4f9b\u5e94\u94fe\u653b\u51fb\u548c\u4f9d\u8d56\u6f0f\u6d1e\uff1b\u4ec540%\u6ee1\u610f\u73b0\u6709\u5b89\u5168\u5de5\u5177\uff0c\u504f\u597d\u81ea\u52a8\u5316\u65b9\u6cd5\uff1b\u6539\u8fdb\u5efa\u8bae\u5305\u62ec\u66f4\u597d\u68c0\u6d4b\u5de5\u5177\u3001\u6587\u6863\u3001\u8d26\u6237\u4fdd\u62a4\u548c\u6559\u80b2\u3002", "motivation": "npm\u751f\u6001\u7cfb\u7edf\u5728\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7b2c\u4e09\u65b9\u5305\u6f0f\u6d1e\u5bfc\u81f4\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5npm\u5305\u5f00\u53d1\u8005\u5982\u4f55\u770b\u5f85\u548c\u5904\u7406\u5b89\u5168\u95ee\u9898\uff0c\u4e86\u89e3\u4ed6\u4eec\u7684\u5b89\u5168\u98ce\u9669\u8ba4\u77e5\u3001\u5b9e\u8df5\u5de5\u5177\u3001\u969c\u788d\u4ee5\u53ca\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5bf975\u540dnpm\u5305\u5f00\u53d1\u8005\u8fdb\u884c\u5728\u7ebf\u8c03\u67e5\uff0c\u5206\u6790\u4ed6\u4eec\u5bf9\u5b89\u5168\u7684\u7406\u89e3\u3001\u5b9e\u8df5\u3001\u5de5\u5177\u4f7f\u7528\u3001\u969c\u788d\u548c\u6539\u8fdb\u5efa\u8bae\u3002", "result": "\u5f00\u53d1\u8005\u4f18\u5148\u8003\u8651\u5b89\u5168\u4f46\u8ba4\u4e3a\u5305\u4ec5\u4e2d\u7b49\u5b89\u5168\uff0c\u4e3b\u8981\u62c5\u5fe7\u4f9b\u5e94\u94fe\u653b\u51fb\u3001\u4f9d\u8d56\u6f0f\u6d1e\u548c\u6076\u610f\u4ee3\u7801\uff1b\u4ec540%\u6ee1\u610f\u73b0\u6709\u5b89\u5168\u5de5\u5177\uff08\u5b58\u5728\u8b66\u62a5\u75b2\u52b3\u95ee\u9898\uff09\uff1b\u504f\u597d\u81ea\u52a8\u5316\u65b9\u6cd5\uff08\u53cc\u56e0\u7d20\u8ba4\u8bc1\u3001npm audit\uff09\u800c\u975e\u4ee3\u7801\u5ba1\u67e5\uff1b\u5e38\u56e0\u5e9f\u5f03\u6216\u6f0f\u6d1e\u800c\u5220\u9664\u4f9d\u8d56\uff1b\u5bf9\u6f0f\u6d1e\u5feb\u901f\u53d1\u5e03\u8865\u4e01\uff1b\u4e3b\u8981\u969c\u788d\u5305\u62ec\u65f6\u95f4\u9650\u5236\u548c\u9ad8\u8bef\u62a5\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8enpm\u8d21\u732e\u8005\u548c\u7ef4\u62a4\u8005\u4e86\u89e3\u666e\u904d\u5b89\u5168\u6311\u6218\uff0c\u4fc3\u8fdb\u6700\u4f73\u5b9e\u8df5\u8ba8\u8bba\uff0c\u4ee5\u589e\u5f3anpm\u751f\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002\u6539\u8fdb\u65b9\u5411\u5305\u62ec\u66f4\u597d\u7684\u68c0\u6d4b\u5de5\u5177\u3001\u6e05\u6670\u6587\u6863\u3001\u5f3a\u8d26\u6237\u4fdd\u62a4\u548c\u66f4\u591a\u6559\u80b2\u8ba1\u5212\u3002"}}
{"id": "2601.20382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20382", "abs": "https://arxiv.org/abs/2601.20382", "authors": ["Klara Borowa", "Andrzej Zalewski", "Lech Madeyski"], "title": "How Software Engineering Research Overlooks Local Industry: A Smaller Economy Perspective", "comment": "Accepted for ICSE - FOSE 2026 (International Conference on Software Engineering: Future of Software Engineering)", "summary": "The software engineering researchers from countries with smaller economies, particularly non-English speaking ones, represent valuable minorities within the software engineering community. As researchers from Poland, we represent such a country. We analyzed the ICSE FOSE (Future of Software Engineering) community survey through reflexive thematic analysis to show our viewpoint on key software community issues. We believe that the main problem is the growing research-industry gap, which particularly impacts smaller communities and small local companies. Based on this analysis and our experiences, we present a set of recommendations for improvements that would enhance software engineering research and industrial collaborations in smaller economies.", "AI": {"tldr": "\u6ce2\u5170\u7814\u7a76\u8005\u901a\u8fc7ICSE FOSE\u793e\u533a\u8c03\u67e5\u5206\u6790\uff0c\u6307\u51fa\u7814\u7a76-\u4ea7\u4e1a\u9e3f\u6c9f\u5bf9\u5c0f\u578b\u7ecf\u6d4e\u4f53\u8f6f\u4ef6\u793e\u533a\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u4f5c\u4e3a\u6765\u81ea\u6ce2\u5170\uff08\u5c0f\u578b\u7ecf\u6d4e\u4f53\u3001\u975e\u82f1\u8bed\u56fd\u5bb6\uff09\u7684\u7814\u7a76\u8005\uff0c\u4f5c\u8005\u5e0c\u671b\u4ece\u5c11\u6570\u7fa4\u4f53\u89c6\u89d2\u5206\u6790\u8f6f\u4ef6\u5de5\u7a0b\u793e\u533a\u7684\u5173\u952e\u95ee\u9898\uff0c\u7279\u522b\u662f\u7814\u7a76-\u4ea7\u4e1a\u9e3f\u6c9f\u5bf9\u5c0f\u578b\u793e\u533a\u548c\u672c\u5730\u516c\u53f8\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u53cd\u601d\u6027\u4e3b\u9898\u5206\u6790\u6cd5\u5206\u6790ICSE FOSE\uff08\u8f6f\u4ef6\u5de5\u7a0b\u672a\u6765\uff09\u793e\u533a\u8c03\u67e5\u6570\u636e\uff0c\u7ed3\u5408\u6ce2\u5170\u7814\u7a76\u8005\u7684\u5b9e\u9645\u7ecf\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e3b\u8981\u95ee\u9898\u662f\u65e5\u76ca\u6269\u5927\u7684\u7814\u7a76-\u4ea7\u4e1a\u9e3f\u6c9f\uff0c\u8fd9\u5bf9\u5c0f\u578b\u7ecf\u6d4e\u4f53\u7684\u8f6f\u4ef6\u793e\u533a\u548c\u672c\u5730\u516c\u53f8\u9020\u6210\u7279\u522b\u5927\u7684\u5f71\u54cd\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6790\u63d0\u51fa\u4e00\u7cfb\u5217\u6539\u8fdb\u5efa\u8bae\uff0c\u65e8\u5728\u589e\u5f3a\u5c0f\u578b\u7ecf\u6d4e\u4f53\u4e2d\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e0e\u4ea7\u4e1a\u5408\u4f5c\u7684\u8054\u7cfb\u3002"}}
{"id": "2601.20394", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20394", "abs": "https://arxiv.org/abs/2601.20394", "authors": ["Giovanna Broccia", "Maurice H. ter Beek", "Walter Cazzola", "Luca Favalli", "Francesco Bertolotti", "Alessio Ferrari"], "title": "Comprehension vs. Adoption: Evaluating a Language Workbench Through a Family of Experiments", "comment": null, "summary": "Language workbenches are tools that enable the definition, reuse, and composition of programming languages and their ecosystems, aiming to streamline language development. To facilitate their adoption by language designers, the comprehensibility of the language used to define other languages is an important aspect to evaluate. Moreover, considering that language workbenches are relatively new tools, user acceptance emerges as a crucial factor to be accounted for during their assessment. Current literature often neglects user-centred aspects like comprehensibility and acceptance in the assessment of this breed of tools. This paper addresses this gap through a family of experiments assessing Neverlang, a modular language workbench. The study adopts a tailored version of the Method Evaluation Model (MEM) to evaluate the comprehensibility of Neverlang's meta-language and programs, as well as user acceptance in terms of perceived ease of use, perceived usefulness, and intention to use. It also investigates the relationships among these dimensions. The experiments were conducted in three iterations involving participants from academia. The results reveal that users demonstrate sufficient comprehension of Neverlang's meta-language, particularly concerning its syntax, express a favourable perception of its usefulness, and indicate their intention to use it. However, the results also indicate that Neverlang's ease of use remains a challenge. Additionally, variations in the perceived ease of use and perceived usefulness, whether low or high, influence the users' intention to use the tool. Surprisingly, no significant correlation is found between comprehensibility and user acceptance. Notably, higher comprehensibility of the meta-language does not necessarily translate into greater acceptance, underscoring the complex interplay between comprehension and adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30Neverlang\u8bed\u8a00\u5de5\u4f5c\u53f0\u7684\u5143\u8bed\u8a00\u53ef\u7406\u89e3\u6027\u548c\u7528\u6237\u63a5\u53d7\u5ea6\uff0c\u53d1\u73b0\u7528\u6237\u80fd\u5145\u5206\u7406\u89e3\u5176\u8bed\u6cd5\u5e76\u8ba4\u53ef\u5176\u6709\u7528\u6027\uff0c\u4f46\u6613\u7528\u6027\u4ecd\u662f\u6311\u6218\uff0c\u4e14\u53ef\u7406\u89e3\u6027\u4e0e\u63a5\u53d7\u5ea6\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u732e\u5728\u8bc4\u4f30\u8bed\u8a00\u5de5\u4f5c\u53f0\u65f6\u5f80\u5f80\u5ffd\u89c6\u7528\u6237\u4e2d\u5fc3\u65b9\u9762\uff0c\u5982\u53ef\u7406\u89e3\u6027\u548c\u63a5\u53d7\u5ea6\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30Neverlang\u8fd9\u4e00\u6a21\u5757\u5316\u8bed\u8a00\u5de5\u4f5c\u53f0\uff0c\u5173\u6ce8\u5176\u5143\u8bed\u8a00\u53ef\u7406\u89e3\u6027\u548c\u7528\u6237\u63a5\u53d7\u5ea6\u3002", "method": "\u91c7\u7528\u5b9a\u5236\u7248\u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\uff08MEM\uff09\uff0c\u901a\u8fc7\u4e09\u4e2a\u8fed\u4ee3\u5b9e\u9a8c\u8bc4\u4f30Neverlang\u5143\u8bed\u8a00\u548c\u7a0b\u5e8f\u7684\u53ef\u7406\u89e3\u6027\uff0c\u4ee5\u53ca\u7528\u6237\u63a5\u53d7\u5ea6\uff08\u611f\u77e5\u6613\u7528\u6027\u3001\u611f\u77e5\u6709\u7528\u6027\u548c\u4f7f\u7528\u610f\u56fe\uff09\u3002\u5b9e\u9a8c\u6d89\u53ca\u5b66\u672f\u754c\u53c2\u4e0e\u8005\u3002", "result": "\u7528\u6237\u5bf9Neverlang\u5143\u8bed\u8a00\uff08\u7279\u522b\u662f\u8bed\u6cd5\uff09\u8868\u73b0\u51fa\u5145\u5206\u7406\u89e3\uff0c\u5bf9\u5176\u6709\u7528\u6027\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u5e76\u8868\u8fbe\u4f7f\u7528\u610f\u56fe\u3002\u4f46\u6613\u7528\u6027\u4ecd\u662f\u6311\u6218\u3002\u611f\u77e5\u6613\u7528\u6027\u548c\u6709\u7528\u6027\u7684\u53d8\u5316\u5f71\u54cd\u4f7f\u7528\u610f\u56fe\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u53ef\u7406\u89e3\u6027\u4e0e\u7528\u6237\u63a5\u53d7\u5ea6\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u5143\u8bed\u8a00\u53ef\u7406\u89e3\u6027\u9ad8\u5e76\u4e0d\u5fc5\u7136\u5bfc\u81f4\u66f4\u9ad8\u7684\u63a5\u53d7\u5ea6\uff0c\u8868\u660e\u7406\u89e3\u4e0e\u91c7\u7eb3\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u5173\u7cfb\u3002\u867d\u7136Neverlang\u5728\u6709\u7528\u6027\u548c\u4f7f\u7528\u610f\u56fe\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u8981\u6539\u8fdb\u6613\u7528\u6027\u4ee5\u4fc3\u8fdb\u66f4\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2601.20404", "categories": ["cs.SE", "cs.AI", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.20404", "abs": "https://arxiv.org/abs/2601.20404", "authors": ["Jai Lal Lulla", "Seyedmoein Mohsenimofidi", "Matthias Galster", "Jie M. Zhang", "Sebastian Baltes", "Christoph Treude"], "title": "On the Impact of AGENTS.md Files on the Efficiency of AI Coding Agents", "comment": "5 pages, 1 figure, 1 table", "summary": "AI coding agents such as Codex and Claude Code are increasingly used to autonomously contribute to software repositories. However, little is known about how repository-level configuration artifacts affect operational efficiency of the agents. In this paper, we study the impact of AGENTS.md files on the runtime and token consumption of AI coding agents operating on GitHub pull requests. We analyze 10 repositories and 124 pull requests, executing agents under two conditions: with and without an AGENTS.md file. We measure wall-clock execution time and token usage during agent execution. Our results show that the presence of AGENTS.md is associated with a lower median runtime ($\u039428.64$%) and reduced output token consumption ($\u039416.58$%), while maintaining a comparable task completion behavior. Based on these results, we discuss immediate implications for the configuration and deployment of AI coding agents in practice, and outline a broader research agenda on the role of repository-level instructions in shaping the behavior, efficiency, and integration of AI coding agents in software development workflows.", "AI": {"tldr": "AGENTS.md\u6587\u4ef6\u80fd\u663e\u8457\u964d\u4f4eAI\u7f16\u7a0b\u4ee3\u7406\u7684\u8fd0\u884c\u65f6\u95f4\u548ctoken\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u5b8c\u6210\u8d28\u91cf", "motivation": "AI\u7f16\u7a0b\u4ee3\u7406\uff08\u5982Codex\u3001Claude Code\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u81ea\u4e3b\u8d21\u732e\u8f6f\u4ef6\u4ed3\u5e93\uff0c\u4f46\u4eba\u4eec\u5bf9\u4ed3\u5e93\u7ea7\u914d\u7f6e\u5de5\u4ef6\u5982\u4f55\u5f71\u54cd\u4ee3\u7406\u64cd\u4f5c\u6548\u7387\u77e5\u4e4b\u751a\u5c11", "method": "\u5206\u679010\u4e2a\u4ed3\u5e93\u548c124\u4e2apull requests\uff0c\u5728\u6709\u65e0AGENTS.md\u6587\u4ef6\u7684\u4e24\u79cd\u6761\u4ef6\u4e0b\u6267\u884c\u4ee3\u7406\uff0c\u6d4b\u91cf\u6267\u884c\u65f6\u95f4\u548ctoken\u4f7f\u7528\u91cf", "result": "AGENTS.md\u6587\u4ef6\u7684\u5b58\u5728\u4e0e\u8f83\u4f4e\u7684\u4e2d\u4f4d\u6570\u8fd0\u884c\u65f6\u95f4\uff08\u51cf\u5c1128.64%\uff09\u548c\u51cf\u5c11\u7684\u8f93\u51fatoken\u6d88\u8017\uff08\u51cf\u5c1116.58%\uff09\u76f8\u5173\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u8f83\u7684\u4efb\u52a1\u5b8c\u6210\u884c\u4e3a", "conclusion": "AGENTS.md\u6587\u4ef6\u80fd\u663e\u8457\u63d0\u9ad8AI\u7f16\u7a0b\u4ee3\u7406\u7684\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u914d\u7f6e\u548c\u90e8\u7f72\u63d0\u4f9b\u76f4\u63a5\u542f\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u4e8e\u4ed3\u5e93\u7ea7\u6307\u4ee4\u5728\u5851\u9020AI\u7f16\u7a0b\u4ee3\u7406\u884c\u4e3a\u3001\u6548\u7387\u548c\u96c6\u6210\u65b9\u9762\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\u8bae\u7a0b"}}
{"id": "2601.20415", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20415", "abs": "https://arxiv.org/abs/2601.20415", "authors": ["Jon Marcos-Mercad\u00e9", "Unai Lopez-Novoa", "Mikel Ega\u00f1a Aranguren"], "title": "An Empirical Evaluation of Modern MLOps Frameworks", "comment": "Supplementary code is available in the following GitHub repository: https://github.com/Jonmaa/MLOps", "summary": "Given the increasing adoption of AI solutions in professional environments, it is necessary for developers to be able to make informed decisions about the current tool landscape. This work empirically evaluates various MLOps (Machine Learning Operations) tools to facilitate the management of the ML model lifecycle: MLflow, Metaflow, Apache Airflow, and Kubeflow Pipelines. The tools are evaluated by assessing the criteria of Ease of installation, Configuration flexibility, Interoperability, Code instrumentation complexity, result interpretability, and Documentation when implementing two common ML scenarios: Digit classifier with MNIST and Sentiment classifier with IMDB and BERT. The evaluation is completed by providing weighted results that lead to practical conclusions on which tools are best suited for different scenarios.", "AI": {"tldr": "\u5bf9MLflow\u3001Metaflow\u3001Apache Airflow\u548cKubeflow Pipelines\u56db\u79cdMLOps\u5de5\u5177\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u901a\u8fc7MNIST\u6570\u5b57\u5206\u7c7b\u548cIMDB\u60c5\u611f\u5206\u7c7b\u4e24\u4e2a\u573a\u666f\uff0c\u4ece\u5b89\u88c5\u6613\u7528\u6027\u3001\u914d\u7f6e\u7075\u6d3b\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u7b49\u516d\u4e2a\u7ef4\u5ea6\u6bd4\u8f83\uff0c\u63d0\u4f9b\u52a0\u6743\u7ed3\u679c\u548c\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u968f\u7740AI\u89e3\u51b3\u65b9\u6848\u5728\u4e13\u4e1a\u73af\u5883\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u5f00\u53d1\u8005\u9700\u8981\u5bf9\u5f53\u524d\u5de5\u5177\u751f\u6001\u6709\u6e05\u6670\u7684\u4e86\u89e3\uff0c\u4ee5\u4fbf\u4e3aML\u6a21\u578b\u751f\u547d\u5468\u671f\u7ba1\u7406\u505a\u51fa\u660e\u667a\u7684\u5de5\u5177\u9009\u62e9\u51b3\u7b56\u3002", "method": "\u901a\u8fc7\u5b9e\u73b0\u4e24\u4e2a\u5e38\u89c1ML\u573a\u666f\uff08MNIST\u6570\u5b57\u5206\u7c7b\u5668\u548cIMDB+BERT\u60c5\u611f\u5206\u7c7b\u5668\uff09\uff0c\u4ece\u516d\u4e2a\u8bc4\u4f30\u6807\u51c6\uff08\u5b89\u88c5\u6613\u7528\u6027\u3001\u914d\u7f6e\u7075\u6d3b\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u3001\u4ee3\u7801\u63d2\u6869\u590d\u6742\u6027\u3001\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u3001\u6587\u6863\u8d28\u91cf\uff09\u5bf9\u56db\u79cdMLOps\u5de5\u5177\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u63d0\u4f9b\u4e86\u52a0\u6743\u8bc4\u4f30\u7ed3\u679c\uff0c\u660e\u786e\u4e86\u4e0d\u540c\u5de5\u5177\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\uff0c\u4e3a\u5f00\u53d1\u8005\u9009\u62e9\u6700\u9002\u5408\u7279\u5b9a\u9700\u6c42\u7684MLOps\u5de5\u5177\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u4e0d\u540cMLOps\u5de5\u5177\u5404\u6709\u4f18\u52a3\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u4f7f\u7528\u573a\u666f\u548c\u9700\u6c42\uff0c\u5f00\u53d1\u8005\u5e94\u6839\u636e\u5177\u4f53\u9879\u76ee\u9700\u6c42\uff08\u5982\u6613\u7528\u6027\u3001\u7075\u6d3b\u6027\u3001\u6269\u5c55\u6027\u7b49\uff09\u9009\u62e9\u6700\u5408\u9002\u7684\u5de5\u5177\u3002"}}
{"id": "2601.20459", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20459", "abs": "https://arxiv.org/abs/2601.20459", "authors": ["Mugdha Khedkar", "Michael Schlichtig", "Mohamed Soliman", "Eric Bodden"], "title": "Challenges in Android Data Disclosure: An Empirical Study", "comment": "Accepted at MOBILESoft 2026 Research Track", "summary": "Current legal frameworks enforce that Android developers accurately report the data their apps collect. However, large codebases can make this reporting challenging. This paper employs an empirical approach to understand developers' experience with Google Play Store's Data Safety Section (DSS) form.\n  We first survey 41 Android developers to understand how they categorize privacy-related data into DSS categories and how confident they feel when completing the DSS form. To gain a broader and more detailed view of the challenges developers encounter during the process, we complement the survey with an analysis of 172 online developer discussions, capturing the perspectives of 642 additional developers. Together, these two data sources represent insights from 683 developers.\n  Our findings reveal that developers often manually classify the privacy-related data their apps collect into the data categories defined by Google-or, in some cases, omit classification entirely-and rely heavily on existing online resources when completing the form. Moreover, developers are generally confident in recognizing the data their apps collect, yet they lack confidence in translating this knowledge into DSS-compliant disclosures. Key challenges include issues in identifying privacy-relevant data to complete the form, limited understanding of the form, and concerns about app rejection due to discrepancies with Google's privacy requirements.\n  These results underscore the need for clearer guidance and more accessible tooling to support developers in meeting privacy-aware reporting obligations.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u8c03\u67e541\u540dAndroid\u5f00\u53d1\u8005\u548c\u5206\u6790172\u4e2a\u5728\u7ebf\u8ba8\u8bba\uff0c\u53d1\u73b0\u5f00\u53d1\u8005\u5728\u586b\u5199Google Play\u6570\u636e\u5b89\u5168\u90e8\u5206\u8868\u5355\u65f6\u9762\u4e34\u6311\u6218\uff1a\u624b\u52a8\u5206\u7c7b\u9690\u79c1\u6570\u636e\u3001\u4f9d\u8d56\u5728\u7ebf\u8d44\u6e90\u3001\u5bf9\u8bc6\u522b\u6536\u96c6\u7684\u6570\u636e\u6709\u4fe1\u5fc3\u4f46\u7f3a\u4e4f\u8f6c\u6362\u4e3a\u5408\u89c4\u62ab\u9732\u7684\u4fe1\u5fc3\u3002", "motivation": "\u5f53\u524d\u6cd5\u5f8b\u6846\u67b6\u8981\u6c42Android\u5f00\u53d1\u8005\u51c6\u786e\u62a5\u544a\u5e94\u7528\u6536\u96c6\u7684\u6570\u636e\uff0c\u4f46\u5927\u578b\u4ee3\u7801\u5e93\u4f7f\u5f97\u8fd9\u4e00\u62a5\u544a\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5f00\u53d1\u8005\u5728\u586b\u5199Google Play\u6570\u636e\u5b89\u5168\u90e8\u5206(DSS)\u8868\u5355\u65f6\u7684\u5b9e\u9645\u4f53\u9a8c\u548c\u6311\u6218\u3002", "method": "\u91c7\u7528\u5b9e\u8bc1\u7814\u7a76\u65b9\u6cd5\uff1a1) \u8c03\u67e541\u540dAndroid\u5f00\u53d1\u8005\uff0c\u4e86\u89e3\u4ed6\u4eec\u5982\u4f55\u5c06\u9690\u79c1\u76f8\u5173\u6570\u636e\u5206\u7c7b\u5230DSS\u7c7b\u522b\u4ee5\u53ca\u586b\u5199\u8868\u5355\u65f6\u7684\u4fe1\u5fc3\u6c34\u5e73\uff1b2) \u5206\u6790172\u4e2a\u5728\u7ebf\u5f00\u53d1\u8005\u8ba8\u8bba\uff0c\u5305\u542b642\u540d\u5f00\u53d1\u8005\u7684\u89c2\u70b9\u3002\u4e24\u79cd\u6570\u636e\u6e90\u5171\u4ee3\u8868683\u540d\u5f00\u53d1\u8005\u7684\u89c1\u89e3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u5f00\u53d1\u8005\u901a\u5e38\u624b\u52a8\u5c06\u5e94\u7528\u6536\u96c6\u7684\u9690\u79c1\u6570\u636e\u5206\u7c7b\u5230Google\u5b9a\u4e49\u7684\u7c7b\u522b\u4e2d\uff08\u6709\u65f6\u5b8c\u5168\u7701\u7565\u5206\u7c7b\uff09\uff0c\u5e76\u4e25\u91cd\u4f9d\u8d56\u73b0\u6709\u5728\u7ebf\u8d44\u6e90\u586b\u5199\u8868\u5355\u3002\u5f00\u53d1\u8005\u5bf9\u8bc6\u522b\u5e94\u7528\u6536\u96c6\u7684\u6570\u636e\u6709\u4fe1\u5fc3\uff0c\u4f46\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u77e5\u8bc6\u8f6c\u5316\u4e3aDSS\u5408\u89c4\u62ab\u9732\u7684\u4fe1\u5fc3\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u8bc6\u522b\u9690\u79c1\u76f8\u5173\u6570\u636e\u3001\u5bf9\u8868\u5355\u7406\u89e3\u6709\u9650\u3001\u62c5\u5fc3\u56e0\u4e0eGoogle\u9690\u79c1\u8981\u6c42\u4e0d\u7b26\u800c\u5bfc\u81f4\u5e94\u7528\u88ab\u62d2\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u9700\u8981\u66f4\u6e05\u6670\u7684\u6307\u5bfc\u548c\u66f4\u6613\u7528\u7684\u5de5\u5177\u6765\u652f\u6301\u5f00\u53d1\u8005\u6ee1\u8db3\u9690\u79c1\u610f\u8bc6\u62a5\u544a\u4e49\u52a1\u3002\u5f53\u524d\u7684DSS\u8868\u5355\u586b\u5199\u8fc7\u7a0b\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u9700\u8981\u6539\u8fdb\u4ee5\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u51c6\u786e\u3001\u66f4\u81ea\u4fe1\u5730\u5b8c\u6210\u9690\u79c1\u6570\u636e\u62ab\u9732\u3002"}}
{"id": "2601.20615", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20615", "abs": "https://arxiv.org/abs/2601.20615", "authors": ["Yanlin Wang", "Jiadong Wu", "Tianyue Jiang", "Mingwei Liu", "Jiachi Chen", "Chong Wang", "Ensheng Shi", "Xilin Liu", "Yuchi Ma", "Zibin Zheng"], "title": "DRAINCODE: Stealthy Energy Consumption Attacks on Retrieval-Augmented Code Generation via Context Poisoning", "comment": "12 pages, 4 figures", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in code generation by leveraging retrieval-augmented generation (RAG) methods. However, the computational costs associated with LLM inference, particularly in terms of latency and energy consumption, have received limited attention in the security context. This paper introduces DrainCode, the first adversarial attack targeting the computational efficiency of RAG-based code generation systems. By strategically poisoning retrieval contexts through a mutation-based approach, DrainCode forces LLMs to produce significantly longer outputs, thereby increasing GPU latency and energy consumption. We evaluate the effectiveness of DrainCode across multiple models. Our experiments show that DrainCode achieves up to an 85% increase in latency, a 49% increase in energy consumption, and more than a 3x increase in output length compared to the baseline. Furthermore, we demonstrate the generalizability of the attack across different prompting strategies and its effectiveness compared to different defenses. The results highlight DrainCode as a potential method for increasing the computational overhead of LLMs, making it useful for evaluating LLM security in resource-constrained environments. We provide code and data at https://github.com/DeepSoftwareAnalytics/DrainCode.", "AI": {"tldr": "DrainCode\u662f\u4e00\u79cd\u9488\u5bf9RAG\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u901a\u8fc7\u6c61\u67d3\u68c0\u7d22\u4e0a\u4e0b\u6587\u8feb\u4f7fLLM\u751f\u6210\u66f4\u957f\u8f93\u51fa\uff0c\u4ece\u800c\u663e\u8457\u589e\u52a0GPU\u5ef6\u8fdf\u548c\u80fd\u8017\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46LLM\u63a8\u7406\u7684\u8ba1\u7b97\u6210\u672c\uff08\u5ef6\u8fdf\u548c\u80fd\u8017\uff09\u5728\u5b89\u5168\u9886\u57df\u5173\u6ce8\u4e0d\u8db3\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u5b89\u5168\uff0c\u800c\u8ba1\u7b97\u6548\u7387\u5b89\u5168\u88ab\u5ffd\u89c6\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7a81\u53d8\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u7b56\u7565\u6027\u5730\u6c61\u67d3\u68c0\u7d22\u4e0a\u4e0b\u6587\uff0c\u8feb\u4f7fLLM\u751f\u6210\u663e\u8457\u66f4\u957f\u7684\u8f93\u51fa\uff0c\u4ece\u800c\u589e\u52a0GPU\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDrainCode\u80fd\u5b9e\u73b0\u9ad8\u8fbe85%\u7684\u5ef6\u8fdf\u589e\u52a0\u300149%\u7684\u80fd\u8017\u589e\u52a0\u548c3\u500d\u4ee5\u4e0a\u7684\u8f93\u51fa\u957f\u5ea6\u589e\u52a0\u3002\u653b\u51fb\u5728\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u5177\u6709\u901a\u7528\u6027\uff0c\u4e14\u80fd\u7ed5\u8fc7\u591a\u79cd\u9632\u5fa1\u3002", "conclusion": "DrainCode\u662f\u9996\u4e2a\u9488\u5bf9RAG\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u8ba1\u7b97\u6548\u7387\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u63ed\u793a\u4e86LLM\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4e3a\u8bc4\u4f30LLM\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2601.20662", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20662", "abs": "https://arxiv.org/abs/2601.20662", "authors": ["Julien Malka", "Arnout Engelen"], "title": "Lila: Decentralized Build Reproducibility Monitoring for the Functional Package Management Model", "comment": null, "summary": "Ensuring the integrity of software build artifacts is an increasingly important concern for modern software engineering, driven by increasingly sophisticated attacks on build systems, distribution channels, and development infrastructures. Reproducible builds $\\unicode{x2013}$ where binaries built independently from the same source code can be verified to be bit-for-bit identical to the distributed artifacts $\\unicode{x2013}$ provide a principled foundation for transparency and trust in software distribution.\n  Despite their potential, the large-scale adoption of reproducible builds faces two significant challenges: achieving high reproducibility rates across vast software collections and establishing reproducibility monitoring infrastructure that can operate at very large scale. While recent studies have shown that high reproducibility rates are achievable at scale $\\unicode{x2013}$ demonstrated by the Nix ecosystem achieving over 90% reproducibility on more than 80,000 packages $\\unicode{x2013}$ the problem of effective reproducibility monitoring remains largely unsolved.\n  In this work, we address the reproducibility monitoring challenge by introducing Lila, a decentralized system for reproducibility assessment tailored to the functional package management model. Lila enables distributed reporting of build results and aggregation into a reproducibility database, benefiting both practitioners and future empirical build reproducibility studies.", "AI": {"tldr": "Lila\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u53ef\u91cd\u73b0\u6784\u5efa\u76d1\u63a7\u7cfb\u7edf\uff0c\u9488\u5bf9\u529f\u80fd\u5305\u7ba1\u7406\u6a21\u578b\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u62a5\u544a\u548c\u805a\u5408\u6784\u5efa\u7ed3\u679c\u6765\u89e3\u51b3\u5927\u89c4\u6a21\u53ef\u91cd\u73b0\u6784\u5efa\u76d1\u63a7\u7684\u6311\u6218\u3002", "motivation": "\u8f6f\u4ef6\u6784\u5efa\u5b8c\u6574\u6027\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u5927\u89c4\u6a21\u91c7\u7528\u53ef\u91cd\u73b0\u6784\u5efa\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u8de8\u5927\u578b\u8f6f\u4ef6\u96c6\u5408\u5b9e\u73b0\u9ad8\u53ef\u91cd\u73b0\u7387\uff0c\u4ee5\u53ca\u5efa\u7acb\u80fd\u591f\u5728\u5927\u89c4\u6a21\u8fd0\u884c\u7684\u53ef\u91cd\u73b0\u6027\u76d1\u63a7\u57fa\u7840\u8bbe\u65bd\u3002\u867d\u7136Nix\u751f\u6001\u7cfb\u7edf\u5df2\u5c55\u793a\u8d85\u8fc790%\u7684\u53ef\u91cd\u73b0\u7387\uff0c\u4f46\u6709\u6548\u7684\u53ef\u91cd\u73b0\u6027\u76d1\u63a7\u95ee\u9898\u4ecd\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51faLila\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u9488\u5bf9\u529f\u80fd\u5305\u7ba1\u7406\u6a21\u578b\u7684\u53bb\u4e2d\u5fc3\u5316\u53ef\u91cd\u73b0\u6027\u8bc4\u4f30\u7cfb\u7edf\u3002Lila\u652f\u6301\u5206\u5e03\u5f0f\u62a5\u544a\u6784\u5efa\u7ed3\u679c\uff0c\u5e76\u5c06\u8fd9\u4e9b\u7ed3\u679c\u805a\u5408\u5230\u53ef\u91cd\u73b0\u6027\u6570\u636e\u5e93\u4e2d\u3002", "result": "Lila\u7cfb\u7edf\u80fd\u591f\u4e3a\u5b9e\u8df5\u8005\u548c\u672a\u6765\u7684\u7ecf\u9a8c\u6027\u6784\u5efa\u53ef\u91cd\u73b0\u6027\u7814\u7a76\u63d0\u4f9b\u652f\u6301\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u76d1\u63a7\u673a\u5236\u89e3\u51b3\u5927\u89c4\u6a21\u53ef\u91cd\u73b0\u6784\u5efa\u7684\u76d1\u63a7\u6311\u6218\u3002", "conclusion": "Lila\u89e3\u51b3\u4e86\u53ef\u91cd\u73b0\u6784\u5efa\u76d1\u63a7\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u8f6f\u4ef6\u5206\u53d1\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u4fe1\u4efb\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\u652f\u6301\uff0c\u7279\u522b\u9002\u7528\u4e8e\u529f\u80fd\u5305\u7ba1\u7406\u6a21\u578b\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u3002"}}
{"id": "2601.20755", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.20755", "abs": "https://arxiv.org/abs/2601.20755", "authors": ["Bohua Zou", "Debayan Roy", "Dhimankumar Yogesh Airao", "Weihao Xu", "Binqi Sun", "Yutao Liu", "Haibo Chen"], "title": "ProfInfer: An eBPF-based Fine-Grained LLM Inference Profiler", "comment": "Accepted in the 9th Annual Conference on Machine Learning and Systems (MLSys 2026)", "summary": "As large language models (LLMs) move from research to production, understanding how inference engines behave in real time has become both essential and elusive. Unlike general-purpose engines such as ONNX Runtime, today's LLM inference systems offer little operator-level visibility, leaving developers blind to where time and resources go. Even basic questions -- is this workload memory-bound or compute-bound? -- often remain unanswered. To close this gap, we develop a fine-grained, non-intrusive profiling framework for modern LLM inference engines, exemplified by llama.cpp but applicable to similar runtime architectures. Built on extended Berkeley Packet Filter (eBPF) technology, our system dynamically attaches probes to runtime functions across multiple layers -- without modifying or recompiling the source. It transforms collected traces into rich visualizations of operators, graphs, timelines, and hardware counter trends, exposing how dense inference, Mixture-of-Experts routing, and operator offloading behave in practice. With less than 4% runtime overhead and high profiling fidelity, our framework makes LLM inference both transparent and diagnosable, turning performance profiling into a practical tool for optimization, scheduling, and resource-aware deployment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eeBPF\u7684\u7ec6\u7c92\u5ea6\u3001\u975e\u4fb5\u5165\u5f0fLLM\u63a8\u7406\u5f15\u64ce\u6027\u80fd\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u4fee\u6539\u6e90\u4ee3\u7801\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u7b97\u5b50\u7ea7\u53ef\u89c1\u6027\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u7406\u89e3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u7814\u7a76\u8f6c\u5411\u751f\u4ea7\u5e94\u7528\uff0c\u7406\u89e3\u63a8\u7406\u5f15\u64ce\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u4f46\u96be\u4ee5\u5b9e\u73b0\u3002\u73b0\u6709\u7684LLM\u63a8\u7406\u7cfb\u7edf\u7f3a\u4e4f\u7b97\u5b50\u7ea7\u53ef\u89c1\u6027\uff0c\u5f00\u53d1\u8005\u65e0\u6cd5\u4e86\u89e3\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u7684\u5177\u4f53\u5206\u5e03\uff0c\u751a\u81f3\u8fde\u57fa\u672c\u95ee\u9898\uff08\u5982\u5de5\u4f5c\u8d1f\u8f7d\u662f\u5185\u5b58\u53d7\u9650\u8fd8\u662f\u8ba1\u7b97\u53d7\u9650\uff09\u90fd\u96be\u4ee5\u56de\u7b54\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u7684\u4f2f\u514b\u5229\u5305\u8fc7\u6ee4\u5668\uff08eBPF\uff09\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u975e\u4fb5\u5165\u5f0f\u6027\u80fd\u5206\u6790\u6846\u67b6\u3002\u8be5\u7cfb\u7edf\u52a8\u6001\u5730\u5c06\u63a2\u9488\u9644\u52a0\u5230\u8fd0\u884c\u65f6\u51fd\u6570\u7684\u591a\u4e2a\u5c42\u6b21\uff0c\u65e0\u9700\u4fee\u6539\u6216\u91cd\u65b0\u7f16\u8bd1\u6e90\u4ee3\u7801\u3002\u6536\u96c6\u7684\u8ddf\u8e2a\u6570\u636e\u88ab\u8f6c\u6362\u4e3a\u4e30\u5bcc\u7684\u53ef\u89c6\u5316\u56fe\u8868\uff0c\u5305\u62ec\u7b97\u5b50\u3001\u8ba1\u7b97\u56fe\u3001\u65f6\u95f4\u7ebf\u548c\u786c\u4ef6\u8ba1\u6570\u5668\u8d8b\u52bf\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u63ed\u793a\u5bc6\u96c6\u63a8\u7406\u3001\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u548c\u7b97\u5b50\u5378\u8f7d\u5728\u5b9e\u9645\u4e2d\u7684\u884c\u4e3a\u8868\u73b0\u3002\u7cfb\u7edf\u8fd0\u884c\u65f6\u5f00\u9500\u4f4e\u4e8e4%\uff0c\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u7684\u6027\u80fd\u5206\u6790\u80fd\u529b\uff0c\u4f7fLLM\u63a8\u7406\u53d8\u5f97\u900f\u660e\u4e14\u53ef\u8bca\u65ad\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u6027\u80fd\u5206\u6790\u8f6c\u53d8\u4e3a\u5b9e\u7528\u7684\u4f18\u5316\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u8c03\u5ea6\u548c\u8d44\u6e90\u611f\u77e5\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86LLM\u63a8\u7406\u5f15\u64ce\u7f3a\u4e4f\u53ef\u89c1\u6027\u7684\u95ee\u9898\uff0c\u4e3a\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2601.20810", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.20810", "abs": "https://arxiv.org/abs/2601.20810", "authors": ["Shahd Seddik", "Fahd Seddik", "Iman Saberi", "Fatemeh Fard", "Minh Hieu Huynh", "Patanamon Thongtanunam"], "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph", "AI": {"tldr": "\u63d0\u51faProgramming Knowledge Graph (PKG)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u8868\u793a\u548c\u7ec6\u7c92\u5ea6\u68c0\u7d22\u589e\u5f3aLLM\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u89e3\u51b3RAG\u4e2d\u68c0\u7d22\u4e0d\u7cbe\u786e\u548c\u751f\u6210\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u5b58\u5728\u56f0\u96be\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u901a\u8fc7\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u7ecf\u5e38\u9519\u8fc7\u76f8\u5173\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u6a21\u578b\u4e5f\u4f1a\u56e0\u65e0\u5173\u6570\u636e\u4ea7\u751f\u5e7b\u89c9", "method": "\u63d0\u51fa\u7f16\u7a0b\u77e5\u8bc6\u56fe(PKG)\u65b9\u6cd5\uff0c\u5c06\u5916\u90e8\u6570\u636e\u7ed3\u6784\u5316\u4e3a\u66f4\u7ec6\u7c92\u5ea6\u7684\u8282\u70b9\u4ee5\u63d0\u9ad8\u68c0\u7d22\u7c92\u5ea6\u3002\u901a\u8fc7\u6811\u526a\u679d\u6280\u672f\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u91cd\u6392\u5e8f\u673a\u5236\u6574\u5408\u975eRAG\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u5c11\u5e7b\u89c9", "result": "\u5728HumanEval\u548cMBPP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cpass@1\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe20%\uff0c\u5728MBPP\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534734%\u3002PKG\u65b9\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u95ee\u9898\uff0c\u540c\u65f6\u5bf9\u539f\u672c\u6b63\u786e\u7684\u975eRAG\u89e3\u51b3\u65b9\u6848\u5f71\u54cd\u6700\u5c0f", "conclusion": "\u63d0\u51fa\u7684PKG\u65b9\u6cd5\u7ed3\u5408\u91cd\u6392\u5e8f\u673a\u5236\u80fd\u6709\u6548\u89e3\u51b3\u590d\u6742\u4ee3\u7801\u751f\u6210\u95ee\u9898\uff0c\u63d0\u9ad8\u68c0\u7d22\u7cbe\u5ea6\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5df2\u6709\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u7684\u6700\u5c0f\u8d1f\u9762\u5f71\u54cd"}}
