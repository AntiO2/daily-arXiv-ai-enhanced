<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.SE](#cs.SE) [Total: 11]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference](https://arxiv.org/abs/2511.21160)
*Wu Sai,Xia Ruichen,Yang Dingyu,Wang Rui,Lai Huihang,Guan Jiarui,Bai Jiameng,Zhang Dongxiang,Tang Xiu,Xie Zhongle,Lu Peng,Chen Gang*

Main category: cs.DB

TL;DR: MorphingDB是一个任务中心的AI原生数据库系统，在PostgreSQL中自动化模型存储、选择和推理，通过专门的模式和多维张量数据类型实现灵活存储，采用两阶段迁移学习框架进行模型选择，并通过预嵌入和DAG批处理管道优化推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI原生DBMS解决方案存在模型中心设计需要手动选择配置模型导致开发成本高，或任务中心AutoML方法计算成本高且与DBMS集成差的问题。

Method: 1. 引入专门模式和多维张量数据类型支持BLOB-based模型存储；2. 设计两阶段迁移学习框架，通过离线嵌入历史任务构建迁移性子空间，在线投影进行实时任务处理；3. 采用预嵌入与向量共享消除冗余计算，DAG批处理管道与成本感知调度优化推理。

Result: 在9个公共数据集（序列、NLP、图像任务）上，MorphingDB优于AI原生DBMS（EvaDB、Madlib、GaussML）和AutoML平台（AutoGluon、AutoKeras、AutoSklearn），在模型选择的准确性、资源消耗和时间成本之间达到良好平衡，推理吞吐量和资源效率显著提升。

Conclusion: MorphingDB展示了在PostgreSQL中实现任务中心AI原生数据库的可行性，通过自动化模型存储、选择和推理，在准确性、资源效率和时间成本方面取得了显著改进。

Abstract: The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.

</details>


### [2] [HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads](https://arxiv.org/abs/2511.21307)
*Xinyi Zhang,Liang Liang,Anastasia Ailamaki,Jianliang Xu*

Main category: cs.DB

TL;DR: HIRE是一种混合内存索引结构，结合了传统索引的稳健性和学习索引的预测能力，在范围查询吞吐量、尾延迟和整体稳定性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 学习索引在点查询上表现优异，但存在高尾延迟、范围查询性能不佳以及在不同工作负载下效果不一致的问题，需要一种能稳定高效处理各种查询类型的索引结构。

Method: HIRE采用混合方法：(1)自适应混合叶子节点，(2)模型加速的内部节点配合日志更新，(3)非阻塞成本驱动重校准机制，(4)跨层优化的批量加载算法。

Result: 在多个真实数据集上的实验表明，HIRE在混合工作负载下吞吐量最高提升41.7倍，尾延迟降低达98%，在范围查询吞吐量、尾延迟和稳定性方面均优于现有学习索引和传统索引。

Conclusion: HIRE成功地将传统索引的稳健性与学习索引的预测能力相结合，为现代数据库提供了高效且稳定的索引解决方案。

Abstract: Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 提出了一种在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务LLM的解决方案，能够高效处理100-1000个并发请求，延迟开销仅约500毫秒。


<details>
  <summary>Details</summary>
Motivation: AI推理需求增长，特别是在高等教育领域，需要利用现有基础设施的新解决方案。传统HPC模型不适用于同步、面向用户的动态AI应用工作负载。

Method: 在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务大型语言模型(LLM)。

Result: 初始基准测试表明，该架构能够高效扩展处理100、500和1000个并发请求，端到端延迟仅增加约500毫秒的开销。

Conclusion: 提出的解决方案成功解决了传统HPC模型不适用于动态AI应用的问题，实现了高效的LLM服务部署。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [4] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 本文提出了一种基于随机Petri网的方法来分析私有云环境中Nextcloud文件服务器的可用性，评估了四种冗余策略对系统可靠性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中的普及，可靠性成为关键需求，特别是对于寻求公共云替代方案的组织。评估这些系统的可靠性至关重要。

Method: 使用Apache CloudStack在私有云环境中托管Nextcloud文件服务器，通过随机Petri网建模方法分析四种架构配置：基线、主机级冗余、虚拟机冗余以及两者组合。

Result: 结果显示，在主机和虚拟机级别同时实施冗余策略能显著提高可用性并减少预期停机时间。

Conclusion: 提出的方法为评估私有云可用性和支持基础设施设计决策提供了一种有效手段。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [5] [Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks](https://arxiv.org/abs/2511.20834)
*Dionysios Adamopoulos,Anastasia Poulopoulou,Georgios Goumas,Christina Giannoula*

Main category: cs.DC

TL;DR: Spira是一个基于GPU的稀疏卷积引擎，通过利用体素坐标的整数性、有界性和几何连续性等特性，显著提升了3D点云网络中稀疏卷积的性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏卷积引擎未能充分利用体素坐标的关键特性（整数性、空间有界性、几何连续性），导致核映射构建时的预处理和后处理开销过高。

Method: 提出四种关键技术：一次性搜索算法、压缩原生处理方案、双数据流执行机制、网络级并行化策略，以优化核映射构建和特征计算。

Result: 相比现有稀疏卷积引擎，Spira在端到端推理上平均提升1.71倍（最高2.31倍），在逐层执行上平均提升2.13倍（最高3.32倍）。

Conclusion: Spira通过充分利用体素坐标特性，显著降低了稀疏卷积的计算开销，为3D点云处理提供了高效的解决方案。

Abstract: Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.

</details>


### [6] [Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975)
*Yinwei Dai,Zhuofu Chen,Anand Iyer,Ravi Netravali*

Main category: cs.DC

TL;DR: Aragog系统通过动态调整工作流配置来优化多阶段LLM推理任务的性能和成本，在保持精度的同时显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有工作流配置方法在请求执行前固定配置，无法适应系统负载的动态变化，导致在长时间执行过程中配置变得次优。

Method: 将配置选择问题解耦为一次性路由步骤（识别所有保持精度的配置）和廉价每阶段调度器（使用最新系统观察选择配置），并引入加速策略。

Result: 在多样化工作流和模型家族中，Aragog将最大服务吞吐量提升50.0-217.0%，在峰值请求率下将中位延迟降低32.5-78.9%，同时保持与最昂贵配置相当的精度。

Conclusion: Aragog证明了在LLM工作流服务中动态配置适应的有效性，能够显著优化性能成本权衡。

Abstract: Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.

</details>


### [7] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD是一个动态LLM推理系统，通过实时监控负载动态调整prefill和decoding实例分配比例，解决异构工作负载下的生产者-消费者不平衡问题，显著提升系统吞吐量和响应性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理系统将prefill和decoding阶段解耦到不同GPU上，但异构工作负载会导致两个实例类型之间的生产者-消费者不平衡，影响SLO达成。

Method: 提出DOPD系统，基于实时负载监控动态调整prefill-to-decoding比例，结合适当的请求调度策略，解决实例间不平衡和混合长度请求下的资源分配不匹配问题。

Result: 相比vLLM和DistServe，DOPD将系统吞吐量提升至1.5倍，P90 TTFT降低67.5%，P90 TPOT降低22.8%，SLO达成率超过99%且使用更少额外资源。

Conclusion: DOPD通过动态P/D调整技术实现了高效的LLM推理，有效解决了异构工作负载下的资源不平衡问题，显著提升了系统性能。

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [8] [Handling of Memory Page Faults during Virtual-Address RDMA](https://arxiv.org/abs/2511.21018)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文实现了一种与DMA引擎集成的页面错误处理机制，通过硬件-软件协同方案解决RDMA通信中的页面错误问题，避免了传统内存固定方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RDMA技术无法容忍页面错误，通常采用内存固定技术，但这导致编程复杂性增加、内存使用效率低下，且无法完全防止页面错误。

Method: 在ExaNeSt项目中，通过ARM SMMU检测页面错误，开发硬件-软件解决方案，包括修改Linux SMMU驱动、开发新软件库、调整DMA引擎硬件和调度逻辑。

Result: 在ExaNeSt的QFDB平台上进行了实验评估，与内存固定和预错误处理等替代方案进行了比较。

Conclusion: 提出的页面错误处理机制相比传统方法具有明显优势，能够有效解决RDMA通信中的内存管理问题。

Abstract: Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.
  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.
  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.
  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.

</details>


### [9] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine是一个内存感知的细粒度调度框架，通过分块重计算策略解决MoE训练中的内存瓶颈问题，在内存受限的GPU上实现稳定的大规模MoE训练。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型训练面临严重的内存瓶颈，动态令牌路由导致的负载不平衡会造成GPU内存溢出，限制了模型的可扩展性。现有负载均衡方法会牺牲模型精度且在内存受限硬件上失效。

Method: MemFine将令牌分布和专家计算分解为可管理的块，采用分块重计算策略，并通过理论内存模型动态优化以平衡内存效率和吞吐量。

Result: 实验表明，MemFine相比完全重计算基线减少激活内存48.03%，提升吞吐量4.42%，能够在内存受限的GPU上实现稳定的大规模MoE训练。

Conclusion: MemFine有效解决了MoE训练中的内存瓶颈问题，为在有限内存硬件上训练大规模MoE模型提供了可行方案。

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [10] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 通过引入数据冗余改善MLFMA中P2P算子的GPU内存局部性，提出基于局部性度量的分析模型预测性能趋势，在电磁求解器和恒星动力学代码中验证，获得最高7倍内核加速但受限于数据重组开销，端到端应用加速仅为1.04倍。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场(P2P)算子在GPU上因内存局部性差成为性能瓶颈，需要改善内存访问模式以提升性能。

Method: 引入数据冗余减少内存访问分散，提出结合数据量和访问分散度的局部性度量分析模型，在DBIM-MLFMA电磁求解器和PhotoNs-2.0恒星动力学代码中验证。

Result: 内核速度最高提升7倍，缓存行为显著改善，但数据重组开销限制了端到端应用加速仅为1.04倍，分析模型能可靠捕捉不同问题规模和密度下的性能趋势。

Conclusion: 数据冗余可有效提升GPU上P2P算子性能，前提是局部性收益超过数据移动成本，该技术可最小化代码修改注入现有实现。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation](https://arxiv.org/abs/2511.20709)
*Abhijeet Pathak,Suvadra Barua,Dinesh Gudimetla,Rupam Patir,Jiawei Guo,Hongxin Hu,Haipeng Cai*

Main category: cs.SE

TL;DR: DUALGAUGE是首个自动化基准测试框架，用于同时评估LLM生成代码的安全性和正确性，包含DUALGAUGE-BENCH基准套件和代理程序执行器。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在安全代码生成评估方面存在不足，要么只衡量漏洞减少，要么忽视正确性保持，或者在不同数据集上分别评估安全性和功能性，无法实现联合评估。

Method: 开发了DUALGAUGE框架，包含手工验证的安全和功能测试套件，使用沙盒环境运行程序，并基于LLM评估正确性和漏洞行为。

Result: 对十个领先LLM的评估揭示了在正确和安全代码生成方面的关键差距，系统可帮助通过可重复、可扩展的严格评估加速进展。

Conclusion: DUALGAUGE填补了安全代码生成联合评估的空白，为LLM生成的代码提供了同时评估安全性和正确性的标准化方法。

Abstract: Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.

</details>


### [12] [Data-Driven Methods and AI in Engineering Design: A Systematic Literature Review Focusing on Challenges and Opportunities](https://arxiv.org/abs/2511.20730)
*Nehal Afifi,Christoph Wittig,Lukas Paehler,Andreas Lindenmann,Kai Wolter,Felix Leitenberger,Melih Dogru,Patric Grauberger,Tobias Düser,Albert Albers,Sven Matthiesen*

Main category: cs.SE

TL;DR: 本文通过系统文献综述分析了数据驱动方法在产品开发中的应用现状，发现机器学习、统计方法占主导，深度学习呈上升趋势，但在验证阶段应用有限，存在模型可解释性、跨阶段可追溯性等挑战。


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法在产品开发中的应用日益增多但整合零散，缺乏关于在开发生命周期中何时使用何种方法的明确指导。

Method: 采用PRISMA系统文献综述方法，使用V模型作为产品开发框架，对Scopus、Web of Science和IEEE Xplore（2014-2024）的1689条记录进行筛选，最终分析114篇文献。

Result: 监督学习、聚类、回归分析和代理建模在系统设计、实施和集成阶段普遍应用，但在验证阶段贡献有限；深度学习应用呈上升趋势；现有应用面临模型可解释性差、跨阶段可追溯性不足等挑战。

Conclusion: 这是制定设计阶段指南的第一步，后续需要将计算机科学算法映射到工程设计问题和活动中，并开发可解释的混合模型。

Abstract: The increasing availability of data and advancements in computational intelligence have accelerated the adoption of data-driven methods (DDMs) in product development. However, their integration into product development remains fragmented. This fragmentation stems from uncertainty, particularly the lack of clarity on what types of DDMs to use and when to employ them across the product development lifecycle. To address this, a necessary first step is to investigate the usage of DDM in engineering design by identifying which methods are being used, at which development stages, and for what application. This paper presents a PRISMA systematic literature review. The V-model as a product development framework was adopted and simplified into four stages: system design, system implementation, system integration, and validation. A structured search across Scopus, Web of Science, and IEEE Xplore (2014--2024) retrieved 1{,}689 records. After screening, 114 publications underwent full-text analysis. Findings show that machine learning (ML) and statistical methods dominate current practice, whereas deep learning (DL), though still less common, exhibits a clear upward trend in adoption. Additionally, supervised learning, clustering, regression analysis, and surrogate modeling are prevalent in design, implementation, and integration system stages but contributions to validation remain limited. Key challenges in existing applications include limited model interpretability, poor cross-stage traceability, and insufficient validation under real-world conditions. Additionally, it highlights key limitations and opportunities such as the need for interpretable hybrid models. This review is a first step toward design-stage guidelines; a follow-up synthesis should map computer science algorithms to engineering design problems and activities.

</details>


### [13] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: 本文分析了支持"边战边训"(TWYF)模式的先进分布式学习平台的技术需求，通过设计科学研究方法识别了7个关键技术挑战，并展示了如何用现有软件工程模式解决这些挑战。


<details>
  <summary>Details</summary>
Motivation: 现代军事训练需要从传统的战前/战后训练转向"边战边训"的持续学习模式，这要求先进的分布式学习平台具备新的技术能力。

Method: 采用设计科学研究方法：(i)从PfPC/北约文档和近期实践中推导挑战；(ii)定义解决方案目标；(iii)系统性地将挑战映射到已验证的模式。

Result: 识别了7个关键技术挑战：互操作性、弹性、多语言支持、数据安全与隐私、可扩展性、平台独立性、模块化，并通过德国武装部队的国家用例进行了模式说明。

Conclusion: 现有软件工程模式能够满足"边战边训"模式的技术需求，为先进分布式学习平台的开发提供了可行的解决方案框架。

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [14] [Application of machine learning for infrastructure reconstruction programs management](https://arxiv.org/abs/2511.20916)
*Illia Khudiakov,Vladyslav Pliuhin,Sergiy Plankovskyy,Yevgen Tsegelnyk*

Main category: cs.SE

TL;DR: 开发了一个自适应决策支持模型，用于提高工程基础设施重建项目的管理效率，通过系统建模和机器学习预测来优化项目架构和工作分解结构。


<details>
  <summary>Details</summary>
Motivation: 现有工程基础设施重建项目管理工具效率不足，需要开发自适应模型来改进项目架构和工作分解结构的创建过程，以适应不同的项目目标和系统类型。

Method: 结合系统建模工具，使用机器学习和人工神经网络构建自适应模型，包含决策者偏好、决策任务、输入数据集和应用软件组件，通过历史数据训练模型预测目标函数值。

Result: 开发了基于Microsoft Azure Machine Learning Studio的功能组件，确定了神经网络参数并给出了评估结果，模型能够适应不同的工程系统类型和项目目标。

Conclusion: 该自适应模型可有效应用于热力、燃气、电力供应、供水和排水等工程系统重建项目的管理，提高了决策过程的适应性和效率。

Abstract: The purpose of this article is to describe an adaptive decision-making support model aimed at improving the efficiency of engineering infrastructure reconstruction program management in the context of developing the architecture and work breakdown structure of programs. As part of the study, the existing adaptive program management tools are analyzed, the use of infrastructure systems modelling tools is justified for program architecture and WBS creation. Existing models and modelling methods are viewed, and machine learning and artificial neural networks are selected for the model. The main components of the model are defined, which include a set of decision-maker preferences, decision-making tasks, sets of input data, and applied software components of the model. To support decision-making, the adaptive model applies the method of system modeling and predicting the value of the objective function at a given system configuration. Prediction is done using machine learning methods based on a dataset consisting of historical data related to existing engineering systems. The work describes the components of the redistribution of varied model parameters, which modify the model dataset based on the selected object type, which allows adapting the decision-making process to the existing program implementation goals. The functional composition done in Microsoft Azure Machine Learning Studio is described. The neural network parameters and evaluation results are given. The application of the developed adaptive model is possible in the management of programs for the reconstruction of such engineering systems as systems of heat, gas, electricity supply, water supply, and drainage, etc.

</details>


### [15] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: LLMs对软件设计概念的理解存在脆弱性和不对称性：耦合分析在噪声环境中表现脆弱，而内聚分析在指导任务中相对稳健但无指导时也会失效。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs对软件设计核心概念（内聚性和耦合性）的理解鲁棒性，特别是在实际开发环境中。

Method: 通过程序化生成设计不良的代码片段，测试DeepSeek-R1模型家族在不同指导级别（验证、引导、开放式生成）和不同上下文噪声下的表现。

Result: 模型在理想条件下对两个概念有基本理解，但实际知识脆弱且高度不对称：耦合分析在噪声开放式场景中F1分数下降超50%，内聚分析在引导任务中对内部噪声稳健但无指导时失效。

Conclusion: LLMs能可靠识别设计缺陷，但在噪声现实环境中自主推理能力有限，需要更可扩展和鲁棒的程序理解能力。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [16] [SpaceX: Exploring metrics with the SPACE model for developer productivity](https://arxiv.org/abs/2511.20955)
*Sanchit Kaul,Kevin Nhu,Jason Eissayou,Ivan Eser,Victor Borup*

Main category: cs.SE

TL;DR: 本研究通过SPACE框架和统计方法分析开源代码库，发现负面情绪与提交频率正相关，并提出复合生产力评分(CPS)来更全面地衡量开发者效率。


<details>
  <summary>Details</summary>
Motivation: 传统单一维度的生产力启发式方法存在局限性，需要更全面、多维度的生产力评估框架。

Method: 使用广义线性混合模型(GLM)和基于RoBERTa的情感分类，通过挖掘开源代码库数据来构建综合生产力指标。

Result: 发现负面情感状态与提交频率存在显著正相关，表明存在由挫败感驱动的迭代修复循环；贡献者互动拓扑分析比传统基于数量的指标能更好地映射协作动态。

Conclusion: 提出了复合生产力评分(CPS)来解决开发者效率的异质性问题，为生产力评估提供了更全面的框架。

Abstract: This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.

</details>


### [17] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 本文系统研究了10种模型编辑技术用于更新LLMs中过时的API知识，提出了AdaLoRA-L方法，通过区分通用API层和特定API层来提升编辑特异性。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码补全任务中经常生成已弃用的API，因为其训练数据存在时效性问题。重新训练成本高昂，而现有的轻量级模型编辑方法是否能有效更新API知识尚不明确。

Method: 构建EDAPIBench基准，包含70+个弃用API和3000+编辑实例。比较10种SOTA模型编辑技术，并提出AdaLoRA-L方法，通过识别和区分'通用API层'和'特定API层'来限制编辑范围。

Result: AdaLoRA在生成正确API方面表现最佳，但特异性不足。AdaLoRA-L显著提高了特异性，同时在其他评估指标上保持可比性能。

Conclusion: 模型编辑技术可以有效更新LLMs中的API知识，AdaLoRA-L通过分层编辑策略在保持性能的同时显著提升了特异性。

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [18] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: 研究发现Android应用存在地理差异现象：GeoTwins（功能相似但不同地区的应用变体）和App Bundle基础文件的区域差异，这些差异影响安全评估的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 探索移动应用行为的地理差异，揭示现有研究中被忽视的区域性安全、隐私和公平性问题。

Method: 构建分布式应用收集管道，跨多个地区收集并分析数千个应用，识别GeoTwins和App Bundle的区域差异。

Result: 发现81,963个GeoTwins，揭示了应用权限、第三方库和隐私声明的区域差异，以及App Bundle基础文件的隐藏定制化。

Conclusion: 移动软件存在系统性区域差异，这对研究人员、开发者、平台架构师和政策制定者具有重要意义，需要提高透明度和一致性。

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [19] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: 开发者将AI辅助bug检测工具视为"bug侦探"，将可读性评估工具视为"质量教练"，信任取决于解释清晰度、时机和用户控制。研究提出了以人为中心的IDE设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助工具在技术特性上有所进步，但开发者如何心理建模这些工具以及不匹配如何影响信任、控制和采用尚不清楚。

Method: 通过6个共同设计工作坊与58名开发者合作，引出他们对AI辅助bug检测和可读性功能的心理模型。

Result: 开发者将bug检测工具视为只警告关键问题的bug侦探，要求透明度、可操作反馈和信心提示；可读性工具则被视为提供情境化、个性化和渐进指导的质量教练。

Conclusion: 提出了平衡干扰与支持、简洁与深度、自动化与人类能动性的人类中心AI在IDE中的设计原则。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [20] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 首个关于多智能体系统在数据集适应任务中表现的实证研究，评估了基于GPT-4.1和Claude Sonnet 4的Copilot在软件工程研究工件适应中的能力。


<details>
  <summary>Details</summary>
Motivation: 自动化软件工程研究工件的跨数据集适应对于可扩展性和可复现性至关重要，但目前研究较少。多智能体系统有望通过协调推理、代码生成和工具交互来自动化复杂开发工作流。

Method: 使用五阶段评估流水线（文件理解、代码编辑、命令生成、验证和最终执行），评估Copilot在ROCODE和LogHub2.0等基准仓库中的表现，测量成功率，分析失败模式，并评估提示干预策略。

Result: 当前系统能识别关键文件并生成部分适应，但很少产生功能正确的实现。提示干预（特别是提供执行错误信息和参考代码）显著提高了与真实结果的结构相似性（从7.25%到67.14%）。

Conclusion: 研究揭示了当前多智能体LLM系统在数据集适应方面的潜力和局限性，为构建更可靠、自校正的智能体提供了具体方向。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


### [21] [Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead](https://arxiv.org/abs/2511.21382)
*Bei Chu,Yang Feng,Kui Liu,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: 本文对115篇关于使用大语言模型进行单元测试生成的文献进行了系统综述，提出了基于测试生成生命周期的统一分类法，分析了核心生成策略和增强技术。


<details>
  <summary>Details</summary>
Motivation: 传统自动化测试方法缺乏语义信息来生成真实输入和断言，而大语言模型可以利用其代码语义和编程模式的数据驱动知识来弥补这一局限。

Method: 采用系统文献综述方法，分析2021年5月至2025年8月间的115篇出版物，提出基于单元测试生成生命周期的统一分类框架。

Result: 研究发现提示工程已成为主导策略（占89%的研究），迭代验证和修复循环成为确保鲁棒性的标准机制，显著提高了编译和执行通过率。

Conclusion: 尽管取得了进展，但在故障检测能力和标准化评估基准方面仍存在挑战。未来研究应朝着自主测试代理和混合系统方向发展，将大语言模型与传统软件工程工具相结合。

Abstract: Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.

</details>
