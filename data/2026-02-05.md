<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 7]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [StraTyper: Automated Semantic Type Discovery and Multi-Type Annotation for Dataset Collections](https://arxiv.org/abs/2602.04004)
*Christos Koutras,Juliana Freire*

Main category: cs.DB

TL;DR: StraTyper：一种用于数据集集合中列类型发现和多类型标注的成本效益方法，无需预定义语义标签，通过LLM策略性使用实现类型发现。


<details>
  <summary>Details</summary>
Motivation: 现有列类型标注方法需要用户指定封闭的语义类型集合，限制了在领域特定数据集上的应用，且无法处理多类型列。专有LLM成本高且输出不一致。

Method: 通过策略性列聚类、受控类型生成和迭代级联发现，系统性地使用LLM发现针对特定数据集集合定制的类型，平衡类型精度与标注覆盖率，同时最小化LLM成本。

Result: 在真实世界基准测试中，StraTyper为数值和非数值数据发现准确类型，相比商业LLM实现显著成本节省，有效处理多类型列，并提升下游任务（如连接发现和模式匹配）性能。

Conclusion: StraTyper解决了现有列类型标注方法的局限性，提供了一种成本效益高的类型发现和多类型标注方法，无需预定义标签，并能改善下游数据集成任务。

Abstract: Understanding dataset semantics is crucial for effective search, discovery, and integration pipelines. To this end, column type annotation (CTA) methods associate columns of tabular datasets with semantic types that accurately describe their contents, using pre-trained deep learning models or Large Language Models (LLMs). However, existing approaches require users to specify a closed set of semantic types either at training or inference time, hindering their application to domain-specific datasets where pre-defined labels often lack adequate coverage and specificity. Furthermore, real-world datasets frequently contain columns with values belonging to multiple semantic types, violating the single-type assumption of existing CTA methods. While proprietary LLMs have shown effectiveness for CTA, they incur high monetary costs and produce inconsistent outputs for similar columns, leading to type redundancy that negatively affects downstream applications. To address these challenges, we introduce StraTyper, a cost-effective method for column type discovery (CTD) and multi-type annotation (CMTA) in dataset collections. StraTyper eliminates the need for pre-defined semantic labels by systematically employing LLMs to discovery types tailored to the dataset collection at hand. Through strategic column clustering, controlled type generation, and iterative cascading discovery, StraTyper balances type precision with annotation coverage while minimizing LLM costs. Our experimental evaluation-both manual and LLM-assisted-on real-world benchmarks demonstrates that StraTyper discovers accurate types for both numerical and non-numerical data, achieves substantial cost savings compared to commercial LLMs, and effectively handles multi-typed columns. We further show that StraTyper's annotations improve downstream tasks, including join discovery and schema matching, outperforming LLM-only baselines.

</details>


### [2] [PluRel: Synthetic Data unlocks Scaling Laws for Relational Foundation Models](https://arxiv.org/abs/2602.04029)
*Vignesh Kothapalli,Rishabh Ranjan,Valter Hudovernik,Vijay Prakash Dwivedi,Johannes Hoffart,Carlos Guestrin,Jure Leskovec*

Main category: cs.DB

TL;DR: PluRel是一个生成多表关系数据库的框架，通过建模模式图、主外键连接和特征分布，支持合成多样化数据库，并首次观察到关系基础模型在合成数据上的预训练损失呈现幂律缩放规律。


<details>
  <summary>Details</summary>
Motivation: 训练关系基础模型需要多样化的关系数据库，但由于隐私限制，这类数据很少公开。现有的表格数据生成方法难以处理多表结构和主外键连接，因此需要开发能够生成完整关系数据库的框架。

Method: PluRel采用分步方法：1) 用有向图建模数据库模式；2) 用二分图建模表间主外键连接；3) 通过条件因果机制建模表中特征分布。这种设计支持生成多样化数据库，且计算轻量。

Result: 首次观察到：1) RFM预训练损失随合成数据库数量和总预训练标记数呈幂律缩放；2) 增加合成数据库数量能改善对真实数据库的泛化能力；3) 合成预训练能为真实数据库的持续预训练提供强基础模型。

Conclusion: PluRel框架及其结果表明，合成数据缩放是关系基础模型的一个有前景的研究范式，能够解决真实关系数据库稀缺的问题，并支持模型的有效预训练和泛化。

Abstract: Relational Foundation Models (RFMs) facilitate data-driven decision-making by learning from complex multi-table databases. However, the diverse relational databases needed to train such models are rarely public due to privacy constraints. While there are methods to generate synthetic tabular data of arbitrary size, incorporating schema structure and primary--foreign key connectivity for multi-table generation remains challenging. Here we introduce PluRel, a framework to synthesize multi-tabular relational databases from scratch. In a step-by-step fashion, PluRel models (1) schemas with directed graphs, (2) inter-table primary-foreign key connectivity with bipartite graphs, and, (3) feature distributions in tables via conditional causal mechanisms. The design space across these stages supports the synthesis of a wide range of diverse databases, while being computationally lightweight. Using PluRel, we observe for the first time that (1) RFM pretraining loss exhibits power-law scaling with the number of synthetic databases and total pretraining tokens, (2) scaling the number of synthetic databases improves generalization to real databases, and (3) synthetic pretraining yields strong base models for continued pretraining on real databases. Overall, our framework and results position synthetic data scaling as a promising paradigm for RFMs.

</details>


### [3] [Piece of CAKE: Adaptive Execution Engines via Microsecond-Scale Learning](https://arxiv.org/abs/2602.04181)
*Zijie Zhao,Ryan Marcus*

Main category: cs.DB

TL;DR: CAKE系统使用基于反事实的上下文多臂老虎机，为每个数据片段动态选择最优数据库内核，相比静态启发式方法可将工作负载延迟降低最多2倍。


<details>
  <summary>Details</summary>
Motivation: 现有数据库系统依赖静态启发式或最坏情况最优默认值来选择物理实现内核，无法根据输入数据分布动态调整，错失了显著的性能优化机会。

Method: 提出CAKE系统，使用微秒级上下文多臂老虎机学习为每个数据片段选择最优内核；通过利用反事实的廉价性（选择性运行多个内核获取完整反馈）避免传统强化学习的高延迟，并将策略编译为低延迟的遗憾树。

Result: 实验表明，CAKE相比最先进的静态启发式方法，可将端到端工作负载延迟降低最多2倍。

Conclusion: CAKE通过反事实自适应内核执行，实现了对数据库内核选择的动态优化，显著提升了数据库操作性能。

Abstract: Low-level database operators often admit multiple physical implementations ("kernels") that are semantically equivalent but have vastly different performance characteristics depending on the input data distribution. Existing database systems typically rely on static heuristics or worst-case optimal defaults to select these kernels, often missing significant performance opportunities. In this work, we propose CAKE (Counterfactual Adaptive Kernel Execution), a system that learns to select the optimal kernel for each data "morsel" using a microsecond-scale contextual multi-armed bandit. CAKE circumvents the high latency of traditional reinforcement learning by exploiting the cheapness of counterfactuals -- selectively running multiple kernels to obtain full feedback -- and compiling policies into low-latency regret trees. Experimentally, we show that CAKE can reduce end-to-end workload latency by up to 2x compared to state-of-the-art static heuristics.

</details>


### [4] [LatentTune: Efficient Tuning of High Dimensional Database Parameters via Latent Representation Learning](https://arxiv.org/abs/2602.04190)
*Sein Kwon,Youngwan Jo,Seungyeon Choi,Jieun Lee,Huijun Jin,Sanghyun Park*

Main category: cs.DB

TL;DR: LatentTune是一种新颖的数据库参数调优方法，通过数据增强策略减少训练数据生成时间，构建潜在空间压缩所有参数信息以实现全配置空间优化，并整合外部指标信息针对目标工作负载进行精确调优。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在数据库参数调优中存在三个主要限制：1) 需要大量时间生成训练数据集；2) 只能优化参数子集而非全配置空间；3) 依赖相似工作负载信息而非直接利用目标工作负载信息。

Method: LatentTune采用数据增强策略减少数据生成时间，构建潜在空间压缩所有数据库参数信息以实现全配置空间优化，并将外部指标信息整合到潜在空间中，针对实际目标工作负载进行精确调优。

Result: 在MySQL和RocksDB的四个工作负载上，LatentTune优于基线模型，RocksDB性能提升最高达1332%，MySQL吞吐量提升11.82%且延迟降低46.01%。

Conclusion: LatentTune通过创新的潜在空间构建和数据增强策略，有效解决了现有数据库参数调优方法的局限性，实现了更高效、更精确的全配置空间优化。

Abstract: As data volumes continue to grow, optimizing database performance has become increasingly critical, making the implementation of effective tuning methods essential. Among various approaches, database parameter tuning has proven to be a highly effective means of enhancing performance. Recent studies have shown that machine learning techniques can successfully optimize database parameters, leading to significant performance improvements. However, existing methods still face several limitations. First, they require substantial time to generate large training datasets. Second, to cope with the challenges of highdimensional optimization, they typically optimize only a subset of parameters rather than the full configuration space. Third, they often rely on information from similar workloads instead of directly leveraging information from the target workload. To address these limitations, we propose LatentTune, a novel approach that differs fundamentally from traditional methods. To reduce the time required for data generation, LatentTune incorporates a data augmentation strategy. Furthermore, it constructs a latent space that compresses information from all database parameters, enabling the optimization of the full configuration space. In addition, LatentTune integrates external metric information into the latent space, allowing for precise tuning tailored to the actual target workload. Experimental results demonstrate that LatentTune outperforms baseline models across four workloads on MySQL and RocksDB, achieving up to 1332% improvement for RocksDB and 11.82% throughput gain with 46.01% latency reduction for MySQL.

</details>


### [5] [Data Agents: Levels, State of the Art, and Open Problems](https://arxiv.org/abs/2602.04261)
*Yuyu Luo,Guoliang Li,Ju Fan,Nan Tang*

Main category: cs.DB

TL;DR: 本文提出了首个数据代理的分层分类法（L0-L5），从无自主性到完全自主，旨在澄清当前"数据代理"术语的模糊性，并为该领域提供发展路线图。


<details>
  <summary>Details</summary>
Motivation: 当前"数据代理"术语使用不一致，将简单的查询响应助手与完全自主的"数据科学家"混为一谈。这种模糊性模糊了能力边界和责任归属，使用户、系统构建者和监管者难以理解"数据代理"的实际能力范围。

Method: 提出了首个数据代理的分层分类法（L0-L5），并基于此分类法引入了生命周期和层级驱动的数据代理视图。具体包括：1）介绍L0-L5分类法及关键进化跃迁；2）回顾数据管理、准备和分析中的代表性L0-L2系统；3）突出展示新兴的Proto-L3系统；4）讨论面向主动式（L4）和生成式（L5）数据代理的前瞻性研究挑战。

Result: 建立了一个从L0（无自主性）到L5（完全自主性）的六层分类框架，为当前系统提供了实用地图，并为未来十年的数据代理发展制定了研究路线图。

Conclusion: 该分类法旨在澄清数据代理的能力边界，为领域提供统一的术语框架，既映射了当前系统现状，又为未来研究指明了方向，有助于推动数据代理从简单助手向真正自主系统的演进。

Abstract: Data agents are an emerging paradigm that leverages large language models (LLMs) and tool-using agents to automate data management, preparation, and analysis tasks. However, the term "data agent" is currently used inconsistently, conflating simple query responsive assistants with aspirational fully autonomous "data scientists". This ambiguity blurs capability boundaries and accountability, making it difficult for users, system builders, and regulators to reason about what a "data agent" can and cannot do.
  In this tutorial, we propose the first hierarchical taxonomy of data agents from Level 0 (L0, no autonomy) to Level 5 (L5, full autonomy). Building on this taxonomy, we will introduce a lifecycleand level-driven view of data agents. We will (1) present the L0-L5 taxonomy and the key evolutionary leaps that separate simple assistants from truly autonomous data agents, (2) review representative L0-L2 systems across data management, preparation, and analysis, (3) highlight emerging Proto-L3 systems that strive to autonomously orchestrate end-to-end data workflows to tackle diverse and comprehensive data-related tasks under supervision, and (4) discuss forward-looking research challenges towards proactive (L4) and generative (L5) data agents. We aim to offer both a practical map of today's systems and a research roadmap for the next decade of data-agent development.

</details>


### [6] [Identifying knowledge gaps in biodiversity data and their determinants at the regional level](https://arxiv.org/abs/2602.04314)
*Didier Alard,Anaïs Guéry*

Main category: cs.DB

TL;DR: 该研究分析了法国最大行政区生物多样性开放数据库中的知识缺口，发现无脊椎动物比脊椎动物知识缺口更大，知识缺口主要受地点可达性而非生态吸引力影响，并强调生物多样性治理对知识分布的影响。


<details>
  <summary>Details</summary>
Motivation: 生物多样性开放数据库虽然整合了多种数据源，但存在空间和分类学上的知识分布不均。理解这些知识缺口的决定因素有助于更好地利用开放获取数据，并指导资源分配以减少偏差。

Method: 使用完整性和无知度两个指标评估8个分类群（5个脊椎动物和3个无脊椎动物）的知识缺口。在法国最大行政区的整个区域及其三个前子区域层面分析数据，识别知识缺口的潜在驱动因素。

Result: 无脊椎动物比脊椎动物知识缺口更大；知识缺口主要受地点可达性变量影响，而非生态吸引力；农业压力对无脊椎动物的影响比对脊椎动物更显著；生物多样性治理（地方资金和区域政治决策）对知识分布有重要影响。

Conclusion: 建议将生物多样性资金重新导向采样不足的分类群和勘探不足的地区。如不可行，用户应使用知识缺口地图进行空间采样偏差校正，以获得更准确的物种分布理解。强调了生物多样性治理对开放数据库知识分布的影响。

Abstract: Biodiversity open-access databases are valuable resources in the structuring and accessibility of species occurrence data. By compiling different data sources, they reveal the uneven spatial distribution of knowledge, with areas or taxonomic groups better prospected than others. Understanding the determinants of spatial and taxonomic knowledge gaps helps in informing the use of open-access data. Here, we identified knowledge gaps' determinants within a French regional biodiversity database, in the largest administrative region in France. Knowledge gaps were assessed using two metrics, completeness and ignorance scores, for 8 taxonomic groups covering five vertebrates and three invertebrates groups. The data was analyzed for the entire region, but also at the level of the three former sub-regions, to identify the potential drivers that may account for knowledge gaps' determinants. Our findings show that invertebrates were characterized by higher knowledge gaps than vertebrates. Overall, knowledge gaps are influenced by variables related to sites' accessibility rather than ecological appeal across both metrics. All groups shared similar determinants of gaps, except for the impact of agricultural pressure which is found to be more significant for invertebrates than vertebrates. Ultimately, our study emphasizes the impact of biodiversity governance, through local funding and regional political decisions, on knowledge distribution in open-access databases. We recommend limiting these biases by redirecting biodiversity funding towards under-sampled taxonomic groups and under-prospected areas. When not possible, users of data extracted from these databases should correct for spatial-sampling biases (SSP) using knowledge gaps' maps in order to get a more accurate understanding of species occurrence.

</details>


### [7] [The Stretto Execution Engine for LLM-Augmented Data Systems](https://arxiv.org/abs/2602.04430)
*Gabriele Sanmartino,Matthias Urban,Paolo Papotti,Carsten Binnig*

Main category: cs.DB

TL;DR: Stretto是一个LLM增强的数据系统执行引擎，通过联合优化算子实现和误差预算分配，在保证查询质量的同时高效权衡运行时与准确性。


<details>
  <summary>Details</summary>
Motivation: LLM增强的数据系统支持对结构化和非结构化数据进行语义查询，但使用LLM驱动的算子执行查询时存在运行时与准确性的基本权衡问题，需要系统化的解决方案。

Method: 1. 将查询规划建模为约束优化问题，使用基于梯度的优化器联合选择算子实现和分配误差预算；2. 引入新颖的KV缓存技术，实现从稀疏设计空间到密集连续运行时-准确性权衡的物理算子谱系。

Result: 实验表明Stretto在持续满足质量保证的同时，性能优于现有最先进系统。

Conclusion: Stretto通过端到端查询保证和整体化权衡管理，为LLM增强数据系统提供了高效的执行引擎解决方案。

Abstract: LLM-augmented data systems enable semantic querying over structured and unstructured data, but executing queries with LLM-powered operators introduces a fundamental runtime--accuracy trade-off. In this paper, we present Stretto, a new execution engine that provides end-to-end query guarantees while efficiently navigating this trade-off in a holistic manner. For this, Stretto formulates query planning as a constrained optimization problem and uses a gradient-based optimizer to jointly select operator implementations and allocate error budgets across pipelines. Moreover, to enable fine-grained execution choices, Stretto introduces a novel idea on how KV-caching can be used to realize a spectrum of different physical operators that transform a sparse design space into a dense continuum of runtime--accuracy trade-offs. Experiments show that Stretto outperforms state-of-the-art systems while consistently meeting quality guarantees.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Accountability in Open Source Software Ecosystems: Workshop Report](https://arxiv.org/abs/2602.04026)
*Nandini Sharma,Thomas Bock,Rich Bowen,Sayeed Choudhury,Brian Fitzgerald,Matt Germonprez,Jim Herbsleb,James Howison,Tom Hughes,Min Kyung Lee,Stephanie Lieggi,Andreas Liesenfeld,Georg Link,Nicholas Matsakis,Audris Mockus,Narayan Ramasubbu,Christopher Robinson,Gregorio Robles,Nithya Ruff,Sonali Shah,Igor Steinmacher,Bogdan Vasilescu,Stephen Walli,Christopher Yoo*

Main category: cs.SE

TL;DR: 该论文通过举办专家研讨会，探讨开源软件生态系统中利益相关者的问责机制，旨在建立研究议程和实践参与框架。


<details>
  <summary>Details</summary>
Motivation: 开源生态系统包含多元利益相关者（非营利组织、志愿者、用户、企业等），其需求和动机多样且可能冲突。目前缺乏对这些利益相关者的识别、需求理解和问责机制的系统研究。

Method: 在卡内基梅隆大学举办为期两天的线下研讨会，召集24位研究和工作于开源社区的专家学者和实践者，进行探索性讨论。

Result: 研讨会启动了关于开源软件生态系统中问责机制重要问题的对话，为相关研究议程和实践参与提供了初步框架和启发。

Conclusion: 开源软件生态系统需要建立有效的问责机制来协调多元利益相关者，研讨会为此领域的研究和实践提供了重要起点和方向。

Abstract: Open source software ecosystems are composed of a variety of stakeholders including but not limited to non-profit organizations, volunteer contributors, users, and corporations. The needs and motivations of these stakeholders are often diverse, unknown, and sometimes even conflicting given the engagement and investment of both volunteers and corporate actors. Given this, it is not clear how open source communities identify and engage with their stakeholders, understand their needs, and hold themselves accountable to those needs. We convened 24 expert scholars and practitioners studying and working with open source software communities for an exploratory workshop discussion on these ideas. The workshop titled "Accountability and Open Source Software Ecosystems" was organized on Oct 14-15 on campus in Carnegie Mellon University, Pittsburgh, PA. The purpose of this in-person workshop was to initiate conversations that explore important and urgent questions related to the role of accountability in open source software ecosystems, and to inspire an exciting research agenda and meaningful stakeholder engagement ideas for practitioners.

</details>


### [9] [Exploring the Potential of Large Language Models in Simulink-Stateflow Mutant Generation](https://arxiv.org/abs/2602.04066)
*Pablo Valle,Shaukat Ali,Aitor Arrieta*

Main category: cs.SE

TL;DR: LLMs生成Simulink-Stateflow模型突变体的自动化方法，比传统方法快13倍，产生更少等价和重复突变体，质量更高。


<details>
  <summary>Details</summary>
Motivation: 传统突变分析在Simulink-Stateflow模型中面临冗余、等价和非可执行突变体的问题，现有机器学习方法受限于训练数据和可扩展性，因此探索LLMs在生成高质量领域特定突变体方面的潜力。

Method: 开发自动化管道将Simulink-Stateflow模型转换为结构化JSON表示，系统评估8个最先进LLMs的不同突变和提示策略，使用少量样本提示结合中低温度值。

Result: 在4个Simulink-Stateflow模型上生成38,400个LLM突变体，比手动工程基线快13倍，产生显著更少的等价和重复突变体，突变体质量持续更优。

Conclusion: LLMs能高效生成高质量Simulink-Stateflow突变体，少量样本提示加中低温度值效果最佳，提供开源工具和完整数据集促进可重复性和未来研究。

Abstract: Mutation analysis is a powerful technique for assessing test-suite adequacy, yet conventional approaches suffer from generating redundant, equivalent, or non-executable mutants. These challenges are particularly amplified in Simulink-Stateflow models due to the hierarchical structure these models have, which integrate continuous dynamics with discrete-event behaviors and are widely deployed in safety-critical Cyber-Physical Systems (CPSs). While prior work has explored machine learning and manually engineered mutation operators, these approaches remain constrained by limited training data and scalability issues. Motivated by recent advances in Large Language Models (LLMs), we investigate their potential to generate high-quality, domain-specific mutants for Simulink-Stateflow models. We develop an automated pipeline that converts Simulink-Stateflow models to structured JSON representations and systematically evaluates different mutation and prompting strategies across eight state-of-the-art LLMs. Through a comprehensive empirical study involving 38,400 LLM-generated mutants across four Simulink-Stateflow models, we demonstrate that LLMs generate mutants up to 13x faster than a manually engineered mutation-based baseline while producing significantly fewer equivalent and duplicate mutants and consistently achieving superior mutant quality. Moreover, our analysis reveals that few-shot prompting combined with low-to-medium temperature values yields optimal results. We provide an open-source prototype tool and release our complete dataset to facilitate reproducibility and advance future research in this domain.

</details>


### [10] [I Can't Believe It's Not a Valid Exploit](https://arxiv.org/abs/2602.04165)
*Derin Gezgin,Amartya Das,Shinhae Kim,Zhengdong Huang,Nevena Stojkovic,Claire Wang*

Main category: cs.SE

TL;DR: LLM生成PoC漏洞利用代码时，使用静态分析工具指导可提升21%成功率，但71.5%的PoC实际无效，现有验证机制存在误导性


<details>
  <summary>Details</summary>
Motivation: 评估LLM在安全漏洞检测和PoC生成任务中的有效性，特别是验证静态分析工具提供的指导是否能真正提高PoC生成成功率

Method: 开发PoC-Gym框架，用于Java安全漏洞的PoC生成和系统验证；使用Claude Sonnet 4、GPT-5 Medium和gpt-oss-20b等LLM，结合静态分析工具提供指导

Result: 使用静态分析指导比基线FaultLine提高21%成功率，但手动检查发现71.5%的PoC是无效的，表明当前验证机制存在严重误导

Conclusion: LLM生成的PoC成功率报告具有误导性，需要更严格的验证机制来准确评估实际有效性

Abstract: Recently Large Language Models (LLMs) have been used in security vulnerability detection tasks including generating proof-of-concept (PoC) exploits. A PoC exploit is a program used to demonstrate how a vulnerability can be exploited. Several approaches suggest that supporting LLMs with additional guidance can improve PoC generation outcomes, motivating further evaluation of their effectiveness. In this work, we develop PoC-Gym, a framework for PoC generation for Java security vulnerabilities via LLMs and systematic validation of generated exploits. Using PoC-Gym, we evaluate whether the guidance from static analysis tools improves the PoC generation success rate and manually inspect the resulting PoCs. Our results from running PoC-Gym with Claude Sonnet 4, GPT-5 Medium, and gpt-oss-20b show that using static analysis for guidance and criteria lead to 21% higher success rates than the prior baseline, FaultLine. However, manual inspection of both successful and failed PoCs reveals that 71.5% of the PoCs are invalid. These results show that the reported success of LLM-based PoC generation can be significantly misleading, which is hard to detect with current validation mechanisms.

</details>


### [11] [SOGPTSpotter: Detecting ChatGPT-Generated Answers on Stack Overflow](https://arxiv.org/abs/2602.04185)
*Suyu Ma,Chunyang Chen,Hourieh Khalajzadeh,John Grundy*

Main category: cs.SE

TL;DR: SOGPTSpotter使用Siamese神经网络结合BigBird模型和Triplet损失函数，检测Stack Overflow上的ChatGPT生成答案，性能优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: Stack Overflow上ChatGPT生成答案数量激增，导致错误和不可靠信息传播，但检测AI生成内容仍具挑战性，需要有效识别方法。

Method: 提出SOGPTSpotter方法，采用Siamese神经网络架构，利用BigBird模型和Triplet损失函数，使用人类答案、参考答案和ChatGPT答案的三元组进行训练。

Result: SOGPTSpotter在检测ChatGPT生成的Stack Overflow答案方面，性能优于GPTZero、DetectGPT、GLTR、BERT、RoBERTa和GPT-2等基线方法，并通过消融研究验证了模型有效性。

Conclusion: 该方法在文本长度影响、对抗攻击鲁棒性、跨领域泛化能力等方面表现良好，实际案例研究证明其能有效帮助Stack Overflow管理员识别和删除可疑的ChatGPT生成答案。

Abstract: Stack Overflow is a popular Q&A platform where users ask technical questions and receive answers from a community of experts. Recently, there has been a significant increase in the number of answers generated by ChatGPT, which can lead to incorrect and unreliable information being posted on the site. While Stack Overflow has banned such AI-generated content, detecting whether a post is ChatGPT-generated remains a challenging task. We introduce a novel approach, SOGPTSpotter, that employs Siamese Neural Networks, leveraging the BigBird model and the Triplet loss, to detect ChatGPT-generated answers on Stack Overflow. We use triplets of human answers, reference answers, and ChatGPT answers. Our empirical evaluation reveals that our approach outperforms well-established baselines like GPTZero, DetectGPT, GLTR, BERT, RoBERTa, and GPT-2 in identifying ChatGPT-synthesized Stack Overflow responses. We also conducted an ablation study to show the effectiveness of our model. Additional experiments were conducted to assess various factors, including the impact of text length, the model's robustness against adversarial attacks, and its generalization capabilities across different domains and large language models. We also conducted a real-world case study on Stack Overflow. Using our tool's recommendations, Stack Overflow moderators were able to identify and take down ChatGPT-suspected generated answers, demonstrating the practical applicability and effectiveness of our approach.

</details>


### [12] [Semantic Consensus Decoding: Backdoor Defense for Verilog Code Generation](https://arxiv.org/abs/2602.04195)
*Guang Yang,Xing Hu,Xiang Chen,Xin Xia*

Main category: cs.SE

TL;DR: 针对Verilog代码生成的LLM后门攻击防御方法，通过语义共识解码在推理时检测并抑制恶意触发，将攻击成功率从89%降至3%以下


<details>
  <summary>Details</summary>
Motivation: 硬件设计中的LLM易受后门攻击，一旦硬件制造完成，漏洞无法修复。现有防御方法要么需要训练数据（不实用），要么难以应对语义隐蔽的触发。攻击者倾向于在非功能需求中嵌入触发而非功能规范。

Method: 提出语义共识解码(SCD)，包含两个核心组件：1) 功能需求提取：从用户规范中识别关键功能需求；2) 共识解码：基于完整用户规范和提取的功能需求自适应融合输出分布，当分布显著分歧时自动抑制可疑组件。

Result: 在三种代表性后门攻击上的实验表明，SCD将平均攻击成功率从89%降至3%以下，且对生成质量影响可忽略。

Conclusion: 通过利用攻击者在非功能需求中嵌入触发的偏见，SCD提供了一种有效的推理时被动防御方法，能够显著降低后门攻击成功率而不损害生成质量。

Abstract: Large language models (LLMs) for Verilog code generation are increasingly adopted in hardware design, yet remain vulnerable to backdoor attacks where adversaries inject malicious triggers during training to induce vulnerable hardware designs. Unlike patchable software vulnerabilities, hardware trojans become irreversible once fabricated, making remediation extremely costly or impossible. Existing active defenses require access to training data, impractical for third-party LLM users, while passive defenses struggle against semantically stealthy triggers that naturally blend into design specifications. In this paper, we hypothesize that under the requirements of both effectiveness and stealthiness, attackers are strongly biased toward embedding triggers in non-functional requirements (e.g., style modifiers, quality descriptors) rather than functional specifications that determine hardware behavior. Exploiting this insight, we propose Semantic Consensus Decoding (SCD), an inference-time passive defense with two key components: (1) functional requirement extraction that identifies essential requirements from user specifications, and (2) consensus decoding that adaptively fuses output distributions based on full user specifications and extracted functional requirements. When these distributions diverge significantly, SCD automatically suppresses suspicious components. Extensive experiments with three representative backdoor attacks demonstrate that SCD reduces average attack success rate from 89% to under 3% with negligible impact on generation quality.

</details>


### [13] [Why Agentic-PRs Get Rejected: A Comparative Study of Coding Agents](https://arxiv.org/abs/2602.04226)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Shane Mclntosh,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 研究比较不同AI编程代理生成的PR被拒绝原因，发现AI-PR有7种特有拒绝模式，且不同代理有特定失败模式，67.9%被拒PR缺乏明确反馈。


<details>
  <summary>Details</summary>
Motivation: AI编程代理（Agentic coding）日益流行，但之前研究发现AI生成的PR接受率较低。虽然已探索单个代理（Claude Code）的拒绝原因，但不同代理间的拒绝原因差异尚未比较。这种比较很重要，因为不同代理用于不同目的，可能导致代理特定的失败模式。

Method: 分析AIDev数据集中的654个被拒绝PR，涵盖5个编程代理和人类基准。识别拒绝模式，比较不同代理间的差异，并提出启发式方法处理缺乏明确反馈的情况。

Result: 发现7种仅出现在AI-PR中的拒绝模式（包括对AI生成代码的不信任）。观察到代理特定模式（如Devin自动撤回不活跃PR），反映代理配置和使用差异。67.9%被拒PR缺乏明确审阅者反馈，使拒绝原因难以确定。提出的启发式方法可减少此类情况。

Conclusion: 不同AI编程代理有特定失败模式，需要针对性的改进。大量被拒PR缺乏反馈是研究挑战，提出的启发式方法为未来研究提供实用预处理步骤。研究强调了理解代理特定失败模式的重要性。

Abstract: Agentic coding -- software development workflows in which autonomous coding agents plan, implement, and submit code changes with minimal human involvement -- is rapidly gaining traction. Prior work has shown that Pull Requests (PRs) produced using coding agents (Agentic-PRs) are accepted less often than PRs that are not labeled as agentic (Human-PRs). The rejection reasons for a single agent (Claude Code) have been explored, but a comparison of how rejection reasons differ between Agentic-PRs generated by different agents has not yet been performed. This comparison is important since different coding agents are often used for different purposes, which can lead to agent-specific failure patterns. In this paper, we inspect 654 rejected PRs from the AIDev dataset covering five coding agents, as well as a human baseline. Our results show that seven rejection modes occur only in Agentic-PRs, including distrust of AI-generated code. We also observe agent-specific patterns (e.g., automated withdrawal of inactive PRs by Devin), reflecting differences in how agents are configured and used in practice. Notably, a large proportion of rejected PRs (67.9%) lack explicit reviewer feedback, making their rejection reasons difficult to determine. To mitigate this issue, we propose a set of heuristics that reduce the proportion of such cases, offering a practical preprocessing step for future studies of PR rejection in agentic coding.

</details>


### [14] [ProxyWar: Dynamic Assessment of LLM Code Generation in Game Arenas](https://arxiv.org/abs/2602.04296)
*Wenjun Peng,Xinyu Wang,Qi Wu*

Main category: cs.SE

TL;DR: ProxyWar是一个通过将LLM生成的智能体嵌入多样化竞争游戏环境来系统评估代码生成质量的框架，超越了传统静态基准测试，揭示了基准分数与动态实际性能之间的显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成方面的评估主要依赖静态基准和简单指标，无法充分反映其在真实动态环境中的实际效果，需要更全面、竞争性的评估方法。

Method: ProxyWar框架将LLM生成的智能体嵌入多样化的竞争游戏环境中，结合自动化测试、迭代代码修复和多智能体锦标赛，全面评估程序的功能正确性和运行特性。

Result: 应用于多种先进代码生成模型和游戏环境后，发现基准分数与动态环境中的实际性能存在显著差异，揭示了传统评估方法忽视的局限性和改进机会。

Conclusion: ProxyWar为基于竞争的代码生成评估奠定了基础，未来可应用于LLM驱动的算法发现、自适应问题解决以及实用效率和鲁棒性研究，包括模型超越手工智能体的潜力。

Abstract: Large language models (LLMs) have revolutionized automated code generation, yet the evaluation of their real-world effectiveness remains limited by static benchmarks and simplistic metrics. We present ProxyWar, a novel framework that systematically assesses code generation quality by embedding LLM-generated agents within diverse, competitive game environments. Unlike existing approaches, ProxyWar evaluates not only functional correctness but also the operational characteristics of generated programs, combining automated testing, iterative code repair, and multi-agent tournaments to provide a holistic view of program behavior. Applied to a range of state-of-the-art coders and games, our approach uncovers notable discrepancies between benchmark scores and actual performance in dynamic settings, revealing overlooked limitations and opportunities for improvement. These findings highlight the need for richer, competition-based evaluation of code generation. Looking forward, ProxyWar lays a foundation for research into LLM-driven algorithm discovery, adaptive problem solving, and the study of practical efficiency and robustness, including the potential for models to outperform hand-crafted agents. The project is available at https://github.com/xinke-wang/ProxyWar.

</details>


### [15] [Model-Driven Legacy System Modernization at Scale](https://arxiv.org/abs/2602.04341)
*Tobias Böhm,Jens Guan Su Tien,Mohini Nonnenmann,Tom Schoonbaert,Bart Carpels,Andreas Biesdorf*

Main category: cs.SE

TL;DR: 提出一种基于模型驱动的遗留系统现代化方法，通过技术无关的中间模型实现从遗留代码到现代平台的迁移，并在工业.NET应用中验证了核心UI组件可半自动迁移。


<details>
  <summary>Details</summary>
Motivation: 解决遗留系统现代化过程中面临的高风险、高成本问题，特别是将基于旧版.NET Framework和ASP.NET MVC的大型工业应用迁移到现代Web技术栈。

Method: 采用四阶段模型驱动流程：分析、丰富、合成、转换。构建技术无关的中间模型，捕获结构、依赖关系和语义元数据，通过转换规则实现功能行为和非功能特性的保留。

Result: 成功将大型工业应用的核心UI组件和页面结构半自动迁移到现代Web技术栈，保持功能行为和非功能特性，生成的代码库具有更高的可维护性和可扩展性。

Conclusion: 基于模型的抽象降低了现代化风险和成本，支持可扩展、可追溯的遗留应用现代化。方法可推广到类似现代化场景，促进迁移模式的重用，但自定义布局组合仍需手动适配。

Abstract: This experience report presents a model-driven approach to legacy system modernization that inserts an enriched, technology-agnostic intermediate model between the legacy codebase and the modern target platform, and reports on its application and evaluation. The four-stage process of analysis, enrichment, synthesis, and transition systematically extracts, abstracts, and transforms system artifacts. We apply our approach to a large industrial application built on legacy versions of the .NET Framework and ASP.NET MVC and show that core user interface components and page structures can be migrated semi-automatically to a modern web stack while preserving functional behavior and essential non-functional qualities. By consolidating architectural knowledge into explicit model representations, the resulting codebase exhibits higher maintainability and extensibility, thereby improving developer experience. Although automation is effective for standard patterns, migration of bespoke layout composites remains challenging and requires targeted manual adaptation. Our contributions are: (i) an end-to-end model-driven process, (ii) an enriched intermediate model that captures structure, dependencies, and semantic metadata, (iii) transformation rules that preserve functional behavior and essential non-functional qualities, and (iv) application and evaluation of the approach in an industrial setting. Overall, model-based abstractions reduce risk and effort while supporting scalable, traceable modernization of legacy applications. Our approach generalizes to comparable modernization contexts and promotes reuse of migration patterns.

</details>


### [16] [Generative AI in Systems Engineering: A Framework for Risk Assessment of Large Language Models](https://arxiv.org/abs/2602.04358)
*Stefan Otten,Philipp Reis,Philipp Rigoll,Joshua Ransiek,Tobias Schürmann,Jacob Langner,Eric Sax*

Main category: cs.SE

TL;DR: 本文提出了LLM风险评估框架(LRF)，用于评估大语言模型在系统工程环境中的应用风险，通过自主性和影响两个维度进行分类，支持风险感知的LLM部署。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在工程生命周期中提供了重要机会，但组织在评估LLM使用风险方面面临重大挑战，导致集成不一致、故障模式未知和可扩展性有限的问题。

Method: 引入LLM风险评估框架(LRF)，通过两个基本维度对LLM应用进行分类：自主性（从辅助支持到完全自动化决策）和影响（反映错误或误导性输出对工程过程和系统元素的潜在严重性）。

Result: LRF能够在整个开发生命周期中一致地确定相应的风险水平，支持组织识别适当的验证策略、人工监督水平和所需对策，确保安全透明的部署。

Conclusion: LRF为复杂工程环境中风险感知的LLM采用提供了基础，代表了系统工程中标准化AI保证实践的第一步，有助于将AI技术的快速发展与可靠性、可追溯性和受控过程集成等既定工程原则对齐。

Abstract: The increasing use of Large Language Models (LLMs) offers significant opportunities across the engineering lifecycle, including requirements engineering, software development, process optimization, and decision support. Despite this potential, organizations face substantial challenges in assessing the risks associated with LLM use, resulting in inconsistent integration, unknown failure modes, and limited scalability. This paper introduces the LLM Risk Assessment Framework (LRF), a structured approach for evaluating the application of LLMs within Systems Engineering (SE) environments. The framework classifies LLM-based applications along two fundamental dimensions: autonomy, ranging from supportive assistance to fully automated decision making, and impact, reflecting the potential severity of incorrect or misleading model outputs on engineering processes and system elements. By combining these dimensions, the LRF enables consistent determination of corresponding risk levels across the development lifecycle. The resulting classification supports organizations in identifying appropriate validation strategies, levels of human oversight, and required countermeasures to ensure safe and transparent deployment. The framework thereby helps align the rapid evolution of AI technologies with established engineering principles of reliability, traceability, and controlled process integration. Overall, the LRF provides a basis for risk-aware adoption of LLMs in complex engineering environments and represents a first step toward standardized AI assurance practices in systems engineering.

</details>


### [17] [AgenticAKM : Enroute to Agentic Architecture Knowledge Management](https://arxiv.org/abs/2602.04445)
*Rudra Dhar,Karthik Vaidhyanathan,Vasudeva Varma*

Main category: cs.SE

TL;DR: 本文提出AgenticAKM，一种基于智能代理的架构知识管理方法，通过分解复杂任务为可管理的子任务，自动生成架构决策记录（ADRs）。


<details>
  <summary>Details</summary>
Motivation: 架构知识管理（AKM）对软件项目至关重要，但通常是一个繁琐的过程，开发者和架构师不愿采用。虽然大语言模型（LLMs）提供了自动化机会，但简单的单提示方法受限于上下文长度限制，无法理解架构知识的分布式特性。

Method: 提出AgenticAKM方法，将架构恢复和文档生成的复杂问题分解为可管理的子任务。专门化的代理（提取、检索、生成、验证）在结构化工作流中协作生成架构知识。具体实例化为从代码仓库生成架构决策记录（ADRs）。

Result: 通过对29个仓库的用户研究验证，结果表明该代理方法能生成更好的ADRs，是一种有前景且实用的AKM自动化方法。

Conclusion: AgenticAKM方法通过智能代理协作，有效解决了传统AKM的局限性，为架构知识管理的自动化提供了可行方案。

Abstract: Architecture Knowledge Management (AKM) is crucial for maintaining current and comprehensive software Architecture Knowledge (AK) in a software project. However AKM is often a laborious process and is not adopted by developers and architects. While LLMs present an opportunity for automation, a naive, single-prompt approach is often ineffective, constrained by context limits and an inability to grasp the distributed nature of architectural knowledge. To address these limitations, we propose an Agentic approach for AKM, AgenticAKM, where the complex problem of architecture recovery and documentation is decomposed into manageable sub-tasks. Specialized agents for architecture Extraction, Retrieval, Generation, and Validation collaborate in a structured workflow to generate AK. To validate we made an initial instantiation of our approach to generate Architecture Decision Records (ADRs) from code repositories. We validated our approach through a user study with 29 repositories. The results demonstrate that our agentic approach generates better ADRs, and is a promising and practical approach for automating AKM.

</details>


### [18] [What's in a Benchmark? The Case of SWE-Bench in Automated Program Repair](https://arxiv.org/abs/2602.04449)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 对SWE-Bench两个排行榜（Lite和Verified）的首次全面研究，分析了提交者背景、使用技术、LLM选择和方案开放性，发现行业主导、Claude模型领先的现状。


<details>
  <summary>Details</summary>
Motivation: 随着AI驱动的自动程序修复技术快速发展，SWE-Bench已成为评估修复系统的重要基准。然而，缺乏对排行榜提交情况的系统性分析，不清楚谁在提交、使用什么技术、方案是否开放，这限制了社区对技术发展趋势的理解。

Method: 对SWE-Bench两个排行榜（Lite和Verified）的79个和133个提交进行综合分析，从提交者身份（行业/学术）、背后产品、使用的LLM模型、方案开放性等多个维度进行统计分析。

Result: 1. 大多数提交来自行业，特别是小型公司和大型上市公司，这些提交通常获得最佳结果；2. 学术贡献虽然通常开源且保持竞争力；3. 专有LLM（特别是Claude系列）明显占主导地位，Claude 4 Sonnet在两个排行榜上都取得了最先进的结果。

Conclusion: 研究揭示了SWE-Bench生态系统中行业主导、专有模型领先的现状，这些发现可以为未来基准驱动的研究提供指导，促进更大的透明度和多样性。

Abstract: The rapid progress in Automated Program Repair (APR) has been fueled by advances in AI, particularly large language models (LLMs) and agent-based systems. SWE-Bench is a benchmark designed to evaluate repair systems using real issues mined from popular open-source Python repositories. Its public leaderboards-SWE-Bench Lite and Verified-have become central platforms for tracking progress and comparing solutions. In this paper, we present the first comprehensive study of these two leaderboards, examining who is submitting solutions, the products behind the submissions, the LLMs employed, and the openness of the approaches. We analyze 79 entries submitted to Lite leaderboard and 133 to Verified. Our results show that most entries on both leaderboards originate from industry, particularly small companies and large publicly traded companies. These submissions often achieve top results, although academic contributions-typically open source-also remain competitive. We also find a clear dominance of proprietary LLMs, especially Claude family, with state-of-the-art results on both leaderboards currently achieved by Claude 4 Sonnet. These findings offer insights into the SWE-Bench ecosystem that can guide greater transparency and diversity in future benchmark-driven research.

</details>


### [19] [A Framework of Critical Success Factors for Agile Software Development](https://arxiv.org/abs/2602.04467)
*Ridewaan Hanslo,Maureen Tanner*

Main category: cs.SE

TL;DR: 这篇系统文献综述通过分析53项研究，识别出敏捷软件开发项目的21个关键成功因素，并将其分为组织、人员、技术、流程和项目五大主题，构建了理论框架。


<details>
  <summary>Details</summary>
Motivation: 尽管敏捷软件开发很流行，但实现持续的项目成功仍然具有挑战性。需要识别影响敏捷项目成功的关键因素，为研究和实践提供指导。

Method: 采用系统文献综述方法，分析53项主要研究。运用主题综合与内容分析技术，识别关键成功因素并进行分类。

Result: 识别出21个关键成功因素，分为五大主题：组织、人员、技术、流程和项目。团队效能和项目管理是最常被引用的因素。构建了理论框架来解释这些因素如何影响项目成功。

Conclusion: 研究为研究人员和从业者提供了有价值的见解，指导未来研究验证这些发现，并使用定量方法测试提出的框架。

Abstract: Despite the popularity of Agile software development, achieving consistent project success remains challenging. This systematic literature review identifies critical success factors (CSFs) in Agile projects by analyzing 53 primary studies. Employing thematic synthesis with content analysis, our analysis yielded 21 CSFs categorized into five themes: organizational, people, technical, process, and project. Team effectiveness and project management emerged as the most frequently cited CSFs, highlighting the importance of people and process factors. These interpreted themes and factors contributed to the development of a theoretical framework to identify how these factors contribute to project success. This study offers valuable insights for researchers and practitioners, guiding future research to validate these findings and test the proposed framework using quantitative methods.

</details>


### [20] [Towards Structured, State-Aware, and Execution-Grounded Reasoning for Software Engineering Agents](https://arxiv.org/abs/2602.04640)
*Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 本文主张软件工程智能体应从被动反应式设计转向结构化、状态感知、执行基础推理，以提升长时程任务中的连贯性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程智能体主要基于对话历史和最新响应进行被动决策，缺乏显式结构和持久状态记忆，导致难以进行长时程推理、跨步骤保持连贯理解、适应新证据或将执行反馈整合到系统状态的心理模型中。

Method: 提出通过显式结构、持久演化状态以及执行基础反馈的集成，构建下一代软件工程智能体。具体包括：1）引入显式结构以组织推理过程；2）维护持久且可演化的状态记忆；3）将执行反馈与心理推理模型相结合。

Result: 本文为开发下一代软件工程智能体提供了初步路线图，旨在使其能更有效地执行现实世界任务，实现更连贯可靠的长时程推理。

Conclusion: 软件工程智能体需要从被动反应式设计转向结构化、状态感知、执行基础推理，这是提升其在复杂长时程任务中表现的关键方向，为未来研究提供了重要指导框架。

Abstract: Software Engineering (SE) agents have shown promising abilities in supporting various SE tasks. Current SE agents remain fundamentally reactive, making decisions mainly based on conversation history and the most recent response. However, this reactive design provides no explicit structure or persistent state within the agent's memory, making long-horizon reasoning challenging. As a result, SE agents struggle to maintain a coherent understanding across reasoning steps, adapt their hypotheses as new evidence emerges, or incorporate execution feedback into the mental reasoning model of the system state.
  In this position paper, we argue that, to further advance SE agents, we need to move beyond reactive behavior toward a structured, state-aware, and execution-grounded reasoning. We outline how explicit structure, persistent and evolving state, and the integration of execution-grounded feedback can help SE agents perform more coherent and reliable reasoning in long-horizon tasks. We also provide an initial roadmap for developing next-generation SE agents that can more effectively perform real-world tasks.

</details>


### [21] [Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation](https://arxiv.org/abs/2602.04726)
*Marian Kica,Lukas Radosky,David Slivka,Karin Kubinova,Daniel Dovhun,Tomas Uhercik,Erik Bircak,Ivan Polasek*

Main category: cs.SE

TL;DR: 论文提出两种基于智能代理的AI解决方案：一是从详细需求描述自动生成测试场景，采用星型拓扑的代理架构；二是针对软件工程文档的检索任务，通过专用LLM代理实现搜索、问答、变更追踪和文档摘要等功能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现引发了软件开发模式的重大变革，软件工程研究产生了大量工具和方法。本文旨在通过引入智能代理AI解决方案，为软件工程中的具体任务提供新的自动化方法。

Method: 1. 测试场景生成：采用星型拓扑架构，由监督代理在中心协调多个专业工作代理，从详细需求描述自动生成测试场景。2. 文档检索：为每个用例（搜索、问答、变更追踪、文档摘要）配备专用的基于LLM的代理，处理所有相关子任务。

Result: 1. 在真实世界示例中展示了测试场景生成方案的能力。2. 开发了能够对单个软件开发相关文档集执行多种用例的文档检索解决方案。

Conclusion: 论文展示了智能代理AI在软件工程任务中的实际应用潜力，并暗示了未来研究方向的前景。

Abstract: The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.

</details>


### [22] [Demonstrating ARG-V's Generation of Realistic Java Benchmarks for SV-COMP](https://arxiv.org/abs/2602.04786)
*Charles Moloney,Robert Dyer,Elena Sherman*

Main category: cs.SE

TL;DR: ARG-V工具自动生成SV-COMP格式的Java验证基准程序，在68个新基准上，四个主流Java验证器的准确率和召回率均下降，表明新基准能更全面地评估验证工具的现实适用性。


<details>
  <summary>Details</summary>
Motivation: SV-COMP竞赛的验证工具评估结果受基准程序组成影响。当扩展基准库时，需要确保新增程序能揭示验证器与现有基准不同的行为，以减少竞赛结果的外部有效性威胁。

Method: 应用ARG-V工具自动生成符合SV-COMP格式的Java验证基准程序，并基于新生成的68个现实基准测试四个主流Java验证器的性能。

Result: 在所有四个主流Java验证器上，新基准集上的准确率和召回率均低于现有基准套件，表明新基准能揭示验证器在现实场景中的性能局限。

Conclusion: ARG-V工具能增强验证工具评估的全面性和现实性，为验证器开发者提供了改进工具现实软件适用性的路线图。

Abstract: The SV-COMP competition provides a state-of-the-art platform for evaluating software verification tools on a standardized set of verification tasks. Consequently, verifier development outcomes are influenced by the composition of program benchmarks included in SV-COMP. When expanding this benchmark corpus, it is crucial to consider whether newly added programs cause verifiers to exhibit behavior distinct from that observed on existing benchmarks. Doing so helps mitigate external threats to the validity of the competition's results.
  In this paper, we present the application of the ARG-V tool for automatically generating Java verification benchmarks in the SV-COMP format. We demonstrate that, on a newly generated set of 68 realistic benchmarks, all four leading Java verifiers decrease in accuracy and recall compared to their performance on the existing benchmark suite. These findings highlight the potential of ARG-V to enhance the comprehensiveness and realism of verification tool evaluation, while also providing a roadmap for verifier developers aiming to improve their tools' applicability to real-world software.

</details>


### [23] [Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software](https://arxiv.org/abs/2602.04799)
*Nils Chur,Thorsten Berger,Einar Broch Johnsen,Andrzej Wąsowski*

Main category: cs.SE

TL;DR: 研究发现机器人控制器软件实现存在理论与实际脱节问题，离散化处理随意，测试验证不足，缺乏理论保证的系统验证


<details>
  <summary>Details</summary>
Motivation: 控制器是机器人系统的关键组件，控制理论为标准设计提供安全保证，但软件实现中的复杂性常被忽视。控制器设计在连续空间，而软件在离散空间执行，这削弱了理论保证。尽管控制理论和建模研究广泛，但控制器实现及其理论保证在实际软件系统中如何确保的问题很少受到关注。

Method: 调查了184个开源机器人软件中的真实控制器实现，分析了它们的应用背景、实现特征和用于确保正确性的测试方法。

Result: 发现实现通常以临时方式处理离散化，导致实时可靠性问题。时间不一致、缺乏适当的错误处理、对实时约束考虑不足等挑战使问题更加复杂。测试实践肤浅，没有使用理论保证的系统验证，导致预期行为与实际行为可能存在不一致。

Conclusion: 研究结果强调需要改进实现指南和严格的验证技术，以确保机器人控制器在实际应用中的可靠性和安全性。

Abstract: A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.

</details>


### [24] [Do Developers Read Type Information? An Eye-Tracking Study on TypeScript](https://arxiv.org/abs/2602.04824)
*Samuel W. Flint,Robert Dyer,Bonita Sharif*

Main category: cs.SE

TL;DR: 研究发现开发者在使用TypeScript时，并不会因为类型注解的存在而更频繁地查看包含类型注解的代码行，这对工具设计、开发标准和教育有重要启示。


<details>
  <summary>Details</summary>
Motivation: 静态类型注解已被证明能帮助开发者完成多种编程任务，即使不使用静态类型检查也是如此。假设这是因为开发者将类型注解视为内联文档。本研究旨在提供证据证明开发者确实将类型注解作为内联文档使用，以理解开发者如何使用类型信息，帮助设计更好的开发工具并指导教育决策。

Method: 对26名本科生进行眼动追踪研究，观察他们在TypeScript语言中进行代码理解和错误定位任务时，是否会阅读类型注解。

Result: 研究发现，无论是在代码总结还是错误定位任务中，当存在类型注解时，开发者并不会更频繁地直接查看包含类型注解或类型声明的代码行。

Conclusion: 研究结果对工具开发者改进类型信息的可用性、开发社区建立良好的类型注解使用标准、以及教育领域加强阅读模式的刻意教学都有重要意义。

Abstract: Statically-annotated types have been shown to aid developers in a number of programming tasks, and this benefit holds true even when static type checking is not used. It is hypothesized that this is because developers use type annotations as in-code documentation. In this study, we aim to provide evidence that developers use type annotations as in-code documentation. Understanding this hypothesized use will help to understand how, and in what contexts, developers use type information; additionally, it may help to design better development tools and inform educational decisions. To provide this evidence, we conduct an eye tracking study with 26 undergraduate students to determine if they read type annotations during code comprehension and bug localization in the TypeScript language. We found that developers do not look directly at lines containing type annotations or type declarations more often when they are present, in either code summarization or bug localization tasks. The results have implications for tool builders to improve the availability of type information, the development community to build good standards for use of type annotations, and education to enforce deliberate teaching of reading patterns.

</details>


### [25] [When Code Becomes Abundant: Redefining Software Engineering Around Orchestration and Verification](https://arxiv.org/abs/2602.04830)
*Karina Kohl,Luigi Carro*

Main category: cs.SE

TL;DR: 软件工程需要从代码构建转向意图表达、架构控制和系统验证，以应对AI自动化和硬件能耗约束的双重压力


<details>
  <summary>Details</summary>
Motivation: 软件工程面临AI自动化（降低代码生产成本）和硬件能耗约束（增加失败成本）的双重压力，传统以代码构建和流程管理为核心的软件工程已不再足够

Method: 重新定义软件工程的核心：从代码构建转向意图表达、架构控制和系统验证，强调在自动化环境下的人类判断

Result: 提出了软件工程的根本性转变，将问责制崩溃作为核心风险，需要对研究重点、教育课程和工业实践进行根本性改变

Conclusion: 软件工程必须从生产导向的领域转变为以自动化环境下人类判断为中心的学科，这对研究、实践和教育都有深远影响

Abstract: Software Engineering (SE) faces simultaneous pressure from AI automation (reducing code production costs) and hardware-energy constraints (amplifying failure costs). We position that SE must redefine itself around human discernment-intent articulation, architectural control, and verification-rather than code construction. This shift introduces accountability collapse as a central risk and requires fundamental changes to research priorities, educational curricula, and industrial practices. We argue that Software Engineering, as traditionally defined around code construction and process management, is no longer sufficient. Instead, the discipline must be redefined around intent articulation, architectural control, and systematic verification. This redefinition shifts Software Engineering from a production-oriented field to one centered on human judgment under automation, with profound implications for research, practice, and education.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [26] [Pending Conflicts Make Progress Impossible](https://arxiv.org/abs/2602.04013)
*Petr Kuznetsov,Pierre Sutra,Guillermo Toyos-Marfurt*

Main category: cs.DC

TL;DR: 该论文研究了共享对象实现中的进度条件，提出了冲突阻塞自由的概念，证明了在异步读写共享内存模型中无法实现冲突阻塞自由的通用构造，揭示了冲突操作带来的同步成本这一根本限制。


<details>
  <summary>Details</summary>
Motivation: 观察到可交换操作可以并行执行，因此研究如何利用操作的交换性来改进共享对象实现的进度保证，特别是针对可交换操作和非交换操作的不同处理方式。

Method: 引入冲突阻塞自由的概念：如果一个进程在足够长的时间内运行而没有遇到与非交换操作的步骤争用，那么它就能完成操作。这通过区分可交换操作和非交换操作来推广阻塞自由和等待自由。

Result: 证明了在异步读写共享内存模型中，冲突阻塞自由的通用构造是不可能实现的。这一结果表明，冲突操作的调用本身就带来了同步成本，进度需要等待冲突的最终解决。

Conclusion: 冲突感知的通用构造存在根本性限制：非交换操作的调用会强制同步，即使这些操作最终可能被证明是可交换的。这揭示了冲突操作带来的固有同步成本，是分布式计算中的一个基本限制。

Abstract: In this work, we study progress conditions for commutativity-aware, linearizable implementations of shared objects. Motivated by the observation that commuting operations can be executed in parallel, we introduce conflict-obstruction-freedom: a process is guaranteed to complete its operation if it runs for long enough without encountering step contention with conflicting (non-commuting) operations. This condition generalizes obstruction-freedom and wait-freedom by allowing progress as long as step contention is only induced by commuting operations. We prove that conflict-obstruction-free universal constructions are impossible to implement in the asynchronous read-write shared memory model. This result exposes a fundamental limitation of conflict-aware universal constructions: the mere invocation of conflicting operations imposes a synchronization cost. Progress requires eventual resolution of pending conflicts.

</details>


### [27] [Six Times to Spare: LDPC Acceleration on DGX Spark for AI-Native Open RAN](https://arxiv.org/abs/2602.04652)
*Ryan Barker,Fatemeh Afghah*

Main category: cs.DC

TL;DR: 论文实证量化了将5G LDPC解码从Grace CPU卸载到Blackwell GB10 GPU的性能优势，平均获得6倍吞吐量提升，GPU解码延迟保持在时隙限制内，而CPU解码会超时。


<details>
  <summary>Details</summary>
Motivation: 5G NR物理层中LDPC解码是计算密集型任务，必须在0.5ms传输时间间隔内完成。许多现有系统仍在通用CPU上执行LDPC解码，随着带宽、调制阶数和用户复用的增加，存在错过时隙事件和可扩展性受限的问题。

Method: 使用NVIDIA Sionna PHY/SYS在TensorFlow上构建NR类链路级链，包含LDPC5G编码器/解码器、16-QAM调制和AWGN。通过扫描并行解码的码字数、置信传播迭代次数，测量解码阶段的性能，同时记录CPU和GPU利用率及功耗。

Result: GPU/CPU平均吞吐量提升约6倍。CPU解码延迟在20次迭代时达到0.71ms（超过0.5ms时隙限制），而GB10 GPU在相同工作负载下保持在时隙的6-24%范围内。CPU解码通常消耗约10个Grace核心，而GPU解码仅比空闲状态增加10-15W功耗。

Conclusion: GPU卸载能显著提升LDPC解码性能并满足时隙要求，同时释放CPU资源用于更高层任务。基于Sionna的实现提供了保守的性能下界，并为未来平台评估物理层内核提供了可重用、可脚本化的方法。

Abstract: Low-density parity-check (LDPC) decoding is one of the most computationally intensive kernels in the 5G New Radio (NR) physical layer and must complete within a 0.5\,ms transmission time interval while sharing the budget with FFT, channel estimation, demapping, HARQ, and MAC scheduling. Many open and proprietary stacks still execute LDPC on general-purpose CPUs, raising concerns about missed-slot events and limited scalability as bandwidths, modulation orders, and user multiplexing increase. This paper empirically quantifies the benefit of offloading 5G-style LDPC5G decoding from a Grace CPU to the integrated Blackwell GB10 GPU on an NVIDIA DGX~Spark platform. Using NVIDIA Sionna PHY/SYS on TensorFlow, we construct an NR-like link-level chain with an LDPC5G encoder/decoder, 16-QAM modulation, and AWGN, and sweep both the number of codewords decoded in parallel and the number of belief-propagation iterations, timing only the decoding phase while logging CPU and GPU utilization and power. Across the sweep we observe an average GPU/CPU throughput speedup of approximately $6\times$, with per-codeword CPU latency reaching $\approx 0.71$\,ms at 20 iterations (exceeding the 0.5\,ms slot), while the GB10 GPU remains within 6--24\% of the slot for the same workloads. Resource-usage measurements show that CPU-based LDPC decoding often consumes around ten Grace cores, whereas GPU-based decoding adds only $\approx10-15$\,W over GPU idle while leaving most CPU capacity available for higher-layer tasks. Because our implementation relies on high-level Sionna layers rather than hand-tuned CUDA, these results represent conservative lower bounds on achievable accelerator performance and provide a reusable, scriptable methodology for evaluating LDPC and other physical-layer kernels on future Grace/Blackwell and Aerial/ACAR/AODT platforms.

</details>


### [28] [A TEE-based Approach for Preserving Data Secrecy in Process Mining with Decentralized Sources](https://arxiv.org/abs/2602.04697)
*Davide Basile,Valerio Goretti,Luca Barbaro,Hajo A. Reijers,Claudio Di Ciccio*

Main category: cs.DC

TL;DR: CONFINE提出了一种基于可信执行环境(TEE)的隐私保护跨组织流程挖掘方法，通过四阶段协议安全处理多方事件日志，解决跨组织流程挖掘中的数据保密性问题。


<details>
  <summary>Details</summary>
Motivation: 现有流程挖掘技术主要关注组织内部场景，但许多实际业务流程跨越多个独立组织。跨组织流程挖掘面临重大挑战，特别是数据保密性问题：数据分析可能泄露参与组织不愿向彼此或第三方披露的信息。

Method: CONFINE利用可信执行环境(TEE)部署可信应用程序，安全地挖掘多方事件日志。提出支持四阶段协议的架构，保护数据交换和处理，采用基于分段的策略避免TEE内存限制问题。

Result: 进行了形式化正确性验证和安全性分析，在真实和合成数据上评估实现。结果显示内存使用随事件日志大小呈对数增长，随供应组织数量呈线性增长，表明可扩展性良好。

Conclusion: CONFINE方法能够处理实际工作负载，解决跨组织流程挖掘中的保密性问题，展示了可扩展性并为进一步优化提供了机会。

Abstract: Process mining techniques enable organizations to gain insights into their business processes through the analysis of execution records (event logs) stored by information systems. While most process mining efforts focus on intra-organizational scenarios, many real-world business processes span multiple independent organizations. Inter-organizational process mining, though, faces significant challenges, particularly regarding confidentiality guarantees: The analysis of data can reveal information that the participating organizations may not consent to disclose to one another, or to a third party hosting process mining services. To overcome this issue, this paper presents CONFINE, an approach for secrecy-preserving inter-organizational process mining. CONFINE leverages Trusted Execution Environments (TEEs) to deploy trusted applications that are capable of securely mining multi-party event logs while preserving data secrecy. We propose an architecture supporting a four-stage protocol to secure data exchange and processing, allowing for protected transfer and aggregation of unaltered process data across organizational boundaries. To avoid out-of-memory errors due to the limited capacity of TEEs, our protocol employs a segmentation-based strategy, whereby event logs are transmitted to TEEs in smaller batches. We conduct a formal verification of correctness and a security analysis of the guarantees provided by the TEE core. We evaluate our implementation on real-world and synthetic data, showing that the proposed approach can handle realistic workloads. The results indicate logarithmic memory growth with respect to the event log size and linear growth with the number of provisioning organizations, highlighting scalability properties and opportunities for further optimization.

</details>
