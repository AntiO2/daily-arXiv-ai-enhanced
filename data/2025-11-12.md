<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Generic Algorithm for Universal TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.08034)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Ilija Basicevic*

Main category: cs.DC

TL;DR: 提出了一个通用的TDM通信算法，克服了原有联邦学习框架中节点只能与单个对等节点通信的限制，支持节点与任意数量的对等节点通信。


<details>
  <summary>Details</summary>
Motivation: 原有的联邦学习框架中的TDM通信算法只允许网络节点之间进行成对通信，这限制了通信的灵活性和效率，特别是在需要多节点同时通信的实际场景中。

Method: 设计了一个新的通用TDM通信算法，包括算法的理论基础、系统设计和系统验证，支持节点与任意数量的对等节点进行通信。

Result: 新算法成功克服了原有TDM通信的限制，能够支持节点与多个对等节点同时通信，特别适用于卫星间链路等实际通信场景。

Conclusion: 提出的通用TDM通信算法显著提升了联邦学习框架的通信能力，为实际应用如卫星通信提供了更好的支持。

Abstract: The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.

</details>


### [2] [UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing](https://arxiv.org/abs/2511.08135)
*Zhuoheng Ran,Chong Wu,Renjie Xu,Maolin Che,Hong Yan*

Main category: cs.DC

TL;DR: UniFormer是一个统一的Transformer架构，旨在同时优化通用计算平台（如GPU）和定制计算平台（如FPGA）的性能，解决模型在不同计算平台间迁移时的效率损失问题。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型在通用计算和定制计算平台间迁移时，由于并行计算范式的根本差异，往往需要在复杂性、效率或准确性方面做出妥协，且跨平台优化原则研究不足。

Method: 通过提高并行性和计算-存储融合，设计统一的Transformer架构UniFormer，使其能同时在GPU和FPGA上高效运行。

Result: UniFormer在GPU上实现了最先进的准确性和延迟性能，同时在FPGA上表现出强大的适应性。

Conclusion: 这是首个同时考虑通用计算和定制计算架构的高效Transformer工作，为跨平台模型部署提供了有效解决方案。

Abstract: The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.

</details>


### [3] [\uline{LO}w-c\uline{O}st yet High-\uline{P}erformant \uline{S}parse Matrix-Matrix Multiplication on Arm SME Architectures](https://arxiv.org/abs/2511.08158)
*Kelun Lei,Hailong Yang,Kaige Zhang,Kejie Ma,Yiqing Wang,Xin You,Yufan Xu,Enrique S. Quintana-Orti,Zhongzhi Luan,Yi Liu,Depei Qian*

Main category: cs.DC

TL;DR: LOOPS是一个混合执行框架，通过结合行式CSR和向量式BCSR布局，在Armv9架构上协同利用NEON向量指令和SME矩阵扩展资源，显著提升稀疏矩阵-稠密矩阵乘法的性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏矩阵-稠密矩阵乘法在科学计算和图学习中至关重要，但如何在Armv9架构上有效利用SME和传统SIMD资源处理非结构化稀疏工作负载仍是一个挑战。

Method: 提出LOOPS混合执行框架，采用行式CSR部分和向量式BCSR部分布局，通过轻量级性能模型指导的自适应两级并行化方案，支持FP64、FP32和FP16多精度SpMM。

Result: 在Apple M4Pro CPU上测试SuiteSparse数据集，LOOPS相比CPU基线TACO平均加速9.93倍(FP32)/14.4倍(FP64)，相比Armadillo加速71.3倍(FP32)/54.8倍(FP64)。与NVIDIA A100 GPU上的cuSPARSE和Magicube相比，LOOPS平均加速19.8-33.5倍，且能效显著优于GPU。

Conclusion: LOOPS框架成功展示了在Armv9架构上协同利用SME和NEON资源处理稀疏矩阵计算的潜力，实现了超越GPU的性能和能效表现。

Abstract: Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\times$ (FP32)/14.4$\times$ (FP64) against the CPU baseline TACO and 71.3$\times$ (FP32)/54.8$\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\times$ and 33.5$\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU.

</details>


### [4] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 该论文研究了在顶点和边传递图上的机器人聚集问题，考虑了初始配置可能存在重复位置、机器人无法检测重复位置等限制条件，提出了在无限网格和超立方体上的两种时间最优聚集算法。


<details>
  <summary>Details</summary>
Motivation: 研究在具有重复位置且机器人无法检测重复位置的限制条件下，在顶点和边传递图上实现机器人聚集的问题，以平衡这种'敌对'环境。

Method: 使用轮询调度器，设计两种针对无限网格和超立方体拓扑的算法，充分利用这些图的结构特性。

Result: 提供了基本的不可能性结果，并设计了在无限网格和超立方体上的时间最优聚集算法。

Conclusion: 由于算法严重依赖底层拓扑的特性，推测可能不存在适用于所有可解情况的通用算法。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [5] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对MCP增强的LLM交互进行了测量分析，揭示了能力、性能和成本之间的权衡关系，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: MCP虽然增强了LLM与外部工具交互的能力，但大量上下文信息显著增加了token使用量，导致成本上升和计算负载增加。

Method: 采用基于测量的分析方法，研究不同LLM模型和MCP配置对token效率、成本、任务完成时间和成功率等关键指标的影响。

Result: 发现了MCP交互中能力与成本之间的权衡关系，提出了并行工具调用和任务中止机制等优化方案。

Conclusion: 这些发现为开发更高效、稳健且成本效益更高的MCP工作流程提供了有价值的见解。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>
