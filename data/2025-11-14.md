<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 10]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.SE](#cs.SE) [Total: 6]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 本文评估了Ksurf方法在基于估计的资源编排任务中的表现，特别是在高可变性云环境下作为上下文多臂老虎机目标函数模型的应用。


<details>
  <summary>Details</summary>
Motivation: 云数据中心中容器基础设施的资源编排和配置参数搜索面临巨大配置空间和云不确定性的挑战。云环境中虚拟机数量的变化引入了工作负载和资源指标的变异性，使得编排决策因非线性增加和噪声而变得不准确。

Method: 使用Ksurf作为上下文多臂老虎机目标函数模型，结合Drone编排器，在高度可变的云场景中进行资源估计。Ksurf是一种最先进的方差最小化估计方法，特别适用于高可变性云数据。

Result: Ksurf实现了显著更低的延迟方差：p95降低41%，p99降低47%；CPU使用率降低4%；主节点内存使用减少7MB；在VarBench Kubernetes基准测试中平均工作pod数量减少7%，带来成本节约。

Conclusion: Ksurf在高可变性云环境下能够有效优化资源估计，显著降低延迟方差和资源使用，实现成本节约。

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [2] [A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond](https://arxiv.org/abs/2511.09776)
*Ramesh Adhikari,Costas Busch,Pavan Poudel*

Main category: cs.DC

TL;DR: 本文研究了雾-云计算网络中事务调度问题，提出了在常数倍维度的网络中最小化调度总成本的近似算法，包括单对象和多对象访问场景的分布式调度方案。


<details>
  <summary>Details</summary>
Motivation: 在分布式系统中，事务调度对于高效分配共享资源至关重要。特别是在雾-云计算模型中，事务和共享对象可以在网络中移动，需要设计有效的调度策略来最小化总成本。

Method: 针对常数倍维度网络，提出了两种调度算法：单对象访问场景的O(log n · log D)近似算法，以及多对象访问（最多k个对象）的O(k · log n · log D)近似算法，并提供了完全分布式实现。

Result: 算法在单对象访问时达到O(log n · log D)近似比，在多对象访问时达到O(k · log n · log D)近似比，其中n为节点数，D为网络直径，k为每个事务访问的最大对象数。

Conclusion: 所提出的调度算法在理论上具有良好的近似保证，适用于实际中常见的常数倍维度网络，且支持分布式实现，不需要全局事务知识。

Abstract: Transaction scheduling is crucial to efficiently allocate shared resources in a conflict-free manner in distributed systems. We investigate the efficient scheduling of transactions in a network of fog-cloud computing model, where transactions and their associated shared objects can move within the network. The schedule may require objects to move to transaction nodes, or the transactions to move to the object nodes. Moreover, the schedule may determine intermediate nodes where both objects and transactions meet. Our goal is to minimize the total combined cost of the schedule. We focus on networks of constant doubling dimension, which appear frequently in practice. We consider a batch problem where an arbitrary set of nodes has transactions that need to be scheduled. First, we consider a single shared object required by all the transactions and present a scheduling algorithm that gives an $O(\log n \cdot \log D)$ approximation of the optimal schedule, where $n$ is the number of nodes and $D$ is the diameter of the network. Later, we consider transactions accessing multiple shared objects (at most $k$ objects per transaction) and provide a scheduling algorithm that gives an $O(k \cdot \log n \cdot \log D)$ approximation. We also provide a fully distributed version of the scheduling algorithms where the nodes do not need global knowledge of transactions.

</details>


### [3] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: MoFa是一个新颖的预训练性能建模框架，统一了多维优化特征和容错机制，能够准确预测大规模LLM预训练性能并提供优化指导。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模从数十亿增长到数万亿，分布式预训练成为必要，但混合并行化策略的庞大组合空间带来了显著的优化挑战。传统手动调优方法成本高昂，现有性能建模方法无法全面考虑优化特征和容错机制的开销。

Method: 提出MoFa框架，包含增强的成本模型来准确捕捉关键优化效果，并基于历史集群可靠性数据集成容错模型。开发了基于MoFa的调优系统来探索最优预训练性能和潜在瓶颈。

Result: 广泛的建模评估表明MoFa在各种场景下都能实现高预测精度。调优实验系统揭示了不同配置下影响预训练性能的关键因素。

Conclusion: MoFa为LLM预训练系统设计和部署提供了可靠的先验指导，能够有效解决大规模分布式预训练的性能优化问题。

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [4] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: GPU系统在数据中心存在性能变异问题，Lit Silicon效应揭示了热不平衡导致GPU节点级性能下降，通过检测和缓解技术可实现6%性能提升和4%功耗优化。


<details>
  <summary>Details</summary>
Motivation: GPU系统在现代数据中心中广泛应用，但存在节点和集群级别的性能变异问题，严重影响高性能计算和AI工作负载（如大语言模型训练）的效率。

Method: 分析了多GPU节点运行LLM训练的性能，发现热诱导的拖尾效应与并发计算通信技术相关，提出了Lit Silicon效应模型，并设计了检测和缓解技术，评估了三种电源管理解决方案。

Result: 在两个AMD Instinct MI300X GPU系统和两个LLM训练框架上的实验显示，可实现高达6%的性能提升和4%的功耗优化，为数据中心节省数亿美元成本。

Conclusion: Lit Silicon效应是GPU系统性能变异的重要根源，提出的解决方案几乎零成本，可作为新的节点级电源管理层轻松部署到数据中心中。

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [5] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: CacheX通过在虚拟机内使用驱逐集探测缓存抽象，无需硬件或管理程序支持，实现了LLC竞争感知任务调度和虚拟颜色感知页面缓存管理，有效提升了公共云虚拟机中各种工作负载的缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 在公共云虚拟机中，基于缓存的优化往往无效，因为虚拟机无法了解和控制分配的缓存。CPU缓存可能在虚拟机之间分区或共享，但虚拟机不知道缓存分配细节，也无法通过页面放置策略影响缓存使用。

Method: 提出CacheX解决方案，使用驱逐集在虚拟机内探测准确细粒度的缓存抽象，无需硬件或管理程序支持。展示了两种新技术：LLC竞争感知任务调度和虚拟颜色感知页面缓存管理。

Result: 在x86 Linux内核中实现的CacheX评估表明，它能有效提高公共云虚拟机中各种工作负载的缓存利用率。

Conclusion: CacheX提供了一种在云虚拟机中有效探测和管理缓存的新方法，解决了传统缓存优化在云环境中失效的问题。

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [6] [Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach](https://arxiv.org/abs/2511.10146)
*Jaime Sebastian Burbano,Arnova Abdullah,Eldiyar Zhantileuov,Mohan Liyanage,Rolf Schuster*

Main category: cs.DC

TL;DR: 提出了一种轻量级的边缘服务器选择方法MO-HAN，通过融合延迟预测、自适应可靠性和基于滞后的切换机制，在降低延迟的同时减少50%的切换次数。


<details>
  <summary>Details</summary>
Motivation: 多服务器架构中的动态网络拥塞对延迟敏感的嵌入式应用构成挑战，需要一种轻量级的服务器选择方法来确保低延迟和高可靠性。

Method: 使用被动测量（到达率、利用率、负载大小）和指数调制有理延迟模型，计算平衡预测延迟和可靠性的得分，仅在预期增益有意义时进行切换。

Result: MO-HAN在降低平均和尾部延迟方面持续优于静态和公平分布基线，同时相比纯机会选择减少了近50%的切换次数。

Conclusion: MO-HAN无需侵入式检测或重型学习基础设施，适用于资源受限的嵌入式设备，实现了延迟降低和切换优化的双重目标。

Abstract: Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices.

</details>


### [7] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: 提出基于监督学习的稀疏矩阵重排序算法选择模型，能够根据矩阵特征自动选择最优重排序算法，相比单一AMD算法减少55.37%求解时间。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏矩阵排序算法选择依赖暴力搜索或经验知识，无法适应不同稀疏矩阵结构，需要智能化的自动选择方法。

Method: 开发监督学习模型，学习矩阵特征与常用重排序算法之间的关联，实现稀疏矩阵重排序算法的自动智能选择。

Result: 在Florida稀疏矩阵数据集上的实验表明，模型能准确预测各种矩阵的最优重排序算法，相比单一AMD算法减少55.37%求解时间，平均加速比为1.45。

Conclusion: 基于监督学习的稀疏矩阵重排序算法选择模型能够有效提升求解效率，实现算法选择的自动化和智能化。

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [8] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种现代工作负载调度器分类的新方法，描述了操作系统进程调度器、集群系统作业调度器和大数据调度器三类，并讨论了它们从早期采用到现代实现的演变过程。


<details>
  <summary>Details</summary>
Motivation: 为现代工作负载调度器提供一个系统性的分类框架，理解不同类型调度器的演变历程和算法特征。

Method: 通过描述三类调度器（操作系统进程调度器、集群系统作业调度器、大数据调度器）的演变过程，分析它们的使用情况和算法特性。

Result: 建立了现代工作负载调度器的分类体系，揭示了不同类型调度器之间的差异及其按时间顺序的发展轨迹。

Conclusion: 本地系统和分布式系统在调度策略设计上存在相似的关注点，调度器的发展呈现出从简单到复杂、从本地到分布式的演进趋势。

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [9] [FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing](https://arxiv.org/abs/2511.10442)
*Aarush Agarwal,Raymond He,Jan Kieseler,Matteo Cremonesi,Shah Rukh Qasim*

Main category: cs.DC

TL;DR: FastGraph是一种专为低维空间（2-10维）图构建优化的GPU加速k近邻算法，相比现有方法提速20-40倍且几乎无内存开销。


<details>
  <summary>Details</summary>
Motivation: 为高性能图神经网络中的图构建过程提供GPU优化解决方案，特别是在低维空间中实现高效计算。

Method: 采用GPU驻留的箱分区方法，支持完整梯度流和自适应参数调优，提升计算和内存效率。

Result: 在维度小于10的情况下，比FAISS、ANNOY和SCANN等先进库快20-40倍，内存开销几乎为零。

Conclusion: FastGraph显著提升了GNN工作流程的性能，特别适用于高能物理粒子聚类、视觉目标跟踪和图聚类等计算密集型低维应用。

Abstract: We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.

</details>


### [10] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: STAGE是一个用于生成高保真执行轨迹的框架，能够准确建模大语言模型工作负载，支持全面的并行化策略，可扩展到32K GPU规模。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模AI训练和推理系统建模方法依赖于从真实系统收集执行轨迹，但大型基础设施主要限于主要云提供商，且现有平台的轨迹难以适应未来更大规模系统配置的研究。

Method: 引入符号张量图生成器(STAGE)框架，通过合成高保真执行轨迹来建模LLM工作负载，支持全面的并行化策略，允许系统性地探索广泛的LLM架构和系统配置。

Result: STAGE展示了其可扩展性，能够合成跨越32K GPU的高保真LLM轨迹，同时在计算、内存和通信方面保持张量级精度。

Conclusion: STAGE将公开可用，以促进分布式机器学习系统的进一步研究。

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [11] [Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management](https://arxiv.org/abs/2511.10063)
*Yiwen Wang,Vivek Shah,Marcos Antonio Vaz Salles,Claudia Bauzer Medeiros,Julio Cesar Dos Reis,Yongluan Zhou*

Main category: cs.DB

TL;DR: 本文提出了一种基于移动actor的分布式反应式移动对象数据管理平台M-AODBs及其实现Dolphin，通过在actor模型中增强反应感知、移动和空间查询能力，为反应式移动对象应用提供低延迟、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有移动对象场景研究主要关注时空数据管理，而反应式行为通常需要复杂的终端用户实现，难以同时满足低延迟计算要求和可扩展性需求。

Method: 提出移动actor抽象概念，在actor模型中增强反应感知、移动和空间查询能力；基于此构建M-AODBs平台，并在Microsoft Orleans分布式虚拟actor框架上实现Dolphin系统。

Result: 在真实反应式移动对象场景的实验评估中，Dolphin在多机器上展现出良好的可扩展性，并提供接近实时的反应延迟。

Conclusion: 移动actor抽象和M-AODBs平台能够有效减轻开发者在平衡性能与一致性方面的负担，为反应式移动对象应用提供可行的解决方案。

Abstract: Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency.

</details>


### [12] [CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models](https://arxiv.org/abs/2511.10418)
*Yaqiao Zhu,Hongkai Wen,Mark Birkin,Man Luo*

Main category: cs.DB

TL;DR: CityVerse是首个统一平台，整合多源城市数据、基于能力的任务分类和动态模拟，用于系统评估LLM在城市计算中的表现。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在城市计算任务中面临两大挑战：缺乏统一平台进行一致的多源数据访问，以及任务定义碎片化阻碍公平比较。

Method: 1) 基于坐标的数据API统一十类城市数据；2) 任务API将43个城市计算任务组织为四级认知层次；3) 交互式可视化前端支持实时数据检索和多层显示。

Result: 通过对主流LLM在代表性任务上的评估验证了平台有效性，展示了其支持可重复和系统评估的能力。

Conclusion: CityVerse为推进LLM和多任务方法在城市计算领域提供了可重用的基础平台。

Abstract: Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [13] [Evaluating Software Process Models for Multi-Agent Class-Level Code Generation](https://arxiv.org/abs/2511.09794)
*Wasique Islam Shafin,Md Nakhla Rafi,Zhenhao Li,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 多智能体LLM工作流通过瀑布式开发流程（需求、设计、实现、测试）重组而非增强模型性能，在代码可维护性和功能正确性之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单智能体函数级代码生成，缺乏对多智能体工作流中流程结构和角色专业化如何影响类级代码生成的系统研究。

Method: 使用三个LLM（GPT-4o-mini、DeepSeek-Chat、Claude-3.5-Haiku）在ClassEval基准的100个Python任务上模拟瀑布式开发周期。

Result: 多智能体工作流产生更清晰可维护的代码，但通常降低功能正确性（GPT-4o-mini -37.8%，DeepSeek-Chat -39.8%），Claude-3.5-Haiku例外（+9.5%）。流程约束改变失败特征：结构问题减少，语义和验证错误增加。测试阶段影响最大。

Conclusion: 软件流程结构从根本上改变LLM的推理、协作和失败方式，揭示了多智能体代码生成中严格工作流纪律与灵活问题解决之间的内在权衡。

Abstract: Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\% for GPT-4o-mini and -39.8\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.

</details>


### [14] [EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines](https://arxiv.org/abs/2511.09964)
*Noah van der Vleuten,Anthony Flores,Shray Mathur,Max Rakitin,Thomas Hopkins,Kevin G. Yager,Esther H. R. Tsai*

Main category: cs.SE

TL;DR: EnvTrace是一种基于仿真的方法，通过评估执行轨迹来评估语义代码等价性，用于评估大语言模型在仪器控制方面的能力。该方法使用光束线控制逻辑数字孪生进行演示，评估了30多个LLM，显示许多顶级模型在快速控制代码生成方面接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在仪器控制方面的能力需要超越标准静态算法基准的方法，因为物理系统的行为无法仅通过单元测试完全捕获。

Method: 引入EnvTrace方法，使用基于仿真的执行轨迹评估来评估语义代码等价性，通过光束线控制逻辑数字孪生进行演示，并使用轨迹对齐为30多个LLM生成多方面的功能正确性评分。

Result: 评估显示许多顶级大语言模型在快速控制代码生成方面能够接近人类水平性能。

Conclusion: 这是实现LLM和数字孪生共生协作愿景的第一步：LLM提供直观控制和智能编排，数字孪生提供安全高保真环境，为自主具身AI铺平道路。

Abstract: Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.

</details>


### [15] [Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents](https://arxiv.org/abs/2511.10049)
*Divyanshu Saxena,Rishikesh Maurya,Xiaoxuan Ou,Gagan Somashekar,Shachee Mishra Gupta,Arun Iyer,Yu Kang,Chetan Bansal,Aditya Akella,Saravan Rajmohan*

Main category: cs.SE

TL;DR: 提出了一种用于评估持续演进的企业级AI代理的基准生成方法，通过半结构化文档和LLM生成基准，实现可维护的评估框架。


<details>
  <summary>Details</summary>
Motivation: 传统固定基准测试无法满足企业级AI代理的评估需求，因为企业服务和要求持续演进，且真实案例稀疏。

Method: 使用半结构化文档表达高层意图，利用最先进的LLM从少量文档生成基准，支持基准随需求变化而演进。

Result: 在大型公共企业的服务迁移案例研究中成功应用，实现了可维护的评估框架，能够快速反馈代理性能并促进针对性改进。

Conclusion: 该方法为评估持续演进的企业级AI代理提供了有效的解决方案，解决了传统基准测试在企业环境中的局限性。

Abstract: The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.

</details>


### [16] [Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics](https://arxiv.org/abs/2511.10271)
*Xin Sun,Daniel Ståhl,Kristian Sandahl,Christoph Kessler*

Main category: cs.SE

TL;DR: 该研究系统评估了LLM生成代码的非功能性质量，发现学术关注点与行业优先级存在错配，且LLM在不同质量维度之间存在权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM生成代码的功能正确性，但缺乏对其非功能性质量（如安全性、可维护性、性能效率）的系统性理解和评估。

Method: 采用三种互补研究方法：对108篇论文的系统性综述、与行业从业者的工作坊、使用三个LLM修复真实软件问题的实证分析。

Result: 学术界主要关注安全性和性能效率，而行业更重视可维护性和可读性；LLM生成的正确补丁在不同质量维度间存在权衡，优化一个维度往往牺牲其他维度。

Conclusion: 需要在LLM代码生成流程中集成质量保证机制，确保生成的代码不仅通过测试，而且真正具备质量。

Abstract: In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.

</details>


### [17] [A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports](https://arxiv.org/abs/2511.10323)
*Dávid Kószó,Tamás Aladics,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 提出了一个收集和分类静态代码分析警告的新方法，创建了包含超过100万条Java源代码警告的大规模数据集NASCAR，用于区分可操作和不可操作的警告。


<details>
  <summary>Details</summary>
Motivation: 静态代码分析工具会产生大量警告，其中许多是不可操作的，导致开发者产生'警告疲劳'，可能忽略关键问题，影响生产力和代码质量。目前缺乏足够的数据集来训练机器学习模型改进SCA工具。

Method: 开发了一种新颖的方法来收集和分类SCA警告，有效区分可操作和不可操作的警告，并基于此方法生成了大规模数据集。

Result: 创建了名为NASCAR的数据集，包含超过100万条Java源代码警告条目，并将数据集和生成工具公开提供。

Conclusion: 该研究填补了SCA警告数据集的空白，为改进静态代码分析工具的准确性和可用性提供了重要资源，有助于减轻警告疲劳问题。

Abstract: Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.

</details>


### [18] [Towards Comprehensive Sampling of SMT Solutions](https://arxiv.org/abs/2511.10326)
*Shuangyu Lyu,Chuan Luo,Ruizhi Shi,Wei Wu,Chanjuan Liu,Chunming Hu*

Main category: cs.SE

TL;DR: PanSampler是一个新颖的SMT采样器，通过多样性感知算法、AST引导评分函数和后采样优化技术，用少量解实现高覆盖率，显著提高软件测试和硬件验证效率。


<details>
  <summary>Details</summary>
Motivation: 在软件和硬件测试中，生成多样化的SMT解对于发现故障和安全违规至关重要。传统方法需要大量解才能达到高覆盖率，增加了测试时间和资源消耗。

Method: 结合三种新技术：多样性感知SMT算法、AST引导评分函数和后采样优化技术。通过迭代采样、候选评估和局部搜索来优化解，确保用少量样本达到高覆盖率。

Result: 在实践基准测试中，PanSampler比现有采样器用更少的解达到相同覆盖率水平。在实际软件系统测试中，将所需测试用例数量减少32.6%到76.4%，同时保持相同的故障检测效果。

Conclusion: PanSampler显著推进了SMT采样技术，降低了软件测试和硬件验证的成本，提高了测试效率。

Abstract: This work focuses on effectively generating diverse solutions for satisfiability modulo theories (SMT) formulas, targeting the theories of bit-vectors, arrays, and uninterpreted functions, which is a critical task in software and hardware testing. Generating diverse SMT solutions helps uncover faults and detect safety violations during the verification and testing process, resulting in the SMT sampling problem, i.e., constructing a small number of solutions while achieving comprehensive coverage of the constraint space. While high coverage is crucial for exploring system behaviors, reducing the number of solutions is of great importance, as excessive solutions increase testing time and resource usage, undermining efficiency. In this work, we introduce PanSampler, a novel SMT sampler that achieves high coverage with a small number of solutions. It incorporates three novel techniques, i.e., diversity-aware SMT algorithm, abstract syntax tree (AST)-guided scoring function and post-sampling optimization technology, enhancing its practical performance. It iteratively samples solutions, evaluates candidates, and employs local search to refine solutions, ensuring high coverage with a small number of samples. Extensive experiments on practical benchmarks demonstrate that PanSampler exhibits a significantly stronger capability to reach high target coverage, while requiring fewer solutions than current samplers to achieve the same coverage level. Furthermore, our empirical evaluation on practical subjects, which are collected from real-world software systems, shows that PanSampler achieves higher fault detection capability and reduces the number of required test cases from 32.6\% to 76.4\% to reach the same fault detection effectiveness, leading to a substantial improvement in testing efficiency. PanSampler advances SMT sampling, reducing the cost of software testing and hardware verification.

</details>
