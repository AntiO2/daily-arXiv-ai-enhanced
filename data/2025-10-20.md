<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Automated Snippet-Alignment Data Augmentation for Code Translation](https://arxiv.org/abs/2510.15004)
*Zhiming Zhang,Qingfu Zhu,Xianzhen Luo,Yixuan Wang,Bohan Li,Wanxiang Che*

Main category: cs.SE

TL;DR: 提出了一种利用LLM自动生成代码片段对齐数据的数据增强方法，并采用两阶段训练策略，在TransCoder测试集上获得最高3.78%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 代码翻译需要平行语料库，但现有平行语料有限。程序对齐数据虽然上下文完整但长度过长，片段对齐数据更精细但数量不足。

Method: 利用LLM自动生成片段对齐数据，并设计两阶段训练策略：先在程序对齐数据上训练，再在片段对齐数据上微调。

Result: 在TransCoder测试集上，增强的片段对齐数据结合两阶段训练相比仅在程序对齐数据上微调的基线有持续改进，最高提升3.78%的pass@k指标。

Conclusion: 通过LLM生成片段对齐数据并结合两阶段训练策略，能有效提升代码翻译模型的性能，证明了数据增强和训练策略的重要性。

Abstract: Code translation aims to translate the code from its source language to the
target language and is used in various software development scenarios. Recent
developments in Large Language Models (LLMs) have showcased their capabilities
in code translation, and parallel corpora play a crucial role in training
models for code translation. Parallel corpora can be categorized into
program-alignment (PA) and snippet-alignment (SA) data. Although PA data has
complete context and is suitable for semantic alignment learning, it may not
provide adequate fine-grained training signals due to its extended length,
while the brevity of SA data enables more fine-grained alignment learning. Due
to limited parallel corpora, researchers explore several augmentation methods
for code translation. Previous studies mainly focus on augmenting PA data. In
this paper, we propose a data augmentation method that leverages LLMs to
generate SA data automatically. To fully leverage both PA data and SA data, we
explore a simple yet effective two-stage training strategy, which consistently
enhances model performance compared to fine-tuning solely on PA data.
Experiments on TransCoder-test demonstrate that our augmented SA data combined
with the two-stage training approach yields consistent improvements over the
baseline, achieving a maximum gain of 3.78% on pass@k.

</details>


### [2] [Assessing Coherency and Consistency of Code Execution Reasoning by Large Language Models](https://arxiv.org/abs/2510.15079)
*Changshu Liu,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: CES是一个评估LLM程序执行模拟能力的任务，引入连贯性概念来检测推理逻辑一致性，发现前沿LLM存在大量不连贯的执行推理，主要由于自然语言捷径。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在编程任务中模拟程序执行的能力，排除因推理捷径、幻觉或数据泄露导致的虚假正确预测，系统检验LLM在bug相关任务中的可疑成功。

Method: 提出CES任务，通过测量执行模拟中变量预测的正确性和连贯性，引入新的度量标准评估不同测试用例下的推理一致性（强、弱、随机）。

Result: 在HumanEval上，81.42%的执行模拟是连贯的，其中46.92%预测正确，53.08%预测错误。GPT-4和DeepSeek-R1等前沿LLM存在最多不连贯执行推理。LLM的推理性能大多为随机（48.87%）或弱一致（45.37%）。

Conclusion: LLM在bug预测/定位/修复任务中基本未融入执行推理，成功主要依赖模式匹配或自然语言捷径。缺乏推理能力会威胁LLM处理未见bug或不同上下文模式的泛化能力。

Abstract: This paper proposes CES, a task to evaluate the abilities of LLMs in
simulating program execution and using that reasoning in programming tasks.
Besides measuring the correctness of variable predictions during execution
simulation, CES introduces the notion of coherence to determine whether the
simulation complies with commonsense execution logic, even if the predicted
values along the simulations are incorrect. This enables CES to rule out
suspiciously correct output predictions due to reasoning shortcuts,
hallucinations, or potential data leakage. CES also introduces a novel metric
to measure reasoning consistency across tests with the same or different prime
path coverage in a spectrum: strong, weak, and random. Evaluating 16 LLMs
(including three reasoning LLMs) using CES indicates 81.42% coherent execution
simulation on HumanEval, 46.92% and 53.08% of which result in correct and
incorrect output predictions. Frontier LLMs such as GPT-4 and DeepSeek-R1 have
the most incoherent execution reasoning, mostly due to natural language
shortcuts. Despite relatively coherent execution simulation, LLMs' reasoning
performance across different tests is inconsistent, mostly random (48.87%) or
weak (45.37%), potentially explaining their weakness in programming tasks that
require path-sensitive program analysis to succeed. We also compare CES with
bug prediction/localization/repair, which intuitively requires control- and
data-flow awareness. We observe that LLMs barely incorporate execution
reasoning into their analysis for bug-related tasks, and their success is
primarily due to inherent abilities in pattern matching or natural language
shortcuts, if not data leakage. Without reasoning, there is a threat to the
generalizability of LLMs in dealing with unseen bugs or patterns in different
contexts. CES can be used to vet the suspicious success of LLMs in these tasks
systematically.

</details>


### [3] [Community Engagement and the Lifespan of Open-Source Software Projects](https://arxiv.org/abs/2510.15408)
*Mohit,Kuljit Kaur Chahal*

Main category: cs.SE

TL;DR: 该研究分析了33,946个GitHub仓库，定义了开源软件项目的社区参与度指标，发现社区参与度与项目动态显著相关，且对项目寿命有复杂影响模式。


<details>
  <summary>Details</summary>
Motivation: 开源软件项目依赖社区参与来维持长期发展，但社区参与对项目动态和寿命的量化影响尚未充分探索。

Method: 分析33,946个GitHub仓库，定义并操作化社区参与度指标（问题、评论、关注者、星标），使用非参数检验和相关性分析评估与项目动态和寿命的关系。

Result: 社区参与度指标与项目动态显著相关，在高度参与的项目中相关性更强。项目寿命方面呈现复杂模式：月均参与率在年轻项目中最高，随年龄下降，但部分长寿项目保持极高活跃度。初始参与爆发对项目建立至关重要，持续高参与驱动极端长寿。

Conclusion: 社区参与度动态驱动开源软件项目的寿命和发展，研究建立了验证的社区参与度指标，深入揭示了不同社区活动模式如何促进项目长寿。

Abstract: Open-source software (OSS) projects depend on community engagement (CE) for
longevity. However, CE's quantifiable impact on project dynamics and lifespan
is underexplored. Objectives: This study defines CE in OSS, identifies key
metrics, and evaluates their influence on project dynamics (releases, commits,
branches) and lifespan. Methods: We analyzed 33,946 GitHub repositories,
defining and operationalizing CE with validated per-month metrics (issues,
comments, watchers, stargazers). Non-parametric tests and correlations assessed
relationships with project dynamics and lifespan across quartiles. Results: CE
metrics significantly associate with project dynamics, with stronger
correlations in highly engaged projects. For lifespan, a complex pattern
emerged: per-month CE rates are highest in younger projects, declining with
age. Yet, a subset of long-lived projects maintains exceptionally high
activity. Initial CE bursts appear crucial for establishment, while sustained
high engagement drives extreme longevity. Active issue engagement's influence
intensifies with age, but passive attention's declines. Conclusion: CE
dynamically drives OSS project longevity and development. Our findings
establish validated CE metrics and offer deeper insights into how diverse
community activity patterns contribute to project longevity.

</details>


### [4] [Selecting and Combining Large Language Models for Scalable Code Clone Detection](https://arxiv.org/abs/2510.15480)
*Muslim Chochlov,Gul Aftab Ahmed,James Vincent Patten,Yuanhua Han,Guoxian Lu,David Gregg,Jim Buckley*

Main category: cs.SE

TL;DR: 本文研究了LLM在代码克隆检测中的应用，通过评估76个LLM模型发现没有统一的"最佳LLM"，CodeT5+110M、CuBERT和SPTCode表现最佳。同时探索了LLM集成方法，发现最大或求和集成优于平均集成，在商业大规模数据集上集成方法达到46.91%的精确度。


<details>
  <summary>Details</summary>
Motivation: 源代码克隆存在知识产权侵权和安全漏洞风险，有效的克隆检测特别是对分歧克隆的检测仍然具有挑战性。LLM最近被应用于克隆检测任务，但快速涌现的LLM引发了关于最佳模型选择和LLM集成潜力的疑问。

Method: 识别76个LLM并筛选出适合大规模克隆检测的候选模型，在两个公共工业数据集和一个商业大规模数据集上进行评估。同时探索了LLM集成方法，包括得分归一化和不同的集成策略（最大、求和、平均）。

Result: 没有统一的"最佳LLM"，CodeT5+110M、CuBERT和SPTCode表现最佳。在商业大规模数据集上，表现最佳的CodeT5+110M达到39.71%的精确度，是之前使用的CodeBERT的两倍。集成方法在商业大规模代码上达到46.91%的精确度，优于单个LLM。

Conclusion: 较小的嵌入尺寸、较小的分词器词汇量和定制数据集对LLM克隆检测有利。集成方法在统计上显著且在大数据集上更有效，最大或求和集成优于平均集成。LLM集成是提高克隆检测效果的有效方法。

Abstract: Source code clones pose risks ranging from intellectual property violations
to unintended vulnerabilities. Effective and efficient scalable clone
detection, especially for diverged clones, remains challenging. Large language
models (LLMs) have recently been applied to clone detection tasks. However, the
rapid emergence of LLMs raises questions about optimal model selection and
potential LLM-ensemble efficacy.
  This paper addresses the first question by identifying 76 LLMs and filtering
them down to suitable candidates for large-scale clone detection. The
candidates were evaluated on two public industrial datasets, BigCloneBench, and
a commercial large-scale dataset. No uniformly 'best-LLM' emerged, though
CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates
suggested that smaller embedding sizes, smaller tokenizer vocabularies and
tailored datasets are advantageous. On commercial large-scale dataset a
top-performing CodeT5+110M achieved 39.71\% precision: twice the precision of
previously used CodeBERT.
  To address the second question, this paper explores ensembling of the
selected LLMs: effort-effective approach to improving effectiveness. Results
suggest the importance of score normalization and favoring ensembling methods
like maximum or sum over averaging. Also, findings indicate that ensembling
approach can be statistically significant and effective on larger datasets: the
best-performing ensemble achieved even higher precision of 46.91\% over
individual LLM on the commercial large-scale code.

</details>


### [5] [An Experimental Study of Real-Life LLM-Proposed Performance Improvements](https://arxiv.org/abs/2510.15494)
*Lirong Yi,Gregory Gay,Philipp Leitner*

Main category: cs.SE

TL;DR: LLM生成的代码在大多数情况下能提升性能，但仍不如人类开发者优化的代码，且LLM的原创优化方案很少能带来显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型是否能生成高性能代码，而不仅仅是功能性代码。

Method: 使用从开源Java程序中挖掘的65个真实任务，采用自动化流程用两种领先的LLM生成补丁，并与基准和人工解决方案进行严格基准测试。

Result: LLM生成的代码在大多数情况下确实提高了性能，但人类开发者提出的补丁在统计上显著优于LLM修复方案。约三分之二的LLM解决方案与开发者优化思路语义相同或相似，其余三分之一提出更原创的想法但很少带来显著性能提升。

Conclusion: LLM在代码优化方面有潜力，但尚未达到人类开发者的优化水平，特别是在寻找真正最优解决方案方面存在不足。

Abstract: Large Language Models (LLMs) can generate code, but can they generate fast
code? In this paper, we study this question using a dataset of 65 real-world
tasks mined from open-source Java programs. We specifically select tasks where
developers achieved significant speedups, and employ an automated pipeline to
generate patches for these issues using two leading LLMs under four prompt
variations. By rigorously benchmarking the results against the baseline and
human-authored solutions, we demonstrate that LLM-generated code indeed
improves performance over the baseline in most cases. However, patches proposed
by human developers outperform LLM fixes by a statistically significant margin,
indicating that LLMs often fall short of finding truly optimal solutions. We
further find that LLM solutions are semantically identical or similar to the
developer optimization idea in approximately two-thirds of cases, whereas they
propose a more original idea in the remaining one-third. However, these
original ideas only occasionally yield substantial performance gains.

</details>


### [6] [Enhancing Code Review through Fuzzing and Likely Invariants](https://arxiv.org/abs/2510.15512)
*Wachiraphan Charoenwet,Patanamon Thongtanunam,Van-Thuan Pham,Christoph Treude*

Main category: cs.SE

TL;DR: FuzzSight是一个利用模糊测试和不变式分析来检测代码变更中行为差异的框架，能够在代码审查早期发现潜在缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查主要依赖静态检查，难以发现程序动态行为问题。模糊测试虽然能产生丰富的运行时数据，但缺乏合适的分析机制来帮助审查者识别非崩溃性行为变化。

Method: 通过模糊测试生成非崩溃输入，捕获程序运行时行为，将其表示为可能不变式（likely invariants），然后比较不同版本间的不变式差异来识别行为变化。

Result: 在评估中，FuzzSight标记了75%的回归缺陷和高达80%的漏洞，相比静态应用安全测试（SAST）检测率提高了10倍且误报更少。

Conclusion: FuzzSight展示了将模糊测试和不变式分析结合用于早期代码审查的潜力，能够有效连接静态检查与动态行为洞察。

Abstract: Many software projects employ manual code review to gatekeep defects and
vulnerabilities in the code before integration. However, reviewers often work
under time pressure and rely primarily on static inspection, leaving the
dynamic aspects of the program unexplored. Dynamic analyses could reveal such
behaviors, but they are rarely integrated into reviews. Among them, fuzzing is
typically applied later to uncover crashing bugs. Yet its ability to exercise
code with diverse inputs makes it promising for exposing non-crashing, but
unexpected, behaviors earlier. Still, without suitable mechanisms to analyze
program behaviors, the rich data produced during fuzzing remains inaccessible
to reviewers, limiting its practical value in this context.
  We hypothesize that unexpected variations in program behaviors could signify
potential bugs. The impact of code changes can be automatically captured at
runtime. Representing program behavior as likely invariants, dynamic properties
consistently observed at specific program points, can provide practical signals
of behavioral changes. Such signals offer a way to distinguish between intended
changes and unexpected behavioral shifts from code changes.
  We present FuzzSight, a framework that leverages likely invariants from
non-crashing fuzzing inputs to highlight behavioral differences across program
versions. By surfacing such differences, it provides insights into which code
blocks may need closer attention. In our evaluation, FuzzSight flagged 75% of
regression bugs and up to 80% of vulnerabilities uncovered by 24-hour fuzzing.
It also outperformed SAST in identifying buggy code blocks, achieving ten times
higher detection rates with fewer false alarms. In summary, FuzzSight
demonstrates the potential and value of leveraging fuzzing and invariant
analysis for early-stage code review, bridging static inspection with dynamic
behavioral insights.

</details>


### [7] [Colepp: uma ferramenta multiplataforma para coleta de dados de dispositivos vestiveis](https://arxiv.org/abs/2510.15565)
*Vinicius Moraes de Jesus,Andre Georghton Cardoso Pacheco*

Main category: cs.SE

TL;DR: Colepp是一个开源跨平台工具，用于从多个可穿戴设备收集和同步生理与运动数据，包括心率和运动信号。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备广泛使用，但缺乏高质量公共数据集和可控数据收集条件，阻碍了稳健算法的发展。

Method: 使用智能手机作为中心枢纽，集成Polar H10胸带和Wear OS智能手表，通过自定义同步协议收集ECG、PPG、加速度计和陀螺仪数据，导出CSV格式的同步数据集。

Result: 工具能够产生一致且同步的信号，适用于人类活动识别和心率估计等应用。

Conclusion: Colepp提供了一个用户友好的解决方案，便于生成可定制的真实世界数据集，支持可穿戴设备相关算法的开发。

Abstract: The widespread adoption of wearable devices such as smartwatches and fitness
trackers has fueled the demand for reliable physiological and movement data
collection tools. However, challenges such as limited access to large,
high-quality public datasets and a lack of control over data collection
conditions hinder the development of robust algorithms. This work presents
Colepp, an open-source, cross-platform tool designed to collect and synchronize
data from multiple wearable devices, including heart rate (via ECG and PPG) and
motion signals (accelerometer and gyroscope). The system integrates a
smartphone as a central hub, receiving data from a Polar H10 chest strap and a
Wear OS smartwatch, and exporting synchronized datasets in CSV format. Through
a custom synchronization protocol and user-friendly interface, Colepp
facilitates the generation of customizable, real-world datasets suitable for
applications such as human activity recognition and heart rate estimation. A
use case shows the effectiveness of the tool in producing consistent and
synchronized signals.

</details>


### [8] [Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework](https://arxiv.org/abs/2510.15585)
*Dr Simon Thorne,Dr Advait Sarkar*

Main category: cs.SE

TL;DR: 该立场论文提出将测试驱动开发(TDD)与大型语言模型(LLM)生成相结合的研究框架，旨在提高生成代码的正确性和可靠性，特别是在金融建模和科学计算等高风险领域。


<details>
  <summary>Details</summary>
Motivation: LLM在生成代码时经常出现幻觉、逻辑不一致和语法错误等问题，这些风险在准确性至关重要的领域尤为严重。需要一种方法来增强生成输出的可靠性和用户信心。

Method: 提出"测试优先"的方法论，将TDD实践与LLM驱动生成相结合，通过技术约束和认知支架来引导LLM输出更准确、可验证和可理解的解决方案。

Result: 该框架适用于多种编程环境，包括电子表格公式生成、Python脚本和Rust等强类型语言，并设计了明确的实验方案、评估指标和TDD提示示例。

Conclusion: 通过强调测试驱动思维，旨在提高计算思维、提示工程技能和用户参与度，特别有利于缺乏正式编程培训的电子表格用户。最终目标是建立负责任和可靠的LLM集成实践。

Abstract: Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

</details>


### [9] [Interact and React: Exploring Gender Patterns in Development and the Impact on Innovation and Robustness of a User Interface Tool](https://arxiv.org/abs/2510.15642)
*Sian Brooke*

Main category: cs.SE

TL;DR: 该研究分析了React项目中性别多样性对软件开发的影响，发现女性在功能增强和依赖管理方面贡献显著，性别排斥对软件质量有害。


<details>
  <summary>Details</summary>
Motivation: 开源软件设计中虽然提及女性存在，但很少关注性别多样性如何从根本上改变开发模式，因此研究性别包容对软件开发的实际影响。

Method: 研究React项目11年的数据，分析性别差异在稳健性和创新性指标上的表现，以及主要版本发布前的贡献模式变化。

Result: 女性在功能增强和依赖管理方面贡献显著更多，性别排斥对软件质量产生负面影响。

Conclusion: 增加性别多样性可以带来更包容、创新和稳健的软件。

Abstract: In open-source software design, the inclusion of women is often highlighted
simply to remind programmers that women exist. Yet, little attention is given
to how greater gender diversity, specifically women's participation, could
fundamentally alter development patterns. To understand the potential impact of
gender inclusion, this study investigates React, a widely used JavaScript
library for building user interfaces with an active contributor community. I
examine gender differences in metrics of robustness and innovation, as well as
shifts in contribution patterns leading up to major version releases over 11
years of the React project. My results show that the exclusion of women is
detrimental to software as women contribute significantly more to feature
enhancement and dependency management. By exploring how gender influences
innovation and robustness in the development of React, the study offers
critical insights into how increasing gender diversity could lead to more
inclusive, innovative, and robust software.

</details>


### [10] [MirrorFuzz: Leveraging LLM and Shared Bugs for Deep Learning Framework APIs Fuzzing](https://arxiv.org/abs/2510.15690)
*Shiwen Ou,Yuwei Li,Lu Yu,Chengkun Wei,Tingke Wen,Qiangpu Chen,Yu Chen,Haizhi Tang,Zulie Pan*

Main category: cs.SE

TL;DR: MirrorFuzz是一个自动化API模糊测试解决方案，用于发现深度学习框架中的共享bug。它通过收集历史bug数据、匹配相似API以及使用LLM生成测试代码，在四个流行DL框架中发现了315个bug。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架中的bug会级联影响上层应用，但现有研究对跨框架API共享bug模式探索有限。许多DL框架暴露相似API，存在共享漏洞风险。

Method: 三阶段方法：1)收集各API历史bug数据识别潜在buggy API；2)在框架内和跨框架匹配相似API；3)使用LLM基于相似API历史bug数据合成测试代码触发类比bug。

Result: 在TensorFlow、PyTorch、OneFlow和Jittor上评估，代码覆盖率分别提升39.92%和98.20%。发现315个bug，其中262个是新bug，80个已修复，52个获得CNVD ID。

Conclusion: MirrorFuzz能有效发现DL框架中的共享bug，证明了跨框架API相似性分析在bug检测中的重要性。

Abstract: Deep learning (DL) frameworks serve as the backbone for a wide range of
artificial intelligence applications. However, bugs within DL frameworks can
cascade into critical issues in higher-level applications, jeopardizing
reliability and security. While numerous techniques have been proposed to
detect bugs in DL frameworks, research exploring common API patterns across
frameworks and the potential risks they entail remains limited. Notably, many
DL frameworks expose similar APIs with overlapping input parameters and
functionalities, rendering them vulnerable to shared bugs, where a flaw in one
API may extend to analogous APIs in other frameworks. To address this
challenge, we propose MirrorFuzz, an automated API fuzzing solution to discover
shared bugs in DL frameworks. MirrorFuzz operates in three stages: First,
MirrorFuzz collects historical bug data for each API within a DL framework to
identify potentially buggy APIs. Second, it matches each buggy API in a
specific framework with similar APIs within and across other DL frameworks.
Third, it employs large language models (LLMs) to synthesize code for the API
under test, leveraging the historical bug data of similar APIs to trigger
analogous bugs across APIs. We implement MirrorFuzz and evaluate it on four
popular DL frameworks (TensorFlow, PyTorch, OneFlow, and Jittor). Extensive
evaluation demonstrates that MirrorFuzz improves code coverage by 39.92\% and
98.20\% compared to state-of-the-art methods on TensorFlow and PyTorch,
respectively. Moreover, MirrorFuzz discovers 315 bugs, 262 of which are newly
found, and 80 bugs are fixed, with 52 of these bugs assigned CNVD IDs.

</details>


### [11] [EASELAN: An Open-Source Framework for Multimodal Biosignal Annotation and Data Management](https://arxiv.org/abs/2510.15767)
*Rathi Adarshi Rammohan,Moritz Meier,Dennis Küster,Tanja Schultz*

Main category: cs.SE

TL;DR: EASELAN是一个基于ELAN的多模态生物信号标注框架，通过简化标注流程、集成GitHub版本控制和简化后处理，支持生物信号与视听数据的融合标注，应用于认知机器人日常活动研究。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和自适应认知系统的发展，对大规模多模态标注数据的需求日益增长，特别是融合生物信号的复杂数据集需要更高效的标注工具。

Method: 在ELAN工具基础上开发EASELAN框架，增加支持标注流程各阶段的新组件：从标注文件准备、额外通道设置、集成GitHub版本控制到简化后处理。

Result: 成功应用于DFG资助的EASE项目中的人类日常活动（摆桌子）高维生物信号采集，创建了完整的Table Setting Database，代码和标注数据库已公开。

Conclusion: EASELAN为生物信号采集、标注和处理研究提供了有效工具，讨论了使用中的机遇、限制和经验教训，促进了多模态数据标注工作流程的改进。

Abstract: Recent advancements in machine learning and adaptive cognitive systems are
driving a growing demand for large and richly annotated multimodal data. A
prominent example of this trend are fusion models, which increasingly
incorporate multiple biosignals in addition to traditional audiovisual
channels. This paper introduces the EASELAN annotation framework to improve
annotation workflows designed to address the resulting rising complexity of
multimodal and biosignals datasets. It builds on the robust ELAN tool by adding
new components tailored to support all stages of the annotation pipeline: From
streamlining the preparation of annotation files to setting up additional
channels, integrated version control with GitHub, and simplified
post-processing. EASELAN delivers a seamless workflow designed to integrate
biosignals and facilitate rich annotations to be readily exported for further
analyses and machine learning-supported model training. The EASELAN framework
is successfully applied to a high-dimensional biosignals collection initiative
on human everyday activities (here, table setting) for cognitive robots within
the DFG-funded Collaborative Research Center 1320 Everyday Activity Science and
Engineering (EASE). In this paper we discuss the opportunities, limitations,
and lessons learned when using EASELAN for this initiative. To foster research
on biosignal collection, annotation, and processing, the code of EASELAN is
publicly available(https://github.com/cognitive-systems-lab/easelan), along
with the EASELAN-supported fully annotated Table Setting Database.

</details>


### [12] [Towards Supporting Open Source Library Maintainers with Community-Based Analytics](https://arxiv.org/abs/2510.15794)
*Rachna Raj,Diego Elias Costa*

Main category: cs.SE

TL;DR: 本文提出社区分析方法来追踪开源库在其依赖生态系统中的使用情况，发现平均只有16%的API方法被实际使用，且仅有74%的被使用API方法在测试套件中得到了部分或完全覆盖。


<details>
  <summary>Details</summary>
Motivation: 开源软件维护者缺乏关于其库在实际项目中如何被使用的持续反馈，这些洞察能帮助他们改进测试策略、理解变更影响并更有效地指导库的演进。

Method: 对10个流行Java库及其各自50个依赖项目进行实证研究，提出基于社区使用的测试评估指标，并对开源实践者进行调研。

Result: 研究发现库开发者提供的API方法中平均只有16%被依赖生态系统实际使用，且只有74%的被使用API方法在测试套件中有覆盖。

Conclusion: 社区分析能为维护者提供有价值的洞察，帮助他们根据实际使用情况优化测试策略和库的演进方向。

Abstract: Open-source software (OSS) is a pillar of modern software development. Its
success depends on the dedication of maintainers who work constantly to keep
their libraries stable, adapt to changing needs, and support a growing
community. Yet, they receive little to no continuous feedback on how the
projects that rely on their libraries actually use their APIs. We believe that
gaining these insights can help maintainers make better decisions, such as
refining testing strategies, understanding the impact of changes, and guiding
the evolution of their libraries more effectively. We propose the use of
community-based analytics to analyze how an OSS library is used across its
dependent ecosystem. We conduct an empirical study of 10 popular Java libraries
and each with their respective dependent ecosystem of 50 projects. Our results
reveal that while library developers offer a wide range of API methods, only
16% on average are actively used by their dependent ecosystem. Moreover, only
74% of the used API methods are partially or fully covered by their library
test suite. We propose two metrics to help developers evaluate their test suite
according to the APIs used by their community, and we conduct a survey on
open-source practitioners to assess the practical value of these insights in
guiding maintenance decisions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [13] [Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)](https://arxiv.org/abs/2510.15485)
*Dāvis Kažemaks,Laurens Versluis,Burcu Kulahcioglu Ozkan,Jérémie Decouchant*

Main category: cs.DC

TL;DR: 提出了UWFQ调度器，通过虚拟公平队列系统和运行时分区技术，在Spark中实现用户级公平调度，显著降低小作业响应时间。


<details>
  <summary>Details</summary>
Motivation: Spark内置调度器在工业分析环境中难以同时保证用户级公平性和低响应时间，现有解决方案偏向提交更多作业的用户，公平调度器缺乏对动态用户工作负载的适应性。

Method: 设计UWFQ调度器，模拟虚拟公平队列系统，基于有界公平模型按估计完成时间调度作业；引入运行时分区技术动态调整任务粒度以减少任务倾斜和优先级反转。

Result: UWFQ在Spark框架中实现，使用多用户合成工作负载和Google集群跟踪进行评估，相比现有Spark调度器和先进公平调度算法，小作业平均响应时间最多降低74%。

Conclusion: UWFQ调度器能有效平衡用户公平性和作业性能，显著改善Spark在多用户环境中的调度效率。

Abstract: Apache Spark is a widely adopted framework for large-scale data processing.
However, in industrial analytics environments, Spark's built-in schedulers,
such as FIFO and fair scheduling, struggle to maintain both user-level fairness
and low mean response time, particularly in long-running shared applications.
Existing solutions typically focus on job-level fairness which unintentionally
favors users who submit more jobs. Although Spark offers a built-in fair
scheduler, it lacks adaptability to dynamic user workloads and may degrade
overall job performance. We present the User Weighted Fair Queuing (UWFQ)
scheduler, designed to minimize job response times while ensuring equitable
resource distribution across users and their respective jobs. UWFQ simulates a
virtual fair queuing system and schedules jobs based on their estimated finish
times under a bounded fairness model. To further address task skew and reduce
priority inversions, which are common in Spark workloads, we introduce runtime
partitioning, a method that dynamically refines task granularity based on
expected runtime. We implement UWFQ within the Spark framework and evaluate its
performance using multi-user synthetic workloads and Google cluster traces. We
show that UWFQ reduces the average response time of small jobs by up to 74%
compared to existing built-in Spark schedulers and to state-of-the-art fair
scheduling algorithms.

</details>


### [14] [Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs](https://arxiv.org/abs/2510.15095)
*Md Sabbir Hossain Polak,David Troendle,Byunghyun Jang*

Main category: cs.DC

TL;DR: Hive哈希表是一种高性能、支持动态调整大小的GPU哈希表，通过缓存对齐的桶布局、warp同步并发协议和负载感知动态调整策略，在保持95%高负载因子的同时，比现有GPU哈希表性能提升1.5-2倍。


<details>
  <summary>Details</summary>
Motivation: 现有GPU哈希表实现难以处理并发更新、高负载因子和不规则内存访问模式的问题，需要设计一种能够适应不同工作负载且无需全局重哈希的高性能解决方案。

Method: 采用缓存对齐的打包桶布局存储键值对；使用WABC和WCME两种warp同步并发协议减少竞争；基于负载因子的动态调整策略使用线性哈希进行容量扩展/收缩；四步插入策略处理高竞争情况。

Result: 在NVIDIA RTX 4090上的实验显示，Hive哈希表在混合插入-删除-查询工作负载下，吞吐量比最先进的GPU哈希表高1.5-2倍，在平衡工作负载下达到35亿次更新/秒和近40亿次查询/秒。

Conclusion: Hive哈希表通过创新的设计实现了高负载因子下的优异性能，为GPU加速数据处理提供了可扩展且高效的哈希表解决方案。

Abstract: Hash tables are essential building blocks in data-intensive applications, yet
existing GPU implementations often struggle with concurrent updates, high load
factors, and irregular memory access patterns. We present Hive hash table, a
high-performance, warp-cooperative and dynamically resizable GPU hash table
that adapts to varying workloads without global rehashing.
  Hive hash table makes three key contributions. First, a cache-aligned packed
bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory
access and atomic updates via single-CAS operations. Second, warp-synchronous
concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and
Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic
operation per warp while ensuring lock-free progress. Third, a
load-factor-aware dynamic resizing strategy expands or contracts capacity in
warp-parallel K-bucket batches using linear hashing, maintaining balanced
occupancy. To handle insertions under heavy contention, Hive hash table employs
a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and
overflow-stash fallback. This design provides lock-free fast paths and bounded
recovery cost under contention determined by a fixed eviction depth, while
eliminating ABA hazards during concurrent updates.
  Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains
load factors up to 95% while delivering 1.5-2x higher throughput than
state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed
insert-delete-lookup workloads. On balanced workload, Hive hash table reaches
3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability
and efficiency for GPU-accelerated data processing.

</details>


### [15] [NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)](https://arxiv.org/abs/2510.15122)
*François Ezard,Can Umut Ileri,Jérémie Decouchant*

Main category: cs.DC

TL;DR: NEMO是一个新的区块链执行引擎，结合乐观并发控制(OCC)和对象数据模型来解决高竞争负载下的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着区块链共识算法的效率提升，执行层已成为新的性能瓶颈，特别是在高竞争场景下。现有的并行执行框架（乐观或悲观并发控制）在高竞争负载下性能都会下降。

Method: NEMO引入四个核心创新：1) 对仅使用自有对象的交易采用贪婪提交规则；2) 精细化处理依赖以减少重执行；3) 使用静态可推导的读写提示指导执行；4) 基于优先级的调度器，优先处理能解锁其他交易的交易。

Result: 模拟执行实验表明，NEMO显著减少了冗余计算，在16个工作线程下，吞吐量比最先进的OCC方法Block-STM高42%，比悲观并发控制基线高61%。

Conclusion: NEMO通过结合OCC和对象数据模型，有效解决了高竞争负载下区块链执行层的性能瓶颈问题，显著提升了吞吐量。

Abstract: Following the design of more efficient blockchain consensus algorithms, the
execution layer has emerged as the new performance bottleneck of blockchains,
especially under high contention. Current parallel execution frameworks either
rely on optimistic concurrency control (OCC) or on pessimistic concurrency
control (PCC), both of which see their performance decrease when workloads are
highly contended, albeit for different reasons. In this work, we present NEMO,
a new blockchain execution engine that combines OCC with the object data model
to address this challenge. NEMO introduces four core innovations: (i) a greedy
commit rule for transactions using only owned objects; (ii) refined handling of
dependencies to reduce re-executions; (iii) the use of incomplete but
statically derivable read/write hints to guide execution; and (iv) a
priority-based scheduler that favors transactions that unblock others. Through
simulated execution experiments, we demonstrate that NEMO significantly reduces
redundant computation and achieves higher throughput than representative
approaches. For example, with 16 workers NEMO's throughput is up to 42% higher
than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher
than the pessimistic concurrency control baseline used.

</details>


### [16] [An Elastic Job Scheduler for HPC Applications on the Cloud](https://arxiv.org/abs/2510.15147)
*Aditya Bhosale,Kavitha Chandrasekar,Laxmikant Kale,Sara Kokkila-Schumacher*

Main category: cs.DC

TL;DR: 开发了一个Kubernetes操作符来运行Charm++应用，并提出了基于优先级的弹性作业调度器，能够动态调整HPC作业规模以最大化集群利用率并减少高优先级作业的响应时间。


<details>
  <summary>Details</summary>
Motivation: 随着HPC应用在云环境中的采用增加，需要专门的编程模型和调度器来高效利用云资源。传统MPI等并行编程模型不支持自动缩放，而Charm++通过其可迁移对象范式原生支持动态重缩放。

Method: 开发了Kubernetes操作符来运行Charm++应用，并设计了基于优先级的弹性作业调度器，该调度器能够根据Kubernetes集群状态动态调整作业规模。

Result: 弹性调度器能够以最小开销重缩放HPC作业，与传统静态调度器相比表现出显著的性能改进。

Conclusion: 提出的弹性调度器结合Charm++的动态重缩放能力，在Kubernetes集群上实现了更好的资源利用率和作业响应时间。

Abstract: The last few years have seen an increase in adoption of the cloud for running
HPC applications. The pay-as-you-go cost model of these cloud resources has
necessitated the development of specialized programming models and schedulers
for HPC jobs for efficient utilization of cloud resources. A key aspect of
efficient utilization is the ability to rescale applications on the fly to
maximize the utilization of cloud resources. Most commonly used parallel
programming models like MPI have traditionally not supported autoscaling either
in a cloud environment or on supercomputers. While more recent work has been
done to implement this functionality in MPI, it is still nascent and requires
additional programmer effort. Charm++ is a parallel programming model that
natively supports dynamic rescaling through its migratable objects paradigm. In
this paper, we present a Kubernetes operator to run Charm++ applications on a
Kubernetes cluster. We then present a priority-based elastic job scheduler that
can dynamically rescale jobs based on the state of a Kubernetes cluster to
maximize cluster utilization while minimizing response time for high-priority
jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs
with minimal overhead, demonstrates significant performance improvements over
traditional static schedulers.

</details>


### [17] [Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks](https://arxiv.org/abs/2510.15215)
*Zhimin Qiu,Feng Liu,Yuxiao Wang,Chenrui Hu,Ziyu Cheng,Di Wu*

Main category: cs.DC

TL;DR: 提出基于图神经网络的分布式后端系统流量预测方法，通过图卷积和门控循环结构整合时空特征，显著提升预测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉分布式系统中复杂的依赖关系和动态特征，需要更有效的建模方法来提高流量预测精度。

Method: 将系统抽象为图结构，使用图卷积进行多阶特征传播，结合门控循环结构建模历史序列，通过时空联合建模模块融合图表示与时间依赖，最后用解码器生成未来流量预测。

Result: 在公开分布式系统日志上的实验表明，该方法在不同预测时域和模型深度下均表现稳定且误差较低，各项指标（MSE、RMSE、MAE、MAPE）均优于主流基线方法。

Conclusion: 图神经网络在复杂系统建模中具有巨大潜力，所提方法能显著提升分布式后端系统流量预测的准确性和鲁棒性。

Abstract: This paper addresses the problem of traffic prediction in distributed backend
systems and proposes a graph neural network based modeling approach to overcome
the limitations of traditional models in capturing complex dependencies and
dynamic features. The system is abstracted as a graph with nodes and edges,
where node features represent traffic and resource states, and adjacency
relations describe service interactions. A graph convolution mechanism enables
multi order propagation and aggregation of node features, while a gated
recurrent structure models historical sequences dynamically, thus integrating
spatial structures with temporal evolution. A spatiotemporal joint modeling
module further fuses graph representation with temporal dependency, and a
decoder generates future traffic predictions. The model is trained with mean
squared error to minimize deviations from actual values. Experiments based on
public distributed system logs construct combined inputs of node features,
topology, and sequences, and compare the proposed method with mainstream
baselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed method
achieves stable performance and low error across different prediction horizons
and model depths, significantly improving the accuracy and robustness of
traffic forecasting in distributed backend systems and verifying the potential
of graph neural networks in complex system modeling.

</details>


### [18] [BeLLMan: Controlling LLM Congestion](https://arxiv.org/abs/2510.15330)
*Tella Rajashekhar Reddy,Atharva Deshmukh,Karan Tandon,Rohan Gandhi,Anjaly Parayil,Debopam Bhattacherjee*

Main category: cs.DC

TL;DR: beLLMan控制器通过主动调整LLM应用输出长度来应对系统负载变化，显著降低推理延迟和能耗


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用对底层基础设施不感知，在系统负载变化时无法自适应调整，导致推理延迟增加和用户体验下降

Method: 开发beLLMan控制器，使LLM基础设施能够主动向第一方LLM应用发送信号，根据系统负载动态调整输出长度

Result: 在H100 GPU测试平台上，beLLMan将端到端延迟降低高达8倍，能耗减少25%，同时在高负载期间多服务19%的请求

Conclusion: beLLMan通过系统负载感知的输出长度调整，有效控制了LLM推理延迟并提升了能效

Abstract: Large language model (LLM) applications are blindfolded to the infrastructure
underneath and generate tokens autoregressively, indifferent to the system
load, thus risking inferencing latency inflation and poor user experience. Our
first-cut controller, named beLLMan, enables the LLM infrastructure to actively
and progressively signal the first-party LLM application to adjust the output
length in response to changing system load. On a real testbed with H100 GPUs,
beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end
latency) and reduces energy consumption by 25% (while serving 19% more
requests) during periods of congestion for a summarization workload.

</details>


### [19] [Cloud-Enabled Virtual Prototypes](https://arxiv.org/abs/2510.15355)
*Tim Kraus,Axel Sauer,Ingo Feldner*

Main category: cs.DC

TL;DR: 本文探讨了本地与云端仿真环境之间的权衡，重点关注可扩展性与隐私保护的平衡，旨在提高远程仿真的可信度并促进虚拟原型技术的采用。


<details>
  <summary>Details</summary>
Motivation: 嵌入式系统的快速发展和AI算法的复杂性增加，需要基于虚拟原型技术的强大硬件/软件协同设计方法，同时云计算资源的普及为仿真执行提供了新的基础设施选择。

Method: 分析本地与云端仿真环境的对比，讨论计算基础设施设置对执行性能和数据安全的影响，强调嵌入式AI开发工作流程中高效仿真的关键作用。

Result: 提出了一个解决方案，旨在可持续地提高远程仿真的可信度，并促进虚拟原型实践的应用。

Conclusion: 通过平衡可扩展性与隐私保护，云端仿真环境可以为嵌入式AI开发提供有效的解决方案，但需要解决数据安全和性能优化等关键问题。

Abstract: The rapid evolution of embedded systems, along with the growing variety and
complexity of AI algorithms, necessitates a powerful hardware/software
co-design methodology based on virtual prototyping technologies. The market
offers a diverse range of simulation solutions, each with its unique
technological approach and therefore strengths and weaknesses. Additionally,
with the increasing availability of remote on-demand computing resources and
their adaptation throughout the industry, the choice of the host infrastructure
for execution opens even more new possibilities for operational strategies.
This work explores the dichotomy between local and cloud-based simulation
environments, focusing on the trade-offs between scalability and privacy. We
discuss how the setup of the compute infrastructure impacts the performance of
the execution and security of data involved in the process. Furthermore, we
highlight the development workflow associated with embedded AI and the critical
role of efficient simulations in optimizing these algorithms. With the proposed
solution, we aim to sustainably improve trust in remote simulations and
facilitate the adoption of virtual prototyping practices.

</details>


### [20] [(Almost) Perfect Discrete Iterative Load Balancing](https://arxiv.org/abs/2510.15473)
*Petra Berenbrink,Robert Elsässer,Tom Friedetzky,Hamed Hosseinpour,Dominik Kaaser,Peter Kling,Thomas Sauerwald*

Main category: cs.DC

TL;DR: 该论文研究离散负载平衡问题，通过匹配算法在任意图上实现负载均衡，最终达到最大负载差仅为3的优异结果。


<details>
  <summary>Details</summary>
Motivation: 研究离散负载平衡问题，探索在任意图上通过匹配算法实现高效负载均衡的方法，旨在改进现有结果并证明离散负载平衡与连续负载平衡具有相同的复杂度。

Method: 使用基于匹配的局部平衡方案，每轮通过匹配节点来平均化负载。如果两个节点的负载和为奇数，则随机选择一个节点接收多余令牌。研究覆盖三种模型：匹配模型、平衡电路模型和异步模型。

Result: 证明该离散平衡方案以高概率在渐进匹配连续负载平衡谱界轮数内达到最大负载差为3的优异结果，适用于任意图而非仅限于正则图。

Conclusion: 离散负载平衡与连续负载平衡具有相同的复杂度，且能够在任意图上实现常数级别的负载差异，改进了先前工作的结果。

Abstract: We consider discrete, iterative load balancing via matchings on arbitrary
graphs. Initially each node holds a certain number of tokens, defining the load
of the node, and the objective is to redistribute the tokens such that
eventually each node has approximately the same number of tokens. We present
results for a general class of simple local balancing schemes where the tokens
are balanced via matchings. In each round the process averages the tokens of
any two matched nodes. If the sum of their tokens is odd, the node to receive
the one excess token is selected at random. Our class covers three popular
models: in the matching model a new matching is generated randomly in each
round, in the balancing circuit model a fixed sequence of matchings is applied
periodically, and in the asynchronous model the load is balanced over a
randomly chosen edge.
  We measure the quality of a load vector by its discrepancy, defined as the
difference between the maximum and minimum load across all nodes. As our main
result we show that with high probability our discrete balancing scheme reaches
a discrepancy of $3$ in a number of rounds which asymptotically matches the
spectral bound for continuous load balancing with fractional load.
  This result improves and tightens a long line of previous works, by not only
achieving a small constant discrepancy (instead of a non-explicit, large
constant) but also holding for arbitrary instead of regular graphs. The result
also demonstrates that in the general model we consider, discrete load
balancing is no harder than continuous load balancing.

</details>


### [21] [Retrofitting Service Dependency Discovery in Distributed Systems](https://arxiv.org/abs/2510.15490)
*Diogo Landau,Gijs Blanken,Jorge Barbosa,Nishant Saurabh*

Main category: cs.DC

TL;DR: 提出了一种名为XXXX的新型运行时系统，用于构建进程级服务依赖图，能够在复杂网络路由机制（包括NAT）下准确推断服务依赖关系，无需源代码插桩。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统中复杂的服务依赖关系容易导致级联故障，而NAT等复杂路由技术会模糊服务的实际主机位置，使得现有基于网络元数据的运行时方法无法准确推断服务依赖关系。

Method: XXXX通过在TCP包头中非破坏性地注入元数据来维护协议正确性，即使接收端没有代理也不会影响现有TCP连接，支持部分部署。

Result: 在9种场景下的评估显示，XXXX是唯一能在不同网络配置下保持稳定性能的方法，在大多数场景下精确率和召回率都达到100%，优于或与现有最优方法相当。

Conclusion: XXXX系统能够有效解决NAT环境下的服务依赖图构建问题，提供准确且非破坏性的依赖关系推断能力。

Abstract: Modern distributed systems rely on complex networks of interconnected
services, creating direct or indirect dependencies that can propagate faults
and cause cascading failures. To localize the root cause of performance
degradation in these environments, constructing a service dependency graph is
highly beneficial. However, building an accurate service dependency graph is
impaired by complex routing techniques, such as Network Address Translation
(NAT), an essential mechanism for connecting services across networks. NAT
obfuscates the actual hosts running the services, causing existing run-time
approaches that passively observe network metadata to fail in accurately
inferring service dependencies. To this end, this paper introduces XXXX, a
novel run-time system for constructing process-level service dependency graphs.
It operates without source code instrumentation and remains resilient under
complex network routing mechanisms, including NAT. XXXX implements a
non-disruptive method of injecting metadata onto a TCP packet's header that
maintains protocol correctness across host boundaries. In other words, if no
receiving agent is present, the instrumentation leaves existing TCP connections
unaffected, ensuring non-disruptive operation when it is partially deployed
across hosts. We evaluated XXXX extensively against three state-of-the-art
systems across nine scenarios, involving three network configurations
(NAT-free, internal-NAT, external-NAT) and three microservice benchmarks. XXXX
was the only approach that performed consistently across networking
configurations. With regards to correctness, it performed on par with, or
better than, the state-of-the-art with precision and recall values of 100% in
the majority of the scenarios.

</details>


### [22] [PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training](https://arxiv.org/abs/2510.15596)
*Alicia Golden,Michael Kuchnik,Samuel Hsia,Zachary DeVito,Gu-Yeon Wei,David Brooks,Carole-Jean Wu*

Main category: cs.DC

TL;DR: PRISM是一个性能建模框架，专门针对大规模分布式训练中的性能变异性问题，通过统计方法提供训练时间的概率保证，并探索训练系统的设计和优化空间。


<details>
  <summary>Details</summary>
Motivation: 在数万GPU规模的大模型训练中，性能变异性不可避免，会降低训练效率。在64k GPU规模下已观察到9%的GPU时间变异性，需要理解变异性原因并优化训练系统。

Method: 提出PRISM性能建模框架，采用统计方法量化训练时间的概率保证。通过GPU微基准测试分析不同平台上的性能变异性，并使用真实系统测量验证模型准确性。

Result: PRISM的训练时间预测准确率达到20.8% Kolmogorov-Smirnov距离。研究表明，通过优化计算节点布局可获得1.26倍性能提升潜力，通信内核（如AllGather和ReduceScatter）的优化对减少训练步骤时间变异性贡献最大。

Conclusion: 大规模分布式训练必须考虑性能变异性，PRISM框架为训练系统设计和优化提供了有效工具，通信内核优化是减少变异性的关键。

Abstract: Large model training beyond tens of thousands of GPUs is an uncharted
territory. At such scales, disruptions to the training process are not a matter
of if, but a matter of when -- a stochastic process degrading training
productivity. Dynamic runtime variation will become increasingly more frequent
as training scales up and GPUs are operated in increasingly power-limited and
thermally-stressed environments. At the 64k GPU scale, we already observed 9%
GPU time variability for frontier foundation model training. To understand
potential causes of variability, we analyze GPU microbenchmarks at scale across
a variety of platforms, showing up to 14% variation in GPU performance on GEMM
workloads depending on training hardware and deployed environment.
  Motivated by our analysis and the large design space around performance
variability, we present PRISM -- a performance modeling framework that
considers the stochastic nature of the large-scale distributed training. The
core of PRISM is the statistical method that provides a quantifiable measure
for probabilistic guarantees on training time. Using PRISM, we explore the
design and optimization space of distributed training, from parallelization
methods to next-generation training systems. PRISM is validated with
real-system measurement, showing training time prediction accuracy with 20.8%
Kolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending on
computation node placement, up to 1.26x performance improvement potential is
available if we factor in sensitivities of parallelization strategies to
variation. In addition, we use PRISM to identify kernels to optimize for
reducing performance variability and predict probability of slow-down for
large-scale jobs where variation is magnified. We find optimizing communication
kernels, such as AllGather and ReduceScatter, contribute most to minimizing
variability in training step time.

</details>


### [23] [GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters](https://arxiv.org/abs/2510.15652)
*Ahmad Raeisi,Mahdi Dolati,Sina Darabi,Sadegh Talebi,Patrick Eugster,Ahmad Khonsari*

Main category: cs.DC

TL;DR: 提出了一种基于学习的异构集群资源管理架构，使用两个神经网络来优化机器学习工作负载的资源分配，降低能耗并满足性能要求。


<details>
  <summary>Details</summary>
Motivation: 机器学习对计算资源的需求日益增长，在异构硬件集群中高效分配资源成为关键挑战。由于升级到最新硬件往往不可行，可持续利用现有混合代际资源变得至关重要。

Method: 系统采用在线运行方式，使用两个神经网络：第一个提供新模型在不同硬件上的初始性能估计和共置影响；优化器基于这些估计分配资源；部署后通过第二个神经网络监控实际性能并改进预测。

Result: 该方法能够自适应地学习，随时间推移在异构深度学习集群中做出更有效的资源分配决策，改进对未分配硬件和未观察共置场景的估计。

Conclusion: 提出的迭代学习方法能够在异构硬件集群中有效管理机器学习工作负载，实现能耗最小化和性能要求满足，为可持续利用现有资源提供了可行方案。

Abstract: The growing demand for computational resources in machine learning has made
efficient resource allocation a critical challenge, especially in heterogeneous
hardware clusters where devices vary in capability, age, and energy efficiency.
Upgrading to the latest hardware is often infeasible, making sustainable use of
existing, mixed-generation resources essential. In this paper, we propose a
learning-based architecture for managing machine learning workloads in
heterogeneous clusters. The system operates online, allocating resources to
incoming training or inference requests while minimizing energy consumption and
meeting performance requirements. It uses two neural networks: the first
provides initial estimates of how well a new model will utilize different
hardware types and how it will affect co-located models. An optimizer then
allocates resources based on these estimates. After deployment, the system
monitors real performance and uses this data to refine its predictions via a
second neural network. This updated model improves estimates not only for the
current hardware but also for hardware not initially allocated and for
co-location scenarios not yet observed. The result is an adaptive, iterative
approach that learns over time to make more effective resource allocation
decisions in heterogeneous deep learning clusters.

</details>


### [24] [A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma](https://arxiv.org/abs/2510.15698)
*Sebastian Brandt,Tim Göttlicher*

Main category: cs.DC

TL;DR: 本文在分布式量子计算领域研究了Lovász局部引理问题，证明了量子LOCAL模型中分布式LLL的复杂度下界为2^Ω(log* n)，这是该问题在多种模型中的首个超常数下界。


<details>
  <summary>Details</summary>
Motivation: 近年来量子计算在STOC会议上取得重要进展，但分布式量子计算中的Lovász局部引理问题尚未得到充分研究。本文旨在填补这一空白，为分布式LLL问题提供首个超常数下界。

Method: 通过研究LLL的一个特例——无汇点定向问题，在比量子LOCAL更强的随机在线LOCAL模型中建立下界。开发了全新的下界证明技术，这是首个适用于后量子环境下局部性问题的通用技术。

Result: 证明了分布式LLL和无汇点定向问题的复杂度下界为2^Ω(log* n)，这是该问题在多种研究模型中首次获得的超常数下界，解决了近期提出的开放性问题。

Conclusion: 本文不仅为分布式量子计算中的LLL问题提供了首个超常数下界，还开发了具有广泛应用潜力的新下界证明技术，为后量子时代的局部性问题研究奠定了基础。

Abstract: In this work, we study the Lov\'asz local lemma (LLL) problem in the area of
distributed quantum computing, which has been the focus of attention of recent
advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower
bound of $2^{\Omega(\log^* n)}$ for the complexity of the distributed LLL in
the quantum-LOCAL model. More specifically, we obtain our lower bound already
for a very well-studied special case of the LLL, called sinkless orientation,
in a stronger model than quantum-LOCAL, called the randomized online-LOCAL
model. As a consequence, we obtain the same lower bounds for sinkless
orientation and the distributed LLL also in a variety of other models studied
across different research communities.
  Our work provides the first superconstant lower bound for sinkless
orientation and the distributed LLL in all of these models, addressing recently
stated open questions. Moreover, to obtain our results, we develop an entirely
new lower bound technique that we believe has the potential to become the first
generic technique for proving post-quantum lower bounds for many of the most
important problems studied in the context of locality.

</details>


### [25] [Funky: Cloud-Native FPGA Virtualization and Orchestration](https://arxiv.org/abs/2510.15755)
*Atsushi Koshiba,Charalampos Mainas,Pramod Bhatotia*

Main category: cs.DC

TL;DR: Funky是一个面向云原生应用的FPGA感知编排引擎，通过FPGA虚拟化、状态管理和编排组件解决了FPGA在云环境中的编排问题，实现了高性能、高利用率和容错能力。


<details>
  <summary>Details</summary>
Motivation: FPGA在云原生环境中的采用面临障碍，因为FPGA缺乏虚拟化、隔离和抢占支持，导致云提供商无法提供FPGA编排服务，造成可扩展性、灵活性和弹性不足。

Method: Funky通过三个主要贡献实现FPGA编排：(1) FPGA虚拟化创建轻量级沙箱，(2) FPGA状态管理支持任务抢占和检查点，(3) 遵循CRI/OCI标准的FPGA感知编排组件。

Result: 在4台x86服务器和Alveo U50 FPGA卡上的评估显示，Funky能够移植23个OpenCL应用，仅修改3.4%源代码，OCI镜像比AMD的FPGA Docker容器小28.7倍，性能开销仅7.4%，并提供强隔离和分布式FPGA编排。

Conclusion: Funky通过大规模集群测试验证了其可扩展性、容错能力和调度效率，为云原生环境中的FPGA编排提供了有效解决方案。

Abstract: The adoption of FPGAs in cloud-native environments is facing impediments due
to FPGA limitations and CPU-oriented design of orchestrators, as they lack
virtualization, isolation, and preemption support for FPGAs. Consequently,
cloud providers offer no orchestration services for FPGAs, leading to low
scalability, flexibility, and resiliency.
  This paper presents Funky, a full-stack FPGA-aware orchestration engine for
cloud-native applications. Funky offers primary orchestration services for FPGA
workloads to achieve high performance, utilization, scalability, and fault
tolerance, accomplished by three contributions: (1) FPGA virtualization for
lightweight sandboxes, (2) FPGA state management enabling task preemption and
checkpointing, and (3) FPGA-aware orchestration components following the
industry-standard CRI/OCI specifications.
  We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA
cards. Our evaluation highlights that Funky allows us to port 23 OpenCL
applications from the Xilinx Vitis and Rosetta benchmark suites by modifying
3.4% of the source code while keeping the OCI image sizes 28.7 times smaller
than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only
7.4% performance overheads compared to native execution, while providing
virtualization support with strong hypervisor-enforced isolation and
cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate
Funky's orchestration services in a large-scale cluster using Google production
traces, showing its scalability, fault tolerance, and scheduling efficiency.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [26] [TKHist: Cardinality Estimation for Join Queries via Histograms with Dominant Attribute Correlation Finding](https://arxiv.org/abs/2510.15368)
*Renrui Li,Qingzhi Ma,Jiajie Xu,Lei Zhao,An Liu*

Main category: cs.DB

TL;DR: TKHist是一种新颖的基数估计方法，通过放宽直方图的均匀性假设来改进多表连接查询的基数估计，同时提出主导连接路径相关性发现算法来处理连接键与过滤谓词之间的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的多表连接查询基数估计方法虽然提高了准确性，但带来了更高的空间开销、延迟和复杂性，特别是在与二元连接框架集成时。

Method: TKHist通过捕获分箱非均匀性信息来准确估计无过滤谓词的连接查询基数，并提出主导连接路径相关性发现算法来管理连接键与过滤谓词之间的相关性。

Result: 在流行基准测试上的广泛实验表明，TKHist相比最先进方法将误差方差降低了2-3个数量级，同时保持相当或更低的内存使用。

Conclusion: TKHist在保持低内存使用的同时显著提高了基数估计的准确性，解决了现有方法在空间开销和复杂性方面的挑战。

Abstract: Cardinality estimation has long been crucial for cost-based database
optimizers in identifying optimal query execution plans, attracting significant
attention over the past decades. While recent advancements have significantly
improved the accuracy of multi-table join query estimations, these methods
introduce challenges such as higher space overhead, increased latency, and
greater complexity, especially when integrated with the binary join framework.
In this paper, we introduce a novel cardinality estimation method named TKHist,
which addresses these challenges by relaxing the uniformity assumption in
histograms. TKHist captures bin-wise non-uniformity information, enabling
accurate cardinality estimation for join queries without filter predicates.
Furthermore, we explore the attribute independent assumption, which can lead to
significant over-estimation rather than under-estimation in multi-table join
queries. To address this issue, we propose the dominating join path correlation
discovery algorithm to highlight and manage correlations between join keys and
filter predicates. Our extensive experiments on popular benchmarks demonstrate
that TKHist reduces error variance by 2-3 orders of magnitude compared to SOTA
methods, while maintaining comparable or lower memory usage.

</details>


### [27] [Optimizing Data Lakes' Queries](https://arxiv.org/abs/2510.15445)
*Gregory,Weintraub*

Main category: cs.DB

TL;DR: 提出了一种在云数据湖中优化查询性能的方法，通过识别查询的最小覆盖文件集来减少网络数据传输。


<details>
  <summary>Details</summary>
Motivation: 云数据湖采用计算与存储分离架构，每次查询都需要通过网络传输数据，这会降低计算性能并消耗大量网络带宽。

Method: 建立理论框架，引入'查询覆盖集'概念，识别每个查询需要访问的最小文件集合，并仅在该子集上执行查询。

Result: 通过最小化需要从存储层传输到计算层的文件数量，显著提升了查询性能。

Conclusion: 提出的查询覆盖集方法能有效解决云数据湖架构中的网络传输瓶颈问题，为查询性能优化提供了理论基础。

Abstract: Cloud data lakes provide a modern solution for managing large volumes of
data. The fundamental principle behind these systems is the separation of
compute and storage layers. In this architecture, inexpensive cloud storage is
utilized for data storage, while compute engines are employed to perform
analytics on this data in an "on-demand" mode. However, to execute any
calculations on the data, it must be transferred from the storage layer to the
compute layer over the network for each query. This transfer can negatively
impact calculation performance and requires significant network bandwidth. In
this thesis, we examine various strategies to enhance query performance within
a cloud data lake architecture. We begin by formalizing the problem and
proposing a straightforward yet robust theoretical framework that clearly
outlines the associated trade-offs. Central to our framework is the concept of
a "query coverage set," which is defined as the collection of files that need
to be accessed from storage to fulfill a specific query. Our objective is to
identify the minimal coverage set for each query and execute the query
exclusively on this subset of files. This approach enables us to significantly
improve query performance.

</details>
