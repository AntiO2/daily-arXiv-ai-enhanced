<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.SE](#cs.SE) [Total: 17]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis](https://arxiv.org/abs/2510.12642)
*Meihui Zhang,Liming Wang,Chi Zhang,Zhaojing Luo*

Main category: cs.DB

TL;DR: Aixel是一个统一、自适应、可扩展的AI驱动数据分析系统，通过四层架构解决现有系统在数据集成、学习优化和成本控制方面的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 现代数据分析需要集成多源异构数据，同时满足准确性、延迟和成本要求。现有系统在数据库、分析库和调优服务之间分散，导致用户交互复杂、适应性有限、性能不佳和组件扩展性差。

Method: 采用四层架构：应用层、任务层、模型层和数据层。任务层提供声明式接口捕获用户意图，解析为可执行操作计划；模型层提供版本化存储和自适应构建；数据层提供统一数据管理能力。

Result: Aixel系统实现了用户友好、自适应、高效和可扩展的数据分析平台，支持重用、缓存和组件共享以提高效率。

Conclusion: Aixel通过统一的多层架构设计，有效解决了现有数据分析系统的碎片化问题，为AI驱动的数据分析提供了更优的解决方案。

Abstract: A growing trend in modern data analysis is the integration of data management
with learning, guided by accuracy, latency, and cost requirements. In practice,
applications draw data of different formats from many sources. In the
meanwhile, the objectives and budgets change over time. Existing systems handle
these applications across databases, analysis libraries, and tuning services.
Such fragmentation leads to complex user interaction, limited adaptability,
suboptimal performance, and poor extensibility across components. To address
these challenges, we present Aixel, a unified, adaptive, and extensible system
for AI-powered data analysis. The system organizes work across four layers:
application, task, model, and data. The task layer provides a declarative
interface to capture user intent, which is parsed into an executable operator
plan. An optimizer compiles and schedules this plan to meet specified goals in
accuracy, latency, and cost. The task layer coordinates the execution of data
and model operators, with built-in support for reuse and caching to improve
efficiency. The model layer offers versioned storage for index, metadata,
tensors, and model artifacts. It supports adaptive construction, task-aligned
drift detection, and safe updates that reuse shared components. The data layer
provides unified data management capabilities, including indexing,
constraint-aware discovery, task-aligned selection, and comprehensive feature
management. With the above designed layers, Aixel delivers a user friendly,
adaptive, efficient, and extensible system.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: eye2vec是一个分析软件开发者在阅读源代码时眼动的基础设施，使用分布式表示将连续注视表示为语法元素间的转换，减少手动分析工作。


<details>
  <summary>Details</summary>
Motivation: 传统眼动追踪研究需要研究者预先选择分析目标并开发分析方法，不同级别的关注区域定义会导致不同结果，且分析过程依赖耗时的手动工作。

Method: 使用分布式表示将连续两个注视表示为语法元素间的转换，便于采用多样化的数据分析方法。

Result: 该方法能够更有效地分析程序理解过程中的眼动数据，提供丰富的语义解释。

Conclusion: eye2vec通过分布式表示简化了软件开发者眼动数据的分析过程，提高了分析效率和语义丰富性。

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [3] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 本文提出将LLM的token预算视为注意力预算，将任务感知的文本缩减作为语言-数据系统的核心设计原则，以解决数据密集型工作流中文本数据冗长、嘈杂的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的文本密集型数据（如日志、遥测数据）通常冗长且嘈杂，直接输入LLM成本高昂、不环保且与任务目标不一致。现有LLM效率优化主要关注模型或架构层面，而输入侧缩减问题尚未充分探索。

Method: 将输入侧缩减视为注意力分配而非压缩，优先保留与下游任务最相关的信息。提出构建基准测试、设计自适应缩减管道、将token预算感知预处理集成到数据库和检索系统中。

Result: 提出了一个研究框架，将文本缩减作为LLM-数据集成系统的核心组件，以更有效地利用有限的注意力资源。

Conclusion: 通过将稀缺的注意力资源引导到嘈杂数据流中的有意义信号，可以实现可扩展、准确和可持续的LLM-数据集成。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [4] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: Lingxi是一个基于历史问题修复数据提取过程性知识的软件问题解决框架，通过分层抽象机制构建知识，指导智能体解决仓库级别的问题，在SWE-bench基准上达到74.6%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在解决复杂仓库级别问题时存在两个关键限制：缺乏过程性知识（问题修复的步骤和原理）以及依赖大量计算资源进行盲目探索。

Method: Lingxi通过分层抽象机制离线构建过程性知识，让智能体学习修复的'如何'和'为什么'；在线应用时采用知识驱动的扩展方法，利用相似问题的过程性知识从多角度智能分析目标问题。

Result: 在SWE-bench Verified基准的Past@1设置下，Lingxi成功解决了74.6%的错误，显著优于五种最先进技术（领先5.4%到14.9%）。消融研究确认性能提升直接来自过程性知识的使用。

Conclusion: 过程性知识是Lingxi成功的关键，其中'设计模式和编码实践'是最重要的知识方面，且不同知识方面在不同阶段（分析、规划、修复）的作用会切换。

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [5] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: DMAS-Forge是一个框架，旨在简化分布式多智能体AI应用的部署和测试，通过解耦应用逻辑与部署选择，自动生成必要的连接代码和配置。


<details>
  <summary>Details</summary>
Motivation: 随着AI应用越来越依赖具有不同角色、专用工具和内存层的多个智能体来解决复杂任务，部署和测试这些分布式系统仍然是一项艰巨且劳动密集型的任务。

Method: DMAS-Forge框架通过解耦应用逻辑与具体部署选择，透明地生成必要的连接代码和配置，以最小的手动工作在各种部署场景中启动分布式多智能体应用。

Result: 提出了DMAS-Forge的愿景、设计原则和原型，展示了该框架如何简化分布式多智能体AI应用的部署过程。

Conclusion: DMAS-Forge为分布式多智能体AI应用的部署提供了有效的解决方案，并讨论了该方法的机遇和未来工作方向。

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [6] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: TorchCor是一个基于PyTorch的高性能Python库，用于在通用GPU上进行心脏电生理学有限元模拟，显著加速了大型3D网格的CEP模拟。


<details>
  <summary>Details</summary>
Motivation: 心脏电生理学模拟通常需要高性能计算资源，但许多研究团队和临床医生无法获得这些资源。

Method: 基于PyTorch构建，使用有限元方法在通用GPU上进行CEP模拟。

Result: TorchCor显著加速了CEP模拟，特别是对于大型3D网格，其求解器精度通过制造分析解和N版本基准问题验证。

Conclusion: TorchCor是一个免费且无使用限制的高性能CEP模拟库，适用于学术和商业用途。

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [7] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 该论文通过实证研究证明，在代码表示中融入上下文信息（如版本历史和调用图）能显著提升深度学习模型在代码克隆检测和代码摘要任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型主要依赖源代码本身，忽略了版本历史和结构关系等上下文信息，限制了模型理解代码演变和运行方式的能力。

Method: 在两个数据集（SeSaMe和CodeSearchNet）上评估五个代表性模型（CodeBERT、GraphCodeBERT、CodeT5、PLBART、ASTNN），比较仅使用代码和使用上下文增强设置下的性能。

Result: 上下文信息普遍提升模型性能：版本历史持续提升克隆检测（如CodeT5 F1提升15.92%）和代码摘要（如GraphCodeBERT METEOR提升5.56%），调用图效果因模型和任务而异。多上下文组合带来更大增益（最高提升21.48% macro-F1）。

Conclusion: 上下文信号有潜力显著增强代码理解能力，为优化神经软件工程模型中的上下文编码开辟了新方向。

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [8] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: SEMAP是一个协议层方法，通过三个核心软件工程设计原则来改进多智能体LLM系统：明确行为契约建模、结构化消息传递、生命周期引导执行与验证，显著减少了软件开发任务中的失败率。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体LLM系统在模拟协作开发工作流时经常失败，主要由于缺乏基础软件工程结构化原则导致的三个核心缺陷：规范不足、协调错位和不适当的验证。

Method: 提出SEMAP协议，基于Google的A2A基础设施实现三个核心SE设计原则：明确行为契约建模、结构化消息传递、生命周期引导执行与验证。

Result: 在代码开发中，函数级开发总失败率降低69.6%，部署级开发降低56.7%；在漏洞检测中，Python任务失败减少47.4%，C/C++任务减少28.2%。

Conclusion: SEMAP通过引入软件工程结构化原则，有效解决了多智能体LLM系统的核心缺陷，显著提升了软件开发任务的性能和可靠性。

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [9] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: iCodeReviewer是一种基于大语言模型的自动化安全代码审查方法，通过混合提示架构和路由算法提高安全问题的覆盖率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化安全代码审查方法（如静态分析、深度学习模型和提示方法）存在精度和覆盖率有限、缺乏全面评估的问题，需要更有效的解决方案。

Method: 采用基于大语言模型的混合提示架构，包含多个动态提示专家管道来检查特定安全问题，并通过路由算法基于代码特征激活必要专家以减少误报。

Result: 在内部数据集上实现63.98%的F1分数，在生产环境中生成的审查评论接受率高达84%。

Conclusion: iCodeReviewer通过创新的混合提示架构和路由算法，在安全代码审查中表现出色，能够有效识别和定位安全问题。

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [10] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: 本文通过LLM辅助筛选9000多篇论文，研究了软件工程与心理学交叉领域中言语数据的使用情况，发现SE经常借用PSY方法，但反向影响很少


<details>
  <summary>Details</summary>
Motivation: 理解软件开发者的思维、决策和行为是软件工程的关键挑战，言语化技术提供了一种轻量级方法来研究这些认知方面

Method: 采用GPT辅助的筛选管道，基于标题评估9000多篇论文的相关性，并进行人工验证

Result: GPT输出与人工评审高度一致，分歧率仅13%；主要主题与SE工艺相关，人本主题较少；SE频繁借用PSY方法，但反向影响罕见

Conclusion: LLM可以有效支持跨学科文献综述，SE与PSY的交叉研究存在不对称性，需要更多关注人本主题

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [11] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: 本文研究了新兴的Vibe Coding范式，这是一种强调开发者与AI系统之间直觉、情感驱动和即兴交互的编程方法，与传统AI辅助开发工具形成对比。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI特别是大语言模型如何改变软件开发实践，研究Vibe Coding这一新兴范式与传统的AI辅助编程（如GitHub Copilot）有何不同。

Method: 通过对10名经验丰富的软件从业者进行5次半结构化访谈，识别出五个主题维度：创造力、可持续性、编程未来、协作和批评。

Result: 将Vibe Coding概念化为"共同漂流"的隐喻，与主流的"共同驾驶"视角形成对比。Vibe Coding重新配置了开发者角色，模糊了专业与非专业开发者之间的界限。

Conclusion: Vibe Coding代表了编程文化的有意义转变，虽然支持新颖的表达形式和快速原型设计，但也带来了可重复性、可扩展性和包容性方面的挑战，值得在HCI和软件工程领域进一步研究。

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [12] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: 该研究扩展了之前对云环境性能变异性的调查，特别关注重大全球事件（如黑色星期五）对性能基准测试结果的影响。


<details>
  <summary>Details</summary>
Motivation: 云环境中基准测试结果的可重现性和可信度常因性能的高变异性而受到质疑，研究旨在量化这种变异性并探索重大事件的影响。

Method: 通过在不同时间段重复执行流处理应用基准测试，并特别分析黑色星期五等重大全球事件期间的性能数据。

Result: 确认应用层面确实存在性能变异性，但比通常假设的要小；发现了细微的每日和每周性能模式；正在调查重大事件的影响。

Conclusion: 云环境性能变异性确实存在但被高估，需要进一步研究重大事件对基准测试结果的具体影响。

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [13] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: 本文通过SysMLv2开发了DarTwin DSL，用于数字孪生演化建模，评估了SysMLv2在领域特定语言创建方面的能力，并指出了当前工具在图形表示方面的局限性。


<details>
  <summary>Details</summary>
Motivation: SysMLv2新增了内置的领域特定概念和语言扩展机制，这有望促进领域特定语言(DSL)的创建以及与现有系统描述和技术设计的接口。

Method: 通过具体用例评估SysMLv2的能力，开发DarTwin DSL来形式化现有的DarTwin数字孪生演化表示法，利用SysMLv2实现DarTwin演化模板的广泛应用。

Result: 成功演示了DarTwin DSL，但发现当前SysMLv2工具在图形表示能力方面存在局限性。

Conclusion: 这项工作将模型驱动工程(MDE)与数字孪生结合，利用SysMLv2的发布，为系统工程中的数字孪生演化管理提供了系统化方法。

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [14] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: Diff-XYZ是一个用于代码差异理解的紧凑基准，包含三个监督任务：应用差异、反应用差异和差异生成，基于真实提交数据构建。


<details>
  <summary>Details</summary>
Motivation: 可靠处理代码差异对于大规模编辑和重构仓库的智能体至关重要，需要评估和改进LLM处理代码差异的能力。

Method: 构建包含<旧代码,新代码,差异>三元组的基准数据集，使用统一差异格式，并进行跨格式比较研究。

Result: 研究发现不同格式应根据使用场景和模型大小选择，搜索替换格式适合大模型生成差异，但不适合差异分析和小模型。

Conclusion: Diff-XYZ基准为评估和改进LLM处理代码差异提供了可重复的基础，有助于未来差异格式和代码编辑模型的发展。

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [15] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: 开发了两个专门针对软件工程领域的共情量表：EmpathiSEr-P（测量从业者间共情）和EmpathiSEr-U（测量从业者对用户的共情），填补了软件工程领域缺乏验证共情测量工具的空缺。


<details>
  <summary>Details</summary>
Motivation: 现有通用共情量表基于心理学和医疗领域，其语言、场景和假设在软件工程环境中不具意义或难以解释，无法捕捉软件工程中角色特定和领域约束的共情表达。

Method: 采用严谨的多阶段方法，包括专家评估、认知访谈和两次从业者调查，基于从业者知情概念框架开发量表，涵盖认知共情、情感共情和共情反应三个维度。

Result: 成功开发出首个心理测量学验证的软件工程专用共情量表，为评估共情和设计共情增强干预提供了工具。

Conclusion: 这些量表为软件工程研究者和从业者提供了在软件团队和用户互动中评估共情和设计干预措施的有效工具。

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [16] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: 该论文通过实证研究评估了简化的能源和碳模型（如Digst和DIMPACT）在反映真实网站用户交互能耗时的准确性，发现常用的恒定功率近似法与实际测量存在显著偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管简化的能源和碳模型被广泛采用，但其准确性和精确性仍未得到充分探索。研究旨在评估这些模型在反映真实用户与常见网站类别交互时的能耗表现。

Method: 通过预定义用户流程在四种笔记本电脑平台上测量购物、预订、导航和新闻服务的能耗使用情况。

Result: 结果显示常用的恒定功率近似法（P * t）与实际测量能耗存在显著差异，具体取决于网站类别、设备类型和任务特征。模型偏差是系统性的而非随机的。

Conclusion: 研究强调了在可重复的可持续性报告框架中需要采用类别感知和设备反映的功率参数，以提高模型的准确性。

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [17] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: 本研究通过系统文献综述分析了动态系统之系统(SoSs)中的运行时组合问题，识别了核心挑战、解决方案、工具和评估方法，旨在指导研究人员和从业者开发动态可组合的SoSs。


<details>
  <summary>Details</summary>
Motivation: 现代系统之系统(SoSs)在动态环境(如智慧城市、自动驾驶)中运行，运行时组合对于适应性至关重要，但文献缺乏对此的综合分析。

Method: 采用系统文献综述(SLR)方法，筛选2019-2024年间1,774项研究，选择80项主要研究进行主题分析(TA)。

Result: 挑战分为四类：建模与分析、弹性操作、系统编排和CSs异质性。解决方案涵盖七个领域：协同仿真与数字孪生、语义本体、集成框架、自适应架构、中间件、形式化方法和AI驱动的弹性。

Conclusion: 分析揭示了自主性与协调性、建模-现实差距等张力，呼吁标准化评估指标、可扩展的去中心化架构和跨领域框架。

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


### [18] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: NL2Contract任务使用LLM将自然语言转换为形式化功能契约（包括前置条件和后置条件），相比仅生成后置条件，能显著减少自动软件验证中的误报。


<details>
  <summary>Details</summary>
Motivation: 解决自动软件验证器在实际应用中因缺乏形式化规范而受限的问题，以及LLM仅生成后置条件时导致的验证误报问题。

Method: 引入NL2Contract任务，使用LLM从代码中的自然语言提示（如函数名、注释）推断形式化功能契约，包括前置条件和后置条件。

Result: LLM能有效生成对所有可能输入都可靠的功能契约，生成的契约具有足够的表达能力来区分错误和正确行为，与仅使用后置条件相比，验证器产生的误报更少。

Conclusion: LLM推断的前置条件与开发者意图高度一致，使得自动软件验证器能够捕获真实世界中的错误。

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [19] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe是一个动态重构LLM服务管道的系统，通过细粒度模型划分、运行时管道重构和拓扑感知资源分配，显著提升资源效率并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统采用静态管道配置，无法适应动态工作负载变化，导致资源效率低下和GPU碎片化问题严重。

Method: 将模型分解为细粒度阶段，基于实时请求模式分析智能调整管道粒度，实现细粒度模型划分、运行时管道重构和拓扑感知资源分配。

Result: 在82-GPU集群上评估显示，FlexPipe实现8.5倍资源效率提升，延迟降低38.3%，GPU预留需求从峰值容量的75%降至30%。

Conclusion: FlexPipe通过动态管道重构有效解决了LLM服务中的资源效率问题，显著降低了部署成本并提升了服务质量。

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [20] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: 本文提出以单个计算节点作为跨平台性能比较的基础单位，并提供节点到节点扩展研究的设置、运行和分析指南。


<details>
  <summary>Details</summary>
Motivation: 由于高性能计算架构日益多样化，研究人员需要跨平台比较代码性能和可扩展性，但缺乏相关指导。

Method: 将单个计算节点作为跨平台比较的基础单位，提供节点到节点扩展研究的设置、运行和分析指南，包括结果展示模板。

Result: 通过多个案例研究展示了这种方法的优势。

Conclusion: 以节点为基元进行跨平台性能比较是有效的方法，为研究人员提供了实用的指导框架。

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [21] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: 提出了两种GPU加速的进程映射算法，分别采用层次多分割和现代多级图划分流水线，相比CPU算法获得超过300倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 受GPU图划分器近期成功的启发，为超级计算机中的进程映射问题开发GPU加速算法，以平衡计算负载并最小化通信成本。

Method: 第一种算法采用层次多分割，沿超级计算机层次结构划分任务图；第二种算法将进程映射直接集成到现代多级图划分流水线中，利用GPU并行性加速关键阶段。

Result: 两种方法相比最先进的CPU算法获得超过300倍的加速比。第一种算法通信成本平均增加约10%，第二种算法几何平均加速比为77.6倍，峰值达598倍，但解质量较低。

Conclusion: 这是首个基于GPU的进程映射算法，在保持竞争力的同时显著提升了计算效率。

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [22] [Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness](https://arxiv.org/abs/2510.12274)
*Hao Jiang,Meng Qin,Ruijie Kuai,Dandan Liang*

Main category: cs.DC

TL;DR: Metronome是一种面向云原生网络的网络感知和优先级感知调度机制，针对具有周期性流量模式和动态带宽需求的分布式训练作业，通过时分复用和弹性网络资源分配模型，提升集群资源利用率和作业完成效率。


<details>
  <summary>Details</summary>
Motivation: 随着计算需求快速增长，云原生网络需要解决高效资源协调的挑战，特别是应对集群中网络带宽动态波动的问题。现有调度机制难以有效处理分布式训练作业的周期性流量特征和动态带宽需求。

Method: 采用时分复用方法，利用作业流量特征构建弹性网络资源分配模型；结合多目标优化策略，同时考虑延迟和作业优先级；通过监控集群状态进行动态重配置。

Result: 在13个常见机器学习模型上的实验表明，相比现有Kubernetes调度机制，Metronome可将作业完成时间最多减少19.50%，平均带宽利用率最多提升23.20%。

Conclusion: Metronome能够有效提升集群资源利用率并保证服务性能，为云原生网络中具有周期性流量特征的作业提供了高效的调度解决方案。

Abstract: With the rapid growth in computing power demand, cloud native networks have
emerged as a promising solution to address the challenges of efficient resource
coordination, particularly in coping with the dynamic fluctuations of network
bandwidth in clusters. We propose Metronome, a network-aware and priority-aware
scheduling mechanism for cloud native networks. This mechanism is designed to
support jobs that exhibit periodic traffic patterns and dynamic bandwidth
demands, particularly in the context of distributed training. Specifically,
Metronome employs a time-division multiplexing approach that leverages job
traffic characteristics to construct an elastic network resource allocation
model, enabling efficient bandwidth sharing across multiple jobs. In addition,
it incorporates a multi-objective optimization strategy, jointly considering
latency and job priorities to achieve globally optimal as well as dynamic
resource allocation. Finally, Metronome adapts to the dynamic environment by
monitoring the cluster and performing reconfiguration operations. Extensive
experiments with 13 common machine learning models demonstrate that Metronome
can enhance cluster resource utilization while guaranteeing service
performance. Compared with the existing Kubernetes scheduling mechanisms across
multiple scenarios, Metronome reduces job completion time by up to 19.50% while
improving average bandwidth utilization by up to 23.20%.

</details>


### [23] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: 提出基于Kubernetes的工具，支持在数据共享管道中延迟、非侵入式地应用云设计模式，无需修改服务源代码，同时收集能耗指标以支持能源感知决策。


<details>
  <summary>Details</summary>
Motivation: 在联邦环境中构建消费者特定的数据共享管道时，传统云设计模式的预定义和嵌入会损害模块化、降低可重用性，与管道的动态、消费者驱动特性冲突。

Method: 开发基于Kubernetes的工具，支持自动化模式注入，无需修改服务源代码，同时收集能耗指标。

Result: 该工具能够在不影响管道灵活可组合结构的前提下，实现云设计模式的延迟和非侵入式应用，并提供能耗数据支持决策。

Conclusion: 该方法解决了在可重用数据共享管道中集成云设计模式的挑战，平衡了模块化、可重用性与模式应用的需求，同时支持能源效率优化。

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [24] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: TALP-Pages是一个易于集成的框架，为开发者提供代码性能的快速反馈，通过可视化性能因子回归和扩展效率表来检测性能退化。


<details>
  <summary>Details</summary>
Motivation: 在HPC代码开发过程中，需要早期检测性能退化并获得应用扩展行为的洞察，以改进开发工作流程。

Method: 基于TALP工具实时收集性能指标，通过适合CI的文件夹结构生成HTML报告，包含性能因子回归可视化和扩展效率表。

Result: 与基于追踪的工具相比，TALP-Pages在开销和后处理要求方面表现更好，能更快生成扩展效率表且在更严格的资源约束下工作。

Conclusion: TALP-Pages方法易于使用且有效，通过最小改动扩展GENE-X的CI设置，能够检测和解释性能改进。

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>


### [25] [Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT](https://arxiv.org/abs/2510.12597)
*Ilya Baldin,Michael Goodrich,Vardan Gyurjyan,Graham Heyes,Derek Howard,Yatish Kumar,David Lawrence,Brad Sawatzky,Stacey Sheldon,Carl Timmer*

Main category: cs.DC

TL;DR: EJFAT架构通过FPGA加速实现边缘到计算集群的负载均衡，支持实验数据流的直接处理，提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决JLab科学项目和数据中心对高吞吐量、低延迟数据处理的需求，支持时间关键型数据采集系统和工作流。

Method: 采用FPGA加速技术处理压缩、分片、UDP包目标重定向(NAT)、解压缩和重组，实现边缘与集群计算的无缝集成。

Result: 在JLab、ESnet和LBNL的测试中展示了架构的有效性，并与DOE的集成研究基础设施(IRI)等活动形成协同效应。

Conclusion: EJFAT架构为未来数据中心提供了高效的数据处理解决方案，特别适用于需要高吞吐量和低延迟的科学应用场景。

Abstract: Thomas Jefferson National Accelerator Facility (JLab) has partnered with
Energy Sciences Network (ESnet) to define and implement an edge to compute
cluster computational load balancing acceleration architecture. The ESnet-JLab
FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to
address compression, fragmentation, UDP packet destination redirection (Network
Address Translation (NAT)) and decompression and reassembly.
  EJFAT seamlessly integrates edge and cluster computing to support direct
processing of streamed experimental data. This will directly benefit the JLab
science program as well as data centers of the future that require high
throughput and low latency for both time-critical data acquisition systems and
data center workflows.
  The EJFAT project will be presented along with how it is synergistic with
other DOE activities such as an Integrated Research Infrastructure (IRI), and
recent results using data sources at JLab, an EJFAT LB at ESnet, and
computational cluster resources at Lawrence Berkeley National Laboratory
(LBNL).

</details>


### [26] [A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices](https://arxiv.org/abs/2510.12705)
*Evelyne Ringoot,Rabab Alomairy,Alan Edelman*

Main category: cs.DC

TL;DR: 本文提出了首个GPU算法，用于将带状矩阵转换为双对角形式，作为SVD的关键步骤。该算法通过优化GPU吞吐量，在多种GPU硬件上实现高性能，显著超越了CPU实现。


<details>
  <summary>Details</summary>
Motivation: 带状矩阵到双对角形式的转换是SVD计算的关键步骤，传统上被认为不适合GPU计算，因为它是内存带宽受限的。但现代GPU硬件的发展（如更大的L1内存）改变了这一现状。

Method: 基于CPU多核并行缓存高效凸点追逐算法，适配优化GPU吞吐量。使用Julia语言的数组抽象和KernelAbstractions，实现跨NVIDIA、AMD、Intel和Apple Metal GPU的硬件和数据精度无关函数。开发硬件感知性能模型，识别关键超参数。

Result: GPU算法在1024x1024矩阵大小上超越多线程CPU高性能库PLASMA和SLATE，在32k x 32k矩阵上性能提升超过100倍。算法性能随矩阵带宽大小线性增长。

Conclusion: 这项工作突破了内存带宽和矩阵带宽的限制，在GPU上实现了数量级更快的带状矩阵到双对角形式的转换算法。

Abstract: The reduction of a banded matrix to a bidiagonal form is a crucial step in
the Singular Value Decomposition (SVD), a cornerstone of scientific computing
and AI. Despite being a highly parallel algorithm, it was previously believed
to be unsuitable for GPU computation because it is memory bandwidth-bound.
Recent developments in GPU hardware, including larger L1 memory per Streaming
Multiprocessor/Compute Unit, have changed that. We present the first GPU
algorithm for reducing a banded matrix to bidiagonal form as part of the
NextLA.jl open-source software package. Our algorithm is based on previous
CPU-based multicore parallel cache-efficient bulge chasing algorithms and
adapted to optimize for GPU throughput. We leverage Julia Language's Array
abstractions and KernelAbstractions to implement a single hardware- and data
precision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for
half, single, and double precision, and examine performance optimization across
hardware architectures and data precision. We also develop a hardware-aware
performance model and identify key hyperparameters, such as inner tilewidth and
block concurrency, that govern optimal GPU execution for bandwidth-bound
workloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU
can outperform CPU-based implementations: the GPU algorithm outperforms
multithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size
1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,
the performance of the algorithm increases linearly with matrix bandwidth size,
making faster reduction of larger matrix bandwidths now also possible. With
this work, we break memory bandwidth barriers, as well as matrix bandwidth
barriers, resulting in orders-of-magnitude faster algorithms for the reduction
of banded matrices to bidiagonal form on the GPU.

</details>
