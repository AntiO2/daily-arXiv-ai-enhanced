{"id": "2601.03364", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03364", "abs": "https://arxiv.org/abs/2601.03364", "authors": ["Nadia Damianova", "Santiago Berrezueta-Guzman"], "title": "The Anatomy of a Successful Student Scrum Team: Motivation, Personalities, and Academic Adaptation", "comment": "Preprint submitted to Elsevier", "summary": "Agile methods, and Scrum in particular, are widely taught in software engineering education; however, there is limited empirical evidence on how these practices function in long-running, student-led projects under academic and hybrid work constraints. This paper presents a year-long case study of an eight-person student development team tasked with designing and implementing a virtual reality game that simulates a university campus and provides program-related educational content. We analyze how the team adapted Scrum practices (sprint structure, roles, backlog management) to fit semester rhythms, exams, travel, and part-time availability, and how communication and coordination were maintained in a hybrid on-site/remote environment. Using qualitative observations and artifacts from Discord, Notion, and GitHub, as well as contribution metrics and a custom communication effectiveness index (score: 0.76/1.00), we evaluate three dimensions: (1) the effectiveness of collaboration tools, (2) the impact of hybrid work on communication and productivity, and (3) the feasibility of aligning Scrum with academic timelines. Our findings show that (i) lightweight, tool-mediated coordination enabled stable progress even during remote periods; (ii) one-week sprints and flexible ceremonies helped reconcile Scrum with academic obligations; and (iii) shared motivation, role clarity, and compatible working styles were as critical as process mechanics. We propose practical recommendations for instructors and student teams adopting agile methods in hybrid, project-based learning settings."}
{"id": "2601.03378", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03378", "abs": "https://arxiv.org/abs/2601.03378", "authors": ["Yu Huo", "Siyu Zhang", "Kun Zeng", "Yuquan Lu", "Cheng Yang", "Yifu Guo", "Xiaoying Tang"], "title": "RepoShapley: Shapley-Enhanced Context Filtering for Repository-Level Code Completion", "comment": "22pages, 9 figures, conference", "summary": "Repository-level code completion benefits from retrieval-augmented generation (RAG). However, controlling cross-file evidence is difficult because chunk utility is often interaction-dependent: some snippets help only when paired with complementary context, while others harm decoding when they conflict. We propose RepoShapley, a coalition-aware context filtering framework supervised by Shapley-style marginal contributions. Our module ChunkShapley constructs offline labels by (i) single-chunk probing with teacher-forced likelihood to estimate signed, weighted effects, (ii) a surrogate game that captures saturation and interference, (iii) exact Shapley computation for small retrieval sets, and (iv) bounded post-verification that selects a decoding-optimal coalition using the frozen generator. We distill verified $KEEP$ or $DROP$ decisions and retrieval triggering into a single model via discrete control tokens. Experiments across benchmarks and backbones show that RepoShapley improves completion quality while reducing harmful context and unnecessary retrieval. Code: https://anonymous.4open.science/r/a7f3c9."}
{"id": "2601.03430", "categories": ["cs.SE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.03430", "abs": "https://arxiv.org/abs/2601.03430", "authors": ["Mohamed Ouf", "Shayan Noei", "Zeph Van Iterson", "Mariam Guizani", "Ying Zou"], "title": "An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS", "comment": null, "summary": "Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and \"sticky\" (63.4%) communities, whereas conventional OSS projects are more \"magnetic\" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality."}
{"id": "2601.03432", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03432", "abs": "https://arxiv.org/abs/2601.03432", "authors": ["Danny Brahman", "Mohammad Mahoor"], "title": "CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models", "comment": "Accepted at the International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025. Will be published at ACL anthology", "summary": "Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies."}
{"id": "2601.03390", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.03390", "abs": "https://arxiv.org/abs/2601.03390", "authors": ["Daniel Qian", "Xiyu Hao", "Jinkun Geng", "Yuncheng Yao", "Aurojit Panda", "Jinyang Li", "Anirudh Sivaraman"], "title": "Revisiting Speculative Leaderless Protocols for Low-Latency BFT Replication", "comment": null, "summary": "As Byzantine Fault Tolerant (BFT) protocols begin to be used in permissioned blockchains for user-facing applications such as payments, it is crucial that they provide low latency. In pursuit of low latency, some recently proposed BFT consensus protocols employ a leaderless optimistic fast path, in which clients broadcast their requests directly to replicas without first serializing requests at a leader, resulting in an end-to-end commit latency of 2 message delays ($2Δ$) during fault-free, synchronous periods. However, such a fast path only works if there is no contention: concurrent contending requests can cause replicas to diverge if they receive conflicting requests in different orders, triggering costly recovery procedures.\n  In this work, we present Aspen, a leaderless BFT protocol that achieves a near-optimal latency of $2Δ+ \\varepsilon$, where $\\varepsilon$ indicates a short waiting delay. Aspen removes the no-contention condition by utilizing a best-effort sequencing layer based on loosely synchronized clocks and network delay estimates. Aspen requires $n = 3f + 2p + 1$ replicas to cope with up to $f$ Byzantine nodes. The $2p$ extra nodes allow Aspen's fast path to proceed even if up to $p$ replicas diverge due to unpredictable network delays. When its optimistic conditions do not hold, Aspen falls back to PBFT-style protocol, guaranteeing safety and liveness under partial synchrony. In experiments with wide-area distributed replicas, Aspen commits requests in less than 75 ms, a 1.2 to 3.3$\\times$ improvement compared to previous protocols, while supporting 19,000 requests per second."}
{"id": "2601.03618", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2601.03618", "abs": "https://arxiv.org/abs/2601.03618", "authors": ["Muhammad Imam Luthfi Balaka", "Raul Castro Fernandez"], "title": "The Pneuma Project: Reifying Information Needs as Relational Schemas to Automate Discovery, Guide Preparation, and Align Data with Intent", "comment": "CIDR 2026 Paper", "summary": "Data discovery and preparation remain persistent bottlenecks in the data management lifecycle, especially when user intent is vague, evolving, or difficult to operationalize. The Pneuma Project introduces Pneuma-Seeker, a system that helps users articulate and fulfill information needs through iterative interaction with a language model-powered platform. The system reifies the user's evolving information need as a relational data model and incrementally converges toward a usable document aligned with that intent. To achieve this, the system combines three architectural ideas: context specialization to reduce LLM burden across subtasks, a conductor-style planner to assemble dynamic execution plans, and a convergence mechanism based on shared state. The system integrates recent advances in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation to support semi-automatic, language-guided workflows. We evaluate the system through LLM-based user simulations and show that it helps surface latent intent, guide discovery, and produce fit-for-purpose documents. It also acts as an emergent documentation layer, capturing institutional knowledge and supporting organizational memory."}
{"id": "2601.03512", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03512", "abs": "https://arxiv.org/abs/2601.03512", "authors": ["Yuhan Wu", "Huan Zhang", "Wei Cheng", "Chen Shen", "Jingyue Yang", "Wei Hu"], "title": "Bootstrapping Code Translation with Weighted Multilanguage Exploration", "comment": null, "summary": "Code translation across multiple programming languages is essential yet challenging due to two vital obstacles: scarcity of parallel data paired with executable test oracles, and optimization imbalance when handling diverse language pairs. We propose BootTrans, a bootstrapping method that resolves both obstacles. Its key idea is to leverage the functional invariance and cross-lingual portability of test suites, adapting abundant pivot-language unit tests to serve as universal verification oracles for multilingual RL training. Our method introduces a dual-pool architecture with seed and exploration pools to progressively expand training data via execution-guided experience collection. Furthermore, we design a language-aware weighting mechanism that dynamically prioritizes harder translation directions based on relative performance across sibling languages, mitigating optimization imbalance. Extensive experiments on the HumanEval-X and TransCoder-Test benchmarks demonstrate substantial improvements over baseline LLMs across all translation directions, with ablations validating the effectiveness of both bootstrapping and weighting components."}
{"id": "2601.03862", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.03862", "abs": "https://arxiv.org/abs/2601.03862", "authors": ["Francesco D'Amato", "Roberto Saltini", "Thanh-Hai Tran", "Yann Vonlanthen", "Luca Zanolini"], "title": "Majorum: Ebb-and-Flow Consensus with Dynamic Quorums", "comment": null, "summary": "Dynamic availability is the ability of a consensus protocol to remain live despite honest participants going offline and later rejoining. A well-known limitation is that dynamically available protocols, on their own, cannot provide strong safety guarantees during network partitions or extended asynchrony. Ebb-and-flow protocols [SP21] address this by combining a dynamically available protocol with a partially synchronous finality protocol that irrevocably finalizes a prefix.\n  We present Majorum, an ebb-and-flow construction whose dynamically available component builds on a quorum-based protocol (TOB-SVD). Under optimistic conditions, Majorum finalizes blocks in as few as three slots while requiring only a single voting phase per slot. In particular, when conditions remain favourable, each slot finalizes the next block extending the previously finalized one."}
{"id": "2601.03513", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03513", "abs": "https://arxiv.org/abs/2601.03513", "authors": ["Yi Wang", "Zhenting Huang", "Zhaohan Ding", "Ruoxue Liao", "Yuan Huang", "Xinzijian Liu", "Jiajun Xie", "Siheng Chen", "Linfeng Zhang"], "title": "Deploy-Master: Automating the Deployment of 50,000+ Agent-Ready Scientific Tools in One Day", "comment": null, "summary": "Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows.\n  We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation.\n  Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science."}
{"id": "2601.03992", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03992", "abs": "https://arxiv.org/abs/2601.03992", "authors": ["Qi Wu", "Chao Fang", "Jiayuan Chen", "Ye Lin", "Yueqi Zhang", "Yichuan Bai", "Yuan Du", "Li Du"], "title": "A Scheduling Framework for Efficient MoE Inference on Edge GPU-NDP Systems", "comment": "To appear in 2026 Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "Mixture-of-Experts (MoE) models facilitate edge deployment by decoupling model capacity from active computation, yet their large memory footprint drives the need for GPU systems with near-data processing (NDP) capabilities that offload experts to dedicated processing units. However, deploying MoE models on such edge-based GPU-NDP systems faces three critical challenges: 1) severe load imbalance across NDP units due to non-uniform expert selection and expert parallelism, 2) insufficient GPU utilization during expert computation within NDP units, and 3) extensive data pre-profiling necessitated by unpredictable expert activation patterns for pre-fetching. To address these challenges, this paper proposes an efficient inference framework featuring three key optimizations. First, the underexplored tensor parallelism in MoE inference is exploited to partition and compute large expert parameters across multiple NDP units simultaneously towards edge low-batch scenarios. Second, a load-balancing-aware scheduling algorithm distributes expert computations across NDP units and GPU to maximize resource utilization. Third, a dataset-free pre-fetching strategy proactively loads frequently accessed experts to minimize activation delays. Experimental results show that our framework enables GPU-NDP systems to achieve 2.41x on average and up to 2.56x speedup in end-to-end latency compared to state-of-the-art approaches, significantly enhancing MoE inference efficiency in resource-constrained environments."}
{"id": "2601.03556", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03556", "abs": "https://arxiv.org/abs/2601.03556", "authors": ["Sabrina Haque", "Sarvesh Ingale", "Christoph Csallner"], "title": "Do Autonomous Agents Contribute Test Code? A Study of Tests in Agentic Pull Requests", "comment": null, "summary": "Testing is a critical practice for ensuring software correctness and long-term maintainability. As agentic coding tools increasingly submit pull requests (PRs), it becomes essential to understand how testing appears in these agent-driven workflows. Using the AIDev dataset, we present an empirical study of test inclusion in agentic pull requests. We examine how often tests are included, when they are introduced during the PR lifecycle and how test-containing PRs differ from non-test PRs in terms of size, turnaround time, and merge outcomes. Across agents, test-containing PRs are more common over time and tend to be larger and take longer to complete, while merge rates remain largely similar. We also observe variation across agents in both test adoption and the balance between test and production code within test PRs. Our findings provide a descriptive view of testing behavior in agentic pull requests and offer empirical grounding for future studies of autonomous software development."}
{"id": "2601.04071", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04071", "abs": "https://arxiv.org/abs/2601.04071", "authors": ["Tiancheng Hu", "Chenxi Wang", "Ting Cao", "Jin Qin", "Lei Chen", "Xinyu Xiao", "Junhao Hu", "Hongliang Tian", "Shoumeng Yan", "Huimin Cui", "Quan Chen", "Tao Xie"], "title": "Hummingbird: SLO-Oriented GPU Preemption at Microsecond-scale", "comment": null, "summary": "Existing GPU-sharing techniques, including spatial and temporal sharing, aim to improve utilization but face challenges in simultaneously ensuring SLO adherence and maximizing efficiency due to the lack of fine-grained task scheduling on closed-source GPUs. This paper presents Hummingbird, an SLO-oriented GPU scheduling system that overcomes these challenges by enabling microsecond-scale preemption on closed-source GPUs while effectively harvesting idle GPU time slices. Comprehensive evaluations across diverse GPU architectures reveal that Hummingbird improves the SLO attainment of high-priority tasks by 9.7x and 3.5x compared to the state-of-the-art spatial and temporal-sharing approaches. When compared to executing exclusively, the SLO attainment of the high-priority task, collocating with low-priority tasks on Hummingbird, only drops by less than 1%. Meanwhile, the throughput of the low-priority task outperforms the state-of-the-art temporal-sharing approaches by 2.4x. Hummingbird demonstrates significant effectiveness in ensuring the SLO while enhancing GPU utilization."}
{"id": "2601.03574", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03574", "abs": "https://arxiv.org/abs/2601.03574", "authors": ["Mamdouh Alenezi"], "title": "Auditable DevOps Automation via VSM and GQM", "comment": null, "summary": "DevOps automation can accelerate software delivery, yet many organizations still struggle to justify and prioritize automation work in terms of strategic project-management outcomes such as waste reduction, delivery predictability, cross-team coordination, and customer-facing quality. This paper presents \\textit{VSM--GQM--DevOps}, a unified, traceable framework that integrates (i) Value Stream Mapping (VSM) to visualize the end-to-end delivery system and quantify delays, rework, and handoffs, (ii) the Goal--Question--Metric (GQM) paradigm to translate stakeholder objectives into a minimal, decision-relevant measurement model (combining DORA with project and team outcomes), and (iii) maturity-aligned DevOps automation to remediate empirically observed bottlenecks through small, reversible interventions. The framework operationalizes traceability from observed waste to goal-aligned questions, metrics, and automation candidates, and provides a defensible prioritization approach that balances expected impact, confidence, and cost. We also define a multi-site, longitudinal mixed-method validation protocol that combines telemetry-based quasi-experimental analysis (interrupted time series and, where feasible, controlled rollouts) with qualitative triangulation from interviews and retrospectives. The expected contribution is a validated pathway and a set of practical instruments that enables organizations to select automation investments that demonstrably improve both delivery performance and project-management outcomes."}
{"id": "2601.04123", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2601.04123", "abs": "https://arxiv.org/abs/2601.04123", "authors": ["Francisco Ponce", "Simone Gazza", "Andrea D'Iapico", "Roberto Amadini", "Antonio Brogi", "Stefano Forti", "Saverio Giallorenzo", "Pierluigi Plebani", "Davide Usai", "Monica Vitali", "Gianluigi Zavattaro", "Jacopo Soldani"], "title": "Failure-Resilient and Carbon-Efficient Deployment of Microservices over the Cloud-Edge Continuum", "comment": "Submitted to Cluster Computing", "summary": "Deploying microservice-based applications (MSAs) on heterogeneous and dynamic Cloud-Edge infrastructures requires balancing conflicting objectives, such as failure resilience, performance, and environmental sustainability. In this article, we introduce the FREEDA toolchain, designed to automate the failure-resilient and carbon-efficient deployment of MSAs over the Cloud-Edge Continuum.\n  The FREEDA toolchain continuously adapts deployment configurations to changing operational conditions, resource availability, and sustainability constraints, aiming to maintain the MSA quality and service continuity while reducing carbon emissions. We also introduce an experimental suite using diverse simulated and emulated scenarios to validate the effectiveness of the toolchain against real-world challenges, including resource exhaustion, node failures, and carbon intensity fluctuations. The results demonstrate FREEDA's capability to autonomously reconfigure deployments by migrating services, adjusting flavour selections, or rebalancing workloads, successfully achieving an optimal balance among resilience, efficiency, and environmental impact."}
{"id": "2601.03621", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03621", "abs": "https://arxiv.org/abs/2601.03621", "authors": ["Verya Monjezi", "Ashish Kumar", "Ashutosh Trivedi", "Gang Tan", "Saeid Tizpaz-Niari"], "title": "On the Robustness of Fairness Practices: A Causal Framework for Systematic Evaluation", "comment": null, "summary": "Machine learning (ML) algorithms are increasingly deployed to make critical decisions in socioeconomic applications such as finance, criminal justice, and autonomous driving. However, due to their data-driven and pattern-seeking nature, ML algorithms may develop decision logic that disproportionately distributes opportunities, benefits, resources, or information among different population groups, potentially harming marginalized communities. In response to such fairness concerns, the software engineering and ML communities have made significant efforts to establish the best practices for creating fair ML software. These include fairness interventions for training ML models, such as including sensitive features, selecting non-sensitive attributes, and applying bias mitigators. But how reliably can software professionals tasked with developing data-driven systems depend on these recommendations? And how well do these practices generalize in the presence of faulty labels, missing data, or distribution shifts? These questions form the core theme of this paper."}
{"id": "2601.03640", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.03640", "abs": "https://arxiv.org/abs/2601.03640", "authors": ["Mohd Ariful Haque", "Kishor Datta Gupta", "Mohammad Ashiqur Rahman", "Roy George"], "title": "Verbatim Data Transcription Failures in LLM Code Generation: A State-Tracking Stress Test", "comment": null, "summary": "Many real-world software tasks require exact transcription of provided data into code, such as cryptographic constants, protocol test vectors, allowlists, and calibration tables. These tasks are operationally sensitive because small omissions or alterations can remain silent while producing syntactically valid programs. This paper introduces a deliberately minimal transcription-to-code benchmark to isolate this reliability concern in LLM-based code generation. Given a list of high-precision decimal constants, a model must generate Python code that embeds the constants verbatim and performs a simple aggregate computation. We describe the prompting variants, evaluation protocol based on exact-string inclusion, and analysis framework used to characterize state-tracking and long-horizon generation failures. The benchmark is intended as a compact stress test that complements existing code-generation evaluations by focusing on data integrity rather than algorithmic reasoning."}
{"id": "2601.03731", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.03731", "abs": "https://arxiv.org/abs/2601.03731", "authors": ["Jia Li", "Yuxin Su", "Michael R. Lyu"], "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level", "comment": null, "summary": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering."}
{"id": "2601.03780", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03780", "abs": "https://arxiv.org/abs/2601.03780", "authors": ["Md Ahasanuzzaman", "Bram Adams", "Emad Fallahzadeh", "Gustavo A. Oliva", "Ahmed E. Hassan"], "title": "Assessing and Improving the Representativeness of Code Generation Benchmarks Using Knowledge Units (KUs) of Programming Languages -- An Empirical Study", "comment": null, "summary": "Large Language Models (LLMs) such as GPT-4, Claude and LLaMA have shown impressive performance in code generation, typically evaluated using benchmarks (e.g., HumanEval). However, effective code generation requires models to understand and apply a wide range of language concepts. If the concepts exercised in benchmarks are not representative of those used in real-world projects, evaluations may yield incomplete. Despite this concern, the representativeness of code concepts in benchmarks has not been systematically examined.\n  To address this gap, we present the first empirical study that analyzes the representativeness of code generation benchmarks through the lens of Knowledge Units (KUs) - cohesive sets of programming language capabilities provided by language constructs and APIs. We analyze KU coverage in two widely used Python benchmarks, HumanEval and MBPP, and compare them with 30 real-world Python projects. Our results show that each benchmark covers only half of the identified 20 KUs, whereas projects exercise all KUs with relatively balanced distributions. In contrast, benchmark tasks exhibit highly skewed KU distributions.\n  To mitigate this misalignment, we propose a prompt-based LLM framework that synthesizes KU-based tasks to rebalance benchmark KU distributions and better align them with real-world usage. Using this framework, we generate 440 new tasks and augment existing benchmarks. The augmented benchmarks substantially improve KU coverage and achieve over a 60% improvement in distributional alignment. Evaluations of state-of-the-art LLMs on these augmented benchmarks reveal consistent and statistically significant performance drops (12.54-44.82%), indicating that existing benchmarks overestimate LLM performance due to their limited KU coverage. Our findings provide actionable guidance for building more realistic evaluations of LLM code-generation capabilities."}
{"id": "2601.03857", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03857", "abs": "https://arxiv.org/abs/2601.03857", "authors": ["Alessandra Parziale", "Gianmario Voria", "Valeria Pontillo", "Amleto Di Salle", "Patrizio Pelliccione", "Gemma Catolino", "Fabio Palomba"], "title": "Once Upon a Team: Investigating Bias in LLM-Driven Software Team Composition and Task Allocation", "comment": null, "summary": "LLMs are increasingly used to boost productivity and support software engineering tasks. However, when applied to socially sensitive decisions such as team composition and task allocation, they raise concerns of fairness. Prior studies have revealed that LLMs may reproduce stereotypes; however, these analyses remain exploratory and examine sensitive attributes in isolation. This study investigates whether LLMs exhibit bias in team composition and task assignment by analyzing the combined effects of candidates' country and pronouns. Using three LLMs and 3,000 simulated decisions, we find systematic disparities: demographic attributes significantly shaped both selection likelihood and task allocation, even when accounting for expertise-related factors. Task distributions further reflected stereotypes, with technical and leadership roles unevenly assigned across groups. Our findings indicate that LLMs exacerbate demographic inequities in software engineering contexts, underscoring the need for fairness-aware assessment."}
{"id": "2601.03878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.03878", "abs": "https://arxiv.org/abs/2601.03878", "authors": ["Giovanni Rosa", "David Moreno-Lumbreras", "Gregorio Robles", "Jesús M. González-Barahona"], "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design", "comment": "This paper is a Stage 1 Registered Report. The study protocol and analysis plan were peer reviewed and accepted at SANER 2026 with a Continuity Acceptance (CA) score for Stage 2", "summary": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation."}
{"id": "2601.03988", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.03988", "abs": "https://arxiv.org/abs/2601.03988", "authors": ["Nicolas Lacroix", "Mireille Blay-Fornarino", "Sébastien Mosser", "Frederic Precioso"], "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures", "comment": "SANER 2026 Registered Report", "summary": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.\n  Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.\n  Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies."}
{"id": "2601.04010", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2601.04010", "abs": "https://arxiv.org/abs/2601.04010", "authors": ["Yannick Landeck", "Dian Balta", "Martin Wimmer", "Christian Knierim"], "title": "An Ontology-Based Approach to Security Risk Identification of Container Deployments in OT Contexts", "comment": "Accepted for publication on the Software Engineering in Practice (SEIP) track of the Internation Conference on Software Engineering (ICSE'26)", "summary": "In operational technology (OT) contexts, containerised applications often require elevated privileges to access low-level network interfaces or perform administrative tasks such as application monitoring. These privileges reduce the default isolation provided by containers and introduce significant security risks. Security risk identification for OT container deployments is challenged by hybrid IT/OT architectures, fragmented stakeholder knowledge, and continuous system changes. Existing approaches lack reproducibility, interpretability across contexts, and technical integration with deployment artefacts. We propose a model-based approach, implemented as the Container Security Risk Ontology (CSRO), which integrates five key domains: adversarial behaviour, contextual assumptions, attack scenarios, risk assessment rules, and container security artefacts. Our evaluation of CSRO in a case study demonstrates that the end-to-end formalisation of risk calculation, from artefact to risk level, enables automated and reproducible risk identification. While CSRO currently focuses on technical, container-level treatment measures, its modular and flexible design provides a solid foundation for extending the approach to host-level and organisational risk factors."}
{"id": "2601.04124", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2601.04124", "abs": "https://arxiv.org/abs/2601.04124", "authors": ["Lloyd Montgomery", "Clara Lüders", "Christian Rahe", "Walid Maalej"], "title": "Smells Depend on the Context: An Interview Study of Issue Tracking Problems and Smells in Practice", "comment": "30 pages, 1 figure, accepted at the ACM TOSEM journal", "summary": "Issue Tracking Systems (ITSs) enable software developers and managers to collect and resolve issues collaboratively. While researchers have extensively analysed ITS data to automate or assist specific activities such as issue assignments, duplicate detection, or priority prediction, developer studies on ITSs remain rare. Particularly, little is known about the challenges Software Engineering (SE) teams encounter in ITSs and when certain practices and workarounds (such as leaving issue fields like \"priority\" empty) are considered problematic. To fill this gap, we conducted an in-depth interview study with 26 experienced SE practitioners from different organisations and industries. We asked them about general problems encountered, as well as the relevance of 31 ITS smells (aka potentially problematic practices) discussed in the literature. By applying Thematic Analysis to the interview notes, we identified 14 common problems including issue findability, zombie issues, workflow bloat, and lack of workflow enforcement. Participants also stated that many of the ITS smells do not occur or are not problematic. Our results suggest that ITS problems and smells are highly dependent on context factors such as ITS configuration, workflow stage, and team size. We also discuss potential tooling solutions to configure, monitor, and visualise ITS smells to cope with these challenges."}
