<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 7]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [From Reflection to Repair: A Scoping Review of Dataset Documentation Tools](https://arxiv.org/abs/2602.15968)
*Pedro Reynolds-Cuéllar,Marisol Wong-Villacres,Adriana Alvarado Garcia,Heila Precel*

Main category: cs.SE

TL;DR: 对59篇数据集文档工具文献的系统综述发现，文档工具设计存在四大障碍：价值定义模糊、设计脱离上下文、忽视劳动需求、集成被视为未来工作，建议转向机构化解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管数据集文档对负责任AI开发至关重要，但现有文档工具的设计动机、采用障碍以及如何与现有系统、法规和文化规范连接等方面缺乏系统研究。

Method: 采用混合方法分析，对59篇数据集文档出版物进行系统综述，分析文档工具的设计动机、文档实践概念化方式，以及工具与现有系统的连接情况。

Result: 发现数据集文档概念化存在四大持续模式：文档价值操作化不明确、设计脱离上下文、未解决的劳动需求、倾向于将集成视为未来工作，这些模式阻碍了工具的采用和标准化。

Conclusion: 建议负责任AI工具设计应从个体解决方案转向机构化解决方案，并概述了HCI社区可以采取的行动，以支持可持续的文档实践。

Abstract: Dataset documentation is widely recognized as essential for the responsible development of automated systems. Despite growing efforts to support documentation through different kinds of artifacts, little is known about the motivations shaping documentation tool design or the factors hindering their adoption. We present a systematic review supported by mixed-methods analysis of 59 dataset documentation publications to examine the motivations behind building documentation tools, how authors conceptualize documentation practices, and how these tools connect to existing systems, regulations, and cultural norms. Our analysis shows four persistent patterns in dataset documentation conceptualization that potentially impede adoption and standardization: unclear operationalizations of documentation's value, decontextualized designs, unaddressed labor demands, and a tendency to treat integration as future work. Building on these findings, we propose a shift in Responsible AI tool design toward institutional rather than individual solutions, and outline actions the HCI community can take to enable sustainable documentation practices.

</details>


### [2] [ReLoop: Structured Modeling and Behavioral Verification for Reliable LLM-Based Optimization](https://arxiv.org/abs/2602.15983)
*Junbo Jacob Lian,Yujun Sun,Huiling Chen,Chaoyu Zhang,Chung-Piaw Teo*

Main category: cs.SE

TL;DR: ReLoop框架通过结构化生成和行为验证解决LLM生成优化代码时的静默失败问题，将正确率从22.6%提升到31.1%，执行率从72.1%提升到100%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型能将自然语言转换为优化代码，但存在静默失败风险：代码可以执行并返回求解器可行解，但可能编码了语义错误的公式，在组合问题上可行性-正确性差距高达90个百分点。

Method: ReLoop从两个互补方向解决静默失败：1) 结构化生成将代码生成分解为四阶段推理链（理解、形式化、合成、验证），模拟专家建模实践；2) 行为验证通过基于求解器的参数扰动测试检测生成后错误，无需真实标签。

Result: 在最强模型上，ReLoop将正确率从22.6%提升到31.1%，执行率从72.1%提升到100%。在五个模型（基础、SFT、RL）和三个基准测试上均获得一致提升。同时发布了RetailOpt-190数据集。

Conclusion: 结构化生成在复杂组合问题上占主导，行为验证在局部公式缺陷问题上贡献最大。两种机制互补，结合IIS增强的诊断执行恢复，显著提升了LLM生成优化代码的可靠性和正确性。

Abstract: Large language models (LLMs) can translate natural language into optimization code, but silent failures pose a critical risk: code that executes and returns solver-feasible solutions may encode semantically incorrect formulations, creating a feasibility-correctness gap of up to 90 percentage points on compositional problems. We introduce ReLoop, addressing silent failures from two complementary directions. Structured generation decomposes code production into a four-stage reasoning chain (understand, formalize, synthesize, verify) that mirrors expert modeling practice, with explicit variable-type reasoning and self-verification to prevent formulation errors at their source. Behavioral verification detects errors that survive generation by testing whether the formulation responds correctly to solver-based parameter perturbation, without requiring ground truth -- an external semantic signal that bypasses the self-consistency problem inherent in LLM-based code review. The two mechanisms are complementary: structured generation dominates on complex compositional problems, while behavioral verification becomes the largest single contributor on problems with localized formulation defects. Together with execution recovery via IIS-enhanced diagnostics, ReLoop raises correctness from 22.6% to 31.1% and execution from 72.1% to 100.0% on the strongest model, with consistent gains across five models spanning three paradigms (foundation, SFT, RL) and three benchmarks. We additionally release RetailOpt-190, 190 compositional retail optimization scenarios targeting the multi-constraint interactions where LLMs most frequently fail.

</details>


### [3] [The Limits of Long-Context Reasoning in Automated Bug Fixing](https://arxiv.org/abs/2602.16069)
*Ravi Raju,Mengmeng Ji,Shubhangi Upasani,Bo Li,Urmish Thakker*

Main category: cs.SE

TL;DR: 当前LLMs在长上下文代码调试和补丁生成方面存在显著能力差距，尽管代理工作流在短上下文任务中表现良好，但真正的长上下文推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度快速增加，人们普遍假设大型语言模型可以直接对整个代码库进行推理。同时，LLMs在软件工程基准测试中表现出色，特别是在代理工作流中。本研究旨在系统评估当前LLMs是否能够可靠地执行长上下文代码调试和补丁生成。

Method: 使用SWE-bench Verified作为受控实验设置：1）在代理框架（mini-SWE-agent）中评估最先进模型；2）构建数据管道，通过将相关文件放入上下文来人为增加输入长度，研究真正长上下文（64k-128k tokens）下的单次补丁生成。

Result: 代理工作流显著提升性能（GPT-5-nano在100个样本上达到31%解决率），但成功轨迹通常保持在20k tokens以下，且更长的累积上下文与较低成功率相关。在真正长上下文设置中，性能急剧下降：Qwen3-Coder-30B-A3B在64k上下文中仅达到7%解决率，GPT-5-nano则无法解决任何任务。定性分析揭示了系统性的失败模式。

Conclusion: 当前LLMs的名义上下文长度与实际可用上下文容量之间存在显著差距，现有的代理编码基准测试并未真正评估长上下文推理能力。成功主要来自将任务分解为短上下文步骤，而非有效的长上下文推理。

Abstract: Rapidly increasing context lengths have led to the assumption that large language models (LLMs) can directly reason over entire codebases. Concurrently, recent advances in LLMs have enabled strong performance on software engineering benchmarks, particularly when paired with agentic workflows. In this work, we systematically evaluate whether current LLMs can reliably perform long-context code debugging and patch generation. Using SWE-bench Verified as a controlled experimental setting, we first evaluate state-of-the-art models within an agentic harness (mini-SWE-agent), where performance improves substantially: GPT-5-nano achieves up to a 31\% resolve rate on 100 samples, and open-source models such as Deepseek-R1-0528 obtain competitive results. However, token-level analysis shows that successful agentic trajectories typically remain under 20k tokens, and that longer accumulated contexts correlate with lower success rates, indicating that agentic success primarily arises from task decomposition into short-context steps rather than effective long-context reasoning. To directly test long-context capability, we construct a data pipeline where we artificially inflate the context length of the input by placing the relevant files into the context (ensuring perfect retrieval recall); we then study single-shot patch generation under genuinely long contexts (64k-128k tokens). Despite this setup, performance degrades sharply: Qwen3-Coder-30B-A3B achieves only a 7\% resolve rate at 64k context, while GPT-5-nano solves none of the tasks. Qualitative analysis reveals systematic failure modes, including hallucinated diffs, incorrect file targets, and malformed patch headers. Overall, our findings highlight a significant gap between nominal context length and usable context capacity in current LLMs, and suggest that existing agentic coding benchmarks do not meaningfully evaluate long-context reasoning.

</details>


### [4] [Can Causality Cure Confusion Caused By Correlation (in Software Analytics)?](https://arxiv.org/abs/2602.16091)
*Amirali Rayegan,Tim Menzies*

Main category: cs.SE

TL;DR: 研究探讨在符号模型（决策树）中引入因果感知分裂准则是否能提升稳定性，同时评估对预测性能的影响，并比较人类专家与自动化模型的稳定性差异。


<details>
  <summary>Details</summary>
Motivation: 当前软件工程中广泛使用的符号模型（如决策树）主要基于相关性分裂准则（方差减少、信息增益等），这些方法混淆了关联与因果关系。同时，因果发现算法因NP难问题而依赖启发式近似，导致结果不稳定。这些问题影响了SE任务中的可信度、可复现性和解释可靠性。

Method: 使用MOOT仓库中的120多个多目标优化任务，通过预注册的自举集成协议评估稳定性，采用赢分分配方法测量方差。比较人类因果评估、基于相关性的决策树（EZR）以及因果感知树（利用条件熵分裂准则和混杂因素过滤）。使用统计方法（方差、基尼不纯度、KS检验、Cliff's delta）分析稳定性和性能差异。

Result: 论文未提供具体结果数据，但研究框架已建立：通过系统比较人类专家、传统相关性决策树和因果感知树在稳定性、预测性能和优化性能方面的表现，旨在验证因果感知方法是否能提升稳定性而不牺牲性能。

Conclusion: 研究旨在证明在符号模型中引入因果感知分裂准则可以改善模型稳定性，同时保持或提升预测和优化性能，为软件工程中的可解释分析提供更可靠的基础。

Abstract: Background: Symbolic models, particularly decision trees, are widely used in software engineering for explainable analytics in defect prediction, configuration tuning, and software quality assessment. Most of these models rely on correlational split criteria, such as variance reduction or information gain, which identify statistical associations but cannot imply causation between X and Y. Recent empirical studies in software engineering show that both correlational models and causal discovery algorithms suffer from pronounced instability. This instability arises from two complementary issues: 1-Correlation-based methods conflate association with causation. 2-Causal discovery algorithms rely on heuristic approximations to cope with the NP-hard nature of structure learning, causing their inferred graphs to vary widely under minor input perturbations. Together, these issues undermine trust, reproducibility, and the reliability of explanations in real-world SE tasks. Objective: This study investigates whether incorporating causality-aware split criteria into symbolic models can improve their stability and robustness, and whether such gains come at the cost of predictive or optimization performance. We additionally examine how the stability of human expert judgments compares to that of automated models. Method: Using 120+ multi-objective optimization tasks from the MOOT repository of multi-objective optimization tasks, we evaluate stability through a preregistered bootstrap-ensemble protocol that measures variance with win-score assignments. We compare the stability of human causal assessments with correlation-based decision trees (EZR). We would also compare the causality-aware trees, which leverage conditional-entropy split criteria and confounder filtering. Stability and performance differences are analyzed using statistical methods (variance, Gini Impurity, KS test, Cliff's delta)

</details>


### [5] [Algorithm-Based Pipeline for Reliable and Intent-Preserving Code Translation with LLMs](https://arxiv.org/abs/2602.16106)
*Shahriar Rumi Dipto,Saikat Mondal,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 算法驱动的代码翻译管道通过引入语言中立的中间规范，将翻译准确率从67.7%提升至78.5%，显著减少各类编译和运行时错误。


<details>
  <summary>Details</summary>
Motivation: 直接使用大语言模型进行代码翻译往往无法保持程序意图，导致控制流、类型处理和I/O行为错误。需要更可靠的方法来确保翻译的准确性和意图保持。

Method: 提出基于算法的翻译管道，引入语言中立的中间规范来捕获程序细节后再生成代码。在Avatar和CodeNet数据集上，使用五种主流LLM进行Python和Java之间的翻译对比实验，评估编译结果、运行时行为和测试通过率。

Result: 算法方法将平均准确率从67.7%提升至78.5%（增加10.8%），完全消除词法和标记错误，减少72.7%的不完整构造，降低61.1%的结构和声明问题，减少78.4%的运行时依赖和入口点失败。

Conclusion: 基于算法的管道能够实现更可靠、意图保持的代码翻译，为健壮的多语言编程助手奠定了基础。

Abstract: Code translation, the automatic conversion of programs between languages, is a growing use case for Large Language Models (LLMs). However, direct one-shot translation often fails to preserve program intent, leading to errors in control flow, type handling, and I/O behavior. We propose an algorithm-based pipeline that introduces a language-neutral intermediate specification to capture these details before code generation. This study empirically evaluates the extent to which structured planning can improve translation accuracy and reliability relative to direct translation. We conduct an automated paired experiment - direct and algorithm-based to translate between Python and Java using five widely used LLMs on the Avatar and CodeNet datasets. For each combination (model, dataset, approach, and direction), we compile and execute the translated program and run the tests provided. We record compilation results, runtime behavior, timeouts (e.g., infinite loop), and test outcomes. We compute accuracy from these tests, counting a translation as correct only if it compiles, runs without exceptions or timeouts, and passes all tests. We then map every failed compile-time and runtime case to a unified, language-aware taxonomy and compare subtype frequencies between the direct and algorithm-based approaches. Overall, the Algorithm-based approach increases micro-average accuracy from 67.7% to 78.5% (10.8% increase). It eliminates lexical and token errors by 100%, reduces incomplete constructs by 72.7%, and structural and declaration issues by 61.1%. It also substantially lowers runtime dependency and entry-point failures by 78.4%. These results demonstrate that algorithm-based pipelines enable more reliable, intent-preserving code translation, providing a foundation for robust multilingual programming assistants.

</details>


### [6] [Software-heavy Asset Administration Shells: Classification and Use Cases](https://arxiv.org/abs/2602.16499)
*Carsten Ellwein,David Dietrich,Jessica Roth,Rozana Cvitkovic,Andreas Wortmann*

Main category: cs.SE

TL;DR: 该论文系统分析了将软件服务直接集成到资产管理壳(AAS)中的软件架构，基于软件质量标准和典型制造用例进行分类，为学术界和实践者提供指导。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生和人工智能在制造业中的应用日益重要，需要将软件服务直接集成到AAS中。现有文献只有零散解决方案，缺乏对相关软件架构的系统分析。

Method: 基于软件质量标准和典型制造用例，对将软件服务直接集成到AAS的软件架构进行系统分析和分类。

Result: 提出了针对软件密集型AAS的架构分类框架，为不同制造场景下的软件服务集成提供系统化的架构选择指导。

Conclusion: 该研究填补了软件密集型AAS架构系统分析的空白，为学术界和实践者提供了实用的架构选择指导框架。

Abstract: The Asset Administration Shell (AAS) is an emerging technology for the implementation of digital twins in the field of manufacturing. Software is becoming increasingly important, not only in general but specifically in relation to manufacturing, especially with regard to digital manufacturing and a shift towards the usage of artificial intelligence. This increases the need not only to model software, but also to integrate services directly into the AAS. The existing literature contains individual solutions to implement such software-heavy AAS. However, there is no systematic analysis of software architectures that integrate software services directly into the AAS. This paper aims to fill this research gap and differentiate architectures based on software quality criteria as well as typical manufacturing use cases. This work may be considered as an interpretation guideline for software-heavy AAS, both in academia and for practitioners.

</details>


### [7] [SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation](https://arxiv.org/abs/2602.16671)
*Jaid Monwar Chowdhury,Chi-An Fu,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: SPARC是一个神经符号、基于场景的框架，通过结合控制流图分析、操作映射、路径定向测试合成和迭代自校正验证，显著提升C语言单元测试生成的质量和覆盖率。


<details>
  <summary>Details</summary>
Motivation: C语言的自动化单元测试生成面临巨大挑战，因为高级程序意图与指针运算和手动内存管理的严格语法约束之间存在语义鸿沟。大型语言模型虽然具有强大的生成能力，但直接意图到代码的合成经常出现"跳跃到代码"的失败模式，导致模型过早生成代码而缺乏对程序结构、约束和语义的基础理解。

Method: SPARC采用四阶段神经符号框架：1) 控制流图分析；2) 操作映射，将LLM推理基于已验证的实用辅助函数；3) 路径定向测试合成；4) 使用编译器和运行时反馈的迭代自校正验证循环。

Result: 在59个真实世界和算法主题上评估，SPARC相比基线提示生成在线覆盖率上提升31.36%，分支覆盖率提升26.01%，变异分数提升20.78%，在复杂主题上匹配或超过符号执行工具KLEE。通过迭代修复保留了94.3%的测试，生成的代码在开发者评定的可读性和可维护性方面显著更高。

Conclusion: 通过将LLM推理与程序结构对齐，SPARC为工业级遗留C代码库测试提供了可扩展的路径，显著提升了测试生成的质量和实用性。

Abstract: Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [8] [Automated Extraction of Mechanical Constitutive Models from Scientific Literature using Large Language Models: Applications in Cultural Heritage Conservation](https://arxiv.org/abs/2602.16551)
*Rui Hu,Yue Wu,Tianhao Su,Yin Wang,Shunbo Hu,Jizhong Huang*

Main category: cs.DB

TL;DR: 提出基于大语言模型的两阶段智能代理框架，从PDF文献中自动提取力学本构方程、参数和元数据，解决文化遗产保护中数据碎片化问题


<details>
  <summary>Details</summary>
Motivation: 文化遗产保护正向数据驱动的预测性维护和"数字孪生"构建发展，但所需的力学本构模型分散在数十年的非结构化科学文献中，形成"数据孤岛"，阻碍了保护工程的发展

Method: 采用两阶段智能代理框架：1) 资源高效的"守门员"代理进行相关性筛选；2) 高能力"分析师"代理进行细粒度提取，包含新颖的上下文感知符号接地机制解决数学歧义

Result: 应用于2000多篇研究论文，成功筛选出113篇核心文档，构建了包含185个本构模型实例和450多个校准参数的结构化数据库，提取精度达80.4%，减少人工数据整理时间约90%

Conclusion: 该工作将分散的文献转化为可查询的数字资产，为建筑遗产的"数字材料孪生"奠定数据基础，通过基于网络的知识检索平台展示了系统的实用性

Abstract: The preservation of cultural heritage is increasingly transitioning towards data-driven predictive maintenance and "Digital Twin" construction. However, the mechanical constitutive models required for high-fidelity simulations remain fragmented across decades of unstructured scientific literature, creating a "Data Silo" that hinders conservation engineering. To address this, we present an automated, two-stage agentic framework leveraging Large Language Models (LLMs) to extract mechanical constitutive equations, calibrated parameters, and metadata from PDF documents. The workflow employs a resource-efficient "Gatekeeper" agent for relevance filtering and a high-capability "Analyst" agent for fine-grained extraction, featuring a novel Context-Aware Symbolic Grounding mechanism to resolve mathematical ambiguities. Applied to a corpus of over 2,000 research papers, the system successfully isolated 113 core documents and constructed a structured database containing 185 constitutive model instances and over 450 calibrated parameters. The extraction precision reached 80.4\%, establishing a highly efficient "Human-in-the-loop" workflow that reduces manual data curation time by approximately 90\%. We demonstrate the system's utility through a web-based Knowledge Retrieval Platform, which enables rapid parameter discovery for computational modeling. This work transforms scattered literature into a queryable digital asset, laying the data foundation for the "Digital Material Twin" of built heritage.

</details>


### [9] [DataJoint 2.0: A Computational Substrate for Agentic Scientific Workflows](https://arxiv.org/abs/2602.16585)
*Dimitri Yatsenko,Thinh T. Nguyen*

Main category: cs.DB

TL;DR: DataJoint 2.0提出关系型工作流模型，通过四个技术创新实现科学数据管道的SciOps（科学运维），统一数据、结构和计算转换。


<details>
  <summary>Details</summary>
Motivation: 科学数据管道需要类似DevOps的SciOps方法，但现有方法将数据溯源分散在不同系统中，缺乏事务保证，导致人机协作容易失败。

Method: 采用关系型工作流模型：用表表示工作流步骤，行表示数据产物，外键规定执行顺序。通过四个技术创新扩展：对象增强模式、语义匹配、可扩展类型系统和分布式作业协调。

Result: 创建了一个统一数据结构、数据和计算转换的平台，使代理能够参与科学工作流而不会导致数据损坏，实现了SciOps的基础设施。

Conclusion: DataJoint 2.0通过关系型工作流模型和四个技术创新，为科学数据管道提供了完整的SciOps解决方案，解决了数据溯源分散和缺乏事务保证的问题。

Abstract: Operational rigor determines whether human-agent collaboration succeeds or fails. Scientific data pipelines need the equivalent of DevOps -- SciOps -- yet common approaches fragment provenance across disconnected systems without transactional guarantees. DataJoint 2.0 addresses this gap through the relational workflow model: tables represent workflow steps, rows represent artifacts, foreign keys prescribe execution order. The schema specifies not only what data exists but how it is derived -- a single formal system where data structure, computational dependencies, and integrity constraints are all queryable, enforceable, and machine-readable. Four technical innovations extend this foundation: object-augmented schemas integrating relational metadata with scalable object storage, semantic matching using attribute lineage to prevent erroneous joins, an extensible type system for domain-specific formats, and distributed job coordination designed for composability with external orchestration. By unifying data structure, data, and computational transformations, DataJoint creates a substrate for SciOps where agents can participate in scientific workflows without risking data corruption.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [Distributed Order Recording Techniques for Efficient Record-and-Replay of Multi-threaded Programs](https://arxiv.org/abs/2602.15995)
*Xiang Fu,Shiman Meng,Weiping Zhang,Luanzheng Guo,Kento Sato,Dong H. Ahn,Ignacio Laguna,Gregory L. Lee,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出两种新技术（分布式时钟DC和分布式纪元DE）来高效记录和重放OpenMP程序，解决了OpenMP程序可扩展重放的难题，性能比传统方法提升2-5倍。


<details>
  <summary>Details</summary>
Motivation: OpenMP虽然是最流行的共享内存编程框架，但其非确定性执行特性使得调试和测试变得困难。记录和确定性重放程序执行是解决这一挑战的关键，但目前可扩展地重放OpenMP程序仍是一个未解决的问题。

Method: 提出两种新颖技术：分布式时钟（DC）和分布式纪元（DE）记录方案，用于消除OpenMP记录和重放中过多的线程同步。通过ReOMP工具实现DC和DE记录，并与MPI级重放工具ReMPI集成，支持MPI+OpenMP应用程序的重放。

Result: 在代表性HPC应用上的评估显示，该方法比传统在每个共享内存访问时同步的方法效率高2-5倍。与ReMPI集成后，仅产生少量与MPI规模无关的运行时开销，能够重放复杂的MPI+OpenMP应用。

Conclusion: 提出的DC和DE记录方案有效解决了OpenMP程序可扩展重放的难题，显著提升了记录和重放的效率，并能与现有MPI重放工具无缝集成，为调试和测试OpenMP程序提供了实用解决方案。

Abstract: After all these years and all these other shared memory programming frameworks, OpenMP is still the most popular one. However, its greater levels of non-deterministic execution makes debugging and testing more challenging. The ability to record and deterministically replay the program execution is key to address this challenge. However, scalably replaying OpenMP programs is still an unresolved problem. In this paper, we propose two novel techniques that use Distributed Clock (DC) and Distributed Epoch (DE) recording schemes to eliminate excessive thread synchronization for OpenMP record and replay. Our evaluation on representative HPC applications with ReOMP, which we used to realize DC and DE recording, shows that our approach is 2-5x more efficient than traditional approaches that synchronize on every shared-memory access. Furthermore, we demonstrate that our approach can be easily combined with MPI-level replay tools to replay non-trivial MPI+OpenMP applications. We achieve this by integrating \toolname into ReMPI, an existing scalable MPI record-and-replay tool, with only a small MPI-scale-independent runtime overhead.

</details>


### [11] [Scrutinizing Variables for Checkpoint Using Automatic Differentiation](https://arxiv.org/abs/2602.16010)
*Xin Huang,Weiping Zhang,Shiman Meng,Wubiao Xu,Xiang Fu,Luanzheng Guo,Kento Sato*

Main category: cs.DC

TL;DR: 利用自动微分技术分析HPC应用中变量元素的"关键性"，仅对影响程序输出的关键元素进行checkpoint，可节省高达20%的存储空间


<details>
  <summary>Details</summary>
Motivation: 传统checkpoint/restart机制保存整个程序状态，消耗大量系统资源。但HPC应用中并非所有数据都参与计算，未使用的数据应从checkpoint中排除以提高存储和计算效率

Method: 提出系统化方法，利用自动微分工具检查变量（如数组）中每个元素对checkpoint的必要性。通过AD分析每个元素是否影响程序输出，识别关键/非关键元素，仅对关键元素进行checkpoint

Result: 在NAS Parallel Benchmark的8个基准测试中成功可视化变量内关键/非关键元素区域，发现这些模式分布与算法的物理公式/逻辑相符。评估显示该方法可节省高达20%的checkpoint存储空间

Conclusion: 基于自动微分的元素级关键性分析方法能有效识别HPC应用中可排除checkpoint的数据，显著减少存储开销，同时保持程序正确性

Abstract: Checkpoint/Restart (C/R) saves the running state of the programs periodically, which consumes considerable system resources. We observe that not every piece of data is involved in the computation in typical HPC applications; such unused data should be excluded from checkpointing for better storage/compute efficiency. To find out, we propose a systematic approach that leverages automatic differentiation (AD) to scrutinize every element within variables (e.g., arrays) for checkpointing allowing us to identify critical/uncritical elements and eliminate uncritical elements from checkpointing. Specifically, we inspect every single element within a variable for checkpointing with an AD tool to determine whether the element has an impact on the application output or not. We empirically validate our approach with eight benchmarks from the NAS Parallel Benchmark (NPB) suite. We successfully visualize critical/uncritical elements/regions within a variable with respect to its impact (yes or no) on the application output. We find patterns/distributions of critical/uncritical elements/regions quite interesting and follow the physical formulation/logic of the algorithm.The evaluation on NPB benchmarks shows that our approach saves storage for checkpointing by up to 20%.

</details>


### [12] [LLM-Driven Intent-Based Privacy-Aware Orchestration Across the Cloud-Edge Continuum](https://arxiv.org/abs/2602.16100)
*Zijie Su,Muhammed Tawfiqul Islam,Mohammad Goudarzi,Adel N. Toosi*

Main category: cs.DC

TL;DR: 提出动态流水线重配置方法，在异构GPU集群上实现LLM推理服务的在线配置调整，最小化服务中断时间


<details>
  <summary>Details</summary>
Motivation: 随着LLM快速发展，在有限GPU资源下高效服务LLM推理成为关键挑战。现有研究探索将无服务器计算范式应用于LLM服务以最大化资源利用率，但LLM推理工作负载高度多样化，现代GPU集群本质上是异构的，需要动态调整部署配置以适应无服务器环境的弹性和动态特性。同时，由于LLM推理的有状态特性和模型参数规模巨大，实现在线重配置特别具有挑战性。

Method: 提出动态流水线重配置方法，支持在线调整流水线配置，同时最小化服务停机时间和性能下降。该方法允许系统根据变化的工作负载选择最优的流水线配置。

Result: 在包括NVIDIA A100和L40s的异构GPU平台上进行实验，结果表明迁移机制导致的服务停机时间少于50毫秒，同时对首令牌时间（TTFT）和每输出令牌时间（TPOT）引入的额外开销均低于10%。

Conclusion: 提出的动态流水线重配置方法能够有效应对LLM推理服务在异构无服务器环境中的挑战，实现高效的在线配置调整，为弹性LLM服务提供了可行的解决方案。

Abstract: With the rapid advancement of large language models (LLMs), efficiently serving LLM inference under limited GPU resources has become a critical challenge. Recently, an increasing number of studies have explored applying serverless computing paradigms to LLM serving in order to maximize resource utilization. However, LLM inference workloads are highly diverse, and modern GPU clusters are inherently heterogeneous, making it necessary to dynamically adjust deployment configurations online to better adapt to the elastic and dynamic nature of serverless environments. At the same time, enabling such online reconfiguration is particularly challenging due to the stateful nature of LLM inference and the massive size of model parameters. In this paper, we propose a dynamic pipeline reconfiguration approach that enables online adjustment of pipeline configurations while minimizing service downtime and performance degradation. Our method allows the system to select the optimal pipeline configuration in response to changing workloads. Experimental results on heterogeneous GPU platforms, including NVIDIA A100 and L40s, demonstrate that our migration mechanism incurs less than 50 ms of service downtime, while introducing under 10% overhead on both time-to-first-token (TTFT) and time-per-output-token (TPOT).

</details>


### [13] [Near-optimal population protocols on bounded-degree trees](https://arxiv.org/abs/2602.16222)
*Joel Rybicki,Jakob Solnerzik,Robin Vacus*

Main category: cs.DC

TL;DR: 在稀疏交互图（有界度树）上，群体协议对于领导者选举和精确多数问题不存在显著的空间-时间权衡，这与完全图不同。作者提出了常数空间协议，实现了接近最优的最坏情况稳定时间。


<details>
  <summary>Details</summary>
Motivation: 先前研究已知在完全交互图中，领导者选举和精确多数问题存在最优的空间-时间权衡。但现有下界技术无法扩展到高密度图之外，因此不清楚其他图族是否表现出类似的空间-时间复杂度权衡。

Method: 提出了两种新协议：1）适用于一般交互图的快速自稳定2跳着色协议，使用随机漂移论证来界定稳定时间；2）在任何树上以最优时间构建有根树的自稳定树定向算法。利用这些协议，可以使用为有向树设计的简单常数状态协议来快速解决领导者选举和精确多数问题。

Result: 在有界度树上，领导者选举和精确多数问题可以实现常数空间协议，且具有接近最优的最坏情况期望稳定时间。新协议相比现有技术实现了线性加速。例如，"有向"湮灭动力学可以在有向树上以O(n² log n)步解决精确多数问题。

Conclusion: 与完全图不同，有界度树上的群体协议对于领导者选举和精确多数问题不存在显著的空间-时间权衡。提出的新协议和算法为稀疏交互图上的群体协议提供了高效解决方案，这些协议具有独立的理论价值。

Abstract: We investigate space-time trade-offs for population protocols in sparse interaction graphs. In complete interaction graphs, optimal space-time trade-offs are known for the leader election and exact majority problems. However, it has remained open if other graph families exhibit similar space-time complexity trade-offs, as existing lower bound techniques do not extend beyond highly dense graphs.
  In this work, we show that -- unlike in complete graphs -- population protocols on bounded-degree trees do not exhibit significant asymptotic space-time trade-offs for leader election and exact majority. For these problems, we give constant-space protocols that have near-optimal worst-case expected stabilisation time. These new protocols achieve a linear speed-up compared to the state-of-the-art.
  Our results are based on two novel protocols, which we believe are of independent interest. First, we give a new fast self-stabilising 2-hop colouring protocol for general interaction graphs, whose stabilisation time we bound using a stochastic drift argument. Second, we give a self-stabilising tree orientation algorithm that builds a rooted tree in optimal time on any tree. As a consequence, we can use simple constant-state protocols designed for directed trees to solve leader election and exact majority fast. For example, we show that ``directed'' annihilation dynamics solve exact majority in $O(n^2 \log n)$ steps on directed trees.

</details>


### [14] [DistributedEstimator: Distributed Training of Quantum Neural Networks via Circuit Cutting](https://arxiv.org/abs/2602.16233)
*Prabhjot Singh,Adel N. Toosi,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 电路切割将大量子电路分解为小电路，通过经典重构恢复期望值。本文提出切割感知的估计器执行流程，量化切割在训练中的端到端开销，发现重构是主要瓶颈，但准确性和鲁棒性得以保持。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注电路切割的子电路数量和采样复杂度，但对端到端训练流程的系统影响缺乏测量。需要量化切割在迭代训练中的实际开销、扩展限制以及对学习结果的影响。

Method: 提出切割感知的估计器执行流程，将电路切割视为分阶段分布式工作负载，将每个估计器查询分解为：分区、子实验生成、并行执行和经典重构四个阶段。使用Iris和MNIST二分类任务的运行时跟踪和学习结果进行测量。

Result: 切割引入显著的端到端开销，随切割数量增加；重构占每查询时间的主导部分，限制了并行化带来的加速。在测试的配置中，准确性和鲁棒性得以保持，某些切割设置下甚至观察到改进。

Conclusion: 电路切割在量子学习工作负载中的实际扩展取决于减少和重叠重构操作，以及考虑屏障主导关键路径的调度策略。虽然系统开销显著，但学习质量可以保持。

Abstract: Circuit cutting decomposes a large quantum circuit into a collection of smaller subcircuits. The outputs of these subcircuits are then classically reconstructed to recover the original expectation values. While prior work characterises cutting overhead largely in terms of subcircuit counts and sampling complexity, its end-to-end impact on iterative, estimator-driven training pipelines remains insufficiently measured from a systems perspective. In this paper, we propose a cut-aware estimator execution pipeline that treats circuit cutting as a staged distributed workload and instruments each estimator query into partitioning, subexperiment generation, parallel execution, and classical reconstruction phases. Using logged runtime traces and learning outcomes on two binary classification workloads (Iris and MNIST), we quantify cutting overheads, scaling limits, and sensitivity to injected stragglers, and we evaluate whether accuracy and robustness are preserved under matched training budgets. Our measurements show that cutting introduces substantial end-to-end overheads that grow with the number of cuts, and that reconstruction constitutes a dominant fraction of per-query time, bounding achievable speed-up under increased parallelism. Despite these systems costs, test accuracy and robustness are preserved in the measured regimes, with configuration-dependent improvements observed in some cut settings. These results indicate that practical scaling of circuit cutting for learning workloads hinges on reducing and overlapping reconstruction and on scheduling policies that account for barrier-dominated critical paths.

</details>


### [15] [push0: Scalable and Fault-Tolerant Orchestration for Zero-Knowledge Proof Generation](https://arxiv.org/abs/2602.16338)
*Mohsen Ahmadvand,Rok Pajnič,Ching-Lun Chiu*

Main category: cs.DC

TL;DR: push0是一个云原生的零知识证明编排系统，通过解耦证明器二进制文件与调度基础设施，实现区块链系统中严格有序、低延迟、容错的证明生成编排。


<details>
  <summary>Details</summary>
Motivation: 零知识证明生成对区块链系统有严格的时序和可靠性要求。ZK-rollups中延迟证明会导致最终性延迟和经济损失；以太坊L1 zkEVM需要在12秒时隙窗口内完成证明以实现无状态验证。当前缺乏解决严格链头排序、子时隙延迟限制、容错任务重新分配和证明器无关工作流组合等联合挑战的原则性编排框架。

Method: push0采用基于持久优先级队列的事件驱动分发器-收集器架构，将证明器二进制文件与调度基础设施解耦。该系统强制执行块顺序证明，同时利用块内并行性，支持异构zkVMs的无缝集成和通过消息持久化实现自动任务恢复。

Result: 在生产Kubernetes集群实验中，push0实现了5毫秒中位数编排开销，在32个分发器下达到99-100%的扩展效率，相对于典型的7+秒证明计算时间，开销可忽略（小于0.1%）。在受控Docker实验中显示3-10毫秒P50性能。已在Zircuit zkrollup生产部署（自2025年3月以来处理1400多万个主网区块）。

Conclusion: push0为集中式rollup运营商和去中心化多证明器网络提供了必要的调度原语，能够满足实时证明生成的要求，解决了零知识证明编排中的关键挑战。

Abstract: Zero-knowledge proof generation imposes stringent timing and reliability constraints on blockchain systems. For ZK-rollups, delayed proofs cause finality lag and economic loss; for Ethereum's emerging L1 zkEVM, proofs must complete within the 12-second slot window to enable stateless validation. The Ethereum Foundation's Ethproofs initiative coordinates multiple independent zkVMs across proving clusters to achieve real-time block proving, yet no principled orchestration framework addresses the joint challenges of (i) strict head-of-chain ordering, (ii) sub-slot latency bounds, (iii) fault-tolerant task reassignment, and (iv) prover-agnostic workflow composition. We present push0, a cloud-native proof orchestration system that decouples prover binaries from scheduling infrastructure. push0 employs an event-driven dispatcher--collector architecture over persistent priority queues, enforcing block-sequential proving while exploiting intra-block parallelism. We formalize requirements drawn from production ZK-rollup operations and the Ethereum real-time proving specification, then demonstrate via production Kubernetes cluster experiments that push0 achieves 5 ms median orchestration overhead with 99--100% scaling efficiency at 32 dispatchers for realistic workloads--overhead negligible (less than 0.1%) relative to typical proof computation times of 7+ seconds. Controlled Docker experiments validate these results, showing comparable performance (3--10 ms P50) when network variance is eliminated. Production deployment on the Zircuit zkrollup (14+ million mainnet blocks since March 2025) provides ecological validity for these controlled experiments. Our design enables seamless integration of heterogeneous zkVMs, supports automatic task recovery via message persistence, and provides the scheduling primitives necessary for both centralized rollup operators and decentralized multi-prover networks.

</details>


### [16] [Load Balanced Parallel Node Generation for Meshless Numerical Methods](https://arxiv.org/abs/2602.16347)
*Jon Vehovar,Miha Rot,Matjaž Depolli,Gregor Kosec*

Main category: cs.DC

TL;DR: 提出一种并行化的n维泊松圆盘采样方法，用于生成无网格方法节点，通过耦合空间索引和工作分布超树实现高效并行，减少锁竞争


<details>
  <summary>Details</summary>
Motivation: 无网格方法需要合适的节点分布，泊松圆盘采样能处理复杂几何和可变节点密度，但现有方法并行效率低，需要改进并行化策略

Method: 修改n维泊松圆盘采样方法，采用耦合空间索引和工作分布超树，预建工作超树平衡工作负载，线程独立推进并避免冲突，减少锁需求

Result: 提出的算法减少了线程碰撞处理的互斥锁获取次数，提高了并行效率，并探讨了适应分布式系统的要求

Conclusion: 该方法成功实现了泊松圆盘采样的高效并行化，为无网格分析提供了更好的节点生成方案，并具备扩展到分布式系统的潜力

Abstract: Meshless methods are used to solve partial differential equations by approximating differential operators at a node as a weighted sum of values at its neighbours. One of the algorithms for generating nodes suitable for meshless numerical analysis is an n-dimensional Poisson disc sampling based method. It can handle complex geometries and supports variable node density, a crucial feature for adaptive analysis. We modify this method for parallel execution using coupled spatial indexing and work distribution hypertrees. The latter is prebuilt according to the node density function, ensuring that each leaf represents a balanced work unit. Threads advance separate fronts and claim work hypertree leaves as needed while avoiding leaves neighbouring those claimed by other threads. Node placement constraints and the partially prebuilt spatial hypertree are combined to eliminate the need to lock the tree while it is being modified. Thread collision handling is managed by the work hypertree at the leaf level, drastically reducing the number of required mutex acquisitions for point insertion collision checks. We explore the behaviour of the proposed algorithm and compare the performance with existing attempts at parallelisation and consider the requirements for adapting the developed algorithm to distributed systems.

</details>


### [17] [How Reliable is Your Service at the Extreme Edge? Analytical Modeling of Computational Reliability](https://arxiv.org/abs/2602.16362)
*MHD Saria Allahham,Hossam S. Hassanein*

Main category: cs.DC

TL;DR: 提出一个分析框架来量化边缘计算中计算可靠性，定义设备或设备集合在特定QoS阈值下满足处理需求的概率，并推导出闭式可靠性表达式和最优工作负载分配规则。


<details>
  <summary>Details</summary>
Motivation: 极端边缘计算（XEC）利用消费者设备处理流式工作负载，但这些设备由于竞争应用和不可预测的使用模式表现出计算可用性的波动性。这种波动性带来了一个基本挑战：如何量化设备或设备集合能够维持流式服务所需处理速率的概率？

Method: 提出一个计算可靠性分析框架，在两种信息机制下推导闭式可靠性表达式：最小信息（仅需声明的操作边界）和历史数据（通过最大似然估计从过去观测中精炼估计）。框架扩展到多设备部署，为串行、并行和分区工作负载配置提供可靠性表达式，推导最优工作负载分配规则和设备选择的解析边界。

Result: 使用YOLO11m模型进行实时目标检测作为代表性分布式推理流式工作负载，在模拟XED环境中的实验表明，分析预测、蒙特卡洛采样和实际测量在不同容量和需求配置下具有高度一致性。

Conclusion: 该分析框架为编排器提供了可处理的工具来评估部署可行性和配置分布式流式系统，能够有效解决边缘计算中设备计算可用性波动带来的可靠性挑战。

Abstract: Extreme Edge Computing (XEC) distributes streaming workloads across consumer-owned devices, exploiting their proximity to users and ubiquitous availability. Many such workloads are AI-driven, requiring continuous neural network inference for tasks like object detection and video analytics. Distributed Inference (DI), which partitions model execution across multiple edge devices, enables these streaming services to meet strict throughput and latency requirements. Yet consumer devices exhibit volatile computational availability due to competing applications and unpredictable usage patterns. This volatility poses a fundamental challenge: how can we quantify the probability that a device, or ensemble of devices, will maintain the processing rate required by a streaming service? This paper presents an analytical framework for computational reliability in XEC, defined as the probability that instantaneous capacity meets demand at a specified Quality of Service (QoS) threshold. We derive closed-form reliability expressions under two information regimes: Minimal Information (MI), requiring only declared operational bounds, and historical data, which refines estimates via Maximum Likelihood Estimation from past observations. The framework extends to multi-device deployments, providing reliability expressions for series, parallel, and partitioned workload configurations. We derive optimal workload allocation rules and analytical bounds for device selection, equipping orchestrators with tractable tools to evaluate deployment feasibility and configure distributed streaming systems. We validate the framework using real-time object detection with YOLO11m model as a representative DI streaming workload; experiments on emulated XED environments demonstrate close agreement between analytical predictions, Monte Carlo sampling, and empirical measurements across diverse capacity and demand configurations.

</details>


### [18] [FlowPrefill: Decoupling Preemption from Prefill Scheduling Granularity to Mitigate Head-of-Line Blocking in LLM Serving](https://arxiv.org/abs/2602.16603)
*Chia-chi Hsieh,Zan Zong,Xinyang Chen,Jianjiang Li,Jidong Zhai,Lijie Wen*

Main category: cs.DC

TL;DR: FlowPrefill通过解耦抢占粒度与调度频率，提出算子级抢占和事件驱动调度，在保证TTFT SLO的同时提升吞吐量5.6倍


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务系统中，计算密集的prefill阶段存在队头阻塞问题，长请求独占资源导致高优先级请求延迟，造成TTFT SLO违规。现有分块prefill方法存在响应性与吞吐量的固有权衡，需要自适应抢占机制。

Method: 1) 算子级抢占：利用算子边界实现细粒度执行中断，避免固定小分块带来的效率损失；2) 事件驱动调度：仅在请求到达或完成时触发调度决策，支持高效抢占响应性同时最小化控制平面开销。

Result: 在真实生产trace评估中，FlowPrefill相比最先进系统将最大有效吞吐量提升达5.6倍，同时满足异构SLO要求。

Conclusion: FlowPrefill通过解耦抢占粒度与调度频率，解决了LLM服务系统中prefill阶段的响应性与吞吐量权衡问题，实现了TTFT与有效吞吐量的双重优化。

Abstract: The growing demand for large language models (LLMs) requires serving systems to handle many concurrent requests with diverse service level objectives (SLOs). This exacerbates head-of-line (HoL) blocking during the compute-intensive prefill phase, where long-running requests monopolize resources and delay higher-priority ones, leading to widespread time-to-first-token (TTFT) SLO violations. While chunked prefill enables interruptibility, it introduces an inherent trade-off between responsiveness and throughput: reducing chunk size improves response latency but degrades computational efficiency, whereas increasing chunk size maximizes throughput but exacerbates blocking. This necessitates an adaptive preemption mechanism. However, dynamically balancing execution granularity against scheduling overheads remains a key challenge.
  In this paper, we propose FlowPrefill, a TTFT-goodput-optimized serving system that resolves this conflict by decoupling preemption granularity from scheduling frequency. To achieve adaptive prefill scheduling, FlowPrefill introduces two key innovations: 1) Operator-Level Preemption, which leverages operator boundaries to enable fine-grained execution interruption without the efficiency loss associated with fixed small chunking; and 2) Event-Driven Scheduling, which triggers scheduling decisions only upon request arrival or completion events, thereby supporting efficient preemption responsiveness while minimizing control-plane overhead. Evaluation on real-world production traces shows that FlowPrefill improves maximum goodput by up to 5.6$\times$ compared to state-of-the-art systems while satisfying heterogeneous SLOs.

</details>
