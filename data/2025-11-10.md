<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [An Efficient Proximity Graph-based Approach to Table Union Search](https://arxiv.org/abs/2511.05082)
*Yiming Xie,Hua Dai,Mingfeng Jiang,Pengyue Li,zhengkai Zhang,Bohan Li*

Main category: cs.DB

TL;DR: 提出PGTUS方法，通过多阶段流水线结合新颖的优化策略和基于多对一二分图匹配的过滤策略，显著提升表格联合搜索效率，在保持召回率的同时实现3.6-6.0倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决多向量模型在表格联合搜索中因依赖二分图最大匹配计算联合性得分而面临的严重效率挑战。

Method: 采用多阶段流水线，结合新颖的优化策略、基于多对一二分图匹配的过滤策略，以及增强的剪枝策略来修剪候选集。

Result: 在六个基准数据集上的实验表明，该方法相比现有方法实现了3.6-6.0倍的加速，同时保持相当的召回率。

Conclusion: PGTUS方法有效解决了表格联合搜索的效率问题，在保持检索质量的同时显著提升了搜索速度。

Abstract: Neural embedding models are extensively employed in the table union search
problem, which aims to find semantically compatible tables that can be merged
with a given query table. In particular, multi-vector models, which represent a
table as a vector set (typically one vector per column), have been demonstrated
to achieve superior retrieval quality by capturing fine-grained semantic
alignments. However, this problem faces more severe efficiency challenges than
the single-vector problem due to the inherent dependency on bipartite graph
maximum matching to compute unionability scores. Therefore, this paper proposes
an efficient Proximity Graph-based Table Union Search (PGTUS) approach. PGTUS
employs a multi-stage pipeline that combines a novel refinement strategy, a
filtering strategy based on many-to-one bipartite matching. Besides, we propose
an enhanced pruning strategy to prune the candidate set, which further improve
the search efficiency. Extensive experiments on six benchmark datasets
demonstrate that our approach achieves 3.6-6.0X speedup over existing
approaches while maintaining comparable recall rates.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Marionette: Data Structure Description and Management for Heterogeneous Computing](https://arxiv.org/abs/2511.04853)
*Nuno dos Santos Fernandes,Pedro Tomás,Nuno Roma,Frank Winklmeier,Patricia Conde-Muíño*

Main category: cs.DC

TL;DR: Marionette是一个C++17库，旨在通过解耦数据布局与接口描述、支持多种内存管理策略、提供跨设备高效数据传输，来简化大型面向对象C++代码库在异构平台上的硬件加速适配。


<details>
  <summary>Details</summary>
Motivation: 适配大型面向对象C++代码库进行硬件加速（特别是针对GPU等异构平台）极具挑战性，需要解决数据布局、内存管理和跨设备数据传输等问题。

Method: 设计了一个编译时抽象库，支持灵活的数据结构定义，将数据布局与接口描述解耦，提供多种内存管理策略和高效的数据传输转换机制。

Result: 通过CUDA案例研究证明了该库的高效性和灵活性，能够以最小运行时开销实现跨设备数据管理。

Conclusion: Marionette为C++代码库在异构平台上的硬件加速提供了一种高效、便携且兼容现有代码的解决方案。

Abstract: Adapting large, object-oriented C++ codebases for hardware acceleration might
be extremely challenging, particularly when targeting heterogeneous platforms
such as GPUs. Marionette is a C++17 library designed to address this by
enabling flexible, efficient, and portable data structure definitions. It
decouples data layout from the description of the interface, supports multiple
memory management strategies, and provides efficient data transfers and
conversions across devices, all of this with minimal runtime overhead due to
the compile-time nature of its abstractions. By allowing interfaces to be
augmented with arbitrary functions, Marionette maintains compatibility with
existing code and offers a streamlined interface that supports both
straightforward and advanced use cases. This paper outlines its design, usage,
and performance, including a CUDA-based case study demonstrating its efficiency
and flexibility.

</details>


### [3] [Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs](https://arxiv.org/abs/2511.05053)
*Wakuto Matsumi,Riaz-Ul-Haque Mian*

Main category: cs.DC

TL;DR: 本文设计并实现了针对超维计算(HDC)优化的自定义GPU指令，在RISC-V GPU平台上实现了HDC-CNN混合工作负载的高效处理，性能提升最高达56.2倍。


<details>
  <summary>Details</summary>
Motivation: 神经网络机器学习能耗高，而HDC作为轻量级脑启发计算方案在复杂视觉任务上精度较低。现有HDC-CNN混合加速器存在泛化性和可编程性差的问题。RISC-V架构为领域专用GPU设计提供了新机会。

Method: 在RISC-V GPU上设计并实现四种自定义HDC指令，优化HDC操作的处理效率，支持HDC-CNN混合工作负载。

Result: 微基准测试显示，使用自定义HDC指令后性能提升最高达56.2倍。

Conclusion: RISC-V GPU在能效和高性能计算方面具有巨大潜力，特别适合HDC等定制计算模型。

Abstract: Machine learning based on neural networks has advanced rapidly, but the high
energy consumption required for training and inference remains a major
challenge. Hyperdimensional Computing (HDC) offers a lightweight,
brain-inspired alternative that enables high parallelism but often suffers from
lower accuracy on complex visual tasks. To overcome this, hybrid accelerators
combining HDC and Convolutional Neural Networks (CNNs) have been proposed,
though their adoption is limited by poor generalizability and programmability.
The rise of open-source RISC-V architectures has created new opportunities for
domain-specific GPU design. Unlike traditional proprietary GPUs, emerging
RISC-V-based GPUs provide flexible, programmable platforms suitable for custom
computation models such as HDC. In this study, we design and implement custom
GPU instructions optimized for HDC operations, enabling efficient processing
for hybrid HDC-CNN workloads. Experimental results using four types of custom
HDC instructions show a performance improvement of up to 56.2 times in
microbenchmark tests, demonstrating the potential of RISC-V GPUs for
energy-efficient, high-performance computing.

</details>


### [4] [GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters](https://arxiv.org/abs/2511.05067)
*Giuseppe Esposito,Juan-David Guerrero-Balaguera,Josie Esteban Rodriguez Condia,Matteo Sonza Reorda,Marco Barbiero,Rossella Fortuna*

Main category: cs.DC

TL;DR: 结合在线遥测参数和硬件性能计数器来评估GPU在不同应用下的压力，通过性能计数器测量吞吐量、指令数量和停顿事件来预测并行工作负载的压力。


<details>
  <summary>Details</summary>
Motivation: GPU在持续工作负载下会产生显著压力，可能导致组件故障和计算错误，因此需要评估应用对GPU的压力以预测可靠性（特别是老化效应）。

Method: 结合在线遥测参数和硬件性能计数器，通过测量吞吐量、指令数量和停顿事件来评估GPU压力。

Result: 实验结果表明，通过结合遥测数据和性能计数器可以估计并行工作负载对GPU产生的压力，性能计数器揭示了目标工作负载在资源使用效率方面的表现。

Conclusion: 利用遥测数据和性能计数器能够有效评估GPU压力，这对于预测GPU可靠性和老化效应具有重要意义。

Abstract: Graphics Processing Units (GPUs) are specialized accelerators in data centers
and high-performance computing (HPC) systems, enabling the fast execution of
compute-intensive applications, such as Convolutional Neural Networks (CNNs).
However, sustained workloads can impose significant stress on GPU components,
raising reliability concerns due to potential faults that corrupt the
intermediate application computations, leading to incorrect results. Estimating
the stress induced by an application is thus crucial to predict reliability
(with\,special\,emphasis\,on\,aging\,effects). In this work, we combine online
telemetry parameters and hardware performance counters to assess GPU stress
induced by different applications. The experimental results indicate the stress
induced by a parallel workload can be estimated by combining telemetry data and
Performance Counters that reveal the efficiency in the resource usage of the
target workload. For this purpose the selected performance counters focus on
measuring the i) throughput, ii) amount of issued instructions and iii) stall
events.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [5] [Agentic Refactoring: An Empirical Study of AI Coding Agents](https://arxiv.org/abs/2511.04824)
*Kosei Horikawa,Hao Li,Yutaro Kashiwa,Bram Adams,Hajimu Iida,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 本研究对AI智能体在真实开源Java项目中的重构活动进行了大规模实证分析，发现重构是AI开发范式中常见且有意为之的活动，但主要集中于低层次、一致性导向的编辑，与人类重构的高层次设计变更形成对比。


<details>
  <summary>Details</summary>
Motivation: 解决对AI智能体重构实践缺乏实证理解的问题，包括其使用方式、与人类重构的比较以及对代码质量的影响。

Method: 基于AIDev数据集，分析了15,451个重构实例，涉及12,256个拉取请求和14,988个提交，对Java开源项目进行大规模实证研究。

Result: AI智能体在26.1%的提交中明确针对重构；重构类型以低层次编辑为主（如变量类型变更11.8%、参数重命名10.4%）；动机主要是内部质量关注（可维护性52.5%、可读性28.1%）；在结构指标上产生小而显著的改进，特别是中等规模变更。

Conclusion: AI智能体重构是开发过程中的常见活动，但偏向于局部改进而非高层次设计变更，在代码质量指标上能带来统计显著的改进。

Abstract: Agentic coding tools, such as OpenAI Codex, Claude Code, and Cursor, are
transforming the software engineering landscape. These AI-powered systems
function as autonomous teammates capable of planning and executing complex
development tasks. Agents have become active participants in refactoring, a
cornerstone of sustainable software development aimed at improving internal
code quality without altering observable behavior. Despite their increasing
adoption, there is a critical lack of empirical understanding regarding how
agentic refactoring is utilized in practice, how it compares to human-driven
refactoring, and what impact it has on code quality. To address this empirical
gap, we present a large-scale study of AI agent-generated refactorings in
real-world open-source Java projects, analyzing 15,451 refactoring instances
across 12,256 pull requests and 14,988 commits derived from the AIDev dataset.
Our empirical analysis shows that refactoring is a common and intentional
activity in this development paradigm, with agents explicitly targeting
refactoring in 26.1% of commits. Analysis of refactoring types reveals that
agentic efforts are dominated by low-level, consistency-oriented edits, such as
Change Variable Type (11.8%), Rename Parameter (10.4%), and Rename Variable
(8.5%), reflecting a preference for localized improvements over the high-level
design changes common in human refactoring. Additionally, the motivations
behind agentic refactoring focus overwhelmingly on internal quality concerns,
with maintainability (52.5%) and readability (28.1%). Furthermore, quantitative
evaluation of code quality metrics shows that agentic refactoring yields small
but statistically significant improvements in structural metrics, particularly
for medium-level changes, reducing class size and complexity (e.g., Class LOC
median $\Delta$ = -15.25).

</details>


### [6] [Software Defined Vehicle Code Generation: A Few-Shot Prompting Approach](https://arxiv.org/abs/2511.04849)
*Quang-Dung Nguyen,Tri-Dung Tran,Thanh-Hieu Chu,Hoang-Loc Tran,Xiangwei Cheng,Dirk Slama*

Main category: cs.SE

TL;DR: 本研究探讨了如何通过提示工程优化大型语言模型在软件定义车辆代码生成中的表现，发现少量样本提示策略在调整模型输出以匹配预期结果方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 软件定义车辆的发展需要先进的代码生成工具，但专有模型架构的限制阻碍了大型语言模型在该特定任务中的应用。

Method: 使用系统提示和先进的提示工程技术设计高效提示结构，通过不同提示策略（包括裸模型和少量样本提示）在专门创建的基准上对多种模型进行广泛实验。

Result: 结果显示，采用少量样本提示策略的模型在定量指标上表现最优，能够更好地调整模型输出以匹配预期结果。

Conclusion: 仅通过精心设计的提示工程，无需训练或访问基础模型架构，就能有效优化大型语言模型在软件定义车辆代码生成任务中的性能。

Abstract: The emergence of Software-Defined Vehicles (SDVs) marks a paradigm shift in
the automotive industry, where software now plays a pivotal role in defining
vehicle functionality, enabling rapid innovation of modern vehicles. Developing
SDV-specific applications demands advanced tools to streamline code generation
and improve development efficiency. In recent years, general-purpose large
language models (LLMs) have demonstrated transformative potential across
domains. Still, restricted access to proprietary model architectures hinders
their adaption to specific tasks like SDV code generation. In this study, we
propose using prompts, a common and basic strategy to interact with LLMs and
redirect their responses. Using only system prompts with an appropriate and
efficient prompt structure designed using advanced prompt engineering
techniques, LLMs can be crafted without requiring a training session or access
to their base design. This research investigates the extensive experiments on
different models by applying various prompting techniques, including bare
models, using a benchmark specifically created to evaluate LLMs' performance in
generating SDV code. The results reveal that the model with a few-shot
prompting strategy outperforms the others in adjusting the LLM answers to match
the expected outcomes based on quantitative metrics.

</details>


### [7] [What About Our Bug? A Study on the Responsiveness of NPM Package Maintainers](https://arxiv.org/abs/2511.04986)
*Mohammadreza Saeidi,Ethan Thoma,Raula Gaikovina Kula,Gema Rodríguez-Pérez*

Main category: cs.SE

TL;DR: 研究调查了npm生态系统中维护者对bug报告的响应情况，发现维护者通常响应积极（中位响应率70%），但某些bug由于依赖约束、贡献实践和库特定标准等原因未被解决。


<details>
  <summary>Details</summary>
Motivation: 第三方库的广泛使用使得npm等生态系统对现代软件开发至关重要，但依赖链中的bug可能向下游传播。假设维护者可能不总是决定修复bug，特别是当他们认为这超出其在依赖链中的责任范围时。

Method: 采用混合方法分析30,340个bug报告，涵盖500个最受依赖的npm包，通过挖掘仓库问题数据和定性开放编码来分析未处理bug报告的原因。

Result: 维护者通常响应积极，项目级响应率中位数为70%（IQR：55%-89%），反映了他们对支持下游开发者的承诺。

Conclusion: 提出了未解决bug原因的分类法，包括贡献实践、依赖约束和库特定标准。理解维护者行为可以为促进更健壮和响应迅速的开源生态系统提供实践指导。

Abstract: Background: Widespread use of third-party libraries makes ecosystems like
Node Package Manager (npm) critical to modern software development. However,
this interconnected chain of dependencies also creates challenges: bugs in one
library can propagate downstream, potentially impacting many other libraries
that rely on it. We hypothesize that maintainers may not always decide to fix a
bug, especially if the maintainer decides it falls out of their responsibility
within the chain of dependencies. Aims: To confirm this hypothesis, we
investigate the responsiveness of 30,340 bug reports across 500 of the most
depended-upon npm packages. Method: We adopt a mixed-method approach to mine
repository issue data and perform qualitative open coding to analyze reasons
behind unaddressed bug reports. Results: Our findings show that maintainers are
generally responsive, with a median project-level responsiveness of 70% (IQR:
55%-89%), reflecting their commitment to support downstream developers.
Conclusions: We present a taxonomy of the reasons some bugs remain unresolved.
The taxonomy includes contribution practices, dependency constraints, and
library-specific standards as reasons for not being responsive. Understanding
maintainer behavior can inform practices that promote a more robust and
responsive open-source ecosystem that benefits the entire community.

</details>


### [8] [Generating Software Architecture Description from Source Code using Reverse Engineering and Large Language Model](https://arxiv.org/abs/2511.05165)
*Ahmad Hatahet,Christoph Knieke,Andreas Rausch*

Main category: cs.SE

TL;DR: 提出了一种半自动化从源代码生成软件架构描述的方法，结合逆向工程和大语言模型，自动恢复静态和动态架构视图，减少人工参与。


<details>
  <summary>Details</summary>
Motivation: 实践中软件架构描述经常缺失、过时或与实现不符，导致开发者需要从源代码手动推导架构信息，这增加了认知负担，减缓新开发者上手速度，并导致系统清晰度随时间退化。

Method: 通过逆向工程提取完整组件图，使用提示工程过滤架构重要元素（核心组件），基于底层代码逻辑通过少量示例提示生成状态机图来建模组件行为。

Result: 该方法能够抽象组件图，减少对人工专家的依赖，并能准确表示复杂软件行为，特别是在通过少量示例提示注入领域特定知识时。

Conclusion: 该方法为显著减少人工工作量、增强系统理解和长期可维护性提供了一条可行路径，展示了LLM在软件架构文档生成中的强大能力。

Abstract: Software Architecture Descriptions (SADs) are essential for managing the
inherent complexity of modern software systems. They enable high-level
architectural reasoning, guide design decisions, and facilitate effective
communication among diverse stakeholders. However, in practice, SADs are often
missing, outdated, or poorly aligned with the system's actual implementation.
Consequently, developers are compelled to derive architectural insights
directly from source code-a time-intensive process that increases cognitive
load, slows new developer onboarding, and contributes to the gradual
degradation of clarity over the system's lifetime. To address these issues, we
propose a semi-automated generation of SADs from source code by integrating
reverse engineering (RE) techniques with a Large Language Model (LLM). Our
approach recovers both static and behavioral architectural views by extracting
a comprehensive component diagram, filtering architecturally significant
elements (core components) via prompt engineering, and generating state machine
diagrams to model component behavior based on underlying code logic with
few-shots prompting. This resulting views representation offer a scalable and
maintainable alternative to traditional manual architectural documentation.
This methodology, demonstrated using C++ examples, highlights the potent
capability of LLMs to: 1) abstract the component diagram, thereby reducing the
reliance on human expert involvement, and 2) accurately represent complex
software behaviors, especially when enriched with domain-specific knowledge
through few-shot prompting. These findings suggest a viable path toward
significantly reducing manual effort while enhancing system understanding and
long-term maintainability.

</details>


### [9] [CodeMapper: A Language-Agnostic Approach to Mapping Code Regions Across Commits](https://arxiv.org/abs/2511.05205)
*Huimin Hu,Michael Pradel*

Main category: cs.SE

TL;DR: CodeMapper解决了代码映射问题，能够在不同提交之间找到特定代码区域的对应关系，支持多种编程语言且不依赖特定程序元素。


<details>
  <summary>Details</summary>
Motivation: 现有技术如git diff只能显示文件的所有更改，无法聚焦开发者选择的特定代码区域；其他技术局限于特定代码元素和编程语言，适用性有限。

Method: CodeMapper采用两阶段方法：1) 通过分析差异、检测代码移动和搜索特定代码片段计算候选区域；2) 通过计算相似度选择最可能的目标区域。

Result: 在四个数据集上的评估显示，CodeMapper在71.0%-94.5%的情况下能正确识别预期目标区域，比现有最佳基线方法提高了1.5-58.8个绝对百分点。

Conclusion: CodeMapper提供了一种独立于特定程序元素和编程语言的代码映射解决方案，显著优于现有方法，能够有效解决软件演化中的代码映射问题。

Abstract: During software evolution, developers commonly face the problem of mapping a
specific code region from one commit to another. For example, they may want to
determine how the condition of an if-statement, a specific line in a
configuration file, or the definition of a function changes. We call this the
code mapping problem. Existing techniques, such as git diff, address this
problem only insufficiently because they show all changes made to a file
instead of focusing on a code region of the developer's choice. Other
techniques focus on specific code elements and programming languages (e.g.,
methods in Java), limiting their applicability. This paper introduces
CodeMapper, an approach to address the code mapping problem in a way that is
independent of specific program elements and programming languages. Given a
code region in one commit, CodeMapper finds the corresponding region in another
commit. The approach consists of two phases: (i) computing candidate regions by
analyzing diffs, detecting code movements, and searching for specific code
fragments, and (ii) selecting the most likely target region by calculating
similarities. Our evaluation applies CodeMapper to four datasets, including two
new hand-annotated datasets containing code region pairs in ten popular
programming languages. CodeMapper correctly identifies the expected target
region in 71.0%--94.5% of all cases, improving over the best available
baselines by 1.5--58.8 absolute percent points.

</details>


### [10] [Building Specialized Software-Assistant ChatBot with Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2511.05297)
*Mohammed Hilel,Yannis Karmim,Jean De Bodinat,Reda Sarehane,Antoine Gillon*

Main category: cs.SE

TL;DR: 提出基于图的检索增强生成框架，将企业Web应用自动转换为状态-动作知识图谱，使LLM能够生成基于实际软件界面的可靠指导，解决传统DAP构建维护成本高和LLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 传统数字采用平台(DAPs)构建和维护需要大量人工工作，而直接使用LLM作为虚拟助手会产生幻觉和不准确答案，且生产级LLM无法微调。

Method: 开发图基检索增强生成框架，自动将企业Web应用转换为状态-动作知识图谱，通过结构化软件界面信息为LLM提供可靠上下文。

Result: 框架已在企业RAKAM和Lemon Learning中部署，能够生成基于实际软件界面的可靠指导，解决了LLM幻觉问题。

Conclusion: 该框架为DAP提供可扩展、稳健的自动化解决方案，显著降低了构建和维护成本，同时保证了指导的准确性和可靠性。

Abstract: Digital Adoption Platforms (DAPs) have become essential tools for helping
employees navigate complex enterprise software such as CRM, ERP, or HRMS
systems. Companies like LemonLearning have shown how digital guidance can
reduce training costs and accelerate onboarding. However, building and
maintaining these interactive guides still requires extensive manual effort.
Leveraging Large Language Models as virtual assistants is an appealing
alternative, yet without a structured understanding of the target software,
LLMs often hallucinate and produce unreliable answers. Moreover, most
production-grade LLMs are black-box APIs, making fine-tuning impractical due to
the lack of access to model weights. In this work, we introduce a Graph-based
Retrieval-Augmented Generation framework that automatically converts enterprise
web applications into state-action knowledge graphs, enabling LLMs to generate
grounded and context-aware assistance. The framework was co-developed with the
AI enterprise RAKAM, in collaboration with Lemon Learning. We detail the
engineering pipeline that extracts and structures software interfaces, the
design of the graph-based retrieval process, and the integration of our
approach into production DAP workflows. Finally, we discuss scalability,
robustness, and deployment lessons learned from industrial use cases.

</details>


### [11] [Code Review Automation using Retrieval Augmented Generation](https://arxiv.org/abs/2511.05302)
*Qianru Meng,Xiao Zhang,Zhaochen Ren,Joost Visser*

Main category: cs.SE

TL;DR: 提出RARe方法，结合检索和生成技术，通过检索增强生成(RAG)将外部领域知识融入代码审查过程，显著提升自动代码审查质量。


<details>
  <summary>Details</summary>
Motivation: 自动代码审查生成虽然前景广阔，但现有方法存在生成评论偏离重点或过于笼统的问题，需要结合外部知识来提升质量。

Method: 使用密集检索器从代码库中选择最相关评论，然后利用大型语言模型的上下文学习能力生成最终审查意见。

Result: 在两个基准数据集上超越现有最优方法，BLEU-4得分分别达到12.32和12.96，并通过人工评估和案例研究验证了有效性。

Conclusion: RARe方法通过检索增强生成有效提升了自动代码审查的质量和实用性，证明了结合检索和生成方法的优势。

Abstract: Code review is essential for maintaining software quality but is
labor-intensive. Automated code review generation offers a promising solution
to this challenge. Both deep learning-based generative techniques and
retrieval-based methods have demonstrated strong performance in this task.
However, despite these advancements, there are still some limitations where
generated reviews can be either off-point or overly general. To address these
issues, we introduce Retrieval-Augmented Reviewer (RARe), which leverages
Retrieval-Augmented Generation (RAG) to combine retrieval-based and generative
methods, explicitly incorporating external domain knowledge into the code
review process. RARe uses a dense retriever to select the most relevant reviews
from the codebase, which then enrich the input for a neural generator,
utilizing the contextual learning capacity of large language models (LLMs), to
produce the final review. RARe outperforms state-of-the-art methods on two
benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively.
Its effectiveness is further validated through a detailed human evaluation and
a case study using an interpretability tool, demonstrating its practical
utility and reliability.

</details>


### [12] [SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for Large Language Models](https://arxiv.org/abs/2511.05459)
*Jingxuan Xu,Ken Deng,Weihao Li,Songwei Yu,Huaixi Tang,Haoyang Huang,Zhiyi Lai,Zizheng Zhan,Yanan Wu,Chenchen Zhang,Kepeng Lei,Yifan Yao,Xinping Lei,Wenqiang Zhu,Zongxian Feng,Han Li,Junqi Xiong,Dailin Li,Zuchen Gao,Kun Wu,Wen Xiang,Ziqi Zhan,Yuanxing Zhang,Wuxuan Gong,Ziyuan Gao,Guanxiang Wang,Yirong Xue,Xiaojiang Zhang,Jinghui Wang,Huiming Wang,Wenhao Zhuang,Zhaoxiang Zhang,Yuqun Zhang,Haotian Zhang,Bin Chen,Jiaheng Liu*

Main category: cs.SE

TL;DR: SWE-Compass是一个全面的软件工程基准测试，涵盖8种任务类型、8种编程场景和10种编程语言，包含2000个从真实GitHub拉取请求中提取的高质量实例，用于评估大语言模型在软件工程中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型软件工程评估存在任务覆盖范围窄、语言偏见、与真实开发者工作流程对齐不足等问题，需要更全面、真实的评估框架。

Method: 构建SWE-Compass基准测试，整合异构代码相关评估到结构化框架中，从真实GitHub拉取请求中筛选和验证2000个实例，在两种代理框架(SWE-Agent和Claude Code)下评估10个最先进的大语言模型。

Result: 揭示了不同任务类型、编程语言和场景之间的难度层次结构，为诊断和提升大语言模型的代理编码能力提供了严格可复现的基础。

Conclusion: SWE-Compass通过与真实开发者实践对齐，为大语言模型在软件工程领域的评估提供了全面、严谨的基准测试框架。

Abstract: Evaluating large language models (LLMs) for software engineering has been
limited by narrow task coverage, language bias, and insufficient alignment with
real-world developer workflows. Existing benchmarks often focus on algorithmic
problems or Python-centric bug fixing, leaving critical dimensions of software
engineering underexplored. To address these gaps, we introduce SWE-Compass1, a
comprehensive benchmark that unifies heterogeneous code-related evaluations
into a structured and production-aligned framework. SWE-Compass spans 8 task
types, 8 programming scenarios, and 10 programming languages, with 2000
high-quality instances curated from authentic GitHub pull requests and refined
through systematic filtering and validation. We benchmark ten state-of-the-art
LLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear
hierarchy of difficulty across task types, languages, and scenarios. Moreover,
by aligning evaluation with real-world developer practices, SWE-Compass
provides a rigorous and reproducible foundation for diagnosing and advancing
agentic coding capabilities in large language models.

</details>


### [13] [A Metamorphic Testing Perspective on Knowledge Distillation for Language Models of Code: Does the Student Deeply Mimic the Teacher?](https://arxiv.org/abs/2511.05476)
*Md. Abdul Awal,Mrigank Rochan,Chanchal K. Roy*

Main category: cs.SE

TL;DR: 提出了MetaCompress框架，通过蜕变测试系统评估知识蒸馏中师生模型的行为保真度，发现传统准确率评估无法捕捉深层行为差异，学生模型在对抗攻击下性能下降高达285%。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的代码语言模型计算成本高、推理速度慢，知识蒸馏被用于模型压缩，但当前基于准确率的评估方法无法深入评估学生模型对教师模型预测行为和内部表征的模仿程度。

Method: 提出MetaCompress蜕变测试框架，通过一组行为保持的蜕变关系比较师生模型的输出，系统评估行为保真度。在三种不同知识蒸馏技术（Compressor、AVATAR、MORPH）获得的压缩模型上进行评估。

Result: MetaCompress能够识别高达62%的行为差异，揭示了学生模型未能深度模仿教师模型的问题，这在传统准确率评估中无法发现。

Conclusion: 需要在知识蒸馏流程中加入行为保真度评估，MetaCompress为测试通过知识蒸馏获得的压缩代码语言模型提供了实用框架。

Abstract: Transformer-based language models of code have achieved state-of-the-art
performance across a wide range of software analytics tasks, but their
practical deployment remains limited due to high computational costs, slow
inference speeds, and significant environmental impact. To address these
challenges, recent research has increasingly explored knowledge distillation as
a method for compressing a large language model of code (the teacher) into a
smaller model (the student) while maintaining performance. However, the degree
to which a student model deeply mimics the predictive behavior and internal
representations of its teacher remains largely unexplored, as current
accuracy-based evaluation provides only a surface-level view of model quality
and often fails to capture more profound discrepancies in behavioral fidelity
between the teacher and student models. To address this gap, we empirically
show that the student model often fails to deeply mimic the teacher model,
resulting in up to 285% greater performance drop under adversarial attacks,
which is not captured by traditional accuracy-based evaluation. Therefore, we
propose MetaCompress, a metamorphic testing framework that systematically
evaluates behavioral fidelity by comparing the outputs of teacher and student
models under a set of behavior-preserving metamorphic relations. We evaluate
MetaCompress on two widely studied tasks, using compressed versions of popular
language models of code, obtained via three different knowledge distillation
techniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress
identifies up to 62% behavioral discrepancies in student models, underscoring
the need for behavioral fidelity evaluation within the knowledge distillation
pipeline and establishing MetaCompress as a practical framework for testing
compressed language models of code derived through knowledge distillation.

</details>
