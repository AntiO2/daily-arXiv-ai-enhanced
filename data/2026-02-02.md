<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 26]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Linux Kernel Recency Matters, CVE Severity Doesn't, and History Fades](https://arxiv.org/abs/2601.22196)
*Piotr Przymus,Witold Weiner,Krzysztof Rykaczewski,Gunnar Kudrjavets*

Main category: cs.SE

TL;DR: Linux内核成为CNA后，研究发现内核漏洞修复延迟与严重性评分关系不大，而与内核新旧程度相关，新内核修复更快，旧内核漏洞常未解决。


<details>
  <summary>Details</summary>
Motivation: 分析Linux内核成为CVE编号机构（CNA）后，研究内核漏洞的解剖结构和动态特征，特别是了解驱动漏洞修复的因素，以理解内核独特的开源项目特性。

Method: 使用元数据、相关提交和补丁延迟来分析内核CVE，采用生存模型研究修复时间预测因素，比较引入漏洞的提交与修复提交的复杂性。

Result: 严重性和CVSS评分与补丁延迟关联微弱；内核新旧程度是合理的预测因子，新内核修复更快，旧内核保留未解决的CVE；引入漏洞的提交通常比修复更广泛复杂。

Conclusion: Linux内核的CVE流程具有独特性，修复决策更多基于内核新旧而非传统安全指标，反映了内核作为独特开源项目的特性。

Abstract: In 2024, the Linux kernel became its own Common Vulnerabilities and Exposures (CVE) Numbering Authority (CNA), formalizing how kernel vulnerabilities are identified and tracked. We analyze the anatomy and dynamics of kernel CVEs using metadata, associated commits, and patch latency to understand what drives patching. Results show that severity and Common Vulnerability Scoring System (CVSS) metrics have a negligible association with patch latency, whereas kernel recency is a reasonable predictor in survival models. Kernel developers fix newer kernels sooner, while older ones retain unresolved CVEs. Commits introducing vulnerabilities are typically broader and more complex than their fixes, though often only approximate reconstructions of development history. The Linux kernel remains a unique open-source project -- its CVE process is no exception.

</details>


### [2] [Stalled, Biased, and Confused: Uncovering Reasoning Failures in LLMs for Cloud-Based Root Cause Analysis](https://arxiv.org/abs/2601.22208)
*Evelien Riddell,James Riddell,Gengyi Sun,Michał Antkiewicz,Krzysztof Czarnecki*

Main category: cs.SE

TL;DR: 该研究对LLM在根因分析中的推理能力进行实证评估，通过控制实验框架评估6个LLM在2种智能体工作流下的表现，执行48,000个故障场景，识别16种常见推理失败模式。


<details>
  <summary>Details</summary>
Motivation: 现代云系统的高度分布式特性使多跳故障传播的根因分析变得复杂，LLM为自动化RCA提供了新机会，但现有方法依赖历史事件语料库或复杂多智能体管道，难以区分推理失败与外围设计问题。

Method: 设计控制实验框架，在简化实验环境中评估6个LLM在两种智能体工作流（ReAct和Plan-and-Execute）和非智能体基线下的表现，使用两个真实案例研究（GAIA和OpenRCA），执行48,000个模拟故障场景，通过LLM-as-a-Judge标注16种常见推理失败分类。

Result: 量化了当前开源LLM在多跳RCA中的成功与失败情况，测量了根因准确率和中间推理轨迹质量，识别了对输入数据模态的敏感性，发现了能够预测最终正确性的推理失败模式。

Conclusion: 研究提供了透明可复现的实证结果和失败分类法，为基于推理的系统诊断未来工作提供指导，明确了当前LLM在复杂根因分析中的能力边界和局限性。

Abstract: Root cause analysis (RCA) is essential for diagnosing failures within complex software systems to ensure system reliability. The highly distributed and interdependent nature of modern cloud-based systems often complicates RCA efforts, particularly for multi-hop fault propagation, where symptoms appear far from their true causes. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance automated RCA. However, their practical value for RCA depends on the fidelity of reasoning and decision-making. Existing work relies on historical incident corpora, operates directly on high-volume telemetry beyond current LLM capacity, or embeds reasoning inside complex multi-agent pipelines -- conditions that obscure whether failures arise from reasoning itself or from peripheral design choices.
  We present a focused empirical evaluation that isolates an LLM's reasoning behavior. We design a controlled experimental framework that foregrounds the LLM by using a simplified experimental setting. We evaluate six LLMs under two agentic workflows (ReAct and Plan-and-Execute) and a non-agentic baseline on two real-world case studies (GAIA and OpenRCA). In total, we executed 48,000 simulated failure scenarios, totaling 228 days of execution time. We measure both root-cause accuracy and the quality of intermediate reasoning traces. We produce a labeled taxonomy of 16 common RCA reasoning failures and use an LLM-as-a-Judge for annotation. Our results clarify where current open-source LLMs succeed and fail in multi-hop RCA, quantify sensitivity to input data modalities, and identify reasoning failures that predict final correctness. Together, these contributions provide transparent and reproducible empirical results and a failure taxonomy to guide future work on reasoning-driven system diagnosis.

</details>


### [3] [Predicting Intermittent Job Failure Categories for Diagnosis Using Few-Shot Fine-Tuned Language Models](https://arxiv.org/abs/2601.22264)
*Henri Aïdasso,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: FlaXifyer：基于预训练语言模型的少样本学习方法，用于预测CI流水线中间歇性作业失败类别，仅需作业执行日志和少量标注数据，同时提出LogSift可解释性技术加速故障诊断。


<details>
  <summary>Details</summary>
Motivation: CI流水线中的间歇性（flaky）作业失败导致大量效率损失：重复运行浪费计算资源，诊断时间分散开发者精力，需要专业团队干预。现有机器学习方法仅检测间歇性故障，未解决后续诊断挑战。

Method: 提出FlaXifyer少样本学习方法，利用预训练语言模型基于作业执行日志预测间歇性作业失败类别；同时提出LogSift可解释性技术，快速识别关键日志语句，减少人工审查工作量。

Result: FlaXifyer仅需每类别12个标注样本，达到84.3%宏平均F1分数和92.0%Top-2准确率；LogSift在1秒内识别关键日志语句，减少74.4%审查工作量，在87%情况下能发现相关故障信息。

Conclusion: FlaXifyer和LogSift实现了有效的自动化故障分类，加速了故障诊断过程，为自动化解决间歇性作业失败问题奠定了基础，在TELUS的2,458个作业失败案例中验证了有效性。

Abstract: In principle, Continuous Integration (CI) pipeline failures provide valuable feedback to developers on code-related errors. In practice, however, pipeline jobs often fail intermittently due to non-deterministic tests, network outages, infrastructure failures, resource exhaustion, and other reliability issues. These intermittent (flaky) job failures lead to substantial inefficiencies: wasted computational resources from repeated reruns and significant diagnosis time that distracts developers from core activities and often requires intervention from specialized teams. Prior work has proposed machine learning techniques to detect intermittent failures, but does not address the subsequent diagnosis challenge. To fill this gap, we introduce FlaXifyer, a few-shot learning approach for predicting intermittent job failure categories using pre-trained language models. FlaXifyer requires only job execution logs and achieves 84.3% Macro F1 and 92.0% Top-2 accuracy with just 12 labeled examples per category. We also propose LogSift, an interpretability technique that identifies influential log statements in under one second, reducing review effort by 74.4% while surfacing relevant failure information in 87% of cases. Evaluation on 2,458 job failures from TELUS demonstrates that FlaXifyer and LogSift enable effective automated triage, accelerate failure diagnosis, and pave the way towards the automated resolution of intermittent job failures.

</details>


### [4] [PriviSense: A Frida-Based Framework for Multi-Sensor Spoofing on Android](https://arxiv.org/abs/2601.22414)
*Ibrahim Khalilov,Chaoran Chen,Ziang Xiao,Tianshi Li,Toby Jia-Jun Li,Yaxing Yao*

Main category: cs.SE

TL;DR: PriviSense是一个基于Frida的Android设备工具包，可在已root的设备上运行时伪造传感器和系统信号，支持可重复的上下文敏感应用测试。


<details>
  <summary>Details</summary>
Motivation: 移动应用越来越依赖实时传感器和系统数据来适应用户上下文，但模拟器和插桩构建通常无法在物理设备上支持可重复的上下文敏感应用行为测试。

Method: 基于Frida开发，在已root的Android设备上运行时伪造传感器和系统信号，可脚本化注入时变传感器流（加速度计、陀螺仪、步数计数器）和系统值（电池电量、系统时间、设备元数据）。

Result: 在已root的Android设备上对五个代表性传感器可视化应用进行了实时伪造验证，支持脚本化和可逆的值操作，便于应用逻辑测试、上下文行为发现和隐私分析。

Conclusion: PriviSense无需模拟器或应用重写即可实现可重复的设备上实验，为测试应用逻辑、发现基于上下文的行为和隐私分析提供了工具。为确保道德使用，代码仅与验证过的研究人员共享。

Abstract: Mobile apps increasingly rely on real-time sensor and system data to adapt their behavior to user context. While emulators and instrumented builds offer partial solutions, they often fail to support reproducible testing of context-sensitive app behavior on physical devices. We present PriviSense, a Frida-based, on-device toolkit for runtime spoofing of sensor and system signals on rooted Android devices. PriviSense can script and inject time-varying sensor streams (accelerometer, gyroscope, step counter) and system values (battery level, system time, device metadata) into unmodified apps, enabling reproducible on-device experiments without emulators or app rewrites. Our demo validates real-time spoofing on a rooted Android device across five representative sensor-visualization apps. By supporting scriptable and reversible manipulation of these values, PriviSense facilitates testing of app logic, uncovering of context-based behaviors, and privacy-focused analysis. To ensure ethical use, the code is shared upon request with verified researchers.
  Tool Guide: How to Run PriviSense on Rooted Android https://bit.ly/privisense-guide Demonstration video: https://www.youtube.com/watch?v=4Qwnogcc3pw

</details>


### [5] [Small is Beautiful: A Practical and Efficient Log Parsing Framework](https://arxiv.org/abs/2601.22590)
*Minxing Wang,Yintong Huo*

Main category: cs.SE

TL;DR: EFParser是一个基于LLM的无监督日志解析器，通过双缓存系统和校正模块提升小模型性能，在保持计算效率的同时显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的语义日志解析器虽然泛化能力强，但严重依赖模型规模，导致使用小模型时性能大幅下降。这在数据隐私要求和计算资源受限的实际部署场景中成为主要障碍。

Method: 提出EFParser，采用双缓存系统（自适应更新机制区分新模板和现有模板变体）和专用校正模块（验证和优化LLM生成的模板），在保持缓存一致性的同时防止错误注入。

Result: 在公开大规模数据集上的实验表明，EFParser在小模型上运行时，在所有指标上平均优于最先进基线12.5%，甚至超过某些使用大模型的基线方法，同时保持高计算效率。

Conclusion: EFParser通过系统架构创新，有效提升了小模型在日志解析任务上的性能，为实际部署提供了鲁棒且实用的解决方案，弥合了语义解析器性能与资源需求之间的差距。

Abstract: Log parsing is a fundamental step in log analysis, partitioning raw logs into constant templates and dynamic variables. While recent semantic-based parsers leveraging Large Language Models (LLMs) exhibit superior generalizability over traditional syntax-based methods, their effectiveness is heavily contingent on model scale. This dependency leads to significant performance collapse when employing smaller, more resource-efficient LLMs. Such degradation creates a major barrier to real-world adoption, where data privacy requirements and computational constraints necessitate the use of succinct models. To bridge this gap, we propose EFParser, an unsupervised LLM-based log parser designed to enhance the capabilities of smaller models through systematic architectural innovation. EFParser introduces a dual-cache system with an adaptive updating mechanism that distinguishes between novel patterns and variations of existing templates. This allows the parser to merge redundant templates and rectify prior errors, maintaining cache consistency. Furthermore, a dedicated correction module acts as a gatekeeper, validating and refining every LLM-generated template before caching to prevent error injection. Empirical evaluations on public large-scale datasets demonstrate that EFParser outperforms state-of-the-art baselines by an average of 12.5% across all metrics when running on smaller LLMs, even surpassing some baselines utilizing large-scale models. Despite its additional validation steps, EFParser maintains high computational efficiency, offering a robust and practical solution for real-world log analysis deployment.

</details>


### [6] [TimeMachine-bench: A Benchmark for Evaluating Model Capabilities in Repository-Level Migration Tasks](https://arxiv.org/abs/2601.22597)
*Ryo Fujii,Makoto Morishita,Kazuki Yano,Jun Suzuki*

Main category: cs.SE

TL;DR: TimeMachine-bench是一个用于评估Python项目软件迁移任务的基准测试，包含因依赖更新导致测试失败的GitHub仓库，并评估了11个LLM模型在迁移任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着自动化软件工程的发展，研究重点正转向反映软件工程师日常工作的实际任务。软件迁移作为适应环境变化的关键过程，在研究中被忽视，需要建立评估基准。

Method: 构建了TimeMachine-bench基准，包含因依赖更新导致测试失败的GitHub仓库，采用全自动化构建流程支持实时更新，并创建了人工验证的子集确保问题可解性。在验证子集上评估了基于11个模型（包括开源和SOTA LLM）的智能体基线。

Result: LLM在迁移任务上显示出一定潜力，但仍面临重大可靠性挑战：包括利用低测试覆盖率产生虚假解决方案，以及由于工具使用策略不佳导致的不必要编辑。

Conclusion: TimeMachine-bench为软件迁移任务提供了实用的评估基准，揭示了LLM在该任务上的当前局限性和改进方向，相关数据集和实现已开源。

Abstract: With the advancement of automated software engineering, research focus is increasingly shifting toward practical tasks reflecting the day-to-day work of software engineers. Among these tasks, software migration, a critical process of adapting code to evolving environments, has been largely overlooked. In this study, we introduce TimeMachine-bench, a benchmark designed to evaluate software migration in real-world Python projects. Our benchmark consists of GitHub repositories whose tests begin to fail in response to dependency updates. The construction process is fully automated, enabling live updates of the benchmark. Furthermore, we curated a human-verified subset to ensure problem solvability. We evaluated agent-based baselines built on top of 11 models, including both strong open-weight and state-of-the-art LLMs on this verified subset. Our results indicated that, while LLMs show some promise for migration tasks, they continue to face substantial reliability challenges, including spurious solutions that exploit low test coverage and unnecessary edits stemming from suboptimal tool-use strategies. Our dataset and implementation are available at https://github.com/tohoku-nlp/timemachine-bench.

</details>


### [7] [Elderly HealthMag: Systematic Building and Calibrating a Tool for Identifying and Evaluating Senior User Digital Health Software](https://arxiv.org/abs/2601.22627)
*Yuqing Xiao,John Grundy,Anuradha Madugalla,Elizabeth Manias*

Main category: cs.SE

TL;DR: 提出HealthMag工具，用于数字健康软件的需求建模和评估，特别关注老年用户的包容性需求


<details>
  <summary>Details</summary>
Motivation: 数字健康软件常基于对用户健康状况的隐含错误假设开发，导致产品无法满足特定年龄和健康状况用户的实际需求，缺乏包容性

Method: 基于InclusiveMag框架，通过系统映射和校准开发HealthMag工具，并与AgeMag方法整合形成Elderly HealthMag双重方法，通过认知走查演示应用

Result: 开发了HealthMag和Elderly HealthMag工具，能够识别当前面向老年用户的数字健康应用中的包容性偏见

Conclusion: 提出的工具能帮助数字健康软件开发团队更好地获取、建模和评估用户需求，提高软件对特定健康状况和年龄用户的包容性

Abstract: Digital health (DH) software is increasingly deployed to populations where many end users live with one or more health conditions. Yet, DH software development teams frequently operate using implicit, incorrect assumptions about these users, resulting in products that under-serve the specific requirements imposed by their age and health conditions. Consequently, while software may meet clinical objectives on paper, it often fails to be inclusive during actual user interaction. To address this, we propose \textbf{\textit{HealthMag}}, a tool inspired by GenderMag designed to help better elicit, model and evaluate requirements for digital health software. We developed HealthMag through systematic mapping and calibration following the InclusiveMag framework. Furthermore, we integrated this with a calibrated version of an existing AgeMag method to create a dual-lens approach: \textbf{\textit{Elderly HealthMag}}, designed to aid requirements, design and evaluation of mHealth software for senior end users. We demonstrate application and utility of Age HealthMag via cognitive walkthroughs in identifying inclusivity biases in current senior user-oriented digital health applications.

</details>


### [8] [From Horizontal Layering to Vertical Integration: A Comparative Study of the AI-Driven Software Development Paradigm](https://arxiv.org/abs/2601.22667)
*Chi Zhang,Zehan Li,Ziqian Zhong,Haibing Ma,Dan Xiao,Chen Lin,Ming Dong*

Main category: cs.SE

TL;DR: 生成式AI在软件工程中的应用导致组织从水平分层转向垂直整合，带来8-33倍的资源消耗减少，主要得益于超级员工的出现和跨职能协调开销的消除。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在软件工程中的组织影响，通过对比传统企业（棕地）和AI原生初创公司（绿地）两种开发环境，探索AI如何改变工程组织结构。

Method: 采用多案例比较研究，对比分析传统企业和AI原生初创公司的开发环境，使用总要素生产率分析来识别AI扭曲效应。

Result: 从水平分层转向垂直整合带来8-33倍的资源消耗减少；发现超级员工现象（AI增强工程师跨越传统角色边界）；识别出AI扭曲效应，减少劳动力规模回报同时放大技术杠杆。

Conclusion: 提出人机协作效能应成为工程组织的主要优化目标，取代个人生产力指标；为组织重新设计提供管理策略，包括重新激活高级工程师的闲置认知带宽和抑制盲目规模扩张。

Abstract: This paper examines the organizational implications of Generative AI adoption in software engineering through a multiple-case comparative study. We contrast two development environments: a traditional enterprise (brownfield) and an AI-native startup (greenfield). Our analysis reveals that transitioning from Horizontal Layering (functional specialization) to Vertical Integration (end-to-end ownership) yields 8-fold to 33-fold reductions in resource consumption. We attribute these gains to the emergence of Super Employees, AI-augmented engineers who span traditional role boundaries, and the elimination of inter-functional coordination overhead. Theoretically, we propose Human-AI Collaboration Efficacy as the primary optimization target for engineering organizations, supplanting individual productivity metrics. Our Total Factor Productivity analysis identifies an AI Distortion Effect that diminishes returns to labor scale while amplifying technological leverage. We conclude with managerial strategies for organizational redesign, including the reactivation of idle cognitive bandwidth in senior engineers and the suppression of blind scale expansion.

</details>


### [9] [VarParser: Unleashing the Neglected Power of Variables for LLM-based Log Parsing](https://arxiv.org/abs/2601.22676)
*Jinrui Sun,Tong Jia,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: VarParser提出了一种以变量为中心的日志解析策略，通过利用日志中的变量部分提升解析准确性和效率，相比现有仅关注常量部分的方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的日志解析器都只关注日志的常量部分，忽略了变量部分对日志解析的潜在贡献。这种常量中心策略带来了四个关键问题：1）仅使用常量信息的低效日志分组和采样；2）基于常量的缓存导致大量LLM调用，降低解析准确性和效率；3）提示中消耗大量常量token导致高调用成本；4）结果中只保留占位符，丢失了日志中变量信息带来的系统可见性。

Method: 提出VarParser变量中心日志解析策略，包含三个核心技术：1）变量贡献采样 - 高效捕获日志的变量部分；2）变量中心解析缓存 - 减少LLM调用次数；3）自适应变量感知上下文学习。通过引入变量单元，保留丰富的变量信息，增强日志解析结果的完整性。

Result: 在大规模数据集上的广泛评估表明，VarParser相比现有方法实现了更高的准确性，显著提高了解析效率，同时降低了LLM调用成本。

Conclusion: VarParser通过利用日志中的变量部分，解决了现有LLM-based日志解析器的局限性，在准确性、效率和成本方面都有显著提升，为下游任务如异常检测和故障诊断提供了更完整的日志解析结果。

Abstract: Logs serve as a primary source of information for engineers to diagnose failures in large-scale online service systems. Log parsing, which extracts structured events from massive unstructured log data, is a critical first step for downstream tasks like anomaly detection and failure diagnosis. With advances in large language models (LLMs), leveraging their strong text understanding capabilities has proven effective for accurate log parsing. However, existing LLM-based log parsers all focus on the constant part of logs, ignoring the potential contribution of the variable part to log parsing. This constant-centric strategy brings four key problems. First, inefficient log grouping and sampling with only constant information. Second, a relatively large number of LLM invocations due to constant-based cache, leading to low log parsing accuracy and efficiency. Third, a relatively large number of consumed constant tokens in prompts leads to high LLM invocation costs. At last, these methods only retain placeholders in the results, losing the system visibility brought by variable information in logs.
  Facing these problems, we propose a variable-centric log parsing strategy named VarParser. Through variable contribution sampling, variable-centric parsing cache, and adaptive variable-aware in-context learning, our approach can efficiently capture the variable parts of logs and leverage their contributions to parsing. By introducing variable units, we preserve rich variable information, enhancing the integrity of log parsing results. Extensive evaluations on large-scale datasets demonstrate that VarParser achieves higher accuracy compared to existing methods, significantly improving parsing efficiency while reducing the LLM invocation costs.

</details>


### [10] [AutoMerge: Search-Based Model Merging Framework for Effective Model Reuse](https://arxiv.org/abs/2601.22748)
*You Lu,Jiyang Zhang,Bihuan Chen,Chaofeng Sha,Dingji Wang,Xin Peng*

Main category: cs.SE

TL;DR: 论文首次系统研究了模型合并技术在不同架构和领域的适用性，发现现有方法效果不一致且对超参数敏感，提出了AutoMerge框架来优化合并过程。


<details>
  <summary>Details</summary>
Motivation: 软件复用是软件工程中的重要话题，模型合并作为免训练方法在LLMs中取得成功，但缺乏对其他深度学习模型架构和领域的系统性研究。

Method: 系统评估了5种模型合并技术在3种不同架构（LLMs、图像分类、自动驾驶）上的表现，发现现有方法效果不佳，提出了AutoMerge框架：先将复杂模型分割为异构块，然后系统探索合并空间寻找最优合并技术和超参数配置。

Result: 现有模型合并技术在不同架构和领域效果高度不一致，远不如在LLMs中的成功；单个合并技术难以处理模型内的异构结构特性；合并效果对超参数配置高度敏感。

Conclusion: 模型合并技术不能直接跨架构和领域应用，需要更系统的方法。AutoMerge框架通过分割和搜索策略能够有效解决现有方法的局限性，提升模型合并的适用性和效果。

Abstract: Software reuse has long been recognized as a critical and widely studied topic in software engineering, offering substantial benefits in reducing development costs, improving software quality, and enhancing operational efficiency. This paradigm extends into deep learning through model reuse. Recently, model merging has emerged in the domain of large language models (LLMs) as a training-free approach that takes multiple task-specific models with the same architecture as source models and merges them without retraining, enhancing model reuse within LLMs. However, no prior work has systematically investigated whether such an approach can be effectively applied to other deep learning models with different architectures across domains. To bridge this gap, we present the first systematic study that evaluates five model merging techniques on three distinct model architectures across three domains: LLMs, image classification, and autonomous driving. Our findings reveal that directly applying existing model merging techniques leads to highly inconsistent results and falls notably short of their success within LLMs. Moreover, a single model merging technique often fails to handle the heterogeneous structural properties within a model, limiting its applicability to different model architectures across domains. Furthermore, the effectiveness of model merging techniques is highly sensitive to hyperparameter configurations, thereby constraining their potential for broader adoption. Inspired by these insights, we propose AutoMerge, a novel search-based model merging framework that first segments complex models into multiple heterogeneous blocks and then systematically explores the merging space to identify the merging technique and its hyperparameter configuration.

</details>


### [11] [Constructing Safety Cases for AI Systems: A Reusable Template Framework](https://arxiv.org/abs/2601.22773)
*Sung Une Lee,Liming Zhu,Md Shamsujjoha,Liming Dong,Qinghua Lu,Jieshan Chen*

Main category: cs.SE

TL;DR: 论文提出针对AI系统的可复用安全案例模板框架，解决传统安全案例方法不适用于生成式和智能体AI动态特性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统安全案例方法（来自航空、核工程）依赖明确的系统边界、稳定架构和已知故障模式，而现代AI系统（生成式、智能体AI）具有能力不可预测、行为随提示变化、风险随微调/部署环境动态变化等特点，需要新的安全案例方法。

Method: 提出可复用安全案例模板框架，包含AI特定的声明分类（基于断言、约束、能力）、论证类型（演示性、比较性、因果/解释性、基于风险、规范性）和证据家族（经验性、机制性、比较性、专家驱动、形式化方法、操作/现场数据、基于模型）。每个模板采用预定义结构（声明、论证、证据），针对特定挑战（如无真实标签评估、动态模型更新、基于阈值的风险决策）提供端到端模式。

Result: 建立了一个系统化、可组合、可复用的安全案例构建和维护方法，使安全案例对生成式和前沿AI系统具有可信性、可审计性和适应性。

Conclusion: 该框架为AI系统安全案例提供了结构化方法，能够应对AI系统的动态特性和新兴风险，支持可信、可审计且适应AI行为演化的安全论证。

Abstract: Safety cases, structured arguments that a system is acceptably safe, are becoming central to the governance of AI systems. Yet, traditional safety-case practices from aviation or nuclear engineering rely on well-specified system boundaries, stable architectures, and known failure modes. Modern AI systems such as generative and agentic AI are the opposite. Their capabilities emerge unpredictably from low-level training objectives, their behaviour varies with prompts, and their risk profiles shift through fine-tuning, scaffolding, or deployment context. This study examines how safety cases are currently constructed for AI systems and why classical approaches fail to capture these dynamics. It then proposes a framework of reusable safety-case templates, each following a predefined structure of claims, arguments, and evidence tailored for AI systems. The framework introduces comprehensive taxonomies for AI-specific claim types (assertion-based, constrained-based, capability-based), argument types (demonstrative, comparative, causal/explanatory, risk-based, and normative), and evidence families (empirical, mechanistic, comparative, expert-driven, formal methods, operational/field data, and model-based). Each template is illustrated through end-to-end patterns addressing distinctive challenges such as evaluation without ground truth, dynamic model updates, and threshold-based risk decisions. The result is a systematic, composable, and reusable approach to constructing and maintaining safety cases that are credible, auditable, and adaptive to the evolving behaviour of generative and frontier AI systems.

</details>


### [12] [Understanding on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/abs/2601.22791)
*Sabinakhon Akbarova,Felix Dobslaw,Robert Feldt*

Main category: cs.SE

TL;DR: 研究评估了GPT-4.1生成的边界值分析解释的质量和实用性，通过27名软件专业人员对20个边界对的解释进行评分，发现63.5%的评分是积极的，但参与者期望更结构化、有权威来源支持且包含可操作示例的解释。


<details>
  <summary>Details</summary>
Motivation: 边界值分析在软件质量保证中很重要，但测试人员常常难以理解和证明某些输入-输出对为何代表有意义的行为边界。虽然大语言模型可以生成自然语言解释，但其在边界值分析中的价值尚未经过实证评估。

Method: 进行探索性研究：让27名软件专业人员对GPT-4.1为20个边界对生成的解释进行评分（清晰度、正确性、完整性和感知有用性），并对其中6人进行后续访谈。

Result: 63.5%的评分是积极的（5分制中的4-5分），17%是消极的（1-2分）。参与者偏好结构清晰、引用权威来源、根据读者专业知识调整深度的解释，并强调需要支持调试和文档的可操作示例。

Conclusion: 研究提炼了一个包含7项要求的检查清单，为未来基于LLM的边界解释工具定义了具体设计标准。结果表明，经过进一步改进，基于LLM的工具可以通过使边界解释更具可操作性和可信度来支持测试工作流程。

Abstract: Boundary value analysis and testing (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes, yet testers often struggle to understand and justify why certain input-output pairs represent meaningful behavioral boundaries. Large Language Models (LLMs) could help by producing natural-language rationales, but their value for BVT has not been empirically assessed. We therefore conducted an exploratory study on LLM-generated boundary explanations: in a survey, twenty-seven software professionals rated GPT-4.1 explanations for twenty boundary pairs on clarity, correctness, completeness and perceived usefulness, and six of them elaborated in follow-up interviews. Overall, 63.5% of all ratings were positive (4-5 on a five-point Likert scale) compared to 17% negative (1-2), indicating general agreement but also variability in perceptions. Participants favored explanations that followed a clear structure, cited authoritative sources, and adapted their depth to the reader's expertise; they also stressed the need for actionable examples to support debugging and documentation. From these insights, we distilled a seven-item requirement checklist that defines concrete design criteria for future LLM-based boundary explanation tools. The results suggest that, with further refinement, LLM-based tools can support testing workflows by making boundary explanations more actionable and trustworthy.

</details>


### [13] [Just-in-Time Catching Test Generation at Meta](https://arxiv.org/abs/2601.22832)
*Matthew Becker,Yifei Chen,Nicholas Cochran,Pouyan Ghasemi,Abhishek Gulati,Mark Harman,Zachary Haluza,Mehrdad Honarkhah,Herve Robert,Jiacheng Liu,Weini Liu,Sreeja Thummala,Xiaoning Yang,Rui Xin,Sophie Zeng*

Main category: cs.SE

TL;DR: Meta开发了即时捕获测试生成系统，旨在防止大规模后端系统中的bug，通过生成预期会失败的测试来在代码合并前发现问题，显著减少了假阳性并成功捕获了严重bug。


<details>
  <summary>Details</summary>
Motivation: 传统强化测试在生成时通过，无法有效发现bug。Meta需要一种方法在大规模后端系统（数亿行代码）中预防bug，特别是要在代码合并前发现问题，而不是等到生产环境。

Method: 采用即时捕获测试生成方法，生成预期会失败的测试（捕获测试）。使用代码变更感知方法改进候选捕获生成，结合基于规则和LLM的评估器来减少假阳性，并通过统计分析和人工验证来评估效果。

Result: 代码变更感知方法将候选捕获生成提高了4倍（相比强化测试）和20倍（相比偶然失败的测试）。评估器减少了70%的人工审查工作量。在报告的41个候选捕获中，8个被确认为真阳性，其中4个如果未被捕获将导致严重故障。

Conclusion: 即时捕获测试生成是规模化、工业可用的方法，能有效防止严重故障进入生产环境，通过提前发现bug显著提高了代码质量。

Abstract: We report on Just-in-Time catching test generation at Meta, designed to prevent bugs in large scale backend systems of hundreds of millions of line of code. Unlike traditional hardening tests, which pass at generation time, catching tests are meant to fail, surfacing bugs before code lands. The primary challenge is to reduce development drag from false positive test failures. Analyzing 22,126 generated tests, we show code-change-aware methods improve candidate catch generation 4x over hardening tests and 20x over coincidentally failing tests. To address false positives, we use rule-based and LLM-based assessors. These assessors reduce human review load by 70%. Inferential statistical analysis showed that human-accepted code changes are assessed to have significantly more false positives, while human-rejected changes have significantly more true positives. We reported 41 candidate catches to engineers; 8 were confirmed to be true positives, 4 of which would have led to serious failures had they remained uncaught. Overall, our results show that Just-in-Time catching is scalable, industrially applicable, and that it prevents serious failures from reaching production.

</details>


### [14] [MEnvAgent: Scalable Polyglot Environment Construction for Verifiable Software Engineering](https://arxiv.org/abs/2601.22859)
*Chuanzhe Guo,Jingjing Wu,Sijun He,Yang Chen,Zhaoqi Kuang,Shilong Fan,Bingjin Chen,Siqi Bao,Jing Liu,Hua Wu,Qingfu Zhu,Wanxiang Che,Haifeng Wang*

Main category: cs.SE

TL;DR: MEnvAgent是一个多语言框架，用于自动化构建可执行环境，解决LLM代理在软件工程中缺乏可验证数据集的问题。该框架通过多智能体架构和增量环境复用机制，显著提升了环境构建的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程领域的应用受到可验证数据集稀缺的限制，主要原因是跨多种编程语言构建可执行环境的复杂性。现有方法难以规模化生成可验证的任务实例。

Method: MEnvAgent采用多智能体规划-执行-验证架构，自主解决环境构建失败问题，并集成了创新的环境复用机制，通过增量修补历史环境来减少计算开销。

Result: 在包含10种语言、1000个任务的MEnvBench基准测试中，MEnvAgent优于基线方法，将失败转成功(F2P)率提高了8.6%，同时减少了43%的时间成本。还构建了目前最大的开源多语言可验证Docker环境数据集MEnvData-SWE。

Conclusion: MEnvAgent为LLM代理在软件工程领域的发展提供了可扩展的环境构建解决方案，显著提升了任务实例生成的可验证性和效率，并为该领域贡献了宝贵的数据资源。

Abstract: The evolution of Large Language Model (LLM) agents for software engineering (SWE) is constrained by the scarcity of verifiable datasets, a bottleneck stemming from the complexity of constructing executable environments across diverse languages. To address this, we introduce MEnvAgent, a Multi-language framework for automated Environment construction that facilitates scalable generation of verifiable task instances. MEnvAgent employs a multi-agent Planning-Execution-Verification architecture to autonomously resolve construction failures and integrates a novel Environment Reuse Mechanism that reduces computational overhead by incrementally patching historical environments. Evaluations on MEnvBench, a new benchmark comprising 1,000 tasks across 10 languages, demonstrate that MEnvAgent outperforms baselines, improving Fail-to-Pass (F2P) rates by 8.6% while reducing time costs by 43%. Additionally, we demonstrate the utility of MEnvAgent by constructing MEnvData-SWE, the largest open-source polyglot dataset of realistic verifiable Docker environments to date, alongside solution trajectories that enable consistent performance gains on SWE tasks across a wide range of models. Our code, benchmark, and dataset are available at https://github.com/ernie-research/MEnvAgent.

</details>


### [15] [AnoMod: A Dataset for Anomaly Detection and Root Cause Analysis in Microservice Systems](https://arxiv.org/abs/2601.22881)
*Ke Ping,Hamza Bin Mazhar,Yuqing Wang,Ying Song,Mika V. Mäntylä*

Main category: cs.SE

TL;DR: 提出了AnoMod数据集，一个用于微服务系统异常检测和根因分析的多模态基准数据集，包含四种异常类型和五种监控模态。


<details>
  <summary>Details</summary>
Motivation: 当前微服务系统缺乏高质量、公开可用的异常检测和根因分析数据集，现有基准主要关注性能相关故障且模态单一，限制了跨模态方法和更广泛故障模式的研究。

Method: 基于两个开源微服务系统（SocialNetwork和TrainTicket），设计并注入了四类异常（性能级、服务级、数据库级、代码级），收集了五种模态数据（日志、指标、分布式追踪、API响应、代码覆盖率报告）。

Result: 创建了AnoMod数据集，提供了丰富的端到端系统状态视图，支持跨模态异常检测、融合/消融策略评估，以及跨服务和代码区域的细粒度根因分析。

Conclusion: AnoMod数据集填补了微服务系统异常检测和根因分析研究的数据空白，支持联合考虑检测和定位的端到端故障排除流程研究。

Abstract: Microservice systems (MSS) have become a predominant architectural style for cloud services. Yet the community still lacks high-quality, publicly available datasets for anomaly detection (AD) and root cause analysis (RCA) in MSS. Most benchmarks emphasize performance-related faults and provide only one or two monitoring modalities, limiting research on broader failure modes and cross-modal methods. To address these gaps, we introduce a new multimodal anomaly dataset built on two open-source microservice systems: SocialNetwork and TrainTicket. We design and inject four categories of anomalies (Ano): performance-level, service-level, database-level, and code-level, to emulate realistic anomaly modes. For each scenario, we collect five modalities (Mod): logs, metrics, distributed traces, API responses, and code coverage reports, offering a richer, end-to-end view of system state and inter-service interactions. We name our dataset, reflecting its unique properties, as AnoMod. This dataset enables (1) evaluation of cross-modal anomaly detection and fusion/ablation strategies, and (2) fine-grained RCA studies across service and code regions, supporting end-to-end troubleshooting pipelines that jointly consider detection and localization.

</details>


### [16] [A Serverless Edge-Native Data Processing Architecture for Autonomous Driving Training](https://arxiv.org/abs/2601.22919)
*Fabian Bally,Michael Schötz,Thomas Limbrunner*

Main category: cs.SE

TL;DR: Lambda框架是一个边缘原生平台，通过用户定义函数实现车载数据过滤和处理，将FaaS原则适配到资源受限的汽车环境，支持实时数据处理。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶机器学习面临数据瓶颈，需要大量传感器数据且需覆盖罕见但安全关键的场景，传统方法需要大量驾驶时间和高效选择机制。

Method: 引入Lambda框架，提供服务器无感知抽象层，分离应用逻辑与底层执行关注点，适配FaaS原则到资源受限汽车环境，支持模块化事件驱动过滤算法，兼容ROS 2和现有数据记录管道。

Result: 在NVIDIA Jetson Orin Nano上评估，相比原生ROS 2部署，表现出竞争性性能、降低的延迟和抖动，证实lambda抽象能支持嵌入式自动驾驶系统的实时数据处理。

Conclusion: Lambda框架成功将FaaS原则应用于资源受限的汽车环境，支持实时数据过滤和处理，为自动驾驶数据采集提供了高效解决方案。

Abstract: Data is both the key enabler and a major bottleneck for machine learning in autonomous driving. Effective model training requires not only large quantities of sensor data but also balanced coverage that includes rare yet safety-critical scenarios. Capturing such events demands extensive driving time and efficient selection. This paper introduces the Lambda framework, an edge-native platform that enables on-vehicle data filtering and processing through user-defined functions. The framework provides a serverless-inspired abstraction layer that separates application logic from low-level execution concerns such as scheduling, deployment, and isolation. By adapting Function-as-a-Service (FaaS) principles to resource-constrained automotive environments, it allows developers to implement modular, event-driven filtering algorithms while maintaining compatibility with ROS 2 and existing data recording pipelines. We evaluate the framework on an NVIDIA Jetson Orin Nano and compare it against native ROS 2 deployments. Results show competitive performance, reduced latency and jitter, and confirm that lambda-based abstractions can support real-time data processing in embedded autonomous driving systems. The source code is available at https://github.com/LASFAS/jblambda.

</details>


### [17] [Sifting the Noise: A Comparative Study of LLM Agents in Vulnerability False Positive Filtering](https://arxiv.org/abs/2601.22952)
*Yunpeng Xiong,Ting Zhang*

Main category: cs.SE

TL;DR: LLM智能体可显著降低SAST工具误报率，但效果受模型能力、漏洞类型和架构设计影响，存在计算成本与漏报的权衡。


<details>
  <summary>Details</summary>
Motivation: SAST工具产生大量误报给开发者带来沉重负担，LLM智能体通过迭代推理和环境交互有望优化误报过滤，但不同智能体架构的比较效果尚不明确。

Method: 比较三种最先进的LLM智能体框架（Aider、OpenHands、SWE-agent）在漏洞误报过滤中的表现，使用OWASP Benchmark和真实开源Java项目进行评估。

Result: LLM智能体可将OWASP Benchmark的误报率从92%降至6.3%，在真实项目中CodeQL警报的误报识别率可达93.3%。但效果强烈依赖骨干模型和CWE类型，强模型（Claude Sonnet 4、GPT-5）上智能体显著优于普通提示，弱模型上增益有限或不一致。激进的误报减少可能抑制真实漏洞，且不同框架计算成本差异大。

Conclusion: LLM智能体是SAST误报过滤的强大但非均匀解决方案，实际部署需综合考虑智能体设计、骨干模型选择、漏洞类别和运营成本。

Abstract: Static Application Security Testing (SAST) tools are essential for identifying software vulnerabilities, but they often produce a high volume of false positives (FPs), imposing a substantial manual triage burden on developers. Recent advances in Large Language Model (LLM) agents offer a promising direction by enabling iterative reasoning, tool use, and environment interaction to refine SAST alerts. However, the comparative effectiveness of different LLM-based agent architectures for FP filtering remains poorly understood. In this paper, we present a comparative study of three state-of-the-art LLM-based agent frameworks, i.e., Aider, OpenHands, and SWE-agent, for vulnerability FP filtering. We evaluate these frameworks using the vulnerabilities from the OWASP Benchmark and real-world open-source Java projects. The experimental results show that LLM-based agents can remove the majority of SAST noise, reducing an initial FP detection rate of over 92% on the OWASP Benchmark to as low as 6.3% in the best configuration. On real-world dataset, the best configuration of LLM-based agents can achieve an FP identification rate of up to 93.3% involving CodeQL alerts. However, the benefits of agents are strongly backbone- and CWE-dependent: agentic frameworks significantly outperform vanilla prompting for stronger models such as Claude Sonnet 4 and GPT-5, but yield limited or inconsistent gains for weaker backbones. Moreover, aggressive FP reduction can come at the cost of suppressing true vulnerabilities, highlighting important trade-offs. Finally, we observe large disparities in computational cost across agent frameworks. Overall, our study demonstrates that LLM-based agents are a powerful but non-uniform solution for SAST FP filtering, and that their practical deployment requires careful consideration of agent design, backbone model choice, vulnerability category, and operational cost.

</details>


### [18] [SWE-Manager: Selecting and Synthesizing Golden Proposals Before Coding](https://arxiv.org/abs/2601.22956)
*Boyin Tan,Haoning Deng,Junyuan Zhang,Junjielong Xu,Pinjia He,Youcheng Sun*

Main category: cs.SE

TL;DR: SWE-Manager：一个8B参数模型，通过强化学习训练，用于在多个代码修复提案中选择最佳方案并合成最终实施方案，在SWE-Lancer Manager基准测试中表现优于GPT-5等基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在软件工程中的研究主要关注代码生成和错误修复，但实际开发中团队需要从多个候选方案中选择最佳实施方案。这种选择需要考虑问题的范围、影响和紧迫性，以及每个方案的优缺点。好的选择可以提高问题解决的可靠性，降低回归和操作风险。

Method: 1. 首先对真实世界问题进行人工研究，分析维护者在选择竞争提案时的决策依据；2. 提出SWE-Manager，这是一个8B参数的模型，通过强化学习训练，能够比较提案、证明选择合理性并合成最终实施方案；3. 将提案选择视为推理任务，模拟技术经理在不执行代码或运行测试的情况下评估提案的过程；4. 设计P2A框架模拟真实工作流程。

Result: 在SWE-Lancer Manager基准测试中，SWE-Manager达到53.21%的选择准确率和57.75%的收益率，获得152,750美元收益，表现优于包括GPT-5在内的强基线模型。

Conclusion: SWE-Manager通过将提案选择建模为推理任务，有效模拟了技术经理的决策过程，在软件工程问题解决中展现出优于现有方法的性能，为实际开发工作流程提供了有价值的自动化工具。

Abstract: Large language model (LLM) research in software engineering has largely focused on tasks such as code generation and bug repair. In practice, teams often draft multiple candidate proposals for fixing an issue and then deliberate on one golden proposal for implementation. This selection requires not only assessing the issue's scope, impact, and urgency, but also a clear understanding of each proposal's strengths and weaknesses. A good selection could make issue resolution more reliable while reducing regression and operational risk, whereas a poor choice can increase risk and even cause unpredictable failures.
  We first conduct a manual study of real-world issues to characterize the rationales maintainers use when selecting among competing proposals. Motivated by these findings, we introduce SWE-Manager, a joint selection and synthesis approach that selects the best proposal and synthesizes a golden proposal. SWE-Manager is an 8B model trained via reinforcement learning (RL) to compare proposals, justify its choice, and synthesize a golden proposal for implementation. We view proposal selection as a reasoning task, mirroring how technical managers review competing proposals by weighing issue context and each proposal's solution without executing code or running tests. On the SWE-Lancer Manager benchmark, SWE-Manager achieves 53.21 selection accuracy and 57.75 earn rate, earning 152,750 dollars and outperforming strong baselines including GPT-5. To further evaluate the effectiveness of SWE-Manager in real-world issue resolution, we design the P2A framework, which simulates a real-world workflow where multiple proposals are drafted, reviewed, and a golden proposal is selected for implementation ...

</details>


### [19] [SolAgent: A Specialized Multi-Agent Framework for Solidity Code Generation](https://arxiv.org/abs/2601.23009)
*Wei Chen,Zhiyuan Peng,Xin Yin,Chao Ni,Chenhao Ying,Bang Xie,Yuan Luo*

Main category: cs.SE

TL;DR: SolAgent是一个基于多智能体框架的工具增强系统，专门用于生成安全可靠的智能合约代码，通过双循环优化机制显著提升代码质量和安全性。


<details>
  <summary>Details</summary>
Motivation: 智能合约是去中心化网络的核心，但其功能正确性和安全性保障仍是关键挑战。现有大型语言模型在生成智能合约代码时经常产生有缺陷或易受攻击的代码，无法满足严格的安全要求。

Method: 提出SolAgent框架，采用工具增强的多智能体架构，模拟人类专家工作流程。核心是双循环优化机制：内循环使用Forge编译器确保功能正确性，外循环利用Slither静态分析器消除安全漏洞。系统还具备文件系统能力以解决复杂项目依赖。

Result: 在SolEval+基准测试中，SolAgent达到64.39%的Pass@1率，显著优于最先进的LLM（约25%）、AI IDE（如GitHub Copilot）和现有智能体框架。相比人工编写的基线，安全漏洞减少达39.77%。

Conclusion: SolAgent能有效生成高质量、安全的智能合约代码，其生成的高质量轨迹可用于蒸馏更小的开源模型，促进安全智能合约生成的民主化。

Abstract: Smart contracts are the backbone of the decentralized web, yet ensuring their functional correctness and security remains a critical challenge. While Large Language Models (LLMs) have shown promise in code generation, they often struggle with the rigorous requirements of smart contracts, frequently producing code that is buggy or vulnerable. To address this, we propose SolAgent, a novel tool-augmented multi-agent framework that mimics the workflow of human experts. SolAgent integrates a \textbf{dual-loop refinement mechanism}: an inner loop using the \textit{Forge} compiler to ensure functional correctness, and an outer loop leveraging the \textit{Slither} static analyzer to eliminate security vulnerabilities. Additionally, the agent is equipped with file system capabilities to resolve complex project dependencies. Experiments on the SolEval+ Benchmark, a rigorous suite derived from high-quality real-world projects, demonstrate that SolAgent achieves a Pass@1 rate of up to \textbf{64.39\%}, significantly outperforming state-of-the-art LLMs ($\sim$25\%), AI IDEs (e.g., GitHub Copilot), and existing agent frameworks. Moreover, it reduces security vulnerabilities by up to \textbf{39.77\%} compared to human-written baselines. Finally, we demonstrate that the high-quality trajectories generated by SolAgent can be used to distill smaller, open-source models, democratizing access to secure smart contract generation. We release our data and code at https://github.com/openpaperz/SolAgent.

</details>


### [20] [Uncovering Hidden Inclusions of Vulnerable Dependencies in Real-World Java Projects](https://arxiv.org/abs/2601.23020)
*Stefan Schott,Serena Elisa Ponta,Wolfram Fischer,Jonas Klauke,Eric Bodden*

Main category: cs.SE

TL;DR: Unshade是一个混合依赖扫描工具，结合元数据扫描的高效性和代码中心方法的检测能力，能发现Java项目中修改和隐藏的依赖漏洞。


<details>
  <summary>Details</summary>
Motivation: 现代软件严重依赖开源组件，这虽然提高开发效率但引入安全风险。现有扫描器有局限性：元数据扫描器轻量快速但无法检测修改的依赖；代码中心扫描器能检测修改依赖但效率较低。

Method: Unshade采用混合方法：首先通过基于字节码的指纹识别机制识别修改和隐藏的依赖，增强项目的软件物料清单(SBOM)，然后将增强的SBOM传递给元数据漏洞扫描器，检测声明依赖和新发现依赖中的已知漏洞。

Result: 对GitHub上1,808个最流行的Java Maven项目进行大规模研究，发现近50%的项目包含至少一个与已知漏洞相关的修改、隐藏依赖。平均每个受影响项目包含超过8个这样的隐藏漏洞依赖，传统元数据扫描器会全部漏掉。Unshade共识别出7,712个隐藏依赖中的唯一CVE。

Conclusion: Unshade结合了两种扫描方法的优势，能有效检测传统元数据扫描器无法发现的隐藏依赖漏洞，显著提高了Java项目依赖安全扫描的覆盖率和准确性。

Abstract: Open-source software (OSS) dependencies are a dominant component of modern software code bases. Using proven and well-tested OSS components lets developers reduce development time and cost while improving quality. However, heavy reliance on open-source software also introduces significant security risks, including the incorporation of known vulnerabilities into the codebase. To mitigate these risks, metadata-based dependency scanners, which are lightweight and fast, and code-centric scanners, which enable the detection of modified dependencies hidden from metadata-based approaches, have been developed. In this paper, we present Unshade, a hybrid approach towards dependency scanning in Java that combines the efficiency of metadata-based scanning with the ability to detect modified dependencies of code-centric approaches. Unshade first augments a Java project's software bill of materials (SBOM) by identifying modified and hidden dependencies via a bytecode-based fingerprinting mechanism. This augmented SBOM is then passed to a metadata-based vulnerability scanner to identify known vulnerabilities in both declared and newly revealed dependencies. Leveraging Unshade's high scalability, we conducted a large-scale study of the 1,808 most popular open-source Java Maven projects on GitHub. The results show that nearly 50% of these projects contain at least one modified, hidden dependency associated with a known vulnerability. On average, each affected project includes more than eight such hidden vulnerable dependencies, all missed by traditional metadata-based scanners. Overall, Unshade identified 7,712 unique CVEs in hidden dependencies that would remain undetected when relying on metadata-based scanning alone.

</details>


### [21] [On the Impact of Code Comments for Automated Bug-Fixing: An Empirical Study](https://arxiv.org/abs/2601.23059)
*Antonio Vitale,Emanuela Guglielmi,Simone Scalabrino,Rocco Oliveto*

Main category: cs.SE

TL;DR: 研究探讨代码注释对LLM自动修复bug能力的影响，发现训练和推理阶段都包含注释可将修复准确率提升三倍，且实现细节类注释最有效。


<details>
  <summary>Details</summary>
Motivation: 当前自动bug修复研究中普遍存在去除代码注释的做法，但研究者假设注释可能包含有价值的设计和实现信息，对修复某些类型bug至关重要。需要重新审视去除注释这一常见预处理步骤。

Method: 1) 使用LLM为缺乏注释的数据集自动生成注释；2) 比较两个模型家族在训练和推理阶段所有组合条件下的表现（有/无注释）；3) 进行可解释性分析，识别最有效的注释类型。

Result: 1) 当注释同时存在于训练和推理阶段时，自动bug修复准确率最高可提升三倍；2) 使用带注释数据训练不会降低无注释实例的性能；3) 描述方法实现细节的注释对帮助LLM准确修复bug特别有效。

Conclusion: 代码注释对LLM的自动bug修复能力有显著积极影响，特别是在训练和推理阶段都保留注释时效果最佳。研究建议重新考虑去除注释的常见做法，并强调实现细节类注释的重要性。

Abstract: Large Language Models (LLMs) are increasingly relevant in Software Engineering research and practice, with Automated Bug Fixing (ABF) being one of their key applications. ABF involves transforming a buggy method into its fixed equivalent. A common preprocessing step in ABF involves removing comments from code prior to training. However, we hypothesize that comments may play a critical role in fixing certain types of bugs by providing valuable design and implementation insights. In this study, we investigate how the presence or absence of comments, both during training and at inference time, impacts the bug-fixing capabilities of LLMs. We conduct an empirical evaluation comparing two model families, each evaluated under all combinations of training and inference conditions (with and without comments), and thereby revisiting the common practice of removing comments during training. To address the limited availability of comments in state-of-the-art datasets, we use an LLM to automatically generate comments for methods lacking them. Our findings show that comments improve ABF accuracy by up to threefold when present in both phases, while training with comments does not degrade performance when instances lack them. Additionally, an interpretability analysis identifies that comments detailing method implementation are particularly effective in aiding LLMs to fix bugs accurately.

</details>


### [22] [Automated Testing of Prevalent 3D User Interactions in Virtual Reality Applications](https://arxiv.org/abs/2601.23139)
*Ruizhen Gu,José Miguel Rojas,Donghwan Shin*

Main category: cs.SE

TL;DR: 提出XRintTest自动化测试框架，解决VR应用3D用户交互测试难题，通过交互流图建模实现高效交互覆盖


<details>
  <summary>Details</summary>
Motivation: 现有VR测试方法缺乏自动生成真实3D用户输入（如手柄抓取和触发动作）的能力，且现有指标无法稳健捕捉多样交互覆盖，自动化测试生成和执行仍是未解决的挑战

Method: 1) 经验性识别四种常见VR交互类型；2) 提出交互流图抽象模型，系统建模3D用户交互；3) 构建XRBench3D基准测试集；4) 开发XRintTest自动化测试方法，利用交互流图进行动态场景探索和交互执行

Result: XRintTest在XRBench3D上达到93%的fire、manipulate和socket交互覆盖率，比随机探索效果高12倍、效率高6倍，能检测运行时异常和非异常交互问题，包括细微配置缺陷

Conclusion: 提出的交互流图和XRintTest方法有效解决了VR应用3D交互自动化测试的挑战，不仅能实现高覆盖率测试，还能揭示可能影响功能和测试性能的交互设计问题

Abstract: Virtual Reality (VR) technologies offer immersive user experiences across various domains, but present unique testing challenges compared to traditional software. Existing VR testing approaches enable scene navigation and interaction activation, but lack the ability to automatically synthesise realistic 3D user inputs (e.g, grab and trigger actions via hand-held controllers). Automated testing that generates and executes such input remains an unresolved challenge. Furthermore, existing metrics fail to robustly capture diverse interaction coverage. This paper addresses these gaps through four key contributions. First, we empirically identify four prevalent interaction types in nine open-source VR projects: fire, manipulate, socket, and custom. Second, we introduce the Interaction Flow Graph, a novel abstraction that systematically models 3D user interactions by identifying targets, actions, and conditions. Third, we construct XRBench3D, a benchmark comprising ten VR scenes that encompass 456 distinct user interactions for evaluating VR interaction testing. Finally, we present XRintTest, an automated testing approach that leverages this graph for dynamic scene exploration and interaction execution. Evaluation on XRBench3D shows that XRintTest achieves great effectiveness, reaching 93% coverage of fire, manipulate and socket interactions across all scenes, and performing 12x more effectively and 6x more efficiently than random exploration. Moreover, XRintTest can detect runtime exceptions and non-exception interaction issues, including subtle configuration defects. In addition, the Interaction Flow Graph can reveal potential interaction design smells that may compromise intended functionality and hinder testing performance for VR applications.

</details>


### [23] [From Monolith to Microservices: A Comparative Evaluation of Decomposition Frameworks](https://arxiv.org/abs/2601.23141)
*Mineth Weerasinghe,Himindu Kularathne,Methmini Madhushika,Danuka Lakshan,Nisansa de Silva,Adeesha Wijayasiri,Srinath Perera*

Main category: cs.SE

TL;DR: 该研究对微服务分解方法进行了统一评估，发现基于层次聚类的方法（特别是HDBScan）在不同基准系统中产生最一致的平衡分解结果。


<details>
  <summary>Details</summary>
Motivation: 软件现代化过程中，从单体架构迁移到微服务架构日益重要，但确定有效的服务边界仍然是一个复杂且未解决的挑战。现有的自动化微服务分解框架评估存在碎片化问题，包括基准系统不一致、指标不兼容和可复现性有限，阻碍了客观比较。

Method: 采用统一的比较评估方法，涵盖静态、动态和混合技术的最先进微服务分解方法。使用一致的指标计算管道，在广泛使用的基准系统（JPetStore、AcmeAir、DayTrader、Plants）上评估分解质量，使用结构模块化、接口数量、分区间通信、非极端分布等相关指标。结合先前研究报告的结果和通过可用复制包实验复现的输出进行分析。

Result: 研究发现，基于层次聚类的方法，特别是HDBScan，在不同基准系统中产生最一致的平衡分解结果，在实现强模块化的同时最小化通信和接口开销。

Conclusion: 该研究为微服务分解方法的评估提供了统一的框架，证明了层次聚类方法（尤其是HDBScan）在微服务分解中的优越性和一致性，为软件现代化中的架构迁移提供了有价值的指导。

Abstract: Software modernisation through the migration from monolithic architectures to microservices has become increasingly critical, yet identifying effective service boundaries remains a complex and unresolved challenge. Although numerous automated microservice decomposition frameworks have been proposed, their evaluation is often fragmented due to inconsistent benchmark systems, incompatible metrics, and limited reproducibility, thus hindering objective comparison. This work presents a unified comparative evaluation of state-of-the-art microservice decomposition approaches spanning static, dynamic, and hybrid techniques. Using a consistent metric computation pipeline, we assess the decomposition quality across widely used benchmark systems (JPetStore, AcmeAir, DayTrader, and Plants) using Structural Modularity (SM), Interface Number(IFN), Inter-partition Communication (ICP), Non-Extreme Distribution (NED), and related indicators. Our analysis combines results reported in prior studies with experimentally reproduced outputs from available replication packages. Findings indicate that the hierarchical clustering-based methods, particularly HDBScan, produce the most consistently balanced decompositions across benchmarks, achieving strong modularity while minimizing communication and interface overhead.

</details>


### [24] [Do Good, Stay Longer? Temporal Patterns and Predictors of Newcomer-to-Core Transitions in Conventional OSS and OSS4SG](https://arxiv.org/abs/2601.23142)
*Mohamed Ouf,Amr Mohamed,Mariam Guizani*

Main category: cs.SE

TL;DR: OSS4SG项目相比传统OSS项目，新人转为核心贡献者的保留率更高（2.2倍），核心地位达成概率更高（19.6%），且提供多种成长路径而非单一主导路径


<details>
  <summary>Details</summary>
Motivation: 开源软件可持续性依赖新人转为核心贡献者，但这一管道存在断裂问题。研究旨在探索以社会影响为使命的OSS4SG项目是否与传统OSS项目在新人转核心方面存在差异

Method: 比较375个项目（190个OSS4SG，185个传统OSS），分析92,721名贡献者和350万次提交，研究贡献者保留率、核心地位达成概率、成长路径模式和时间模式

Result: OSS4SG项目保留率是传统项目的2.2倍，核心地位达成概率高19.6%。早期广泛探索项目能预测核心地位达成（22.2%重要性）。传统OSS主要依赖单一主导路径（61.62%），而OSS4SG提供多种路径。Late Spike模式（先学习再集中贡献）比Early Spike模式（从一开始就高强度贡献）达成核心地位快2.4-2.9倍

Conclusion: 项目使命与新人转核心环境显著相关。关键策略包括：选择与个人价值观一致的项目，在主要贡献前花时间理解代码库。OSS4SG支持两种有效时间模式，而传统OSS只有Late Spike模式能最快达成核心地位

Abstract: Open Source Software (OSS) sustainability relies on newcomers transitioning to core contributors, but this pipeline is broken, with most newcomers becoming inactive after initial contributions. Open Source Software for Social Good (OSS4SG) projects, which prioritize societal impact as their primary mission, may be associated with different newcomer-to-core transition outcomes than conventional OSS projects. We compared 375 projects (190 OSS4SG, 185 OSS), analyzing 92,721 contributors and 3.5 million commits. OSS4SG projects retain contributors at 2.2X higher rates and contributors have 19.6% higher probability of achieving core status. Early broad project exploration predicts core achievement (22.2% importance); conventional OSS concentrates on one dominant pathway (61.62% of transitions) while OSS4SG provides multiple pathways. Contrary to intuition, contributors who invest time learning the project before intensifying their contributions (Late Spike pattern) achieve core status 2.4-2.9X faster (21 weeks) than those who contribute intensively from day one (Early Spike pattern, 51-60 weeks). OSS4SG supports two effective temporal patterns while only Late Spike achieves fastest time-to-core in conventional OSS. Our findings suggest that finding a project aligned with personal values and taking time to understand the codebase before major contributions are key strategies for achieving core status. Our findings show that project mission is associated with measurably different environments for newcomer-to-core transitions and provide evidence-based guidance for newcomers and maintainers.

</details>


### [25] [GrepRAG: An Empirical Study and Optimization of Grep-Like Retrieval for Code Completion](https://arxiv.org/abs/2601.23254)
*Baoyi Wang,Xingliang Wang,Guochang Li,Chen Zhi,Junxiao Han,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: GrepRAG：一种基于轻量级词法检索的仓库级代码补全框架，通过ripgrep命令检索相关代码片段，配合后处理管道，在无需复杂索引的情况下超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义索引或图分析的RAG方法计算开销大，而开发者常用轻量级搜索工具（如ripgrep）。研究探索简单词法检索在仓库级代码补全中的潜力，以及何时需要更复杂的检索机制。

Method: 1. 提出Naive GrepRAG基线框架：LLM自主生成ripgrep命令检索相关上下文；2. 分析词法检索的局限性：对高频模糊关键词敏感、上下文碎片化；3. 提出改进版GrepRAG：增加标识符加权重排和结构感知去重的后处理管道。

Result: Naive GrepRAG性能与复杂图基线相当；GrepRAG在CrossCodeEval和RepoEval-Updated上持续超越SOTA方法，在CrossCodeEval上代码精确匹配相对提升7.04-15.58%。

Conclusion: 简单词法检索在仓库级代码补全中具有显著潜力，GrepRAG通过轻量级后处理解决词法检索的局限性，在无需复杂索引的情况下实现最佳性能，为实际部署提供了高效解决方案。

Abstract: Repository-level code completion remains challenging for large language models (LLMs) due to cross-file dependencies and limited context windows. Prior work addresses this challenge using Retrieval-Augmented Generation (RAG) frameworks based on semantic indexing or structure-aware graph analysis, but these approaches incur substantial computational overhead for index construction and maintenance. Motivated by common developer workflows that rely on lightweight search utilities (e.g., ripgrep), we revisit a fundamental yet underexplored question: how far can simple, index-free lexical retrieval support repository-level code completion before more complex retrieval mechanisms become necessary? To answer this question, we systematically investigate lightweight, index-free, intent-aware lexical retrieval through extensive empirical analysis. We first introduce Naive GrepRAG, a baseline framework in which LLMs autonomously generate ripgrep commands to retrieve relevant context. Despite its simplicity, Naive GrepRAG achieves performance comparable to sophisticated graph-based baselines. Further analysis shows that its effectiveness stems from retrieving lexically precise code fragments that are spatially closer to the completion site. We also identify key limitations of lexical retrieval, including sensitivity to noisy matches from high-frequency ambiguous keywords and context fragmentation caused by rigid truncation boundaries. To address these issues, we propose GrepRAG, which augments lexical retrieval with a lightweight post-processing pipeline featuring identifier-weighted re-ranking and structure-aware deduplication. Extensive evaluation on CrossCodeEval and RepoEval-Updated demonstrates that GrepRAG consistently outperforms state-of-the-art (SOTA) methods, achieving 7.04-15.58 percent relative improvement in code exact match (EM) over the best baseline on CrossCodeEval.

</details>


### [26] [Outcome-Conditioned Reasoning Distillation for Resolving Software Issues](https://arxiv.org/abs/2601.23257)
*Chenglin Li,Yisen Xu,Zehao Wang,Shin Hwei Tan,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: 提出O-CRD框架，通过从已验证的历史修复中反向重构修复轨迹，在推理时复用这些指导来引导软件问题定位和补丁合成，无需微调或在线搜索，显著提升修复成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM修复管道通常以重置和解决的方式运作，每次遇到新问题都重新推理，浪费了仓库中已有的相似问题修复经验。现有方法通过前向时间试验过程（如重复精炼或搜索）来利用这些信号，成本高且可能偏离正确补丁。

Method: O-CRD框架使用已解决的仓库问题及其已验证补丁作为监督。从历史修复开始，该方法从已验证结果向后重构阶段式修复轨迹，然后在推理时复用这些蒸馏的指导来引导文件/函数定位和补丁合成，无需微调或在线搜索。

Result: 在SWE-Bench Lite上，该方法将Pass@1提升了10.4%（GPT-4o）、8.6%（DeepSeek-V3）和10.3%（GPT-5），表明结果导向的已验证修复复用可以替代昂贵的前向探索。

Conclusion: 通过从历史修复中反向重构修复轨迹并复用指导，O-CRD框架能够有效利用仓库中的修复经验，显著提升软件问题解决的成功率，同时避免昂贵的在线搜索成本。

Abstract: Software issue resolution in large repositories is a long-range decision process: choices made during localization shape the space of viable edits, and missteps can compound into incorrect patches. Despite this, many LLM-based repair pipelines still operate in a reset-and-solve manner, producing fresh reasoning for every new issue instead of carrying forward what worked in past fixes. This is wasteful because repositories routinely contain earlier issues with overlapping structure, failure modes, or constraints, where prior repair experience could provide useful guidance. Existing approaches typically harvest this signal through forward-time trial procedures, such as repeated refinement or search, incurring high inference cost while still risking divergence from the eventual correct patch. We present an Outcome-Conditioned Reasoning Distillation(O-CRD) framework that uses resolved in-repository issues with verified patches as supervision. Starting from a historical fix, the method reconstructs a stage-wise repair trace backward from the verified outcome, then reuses the distilled guidance at inference time to steer file/function localization and patch synthesis, without fine-tuning or online search. On SWE-Bench Lite, this approach increases Pass@1 by 10.4% with GPT-4o, 8.6% with DeepSeek-V3, and 10.3% with GPT-5, indicating that outcome-conditioned reuse of verified repairs can replace costly forward exploration for software issue resolution.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [Towards Resiliency in Large Language Model Serving with KevlarFlow](https://arxiv.org/abs/2601.22438)
*Shangshu Qian,Kipling Liu,P. C. Sruthi,Lin Tan,Yongle Zhang*

Main category: cs.DC

TL;DR: KevlarFlow是一个用于大语言模型服务的容错架构，通过解耦模型并行初始化、动态流量重路由和后台KV缓存复制，将恢复时间缩短20倍，在故障条件下显著改善延迟指标。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统在超大规模集群中面临硬件故障频繁触发软件栈服务中断的问题，现有恢复机制过于缓慢（需要长达10分钟重新初始化资源和加载模型权重），无法满足服务可用性需求。

Method: 1) 解耦模型并行初始化：分离模型并行组件的初始化过程；2) 动态流量重路由：在部分故障时智能重定向流量；3) 后台KV缓存复制：在后台持续复制键值缓存以支持快速恢复。

Result: KevlarFlow将平均恢复时间（MTTR）缩短20倍，在故障条件下：平均延迟改善3.1倍，p99延迟改善2.8倍，平均首词时间（TTFT）改善378.9倍，p99 TTFT改善574.6倍，运行时开销可忽略。

Conclusion: KevlarFlow通过创新的容错架构有效弥合了硬件不可靠性与服务可用性之间的差距，显著提升了LLM服务系统的韧性和性能表现。

Abstract: Large Language Model (LLM) serving systems remain fundamentally fragile, where frequent hardware faults in hyperscale clusters trigger disproportionate service outages in the software stack. Current recovery mechanisms are prohibitively slow, often requiring up to 10 minutes to reinitialize resources and reload massive model weights. We introduce KevlarFlow, a fault tolerant serving architecture designed to bridge the gap between hardware unreliability and service availability. KevlarFlow leverages 1) decoupled model parallelism initialization, 2) dynamic traffic rerouting, and 3) background KV cache replication to maintain high throughput during partial failures. Our evaluation demonstrates that KevlarFlow reduces mean-time-to-recovery (MTTR) by 20x and, under failure conditions, improves average latency by 3.1x, 99th percentile (p99) latency by 2.8x, average time-to-first-token (TTFT) by 378.9x, and p99 TTFT by 574.6x with negligible runtime overhead in comparison to state-of-the-art LLM serving systems.

</details>


### [28] [Coordinating Power Grid Frequency Regulation Service with Data Center Load Flexibility](https://arxiv.org/abs/2601.22487)
*Ali Jahanshahi,Sara Rashidi Golrouye,Osten Anderson,Nanpeng Yu,Daniel Wong*

Main category: cs.DC

TL;DR: GPU数据中心参与电网频率调节服务可减少化石燃料备用容量需求，从而降低电网侧碳排放，其"外源性碳减排"可能超过数据中心自身运营碳排放。


<details>
  <summary>Details</summary>
Motivation: AI/ML数据中心能耗增长导致碳排放增加，而向可再生能源转型和能源需求增长可能破坏电网稳定。电网依赖化石燃料发电厂进行频率调节，这产生隐性碳排放。研究探索数据中心如何参与频率调节服务来减少对化石燃料备用容量的需求。

Method: 提出"外源性碳"新指标来量化数据中心参与调节服务带来的电网侧碳减排；开发EcoCenter框架，最大化GPU数据中心能提供的频率调节量，从而减少所需化石燃料备用容量。

Result: 数据中心参与频率调节服务可产生显著的外源性碳减排，这些减排量常常超过数据中心自身的运营碳排放。

Conclusion: GPU数据中心与电网协调参与频率调节服务是减少电网碳排放的有效途径，数据中心的外源性碳减排潜力巨大，可能超过其自身运营排放，为实现碳中和提供新思路。

Abstract: AI/ML data center growth have led to higher energy consumption and carbon emissions. The shift to renewable energy and growing data center energy demands can destabilize the power grid. Power grids rely on frequency regulation reserves, typically fossil-fueled power plants, to stabilize and balance the supply and demand of electricity. This paper sheds light on the hidden carbon emissions of frequency regulation service. Our work explores how modern GPU data centers can coordinate with power grids to reduce the need for fossil-fueled frequency regulation reserves. We first introduce a novel metric, Exogenous Carbon, to quantify grid-side carbon emission reductions resulting from data center participation in regulation service. We additionally introduce EcoCenter, a framework to maximize the amount of frequency regulation provision that GPU data centers can provide, and thus, reduce the amount of frequency regulation reserves necessary. We demonstrate that data center participation in frequency regulation can result in Exogenous carbon savings that oftentimes outweigh Operational carbon emissions.

</details>


### [29] [HetCCL: Accelerating LLM Training with Heterogeneous GPUs](https://arxiv.org/abs/2601.22585)
*Heehoon Kim,Jaehwan Lee,Taejeoung Kim,Jongwon Park,Jinpyo Kim,Pyongwon Suh,Ryan H. Choi,Sangwoo Lee,Jaejin Lee*

Main category: cs.DC

TL;DR: HetCCL是一个异构GPU集体通信库，支持跨NVIDIA和AMD GPU的高性能通信，无需修改现有深度学习应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，组织需要扩展多厂商GPU集群，但现有深度学习框架缺乏对异构GPU集体通信的支持，导致效率低下和成本增加。

Method: HetCCL通过统一厂商特定后端（NCCL和RCCL）并引入两种新颖机制，实现跨厂商RDMA通信，无需驱动程序修改，同时利用优化的厂商库。

Result: 在多厂商GPU集群上的评估显示，HetCCL在同类GPU设置中性能与NCCL和RCCL相当，在异构环境中能独特扩展，实现高性能训练。

Conclusion: HetCCL为异构GPU集群提供了实用的高性能集体通信解决方案，支持NVIDIA和AMD GPU混合训练，无需修改现有深度学习应用。

Abstract: The rapid growth of large language models is driving organizations to expand their GPU clusters, often with GPUs from multiple vendors. However, current deep learning frameworks lack support for collective communication across heterogeneous GPUs, leading to inefficiency and higher costs. We present HetCCL, a collective communication library that unifies vendor-specific backends and enables RDMA-based communication across GPUs without requiring driver modifications. HetCCL introduces two novel mechanisms that enable cross-vendor communication while leveraging optimized vendor libraries, NVIDIA NCCL and AMD RCCL. Evaluations on a multi-vendor GPU cluster show that HetCCL matches NCCL and RCCL performance in homogeneous setups while uniquely scaling in heterogeneous environments, enabling practical, high-performance training with both NVIDIA and AMD GPUs without changes to existing deep learning applications.

</details>


### [30] [CONCUR: High-Throughput Agentic Batch Inference of LLM via Congestion-Based Concurrency Control](https://arxiv.org/abs/2601.22705)
*Qiaoling Chen,Zhisheng Ye,Tian Tang,Peng Sun,Boyu Tian,Guoteng Wang,Shenggui Li,Yonggang Wen,Zhenhua Han,Tianwei Zhang*

Main category: cs.DC

TL;DR: CONCUR：针对大模型批量推理中代理工作负载的KV缓存中间阶段颠簸问题，提出基于代理级准入控制的轻量级控制层，通过动态调节活跃代理数量来提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 批量推理中代理工作负载会导致KV缓存出现中间阶段颠簸现象，即缓存效率随着长时间运行的代理状态累积而急剧下降，严重影响吞吐量，而现有请求级缓存管理无法有效解决此问题。

Method: 借鉴分布式系统中的拥塞控制思想，将KV缓存视为共享资源，设计CONCUR控制层实现代理级准入控制。采用缓存感知的控制算法，根据运行时缓存信号动态调整活跃代理数量，在保持执行连续性的同时限制聚合缓存压力。

Result: 在Qwen3-32B上吞吐量提升最高达4.09倍，在DeepSeek-V3上提升1.9倍，有效防止中间阶段颠簸，且与现有LLM服务系统兼容。

Conclusion: KV缓存管理需要从反应式的请求级控制转向主动的代理级准入控制，CONCUR通过轻量级的反馈驱动调节机制，显著提升批量推理中代理工作负载的吞吐性能。

Abstract: Batch inference for agentic workloads stresses the GPU key-value (KV) cache in a sustained and cumulative manner, often causing severe throughput degradation well before memory capacity is exhausted. We identify this phenomenon as middle-phase thrashing, a previously under-characterized pathology in which cache efficiency collapses as long-lived agents accumulate state over time.
  We argue that mitigating this pathology requires moving beyond reactive, request-level cache management to proactive, agent-level admission control. Drawing inspiration from congestion control in distributed systems, we view the KV cache as a shared resource whose efficient utilization depends on feedback-driven regulation. Based on this insight, we present CONCUR, a lightweight control layer that regulates agent admission to bound aggregate cache pressure while preserving execution continuity. CONCUR adapts a cache-aware control algorithm to dynamically adjust the number of active agents using runtime cache signals.
  Across large models and real-world agent workloads, CONCUR prevents middle-phase thrashing and improves batch inference throughput by up to 4.09x on Qwen3-32B and 1.9x on DeepSeek-V3, while remaining compatible with existing LLM serving systems.

</details>


### [31] [AscendCraft: Automatic Ascend NPU Kernel Generation via DSL-Guided Transcompilation](https://arxiv.org/abs/2601.22760)
*Zhongzhen Wen,Shudi Shao,Zhong Li,Yu Ge,Tongtong Xu,Yuanyi Lin,Tian Zhang*

Main category: cs.DC

TL;DR: AscendCraft：一种DSL引导的自动AscendC内核生成方法，通过轻量级DSL抽象复杂性和显式建模Ascend执行语义，实现高编译成功率和功能正确性


<details>
  <summary>Details</summary>
Motivation: 为专用加速器（特别是NPU）开发高性能内核耗时且需要专业知识，而现有LLM方法在GPU上有效但在NPU上效果差，主要由于领域特定编程模型、有限公开示例和稀疏文档

Method: 引入轻量级DSL抽象非必要复杂性并显式建模Ascend执行语义，使用类别特定专家示例在DSL中生成内核，然后通过结构化、约束驱动的LLM降低传递将DSL转编译为AscendC

Result: 在MultiKernelBench的七个算子类别上，达到98.1%的编译成功率和90.4%的功能正确性，46.2%的生成内核匹配或超过PyTorch eager执行性能，并能成功为新提出的mHC架构生成正确内核

Conclusion: DSL引导的转编译方法能使LLM生成既正确又具有竞争力的NPU内核，填补了GPU和NPU内核生成之间的差距，为专用加速器的高效内核开发提供了可行方案

Abstract: The performance of deep learning models critically depends on efficient kernel implementations, yet developing high-performance kernels for specialized accelerators remains time-consuming and expertise-intensive. While recent work demonstrates that large language models (LLMs) can generate correct and performant GPU kernels, kernel generation for neural processing units (NPUs) remains largely underexplored due to domain-specific programming models, limited public examples, and sparse documentation. Consequently, directly generating AscendC kernels with LLMs yields extremely low correctness, highlighting a substantial gap between GPU and NPU kernel generation.
  We present AscendCraft, a DSL-guided approach for automatic AscendC kernel generation. AscendCraft introduces a lightweight DSL that abstracts non-essential complexity while explicitly modeling Ascend-specific execution semantics. Kernels are first generated in the DSL using category-specific expert examples and then transcompiled into AscendC through structured, constraint-driven LLM lowering passes. Evaluated on MultiKernelBench across seven operator categories, AscendCraft achieves 98.1% compilation success and 90.4% functional correctness. Moreover, 46.2% of generated kernels match or exceed PyTorch eager execution performance, demonstrating that DSL-guided transcompilation can enable LLMs to generate both correct and competitive NPU kernels. Beyond benchmarks, AscendCraft further demonstrates its generality by successfully generating two correct kernels for newly proposed mHC architecture, achieving performance that substantially surpasses PyTorch eager execution.

</details>


### [32] [ERA: Epoch-Resolved Arbitration for Duelling Admins in Group Management CRDTs](https://arxiv.org/abs/2601.22963)
*Kegan Dougal*

Main category: cs.DC

TL;DR: CRDTs在并发事件下可能出现状态"回滚"问题，特别是在权限管理场景中，拜占庭管理员可利用并发赢得权限争夺，需要引入异步仲裁机制来建立事件顺序


<details>
  <summary>Details</summary>
Motivation: CRDTs在强最终一致性下优先可用性，但并发事件可能导致状态回滚，在即时通讯等权限管理场景中产生意外行为，如"决斗管理员"问题中拜占庭管理员可利用并发获胜

Method: 引入外部仲裁器通过可选的"纪元事件"异步批量仲裁，建立不可变的事件发生前关系，在纪元内创建有界全序，提供最终性保证

Result: 提出的仲裁机制在保持可用性的同时，通过纪元事件引入有界全序，提高了CRDTs的一致性级别，解决了并发事件导致的权限管理问题

Conclusion: CRDTs需要仲裁机制来处理并发事件，通过异步纪元事件建立事件顺序，在保持可用性的同时提供更好的最终一致性，防止拜占庭管理员利用并发获胜

Abstract: Conflict-Free Replicated Data Types (CRDTs) are used in a range of fields for their coordination-free replication with strong eventual consistency. By prioritising availability over consistency under partition, nodes accumulate events in different orders, and rely on an associative, commutative and idempotent merge function to present a materialised view of the CRDT. Under some circumstances, the state of the materialised view over time can appear to ''roll back'' previously applied events. When the materialised view is used to manage group permissions such as ones found in instant messaging applications, this can lead to surprising behaviour. This can occur when there are multiple concurrent events, such as in the Duelling Admins problem where two equally permissioned admins concurrently revoke each other's permissions. Who wins? This article argues that a Byzantine admin can exploit concurrency to win the duel. As a result, an external arbiter is required to arbitrate an immutable happens-before relation between concurrent events. Arbitration occurs asynchronously in batches via optional ''epoch events'', preserving availability. This introduces a bounded total order within epochs, and the resulting ''finality'' improves on the level of consistency CRDTs can provide.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [33] [An innovating approach to teaching applied to database design. Improvement of Action Learning in Lifelong Learning](https://arxiv.org/abs/2601.22175)
*Christophe Béchade*

Main category: cs.DB

TL;DR: 昂热大学通过行动学习模式，让员工和企业人员在专业项目中进行数据库设计培训，结合技术课程、职业培训和咨询服务优势，实现教学与职业目标


<details>
  <summary>Details</summary>
Motivation: 传统数据库设计通常只限于专业数据处理人员（如技术学院或工程师学校毕业生），而行动学习旨在向没有数据处理能力但具备业务流程知识的学员传授知识和经验，促进大学与企业的合作

Method: 采用行动学习模式，教师担任项目管理的监督者角色，在专业机构资助的实际项目中培训学员数据库设计，结合法国技术课程的横向学期项目经验

Result: 行动学习成功整合了法国技术课程的成功因素、职业培训的适应性教学法以及服务提供商的专业能力，实现了可评估且持久的教学与职业目标

Conclusion: 行动学习是法国促进大学与企业合作政策的一部分，通过结合三种互补方法的优势，为这类培训实现了教学与职业的双重目标

Abstract: For now 10 years, the Action Learning has allowed employees of University of Angers, private and public Companies to be initiated with the design of database, on projects financed by professional structures. These innovating training periods are carried out within the framework of the University College of Further Education of the University of Angers. Database design is a process initially reserved to the professional data processing specialists, coming from French Level-2 technological courses (2-year degrees) or Engineer Schools (Master). The pedagogical model of technological courses has integrated for more than 20 years transverse semester projects, in order to give the students the opportunity to apply newly acquired knowledge, coordinated by teachers. Action Learning requires teachers to assume the role of supervisors for the project management. The objective of Action Learning is to transmit not only knowledge from teachers, but also the experience of consultants to trainees having no competence in data processing, but who have the knowledge of their business process. The present paper shows that Action Learning puts together the factors for success of French technological courses, the adaptability of pedagogy provided to the vocational training, and finally the competence of service provider, Keeping the best parts of those three complementary approaches makes it possible for this kind of formation to achieve teaching and professional, assessable and long lasting goals. Action Learning belongs to the French policy that aims to improve the volume and the quality of the contracts between Universities and companies.

</details>


### [34] [Discovering High-utility Sequential Rules with Increasing Utility Ratio](https://arxiv.org/abs/2601.22178)
*Zhenqiang Ye,Wensheng Gan,Gengsen Huang,Tianlong Gu,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出SRIU算法挖掘具有递增效用比的高效用序列规则，解决传统方法中规则生成与效用变化关系不明确的问题


<details>
  <summary>Details</summary>
Motivation: 当前高效用序列规则挖掘方法中，规则与其生成过程之间的关联不明确，无法确定新增项对规则效用或置信度的影响方向，这限制了决策支持的可靠性

Method: 提出SRIU算法，采用左右扩展和右左扩展两种方法，引入项对估计效用剪枝策略(IPEUP)减少搜索空间，使用Bitmap降低内存消耗，设计紧凑效用表优化挖掘过程

Result: 在真实和合成数据集上的实验表明SRIU方法有效，使用置信度和conviction等指标评估规则质量，证明SRIU能提高挖掘结果的相关性

Conclusion: SRIU算法成功解决了高效用序列规则挖掘中规则生成与效用变化关系不明确的问题，通过递增效用比的概念和多种优化策略，提高了挖掘效率和结果质量

Abstract: Utility-driven mining is an essential task in data science, as it can provide deeper insight into the real world. High-utility sequential rule mining (HUSRM) aims at discovering sequential rules with high utility and high confidence. It can certainly provide reliable information for decision-making because it uses confidence as an evaluation metric, as well as some algorithms like HUSRM and US-Rule. However, in current rule-growth mining methods, the linkage between HUSRs and their generation remains ambiguous. Specifically, it is unclear whether the addition of new items affects the utility or confidence of the former rule, leading to an increase or decrease in their values. Therefore, in this paper, we formulate the problem of mining HUSRs with an increasing utility ratio. To address this, we introduce a novel algorithm called SRIU for discovering all HUSRs with an increasing utility ratio using two distinct expansion methods, including left-right expansion and right-left expansion. SRIU also utilizes the item pair estimated utility pruning strategy (IPEUP) to reduce the search space. Moreover, for the two expansion methods, two sets of upper bounds and corresponding pruning strategies are introduced. To enhance the efficiency of SRIU, several optimizations are incorporated. These include utilizing the Bitmap to reduce memory consumption and designing a compact utility table for the mining procedure. Finally, extensive experimental results from both real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Moreover, to better assess the quality of the generated sequential rules, metrics such as confidence and conviction are employed, which further demonstrate that SRIU can improve the relevance of mining results.

</details>


### [35] [High-utility Sequential Rule Mining Utilizing Segmentation Guided by Confidence](https://arxiv.org/abs/2601.22179)
*Chunkai Zhang,Jiarui Deng,Maohua Lyu,Wensheng Gan,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出RSC算法，通过置信度引导的分割减少高效用序列规则挖掘中的冗余效用计算


<details>
  <summary>Details</summary>
Motivation: 现有高效用序列规则挖掘算法存在冗余效用计算问题，不同规则可能包含相同的项目序列，当这些项目可以形成多个不同规则时需要重复计算效用

Method: 采用置信度引导的分割策略，预先计算分割规则的置信度，利用效用链接表加速候选序列生成，引入更严格的效用上界（序列的减少剩余效用）处理重复项目

Result: 在多个数据集上评估RSC方法，结果显示相比现有最优方法有改进

Conclusion: RSC算法通过减少冗余效用计算，提高了高效用序列规则挖掘的效率

Abstract: Within the domain of data mining, one critical objective is the discovery of sequential rules with high utility. The goal is to discover sequential rules that exhibit both high utility and strong confidence, which are valuable in real-world applications. However, existing high-utility sequential rule mining algorithms suffer from redundant utility computations, as different rules may consist of the same sequence of items. When these items can form multiple distinct rules, additional utility calculations are required. To address this issue, this study proposes a sequential rule mining algorithm that utilizes segmentation guided by confidence (RSC), which employs confidence-guided segmentation to reduce redundant utility computation. It adopts a method that precomputes the confidence of segmented rules by leveraging the support of candidate subsequences in advance. Once the segmentation point is determined, all rules with different antecedents and consequents are generated simultaneously. RSC uses a utility-linked table to accelerate candidate sequence generation and introduces a stricter utility upper bound, called the reduced remaining utility of a sequence, to address sequences with duplicate items. Finally, the proposed RSC method was evaluated on multiple datasets, and the results demonstrate improvements over state-of-the-art approaches.

</details>


### [36] [COL-Trees: Efficient Hierarchical Object Search in Road Networks](https://arxiv.org/abs/2601.22183)
*Tenindra Abeywickrama,Muhammad Aamir Cheema,Sabine Storandt*

Main category: cs.DB

TL;DR: 提出COL-Tree数据结构，使用地标启发式进行层次化图遍历，高效解决聚合k近邻、k远邻等查询问题，性能提升达4个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有基于欧几里得距离的启发式方法在道路网络等图结构中效果有限，无法有效处理聚合k近邻、k远邻等复杂查询需求，需要更精确的图遍历方法。

Method: 提出COL-Tree（压缩对象-地标树）数据结构，利用地标启发式进行层次化图遍历，并基于此设计查询算法处理聚合k近邻、k远邻等查询。

Result: 在真实世界和合成数据集上的实验表明，该方法显著优于现有方法，性能提升高达4个数量级，且预处理开销在理论和实践中都很小。

Conclusion: COL-Tree通过地标启发式实现了高效的层次化图遍历，为聚合k近邻、k远邻等复杂查询提供了有效的解决方案，具有显著性能优势。

Abstract: Location-based services rely heavily on efficient methods that search for relevant points-of-interest (POIs) near a given location. A k Nearest Neighbor (kNN) query is one such example that finds the k closest POIs from an agent's location. While most existing techniques focus on retrieving nearby POIs for a single agent, these search heuristics do not translate to many other applications. For example, Aggregate k Nearest Neighbor (AkNN) queries require POIs that are close to multiple agents. k Farthest Neighbor (kFN) queries require POIs that are the antithesis of nearest. Such problems naturally benefit from a hierarchical approach, but existing methods rely on Euclidean-based heuristics, which have diminished effectiveness in graphs such as road networks. We propose a novel data structure, COL-Tree (Compacted Object-Landmark Tree), to address this gap by enabling efficient hierarchical graph traversal using a more accurate landmark-based heuristic. We then present query algorithms that utilize COL-Trees to efficiently answer AkNN, kFN, and other queries. In our experiments on real-world and synthetic datasets, we demonstrate that our techniques significantly outperform existing approaches, achieving up to 4 orders of magnitude improvement. Moreover, this comes at a small pre-processing overhead in both theory and practice.

</details>
