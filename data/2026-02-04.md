<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.SE](#cs.SE) [Total: 28]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Prefix Consensus For Censorship Resistant BFT](https://arxiv.org/abs/2602.02892)
*Zhuolun Xiang,Andrei Tonkikh,Alexander Spiegelman*

Main category: cs.DC

TL;DR: 提出Prefix Consensus新抽象和协议栈，解决BFT共识中的审查抵抗问题，实现无领导、多提议者的抗审查BFT SMR协议


<details>
  <summary>Details</summary>
Motivation: 现有BFT共识在区块链中审查抵抗性弱，领导者可以排除交易，这对交易和DeFi构成日益增长的担忧

Method: 1. 引入Prefix Consensus抽象，各方输入向量并输出(v_low,v_high)；2. 定义Strong Prefix Consensus，要求对high输出达成一致；3. 构建无领导、多提议者的抗审查BFT SMR协议，每时隙所有方广播提案并运行Strong Prefix Consensus

Result: 1. Prefix Consensus可在异步环境下解决并给出紧的轮复杂度界限；2. Strong Prefix Consensus协议是无领导的且部分同步；3. BFT SMR协议在四轮内提交诚实提案，GST后最多f个时隙可能丢失诚实提案

Conclusion: 通过Prefix Consensus新抽象构建了抗审查的BFT共识协议栈，连接了分级共识和二元/验证共识，实现了最优延迟的分级共识和低复杂度的无领导二元共识

Abstract: Despite broad use of BFT consensus in blockchains, censorship resistance is weak: leaders can exclude transactions, a growing concern for trading and DeFi.
  We address this by introducing a new abstraction and protocol stack. First, we introduce \emph{Prefix Consensus}, where parties input vectors and output $(v^{\sf low},v^{\sf high})$ that (i) extend the maximum common prefix of honest inputs and (ii) satisfy $v_i^{\sf low}\preceq v_j^{\sf high}$ for all honest $i,j$. Unlike classical consensus, no single output is required. We show Prefix Consensus is solvable asynchronously and give tight round-complexity bounds.
  We then define \emph{Strong Prefix Consensus}, requiring agreement on the \emph{high} output. Our protocol is leaderless and partially synchronous: one Prefix Consensus instance decides (possibly different) lows, and additional instances yield a unique safe-to-extend high, even if an adversary can suspend one party per round.
  We lift this to a leaderless, multi-proposer, censorship-resistant BFT SMR protocol: per slot, all parties broadcast proposals, deterministically rank them, and run one Strong Prefix Consensus on proposal hashes, committing honest proposals in \emph{four rounds}. A deterministic demotion rule updates the ranking when a party's proposal is excluded, implying that after GST at most $f$ slots can miss an honest proposal while progress remains leaderless under suspension and up to $f{-}1$ Byzantine faults.
  Finally, we connect Prefix Consensus to graded and binary/validated consensus: we obtain an optimal-latency graded consensus (3 message delays) and leaderless Binary/Validated Consensus with worst-case message complexity $O(n^3)$ and communication $O(n^4)$.

</details>


### [2] [Large-Scale LLM Inference with Heterogeneous Workloads: Prefill-Decode Contention and Asymptotically Optimal Control](https://arxiv.org/abs/2602.02987)
*Ruihan Lin,Zezhen Ding,Zean Han,Jiheng Zhang*

Main category: cs.DC

TL;DR: 提出基于随机控制的LLM推理调度框架，解决GPU集群中预填充和解码阶段资源争用问题，通过门控路由策略实现最优资源分配。


<details>
  <summary>Details</summary>
Motivation: LLM在企业应用中需求激增，但推理过程存在两阶段特性：计算密集的预填充阶段和内存受限的解码阶段。当这两个阶段共享GPU资源时，预填充任务会拖慢并发解码速度，形成状态依赖的资源争用。同时，不同应用的输入输出长度差异很大，增加了调度复杂性。

Method: 将LLM推理建模为具有状态依赖服务速率的多类多服务器排队网络，基于经验迭代时间测量建立模型。分析系统的流体近似，求解稳态线性规划以表征最优资源分配。设计门控路由策略来调节预填充准入和解码路由，证明在大量GPU限制下对捆绑和分离令牌定价方案都是渐近最优的。

Result: 数值实验使用经验迭代时间数据进行校准，表明所提出的策略优于标准服务启发式方法。框架还可扩展以纳入服务级别指标（如延迟和公平性），提供约束调度的通用方法。

Conclusion: 开发了一个随机控制框架来调度异构LLM工作负载，通过理论分析和实证验证，证明了门控路由策略在大型GPU集群中的渐近最优性，为解决LLM推理中的资源争用问题提供了有效方案。

Abstract: Large Language Models (LLMs) are rapidly becoming critical infrastructure for enterprise applications, driving unprecedented demand for GPU-based inference services. A key operational challenge arises from the two-phase nature of LLM inference: a compute-intensive \emph{prefill} phase that processes user input, followed by a memory-bound \emph{decode} phase that generates output tokens. When these phases share GPU resources, prefill tasks throttle the processing speed of concurrent decodes, creating state-dependent contention. This contention is further complicated by workload heterogeneity, as different applications exhibit vastly different input and output lengths. We develop a stochastic control framework for scheduling heterogeneous LLM workloads across large GPU clusters. We formulate LLM inference as a multiclass many-server queueing network with state-dependent service rates, grounded in empirical iteration-time measurements. We analyze the fluid approximation of this system and solve steady-state linear programs that characterize optimal resource allocation. We design gate-and-route policies that regulate prefill admission and decode routing, and prove that they are asymptotically optimal in the many-GPU limit under both bundled and separate token-pricing schemes. We further extend the framework to incorporate Service Level Indicators (SLIs) such as latency and fairness, providing a general approach to constrained scheduling. Numerical experiments calibrated to empirical iteration-time data demonstrate that our policies outperform standard serving heuristics.

</details>


### [3] [Studying the Effect of Schedule Preemption on Dynamic Task Graph Scheduling](https://arxiv.org/abs/2602.03081)
*Mohammadali Khodabandehlou,Jared Coleman,Niranjan Suri,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 研究动态任务图调度中的可控抢占模型（Last-K Preemption），通过选择性重调度近期任务图来平衡性能指标，实验表明适度抢占能在保持公平性和低开销的同时获得大部分性能收益。


<details>
  <summary>Details</summary>
Motivation: 传统动态任务图调度通常不重新审视已分配任务，主要关注最小化完成时间。本研究旨在探索可控的调度抢占机制，在保持早期分配的同时选择性重调度近期任务图。

Method: 提出Last-K抢占模型，选择性重调度最近K个任务图，同时保留更早的任务分配。使用合成数据集、RIoTBench、WFCommons和对抗性工作负载，比较抢占式、非抢占式和部分抢占式策略。

Result: 实验结果表明，适度抢占（部分抢占）能够匹配完全抢占在完成时间和资源利用率方面的大部分收益，同时保持公平性和较低的开销。

Conclusion: 可控的调度抢占机制（如Last-K模型）能够在性能指标（完成时间、利用率）和系统质量（公平性、开销）之间取得良好平衡，为动态任务图调度提供实用解决方案。

Abstract: Dynamic scheduling of task graphs is often addressed without revisiting prior task allocations, with a primary focus on minimizing makespan. We study controlled schedule preemption, introducing the Last-K Preemption model, which selectively reschedules recent task graphs while preserving earlier allocations. Using synthetic, RIoTBench, WFCommons, and adversarial workloads, we compare preemptive, non-preemptive, and partial-preemptive strategies across makespan, fairness, utilization, and runtime. Results show moderate preemption can match most makespan and utilization gains of full preemption while maintaining fairness and low overhead.

</details>


### [4] [Joint Network-and-Server Congestion in Multi-Source Traffic Allocation: A Convex Formulation and Price-Based Decentralization](https://arxiv.org/abs/2602.03246)
*Tamoghna Sarkar,Bhaskar Krishnamachari*

Main category: cs.DC

TL;DR: 研究多源多服务节点的稳态流量分配问题，考虑路径依赖的接入延迟和节点负载依赖的排队延迟，提出分布式定价算法实现系统最优的端到端延迟最小化。


<details>
  <summary>Details</summary>
Motivation: 在分布式网络系统中，当多个源节点向多个服务节点发送流量时，存在两个关键延迟因素：1) 每条源-节点路径上的接入延迟（与速率相关且凸），2) 每个服务节点的排队延迟（与聚合负载相关）。现有研究往往单独处理这两种延迟，需要联合建模以实现系统最优的流量分配。

Method: 将端到端延迟最小化问题建模为凸优化问题，利用KKT条件推导出最优条件：所有使用路径的总边际成本（路径边际接入项+节点拥塞价格）相等。基于此结构设计分布式定价算法：服务节点根据观测负载计算并广播标量拥塞价格，源节点根据价格更新流量分配。

Result: 理论证明该问题为凸规划，存在全局最优解，可通过KKT条件描述。分布式算法能收敛到集中式最优解，数值实验验证了算法的收敛性和性能，展示了联合建模接入和服务拥塞带来的权衡。

Conclusion: 该研究为多源多节点的网络流量分配提供了统一的凸优化框架，提出的分布式定价算法轻量且有效，能够实现系统最优的端到端延迟最小化，为实际网络系统的流量管理提供了理论依据和实用方法。

Abstract: This paper studies an important rate allocation problem that arises in many networked and distributed systems: steady-state traffic rate allocation from multiple sources to multiple service nodes when both (i) the access-path delay on each source-node route is rate-dependent (capacity-constrained) and convex, and (ii) each service node (also capacity-constrained) experiences a load-dependent queueing delay driven by aggregate load from all sources. We show that the resulting flow-weighted end-to-end delay minimization is a convex program, yielding a global system-optimal solution characterized by KKT conditions that equalize total marginal costs (a path marginal access term plus a node congestion price) across all utilized routes. This condition admits a Wardrop-type interpretation: for each source, all utilized options equalize total marginal cost, while any option with strictly larger total marginal cost receives no flow. Building on this structure, we develop a lightweight distributed pricing-based algorithm in which each service node locally computes and broadcasts a scalar congestion price from its observed aggregate load, while each source updates its traffic split by solving a small separable convex allocation problem under the advertised prices. Numerical illustrations demonstrate convergence of the distributed iteration to the centralized optimum and highlight the trade-offs induced by jointly modeling access and service congestion.

</details>


### [5] [Exploiting Multi-Core Parallelism in Blockchain Validation and Construction](https://arxiv.org/abs/2602.03444)
*Arivarasan Karmegam,Lucianna Kiffer,Antonio Fernández Anta*

Main category: cs.DC

TL;DR: 本文系统研究了区块链验证器如何利用多核CPU并行处理区块，同时保持确定性执行和区块链语义。提出了两个优化问题：已排序区块的并行执行调度和内存池交易的选择调度，并开发了MILP精确解和快速启发式算法。


<details>
  <summary>Details</summary>
Motivation: 区块链验证器可以利用多核CPU减少区块处理时间，但必须保持确定性执行，同时尊重交易冲突和每个区块的运行时间限制。目前缺乏系统性的方法来在区块构建和执行阶段有效利用多核并行性而不违反区块链语义。

Method: 1) 形式化两个验证器端优化问题：已排序区块在p个核心上的最小化完成时间调度，以及在运行时限制B下选择调度内存池交易以最大化验证器奖励；2) 开发精确的混合整数线性规划(MILP)公式，捕捉冲突、顺序和容量约束；3) 提出可扩展到实际工作负载的快速确定性启发式算法。

Result: 使用以太坊主网跟踪数据，包括Solana启发的声明访问基线(Sol)用于已排序区块调度，以及简单的奖励贪婪基线(RG)用于区块构建。实证量化了最优性和运行时间之间的权衡关系。

Conclusion: 本文为区块链验证器在多核环境下的并行处理提供了系统性的解决方案，通过形式化优化问题、开发精确算法和启发式方法，在保持区块链语义的同时显著提升了处理效率，为实际部署提供了理论和技术基础。

Abstract: Blockchain validators can reduce block processing time by exploiting multi-core CPUs, but deterministic execution must preserve a given total order while respecting transaction conflicts and per-block runtime limits. This paper systematically examines how validators can exploit multi-core parallelism during both block construction and execution without violating blockchain semantics. We formalize two validator-side optimization problems: (i) executing an already ordered block on \(p\) cores to minimize makespan while ensuring equivalence to sequential execution; and (ii) selecting and scheduling a subset of mempool transactions under a runtime limit \(B\) to maximize validator reward. For both, we develop exact Mixed-Integer Linear Programming (MILP) formulations that capture conflict, order, and capacity constraints, and propose fast deterministic heuristics that scale to realistic workloads. Using Ethereum mainnet traces and including a Solana-inspired declared-access baseline (Sol) for ordered-block scheduling and a simple reward-greedy baseline (RG) for block construction, we empirically quantify the trade-offs between optimality and runtime.

</details>


### [6] [Recursive Energy Efficient Agreement](https://arxiv.org/abs/2602.03474)
*Shachar Meir,David Peleg*

Main category: cs.DC

TL;DR: 提出一种递归式共识算法，使每个参与者仅需参与O(log f)轮通信，显著降低能耗成本


<details>
  <summary>Details</summary>
Motivation: 分布式计算中的共识问题已研究数十年，最近Meir等人提出"能量高效共识"概念，旨在最小化每个参与者参与的轮数以减少能耗成本

Method: 设计递归式共识算法，通过递归结构减少每个参与者的活跃轮数，其中f<n表示系统中的最大崩溃故障数

Result: 算法实现每个参与者仅需参与O(log f)轮活跃通信，相比传统方法显著降低能耗

Conclusion: 该递归算法在保证共识正确性的同时，大幅减少了参与者的能量消耗，为能量高效分布式系统提供了有效解决方案

Abstract: Agreement is a foundational problem in distributed computing that have been studied extensively for over four decades. Recently, Meir, Mirault, Peleg and Robinson introduced the notion of \emph{Energy Efficient Agreement}, where the goal is to solve Agreement while minimizing the number of round a party participates in, thereby reducing the energy cost per participant. We show a recursive Agreement algorithm that has $O(\log f)$ active rounds per participant, where $f<n$ represents the maximum number of crash faults in the system.

</details>


### [7] [DALI: A Workload-Aware Offloading Framework for Efficient MoE Inference on Local PCs](https://arxiv.org/abs/2602.03495)
*Zeyu Zhu,Gang Li,Peisong Wang,Zitao Mo,Minnan Pei,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.DC

TL;DR: DALI：面向本地PC上MoE推理的工作负载感知卸载框架，通过动态专家分配、基于残差的预取和工作负载感知缓存替换，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能提升LLM容量而不成比例增加计算量，但参数规模巨大。现有卸载方法无法匹配专家工作负载的动态特性，导致CPU-GPU负载不平衡、预取不准确和GPU缓存效率低下等问题。

Method: 提出DALI框架：1) 将专家分配建模为0-1整数优化问题，使用贪心分配策略动态分配专家到CPU或GPU；2) 基于残差的预取方法，利用层间残差信息准确预测高负载专家；3) 工作负载感知缓存替换策略，利用专家激活的时间相关性提升GPU缓存效率。

Result: 在各种MoE模型和设置下评估，DALI在预填充和解码阶段相比最先进的卸载框架都实现了显著加速。

Conclusion: DALI通过工作负载感知的卸载方法有效解决了MoE推理中的资源利用和效率问题，为资源受限的本地PC平台提供了高效的MoE推理解决方案。

Abstract: Mixture of Experts (MoE) architectures significantly enhance the capacity of LLMs without proportional increases in computation, but at the cost of a vast parameter size. Offloading MoE expert parameters to host memory and leveraging both CPU and GPU computation has recently emerged as a promising direction to support such models on resourceconstrained local PC platforms. While promising, we notice that existing approaches mismatch the dynamic nature of expert workloads, which leads to three fundamental inefficiencies: (1) Static expert assignment causes severe CPUGPU load imbalance, underutilizing CPU and GPU resources; (2) Existing prefetching techniques fail to accurately predict high-workload experts, leading to costly inaccurate prefetches; (3) GPU cache policies neglect workload dynamics, resulting in poor hit rates and limited effectiveness. To address these challenges, we propose DALI, a workloaDAware offLoadIng framework for efficient MoE inference on local PCs. To fully utilize hardware resources, DALI first dynamically assigns experts to CPU or GPU by modeling assignment as a 0-1 integer optimization problem and solving it efficiently using a Greedy Assignment strategy at runtime. To improve prefetching accuracy, we develop a Residual-Based Prefetching method leveraging inter-layer residual information to accurately predict high-workload experts. Additionally, we introduce a Workload-Aware Cache Replacement policy that exploits temporal correlation in expert activations to improve GPU cache efficiency. By evaluating across various MoE models and settings, DALI achieves significant speedups in the both prefill and decoding phases over the state-of-the-art offloading frameworks.

</details>


### [8] [Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods](https://arxiv.org/abs/2602.03802)
*Grigory Begunov,Alexander Tyurin*

Main category: cs.DC

TL;DR: 同步SGD及其变体m-Synchronous SGD在异构计算场景下近乎最优，打破了异步方法更优的传统认知


<details>
  <summary>Details</summary>
Motivation: 尽管异步优化方法近期取得显著进展，但现代分布式优化仍主要依赖传统同步方法。本文旨在重新评估同步方法在异构计算环境中的性能，挑战异步方法更优的普遍认知。

Method: 重新审视Synchronous SGD及其鲁棒变体m-Synchronous SGD，在随机计算时间和对抗性部分参与的工作节点条件下进行理论分析，评估其在异构计算场景下的性能。

Result: 理论证明同步方法在许多异构计算场景下近乎最优，其时间复杂度在许多实际场景中达到最优（最多相差对数因子），这一结果出人意料。

Conclusion: 虽然同步方法不是通用解决方案（某些任务可能需要异步方法），但研究表明它们足以应对许多现代异构计算场景，挑战了异步方法更优的传统观点。

Abstract: Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Constitutional Spec-Driven Development: Enforcing Security by Construction in AI-Assisted Code Generation](https://arxiv.org/abs/2602.02584)
*Srinivas Rao Marri*

Main category: cs.SE

TL;DR: 论文提出Constitutional Spec-Driven Development方法，将安全原则嵌入规范层，使AI生成的代码从一开始就符合安全要求，在银行微服务案例中减少73%安全缺陷。


<details>
  <summary>Details</summary>
Motivation: AI辅助的"氛围编程"虽然加速软件开发，但大语言模型优先考虑功能正确性而非安全性，引入了重大安全风险。需要一种方法确保AI生成的代码符合安全要求。

Method: 提出Constitutional Spec-Driven Development方法，引入"宪法"概念：一个版本化、机器可读的文档，编码从CWE/MITRE Top 25漏洞和监管框架衍生的安全约束。该方法将不可协商的安全原则嵌入规范层，确保AI生成的代码通过构造而非检查来遵守安全要求。

Result: 在银行微服务应用案例中，通过宪法约束解决了10个关键CWE漏洞，实现了从原则到代码位置的全链路可追溯性。相比无约束的AI生成，宪法约束减少了73%的安全缺陷，同时保持了开发速度。

Conclusion: 贡献了宪法安全的形式化框架、完整的开发方法学，以及实证证据表明在AI辅助开发工作流中，主动的安全规范优于被动的安全验证。该方法本身是领域无关的。

Abstract: The proliferation of AI-assisted "vibe coding" enables rapid software development but introduces significant security risks, as Large Language Models (LLMs) prioritize functional correctness over security. We present Constitutional Spec-Driven Development, a methodology that embeds non-negotiable security principles into the specification layer, ensuring AI-generated code adheres to security requirements by construction rather than inspection. Our approach introduces a Constitution: a versioned, machine-readable document encoding security constraints derived from Common Weakness Enumeration (CWE)/MITRE Top 25 vulnerabilities and regulatory frameworks. We demonstrate the methodology through a banking microservices application, selected as a representative example domain due to its stringent regulatory and security requirements, implementing customer management, account operations, and transaction processing. The methodology itself is domain-agnostic. The implementation addresses 10 critical CWE vulnerabilities through constitutional constraints with full traceability from principles to code locations. Our case study shows that constitutional constraints reduce security defects by 73% compared to unconstrained AI generation while maintaining developer velocity. We contribute a formal framework for constitutional security, a complete development methodology, and empirical evidence that proactive security specification outperforms reactive security verification in AI-assisted development workflows.

</details>


### [10] [Agentic Observability: Automated Alert Triage for Adobe E-Commerce](https://arxiv.org/abs/2602.02585)
*Aprameya Bharadwaj,Kyle Tu*

Main category: cs.SE

TL;DR: 论文提出了一种基于ReAct范式的智能可观测性框架，在Adobe电商基础设施中部署，能自主进行告警分类，相比人工处理将平均洞察时间减少90%


<details>
  <summary>Details</summary>
Motivation: 现代企业系统存在复杂的相互依赖性，使得可观测性和事件响应变得日益困难。人工告警分类（包括日志检查、API验证和操作知识库交叉引用）仍然是降低平均恢复时间（MTTR）的主要瓶颈。

Method: 采用ReAct范式的智能可观测性框架，在检测到告警时，代理能动态识别受影响的服务，检索和分析分布式系统中的相关日志，并规划上下文相关的操作，如手册咨询、运行手册执行或最近部署代码的检索增强分析。

Result: 生产部署的实证结果表明，与人工分类相比，平均洞察时间减少了90%，同时保持了相当的诊断准确性。智能AI使分类延迟降低了一个数量级，并在解决准确性方面实现了阶跃式改进。

Conclusion: 智能AI实现了分类延迟的数量级减少和解决准确性的阶跃式改进，标志着企业运营向自主可观测性的关键转变。

Abstract: Modern enterprise systems exhibit complex interdependencies that make observability and incident response increasingly challenging. Manual alert triage, which typically involves log inspection, API verification, and cross-referencing operational knowledge bases, remains a major bottleneck in reducing mean recovery time (MTTR). This paper presents an agentic observability framework deployed within Adobe's e-commerce infrastructure that autonomously performs alert triage using a ReAct paradigm. Upon alert detection, the agent dynamically identifies the affected service, retrieves and analyzes correlated logs across distributed systems, and plans context-dependent actions such as handbook consultation, runbook execution, or retrieval-augmented analysis of recently deployed code. Empirical results from production deployment indicate a 90% reduction in mean time to insight compared to manual triage, while maintaining comparable diagnostic accuracy. Our results show that agentic AI enables an order-of-magnitude reduction in triage latency and a step-change in resolution accuracy, marking a pivotal shift toward autonomous observability in enterprise operations.

</details>


### [11] [Testing Storage-System Correctness: Challenges, Fuzzing Limitations, and AI-Augmented Opportunities](https://arxiv.org/abs/2602.02614)
*Ying Wang,Jiahui Chen,Dejun Jiang*

Main category: cs.SE

TL;DR: 这篇论文综述了存储系统正确性测试的挑战与现状，分析了现有测试技术的优缺点，并探讨了模糊测试与AI技术在该领域的应用前景。


<details>
  <summary>Details</summary>
Motivation: 存储系统是现代计算基础设施的基础，但确保其正确性在实践中仍然具有挑战性。尽管系统测试研究已有数十年，但许多存储系统故障（包括持久性、顺序性、恢复和一致性违规）仍然难以系统性地暴露。这种困难主要源于存储系统执行的内在特性，而非测试工具不足。

Method: 采用存储中心视角组织系统测试技术，根据执行属性和故障机制对现有技术进行分类。综述了从并发测试、长期运行工作负载到崩溃一致性分析、硬件级语义验证和分布式故障注入等多种方法，并分析其基本优势和局限性。特别关注模糊测试作为自动化测试范式，并探讨AI技术如何通过状态感知和语义指导来补充模糊测试。

Result: 提供了一个统一的存储系统正确性测试视角，识别了现有测试技术的关键局限性，特别是传统模糊测试假设与存储系统语义之间的系统性不匹配问题。

Conclusion: 存储系统测试面临固有挑战，需要针对其特殊执行特性开发专门的测试方法。模糊测试需要适应存储系统语义，而AI技术可能提供新的测试指导方向。该综述为存储系统正确性测试提供了系统化的分析框架，并指出了未来研究的关键挑战。

Abstract: Storage systems are fundamental to modern computing infrastructures, yet ensuring their correctness remains challenging in practice. Despite decades of research on system testing, many storage-system failures (including durability, ordering, recovery, and consistency violations) remain difficult to expose systematically. This difficulty stems not primarily from insufficient testing tooling, but from intrinsic properties of storage-system execution, including nondeterministic interleavings, long-horizon state evolution, and correctness semantics that span multiple layers and execution phases.
  This survey adopts a storage-centric view of system testing and organizes existing techniques according to the execution properties and failure mechanisms they target. We review a broad spectrum of approaches, ranging from concurrency testing and long-running workloads to crash-consistency analysis, hardware-level semantic validation, and distributed fault injection, and analyze their fundamental strengths and limitations. Within this framework, we examine fuzzing as an automated testing paradigm, highlighting systematic mismatches between conventional fuzzing assumptions and storage-system semantics, and discuss how recent artificial intelligence advances may complement fuzzing through state-aware and semantic guidance. Overall, this survey provides a unified perspective on storage-system correctness testing and outlines key challenges

</details>


### [12] [Outrunning LLM Cutoffs: A Live Kernel Crash Resolution Benchmark for All](https://arxiv.org/abs/2602.02690)
*Chenxi Huang,Alex Mathai,Feiyang Yu,Aleksandr Nogikh,Petros Maniatis,Franjo Ivančić,Eugene Wu,Kostis Kaffes,Junfeng Yang,Baishakhi Ray*

Main category: cs.SE

TL;DR: 提出了Live-kBench和kEnv框架，用于评估LLM代理修复Linux内核崩溃的能力，解决了现有静态基准无法捕捉内核演化和数据污染的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理评估基准是静态的，无法反映Linux内核的持续演化特性，且存在因LLM知识截止日期导致的数据污染问题，需要更动态、公平的评估框架。

Method: 开发了Live-kBench（自演化基准评估框架）和kEnv（代理无关的标准化崩溃修复环境），前者持续爬取新发现的内核bug，后者提供内核编译、执行和反馈的统一环境。

Result: 在534个Linux内核bug数据集上，代理在LLM知识截止日期前修复的bug上达到25%更高的等效补丁率；代理首尝试修复74%的崩溃，但只有约20%的补丁与开发者修复高度匹配；暴露崩溃修复反馈可将修复率提高29%。

Conclusion: Live-kBench和kEnv为内核崩溃修复提供了动态、公平的评估基础设施，揭示了当前LLM代理与开发者修复之间的显著差距，并展示了反馈机制的重要性。

Abstract: Repairing system crashes discovered by kernel fuzzers like Syzkaller is a critical yet underexplored challenge in software engineering. While recent works have introduced Large Language Model (LLM) based agents for Linux kernel crash-resolution, their evaluation benchmarks are usually static and thus, do not capture the evolving nature of the Linux kernel, and suffer from potential data contamination due to LLM knowledge cutoffs. To address the above problem, we present (i) Live-kBench, an evaluation framework for self-evolving benchmarks that continuously scrapes and evaluates agents on freshly discovered kernel bugs, and (ii) kEnv, an agent-agnostic standardized crash-resolution environment for kernel compilation, execution, and feedback. This design decouples agent workflows from heavy-weight execution, enabling fair and scalable comparison across diverse agent frameworks under identical conditions.
  To this end, we curate an inaugural dataset of 534 Linux kernel bugs and empirically demonstrate a significant performance gap, with agents achieving up to 25% higher equivalent patch rate on bugs fixed before the LLM knowledge cutoff. Using kEnv, we benchmark three state-of-the-art agents, showing that they resolve 74% of crashes on the first attempt (plausible patches); however only ~20% of generated patches closely match developer fixes. Additionally, exposing crash resolution feedback improves crash resolution rate by 29%. Live-kBench provides the community with an evaluation infrastructure for self-evolving benchmarks that is both time and attribute sensitive; complete with a public dashboard to track agent progress on Linux kernel bugs.

</details>


### [13] [Beyond the Prompt: Assessing Domain Knowledge Strategies for High-Dimensional LLM Optimization in Software Engineering](https://arxiv.org/abs/2602.02752)
*Srinath Srinivasan,Tim Menzies*

Main category: cs.SE

TL;DR: 本研究比较了人类与人工智能生成领域知识的方法，评估了四种架构能否帮助LLM为高维优化生成有效的预热启动，发现结构化知识集成能显著提升LLM在高维优化中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在低维软件工程优化任务中表现良好，但在高维问题上表现不佳，而贝叶斯方法在高维优化中占主导地位。需要理解如何通过系统集成领域知识（无论是来自人类还是自动推理）来弥合这一差距。

Method: 在按维度分层的MOOT数据集上评估四种方法：1）人机交互领域知识提示（H-DKP），利用异步专家反馈循环；2）自适应多阶段提示（AMP），实现顺序约束识别和验证；3）维度感知渐进细化（DAPR），在逐步扩展的特征子空间中进行优化；4）混合知识模型方法（HKMA），将统计侦察（TPE）与RAG增强提示相结合。通过切比雪夫距离到最优解来量化性能，并使用Scott-Knott聚类与LLM生成预热启动的基线进行排名比较。

Result: 论文通过系统评估发现，结构化知识集成能够使LLM为高维优化生成有效的预热启动。四种架构在不同维度问题上表现出不同的性能优势，表明领域知识的系统集成确实能够帮助LLM在高维优化任务中取得更好的表现。

Conclusion: 通过比较人类与人工智能生成领域知识的策略，研究表明结构化知识集成能够显著提升LLM在高维优化任务中的表现，为LLM在高维软件工程优化中的应用提供了有效方法。

Abstract: Background/Context: Large Language Models (LLMs) demonstrate strong performance on low-dimensional software engineering optimization tasks ($\le$11 features) but consistently underperform on high-dimensional problems where Bayesian methods dominate. A fundamental gap exists in understanding how systematic integration of domain knowledge (whether from humans or automated reasoning) can bridge this divide.
  Objective/Aim: We compare human versus artificial intelligence strategies for generating domain knowledge. We systematically evaluate four distinct architectures to determine if structured knowledge integration enables LLMs to generate effective warm starts for high-dimensional optimization.
  Method: We evaluate four approaches on MOOT datasets stratified by dimensionality: (1) Human-in-the-Loop Domain Knowledge Prompting (H-DKP), utilizing asynchronous expert feedback loops; (2) Adaptive Multi-Stage Prompting (AMP), implementing sequential constraint identification and validation; (3) Dimension-Aware Progressive Refinement (DAPR), conducting optimization in progressively expanding feature subspaces; and (4) Hybrid Knowledge-Model Approach (HKMA), synthesizing statistical scouting (TPE) with RAG-enhanced prompting. Performance is quantified via Chebyshev distance to optimal solutions and ranked using Scott-Knott clustering against an established baseline for LLM generated warm starts.
  Note that all human studies conducted as part of this study will comply with the policies of our local Institutional Review Board.

</details>


### [14] [A Proxy Stakeholder Approach to Requirements Engineering for Inclusive Navigation](https://arxiv.org/abs/2602.02869)
*Wei Wang,Anuradha Madugalla,John Grundy,Paul McIntosh,Charmine E. J. Härtel*

Main category: cs.SE

TL;DR: 该研究将导航重新定义为社会分布式任务，强调代理利益相关者在认知障碍者导航中的作用，提出以定制化、协作和基于日常导航为核心的设计建议。


<details>
  <summary>Details</summary>
Motivation: 认知障碍者在导航方面面临重大挑战，而主流导航技术很少考虑他们的多样化需求。研究旨在通过关注代理利益相关者（代表或协调认知障碍者进行导航的人）来开发更包容的导航技术。

Method: 采用质性主导的混合方法，包括国际调查和三阶段访谈研究，考察代理利益相关者在日常导航中使用的现实策略。

Result: 研究发现代理利益相关者采用多种策略支持导航，这些发现被综合成一套基于经验的设计建议，强调定制化、协作使用和基于日常的导航支持。

Conclusion: 通过引入代理利益相关者概念到软件工程文献，提出了更包容的需求获取方法，并为设计能更好反映认知支持复杂现实的导航技术提供了实用指导。

Abstract: Wayfinding, or the ability to navigate one's surroundings, is crucial for independent living and requires a complex combination of cognitive abilities, environmental awareness, and technology to manage this successfully. Individuals with cognitive impairment (IwCI) often face significant challenges in learning and navigating their environment. Despite its importance, mainstream navigation technologies are rarely designed with their diverse needs in mind. This study reframes the search for places as a socially distributed task and emphasizes the role of proxy stakeholders, who act on behalf or in coordination with IwCI during navigation. Using a qualitatively led mixed-methods approach, which includes an international survey and a three-stage interview study, we examine the real-world strategies that proxy stakeholders employ to support daily navigation. The findings are synthesized into a set of empirically grounded design recommendations that emphasize customisability, collaborative use, and support for routine-based navigation. Our findings highlight key challenges and adaptive practices, which are synthesized into design recommendations that prioritize customisability, routine-based navigation, and multi-user coordination. By introducing the proxy stakeholder concept into the software engineering literature, we propose a more inclusive approach to requirements elicitation and offer practical guidance for designing navigation technologies that better reflect the complex realities of cognitive support.

</details>


### [15] [Learning-Infused Formal Reasoning: From Contract Synthesis to Artifact Reuse and Formal Semantics](https://arxiv.org/abs/2602.02881)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 提出一个长期研究议程，将形式化方法与人工智能结合，通过自动化合约合成、语义构件重用和精化理论，构建下一代知识驱动的验证生态系统。


<details>
  <summary>Details</summary>
Motivation: 当前验证系统局限于孤立的正确性证明，需要转向累积性、知识驱动的范式，实现跨系统的规格、合约和证明的持续合成与转移。

Method: 提出混合框架，结合大语言模型和图表示，实现可扩展的语义匹配和验证构件的原则性重用。学习组件提供跨异构表示和抽象层次的语义指导，符号匹配确保形式正确性。

Result: 提出了一个前瞻性愿景，基于组合推理，指向能够系统演化、利用过往验证成果加速未来保证的验证生态系统。

Conclusion: 形式化方法与人工智能的交叉领域具有巨大潜力，通过知识驱动的验证范式转变，可以构建更高效、可扩展的下一代验证系统。

Abstract: This vision paper articulates a long-term research agenda for formal methods at the intersection with artificial intelligence, outlining multiple conceptual and technical dimensions and reporting on our ongoing work toward realising this agenda. It advances a forward-looking perspective on the next generation of formal methods based on the integration of automated contract synthesis, semantic artifact reuse, and refinement-based theory. We argue that future verification systems must move beyond isolated correctness proofs toward a cumulative, knowledge-driven paradigm in which specifications, contracts, and proofs are continuously synthesised and transferred across systems. To support this shift, we outline a hybrid framework combining large language models with graph-based representations to enable scalable semantic matching and principled reuse of verification artifacts. Learning-based components provide semantic guidance across heterogeneous notations and abstraction levels, while symbolic matching ensures formal soundness. Grounded in compositional reasoning, this vision points toward verification ecosystems that evolve systematically, leveraging past verification efforts to accelerate future assurance.

</details>


### [16] [Failure-Aware Enhancements for Large Language Model (LLM) Code Generation: An Empirical Study on Decision Framework](https://arxiv.org/abs/2602.02896)
*Jianru Shen,Zedong Peng,Lucy Owen*

Main category: cs.SE

TL;DR: 论文通过实证研究发现，渐进式提示在代码生成任务中达到96.9%完成率，但仍存在未满足需求。不同增强策略（自我批评、RAG等）的有效性取决于失败类型，作者据此提出了基于失败模式的决策框架。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自动化软件开发方面显示出潜力，但即使使用渐进式提示等高级工作流，仍有一些需求无法满足。现有增强方法（如自我批评、多模型协作、RAG）缺乏明确的使用指导，开发者不知道何时该使用哪种方法。

Method: 对25个GitHub项目进行实证研究，比较渐进式提示和直接提示的效果。针对6个最具代表性的项目，评估四种增强策略（自我批评、多模型协作、RAG等）在不同失败类型上的表现。

Result: 渐进式提示平均任务完成率达到96.9%，显著优于直接提示（80.5%），但仍留下8个项目未完成。不同增强策略的有效性取决于失败特征：自我批评对代码可审查的逻辑错误有效，但对外部服务集成完全无效；RAG在所有失败类型上实现最高完成率且效率最优。

Conclusion: 基于研究发现，提出了一个决策框架，将每种失败模式映射到最合适的增强方法，为从业者提供实用、数据驱动的指导，避免试错。

Abstract: Large language models (LLMs) show promise for automating software development by translating requirements into code. However, even advanced prompting workflows like progressive prompting often leave some requirements unmet. Although methods such as self-critique, multi-model collaboration, and retrieval-augmented generation (RAG) have been proposed to address these gaps, developers lack clear guidance on when to use each. In an empirical study of 25 GitHub projects, we found that progressive prompting achieves 96.9% average task completion, significantly outperforming direct prompting (80.5%, Cohen's d=1.63, p<0.001) but still leaving 8 projects incomplete. For 6 of the most representative projects, we evaluated each enhancement strategy across 4 failure types. Our results reveal that method effectiveness depends critically on failure characteristics: Self-Critique succeeds on code-reviewable logic errors but fails completely on external service integration (0% improvement), while RAG achieves highest completion across all failure types with superior efficiency. Based on these findings, we propose a decision framework that maps each failure pattern to the most suitable enhancement method, giving practitioners practical, data-driven guidance instead of trial-and-error.

</details>


### [17] [Beyond Blame: Rethinking SZZ with Knowledge Graph Search](https://arxiv.org/abs/2602.02934)
*Yu Shi,Hao Li,Bram Adams,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: AgenticSZZ：首个将时序知识图谱应用于软件演化分析的方法，将bug引入提交识别从基于blame的排序问题重构为图搜索问题，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于SZZ的方法依赖git blame，搜索空间仅限于直接修改修复行的提交。初步研究发现超过40%的bug引入提交无法通过blame单独解决，需要超越blame结果进行历史遍历。

Method: 采用两阶段方法：1) 构建时序知识图谱，编码提交的时间顺序和结构关系，从两个参考点（blame提交和bug修复提交）向后遍历文件历史扩展搜索空间；2) 利用LLM代理通过专用工具导航图谱进行候选探索和因果分析。

Result: 在三个数据集上评估，F1分数达到0.48-0.74，相比最先进方法有高达27%的统计显著提升。消融研究证实两个组件都必不可少，体现了经典的探索-利用权衡。

Conclusion: 通过将bug引入提交识别转化为图搜索问题，为软件演化分析中的时序和因果推理开辟了新的研究方向，解决了现有方法的局限性。

Abstract: Identifying Bug-Inducing Commits (BICs) is fundamental for understanding software defects and enabling downstream tasks such as defect prediction and automated program repair. Yet existing SZZ-based approaches are limited by their reliance on git blame, which restricts the search space to commits that directly modified the fixed lines. Our preliminary study on 2,102 validated bug-fixing commits reveals that this limitation is significant: over 40% of cases cannot be solved by blame alone, as 28% of BICs require traversing commit history beyond blame results and 14% are blameless.
  We present AgenticSZZ, the first approach to apply Temporal Knowledge Graphs (TKGs) to software evolution analysis. AgenticSZZ reframes BIC identification from a ranking problem over blame commits into a graph search problem, where temporal ordering is fundamental to causal reasoning about bug introduction. The approach operates in two phases: (1) constructing a TKG that encodes commits with temporal and structural relationships, expanding the search space by traversing file history backward from two reference points (blame commits and the BFC); and (2) leveraging an LLM agent to navigate the graph using specialized tools for candidate exploration and causal analysis.
  Evaluation on three datasets shows that AgenticSZZ achieves F1-scores of 0.48 to 0.74, with statistically significant improvements over state-of-the-art by up to 27%. Our ablation study confirms that both components are essential, reflecting a classic exploration-exploitation trade-off: the TKG expands the search space while the agent provides intelligent selection. By transforming BIC identification into a graph search problem, we open a new research direction for temporal and causal reasoning in software evolution analysis.

</details>


### [18] [Testing Framework Migration with Large Language Models](https://arxiv.org/abs/2602.02964)
*Altino Alves,João Eduardo Montandon,Andre Hora*

Main category: cs.SE

TL;DR: 研究评估大型语言模型（GPT-4o和Claude Sonnet 4）在自动化Python测试框架从unittest迁移到Pytest方面的能力，发现约48.5%的迁移测试能通过执行，但仍有超过一半失败。


<details>
  <summary>Details</summary>
Motivation: Python开发者面临从unittest迁移到Pytest的挑战，这个过程目前是手动且耗时的。自动化迁移可以显著减少工作量并加速测试现代化进程。

Method: 使用GPT-4o和Claude Sonnet 4两种LLM，在三种提示策略（零样本、单样本、思维链）和两种温度设置（0.0和1.0）下进行评估。首先从Top 100 Python开源项目中提取真实迁移数据集，然后实际执行LLM生成的测试迁移。

Result: 总体而言，51.5%的LLM生成的测试迁移失败，48.5%通过。Claude Sonnet 4表现出更保守的迁移策略（保留基于类的测试和遗留unittest引用），而GPT-4o更倾向于转换（如转换为基于函数的测试）。

Conclusion: LLM可以加速测试迁移，但存在局限性。不同模型表现出不同的迁移策略，需要进一步研究改进。对实践者和研究者提出了多个启示。

Abstract: Python developers rely on two major testing frameworks: \texttt{unittest} and \texttt{Pytest}. While \texttt{Pytest} offers simpler assertions, reusable fixtures, and better interoperability, migrating existing suites from \texttt{unittest} remains a manual and time-consuming process. Automating this migration could substantially reduce effort and accelerate test modernization. In this paper, we investigate the capability of Large Language Models (LLMs) to automate test framework migrations from \texttt{unittest} to \texttt{Pytest}. We evaluate GPT 4o and Claude Sonnet 4 under three prompting strategies (Zero-shot, One-shot, and Chain-of-Thought) and two temperature settings (0.0 and 1.0). To support this analysis, we first introduce a curated dataset of real-world migrations extracted from the top 100 Python open-source projects. Next, we actually execute the LLM-generated test migrations in their respective test suites. Overall, we find that 51.5% of the LLM-generated test migrations failed, while 48.5% passed. The results suggest that LLMs can accelerate test migration, but there are often caveats. For example, Claude Sonnet 4 exhibited more conservative migrations (e.g., preserving class-based tests and legacy \texttt{unittest} references), while GPT-4o favored more transformations (e.g., to function-based tests). We conclude by discussing multiple implications for practitioners and researchers.

</details>


### [19] [Understanding Bug-Reproducing Tests: A First Empirical Study](https://arxiv.org/abs/2602.02965)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 该论文对Python系统中的bug复现测试进行了首次实证研究，发现其与普通测试在代码行数、断言数量和复杂度上无显著差异，但包含更多try/except块和弱断言，且95%的bug复现测试仅针对单个bug。


<details>
  <summary>Details</summary>
Motivation: 尽管bug复现测试与普通测试共存于测试套件中，但其特性研究甚少，不清楚它们是否存在根本性差异。本研究旨在通过实证分析更好地理解bug复现测试的特性。

Method: 对15个真实Python系统中的642个bug复现测试进行实证分析，比较其与普通测试在LOC（代码行数）、断言数量、复杂度、try/except块使用以及断言类型等方面的差异。

Result: 1. bug复现测试在LOC、断言数量和复杂度方面与普通测试无统计学显著差异；2. 包含更多try/except块和弱断言（如assertNotEqual）；3. 95%的bug复现测试仅复现单个bug，5%复现多个bug。

Conclusion: bug复现测试与普通测试在多数指标上相似，但在异常处理和断言类型上存在细微差异。研究为理解bug复现测试特性提供了初步基础，并讨论了未来研究方向。

Abstract: Developers create bug-reproducing tests that support debugging by failing as long as the bug is present, and passing once the bug has been fixed. These tests are usually integrated into existing test suites and executed regularly alongside all other tests to ensure that future regressions are caught. Despite this co-existence with other types of tests, the properties of bug-reproducing tests are scarcely researched, and it remains unclear whether they differ fundamentally. In this short paper, we provide an initial empirical study to understand bug-reproducing tests better. We analyze 642 bug-reproducing tests of 15 real-world Python systems. Overall, we find that bug-reproducing tests are not (statistically significantly) different from other tests regarding LOC, number of assertions, and complexity. However, bug-reproducing tests contain slightly more try/except blocks and ``weak assertions'' (e.g.,~\texttt{assertNotEqual}). Lastly, we detect that the majority (95%) of the bug-reproducing tests reproduce a single bug, while 5% reproduce multiple bugs. We conclude by discussing implications and future research directions.

</details>


### [20] [What Do Contribution Guidelines Say About Software Testing?](https://arxiv.org/abs/2602.02966)
*Bruna Falcucci,Felipe Gomide,Andre Hora*

Main category: cs.SE

TL;DR: 对200个Python和JavaScript开源项目的贡献指南进行实证研究，发现78%的项目包含测试文档，但主要集中在如何运行测试（83.5%），较少指导如何编写测试（37%），且测试类型覆盖不均衡。


<details>
  <summary>Details</summary>
Motivation: 开源项目中测试对贡献过程至关重要，但尽管大多数项目要求贡献者编写测试，具体的测试实践指导在贡献指南中仍不明确，需要研究开源项目如何向贡献者传达测试实践。

Method: 分析200个Python和JavaScript开源软件项目的贡献指南，包括CONTRIBUTING文件、外部文档和README文件，统计测试文档的存在位置、内容和覆盖范围。

Result: 78%的项目包含测试文档，主要位于CONTRIBUTING文件（58%）、外部文档（24%）和README文件（8%）。83.5%的文档解释如何运行测试，但只有37%指导如何编写测试。测试类型覆盖不均衡：单元测试71%，集成测试20.5%，端到端测试15.5%。测试覆盖率（25.5%）和模拟（9.5%）较少讨论。

Conclusion: 开源项目在贡献指南中提供了基本的测试文档，但存在重要缺陷：缺乏编写测试的指导，测试类型覆盖不完整，关键测试概念讨论不足。需要改进贡献指南以更好地支持贡献者进行有效的软件测试。

Abstract: Software testing plays a crucial role in the contribution process of open-source projects. For example, contributions introducing new features are expected to include tests, and contributions with tests are more likely to be accepted. Although most real-world projects require contributors to write tests, the specific testing practices communicated to contributors remain unclear. In this paper, we present an empirical study to understand better how software testing is approached in contribution guidelines. We analyze the guidelines of 200 Python and JavaScript open-source software projects. We find that 78\% of the projects include some form of test documentation for contributors. Test documentation is located in multiple sources, including \texttt{CONTRIBUTING} files (58\%), external documentation (24\%), and \texttt{README} files (8\%). Furthermore, test documentation commonly explains how to run tests (83.5\%), but less often provides guidance on how to write tests (37\%). It frequently covers unit tests (71\%), but rarely addresses integration (20.5\%) and end-to-end tests (15.5\%). Other key testing aspects are also less frequently discussed: test coverage (25.5\%) and mocking (9.5\%). We conclude by discussing implications and future research.

</details>


### [21] [Maintaining the Heterogeneity in the Organization of Software Engineering Research](https://arxiv.org/abs/2602.03093)
*Yang Yue,Zheng Jiang,Yi Wang*

Main category: cs.SE

TL;DR: 软件工程研究组织中的异质性（资助研究模式与动手实践模式并存）正受到威胁，作者呼吁社区维护这种多样性


<details>
  <summary>Details</summary>
Motivation: 软件工程研究历史上存在两种组织模式：资助研究模式和动手实践模式，这种异质性使软件工程在过去50年成为繁荣的跨学科领域。然而，近年来资助研究模式逐渐占据主导地位，这种异质性正受到严重系统性威胁。

Method: 本文采用论述性方法，首先解释软件工程研究组织为何需要异质性，然后呈现当前软件工程研究的趋势、后果及潜在未来。

Result: 分析表明，软件工程研究组织的异质性正在被削弱，资助研究模式日益主导，这可能对软件工程领域的多样性和创新产生负面影响。

Conclusion: 选择权在我们手中，作者强烈呼吁软件工程社区认真考虑维护研究组织中的异质性，以保持该领域的活力和创新性。

Abstract: The heterogeneity in the organization of software engineering (SE) research historically exists, i.e., funded research model and hands-on model, which makes software engineering become a thriving interdisciplinary field in the last 50 years. However, the funded research model is becoming dominant in SE research recently, indicating such heterogeneity has been seriously and systematically threatened. In this essay, we first explain why the heterogeneity is needed in the organization of SE research, then present the current trend of SE research nowadays, as well as the consequences and potential futures. The choice is at our hands, and we urge our community to seriously consider maintaining the heterogeneity in the organization of software engineering research.

</details>


### [22] [Synthesizing File-Level Data for Unit Test Generation with Chain-of-Thoughts via Self-Debugging](https://arxiv.org/abs/2602.03181)
*Ziyue Hua,Tianyu Chen,Yeyun Gong,Shuai Lu,Peng Cheng,Qinglin Zhu,Yibo He,Yingjie Fu,Wenpin Jiao,Wei Yang,Tao Xie*

Main category: cs.SE

TL;DR: 提出基于自调试的数据蒸馏方法，生成高质量单元测试训练数据，通过监督微调显著提升LLM生成单元测试的效果。


<details>
  <summary>Details</summary>
Motivation: 现有单元测试生成方法（包括符号执行、基于搜索的方法和LLM生成器）难以生成具有正确断言和可靠思维链解释的人类质量测试。现有训练数据存在缺陷：仓库挖掘的测试缺乏开发者思维链，而LLM蒸馏的思维链往往不正确或不完整。

Method: 提出新颖的数据蒸馏方法，结合(1)引导式测试修复：包含错误、失败和覆盖率聚焦步骤的启发式循环，让模型诊断并迭代修复生成的测试；(2)思维链压缩：将原始和调试思维链压缩为直接证明测试正确的简洁解释。将此流程应用于开源项目构建高质量数据集，用于监督微调基础模型。

Result: 构建了包含74,518个高质量<焦点方法、测试、思维链>示例的数据集。微调后的模型在单元测试生成方面表现优异：测试断言通过率达到36.17%，分支覆盖率达到43.90%，变异测试得分达到88.66%，显著优于o4-mini等最先进的商业模型。

Conclusion: 通过自调试数据蒸馏方法生成的高质量训练数据，能够显著提升LLM生成单元测试的能力，在多个评估指标上超越现有最先进模型，为解决单元测试生成中的思维链质量问题提供了有效方案。

Abstract: Automatic unit test (UT) generation is essential for software quality assurance, but existing approaches--including symbolic execution, search-based approaches, and recent LLM-based generators--struggle to produce human-quality tests with correct, meaningful assertions and reliable chain-of-thought (CoT) explanations. We identify a gap in UT training data: repository-mined tests lack developer CoTs, while LLM-distilled CoTs are often incorrect or incomplete. To address this issue, we propose a novel data-distillation approach that uses self-debugging to produce high-quality UT training examples paired with faithful CoTs. Our approach combines (1) guided test repair, a heuristic loop (error-, failure-, and coverage-focused steps) that asks the used model to diagnose and iteratively fix generated tests, and (2) CoT compression, which compacts original and debugging CoTs into concise explanations that directly justify correct tests. We apply this pipeline to a large corpus of open-source projects to construct a dataset of 74,518 high-quality <focal method, test, CoT> examples, and then use it for supervised fine-tuning of a base model. An empirical evaluation shows that the fine-tuned model achieves high UT generation effectiveness: it attains a pass rate of 36.17% on test assertions, a branch coverage of 43.90%, and a mutation score of 88.66%, substantially higher than state-of-the-art commercial models like o4-mini.

</details>


### [23] [Multi-Level Testing of Conversational AI Systems](https://arxiv.org/abs/2602.03311)
*Elena Masserini*

Main category: cs.SE

TL;DR: 该博士论文提出针对会话AI系统的新型测试方法，关注从语言与AI组件集成到单代理和多代理系统的多层次验证


<details>
  <summary>Details</summary>
Motivation: 现有测试方案难以适应会话交互特性和AI组件行为，需要专门针对会话AI系统的测试方法

Method: 研究新型测试方法家族，关注不同粒度层次的验证：语言与AI组件集成、单个会话代理、多代理实现

Result: 提出了专门针对会话AI系统的多层次测试框架

Conclusion: 该研究填补了会话AI系统测试方法的空白，为这类系统的质量保证提供了系统化解决方案

Abstract: Conversational AI systems combine AI-based solutions with the flexibility of conversational interfaces. However, most existing testing solutions do not straightforwardly adapt to the characteristics of conversational interaction or to the behavior of AI components. To address this limitation, this Ph.D. thesis investigates a new family of testing approaches for conversational AI systems, focusing on the validation of their constituent elements at different levels of granularity, from the integration between the language and the AI components, to individual conversational agents, up to multi-agent implementations of conversational AI systems

</details>


### [24] [Precision in Practice: Knowledge Guided Code Summarizing Grounded in Industrial Expectations](https://arxiv.org/abs/2602.03400)
*Jintai Li,Songqiang Chen,Shuo Jin,Xiaoyuan Xie*

Main category: cs.SE

TL;DR: 本文提出ExpSum方法，通过整合函数元数据抽象、信息过滤、上下文感知领域知识检索和约束驱动提示，生成符合开发者期望的工业级代码摘要，在HarmonyOS项目中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码摘要生成方面取得进展，但在工业环境中生成的摘要实用性不足。研究发现超过57.4%的现有方法生成的摘要因不符合开发者对工业文档的期望而被拒绝，开发者需要适当的领域术语、明确的功能分类，并避免冗余实现细节。

Method: 提出ExpSum方法，包含四个关键组件：1) 函数元数据抽象，2) 信息元数据过滤，3) 上下文感知领域知识检索，4) 约束驱动提示，指导LLM生成结构化、符合期望的摘要。

Result: 在HarmonyOS项目和广泛使用的代码摘要基准测试中，ExpSum始终优于所有基线方法，在HarmonyOS上BLEU-4提升26.71%，ROUGE-L提升20.10%。基于LLM的评估表明，ExpSum生成的摘要在其他项目中也能更好地符合开发者期望。

Conclusion: ExpSum通过整合开发者期望的关键要素，显著提高了工业代码摘要的质量和实用性，为工业代码文档提供了有效的解决方案。

Abstract: Code summaries are essential for helping developers understand code functionality and reducing maintenance and collaboration costs. Although recent advances in large language models (LLMs) have significantly improved automatic code summarization, the practical usefulness of generated summaries in industrial settings remains insufficiently explored. In collaboration with documentation experts from the industrial HarmonyOS project, we conducted a questionnaire study showing that over 57.4% of code summaries produced by state-of-the-art approaches were rejected due to violations of developers' expectations for industrial documentation. Beyond semantic similarity to reference summaries, developers emphasize additional requirements, including the use of appropriate domain terminology, explicit function categorization, and the avoidance of redundant implementation details.
  To address these expectations, we propose ExpSum, an expectation-aware code summarization approach that integrates function metadata abstraction, informative metadata filtering, context-aware domain knowledge retrieval, and constraint-driven prompting to guide LLMs in generating structured, expectation-aligned summaries. We evaluate ExpSum on the HarmonyOS project and widely used code summarization benchmarks. Experimental results show that ExpSum consistently outperforms all baselines, achieving improvements of up to 26.71% in BLEU-4 and 20.10% in ROUGE-L on HarmonyOS. Furthermore, LLM-based evaluations indicate that ExpSum-generated summaries better align with developer expectations across other projects, demonstrating its effectiveness for industrial code documentation.

</details>


### [25] [SWE-Master: Unleashing the Potential of Software Engineering Agents via Post-Training](https://arxiv.org/abs/2602.03411)
*Huatong Song,Lisheng Huang,Shuang Sun,Jinhao Jiang,Ran Le,Daixuan Cheng,Guoxin Chen,Yiwen Hu,Zongchao Chen,Wayne Xin Zhao,Yang Song,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: SWE-Master是一个开源、可复现的软件工程智能体训练框架，通过系统化的后训练方法显著提升模型在软件工程任务上的解决能力，在SWE-bench Verified基准上达到61.4%的解决率，结合测试时扩展后可达70.8%。


<details>
  <summary>Details</summary>
Motivation: 现有开源模型在软件工程任务上的能力有限，需要系统化的训练框架来提升智能体在长视野软件工程任务上的解决能力，同时推动可复现的研究。

Method: 系统探索完整的智能体开发流程：包括教师轨迹合成与数据整理、长视野监督微调、基于真实执行反馈的强化学习、以及推理框架设计。

Result: 在SWE-bench Verified基准上，使用Qwen2.5-Coder-32B模型达到61.4%的解决率，显著优于现有开源基线；结合测试时扩展（TTS@8）后提升至70.8%。

Conclusion: SWE-Master展示了系统化优化方法能够有效激发模型在软件工程任务上的强大能力，为软件工程智能体的可复现研究提供了实用且透明的基础。

Abstract: In this technical report, we present SWE-Master, an open-source and fully reproducible post-training framework for building effective software engineering agents. SWE-Master systematically explores the complete agent development pipeline, including teacher-trajectory synthesis and data curation, long-horizon SFT, RL with real execution feedback, and inference framework design. Starting from an open-source base model with limited initial SWE capability, SWE-Master demonstrates how systematical optimization method can elicit strong long-horizon SWE task solving abilities. We evaluate SWE-Master on SWE-bench Verified, a standard benchmark for realistic software engineering tasks. Under identical experimental settings, our approach achieves a resolve rate of 61.4\% with Qwen2.5-Coder-32B, substantially outperforming existing open-source baselines. By further incorporating test-time scaling~(TTS) with LLM-based environment feedback, SWE-Master reaches 70.8\% at TTS@8, demonstrating a strong performance potential. SWE-Master provides a practical and transparent foundation for advancing reproducible research on software engineering agents. The code is available at https://github.com/RUCAIBox/SWE-Master.

</details>


### [26] [SWE-World: Building Software Engineering Agents in Docker-Free Environments](https://arxiv.org/abs/2602.03419)
*Shuang Sun,Huatong Song,Lisheng Huang,Jinhao Jiang,Ran Le,Zhihao Lv,Zongchao Chen,Yiwen Hu,Wenyang Luo,Wayne Xin Zhao,Yang Song,Hongteng Xu,Tao Zhang,Ji-Rong Wen*

Main category: cs.SE

TL;DR: SWE-World是一个无需Docker的软件工程代理训练框架，使用学习到的执行环境替代物理容器化环境，显著提升代码修改任务的性能


<details>
  <summary>Details</summary>
Motivation: 现有软件工程代理依赖容器化环境的执行反馈，这种方法资源密集且难以维护，限制了代理训练的可扩展性

Method: 提出SWE-World框架，利用基于LLM的模型在真实代理-环境交互数据上训练，预测中间执行结果和最终测试反馈，替代物理执行环境

Result: 在SWE-bench Verified上，SWE-World将Qwen2.5-Coder-32B从6.2%提升到52.0%（无Docker SFT）、55.0%（无Docker RL）和68.2%（进一步TTS）

Conclusion: SWE-World通过消除物理环境依赖，实现了高效的软件工程代理训练和评估，支持测试时扩展，显著提升性能同时降低资源消耗

Abstract: Recent advances in large language models (LLMs) have enabled software engineering agents to tackle complex code modification tasks. Most existing approaches rely on execution feedback from containerized environments, which require dependency-complete setup and physical execution of programs and tests. While effective, this paradigm is resource-intensive and difficult to maintain, substantially complicating agent training and limiting scalability. We propose SWE-World, a Docker-free framework that replaces physical execution environments with a learned surrogate for training and evaluating software engineering agents. SWE-World leverages LLM-based models trained on real agent-environment interaction data to predict intermediate execution outcomes and final test feedback, enabling agents to learn without interacting with physical containerized environments. This design preserves the standard agent-environment interaction loop while eliminating the need for costly environment construction and maintenance during agent optimization and evaluation. Furthermore, because SWE-World can simulate the final evaluation outcomes of candidate trajectories without real submission, it enables selecting the best solution among multiple test-time attempts, thereby facilitating effective test-time scaling (TTS) in software engineering tasks. Experiments on SWE-bench Verified demonstrate that SWE-World raises Qwen2.5-Coder-32B from 6.2\% to 52.0\% via Docker-free SFT, 55.0\% with Docker-free RL, and 68.2\% with further TTS. The code is available at https://github.com/RUCAIBox/SWE-World

</details>


### [27] [RAL-Bench: Benchmarking for Application-Level Functional Correctness and Non-Functional Quality Attributes](https://arxiv.org/abs/2602.03462)
*Ruwei Pan,Yakun Zhang,Qingyuan Liang,Yueheng Zhu,Chao Liu,Lu Zhang,Hongyu Zhang*

Main category: cs.SE

TL;DR: RAL-Bench是一个应用级代码生成基准测试框架，评估LLM生成可运行多文件仓库的能力，涵盖功能正确性和非功能性质量（可维护性、安全性等）。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在应用级代码生成评估方面有限，需要同时评估功能正确性和非功能性质量，以了解LLM能否生成满足实际软件需求的代码仓库。

Method: 从高质量参考项目中提取自然语言需求，构建覆盖功能和非功能属性的黑盒系统测试，使用参考仓库验证测试，功能正确性通过测试通过率衡量，非功能性质量基于ISO/IEC 25010五个维度，使用AHP权重向量聚合。

Result: 评估16个LLM的零样本贪婪解码结果显示，功能正确性是主要瓶颈：在需求驱动、参考验证的测试下，没有模型超过45%的功能通过率。

Conclusion: 当前LLM在应用级代码生成中功能正确性仍有显著不足，RAL-Bench提供了一个全面的评估框架，有助于推动应用级代码生成研究。

Abstract: Code generation has advanced rapidly with code-focused large language models (LLMs), especially on snippet-level tasks. However, application-level generation requires producing a runnable multi-file repository with correct structure, dependencies, and end-to-end executability, and real-world software must satisfy both functional correctness and non-functional quality (e.g., maintainability, security). Existing benchmarks provide a limited execution-based assessment of these requirements at the application level. We ask: Can current LLMs generate application-level repositories that meet both functional and non-functional criteria? We propose RAL-Bench, a benchmark and evaluation framework for application-level code generation. For each task, we distill a concise natural-language requirement from a high-quality reference project, build black-box system tests covering functional and non-functional attributes, and keep only tests that pass on the reference repository to ensure a sound oracle and an end-to-end executable suite. Functional correctness is measured by system-test pass rate. Non-functional quality is measured along five ISO/IEC 25010-inspired dimensions and aggregated with an Analytic Hierarchy Process (AHP)-derived weight vector, with per-dimension diagnostics and baseline-normalized scoring using reference measurements. Across 16 LLMs evaluated zero-shot with greedy decoding, functional correctness is the dominant bottleneck: no model exceeds a 45% functional pass rate under our requirement-driven, reference-validated tests. We release RAL-Bench at https://github.com/Wwstarry/RAL-Bench. .

</details>


### [28] [Formal Evidence Generation for Assurance Cases for Robotic Software Models](https://arxiv.org/abs/2602.03550)
*Fang Yan,Simon Foster,Ana Cavalcanti,Ibrahim Habli,James Baxter*

Main category: cs.SE

TL;DR: 提出基于模型的系统化方法，通过将形式化验证嵌入保证工作流，自动生成保证案例证据


<details>
  <summary>Details</summary>
Motivation: 机器人和自主系统在安全关键领域部署增加，需要证明其安全性。保证案例（ACs）虽然提供结构化论证，但证据生成和维护工作量大、易出错，且系统演化时难以保持一致性。

Method: 基于RoboChart领域特定建模语言，结合模型检查和定理证明。使用模板从自然语言需求系统化推导形式化断言，协调多种形式化验证工具处理不同属性类型，并将形式化证据生产集成到工作流中。

Result: 结构化需求通过预定义模板自动转换为形式化断言，验证结果自动集成为证据。案例研究证明了该方法的有效性。

Conclusion: 该方法解决了保证案例证据生成的关键挑战，通过系统化集成形式化验证，提高了安全关键系统保证工作的效率和可靠性。

Abstract: Robotics and Autonomous Systems are increasingly deployed in safety-critical domains, so that demonstrating their safety is essential. Assurance Cases (ACs) provide structured arguments supported by evidence, but generating and maintaining this evidence is labour-intensive, error-prone, and difficult to keep consistent as systems evolve. We present a model-based approach to systematically generating AC evidence by embedding formal verification into the assurance workflow. The approach addresses three challenges: systematically deriving formal assertions from natural language requirements using templates, orchestrating multiple formal verification tools to handle diverse property types, and integrating formal evidence production into the workflow. Leveraging RoboChart, a domain-specific modelling language with formal semantics, we combine model checking and theorem proving in our approach. Structured requirements are automatically transformed into formal assertions using predefined templates, and verification results are automatically integrated as evidence. Case studies demonstrate the effectiveness of our approach.

</details>


### [29] [Flaky Tests in a Large Industrial Database Management System: An Empirical Study of Fixed Issue Reports for SAP HANA](https://arxiv.org/abs/2602.03556)
*Alexander Berndt,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: 利用LLMs标注方法分析SAP HANA中测试不稳定的根本原因，发现并发问题是主要根源（占23%），不同测试类型面临不同的不稳定性挑战。


<details>
  <summary>Details</summary>
Motivation: 测试不稳定性（flaky tests）会导致代码质量评估的模糊信号，干扰代码变更的自动化评估。由于手动标注不稳定测试的根本原因耗时费力，且不同编程语言、应用领域或项目规模下的主要原因可能不同，需要一种自动化的标注方法来分析工业系统中的不稳定测试模式。

Method: 提出LLMs-as-annotators方法，利用模型内和模型间的一致性来标注与已修复不稳定性问题相关的issue报告，识别相关的根本原因类别。该方法在SAP HANA（大型工业数据库管理系统）的背景下进行评估。

Result: 在分析的559个issue报告中，SAP HANA测试最常见的问题与并发相关（23%，130个报告）。研究还表明，不同的测试类型面临不同的不稳定性挑战。

Conclusion: 并发是SAP HANA测试不稳定性的主要根源。建议未来关于不稳定性缓解的研究应考虑评估所提方法在不同测试类型间的泛化能力，因为不同测试类型面临不同的挑战。

Abstract: Flaky tests yield different results when executed multiple times for the same version of the source code. Thus, they provide an ambiguous signal about the quality of the code and interfere with the automated assessment of code changes. While a variety of factors can cause test flakiness, approaches to fix flaky tests are typically tailored to address specific causes. However, the prevalent root causes of flaky tests can vary depending on the programming language, application domain, or size of the software project. Since manually labeling flaky tests is time-consuming and tedious, this work proposes an LLMs-as-annotators approach that leverages intra- and inter-model consistency to label issue reports related to fixed flakiness issues with the relevant root cause category. This allows us to gain an overview of prevalent flakiness categories in the issue reports. We evaluated our labeling approach in the context of SAP HANA, a large industrial database management system. Our results suggest that SAP HANA's tests most commonly suffer from issues related to concurrency (23%, 130 of 559 analyzed issue reports). Moreover, our results suggest that different test types face different flakiness challenges. Therefore, we encourage future research on flakiness mitigation to consider evaluating the generalizability of proposed approaches across different test types.

</details>


### [30] [Scaling Test-Driven Code Generation from Functions to Classes: An Empirical Study](https://arxiv.org/abs/2602.03557)
*Yunhao Liang,Ruixuan Ying,Shiwen Ni,Zhe Cui*

Main category: cs.SE

TL;DR: 该论文提出了一种将测试驱动开发(TDD)从函数级代码生成扩展到类级代码生成的迭代框架，通过分析类内方法依赖关系制定生成计划，并利用反射式执行反馈和有限修复迭代来增量实现方法。


<details>
  <summary>Details</summary>
Motivation: 现有TDD风格的代码生成研究主要局限于函数级任务，而类级合成中多个方法通过共享状态和调用依赖交互的问题尚未充分探索。需要将测试驱动的代码生成从函数扩展到类，以提高类级代码生成的可靠性。

Method: 提出迭代TDD框架：1) 分析类内方法依赖关系以制定可行的生成计划；2) 在方法级公共测试下增量实现每个方法，利用反射式执行反馈和有限修复迭代。构建ClassEval-TDD数据集，包含一致的规范、确定性测试环境和完整的方法级公共测试。

Result: 在八个LLM上的实证研究表明，类级TDD框架将类级正确性提高了12-26个绝对百分点，达到最高71%完全正确的类，平均只需要少量修复。相比最强的直接生成基线（整体、增量和组合策略的最佳表现）有显著改进。

Conclusion: 测试驱动生成可以有效地扩展到孤立函数之外，显著提高类级代码生成的可靠性。该方法为复杂类级代码合成提供了实用的解决方案。

Abstract: Test-driven development (TDD) has been adopted to improve Large Language Model (LLM)-based code generation by using tests as executable specifications. However, existing TDD-style code generation studies are largely limited to function-level tasks, leaving class-level synthesis where multiple methods interact through shared state and call dependencies underexplored. In this paper, we scale test-driven code generation from functions to classes via an iterative TDD framework. Our approach first analyzes intra-class method dependencies to derive a feasible generation schedule, and then incrementally implements each method under method-level public tests with reflection-style execution feedback and bounded repair iterations. To support test-driven generation and rigorous class-level evaluation, we construct ClassEval-TDD, a cleaned and standardized variant of ClassEval with consistent specifications, deterministic test environments, and complete method-level public tests. We conduct an empirical study across eight LLMs and compare against the strongest direct-generation baseline (the best of holistic, incremental, and compositional strategies). Our class-level TDD framework consistently improves class-level correctness by 12 to 26 absolute points and achieves up to 71% fully correct classes, while requiring only a small number of repairs on average. These results demonstrate that test-driven generation can effectively scale beyond isolated functions and substantially improve class-level code generation reliability. All code and data are available at https://anonymous.4open.science/r/ClassEval-TDD-C4C9/

</details>


### [31] [Causal Inference for the Effect of Code Coverage on Bug Introduction](https://arxiv.org/abs/2602.03585)
*Lukas Schulte,Gordon Fraser,Steffen Herbold*

Main category: cs.SE

TL;DR: 该研究使用因果推断方法量化代码覆盖率对bug引入的影响，通过广义倾向得分调整和双重稳健回归分析JavaScript/TypeScript项目中覆盖率与bug引入的因果关系。


<details>
  <summary>Details</summary>
Motivation: 代码覆盖率被广泛用作软件质量保证指标，但其效果和适当剂量在研究和工程社区中存在争议。先前工作仅报告相关性关联，结果容易受到混杂因素影响，需要量化覆盖率对bug引入的因果效应。

Method: 构建因果有向无环图识别软件工程过程中的混杂因素，从源代码、问题跟踪系统、评审系统和持续集成中建模关键变量。使用广义倾向得分调整，对连续暴露（覆盖率）应用双重稳健回归的因果推断方法，分析bug引入和非bug引入变更的数据集。

Result: 研究将估计平均处理效应和剂量-响应关系，以检查数据集中项目内潜在的非线性模式（如阈值或收益递减效应）。

Conclusion: 该研究旨在为JavaScript/TypeScript开源项目中代码覆盖率对bug引入的因果效应提供量化证据，帮助确定最佳覆盖率水平。

Abstract: Context: Code coverage is widely used as a software quality assurance measure. However, its effect, and specifically the advisable dose, are disputed in both the research and engineering communities. Prior work reports only correlational associations, leaving results vulnerable to confounding factors. Objective: We aim to quantify the causal effect of code coverage (exposure) on bug introduction (outcome) in the context of mature JavaScript and TypeScript open source projects, addressing both the overall effect and its variance across coverage levels. Method: We construct a causal directed acyclic graph to identify confounders within the software engineering process, modeling key variables from the source code, issue- and review systems, and continuous integration. Using generalized propensity score adjustment, we will apply doubly robust regression-based causal inference for continuous exposure to a novel dataset of bug-introducing and non-bug-introducing changes. We estimate the average treatment effect and dose-response relationship to examine potential non-linear patterns (e.g., thresholds or diminishing returns) within the projects of our dataset.

</details>


### [32] [Beyond the Commit: Developer Perspectives on Productivity with AI Coding Assistants](https://arxiv.org/abs/2602.03593)
*Valerie Chen,Jasmyn He,Behnjamin Williams,Jason Valentino,Ameet Talwalkar*

Main category: cs.SE

TL;DR: 该研究通过混合方法分析AI编程助手对开发者生产力的影响，发现需要多维度评估方法，强调长期指标的重要性


<details>
  <summary>Details</summary>
Motivation: 在AI编程助手时代，需要重新评估如何测量其对开发者生产力的影响，并检验现有测量框架是否仍然适用

Method: 采用混合研究方法，在BNY Mellon进行包含2989份开发者调查和11次深度访谈的实证研究

Result: 调查显示对AI工具有用性存在矛盾观点，访谈识别出六个不同因素，涵盖短期和长期生产力维度，强调技术专长和工作所有权等长期指标的重要性

Conclusion: 需要多维度方法测量AI生产力影响，未来研究应纳入更广泛的人本因素，行业应采用更全面的开发者生产力评估方法

Abstract: Measuring developer productivity is a topic that has attracted attention from both academic research and industrial practice. In the age of AI coding assistants, it has become even more important for both academia and industry to understand how to measure their impact on developer productivity, and to reconsider whether earlier measures and frameworks still apply. This study analyzes the validity of different approaches to evaluating the productivity impacts of AI coding assistants by leveraging mixed-method research. At BNY Mellon, we conduct a survey with 2989 developer responses and 11 in-depth interviews. Our findings demonstrate that a multifaceted approach is needed to measure AI productivity impacts: survey results expose conflicting perspectives on AI tool usefulness, while interviews elicit six distinct factors that capture both short-term and long-term dimensions of productivity. In contrast to prior work, our factors highlight the importance of long-term metrics like technical expertise and ownership of work. We hope this work encourages future research to incorporate a broader range of human-centered factors, and supports industry in adopting more holistic approaches to evaluating developer productivity.

</details>


### [33] [CALM: A Self-Adaptive Orchestration Approach for QoS-Aware Routing in Small Language Model based Systems](https://arxiv.org/abs/2602.03632)
*Hemang Jain,Divyansh Pandey,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: CALM是一个基于MAPE-K的自适应编排机制，通过协调多个小型语言模型（SLM）组成的舰队来动态适应运行时不确定性，相比单一LLM基线降低40%延迟和50%能耗。


<details>
  <summary>Details</summary>
Motivation: AI系统面临运行时不确定性（动态工作负载、资源需求、模型漂移等），严重影响QoS。LLM系统存在资源密集或隐私/成本问题，而单个SLM难以满足多样化需求。需要协调多个SLM来动态适应变化。

Method: 提出CALM：基于MAPE-K的自适应编排机制。持续监控用户查询，分析各SLM的QoS指标，识别最优SLM，路由查询到选定SLM，并利用缓存和调度决定哪些SLM保持在内存中。

Result: CALM相比单一LLM基线，延迟降低约40%，能耗降低50%，同时保持领域特定任务性能。

Conclusion: 通过协调多个SLM组成的舰队，结合智能编排和持续自适应，可以有效应对AI系统的运行时不确定性，显著提升QoS（延迟和能耗），同时保持任务性能。

Abstract: AI-enabled systems are subjected to various types of runtime uncertainties, ranging from dynamic workloads, resource requirements, model drift, etc. These uncertainties have a big impact on the overall Quality of Service (QoS). This is particularly true in the case of Language Model (LM) enabled systems where the autoregressive nature of token generation introduces variability in latency, energy usage and response quality. These systems, powered by LLMs, are either resource-intensive (if run on-prem) or raise privacy/cost concerns (if leveraged using APIs). While deploying a Small Language Model (SLM) can be resource-efficient, it often falls short in addressing the diversity and scale of real-world requirements. To this, we argue that, rather than relying on any one SLM, leveraging a coordinated fleet of SLMs, each with specialized strengths can enable systems to dynamically adapt to shifting contexts and workload patterns. However, realizing the full potential of such an approach demands intelligent orchestration and continuous adaptation. To this end, we introduce CALM , a self-adaptive orchestration mechanism based on MAPE-K. Our approach continuously monitors user queries, analyzes the QoS metrics of the SLMs, identifies the optimal SLM to be used, routes the query to the identified SLM and further to enhance the effectiveness and efficiency, leverages caching and scheduling to decide the SLMs to be kept in memory. Our evaluation shows that CALM reduces latency by approximately 40% and energy consumption by 50%, while preserving domain-specific task performance when compared to single-LLM baselines.

</details>


### [34] [SWE-Refactor: A Repository-Level Benchmark for Real-World LLM-Based Code Refactoring](https://arxiv.org/abs/2602.03712)
*Yisen Xu,Jinqiu Yang,Tse-Hsun,Chen*

Main category: cs.SE

TL;DR: SWE-Refactor是一个新的代码重构基准测试，包含1,099个开发者编写的Java重构实例，用于评估LLM在代码重构任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有重构基准测试存在三个主要问题：重构场景覆盖有限、实例中混杂了与重构无关的修改、缺乏真实的仓库级上下文评估。这些问题限制了LLM在代码重构任务上的准确评估。

Method: 从18个Java项目中挖掘出1,099个开发者编写的、行为保持的重构实例（包括922个原子重构和177个复合重构），通过编译、测试执行和自动化重构检测工具验证正确性，然后评估9个广泛使用的LLM模型。

Result: 评估结果显示，复杂和复合重构是失败的主要来源，OpenAI Codex代理在复合实例上仅达到39.4%的成功率。模型在原子重构上表现较好，但在复杂场景中仍有显著挑战。

Conclusion: SWE-Refactor基准测试解决了现有重构评估的不足，为LLM-based代码重构研究提供了更可靠的评估框架。复杂重构仍然是LLM面临的主要挑战，需要进一步研究改进。

Abstract: Large Language Models (LLMs) have recently attracted wide interest for tackling software engineering tasks. In contrast to code generation, refactoring demands precise, semantics-preserving edits that improve program structure, which also makes automated evaluation challenging. However, existing refactoring benchmarks commonly suffer from three shortcomings: limited coverage of refactoring scenarios, the inclusion of instances that mix refactoring with unrelated changes, and insufficient repository-level context for realistic assessment. To mitigate these issues, we introduce SWE-Refactor, a new benchmark for LLM-based code refactoring. SWE-Refactor comprises 1,099 developer-written, behavior-preserving refactorings mined from 18 Java projects, including 922 atomic and 177 compound instances. Each instance is validated via compilation, test execution, and automated refactoring detection tools to ensure correctness. We evaluate nine widely used LLMs on SWE-Refactor, covering models such as GPT-4o-mini, DeepSeek-V3, and CodeLLaMa, to provide representative reference results. Our results show that complex and compound refactorings remain the primary source of failures; notably, an OpenAI Codex agent achieves only 39.4% success on compound instances. We release SWE-Refactor and all evaluation results to facilitate future research on LLM-based code refactoring.

</details>


### [35] [Improving Deep Learning Library Testing with Machine Learning](https://arxiv.org/abs/2602.03755)
*Facundo Molina,M M Abid Naziri,Feiran Qin,Alessandra Gorla,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: 使用机器学习分类器学习深度学习API的输入约束，通过张量形状抽象降低问题维度，提高API规范挖掘精度，显著提升现有bug检测工具的效果。


<details>
  <summary>Details</summary>
Motivation: TensorFlow和PyTorch等深度学习库设计复杂易产生bug，现有bug检测技术因缺乏精确API规范而产生大量误报，而现有的API规范挖掘方法准确性不足。

Method: 提出使用ML分类器判断输入有效性：1) 假设张量形状是编码具体输入和捕获数据关系的精确抽象；2) 形状抽象大幅降低问题维度便于ML训练；3) 通过观察运行时结果获取标注数据；4) 在标注输入集上训练分类器捕获API约束。

Result: 在TensorFlow和PyTorch的183个API上评估，分类器在未见数据上泛化能力良好，准确率超过91%。集成到ACETest（最先进的bug检测技术）后，其通过率从约29%提升到约61%。

Conclusion: ML增强的输入分类是扩展深度学习库测试的重要辅助手段，能够有效提高API规范挖掘精度，从而提升bug检测工具的实用性。

Abstract: Deep Learning (DL) libraries like TensorFlow and Pytorch simplify machine learning (ML) model development but are prone to bugs due to their complex design. Bug-finding techniques exist, but without precise API specifications, they produce many false alarms. Existing methods to mine API specifications lack accuracy. We explore using ML classifiers to determine input validity. We hypothesize that tensor shapes are a precise abstraction to encode concrete inputs and capture relationships of the data. Shape abstraction severely reduces problem dimensionality, which is important to facilitate ML training. Labeled data are obtained by observing runtime outcomes on a sample of inputs and classifiers are trained on sets of labeled inputs to capture API constraints. Our evaluation, conducted over 183 APIs from TensorFlow and Pytorch, shows that the classifiers generalize well on unseen data with over 91% accuracy. Integrating these classifiers into the pipeline of ACETest, a SoTA bug-finding technique, improves its pass rate from ~29% to ~61%. Our findings suggest that ML-enhanced input classification is an important aid to scale DL library testing.

</details>


### [36] [FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation](https://arxiv.org/abs/2602.03798)
*Zimu Lu,Houxing Ren,Yunqiao Yang,Ke Wang,Zhuofan Zong,Mingjie Zhan,Hongsheng Li*

Main category: cs.SE

TL;DR: FullStack-Agent是一个用于全栈Web应用开发的统一代理系统，包含多代理框架、数据扩展自学习方法以及全栈基准测试，显著提升了前端、后端和数据库功能的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码代理主要生成前端网页，缺乏真正的全栈数据处理和存储能力。构建生产级全栈Web应用比仅生成前端页面更具挑战性，需要精心控制数据流、理解不断更新的包和依赖关系，以及准确定位代码库中的隐蔽bug。

Method: 提出FullStack-Agent系统，包含三个部分：1) FullStack-Dev：具有强大规划、代码编辑、代码库导航和bug定位能力的多代理框架；2) FullStack-Learn：通过反向翻译爬取和合成的网站仓库来改进骨干LLM的创新数据扩展和自学习方法；3) FullStack-Bench：系统测试生成网站前端、后端和数据库功能的综合基准。

Result: FullStack-Dev在基准测试中，前端、后端和数据库测试用例分别比先前最先进方法提升了8.7%、38.2%和15.9%。FullStack-Learn通过自改进，使30B模型在三个测试集上的性能分别提升了9.7%、9.5%和2.8%。

Conclusion: FullStack-Agent系统有效解决了全栈Web应用开发的挑战，通过多代理框架、数据驱动的自改进方法和全面基准测试，显著提升了LLM在全栈编码任务中的性能。

Abstract: Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase. To address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts: (1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities. (2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev. (3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website. Our FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively. Additionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach. The code is released at https://github.com/mnluzimu/FullStack-Agent.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [37] [ResQ: Realistic Performance-Aware Query Generation](https://arxiv.org/abs/2602.02999)
*Zhengle Wang,Yanfei Zhang,Chunwei Liu*

Main category: cs.DB

TL;DR: ResQ是一个细粒度工作负载合成系统，能够从匿名性能追踪生成可执行的SQL工作负载，匹配查询执行目标和操作符分布。


<details>
  <summary>Details</summary>
Motivation: 数据库研发需要真实用户工作负载进行基准测试和优化，但由于隐私法规难以获取真实SQL查询。现有云数据库厂商发布的匿名性能追踪缺乏原始查询文本和数据，无法用于需要实际执行的场景。

Method: ResQ构建执行感知的查询图，通过贝叶斯优化驱动的谓词搜索将其实例化为SQL，并在精确查询和参数化模板两个层面显式建模工作负载重复。采用搜索空间边界和轻量级本地成本模型加速优化。

Result: 在公开云追踪(Snowset、Redset)和新发布的工业追踪(Bendset)上的实验表明，ResQ显著优于现有基线方法：节省96.71%的token、减少86.97%的运行时间，CPU时间的最大Q-error降低14.8倍，扫描字节数降低997.7倍，操作符组成匹配度高。

Conclusion: ResQ能够高效生成忠实匹配生产追踪执行特征的可执行SQL工作负载，解决了从匿名性能追踪生成可运行工作负载的挑战。

Abstract: Database research and development rely heavily on realistic user workloads for benchmarking, instance optimization, migration testing, and database tuning. However, acquiring real-world SQL queries is notoriously challenging due to strict privacy regulations. While cloud database vendors have begun releasing anonymized performance traces to the research community, these traces typi- cally provide only high-level execution statistics without the origi- nal query text or data, which is insufficient for scenarios that require actual execution. Existing tools fail to capture fine-grained perfor- mance patterns or generate runnable workloads that reproduce these public traces with both high fidelity and efficiency. To bridge this gap, we propose ResQ, a fine-grained workload synthesis sys- tem designed to generate executable SQL workloads that faithfully match the per-query execution targets and operator distributions of production traces. ResQ constructs execution-aware query graphs, instantiates them into SQL via Bayesian Optimization-driven pred- icate search, and explicitly models workload repetition through reuse at both exact-query and parameterized-template levels. To ensure practical scalability, ResQ combines search-space bounding with lightweight local cost models to accelerate optimization. Ex- periments on public cloud traces (Snowset, Redset) and a newly released industrial trace (Bendset) demonstrate that ResQ signif- icantly outperforms state-of-the-art baselines, achieving 96.71% token savings and a 86.97% reduction in runtime, while lowering maximum Q-error by 14.8x on CPU time and 997.7x on scanned bytes, and closely matching operator composition.

</details>


### [38] [Skill-Based Autonomous Agents for Material Creep Database Construction](https://arxiv.org/abs/2602.03069)
*Yue Wu,Tianhao Su,Shunbo Hu,Deng Pan*

Main category: cs.DB

TL;DR: 提出基于LLM的自主代理框架，从科学PDF中自动提取高质量材料数据，无需人工干预，成功构建材料蠕变力学数据库


<details>
  <summary>Details</summary>
Motivation: 数据驱动的材料科学发展受限于历史实验数据大多被锁定在非结构化的科学文献中，人工整理成本高且易出错

Method: 采用基于技能的模块化架构，部署自主代理框架，执行语义过滤、多模态信息提取和物理验证等复杂认知任务

Result: 对243篇文献应用该框架，图形数据数字化验证提取成功率超过90%，跨模态验证显示视觉提取数据点与文本提取本构参数高度一致（R²>0.99）

Conclusion: 该工作不仅为研究不同材料系统的时间依赖性变形提供了关键资源，还建立了可扩展的自主知识获取范式，为下一代自主实验室铺平道路

Abstract: The advancement of data-driven materials science is currently constrained by a fundamental bottleneck: the vast majority of historical experimental data remains locked within the unstructured text and rasterized figures of legacy scientific literature. Manual curation of this knowledge is prohibitively labor-intensive and prone to human error. To address this challenge, we introduce an autonomous, agent-based framework powered by Large Language Models (LLMs) designed to excavate high-fidelity datasets from scientific PDFs without human intervention. By deploying a modular "skill-based" architecture, the agent orchestrates complex cognitive tasks - including semantic filtering, multi-modal information extraction, and physics-informed validation. We demonstrate the efficacy of this framework by constructing a physically self-consistent database for material creep mechanics, a domain characterized by complex graphical trajectories and heterogeneous constitutive models. Applying the pipeline to 243 publications, the agent achieved a verified extraction success rate exceeding 90% for graphical data digitization. Crucially, we introduce a cross-modal verification protocol, demonstrating that the agent can autonomously align visually extracted data points with textually extracted constitutive parameters ($R^2 > 0.99$), ensuring the physical self-consistency of the database. This work not only provides a critical resource for investigating time-dependent deformation across diverse material systems but also establishes a scalable paradigm for autonomous knowledge acquisition, paving the way for the next generation of self-driving laboratories.

</details>


### [39] [StreamShield: A Production-Proven Resiliency Solution for Apache Flink at ByteDance](https://arxiv.org/abs/2602.03189)
*Yong Fang,Yuxing Han,Meng Wang,Yifan Zhang,Yue Ma,Chi Zhang*

Main category: cs.DB

TL;DR: StreamShield是字节跳动为Apache Flink集群设计的生产级弹性解决方案，通过引擎和集群双视角优化，提升大规模分布式流处理系统的故障恢复能力和运行稳定性。


<details>
  <summary>Details</summary>
Motivation: 在字节跳动这样的大规模生产环境中，分布式流处理系统需要满足严格的SLO要求，但集群规模大、业务多样性和运维开销使得实现弹性和稳定性面临挑战。

Method: StreamShield从引擎和集群两个互补视角设计，包含运行时优化、细粒度容错、混合复制策略和外部系统高可用性等关键技术，并建立了稳健的测试和部署流水线。

Result: 在生产集群上的广泛评估表明，StreamShield提出的技术方案具有高效性和有效性。

Conclusion: StreamShield是一个经过生产验证的弹性解决方案，能够显著提升大规模Flink集群的故障恢复能力和运行稳定性。

Abstract: Distributed Stream Processing Systems (DSPSs) form the backbone of real-time processing and analytics at ByteDance, where Apache Flink powers one of the largest production clusters worldwide. Ensuring resiliency, the ability to withstand and rapidly recover from failures, together with operational stability, which provides consistent and predictable performance under normal conditions, is essential for meeting strict Service Level Objectives (SLOs). However, achieving resiliency and stability in large-scale production environments remains challenging due to the cluster scale, business diversity, and significant operational overhead. In this work, we present StreamShield, a production-proven resiliency solution deployed in ByteDance's Flink clusters. Designed along complementary perspectives of the engine and cluster, StreamShield introduces key techniques to enhance resiliency, covering runtime optimization, fine-grained fault-tolerance, hybrid replication strategy, and high availability under external systems. Furthermore, StreamShield proposes a robust testing and deployment pipeline that ensures reliability and robustness in production releases. Extensive evaluations on a production cluster demonstrate the efficiency and effectiveness of techniques proposed by StreamShield.

</details>


### [40] [A Pipeline for ADNI Resting-State Functional MRI Processing and Quality Control](https://arxiv.org/abs/2602.03278)
*Saige Rutherford,Zeshawn Zahid,Robert C. Welsh,Andrea Avena-Koenigsberger,Vincent Koppelmans,Amanda F. Mejia*

Main category: cs.DB

TL;DR: 开发了一个用于ADNI静息态fMRI数据的标准化处理流程，解决数据获取协议不一致、质量参差不齐和对齐问题，促进大规模功能连接研究。


<details>
  <summary>Details</summary>
Motivation: ADNI收集了大量静息态fMRI数据，但由于采集协议不一致、数据质量参差不齐、成像与临床评估时间不对齐等问题，导致许多研究只能使用小部分数据，限制了统计功效和可重复性。

Method: 开发了一个集成化处理流程，结合开源软件（Clinica、fMRIPrep、MRIQC）和定制工具，涵盖数据下载、临床与成像数据时间对齐、预处理和质量控制，支持ADNI-GO、ADNI-2和ADNI-3数据。

Result: 创建了一个透明、可扩展的框架，输出符合BIDS-derivatives规范的高质量rs-fMRI时间序列数据，提供每个受试者和扫描的质量指标和报告，便于严格数据筛选。

Conclusion: 该流程为ADNI fMRI数据的整理和使用提供了标准化解决方案，支持大规模功能生物标志物发现和阿尔茨海默病研究的整合多模态分析，所有脚本和配置文件公开以确保可重复性。

Abstract: The Alzheimer's Disease Neuroimaging Initiative (ADNI) provides a comprehensive multimodal neuroimaging resource for studying aging and Alzheimer's disease (AD). Since its second wave, ADNI has increasingly collected resting-state functional MRI (rs-fMRI), a valuable resource for discovering brain connectivity changes predictive of cognitive decline and AD. A major barrier to its use is the considerable variability in acquisition protocols and data quality, compounded by missing imaging sessions and inconsistencies in how functional scans temporally align with clinical assessments. As a result, many studies only utilize a small subset of the total rs-fMRI data, limiting statistical power, reproducibility, and the ability to study longitudinal functional brain changes at scale. Here, we describe a pipeline for ADNI rs-fMRI data that encompasses the download of necessary imaging and clinical data, temporally aligning the clinical and imaging data, preprocessing, and quality control. We integrate data curation and preprocessing across all ADNI sites and scanner types using a combination of open-source software (Clinica, fMRIPrep, and MRIQC) and bespoke tools. Quality metrics and reports are generated for each subject and session to facilitate rigorous data screening. All scripts and configuration files are available to enable reproducibility. The pipeline, which currently supports ADNI-GO, ADNI-2, and ADNI-3 data releases, outputs high-quality rs-fMRI time series data adhering to the BIDS-derivatives specification. This protocol provides a transparent and scalable framework for curating and utilizing ADNI fMRI data, empowering large-scale functional biomarker discovery and integrative multimodal analyses in Alzheimer's disease research.

</details>
