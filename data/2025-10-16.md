<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.SE](#cs.SE) [Total: 13]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters](https://arxiv.org/abs/2510.12889)
*Wei Da,Evangelia Kalyvianaki*

Main category: cs.DC

TL;DR: Dodoor是一种高效的随机化去中心化调度器，通过批量更新缓存服务器信息和新型负载评分机制，在异构集群中显著减少通信开销并提升调度性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心需要高效的分布式任务调度，但现有去中心化调度器依赖实时探测远程服务器，导致通信开销大。Dodoor旨在通过缓存信息和批量更新来减少通信成本。

Method: 基于加权球-箱模型的b-batch设置，使用缓存服务器信息进行调度决策，通过新型负载评分机制衡量服务器与任务间的反亲和性，替代传统的待处理任务计数方法。

Result: 在101节点异构集群测试中，Dodoor在两个工作负载下分别减少调度消息55-66%，吞吐量提升最高33.2%和21.5%，平均完成时间减少12.1%和7.2%，尾部延迟改善21.9%和24.6%。

Conclusion: Dodoor通过减少通信开销和优化负载平衡，在异构集群中实现了显著性能提升，证明了基于缓存信息和新型负载评分机制的有效性。

Abstract: This paper introduces Dodoor, an efficient randomized decentralized scheduler
designed for task scheduling in modern data centers. Dodoor leverages advanced
research on the weighted balls-into-bins model with b-batched setting. Unlike
other decentralized schedulers that rely on real-time probing of remote
servers, Dodoor makes scheduling decisions based on cached server information,
which is updated in batches, to reduce communication overheads. To schedule
tasks with dynamic, multidimensional resource requirements in heterogeneous
cluster, Dodoor uses a novel load score to measure servers' loads for each
scheduled task. This score captures the anti-affinity between servers and tasks
in contrast to the commonly used heuristic of counting pending tasks to balance
load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two
workloads: (i) simulated Azure virtual machines placements and (ii) real
serverless Python functions executions in Docker. The evaluation shows that
Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can
also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency
by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two
workloads.

</details>


### [2] [Scrutiny new framework in integrated distributed reliable systems](https://arxiv.org/abs/2510.13203)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 提出了一个新的集成分布式系统框架FDIRS，通过异构分布式数据库技术提高系统性能和响应速度，解决了先前框架的一些问题。


<details>
  <summary>Details</summary>
Motivation: 现有的集成分布式系统框架在性能、效率和可靠性方面存在不足，需要开发新的框架来提升系统表现。

Method: 使用三部分结构构建FDIRS框架，采用异构分布式数据库技术来优化系统性能，并与现有框架ERPSD和ERPDRT进行对比分析。

Result: 仿真结果显示FDIRS框架在效率、性能和可靠性方面均有显著提升，成功解决了先前框架的一些问题。

Conclusion: FDIRS框架通过异构分布式数据库技术有效提高了集成分布式系统的性能、效率和可靠性，是一个有前景的解决方案。

Abstract: In this paper we represent a new framework for integrated distributed
systems. In the proposed framework we have used three parts to increase
Satisfaction and Performance of this framework. At first we analyse integrated
systems and their evolution process and also ERPSD and ERPDRT framework briefly
then we explain the new FDIRS framework. Finally we compare the results of
simulation of the new framework with presented frameworks. Result showed In
FIDRS framework, the technique of heterogeneous distributed data base is used
to improve Performance and speed in responding to users. Finally by using FDIRS
framework we succeeded to increase Efficiency, Performance and reliability of
integrated systems and remove some of previous frameworks problems.

</details>


### [3] [BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure](https://arxiv.org/abs/2510.13223)
*Yiyuan He,Minxian Xu,Jingfeng Wu,Jianmin Hu,Chong Ma,Min Shen,Le Chen,Chengzhong Xu,Lin Qu,Kejiang Ye*

Main category: cs.DC

TL;DR: BanaServe是一个动态编排框架，通过层级权重迁移、注意力级KV缓存迁移和全局KV缓存存储共享机制，解决解耦LLM服务中的资源分配不平衡、负载不均和缓存感知路由导致的效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前解耦LLM服务系统面临三个关键限制：静态资源分配无法适应动态工作负载、预填充和解码阶段固有的负载不平衡、以及前缀缓存感知路由导致的负载分布倾斜，这些都会导致资源浪费或违反SLO。

Method: 引入层级权重迁移、注意力级KV缓存迁移和全局KV缓存存储共享机制，支持粗粒度（层级）和细粒度（注意力级）负载重分配，最小化延迟开销，使路由器能够进行纯负载感知调度。

Result: 相比vLLM，BanaServe实现了1.2x-3.9x的吞吐量提升，总处理时间降低3.9%-78.4%；相比DistServe，吞吐量提升1.1x-2.8x，延迟降低1.4%-70.1%。

Conclusion: BanaServe通过动态资源编排有效解决了当前解耦LLM服务系统的局限性，显著提升了系统吞吐量和效率，同时降低了延迟。

Abstract: Large language models (LLMs) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient serving systems.
Disaggregated LLM serving, which separates prompt prefill from auto-regressive
decode, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current disaggregated
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between prefill and decode stages, where prefill is
compute-bound and decode is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix cache aware routing
skews load distribution, as high cache hit rate prefill nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across prefill
and decode instances while eliminating hotspots induced by cache. BanaServe
introduces layer level weight migration, attention level Key Value Cache (KV
Cache) migration, and Global KV Cache Store sharing with layer wise overlapped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

</details>


### [4] [Distributed Reductions for the Maximum Weight Independent Set Problem](https://arxiv.org/abs/2510.13306)
*Jannick Borowitz,Ernestine Großmann,Mattthias Schimek*

Main category: cs.DC

TL;DR: 提出了首个分布式内存并行缩减算法，用于解决最大权重独立集问题，实现了在大规模图上的高效计算和良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 最大权重独立集是一个重要的NP难优化问题，现有的实用算法使用数据缩减规则来求解，但缺乏分布式并行实现，无法处理超大规模图。

Method: 开发了分布式内存并行缩减算法、分布式reduce-and-greedy和reduce-and-peel启发式算法，支持异步计算。

Result: 在1024个处理器上测试显示良好可扩展性，reduce-and-peel方法相比顺序算法平均加速33倍，reduce-and-greedy方法加速达50倍，能处理超过10亿顶点和170亿边的图。

Conclusion: 分布式并行方法显著提升了最大权重独立集问题的求解效率，能够处理前所未有的图规模，为大规模图优化问题提供了有效解决方案。

Abstract: Finding maximum-weight independent sets in graphs is an important NP-hard
optimization problem. Given a vertex-weighted graph $G$, the task is to find a
subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most
recently published practical exact algorithms and heuristics for this problem
use a variety of data-reduction rules to compute (near-)optimal solutions.
Applying these rules results in an equivalent instance of reduced size. An
optimal solution to the reduced instance can be easily used to construct an
optimal solution for the original input.
  In this work, we present the first distributed-memory parallel reduction
algorithms for this problem, targeting graphs beyond the scale of previous
sequential approaches. Furthermore, we propose the first distributed
reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight
independent set heuristically.
  In our practical evaluation, our experiments on up to $1024$ processors
demonstrate good scalability of our distributed reduce algorithms while
maintaining good reduction impact. Our asynchronous reduce-and-peel approach
achieves an average speedup of $33\times$ over a sequential state-of-the-art
reduce-and-peel approach on 36 real-world graphs with a solution quality close
to the sequential algorithm. Our reduce-and-greedy algorithms even achieve
average speedups of up to $50\times$ at the cost of a lower solution quality.
Moreover, our distributed approach allows us to consider graphs with more than
one billion vertices and 17 billion edges.

</details>


### [5] [Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2510.13447)
*Julian Legler,Sebastian Werner,Maria C. Borges,Stefan Tai*

Main category: cs.DC

TL;DR: 提出了一个服务级能耗模型，专门针对微服务架构中跨容器服务交互的能耗影响，特别是网络和存储组件的能耗，弥补了现有研究只关注CPU和内存的不足。


<details>
  <summary>Details</summary>
Motivation: 微服务架构虽然提供了灵活性和可扩展性，但也增加了云资源需求，导致更高的能耗和碳排放。现有研究主要关注容器级别的CPU和内存能耗，忽略了跨容器服务交互（特别是网络和存储）的能耗影响。

Method: 引入了一个服务级能耗模型，捕捉微服务在容器间分布式执行的特性，并开发了实验工具来测量CPU、内存、网络和存储组件的能耗。

Result: 实验验证表明，忽略网络和存储会导致辅助服务能耗低估高达63%，突显了在能效微服务架构设计中需要更全面的能耗评估。

Conclusion: 微服务架构的能耗评估必须包含网络和存储组件，否则会严重低估实际能耗，这对设计能效优化的云原生系统具有重要意义。

Abstract: Microservice architectures have become the dominant paradigm for cloud-native
systems, offering flexibility and scalability. However, this shift has also led
to increased demand for cloud resources, contributing to higher energy
consumption and carbon emissions. While existing research has focused on
measuring fine-grained energy usage of CPU and memory at the container level,
or on system-wide assessments, these approaches often overlook the energy
impact of cross-container service interactions, especially those involving
network and storage for auxiliary services such as observability and system
monitoring. To address this gap, we introduce a service-level energy model that
captures the distributed nature of microservice execution across containers.
Our model is supported by an experimentation tool that accounts for energy
consumption not just in CPU and memory, but also in network and storage
components. We validate our approach through extensive experimentation with
diverse experiment configurations of auxiliary services for a popular
open-source cloud-native microservice application. Results show that omitting
network and storage can lead to an underestimation of auxiliary service energy
use by up to 63%, highlighting the need for more comprehensive energy
assessments in the design of energy-efficient microservice architectures.

</details>


### [6] [Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference](https://arxiv.org/abs/2510.13668)
*Zhibin Wang,Zetao Hong,Xue Li,Zibo Wang,Shipeng Li,Qingkai Meng,Qing Wang,Chengying Huan,Rong Gu,Sheng Zhong,Chen Tian*

Main category: cs.DC

TL;DR: ARES是一个基于长度预测的自适应解码重调度系统，通过LLM原生预测方法准确预测剩余生成长度，在解码阶段实现动态负载均衡，显著提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中输出长度变化导致解码阶段严重的工作负载不平衡，特别是在长输出推理任务中。现有系统使用静态调度策略，在变化的解码工作负载下经常导致SLO违规和OOM故障。

Method: 提出轻量级连续的LLM原生预测方法，利用LLM隐藏状态建模剩余生成长度；在解码阶段实施重调度解决方案，集成当前和预测工作负载的动态平衡机制。

Result: 预测精度显著提升（MAE降低49.42%），开销大幅减少（预测器参数削减93.28%）；P99 TPOT降低74.77%，吞吐量提升高达2.24倍。

Conclusion: ARES系统通过自适应解码重调度有效解决了LLM推理中的工作负载不平衡问题，显著提升了系统性能和可靠性。

Abstract: Large Language Model (LLM) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the decode phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
prefill-to-decode scheduling, which often results in SLO violations and OOM
failures under evolving decode workloads.
  In this paper, we propose ARES, an adaptive decoding rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous LLM-native prediction
method that leverages LLM hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in decode phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.

</details>


### [7] [FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access](https://arxiv.org/abs/2510.13724)
*Aditya Tanikanti,Benoit Côté,Yanfei Guo,Le Chen,Nickolaus Saint,Ryan Chard,Ken Raffenetti,Rajeev Thakur,Thomas Uram,Ian Foster,Michael E. Papka,Venkatram Vishwanath*

Main category: cs.DC

TL;DR: FIRST是一个联邦推理资源调度工具包，为分布式高性能计算集群提供类似云的AI模型推理服务，支持多种推理后端和自动扩缩容。


<details>
  <summary>Details</summary>
Motivation: 解决科学工作流中对私有、安全、可扩展AI推理日益增长的需求，让研究人员能够在本地基础设施上生成数十亿token，而不依赖商业云服务。

Method: 利用Globus Auth和Globus Compute，通过OpenAI兼容API在私有安全环境中运行并行推理工作负载，支持多种推理后端（如vLLM），自动扩缩资源，维护"热"节点以实现低延迟执行。

Result: 实现了在联邦集群上分发请求的能力，支持高吞吐量批处理和交互模式，能够在现有HPC基础设施上提供云式AI模型访问。

Conclusion: FIRST框架成功满足了科学计算中对私有、安全、可扩展AI推理的需求，为研究人员提供了在本地环境中高效运行大规模AI推理的能力。

Abstract: We present the Federated Inference Resource Scheduling Toolkit (FIRST), a
framework enabling Inference-as-a-Service across distributed High-Performance
Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI
models, like Large Language Models (LLMs), on existing HPC infrastructure.
Leveraging Globus Auth and Globus Compute, the system allows researchers to run
parallel inference workloads via an OpenAI-compliant API on private, secure
environments. This cluster-agnostic API allows requests to be distributed
across federated clusters, targeting numerous hosted models. FIRST supports
multiple inference backends (e.g., vLLM), auto-scales resources, maintains
"hot" nodes for low-latency execution, and offers both high-throughput batch
and interactive modes. The framework addresses the growing demand for private,
secure, and scalable AI inference in scientific workflows, allowing researchers
to generate billions of tokens daily on-premises without relying on commercial
cloud infrastructure.

</details>


### [8] [Tight Conditions for Binary-Output Tasks under Crashes](https://arxiv.org/abs/2510.13755)
*Timothé Albouy,Antonio Fernández Anta,Chryssis Georgiou,Nicolas Nicolaou,Junlang Wang*

Main category: cs.DC

TL;DR: 本文研究了解决具有二进制输出的分布式任务的必要和充分系统条件，为同步和异步系统提供了完整的n和t条件特征化，统一了二进制共识和对称性破坏等多个问题。


<details>
  <summary>Details</summary>
Motivation: 探索分布式系统中解决二进制输出任务的系统条件，重点关注任务可以产生的不同输出值集合，忽略有效性和值多重性，考虑某些进程可能不输出值的情况。

Method: 采用输出集方法，分析在n个进程中最多t个可能崩溃的分布式系统中，二进制输出任务的可解性条件，涵盖同步和异步系统。

Result: 提供了完整的n和t紧条件特征化，证明该方法具有高度通用性，能够统一多个分布式计算问题，并为更强的任务表述提供不可能性证明。

Conclusion: 输出集方法为二进制输出分布式任务提供了统一的理论框架，其不可能性证明适用于考虑有效性、值多重性或超越二进制输出的更强任务表述。

Abstract: This paper explores necessary and sufficient system conditions to solve
distributed tasks with binary outputs (\textit{i.e.}, tasks with output values
in $\{0,1\}$). We focus on the distinct output sets of values a task can
produce (intentionally disregarding validity and value multiplicity),
considering that some processes may output no value. In a distributed system
with $n$ processes, of which up to $t \leq n$ can crash, we provide a complete
characterization of the tight conditions on $n$ and $t$ under which every class
of tasks with binary outputs is solvable, for both synchronous and asynchronous
systems. This output-set approach yields highly general results: it unifies
multiple distributed computing problems, such as binary consensus and symmetry
breaking, and it produces impossibility proofs that hold for stronger task
formulations, including those that consider validity, account for value
multiplicity, or move beyond binary outputs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: AutoCode是一个自动生成竞赛级编程问题及其测试用例的系统，通过多轮验证确保问题质量，在保留问题上测试用例与官方评判一致性接近99%，并能生成被顶级程序员认可的新颖问题。


<details>
  <summary>Details</summary>
Motivation: 编写竞赛编程问题需要精确设置约束条件、输入分布和边界情况，针对特定算法并校准复杂度，这为测试大型语言模型的通用能力提供了理想场景。

Method: 使用多轮验证方法生成竞赛级问题陈述和测试用例，从随机种子问题开始创建新颖变体，并通过交叉验证参考解和暴力解来过滤有缺陷的问题。

Result: 在保留问题上，AutoCode测试套件与官方评判一致性达99%，显著优于HardTests的81%；能生成被Grandmaster级（前0.3%）程序员认可为竞赛质量的新颖问题。

Conclusion: AutoCode系统通过自动化方法可靠地生成高质量的竞赛编程问题，验证了大型语言模型在此任务上的能力，为编程竞赛问题创作提供了有效工具。

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [10] [SpareCodeSearch: Searching for Code Context When You Have No Spare GPU](https://arxiv.org/abs/2510.12948)
*Minh Nguyen*

Main category: cs.SE

TL;DR: 本文证明在大型代码库中使用关键词搜索足以检索相关代码上下文，无需GPU资源，在代码上下文竞赛中达到0.748和0.725的chRF分数。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成框架使用语义搜索，需要大量计算资源训练和托管嵌入模型，难以集成到轻量级应用如IDE代码补全中。

Method: 使用关键词搜索替代语义搜索来检索相关代码上下文，避免对GPU资源的依赖。

Result: 在代码上下文竞赛基准测试中，Kotlin和Python轨道分别达到0.748和0.725的chRF分数，证明关键词搜索的有效性。

Conclusion: 关键词搜索足以在大型代码库中检索相关有用的代码上下文，无需昂贵的GPU资源，适合轻量级应用集成。

Abstract: Retrieval-Augmented Generation (RAG) frameworks aim to enhance Code Language
Models (CLMs) by including another module for retrieving relevant context to
construct the input prompt. However, these retrieval modules commonly use
semantic search, requiring substantial computational resources for training and
hosting these embedded models, making them infeasible to integrate into
lightweight applications such as in-IDE AI-based code completion. In this
solution paper, we prove that using keyword-search is sufficient to retrieve
relevant and useful code context inside large codebases, without the need for
extensive GPU resources. The usefulness of code contexts found by our solution
is demonstrated through their completion results on the Code Context
Competition's benchmark, reaching 0.748 and 0.725 chRF scores on Kotlin and
Python tracks, respectively.

</details>


### [11] [ADPerf: Investigating and Testing Performance in Autonomous Driving Systems](https://arxiv.org/abs/2510.13078)
*Tri Minh-Triet Pham,Diego Elias Costa,Weiyi Shang,Jinqiu Yang*

Main category: cs.SE

TL;DR: 本文提出了ADPerf工具，用于测量和建模自动驾驶系统中障碍物检测模块的性能，生成能暴露检测延迟增加的逼真点云数据测试用例。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统依赖多传感器和深度学习模型进行障碍物检测，但障碍物检测模块的延迟及其对点云数据变化的弹性尚未被充分理解，这对自动驾驶系统的安全性和有效性至关重要。

Method: 首先对Apollo和Autoware两个工业级自动驾驶系统进行障碍物检测模块性能的全面调查，然后开发ADPerf工具生成能增加检测延迟的逼真点云测试数据。

Result: ADPerf成功对广泛使用的3D障碍物检测模块进行压力测试，并评估了延迟增加对轨迹预测模块的传播影响，发现3D障碍物检测可能成为自动驾驶系统延迟增加的主要瓶颈。

Conclusion: 需要对障碍物检测组件特别是3D障碍物检测进行性能测试，因为其延迟增加会成为系统瓶颈，并进一步传播到其他模块，降低自动驾驶系统的整体可靠性。

Abstract: Obstacle detection is crucial to the operation of autonomous driving systems,
which rely on multiple sensors, such as cameras and LiDARs, combined with code
logic and deep learning models to detect obstacles for time-sensitive
decisions. Consequently, obstacle detection latency is critical to the safety
and effectiveness of autonomous driving systems. However, the latency of the
obstacle detection module and its resilience to various changes in the LiDAR
point cloud data are not yet fully understood. In this work, we present the
first comprehensive investigation on measuring and modeling the performance of
the obstacle detection modules in two industry-grade autonomous driving
systems, i.e., Apollo and Autoware. Learning from this investigation, we
introduce ADPerf, a tool that aims to generate realistic point cloud data test
cases that can expose increased detection latency. Increasing latency decreases
the availability of the detected obstacles and stresses the capabilities of
subsequent modules in autonomous driving systems, i.e., the modules may be
negatively impacted by the increased latency in obstacle detection.
  We applied ADPerf to stress-test the performance of widely used 3D obstacle
detection modules in autonomous driving systems, as well as the propagation of
such tests on trajectory prediction modules. Our evaluation highlights the need
to conduct performance testing of obstacle detection components, especially 3D
obstacle detection, as they can be a major bottleneck to increased latency of
the autonomous driving system. Such an adverse outcome will also further
propagate to other modules, reducing the overall reliability of autonomous
driving systems.

</details>


### [12] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: TRUSTVIS是一个自动化评估框架，通过交互式界面可视化LLM可信度指标，结合AutoDAN等扰动方法和多数投票机制，有效识别模型的安全性和鲁棒性漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在NLP应用中的广泛应用，其安全性和鲁棒性等可信度问题日益突出，需要系统性的评估方法来确保模型可靠性。

Method: 集成AutoDAN等知名扰动方法，采用多数投票机制整合多种评估方法，开发交互式用户界面直观展示可信度指标。

Result: 在Vicuna-7b、Llama2-7b和GPT-3.5等模型上的初步案例研究表明，该框架能有效识别安全性和鲁棒性漏洞。

Conclusion: TRUSTVIS框架不仅提供可靠的可信度评估结果，还通过交互式界面使复杂评估过程对用户更加友好，支持针对性的模型改进。

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


### [13] [Isolating Compiler Bugs through Compilation Steps Analysis](https://arxiv.org/abs/2510.13128)
*Yujie Liu,Mingxuan Zhu,Shengyu Cheng,Dan Hao*

Main category: cs.SE

TL;DR: CompSCAN是一种新的编译器bug隔离技术，通过分析编译步骤序列来识别bug原因，在真实LLVM和GCC bug上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 编译器bug会传播到依赖软件中，但现有技术缺乏对内部步骤的因果分析，限制了bug隔离效果。

Method: 三阶段过程：提取导致失败的编译步骤序列，识别bug相关步骤并收集对应代码元素，计算可疑度分数并输出排名列表。

Result: 在185个真实bug上测试，CompSCAN在Top-1/3/5/10排名中分别成功隔离50、85、100、123个bug，相比ETEM和ODFL有显著改进，且运行速度更快。

Conclusion: CompSCAN通过分析编译步骤序列有效解决了编译器bug隔离问题，在效果和效率上都优于现有技术。

Abstract: Compilers are essential to software systems, and their bugs can propagate to
dependent software. Ensuring compiler correctness is critical. However,
isolating compiler bugs remains challenging due to the internal complexity of
compiler execution. Existing techniques primarily mutate compilation inputs to
generate passing and failing tests, but often lack causal analysis of internal
steps, limiting their effectiveness.
  To address this limitation, we propose CompSCAN, a novel compiler bug
isolation technique that applies analysis over the sequence of compilation
steps. CompSCAN follows a three-stage process: (1) extracting the array of
compilation steps that leads to the original failure, (2) identifying
bug-causing steps and collecting corresponding compiler code elements, and (3)
calculating suspicious scores for each code element and outputting a suspicious
ranking list as the bug isolation result.
  We evaluate CompSCAN on 185 real-world LLVM and GCC bugs. Results show that
CompSCAN outperforms state-of-the-art techniques in both effectiveness and
efficiency. CompSCAN successfully isolates 50, 85, 100, and 123 bugs within the
Top-1/3/5/10 ranks, respectively. Compared with ETEM and ODFL, two
state-of-the-art compiler bug isolation techniques, CompSCAN achieves relative
improvements of 44.51% / 50.18% / 36.24% / 24.49% over ETEM, and 31.58% /
49.12% / 44.93% / 21.78% over ODFL on those metrics. Moreover, CompSCAN runs
faster on average per bug than both baselines.

</details>


### [14] [GRACE: Globally-Seeded Representation-Aware Cluster-Specific Evolution for Compiler Auto-Tuning](https://arxiv.org/abs/2510.13176)
*Haolin Pan,Chao Zha,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: GRACE是一个编译器自动调优框架，通过利用pass协同效应和对比学习来减少搜索空间，在LLVM IR指令数优化上实现了显著的性能提升，同时保持较低的调优时间。


<details>
  <summary>Details</summary>
Motivation: 标准编译器启发式方法通常产生次优结果，迭代编译搜索成本过高，机器学习方法泛化能力不足，需要一种既能找到优化解又具有良好泛化能力的编译器自动调优方法。

Method: 利用pass协同效应和加权评分生成高质量候选序列和pass池，使用对比学习和数据增强创建程序嵌入进行相似性聚类，在聚类内进行进化搜索得到核心集序列，测试时选择最佳序列并用轻量级技术优化。

Result: 在7个数据集上，GRACE相比opt -Oz平均减少LLVM IR指令数10.09%(LLVM 10.0.0)和10.19%(LLVM 18.1.6)，平均调优时间小于1秒/程序。

Conclusion: GRACE在编译器自动调优中实现了最先进的性能和实际有效性，在显著优化代码大小的同时保持了高效的调优速度。

Abstract: Compiler pass selection and phase ordering present a significant challenge in
achieving optimal program performance, particularly for objectives like code
size reduction. Standard compiler heuristics offer general applicability but
often yield suboptimal, program-specific results due to their one-size-fits-all
nature. While iterative compilation can find tailored solutions, its
prohibitive search cost limits practical use. Machine learning approaches
promise faster inference but frequently struggle with generalization to unseen
programs. This paper introduces GRACE, a novel framework for compiler
auto-tuning, demonstrated for LLVM IR instruction count optimization. GRACE
effectively curtails the search space by leveraging pass synergies and a
weighted scoring method to generate initial high-quality candidate sequences
and a pass pool. It then employs contrastive learning, using pass
sequence-based data augmentation, to create program embeddings that facilitate
similarity-aware clustering. Evolutionary search within these clusters yields a
coreset of $k$ specialized pass sequences designed for robust generalization to
unseen programs. At test time, GRACE efficiently selects the best coreset
sequence and refines it using lightweight techniques. Experimental results on
seven diverse datasets show that GRACE reduces LLVM IR instruction count by an
average of 10.09% on LLVM 10.0.0 and 10.19% on LLVM 18.1.6 compared to opt -Oz,
while incurring an average tuning time of less than 1s per program,
demonstrating its state-of-the-art performance and practical effectiveness.

</details>


### [15] [Synergy-Guided Compiler Auto-Tuning of Nested LLVM Pass Pipelines](https://arxiv.org/abs/2510.13184)
*Haolin Pan,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.SE

TL;DR: 提出了一个专为LLVM新Pass Manager设计的编译器自动调优框架，通过形式化语法定义有效嵌套流水线空间，使用基于森林的数据结构表示，并开发结构感知遗传算法确保生成有效优化流水线。


<details>
  <summary>Details</summary>
Motivation: 现有编译器自动调优方法假设线性pass序列，这与LLVM新Pass Manager的分层设计不匹配，无法保证生成语法有效的优化流水线。

Method: 引入形式化语法定义有效嵌套流水线空间，使用森林数据结构原生表示，开发结构感知遗传算法直接操作这些森林，确保候选解构造有效。框架首先挖掘协同pass关系指导搜索，可选细化阶段探索不同有效结构安排带来的性能变化。

Result: 在LLVM 18.1.6上评估七个基准数据集，发现的流水线相比标准opt -Oz优化级别平均实现13.62%的额外指令数减少。

Conclusion: 该框架能够在复杂约束搜索空间中导航，识别有效且高效的pass流水线。

Abstract: Compiler optimization relies on sequences of passes to improve program
performance. Selecting and ordering these passes automatically, known as
compiler auto-tuning, is challenging due to the large and complex search space.
Existing approaches generally assume a linear sequence of passes, a model
compatible with legacy compilers but fundamentally misaligned with the
hierarchical design of the LLVM New Pass Manager. This misalignment prevents
them from guaranteeing the generation of syntactically valid optimization
pipelines. In this work, we present a new auto-tuning framework built from the
ground up for the New Pass Manager. We introduce a formal grammar to define the
space of valid nested pipelines and a forest-based data structure for their
native representation. Upon this foundation, we develop a structure-aware
Genetic Algorithm whose operators manipulate these forests directly, ensuring
that all candidate solutions are valid by construction. The framework first
mines synergistic pass relationships to guide the search. An optional
refinement stage further explores subtle performance variations arising from
different valid structural arrangements.
  We evaluate our approach on seven benchmark datasets using LLVM 18.1.6. The
discovered pipelines achieve an average of 13.62% additional instruction count
reduction compared to the standard opt -Oz optimization level, showing that our
framework is capable of navigating this complex, constrained search space to
identify valid and effective pass pipelines.

</details>


### [16] [Towards Richer Challenge Problems for Scientific Computing Correctness](https://arxiv.org/abs/2510.13423)
*Matthew Sottile,Mohit Tekriwal,John Sarracino*

Main category: cs.SE

TL;DR: 提出为科学计算开发专门的验证挑战问题，以弥补形式化方法与编程语言社区在科学计算正确性验证方面的理解差距。


<details>
  <summary>Details</summary>
Motivation: 现有PL/FM验证技术难以应对现实科学计算应用的复杂性，科学计算社区与PL/FM社区之间缺乏对机器可验证正确性挑战的共同理解。

Method: 提出设计专门的挑战问题来指导和评估科学计算正确性的FM/PL验证技术，这些挑战问题旨在补充现有通用程序验证问题。

Result: 提出了科学计算相关的多个正确性维度，并讨论了设计科学计算正确性评估挑战问题的指导原则和标准。

Conclusion: 需要专门的挑战问题来确保形式化方法和编程语言验证技术能够满足科学计算应用的正确性需求。

Abstract: Correctness in scientific computing (SC) is gaining increasing attention in
the formal methods (FM) and programming languages (PL) community. Existing
PL/FM verification techniques struggle with the complexities of realistic SC
applications. Part of the problem is a lack of a common understanding between
the SC and PL/FM communities of machine-verifiable correctness challenges and
dimensions of correctness in SC applications.
  To address this gap, we call for specialized challenge problems to inform the
development and evaluation of FM/PL verification techniques for correctness in
SC. These specialized challenges are intended to augment existing problems
studied by FM/PL researchers for general programs to ensure the needs of SC
applications can be met. We propose several dimensions of correctness relevant
to scientific computing, and discuss some guidelines and criteria for designing
challenge problems to evaluate correctness in scientific computing.

</details>


### [17] [Verifying a Sparse Matrix Algorithm Using Symbolic Execution](https://arxiv.org/abs/2510.13424)
*Alexander C. Wilton*

Main category: cs.SE

TL;DR: 使用符号执行来测试科学软件，特别是稀疏矩阵算法，提供比传统测试更强的验证保证


<details>
  <summary>Details</summary>
Motivation: 科学软件具有复杂性、数学性和高度优化的特点，容易产生传统测试难以发现的细微错误

Method: 采用符号执行方法编写类似传统单元测试的测试，并将其应用于稀疏矩阵算法

Result: 符号执行能够为科学软件提供更强的验证保证

Conclusion: 符号执行是测试复杂科学软件的有效方法，能够检测传统测试难以发现的细微错误

Abstract: Scientific software is, by its very nature, complex. It is mathematical and
highly optimized which makes it prone to subtle bugs not as easily detected by
traditional testing. We outline how symbolic execution can be used to write
tests similar to traditional unit tests while providing stronger verification
guarantees and apply this methodology to a sparse matrix algorithm.

</details>


### [18] [OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design, Implementation, and Case Studies](https://arxiv.org/abs/2510.13561)
*Peng Di,Faqiang Chen,Xiao Bai,Hongjun Yang,Qingfeng Li,Ganglin Wei,Jian Mou,Feng Shi,Keting Chen,Peng Tang,Zhitao Shen,Zheng Li,Wenhui Shi,Junwei Guo,Hang Yu*

Main category: cs.SE

TL;DR: OpenDerisk是一个专为SRE设计的开源多智能体框架，通过集成诊断原生协作模型、可插拔推理引擎和知识引擎，显著提升了复杂软件问题的诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现代软件复杂性不断增加，给SRE团队带来了不可持续的操作负担，需要能够模拟专家诊断推理的AI驱动自动化解决方案。现有方法要么缺乏深度因果推理能力，要么不适用于SRE特有的专业调查工作流程。

Method: 开发了OpenDerisk多智能体框架，包含诊断原生协作模型、可插拔推理引擎、知识引擎和标准化协议(MCP)，使专业智能体能够协作解决复杂的多领域问题。

Result: 全面评估显示OpenDerisk在准确性和效率方面显著优于最先进的基线方法。已在蚂蚁集团大规模生产部署，服务超过3000名日常用户，验证了其工业级可扩展性和实际影响。

Conclusion: OpenDerisk成功填补了SRE领域AI自动化解决方案的空白，提供了一个专门设计的多智能体框架，能够有效模拟专家诊断推理，在实际生产环境中证明了其价值。

Abstract: The escalating complexity of modern software imposes an unsustainable
operational burden on Site Reliability Engineering (SRE) teams, demanding
AI-driven automation that can emulate expert diagnostic reasoning. Existing
solutions, from traditional AI methods to general-purpose multi-agent systems,
fall short: they either lack deep causal reasoning or are not tailored for the
specialized, investigative workflows unique to SRE. To address this gap, we
present OpenDerisk, a specialized, open-source multi-agent framework
architected for SRE. OpenDerisk integrates a diagnostic-native collaboration
model, a pluggable reasoning engine, a knowledge engine, and a standardized
protocol (MCP) to enable specialist agents to collectively solve complex,
multi-domain problems. Our comprehensive evaluation demonstrates that
OpenDerisk significantly outperforms state-of-the-art baselines in both
accuracy and efficiency. This effectiveness is validated by its large-scale
production deployment at Ant Group, where it serves over 3,000 daily users
across diverse scenarios, confirming its industrial-grade scalability and
practical impact. OpenDerisk is open source and available at
https://github.com/derisk-ai/OpenDerisk/

</details>


### [19] [Auto-repair without test cases: How LLMs fix compilation errors in large industrial embedded code](https://arxiv.org/abs/2510.13575)
*Han Fu,Sigrid Eldh,Kristian Wiklund,Andreas Ermedahl,Philipp Haller,Cyrille Artho*

Main category: cs.SE

TL;DR: 使用大型语言模型自动修复工业嵌入式系统中的编译错误，在CI系统中实现高达63%的错误修复率，显著减少调试时间。


<details>
  <summary>Details</summary>
Motivation: 工业嵌入式系统的软硬件协同开发经常在持续集成中出现编译错误，现有修复技术依赖测试用例，但不可编译代码没有测试用例可用。

Method: 收集超过40000个产品源代码提交，使用四种最先进的大型语言模型增强工业CI系统，并与人工修复进行对比评估。

Result: LLM增强的CI系统可以解决基线数据集中63%的编译错误，其中83%的成功修复被认为是合理的，调试时间从几小时减少到8分钟内。

Conclusion: 大型语言模型在自动修复编译错误方面表现优异，能显著提高工业嵌入式系统开发的效率和可靠性。

Abstract: The co-development of hardware and software in industrial embedded systems
frequently leads to compilation errors during continuous integration (CI).
Automated repair of such failures is promising, but existing techniques rely on
test cases, which are not available for non-compilable code.
  We employ an automated repair approach for compilation errors driven by large
language models (LLMs). Our study encompasses the collection of more than 40000
commits from the product's source code. We assess the performance of an
industrial CI system enhanced by four state-of-the-art LLMs, comparing their
outcomes with manual corrections provided by human programmers. LLM-equipped CI
systems can resolve up to 63 % of the compilation errors in our baseline
dataset. Among the fixes associated with successful CI builds, 83 % are deemed
reasonable. Moreover, LLMs significantly reduce debugging time, with the
majority of successful cases completed within 8 minutes, compared to hours
typically required for manual debugging.

</details>


### [20] [Property Testing for Ocean Models. Can We Specify It? (Invited Talk)](https://arxiv.org/abs/2510.13692)
*Deepak A. Cherian*

Main category: cs.SE

TL;DR: 探讨如何将属性测试思想应用于海洋数值模型，利用地球物理流体动力学理论作为属性测试来解决海洋模型正确性验证的oracle问题。


<details>
  <summary>Details</summary>
Motivation: 从属性测试文献中获得启发，特别是John Hughes教授的工作，探索如何将这些思想应用于海洋数值模型，解决模型正确性验证的挑战。

Method: 提出将简单理想化的地球物理流体动力学问题框架化为属性测试，展示物理学如何自然地支持属性测试的规范制定。

Result: 通过示例清晰地说明了物理学如何自然地支持属性测试的规范，但哪些测试最可行和有用仍需进一步研究。

Conclusion: 地球物理流体动力学理论可以有效地框架化为属性测试，为解决海洋模型正确性验证的oracle问题提供了有前景的方法，但具体实施和实用性需要进一步探索。

Abstract: I take inspiration from the property-testing literature, particularly the
work of Prof. John Hughes, and explore how such ideas might be applied to
numerical models of the ocean. Specifically, I ask whether geophysical fluid
dynamics (GFD) theory, expressed as property tests, might be used to address
the oracle problem of testing the correctness of ocean models. I propose that a
number of simple idealized GFD problems can be framed as property tests. These
examples clearly illustrate how physics naturally lends itself to specifying
property tests. Which of these proposed tests might be most feasible and
useful, remains to be seen.

</details>


### [21] [On Pretraining for Project-Level Code Completion](https://arxiv.org/abs/2510.13697)
*Maksim Sapronov,Evgeniy Glukhov*

Main category: cs.SE

TL;DR: 研究了不同仓库处理策略对代码模型上下文学习的影响，通过扩展上下文窗口和训练少量数据，在Long Code Arena基准上取得了与更大模型相当的性能，发现简单文件级训练方法同样有效。


<details>
  <summary>Details</summary>
Motivation: 探索仓库级预训练中不同处理策略如何影响代码模型的上下文学习能力，为资源受限环境提供更有效的训练方法。

Method: 将OpenCoder模型的上下文窗口从4,096扩展到16,384 tokens，使用10亿tokens的仓库级数据进行训练，比较不同仓库处理技术，并测试简单文件级训练方法。

Result: 尽管使用较小数据集，模型在Long Code Arena基准上表现与使用更大数据集的竞争模型相当，各种仓库处理技术效果相似，主要性能提升来自RoPE缩放参数调整。

Conclusion: 简单的文件级训练方法在原始序列长度下仍然非常有效，这为数据计算资源受限环境下的仓库级代码补全研究开辟了新途径。

Abstract: Repository-level pretraining is commonly used to enable large language models
for code to leverage codebase-wide context. This enhances their ability to
generate accurate and context-aware code completions. In this work, we
investigate how different repository-processing strategies affect in-context
learning in OpenCoder, a 1.5B-parameter model. We extend its context window
from 4,096 to 16,384 tokens by training on additional 1B tokens of curated
repository-level data. Despite relying on a smaller dataset than competing
models (which often use hundreds of billions of tokens), our model achieves
comparable performance on the Long Code Arena benchmark. We find that various
repository-processing techniques yield similarly strong results, with the
primary gain coming from adapting to a new rotary positional embedding (RoPE)
scaling parameter. Finally, we show that a simpler file-level training approach
at the original sequence length remains highly effective, opening up
repository-level code completion research to settings with more constrained
data and compute resources.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [Experiments \& Analysis of Privacy-Preserving SQL Query Sanitization Systems](https://arxiv.org/abs/2510.13528)
*Loïs Ecoffet,Veronika Rehn-Sonigo,Jean-François Couchot,Catuscia Palamidessi*

Main category: cs.DB

TL;DR: 本文对SQL查询净化系统进行了系统分类和定量分析，评估了数据效用、查询执行开销和隐私保证之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 分析型SQL查询在提取数据库洞察的同时带来显著的隐私风险，现有查询净化系统设计复杂多样，需要系统性的分类和性能评估。

Method: 基于隐私模型、保护单元和软件架构等定性标准对SQL净化系统进行分类，并对领先系统进行定量分析，测量数据效用、查询执行开销和隐私保证的权衡。

Result: 提供了当前隐私保护数据库技术的结构化概览和性能评估，阐明了各系统的能力和局限性。

Conclusion: 这项工作为研究人员和实践者提供了清晰的框架，帮助他们理解现有SQL查询净化系统的特性和性能表现。

Abstract: Analytical SQL queries are essential for extracting insights from relational
databases but concurrently introduce significant privacy risks by potentially
exposing sensitive information. To mitigate these risks, numerous query
sanitization systems have been developed, employing diverse approaches that
create a complex landscape for both researchers and practitioners. These
systems vary fundamentally in their design, including the underlying privacy
model, such as k-anonymity or Differential Privacy; the protected privacy unit,
whether at the tuple- or user-level; and the software architecture, which can
be proxy-based or integrated. This paper provides a systematic classification
of state-of-the-art SQL sanitization systems based on these qualitative
criteria and the scope of queries they support. Furthermore, we present a
quantitative analysis of leading systems, empirically measuring the trade-offs
between data utility, query execution overhead, and privacy guarantees across a
range of analytical queries. This work offers a structured overview and
performance assessment intended to clarify the capabilities and limitations of
current privacy-preserving database technologies.

</details>


### [23] [The Past Still Matters: A Temporally-Valid Data Discovery System](https://arxiv.org/abs/2510.13662)
*Mahdi Esmailoghli,Matthias Weidlich*

Main category: cs.DB

TL;DR: 提出了时间感知数据发现系统的愿景，强调数据相关性随时间变化的重要性，并定义了时间有效数据发现问题。


<details>
  <summary>Details</summary>
Motivation: 现有数据发现方法忽略了数据相关性的时间变化维度，特别是在缺乏明确时间元数据的情况下，这限制了数据发现的准确性。

Method: 提出了包含版本发现、时间谱系推断、变更日志合成和时间感知数据发现技术的系统架构。

Result: 为新一代数据发现系统奠定了基础，将改变我们与演进数据湖的交互方式。

Conclusion: 时间维度是数据发现的关键因素，需要开发专门的技术和系统来应对数据相关性的时间变化特性。

Abstract: Over the past decade, the proliferation of public and enterprise data lakes
has fueled intensive research into data discovery, aiming to identify the most
relevant data from vast and complex corpora to support diverse user tasks.
Significant progress has been made through the development of innovative index
structures, similarity measures, and querying infrastructures. Despite these
advances, a critical aspect remains overlooked: relevance is time-varying.
Existing discovery methods largely ignore this temporal dimension, especially
when explicit date/time metadata is missing. To fill this gap, we outline a
vision for a data discovery system that incorporates the temporal dimension of
data. Specifically, we define the problem of temporally-valid data discovery
and argue that addressing it requires techniques for version discovery,
temporal lineage inference, change log synthesis, and time-aware data
discovery. We then present a system architecture to deliver these techniques,
before we summarize research challenges and opportunities. As such, we lay the
foundation for a new class of data discovery systems, transforming how we
interact with evolving data lakes.

</details>
