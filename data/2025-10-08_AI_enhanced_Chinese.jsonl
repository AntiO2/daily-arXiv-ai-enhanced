{"id": "2510.05147", "categories": ["cs.SE", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.05147", "abs": "https://arxiv.org/abs/2510.05147", "authors": ["Yu Zhu"], "title": "Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing", "comment": null, "summary": "Ensuring reliability in modern software systems requires rigorous\npre-production testing across highly heterogeneous and evolving environments.\nBecause exhaustive evaluation is infeasible, practitioners must decide how to\nallocate limited testing resources across configurations where failure\nprobabilities may drift over time. Existing combinatorial optimization\napproaches are static, ad hoc, and poorly suited to such non-stationary\nsettings. We introduce a novel reinforcement learning (RL) framework that\nrecasts configuration allocation as a sequential decision-making problem. Our\nmethod is the first to integrate Q-learning with a hybrid reward design that\nfuses simulated outcomes and real-time feedback, enabling both sample\nefficiency and robustness. In addition, we develop an adaptive online-offline\ntraining scheme that allows the agent to quickly track abrupt probability\nshifts while maintaining long-run stability. Extensive simulation studies\ndemonstrate that our approach consistently outperforms static and\noptimization-based baselines, approaching oracle performance. This work\nestablishes RL as a powerful new paradigm for adaptive configuration\nallocation, advancing beyond traditional methods and offering broad\napplicability to dynamic testing and resource scheduling domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u914d\u7f6e\u5206\u914d\u6846\u67b6\uff0c\u5c06\u6d4b\u8bd5\u8d44\u6e90\u914d\u7f6e\u95ee\u9898\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408Q\u5b66\u4e60\u548c\u6df7\u5408\u5956\u52b1\u8bbe\u8ba1\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u9700\u8981\u5728\u5f02\u6784\u4e14\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u4e25\u683c\u6d4b\u8bd5\uff0c\u4f46\u4f20\u7edf\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u9002\u5e94\u6545\u969c\u6982\u7387\u968f\u65f6\u95f4\u6f02\u79fb\u7684\u975e\u5e73\u7a33\u73af\u5883\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u914d\u7f6e\u5206\u914d\u91cd\u6784\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u7ed3\u5408Q\u5b66\u4e60\u548c\u878d\u5408\u6a21\u62df\u7ed3\u679c\u4e0e\u5b9e\u65f6\u53cd\u9988\u7684\u6df7\u5408\u5956\u52b1\u8bbe\u8ba1\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u5728\u7ebf-\u79bb\u7ebf\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u9759\u6001\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1oracle\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u81ea\u9002\u5e94\u914d\u7f6e\u5206\u914d\u7684\u65b0\u8303\u5f0f\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u6d4b\u8bd5\u548c\u8d44\u6e90\u8c03\u5ea6\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.05156", "categories": ["cs.SE", "cs.AI", "cs.CR", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.05156", "abs": "https://arxiv.org/abs/2510.05156", "authors": ["Lesly Miculicich", "Mihir Parmar", "Hamid Palangi", "Krishnamurthy Dj Dvijotham", "Mirko Montanari", "Tomas Pfister", "Long T. Le"], "title": "VeriGuard: Enhancing LLM Agent Safety via Verified Code Generation", "comment": "22 pages", "summary": "The deployment of autonomous AI agents in sensitive domains, such as\nhealthcare, introduces critical risks to safety, security, and privacy. These\nagents may deviate from user objectives, violate data handling policies, or be\ncompromised by adversarial attacks. Mitigating these dangers necessitates a\nmechanism to formally guarantee that an agent's actions adhere to predefined\nsafety constraints, a challenge that existing systems do not fully address. We\nintroduce VeriGuard, a novel framework that provides formal safety guarantees\nfor LLM-based agents through a dual-stage architecture designed for robust and\nverifiable correctness. The initial offline stage involves a comprehensive\nvalidation process. It begins by clarifying user intent to establish precise\nsafety specifications. VeriGuard then synthesizes a behavioral policy and\nsubjects it to both testing and formal verification to prove its compliance\nwith these specifications. This iterative process refines the policy until it\nis deemed correct. Subsequently, the second stage provides online action\nmonitoring, where VeriGuard operates as a runtime monitor to validate each\nproposed agent action against the pre-verified policy before execution. This\nseparation of the exhaustive offline validation from the lightweight online\nmonitoring allows formal guarantees to be practically applied, providing a\nrobust safeguard that substantially improves the trustworthiness of LLM agents.", "AI": {"tldr": "VeriGuard\u662f\u4e00\u4e2a\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\u7684\u53cc\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u9a8c\u8bc1\u548c\u5728\u7ebf\u76d1\u63a7\u786e\u4fdd\u667a\u80fd\u4f53\u884c\u4e3a\u7b26\u5408\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u5728\u533b\u7597\u7b49\u654f\u611f\u9886\u57df\u90e8\u7f72\u81ea\u4e3bAI\u667a\u80fd\u4f53\u5b58\u5728\u4e25\u91cd\u7684\u5b89\u5168\u3001\u9690\u79c1\u548c\u5bf9\u6297\u653b\u51fb\u98ce\u9669\uff0c\u73b0\u6709\u7cfb\u7edf\u65e0\u6cd5\u5b8c\u5168\u4fdd\u8bc1\u667a\u80fd\u4f53\u884c\u4e3a\u7b26\u5408\u9884\u8bbe\u5b89\u5168\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u53cc\u9636\u6bb5\u67b6\u6784\uff1a\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u660e\u786e\u7528\u6237\u610f\u56fe\u3001\u5408\u6210\u884c\u4e3a\u7b56\u7565\u5e76\u8fdb\u884c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6765\u786e\u4fdd\u7b56\u7565\u6b63\u786e\u6027\uff1b\u5728\u7ebf\u9636\u6bb5\u4f5c\u4e3a\u8fd0\u884c\u65f6\u76d1\u63a7\u5668\u9a8c\u8bc1\u6bcf\u4e2a\u62df\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u9645\u5e94\u7528\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8LLM\u667a\u80fd\u4f53\u7684\u53ef\u4fe1\u5ea6\u3002", "conclusion": "VeriGuard\u901a\u8fc7\u5206\u79bb\u79bb\u7ebf\u9a8c\u8bc1\u548c\u5728\u7ebf\u76d1\u63a7\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u4fdd\u969c\u673a\u5236\u3002"}}
{"id": "2510.05365", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05365", "abs": "https://arxiv.org/abs/2510.05365", "authors": ["Irtaza Sajid Qureshi", "Zhen Ming", "Jiang"], "title": "Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to automated software\ntesting, yet their ability to generalize beyond memorized patterns and reason\nabout natural language bug reports remains unclear. We present a systematic\nevaluation of LLM reasoning in test case generation, structured around the\ncognitive layers of Bloom's taxonomy: \\textit{Remember}, \\textit{Understand},\n\\textit{Apply}, \\textit{Analyze}, \\textit{Evaluate}, and \\textit{Create}, which\nprogressively assess higher levels of cognitive and reasoning capabilities.\nBuilding on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J,\nGHRB, and mutated variants that introduce linguistic and semantic challenges.\nOur findings show that both models largely reproduce prior results with minor\ndeviations (\\textit{Remember}), exhibit partial robustness to linguistic\nrephrasings and translations while uncovering unique reproducible bugs\n(\\textit{Understand}), but suffer severe performance drops exceeding 60\\% under\nidentifier mutations (\\textit{Apply}). Conversely, providing near-identical\nfew-shot examples in an open-book setting improves success rates by up to three\ntimes, and component-level analysis reveals that structured technical elements,\nsuch as test code and method names, are far more impactful than narrative\ndescriptions for successful test generation (\\textit{Analyze}). These insights\nilluminate the cognitive processes underlying LLM-generated tests, suggest\nconcrete directions for improving performance, and establish a robust and\nrealistic evaluation paradigm for this task.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u5728\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u57fa\u4e8eBloom\u8ba4\u77e5\u5206\u7c7b\u5b66\u6846\u67b6\uff0c\u53d1\u73b0LLM\u5728\u8bb0\u5fc6\u5c42\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5e94\u7528\u5c42\u9762\u9762\u4e34\u4e25\u91cd\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6807\u8bc6\u7b26\u7a81\u53d8\u65f6\u6027\u80fd\u4e0b\u964d\u8d85\u8fc760%\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63a2\u7a76\u5176\u662f\u5426\u80fd\u8d85\u8d8a\u8bb0\u5fc6\u6a21\u5f0f\u5e76\u771f\u6b63\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u9519\u8bef\u62a5\u544a\u3002", "method": "\u57fa\u4e8eLIBRO\u6846\u67b6\uff0c\u4f7f\u7528Bloom\u8ba4\u77e5\u5206\u7c7b\u5b66\u7684\u516d\u4e2a\u5c42\u6b21\uff08\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u5e94\u7528\u3001\u5206\u6790\u3001\u8bc4\u4f30\u3001\u521b\u9020\uff09\uff0c\u5728Defects4J\u3001GHRB\u6570\u636e\u96c6\u53ca\u5176\u53d8\u4f53\u4e0a\u8bc4\u4f30StarCoder\u548cGPT-4o\u6a21\u578b\u3002", "result": "LLM\u80fd\u591f\u590d\u73b0\u5148\u524d\u7ed3\u679c\uff08\u8bb0\u5fc6\u5c42\uff09\uff0c\u5bf9\u8bed\u8a00\u91cd\u8ff0\u548c\u7ffb\u8bd1\u5177\u6709\u90e8\u5206\u9c81\u68d2\u6027\uff08\u7406\u89e3\u5c42\uff09\uff0c\u4f46\u5728\u6807\u8bc6\u7b26\u7a81\u53d8\u65f6\u6027\u80fd\u4e0b\u964d\u8d85\u8fc760%\uff08\u5e94\u7528\u5c42\uff09\u3002\u5f00\u653e\u4e66\u672c\u8bbe\u7f6e\u4e0b\u7684\u5c11\u6837\u672c\u5b66\u4e60\u53ef\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u4e09\u500d\uff0c\u7ed3\u6784\u5316\u6280\u672f\u5143\u7d20\u6bd4\u53d9\u8ff0\u6027\u63cf\u8ff0\u5bf9\u6d4b\u8bd5\u751f\u6210\u66f4\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u751f\u6210\u6d4b\u8bd5\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u4e3a\u6539\u8fdb\u6027\u80fd\u63d0\u4f9b\u4e86\u5177\u4f53\u65b9\u5411\uff0c\u5e76\u4e3a\u6b64\u4efb\u52a1\u5efa\u7acb\u4e86\u7a33\u5065\u73b0\u5b9e\u7684\u8bc4\u4f30\u8303\u5f0f\u3002"}}
{"id": "2510.05390", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05390", "abs": "https://arxiv.org/abs/2510.05390", "authors": ["Felicity Anderson", "Julien Sindt", "Neil Chue Hong"], "title": "Who Do You Think You Are? Creating RSE Personas from GitHub Interactions", "comment": "36 pages. Invited extended paper of original poster at deRSE2025. To\n  be published in ECEASST", "summary": "We describe data-driven RSE personas: an approach combining software\nrepository mining and data-driven personas applied to research software (RS),\nan attempt to describe and identify common and rare patterns of Research\nSoftware Engineering (RSE) development. This allows individuals and RS project\nteams to understand their contributions, impact and repository dynamics - an\nimportant foundation for improving RSE. We evaluate the method on different\npatterns of collaborative interaction behaviours by contributors to mid-sized\npublic RS repositories (those with 10-300 committers) on GitHub. We demonstrate\nhow the RSE personas method successfully characterises a sample of 115,174\nrepository contributors across 1,284 RS repositories on GitHub, sampled from\n42,284 candidate software repository records queried from Zenodo. We identify,\nname and summarise seven distinct personas from low to high interactivity:\nEphemeral Contributor; Occasional Contributor; Project Organiser; Moderate\nContributor; Low-Process Closer; Low-Coding Closer; and Active Contributor.\nThis demonstrates that large datasets can be analysed despite difficulties of\ncomparing software projects with different project management factors, research\ndomains and contributor backgrounds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f6f\u4ef6\u4ed3\u5e93\u6316\u6398\u548c\u6570\u636e\u9a71\u52a8\u89d2\u8272\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u5e38\u89c1\u548c\u7f55\u89c1\u5f00\u53d1\u6a21\u5f0f\uff0c\u901a\u8fc7\u5bf9GitHub\u4e0a115,174\u540d\u8d21\u732e\u8005\u7684\u5206\u6790\u8bc6\u522b\u51fa7\u79cd\u4e0d\u540c\u4ea4\u4e92\u7a0b\u5ea6\u7684\u89d2\u8272\u7c7b\u578b\u3002", "motivation": "\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u89d2\u8272\u63cf\u8ff0\u65b9\u6cd5\uff0c\u96be\u4ee5\u7406\u89e3\u8d21\u732e\u8005\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u9879\u76ee\u52a8\u6001\uff0c\u8fd9\u963b\u788d\u4e86RSE\u7684\u6539\u8fdb\u548c\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u8f6f\u4ef6\u4ed3\u5e93\u6316\u6398\u548c\u6570\u636e\u9a71\u52a8\u89d2\u8272\u65b9\u6cd5\uff0c\u5206\u6790GitHub\u4e0a1,284\u4e2a\u7814\u7a76\u8f6f\u4ef6\u4ed3\u5e93\u7684\u8d21\u732e\u8005\u884c\u4e3a\u6a21\u5f0f\uff0c\u4ece42,284\u4e2a\u5019\u9009\u4ed3\u5e93\u4e2d\u62bd\u6837\u7814\u7a76\u3002", "result": "\u6210\u529f\u8bc6\u522b\u5e76\u547d\u540d\u4e867\u79cd\u4ece\u4f4e\u5230\u9ad8\u4ea4\u4e92\u7a0b\u5ea6\u7684\u89d2\u8272\uff1a\u4e34\u65f6\u8d21\u732e\u8005\u3001\u5076\u5c14\u8d21\u732e\u8005\u3001\u9879\u76ee\u7ec4\u7ec7\u8005\u3001\u9002\u5ea6\u8d21\u732e\u8005\u3001\u4f4e\u6d41\u7a0b\u5b8c\u6210\u8005\u3001\u4f4e\u7f16\u7801\u5b8c\u6210\u8005\u548c\u6d3b\u8dc3\u8d21\u732e\u8005\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5927\u6570\u636e\u96c6\u5206\u6790\u5728\u6bd4\u8f83\u5177\u6709\u4e0d\u540c\u9879\u76ee\u7ba1\u7406\u56e0\u7d20\u3001\u7814\u7a76\u9886\u57df\u548c\u8d21\u732e\u8005\u80cc\u666f\u7684\u8f6f\u4ef6\u9879\u76ee\u65b9\u9762\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u7814\u7a76\u8f6f\u4ef6\u5de5\u7a0b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2510.05109", "categories": ["cs.DC", "cs.AI", "cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.05109", "abs": "https://arxiv.org/abs/2510.05109", "authors": ["Yilong Li", "Shuai Zhang", "Yijing Zeng", "Hao Zhang", "Xinmiao Xiong", "Jingyu Liu", "Pan Hu", "Suman Banerjee"], "title": "Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices", "comment": null, "summary": "Large Multimodal Models (LMMs) are inherently modular, consisting of vision\nand audio encoders, projectors, and large language models. Yet, they are almost\nalways executed monolithically, which underutilizes the heterogeneous\naccelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end\nlatency. In this paper, we present NANOMIND, a hardware--software co-design\ninference framework for Large Multimodal Models (LMMs) that breaks large models\ninto modular ``bricks'' (vision, language, audio, etc.) and maps each to its\nideal accelerator. The key insight is that large models can be broken into\nmodular components and scheduled to run on the most appropriate compute units.\nIt performs module-level dynamic offloading across accelerators on\nunified-memory SoCs. By combining customized hardware design, system-level\nscheduling, and optimized low-bit computation kernels, we demonstrate our\nframework with a compact, battery-powered device capable of running LMMs\nentirely on device. This prototype functions as a self-contained intelligent\nassistant that requires no network connectivity, while achieving higher\nthroughput and superior power efficiency under strict resource constraints. The\ndesign further bypasses CPU bottlenecks and reduces redundant memory usage\nthrough token-aware buffer management and module-level coordination. Our system\noutperforms existing implementations in resource efficiency, cutting energy\nconsumption by 42.3\\% and GPU memory usage by 11.2\\%. This enables a\nbattery-powered device to run LLaVA-OneVision with a camera for nearly half a\nday and LLaMA-3-8B for voice interactions up to almost 20.8 hours.", "AI": {"tldr": "NANOMIND\u662f\u4e00\u4e2a\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u578b\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7ec4\u4ef6\u5e76\u5728\u5f02\u6784\u52a0\u901f\u5668\u4e0a\u52a8\u6001\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u901a\u5e38\u4ee5\u6574\u4f53\u65b9\u5f0f\u6267\u884c\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u4ee3SoC\u4e2d\u7684\u5f02\u6784\u52a0\u901f\u5668\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u4f4e\u80fd\u6548\u3002", "method": "\u5c06\u5927\u578b\u6a21\u578b\u5206\u89e3\u4e3a\u6a21\u5757\u5316\"\u7816\u5757\"\uff0c\u901a\u8fc7\u6a21\u5757\u7ea7\u52a8\u6001\u5378\u8f7d\u6280\u672f\u5c06\u5404\u7ec4\u4ef6\u6620\u5c04\u5230\u6700\u5408\u9002\u7684\u52a0\u901f\u5668\u4e0a\uff0c\u7ed3\u5408\u5b9a\u5236\u786c\u4ef6\u8bbe\u8ba1\u3001\u7cfb\u7edf\u7ea7\u8c03\u5ea6\u548c\u4f18\u5316\u7684\u4f4e\u6bd4\u7279\u8ba1\u7b97\u5185\u6838\u3002", "result": "\u7cfb\u7edf\u5728\u8d44\u6e90\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u5b9e\u73b0\uff0c\u80fd\u8017\u964d\u4f4e42.3%\uff0cGPU\u5185\u5b58\u4f7f\u7528\u51cf\u5c1111.2%\uff0c\u7535\u6c60\u4f9b\u7535\u8bbe\u5907\u53ef\u8fd0\u884cLLaVA-OneVision\u8fd1\u534a\u5929\uff0cLLaMA-3-8B\u8bed\u97f3\u4ea4\u4e92\u8fbe20.8\u5c0f\u65f6\u3002", "conclusion": "NANOMIND\u6846\u67b6\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u548c\u6a21\u5757\u5316\u8c03\u5ea6\uff0c\u5b9e\u73b0\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4e3a\u8fb9\u7f18\u667a\u80fd\u8bbe\u5907\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05612", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.05612", "abs": "https://arxiv.org/abs/2510.05612", "authors": ["Utsav Pathak", "Amit Mankodi"], "title": "Redefining Cost Estimation in Database Systems: The Role of Execution Plan Features and Machine Learning", "comment": "12 pages, 5 figures, conference", "summary": "Accurate query runtime prediction is a critical component of effective query\noptimization in modern database systems. Traditional cost models, such as those\nused in PostgreSQL, rely on static heuristics that often fail to reflect actual\nquery performance under complex and evolving workloads. This remains an active\narea of research, with recent work exploring machine learning techniques to\nreplace or augment traditional cost estimators. In this paper, we present a\nmachine learning-based framework for predicting SQL query runtimes using\nexecution plan features extracted from PostgreSQL. Our approach integrates\nscalar and structural features from execution plans and semantic\nrepresentations of SQL queries to train predictive models. We construct an\nautomated pipeline for data collection and feature extraction using\nparameterized TPC-H queries, enabling systematic evaluation of multiple\nmodeling techniques. Unlike prior efforts that focus either on cardinality\nestimation or on synthetic cost metrics, we model the actual runtimes using\nfine-grained plan statistics and query embeddings derived from execution\ntraces, to improve the model accuracy. We compare baseline regressors, a\nrefined XGBoost model, and a sequential LSTM-based model to assess their\neffectiveness in runtime prediction. Our dataset includes over 1000 queries\ngenerated from TPC-H query templates executed in PostgreSQL with EXPLAIN\nANALYZE. Experimental results show that the XGBoost model significantly\noutperforms others, achieving a mean squared error of 0.3002 and prediction\naccuracy within 10% of the true runtime in over 65% of cases. The findings\nhighlight the potential of tree-based learning combined with execution plan\nfeatures for improving cost estimation in query optimizers.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684SQL\u67e5\u8be2\u8fd0\u884c\u65f6\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528PostgreSQL\u6267\u884c\u8ba1\u5212\u7279\u5f81\u548c\u67e5\u8be2\u8bed\u4e49\u8868\u793a\uff0cXGBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u572865%\u60c5\u51b5\u4e0b\u9884\u6d4b\u8bef\u5dee\u572810%\u4ee5\u5185\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u9759\u6001\u542f\u53d1\u5f0f\u6210\u672c\u6a21\u578b\u5728\u590d\u6742\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u67e5\u8be2\u6027\u80fd\uff0c\u9700\u8981\u673a\u5668\u5b66\u4e60\u6280\u672f\u6765\u6539\u8fdb\u67e5\u8be2\u4f18\u5316\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u548c\u7279\u5f81\u63d0\u53d6\u6d41\u6c34\u7ebf\uff0c\u6574\u5408\u6267\u884c\u8ba1\u5212\u7684\u6807\u91cf\u548c\u7ed3\u6784\u7279\u5f81\u4ee5\u53caSQL\u67e5\u8be2\u7684\u8bed\u4e49\u8868\u793a\uff0c\u6bd4\u8f83\u57fa\u7ebf\u56de\u5f52\u5668\u3001\u6539\u8fdb\u7684XGBoost\u6a21\u578b\u548c\u57fa\u4e8eLSTM\u7684\u5e8f\u5217\u6a21\u578b\u3002", "result": "XGBoost\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5747\u65b9\u8bef\u5dee\u4e3a0.3002\uff0c\u572865%\u4ee5\u4e0a\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u51c6\u786e\u7387\u5728\u771f\u5b9e\u8fd0\u884c\u65f6\u768410%\u4ee5\u5185\u3002", "conclusion": "\u57fa\u4e8e\u6811\u7684\u673a\u5668\u5b66\u4e60\u4e0e\u6267\u884c\u8ba1\u5212\u7279\u5f81\u7ed3\u5408\u5728\u6539\u8fdb\u67e5\u8be2\u4f18\u5316\u5668\u6210\u672c\u4f30\u8ba1\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.05441", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05441", "abs": "https://arxiv.org/abs/2510.05441", "authors": ["Yiannis Charalambous", "Claudionor N. Coelho Jr", "Luis Lamb", "Lucas C. Cordeiro"], "title": "UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification", "comment": null, "summary": "This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent\nsystem designed to generate unit tests for legacy code, enhancing test coverage\nand critical value testing. UnitTenX leverages a combination of AI agents,\nformal methods, and Large Language Models (LLMs) to automate test generation,\naddressing the challenges posed by complex and legacy codebases. Despite the\nlimitations of LLMs in bug detection, UnitTenX offers a robust framework for\nimproving software reliability and maintainability. Our results demonstrate the\neffectiveness of this approach in generating high-quality tests and identifying\npotential issues. Additionally, our approach enhances the readability and\ndocumentation of legacy code.", "AI": {"tldr": "UnitTenX\u662f\u4e00\u4e2a\u5f00\u6e90AI\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e3a\u9057\u7559\u4ee3\u7801\u751f\u6210\u5355\u5143\u6d4b\u8bd5\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u8986\u76d6\u7387\u548c\u5173\u952e\u503c\u6d4b\u8bd5\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u9057\u7559\u4ee3\u7801\u5e93\u7684\u6d4b\u8bd5\u751f\u6210\u6311\u6218\uff0c\u63d0\u9ad8\u8f6f\u4ef6\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u7ed3\u5408AI\u667a\u80fd\u4f53\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u6765\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u3002", "result": "\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u5e76\u8bc6\u522b\u6f5c\u5728\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u9ad8\u9057\u7559\u4ee3\u7801\u7684\u53ef\u8bfb\u6027\u548c\u6587\u6863\u5316\u3002", "conclusion": "\u5c3d\u7ba1LLMs\u5728\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46UnitTenX\u4e3a\u6539\u5584\u8f6f\u4ef6\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7a33\u5065\u6846\u67b6\u3002"}}
{"id": "2510.05111", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05111", "abs": "https://arxiv.org/abs/2510.05111", "authors": ["Ian McDougall", "Noah Scott", "Joon Huh", "Kirthevasan Kandasamy", "Karthikeyan Sankaralingam"], "title": "Agora: Bridging the GPU Cloud Resource-Price Disconnect", "comment": "15 pages, 6 figures", "summary": "The historic trend of Moore's Law, which predicted exponential growth in\ncomputational performance per dollar, has diverged for modern Graphics\nProcessing Units (GPUs). While Floating Point Operations per Second (FLOPs)\ncapabilities have continued to scale economically, memory bandwidth has not,\ncreating a significant price-performance disconnect. This paper argues that the\nprevailing time-based pricing models for cloud GPUs are economically\ninefficient for bandwidth-bound workloads. These models fail to account for the\nrising marginal cost of memory bandwidth, leading to market distortions and\nsuboptimal hardware allocation. To address this, we propose a novel\nfeature-based pricing framework that directly links cost to resource\nconsumption, including but not limited to memory bandwidth. We provide a robust\neconomic and algorithmic definition of this framework and introduce Agora, a\npractical and secure system architecture for its implementation. Our\nimplementation of Agora shows that a 50us sampling provides nearly perfect\npricing as what ideal sampling would provide - losing only 5\\% of revenue. 10us\nsampling is even better result in 2.4\\% loss. Modern telemetry systems can\nalready provide this rate of measurement, and our prototype implementation\nshows the system design for feature-based pricing is buildable. Our evaluation\nacross diverse GPU applications and hardware generations empirically validates\nthe effectiveness of our approach in creating a more transparent and efficient\nmarket for cloud GPU resources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7684GPU\u4e91\u670d\u52a1\u5b9a\u4ef7\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3FLOPs\u4e0e\u5185\u5b58\u5e26\u5bbd\u6027\u80fd\u589e\u957f\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u5b9a\u4ef7\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3GPU\u7684\u6d6e\u70b9\u8fd0\u7b97\u80fd\u529b\u6301\u7eed\u6309\u6469\u5c14\u5b9a\u5f8b\u589e\u957f\uff0c\u4f46\u5185\u5b58\u5e26\u5bbd\u589e\u957f\u6ede\u540e\uff0c\u5bfc\u81f4\u57fa\u4e8e\u65f6\u95f4\u7684\u4f20\u7edf\u5b9a\u4ef7\u6a21\u578b\u5bf9\u5e26\u5bbd\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u7ecf\u6d4e\uff0c\u9020\u6210\u5e02\u573a\u626d\u66f2\u548c\u786c\u4ef6\u5206\u914d\u4f4e\u6548\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7279\u5f81\u7684\u5b9a\u4ef7\u6846\u67b6\uff0c\u5c06\u6210\u672c\u76f4\u63a5\u4e0e\u8d44\u6e90\u6d88\u8017\uff08\u5305\u62ec\u5185\u5b58\u5e26\u5bbd\uff09\u6302\u94a9\uff0c\u5e76\u8bbe\u8ba1\u4e86Agora\u7cfb\u7edf\u67b6\u6784\u6765\u5b9e\u73b0\u8fd9\u4e00\u6846\u67b6\uff0c\u652f\u630150\u5fae\u79d2\u548c10\u5fae\u79d2\u7684\u91c7\u6837\u9891\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c50\u5fae\u79d2\u91c7\u6837\u53ef\u5b9e\u73b0\u8fd1\u4e4e\u7406\u60f3\u7684\u5b9a\u4ef7\u6548\u679c\uff0c\u4ec5\u635f\u59315%\u6536\u5165\uff1b10\u5fae\u79d2\u91c7\u6837\u6548\u679c\u66f4\u597d\uff0c\u4ec5\u635f\u59312.4%\u6536\u5165\u3002\u73b0\u4ee3\u9065\u6d4b\u7cfb\u7edf\u5df2\u80fd\u652f\u6301\u8fd9\u79cd\u6d4b\u91cf\u9891\u7387\u3002", "conclusion": "\u8be5\u57fa\u4e8e\u7279\u5f81\u7684\u5b9a\u4ef7\u65b9\u6cd5\u80fd\u591f\u4e3a\u4e91GPU\u8d44\u6e90\u521b\u5efa\u66f4\u900f\u660e\u9ad8\u6548\u7684\u5e02\u573a\uff0c\u5728\u4e0d\u540cGPU\u5e94\u7528\u548c\u786c\u4ef6\u4ee3\u9645\u4e0a\u90fd\u5f97\u5230\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002"}}
{"id": "2510.05907", "categories": ["cs.DB", "cs.PF", "cs.SE", "H.2.4; E.1"], "pdf": "https://arxiv.org/pdf/2510.05907", "abs": "https://arxiv.org/abs/2510.05907", "authors": ["Dmitrii Radivonchik", "Yakov Kuzin", "Anton Chizhov", "Dmitriy Shcheka", "Mikhail Firsov", "Kirill Smirnov", "George Chernishev"], "title": "Speeding up SQL subqueries via decoupling of non-correlated predicate (extended version)", "comment": null, "summary": "In this paper, we discuss a novel technique for processing correlated\nsubqueries in SQL. The core idea is to isolate the non-correlated part of the\npredicate and use it to reduce the number of evaluations of the correlated\npart. We begin by providing an overview of several classes of queries that may\nbenefit from this technique. For each class, we propose a potential rewrite and\ndiscuss the conditions under which it is advantageous. Next, we address the\nevaluation aspects of the proposed rewrites: 1) we describe our approach to\nadapting the block-based Volcano query processing model, and 2) we discuss the\nbenefits of implementing that technique within a position-enabled column-store\nwith late materialization support. Finally, we present a simple cost model that\nallows estimation of the benefits of said rewrites.\n  Our evaluation has a quantitative part and a qualitative part. The former\nfocuses on studying the impact of non-correlated predicate selectivity on our\ntechnique. The latter identifies the limitations of our approach by comparing\nit with alternative approaches available in existing systems. Overall,\nexperiments conducted using PosDB (a position-enabled column-store) and\nPostgreSQL demonstrated that, under suitable conditions, our technique can\nachieve a 5x improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406SQL\u76f8\u5173\u5b50\u67e5\u8be2\u7684\u65b0\u6280\u672f\uff0c\u901a\u8fc7\u5206\u79bb\u975e\u76f8\u5173\u8c13\u8bcd\u90e8\u5206\u6765\u51cf\u5c11\u76f8\u5173\u90e8\u5206\u7684\u8bc4\u4f30\u6b21\u6570\uff0c\u5728\u5408\u9002\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b05\u500d\u6027\u80fd\u63d0\u5347", "motivation": "\u63d0\u9ad8\u76f8\u5173\u5b50\u67e5\u8be2\u7684\u5904\u7406\u6548\u7387\uff0c\u901a\u8fc7\u4f18\u5316\u67e5\u8be2\u91cd\u5199\u6765\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97", "method": "1) \u8bc6\u522b\u53ef\u53d7\u76ca\u7684\u67e5\u8be2\u7c7b\u578b\u5e76\u63d0\u51fa\u91cd\u5199\u65b9\u6848\uff1b2) \u5728\u57fa\u4e8e\u5757\u7684Volcano\u67e5\u8be2\u5904\u7406\u6a21\u578b\u4e2d\u9002\u914d\u8be5\u65b9\u6cd5\uff1b3) \u5728\u652f\u6301\u4f4d\u7f6e\u611f\u77e5\u548c\u5ef6\u8fdf\u7269\u5316\u7684\u5217\u5b58\u50a8\u4e2d\u5b9e\u73b0", "result": "\u5728PosDB\u548cPostgreSQL\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5408\u9002\u6761\u4ef6\u4e0b\u6027\u80fd\u53ef\u63d0\u53475\u500d", "conclusion": "\u8be5\u6280\u672f\u80fd\u6709\u6548\u4f18\u5316\u76f8\u5173\u5b50\u67e5\u8be2\u5904\u7406\uff0c\u7279\u522b\u662f\u5728\u975e\u76f8\u5173\u8c13\u8bcd\u9009\u62e9\u6027\u8f83\u9ad8\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u663e\u8457"}}
{"id": "2510.05450", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05450", "abs": "https://arxiv.org/abs/2510.05450", "authors": ["Saul Goldman", "Hong Yi Lin", "Jirat Pasuksmit", "Patanamon Thongtanunam", "Kla Tantithamthavorn", "Zhe Wang", "Ray Zhang", "Ali Behnaz", "Fan Jiang", "Michael Siers", "Ryan Jiang", "Mike Buller", "Minwoo Jeong", "Ming Wu"], "title": "What Types of Code Review Comments Do Developers Most Frequently Resolve?", "comment": "The paper has been accepted the 40th IEEE/ACM International\n  Conference on Automated Software Engineering, ASE 2025", "summary": "Large language model (LLM)-powered code review automation tools have been\nintroduced to generate code review comments. However, not all generated\ncomments will drive code changes. Understanding what types of generated review\ncomments are likely to trigger code changes is crucial for identifying those\nthat are actionable. In this paper, we set out to investigate (1) the types of\nreview comments written by humans and LLMs, and (2) the types of generated\ncomments that are most frequently resolved by developers. To do so, we\ndeveloped an LLM-as-a-Judge to automatically classify review comments based on\nour own taxonomy of five categories. Our empirical study confirms that (1) the\nLLM reviewer and human reviewers exhibit distinct strengths and weaknesses\ndepending on the project context, and (2) readability, bugs, and\nmaintainability-related comments had higher resolution rates than those focused\non code design. These results suggest that a substantial proportion of\nLLM-generated comments are actionable and can be resolved by developers. Our\nwork highlights the complementarity between LLM and human reviewers and offers\nsuggestions to improve the practical effectiveness of LLM-powered code review\ntools.", "AI": {"tldr": "\u7814\u7a76LLM\u751f\u6210\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u4e2d\u54ea\u4e9b\u7c7b\u578b\u6700\u53ef\u80fd\u89e6\u53d1\u4ee3\u7801\u53d8\u66f4\uff0c\u53d1\u73b0\u53ef\u8bfb\u6027\u3001bug\u548c\u7ef4\u62a4\u6027\u76f8\u5173\u7684\u8bc4\u8bba\u6bd4\u4ee3\u7801\u8bbe\u8ba1\u76f8\u5173\u7684\u8bc4\u8bba\u66f4\u6613\u88ab\u5f00\u53d1\u8005\u91c7\u7eb3\u3002", "motivation": "\u7406\u89e3LLM\u751f\u6210\u7684\u4ee3\u7801\u5ba1\u67e5\u8bc4\u8bba\u4e2d\u54ea\u4e9b\u7c7b\u578b\u6700\u53ef\u80fd\u88ab\u5f00\u53d1\u8005\u91c7\u7eb3\u5e76\u89e6\u53d1\u4ee3\u7801\u53d8\u66f4\uff0c\u8fd9\u5bf9\u4e8e\u8bc6\u522b\u53ef\u64cd\u4f5c\u7684\u8bc4\u8bba\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86LLM-as-a-Judge\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e94\u7c7b\u5206\u7c7b\u6cd5\u81ea\u52a8\u5206\u7c7b\u5ba1\u67e5\u8bc4\u8bba\uff0c\u5e76\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4eba\u7c7b\u548cLLM\u8bc4\u8bba\u8005\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "LLM\u548c\u4eba\u7c7b\u5ba1\u67e5\u8005\u5728\u4e0d\u540c\u9879\u76ee\u80cc\u666f\u4e0b\u5404\u6709\u4f18\u52bf\uff1b\u53ef\u8bfb\u6027\u3001bug\u548c\u7ef4\u62a4\u6027\u76f8\u5173\u7684\u8bc4\u8bba\u89e3\u51b3\u7387\u9ad8\u4e8e\u4ee3\u7801\u8bbe\u8ba1\u76f8\u5173\u7684\u8bc4\u8bba\u3002", "conclusion": "\u5927\u90e8\u5206LLM\u751f\u6210\u7684\u8bc4\u8bba\u662f\u53ef\u64cd\u4f5c\u7684\uff0cLLM\u548c\u4eba\u7c7b\u5ba1\u67e5\u8005\u5177\u6709\u4e92\u8865\u6027\uff0c\u7814\u7a76\u4e3a\u63d0\u9ad8LLM\u9a71\u52a8\u7684\u4ee3\u7801\u5ba1\u67e5\u5de5\u5177\u7684\u5b9e\u9645\u6548\u679c\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002"}}
{"id": "2510.05112", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05112", "abs": "https://arxiv.org/abs/2510.05112", "authors": ["Lijuan Jiang", "Xingjian Qian", "Zhenxiang Ma", "Zan Zong", "Hengjie Li", "Chao Yang", "Jidong Zhai"], "title": "A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training", "comment": null, "summary": "Pipeline parallelism is an essential distributed parallelism method.\nIncreasingly complex and diverse DNN models necessitate meticulously customized\npipeline schedules for performance. However, existing practices typically rely\non predefined schedules, each with strengths, but fail to adapt automatically\nto the emerging model architectures. Exploring novel high-efficiency schedules\nis daunting due to the enormous and varying schedule space. Besides, manually\nimplementing schedules can be challenging due to the onerous coding burdens and\nconstantly changing needs. Unfortunately, existing frameworks have limitations\nin automated schedule exploration and lack flexibility and controllability.\n  This paper presents FlexPipe, a programmable pipeline parallelism framework\nwith enhanced productivity, programmability, debuggability, and ease of tuning.\nFlexPipe has two main components: a succinct domain-specific language (DSL) and\nan automated scheduler. FlexPipe enables automated schedule exploration for\nvarious parallel scenarios within a broad spectrum of schedule types at a small\nsearch cost. Besides, users can swiftly develop and customize schedules using\nthe FlexPipe DSL, which embodies flexible controllability in the pipeline order\nof micro-batch computations over stages. It also provides convenient mechanisms\nto include new operations in schedules to meet changing demands. Our evaluation\nresults demonstrate that FlexPipe achieves up to 2.28X performance speedup\ncompared to the popular large-scale parallel framework Megtron-LM, and gains up\nto 1.49X performance speedup compared to the state-of-the-art automated\npipeline parallelism framework.", "AI": {"tldr": "FlexPipe\u662f\u4e00\u4e2a\u53ef\u7f16\u7a0b\u7684\u6d41\u6c34\u7ebf\u5e76\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u548c\u81ea\u52a8\u8c03\u5ea6\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6d41\u6c34\u7ebf\u8c03\u5ea6\u81ea\u52a8\u63a2\u7d22\u548c\u7075\u6d3b\u5b9a\u5236\uff0c\u76f8\u6bd4\u73b0\u6709\u6846\u67b6\u83b7\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u8c03\u5ea6\u7b56\u7565\uff0c\u65e0\u6cd5\u81ea\u52a8\u9002\u5e94\u65b0\u5174\u6a21\u578b\u67b6\u6784\uff0c\u4e14\u624b\u52a8\u5b9e\u73b0\u8c03\u5ea6\u4ee3\u7801\u8d1f\u62c5\u91cd\u3001\u7075\u6d3b\u6027\u5dee\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFlexPipe\u6846\u67b6\uff0c\u5305\u542b\u7b80\u6d01\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00(DSL)\u548c\u81ea\u52a8\u8c03\u5ea6\u5668\uff0c\u652f\u6301\u5e7f\u6cdb\u7684\u8c03\u5ea6\u7c7b\u578b\u81ea\u52a8\u63a2\u7d22\uff0c\u5e76\u63d0\u4f9b\u7075\u6d3b\u7684\u5fae\u6279\u6b21\u8ba1\u7b97\u987a\u5e8f\u63a7\u5236\u673a\u5236\u3002", "result": "\u76f8\u6bd4\u4e3b\u6d41\u5927\u89c4\u6a21\u5e76\u884c\u6846\u67b6Megtron-LM\u83b7\u5f97\u6700\u9ad82.28\u500d\u6027\u80fd\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6d41\u6c34\u7ebf\u5e76\u884c\u6846\u67b6\u83b7\u5f97\u6700\u9ad81.49\u500d\u6027\u80fd\u52a0\u901f\u3002", "conclusion": "FlexPipe\u901a\u8fc7\u81ea\u52a8\u5316\u8c03\u5ea6\u63a2\u7d22\u548c\u7075\u6d3b\u7684\u7a0b\u5e8f\u5316\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u6c34\u7ebf\u5e76\u884c\u7684\u751f\u4ea7\u529b\u548c\u6027\u80fd\uff0c\u4e3a\u590d\u6742DNN\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5e76\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05604", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05604", "abs": "https://arxiv.org/abs/2510.05604", "authors": ["Rintaro Kanaji", "Brittany Reid", "Yutaro Kashiwa", "Raula Gaikovina Kula", "Hajimu Iida"], "title": "An Empirical Study of Security-Policy Related Issues in Open Source Projects", "comment": "Accepted in PROFES 2025", "summary": "GitHub recommends that projects adopt a SECURITY.md file that outlines\nvulnerability reporting procedures. However, the effectiveness and operational\nchallenges of such files are not yet fully understood. This study aims to\nclarify the challenges that SECURITY.md files face in the vulnerability\nreporting process within open-source communities. Specifically, we classified\nand analyzed the content of 711 randomly sampled issues related to SECURITY.md.\nWe also conducted a quantitative comparative analysis of the close time and\nnumber of responses for issues concerning six community health files, including\nSECURITY.md. Our analysis revealed that 79.5% of SECURITY.md-related issues\nwere requests to add the file, and reports that included links were closed,\nwith a median time that was 2 days shorter. These findings offer practical\ninsights for improving security reporting policies and community management,\nultimately contributing to a more secure open-source ecosystem.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86GitHub\u9879\u76ee\u4e2dSECURITY.md\u6587\u4ef6\u5728\u6f0f\u6d1e\u62a5\u544a\u8fc7\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u548c\u64cd\u4f5c\u6311\u6218\uff0c\u901a\u8fc7\u5bf9711\u4e2a\u76f8\u5173issue\u7684\u5206\u7c7b\u5206\u6790\u53d1\u73b079.5%\u662f\u6dfb\u52a0\u6587\u4ef6\u7684\u8bf7\u6c42\uff0c\u5305\u542b\u94fe\u63a5\u7684\u62a5\u544a\u5173\u95ed\u65f6\u95f4\u4e2d\u4f4d\u6570\u7f29\u77ed2\u5929\u3002", "motivation": "\u7406\u89e3SECURITY.md\u6587\u4ef6\u5728\u5f00\u6e90\u793e\u533a\u6f0f\u6d1e\u62a5\u544a\u8fc7\u7a0b\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u548c\u64cd\u4f5c\u6311\u6218\uff0c\u4e3a\u6539\u8fdb\u5b89\u5168\u62a5\u544a\u653f\u7b56\u548c\u793e\u533a\u7ba1\u7406\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u5bf9711\u4e2a\u968f\u673a\u62bd\u6837\u7684SECURITY.md\u76f8\u5173issue\u8fdb\u884c\u5206\u7c7b\u5206\u6790\uff0c\u5e76\u5bf9\u5305\u62ecSECURITY.md\u5728\u5185\u76846\u4e2a\u793e\u533a\u5065\u5eb7\u6587\u4ef6\u7684\u5173\u95ed\u65f6\u95f4\u548c\u54cd\u5e94\u6b21\u6570\u8fdb\u884c\u5b9a\u91cf\u6bd4\u8f83\u5206\u6790\u3002", "result": "79.5%\u7684SECURITY.md\u76f8\u5173issue\u662f\u6dfb\u52a0\u6587\u4ef6\u7684\u8bf7\u6c42\uff1b\u5305\u542b\u94fe\u63a5\u7684\u62a5\u544a\u5173\u95ed\u65f6\u95f4\u4e2d\u4f4d\u6570\u7f29\u77ed2\u5929\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6539\u8fdb\u5b89\u5168\u62a5\u544a\u653f\u7b56\u548c\u793e\u533a\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2510.05118", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05118", "abs": "https://arxiv.org/abs/2510.05118", "authors": ["Cynthia Marcelino", "Noah Krennmair", "Thomas Pusztai", "Stefan Nastic"], "title": "Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum", "comment": null, "summary": "WebAssembly has emerged as a lightweight and portable runtime to execute\nserverless functions, particularly in heterogeneous and resource-constrained\nenvironments such as the Edge Cloud Continuum. However, the performance\nbenefits versus trade-offs remain insufficiently understood. This paper\npresents Lumos, a performance model and benchmarking tool for characterizing\nserverless runtimes. Lumos identifies workload, system, and environment-level\nperformance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art\ncontainers and the Wasm runtime in interpreted mode and with ahead-of-time\ncompilation. Our performance characterization shows that AoT-compiled Wasm\nimages are up to 30x smaller and decrease cold-start latency by up to 16%\ncompared to containers, while interpreted Wasm suffers up to 55x higher warm\nlatency and up to 10x I/O-serialization overhead.", "AI": {"tldr": "Lumos\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u6790\u65e0\u670d\u52a1\u5668\u8fd0\u884c\u65f6\u6027\u80fd\u7684\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u91cd\u70b9\u7814\u7a76WebAssembly\u5728\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "WebAssembly\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u53ef\u79fb\u690d\u8fd0\u884c\u65f6\u5728\u65e0\u670d\u52a1\u5668\u51fd\u6570\u6267\u884c\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u7b49\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\uff0c\u4f46\u5176\u6027\u80fd\u6536\u76ca\u4e0e\u6743\u8861\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "\u5f00\u53d1Lumos\u6027\u80fd\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u8bc6\u522b\u5de5\u4f5c\u8d1f\u8f7d\u3001\u7cfb\u7edf\u548c\u73af\u5883\u5c42\u9762\u7684\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\uff0c\u5bf9\u6700\u5148\u8fdb\u7684\u5bb9\u5668\u548cWasm\u8fd0\u884c\u65f6\uff08\u89e3\u91ca\u6a21\u5f0f\u4e0e\u9884\u7f16\u8bd1\u6a21\u5f0f\uff09\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u9884\u7f16\u8bd1Wasm\u955c\u50cf\u6bd4\u5bb9\u5668\u5c0f30\u500d\uff0c\u51b7\u542f\u52a8\u5ef6\u8fdf\u964d\u4f4e16%\uff1b\u4f46\u89e3\u91ca\u6a21\u5f0fWasm\u7684\u6696\u5ef6\u8fdf\u9ad8\u51fa55\u500d\uff0cI/O\u5e8f\u5217\u5316\u5f00\u9500\u9ad8\u51fa10\u500d\u3002", "conclusion": "Wasm\u5728\u8fb9\u7f18\u4e91\u8fde\u7eed\u4f53\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u7279\u522b\u662f\u9884\u7f16\u8bd1\u6a21\u5f0f\u5728\u8d44\u6e90\u6548\u7387\u548c\u51b7\u542f\u52a8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u89e3\u91ca\u6a21\u5f0f\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2510.05705", "categories": ["cs.SE", "cs.DL", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2510.05705", "abs": "https://arxiv.org/abs/2510.05705", "authors": ["Eva Mart\u00edn del Pico", "Josep Llu\u00eds Gelp\u00ed", "Salvador Capella-Guti\u00e9rrez"], "title": "The Software Observatory: aggregating and analysing software metadata for trend computation and FAIR assessment", "comment": null, "summary": "In the ever-changing realm of research software development, it is crucial\nfor the scientific community to grasp current trends to identify gaps that can\npotentially hinder scientific progress. The adherence to the FAIR (Findable,\nAccessible, Interoperable, Reusable) principles can serve as a proxy to\nunderstand those trends and provide a mechanism to propose specific actions.\n  The Software Observatory at OpenEBench\n(https://openebench.bsc.es/observatory) is a novel web portal that consolidates\nsoftware metadata from various sources, offering comprehensive insights into\ncritical research software aspects. Our platform enables users to analyse\ntrends, identify patterns and advancements within the Life Sciences research\nsoftware ecosystem, and understand its evolution over time. It also evaluates\nresearch software according to FAIR principles for research software, providing\nscores for different indicators.\n  Users have the ability to visualise this metadata at different levels of\ngranularity, ranging from the entire software landscape to specific communities\nto individual software entries through the FAIRsoft Evaluator. Indeed, the\nFAIRsoft Evaluator component streamlines the assessment process, helping\ndevelopers efficiently evaluate and obtain guidance to improve their software's\nFAIRness.\n  The Software Observatory represents a valuable resource for researchers and\nsoftware developers, as well as stakeholders, promoting better software\ndevelopment practices and adherence to FAIR principles for research software.", "AI": {"tldr": "Software Observatory\u662f\u4e00\u4e2a\u6574\u5408\u8f6f\u4ef6\u5143\u6570\u636e\u7684\u7f51\u7edc\u5e73\u53f0\uff0c\u7528\u4e8e\u5206\u6790\u751f\u547d\u79d1\u5b66\u7814\u7a76\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u8d8b\u52bf\uff0c\u5e76\u57fa\u4e8eFAIR\u539f\u5219\u8bc4\u4f30\u8f6f\u4ef6\u8d28\u91cf\u3002", "motivation": "\u5728\u5feb\u901f\u53d8\u5316\u7684\u7814\u7a76\u8f6f\u4ef6\u5f00\u53d1\u73af\u5883\u4e2d\uff0c\u9700\u8981\u4e86\u89e3\u5f53\u524d\u8d8b\u52bf\u548c\u8bc6\u522b\u53ef\u80fd\u963b\u788d\u79d1\u5b66\u8fdb\u5c55\u7684\u5dee\u8ddd\uff0cFAIR\u539f\u5219\u53ef\u4ee5\u4f5c\u4e3a\u7406\u89e3\u8fd9\u4e9b\u8d8b\u52bf\u7684\u4ee3\u7406\u673a\u5236\u3002", "method": "\u901a\u8fc7OpenEBench\u7684Software Observatory\u5e73\u53f0\uff0c\u6574\u5408\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u8f6f\u4ef6\u5143\u6570\u636e\uff0c\u63d0\u4f9b\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u7684\u53ef\u89c6\u5316\u5206\u6790\uff0c\u5e76\u4f7f\u7528FAIRsoft Evaluator\u7ec4\u4ef6\u8bc4\u4f30\u8f6f\u4ef6\u7684FAIR\u6027\u3002", "result": "\u8be5\u5e73\u53f0\u80fd\u591f\u5206\u6790\u7814\u7a76\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684\u8d8b\u52bf\u3001\u8bc6\u522b\u6a21\u5f0f\u548c\u8fdb\u5c55\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u8f6f\u4ef6FAIR\u6027\u8bc4\u4f30\u5206\u6570\u548c\u6539\u8fdb\u6307\u5bfc\u3002", "conclusion": "Software Observatory\u662f\u7814\u7a76\u4eba\u5458\u3001\u8f6f\u4ef6\u5f00\u53d1\u8005\u548c\u5229\u76ca\u76f8\u5173\u8005\u7684\u5b9d\u8d35\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u4fc3\u8fdb\u66f4\u597d\u7684\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u548c\u5bf9FAIR\u539f\u5219\u7684\u9075\u5b88\u3002"}}
{"id": "2510.05127", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05127", "abs": "https://arxiv.org/abs/2510.05127", "authors": ["Harshit Goyal"], "title": "Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines", "comment": "14 pages, 3 figures", "summary": "Efficient resource allocation is a key challenge in modern cloud computing.\nOver-provisioning leads to unnecessary costs, while under-provisioning risks\nperformance degradation and SLA violations. This work presents an artificial\nintelligence approach to predict resource utilization in big data pipelines\nusing Random Forest regression. We preprocess the Google Borg cluster traces to\nclean, transform, and extract relevant features (CPU, memory, usage\ndistributions). The model achieves high predictive accuracy (R Square = 0.99,\nMAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between\nworkload characteristics and resource utilization. Error analysis reveals\nimpressive performance on small-to-medium jobs, with higher variance in rare\nlarge-scale jobs. These results demonstrate the potential of AI-driven\nprediction for cost-aware autoscaling in cloud environments, reducing\nunnecessary provisioning while safeguarding service quality.", "AI": {"tldr": "\u4f7f\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u9884\u6d4b\u5927\u6570\u636e\u7ba1\u9053\u8d44\u6e90\u5229\u7528\u7387\u7684AI\u65b9\u6cd5\uff0c\u5728Google Borg\u96c6\u7fa4\u6570\u636e\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9884\u6d4b(R\u00b2=0.99)\uff0c\u652f\u6301\u4e91\u73af\u5883\u6210\u672c\u611f\u77e5\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u3002", "motivation": "\u73b0\u4ee3\u4e91\u8ba1\u7b97\u4e2d\u8d44\u6e90\u5206\u914d\u9762\u4e34\u6311\u6218\uff1a\u8fc7\u5ea6\u914d\u7f6e\u5bfc\u81f4\u4e0d\u5fc5\u8981\u6210\u672c\uff0c\u914d\u7f6e\u4e0d\u8db3\u5219\u5e26\u6765\u6027\u80fd\u4e0b\u964d\u548cSLA\u8fdd\u89c4\u98ce\u9669\u3002", "method": "\u9884\u5904\u7406Google Borg\u96c6\u7fa4\u8ddf\u8e2a\u6570\u636e\uff0c\u6e05\u6d17\u3001\u8f6c\u6362\u5e76\u63d0\u53d6\u76f8\u5173\u7279\u5f81(CPU\u3001\u5185\u5b58\u3001\u4f7f\u7528\u5206\u5e03)\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u8d44\u6e90\u5229\u7528\u7387\u3002", "result": "\u6a21\u578b\u8fbe\u5230\u9ad8\u9884\u6d4b\u7cbe\u5ea6(R\u00b2=0.99\uff0cMAE=0.0048\uff0cRMSE=0.137)\uff0c\u80fd\u6355\u6349\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u4e0e\u8d44\u6e90\u5229\u7528\u7387\u4e4b\u95f4\u7684\u975e\u7ebf\u6027\u5173\u7cfb\u3002\u5bf9\u5c0f\u5230\u4e2d\u578b\u4f5c\u4e1a\u8868\u73b0\u4f18\u5f02\uff0c\u5927\u89c4\u6a21\u4f5c\u4e1a\u65b9\u5dee\u8f83\u9ad8\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u9884\u6d4b\u5728\u4e91\u73af\u5883\u4e2d\u5177\u6709\u6210\u672c\u611f\u77e5\u81ea\u52a8\u6269\u7f29\u5bb9\u7684\u6f5c\u529b\uff0c\u65e2\u80fd\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u53c8\u80fd\u4fdd\u969c\u670d\u52a1\u8d28\u91cf\u3002"}}
{"id": "2510.05768", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05768", "abs": "https://arxiv.org/abs/2510.05768", "authors": ["Robin Kimmel", "Judith Michael", "Andreas Wortmann", "Jingxi Zhang"], "title": "Digital Twins for Software Engineering Processes", "comment": null, "summary": "Digital twins promise a better understanding and use of complex systems. To\nthis end, they represent these systems at their runtime and may interact with\nthem to control their processes. Software engineering is a wicked challenge in\nwhich stakeholders from many domains collaborate to produce software artifacts\ntogether. In the presence of skilled software engineer shortage, our vision is\nto leverage DTs as means for better rep- resenting, understanding, and\noptimizing software engineering processes to (i) enable software experts making\nthe best use of their time and (ii) support domain experts in producing\nhigh-quality software. This paper outlines why this would be beneficial, what\nsuch a digital twin could look like, and what is missing for realizing and\ndeploying software engineering digital twins.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u6570\u5b57\u5b6a\u751f\u6280\u672f\u6765\u6539\u8fdb\u8f6f\u4ef6\u5de5\u7a0b\u8fc7\u7a0b\uff0c\u5e2e\u52a9\u8f6f\u4ef6\u4e13\u5bb6\u66f4\u9ad8\u6548\u5229\u7528\u65f6\u95f4\uff0c\u652f\u6301\u9886\u57df\u4e13\u5bb6\u5f00\u53d1\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u3002", "motivation": "\u9762\u5bf9\u719f\u7ec3\u8f6f\u4ef6\u5de5\u7a0b\u5e08\u77ed\u7f3a\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u597d\u5730\u8868\u793a\u3001\u7406\u89e3\u548c\u4f18\u5316\u8f6f\u4ef6\u5de5\u7a0b\u8fc7\u7a0b\uff0c\u8ba9\u8f6f\u4ef6\u4e13\u5bb6\u80fd\u6700\u4f73\u5229\u7528\u65f6\u95f4\uff0c\u540c\u65f6\u652f\u6301\u9886\u57df\u4e13\u5bb6\u5f00\u53d1\u9ad8\u8d28\u91cf\u8f6f\u4ef6\u3002", "method": "\u63d0\u51fa\u6784\u5efa\u8f6f\u4ef6\u5de5\u7a0b\u6570\u5b57\u5b6a\u751f\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u8fd0\u884c\u65f6\u8868\u793a\u8f6f\u4ef6\u7cfb\u7edf\u5e76\u4e0e\u4e4b\u4ea4\u4e92\u4ee5\u63a7\u5236\u5176\u8fc7\u7a0b\u3002", "result": "\u6982\u8ff0\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u6570\u5b57\u5b6a\u751f\u7684\u6f5c\u5728\u76ca\u5904\u3001\u53ef\u80fd\u5f62\u6001\uff0c\u4ee5\u53ca\u5b9e\u73b0\u548c\u90e8\u7f72\u6240\u9700\u7684\u6761\u4ef6\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u6709\u6f5c\u529b\u663e\u8457\u6539\u5584\u8f6f\u4ef6\u5de5\u7a0b\u8fc7\u7a0b\uff0c\u4f46\u76ee\u524d\u8fd8\u9700\u8981\u89e3\u51b3\u5b9e\u73b0\u548c\u90e8\u7f72\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2510.05145", "categories": ["cs.DC", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.05145", "abs": "https://arxiv.org/abs/2510.05145", "authors": ["Lunyiu Nie", "Nedim Lipka", "Ryan A. Rossi", "Swarat Chaudhuri"], "title": "FlashResearch: Real-time Agent Orchestration for Efficient Deep Research", "comment": null, "summary": "Deep research agents, which synthesize information across diverse sources,\nare significantly constrained by their sequential reasoning processes. This\narchitectural bottleneck results in high latency, poor runtime adaptability,\nand inefficient resource allocation, making them impractical for interactive\napplications. To overcome this, we introduce FlashResearch, a novel framework\nfor efficient deep research that transforms sequential processing into\nparallel, runtime orchestration by dynamically decomposing complex queries into\ntree-structured sub-tasks. Our core contributions are threefold: (1) an\nadaptive planner that dynamically allocates computational resources by\ndetermining research breadth and depth based on query complexity; (2) a\nreal-time orchestration layer that monitors research progress and prunes\nredundant paths to reallocate resources and optimize efficiency; and (3) a\nmulti-dimensional parallelization framework that enables concurrency across\nboth research breadth and depth. Experiments show that FlashResearch\nconsistently improves final report quality within fixed time budgets, and can\ndeliver up to a 5x speedup while maintaining comparable quality.", "AI": {"tldr": "FlashResearch\u662f\u4e00\u4e2a\u9ad8\u6548\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u987a\u5e8f\u63a8\u7406\u8f6c\u6362\u4e3a\u5e76\u884c\u8fd0\u884c\u65f6\u7f16\u6392\uff0c\u52a8\u6001\u5206\u89e3\u590d\u6742\u67e5\u8be2\u4e3a\u6811\u72b6\u5b50\u4efb\u52a1\uff0c\u5b9e\u73b05\u500d\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u53d7\u9650\u4e8e\u987a\u5e8f\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u3001\u8fd0\u884c\u65f6\u9002\u5e94\u6027\u5dee\u548c\u8d44\u6e90\u5206\u914d\u6548\u7387\u4f4e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4ea4\u4e92\u5f0f\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u6838\u5fc3\u8d21\u732e\uff1a\u81ea\u9002\u5e94\u89c4\u5212\u5668\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff1b\u5b9e\u65f6\u7f16\u6392\u5c42\u76d1\u63a7\u7814\u7a76\u8fdb\u5ea6\u5e76\u526a\u679d\u5197\u4f59\u8def\u5f84\uff1b\u591a\u7ef4\u5e76\u884c\u5316\u6846\u67b6\u652f\u6301\u7814\u7a76\u5e7f\u5ea6\u548c\u6df1\u5ea6\u7684\u5e76\u53d1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFlashResearch\u5728\u56fa\u5b9a\u65f6\u95f4\u9884\u7b97\u5185\u6301\u7eed\u63d0\u5347\u6700\u7ec8\u62a5\u544a\u8d28\u91cf\uff0c\u5728\u4fdd\u6301\u53ef\u6bd4\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8fbe5\u500d\u7684\u52a0\u901f\u3002", "conclusion": "FlashResearch\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u65f6\u7f16\u6392\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u987a\u5e8f\u63a8\u7406\u74f6\u9888\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05788", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05788", "abs": "https://arxiv.org/abs/2510.05788", "authors": ["Nikita Pavlichenko", "Iurii Nazarov", "Ivan Dolgov", "Ekaterina Garanina", "Dmitry Ustalov", "Ivan Bondyrev", "Kseniia Lysaniuk", "Evgeniia Vu", "Kirill Chekmenev", "Joseph Shtok", "Yaroslav Golubev", "Anton Semenkin", "Uladzislau Sazanovich"], "title": "Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding", "comment": "11 pages, 4 figures, 3 tables", "summary": "We present the Mellum models family, open-weight code completion models\ndesigned for interactive use in JetBrains IDEs. Mellums have 4B parameters,\nadopt a Llama-style architecture, and are pre-trained on ~4T tokens of\npermissively licensed, multi-language code. Our studies show that (i) careful\ndata curation and staged training significantly improve the model's quality,\n(ii) editor-critical capabilities such as context packing are necessary for\nhigh-quality suggestions, and (iii) a compact, task-focused model can meet the\ncost and latency constraints of interactive completion.\n  In the paper, we describe an end-to-end industrial pipeline for producing\ncontextualized in-editor completion: disciplined data governance, multi-stage\ntraining that includes fill-in-the-middle and project context via supervised\nfine-tuning, and alignment via direct preference optimization using feedback\nfrom real-world scenarios. Our quality evaluations include both large-scale\noffline benchmarks and online telemetry from production deployments in\nJetBrains IDEs. Mellums are released under the Apache-2.0 license on\nHuggingFace, with a public model card providing a reproducible reference for\npractitioners. Our experience offers a pragmatic blueprint for taking a\nfocused, open model from a research prototype to at scale production for\nhundreds of thousands of users.", "AI": {"tldr": "Mellum\u6a21\u578b\u5bb6\u65cf\u662f\u4e13\u4e3aJetBrains IDE\u4ea4\u4e92\u4f7f\u7528\u8bbe\u8ba1\u76844B\u53c2\u6570\u5f00\u6e90\u4ee3\u7801\u8865\u5168\u6a21\u578b\uff0c\u91c7\u7528Llama\u67b6\u6784\uff0c\u57284T\u591a\u8bed\u8a00\u4ee3\u7801\u4e0a\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u7cbe\u5fc3\u6570\u636e\u7ba1\u7406\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4ee3\u7801\u8865\u5168\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u6ee1\u8db3\u4ea4\u4e92\u5f0f\u4ee3\u7801\u8865\u5168\u6210\u672c\u3001\u5ef6\u8fdf\u8981\u6c42\u7684\u9ad8\u8d28\u91cf\u3001\u7d27\u51d1\u578b\u6a21\u578b\uff0c\u4e3aIDE\u7528\u6237\u63d0\u4f9b\u5b9e\u7528\u7684\u4ee3\u7801\u5efa\u8bae\u529f\u80fd\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u5de5\u4e1a\u7ea7\u6d41\u7a0b\uff1a\u4e25\u683c\u6570\u636e\u6cbb\u7406\u3001\u591a\u9636\u6bb5\u8bad\u7ec3\uff08\u5305\u62ec\u4e2d\u95f4\u586b\u5145\u548c\u9879\u76ee\u4e0a\u4e0b\u6587\u76d1\u7763\u5fae\u8c03\uff09\u3001\u57fa\u4e8e\u771f\u5b9e\u573a\u666f\u53cd\u9988\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5bf9\u9f50\u3002", "result": "\u6a21\u578b\u5728\u79bb\u7ebf\u57fa\u51c6\u6d4b\u8bd5\u548c\u751f\u4ea7\u90e8\u7f72\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u7cbe\u5fc3\u6570\u636e\u7ba1\u7406\u548c\u4e0a\u4e0b\u6587\u6253\u5305\u80fd\u529b\u5bf9\u9ad8\u8d28\u91cf\u5efa\u8bae\u7684\u91cd\u8981\u6027\u3002", "conclusion": "Mellum\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ece\u7814\u7a76\u539f\u578b\u5230\u5927\u89c4\u6a21\u751f\u4ea7\u7684\u5b9e\u7528\u84dd\u56fe\uff0c\u8bc1\u660e\u4e86\u4e13\u6ce8\u3001\u5f00\u6e90\u6a21\u578b\u80fd\u591f\u6ee1\u8db3\u6570\u5341\u4e07\u7528\u6237\u7684\u751f\u4ea7\u9700\u6c42\u3002"}}
{"id": "2510.05149", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.05149", "abs": "https://arxiv.org/abs/2510.05149", "authors": ["Clarisse Sousa", "Tiago Fonseca", "Luis Lino Ferreira", "Ricardo Ven\u00e2ncio", "Ricardo Severino"], "title": "Percepta: High Performance Stream Processing at the Edge", "comment": null, "summary": "The rise of real-time data and the proliferation of Internet of Things (IoT)\ndevices have highlighted the limitations of cloud-centric solutions,\nparticularly regarding latency, bandwidth, and privacy. These challenges have\ndriven the growth of Edge Computing. Associated with IoT appears a set of other\nproblems, like: data rate harmonization between multiple sources, protocol\nconversion, handling the loss of data and the integration with Artificial\nIntelligence (AI) models. This paper presents Percepta, a lightweight Data\nStream Processing (DSP) system tailored to support AI workloads at the edge,\nwith a particular focus on such as Reinforcement Learning (RL). It introduces\nspecialized features such as reward function computation, data storage for\nmodel retraining, and real-time data preparation to support continuous\ndecision-making. Additional functionalities include data normalization,\nharmonization across heterogeneous protocols and sampling rates, and robust\nhandling of missing or incomplete data, making it well suited for the\nchallenges of edge-based AI deployment.", "AI": {"tldr": "Percepta\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6570\u636e\u6d41\u5904\u7406\u7cfb\u7edf\uff0c\u4e13\u4e3a\u8fb9\u7f18AI\u5de5\u4f5c\u8d1f\u8f7d\u8bbe\u8ba1\uff0c\u7279\u522b\u652f\u6301\u5f3a\u5316\u5b66\u4e60\uff0c\u5177\u5907\u5956\u52b1\u51fd\u6570\u8ba1\u7b97\u3001\u6a21\u578b\u91cd\u8bad\u7ec3\u6570\u636e\u5b58\u50a8\u548c\u5b9e\u65f6\u6570\u636e\u51c6\u5907\u7b49\u529f\u80fd\u3002", "motivation": "\u5b9e\u65f6\u6570\u636e\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7684\u589e\u957f\u66b4\u9732\u4e86\u4e91\u4e2d\u5fc3\u89e3\u51b3\u65b9\u6848\u5728\u5ef6\u8fdf\u3001\u5e26\u5bbd\u548c\u9690\u79c1\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e86\u8fb9\u7f18\u8ba1\u7b97\u7684\u53d1\u5c55\u3002\u7269\u8054\u7f51\u5e26\u6765\u7684\u6570\u636e\u901f\u7387\u534f\u8c03\u3001\u534f\u8bae\u8f6c\u6362\u3001\u6570\u636e\u4e22\u5931\u5904\u7406\u548cAI\u6a21\u578b\u96c6\u6210\u7b49\u95ee\u9898\u9700\u8981\u89e3\u51b3\u3002", "method": "\u5f00\u53d1\u4e86Percepta\u7cfb\u7edf\uff0c\u63d0\u4f9b\u6570\u636e\u6807\u51c6\u5316\u3001\u5f02\u6784\u534f\u8bae\u548c\u91c7\u6837\u7387\u534f\u8c03\u3001\u7f3a\u5931\u6570\u636e\u5904\u7406\u7b49\u4e13\u95e8\u529f\u80fd\uff0c\u652f\u6301\u5956\u52b1\u51fd\u6570\u8ba1\u7b97\u548c\u6a21\u578b\u91cd\u8bad\u7ec3\u6570\u636e\u5b58\u50a8\u3002", "result": "Percepta\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5904\u7406\u8fb9\u7f18AI\u90e8\u7f72\u4e2d\u7684\u6311\u6218\uff0c\u652f\u6301\u8fde\u7eed\u51b3\u7b56\u5236\u5b9a\uff0c\u7279\u522b\u9002\u5408\u5f3a\u5316\u5b66\u4e60\u7b49AI\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "Percepta\u4f5c\u4e3a\u4e00\u4e2a\u8f7b\u91cf\u7ea7DSP\u7cfb\u7edf\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8fb9\u7f18\u8ba1\u7b97\u4e2dAI\u90e8\u7f72\u9762\u4e34\u7684\u6570\u636e\u5904\u7406\u6311\u6218\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05878", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05878", "abs": "https://arxiv.org/abs/2510.05878", "authors": ["Darja Smite", "Franz Zieris", "Lars-Ola Damm"], "title": "A Wave of Resignations in the Aftermath of Remote Onboarding", "comment": "9 pages, submitted to the Journal of Systems and Software, In\n  Practice track", "summary": "The COVID-19 pandemic has permanently altered workplace structures,\nnormalizing remote work. However, critical evidence highlights challenges with\nfully remote arrangements, particularly for software teams. This study\ninvestigates employee resignation patterns at Ericsson, a global developer of\nsoftware-intensive systems, before, during, and after the pandemic. Using HR\ndata from 2016-2025 in Ericsson Sweden, we analyze how different work\nmodalities (onsite, remote, and hybrid) influence employee retention. Our\nfindings show a marked increase in resignations from summer 2021 to summer\n2023, especially among employees with less than five years of tenure. Employees\nonboarded remotely during the pandemic were significantly more likely to resign\nwithin their first three years, even after returning to the office. Exit\nsurveys suggest that remote onboarding may fail to establish the necessary\norganizational attachment, the feeling of belonging and long-term retention. By\ncontrast, the company's eventual successful return to pre-pandemic retention\nrates illustrates the value of differentiated work policies and supports\nreconsidering selective return-to-office (RTO) mandates. Our study demonstrates\nthe importance of employee integration practices in hybrid environments where\nthe requirement for in-office presence for recent hires shall be accompanied by\nin-office presence from their team members and more senior staff whose\nmentoring and social interactions contribute to integration into the corporate\nwork environment. We hope these actionable insights will inform HR leaders and\npolicymakers in shaping post-pandemic work practices, demonstrating that\ncarefully crafted hybrid models anchored in organizational attachment and\nmentorship can sustain retention in knowledge-intensive companies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u7231\u7acb\u4fe1\u516c\u53f8\u5728\u75ab\u60c5\u671f\u95f4\u4e0d\u540c\u5de5\u4f5c\u6a21\u5f0f\uff08\u73b0\u573a\u3001\u8fdc\u7a0b\u3001\u6df7\u5408\uff09\u5bf9\u5458\u5de5\u79bb\u804c\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u75ab\u60c5\u671f\u95f4\u8fdc\u7a0b\u5165\u804c\u7684\u5458\u5de5\u5728\u5934\u4e09\u5e74\u79bb\u804c\u7387\u663e\u8457\u66f4\u9ad8\uff0c\u5373\u4f7f\u8fd4\u56de\u529e\u516c\u5ba4\u540e\u4e5f\u662f\u5982\u6b64\u3002", "motivation": "COVID-19\u75ab\u60c5\u6c38\u4e45\u6539\u53d8\u4e86\u5de5\u4f5c\u7ed3\u6784\uff0c\u4f46\u5b8c\u5168\u8fdc\u7a0b\u5de5\u4f5c\u5bf9\u8f6f\u4ef6\u56e2\u961f\u5b58\u5728\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u4e0d\u540c\u5de5\u4f5c\u6a21\u5f0f\u5982\u4f55\u5f71\u54cd\u5458\u5de5\u4fdd\u7559\u7387\u3002", "method": "\u4f7f\u7528\u7231\u7acb\u4fe1\u745e\u5178\u516c\u53f82016-2025\u5e74\u7684\u4eba\u529b\u8d44\u6e90\u6570\u636e\uff0c\u5206\u6790\u75ab\u60c5\u524d\u3001\u671f\u95f4\u548c\u4e4b\u540e\u4e0d\u540c\u5de5\u4f5c\u6a21\u5f0f\u4e0b\u7684\u5458\u5de5\u79bb\u804c\u6a21\u5f0f\u3002", "result": "2021\u5e74\u590f\u5b63\u81f32023\u5e74\u590f\u5b63\u79bb\u804c\u7387\u663e\u8457\u589e\u52a0\uff0c\u7279\u522b\u662f\u5de5\u9f84\u4e0d\u8db35\u5e74\u7684\u5458\u5de5\u3002\u75ab\u60c5\u671f\u95f4\u8fdc\u7a0b\u5165\u804c\u7684\u5458\u5de5\u5728\u524d\u4e09\u5e74\u79bb\u804c\u53ef\u80fd\u6027\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6df7\u5408\u5de5\u4f5c\u6a21\u5f0f\uff0c\u7ed3\u5408\u7ec4\u7ec7\u5f52\u5c5e\u611f\u548c\u5bfc\u5e08\u5236\u5ea6\uff0c\u53ef\u4ee5\u7ef4\u6301\u77e5\u8bc6\u5bc6\u96c6\u578b\u516c\u53f8\u7684\u5458\u5de5\u4fdd\u7559\u7387\u3002\u65b0\u5458\u5de5\u73b0\u573a\u5de5\u4f5c\u8981\u6c42\u5e94\u914d\u5408\u56e2\u961f\u6210\u5458\u548c\u8d44\u6df1\u5458\u5de5\u7684\u73b0\u573a\u5b58\u5728\uff0c\u4ee5\u4fc3\u8fdb\u4f01\u4e1a\u73af\u5883\u6574\u5408\u3002"}}
{"id": "2510.05164", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05164", "abs": "https://arxiv.org/abs/2510.05164", "authors": ["Yuanzhe Shen", "Yide Liu", "Zisu Huang", "Ruicheng Yin", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading", "comment": "Accepted to EMNLP 2025 Main", "summary": "Large language models (LLMs) demonstrate remarkable performance across\ndiverse tasks, yet their effectiveness frequently depends on costly commercial\nAPIs or cloud services. Model selection thus entails a critical trade-off\nbetween performance and cost: high-performing LLMs typically incur substantial\nexpenses, whereas budget-friendly small language models (SLMs) are constrained\nby limited capabilities. Current research primarily proposes two routing\nstrategies: pre-generation routing and cascade routing. Both approaches have\ndistinct characteristics, with cascade routing typically offering superior\ncost-effectiveness and accuracy despite its higher latency. To further address\nthe limitations of both approaches, we introduce SATER, a dual-mode compatible\napproach that fine-tunes models through shortest-response preference\noptimization and a confidence-aware rejection mechanism. SATER significantly\nreduces redundant outputs and response times, while improving both the\nperformance of pre-generation routing and the efficiency of cascade routing.\nExperiments across three SLMs and six datasets, varying in type and complexity,\ndemonstrate that SATER achieves comparable performance while consistently\nreducing computational costs by over 50\\% and cascade latency by over 80\\%.", "AI": {"tldr": "SATER\u662f\u4e00\u4e2a\u53cc\u6a21\u5f0f\u517c\u5bb9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u77ed\u54cd\u5e94\u504f\u597d\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u62d2\u7edd\u673a\u5236\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u5197\u4f59\u8f93\u51fa\u548c\u54cd\u5e94\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u9ad8\u9884\u751f\u6210\u8def\u7531\u7684\u6027\u80fd\u548c\u7ea7\u8054\u8def\u7531\u7684\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u867d\u7136\u6027\u80fd\u51fa\u8272\u4f46\u6210\u672c\u9ad8\u6602\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\u6210\u672c\u4f4e\u4f46\u80fd\u529b\u6709\u9650\u3002\u73b0\u6709\u8def\u7531\u7b56\u7565\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSATER\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u77ed\u54cd\u5e94\u504f\u597d\u4f18\u5316\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u62d2\u7edd\u673a\u5236\u6765\u5fae\u8c03\u6a21\u578b\uff0c\u517c\u5bb9\u9884\u751f\u6210\u8def\u7531\u548c\u7ea7\u8054\u8def\u7531\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2aSLMs\u548c\u516d\u4e2a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSATER\u5728\u4fdd\u6301\u53ef\u6bd4\u6027\u80fd\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u8d85\u8fc750%\uff0c\u7ea7\u8054\u5ef6\u8fdf\u964d\u4f4e\u8d85\u8fc780%\u3002", "conclusion": "SATER\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LLM\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u7531\u7b56\u7565\u7684\u6548\u7387\u3002"}}
{"id": "2510.05968", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.05968", "abs": "https://arxiv.org/abs/2510.05968", "authors": ["Scott Frees"], "title": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications", "comment": null, "summary": "Large language models translate natural language into database queries, yet\ncontext window limitations prevent direct deployment in reporting systems where\ncomplete datasets exhaust available tokens. The Model Context Protocol\nspecification defines ResourceLink for referencing external resources, but\npractical patterns for implementing scalable reporting architectures remain\nundocumented. This paper presents patterns for building LLM-powered reporting\nsystems that decouple query generation from data retrieval. We introduce a\ndual-response pattern extending ResourceLink to support both iterative query\nrefinement and out-of-band data access, accompanied by patterns for\nmulti-tenant security and resource lifecycle management. These patterns address\nfundamental challenges in LLM-driven reporting applications and provide\npractical guidance for developers building them.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6784\u5efaLLM\u9a71\u52a8\u7684\u62a5\u544a\u7cfb\u7edf\u7684\u6a21\u5f0f\uff0c\u901a\u8fc7\u89e3\u8026\u67e5\u8be2\u751f\u6210\u4e0e\u6570\u636e\u68c0\u7d22\u6765\u89e3\u51b3\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u53cc\u54cd\u5e94\u6a21\u5f0f\u6765\u652f\u6301\u8fed\u4ee3\u67e5\u8be2\u4f18\u5316\u548c\u5e26\u5916\u6570\u636e\u8bbf\u95ee\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u62a5\u544a\u7cfb\u7edf\u4e2d\u9762\u4e34\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u5b8c\u6574\u6570\u636e\u96c6\u3002\u867d\u7136Model Context Protocol\u5b9a\u4e49\u4e86ResourceLink\u7528\u4e8e\u5f15\u7528\u5916\u90e8\u8d44\u6e90\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u62a5\u544a\u67b6\u6784\u7684\u5b9e\u73b0\u6a21\u5f0f\u3002", "method": "\u5f15\u5165\u53cc\u54cd\u5e94\u6a21\u5f0f\u6269\u5c55ResourceLink\uff0c\u652f\u6301\u8fed\u4ee3\u67e5\u8be2\u4f18\u5316\u548c\u5e26\u5916\u6570\u636e\u8bbf\u95ee\uff0c\u540c\u65f6\u63d0\u4f9b\u591a\u79df\u6237\u5b89\u5168\u548c\u8d44\u6e90\u751f\u547d\u5468\u671f\u7ba1\u7406\u6a21\u5f0f\u3002", "result": "\u5f00\u53d1\u4e86\u89e3\u51b3LLM\u9a71\u52a8\u62a5\u544a\u5e94\u7528\u57fa\u672c\u6311\u6218\u7684\u5b9e\u7528\u6a21\u5f0f\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u6784\u5efa\u6b64\u7c7b\u7cfb\u7edf\u7684\u5b9e\u8df5\u6307\u5bfc\u3002", "conclusion": "\u8fd9\u4e9b\u6a21\u5f0f\u89e3\u51b3\u4e86LLM\u9a71\u52a8\u62a5\u544a\u7cfb\u7edf\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u62a5\u544a\u67b6\u6784\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2510.05186", "categories": ["cs.DC", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.05186", "abs": "https://arxiv.org/abs/2510.05186", "authors": ["Hongpei Li", "Han Zhang", "Huikang Liu", "Dongdong Ge", "Yinyu Ye"], "title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training", "comment": "Use Mathematical Programming to model Pipeline Parallelism with\n  Offloading to balance efficiency and memory requirement", "summary": "Pipeline parallelism (PP) has become a standard technique for scaling large\nlanguage model (LLM) training across multiple devices. However, despite recent\nprogress in reducing memory consumption through activation offloading, existing\napproaches remain largely heuristic and coarse-grained, often overlooking the\nfine-grained trade-offs between memory, computation, and scheduling latency. In\nthis work, we revisit the pipeline scheduling problem from a principled\noptimization perspective. We observe that prevailing strategies either rely on\nstatic rules or aggressively offload activations without fully leveraging the\ninteraction between memory constraints and scheduling efficiency. To address\nthis, we formulate scheduling as a constrained optimization problem that\njointly accounts for memory capacity, activation reuse, and pipeline bubble\nminimization. Solving this model yields fine-grained schedules that reduce\npipeline bubbles while adhering to strict memory budgets. Our approach\ncomplements existing offloading techniques: whereas prior approaches trade\nmemory for time in a fixed pattern, we dynamically optimize the tradeoff with\nrespect to model structure and hardware configuration. Experimental results\ndemonstrate that our method consistently improves both throughput and memory\nutilization. In particular, we reduce idle pipeline time by up to 50% under the\nsame per-device memory limit, and in some cases, enable the training of larger\nmodels within limited memory budgets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u89c6\u89d2\u7684\u6d41\u6c34\u7ebf\u8c03\u5ea6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8003\u8651\u5185\u5b58\u5bb9\u91cf\u3001\u6fc0\u6d3b\u91cd\u7528\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u6700\u5c0f\u5316\uff0c\u52a8\u6001\u4f18\u5316\u5185\u5b58\u4e0e\u65f6\u95f4\u7684\u6743\u8861\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u541e\u5410\u91cf\u548c\u5185\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u548c\u7c97\u7c92\u5ea6\u7684\u6fc0\u6d3b\u5378\u8f7d\u7b56\u7565\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u5185\u5b58\u7ea6\u675f\u4e0e\u8c03\u5ea6\u6548\u7387\u4e4b\u95f4\u7684\u7cbe\u7ec6\u6743\u8861\uff0c\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\u3002", "method": "\u5c06\u8c03\u5ea6\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u8054\u5408\u8003\u8651\u5185\u5b58\u5bb9\u91cf\u3001\u6fc0\u6d3b\u91cd\u7528\u548c\u6d41\u6c34\u7ebf\u6c14\u6ce1\u6700\u5c0f\u5316\uff0c\u6839\u636e\u6a21\u578b\u7ed3\u6784\u548c\u786c\u4ef6\u914d\u7f6e\u52a8\u6001\u4f18\u5316\u5185\u5b58\u4e0e\u65f6\u95f4\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u8bbe\u5907\u5185\u5b58\u9650\u5236\u4e0b\u53ef\u5c06\u7a7a\u95f2\u6d41\u6c34\u7ebf\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe50%\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fd8\u80fd\u5728\u6709\u9650\u5185\u5b58\u9884\u7b97\u5185\u8bad\u7ec3\u66f4\u5927\u7684\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u6d41\u6c34\u7ebf\u5e76\u884c\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u901a\u8fc7\u7cbe\u7ec6\u8c03\u5ea6\u5b9e\u73b0\u66f4\u597d\u7684\u5185\u5b58\u5229\u7528\u548c\u541e\u5410\u91cf\u8868\u73b0\u3002"}}
{"id": "2510.06000", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06000", "abs": "https://arxiv.org/abs/2510.06000", "authors": ["Daniel Otten", "Trevor Stalnaker", "Nathan Wintersgill", "Oscar Chaparro", "Denys Poshyvanyk"], "title": "Prompting in Practice: Investigating Software Developers' Use of Generative AI Tools", "comment": null, "summary": "The integration of generative artificial intelligence (GenAI) tools has\nfundamentally transformed software development. Although prompt engineering has\nemerged as a critical skill, existing research focuses primarily on individual\ntechniques rather than software developers' broader workflows. This study\npresents a systematic investigation of how software engineers integrate GenAI\ntools into their professional practice through a large-scale survey examining\nprompting strategies, conversation patterns, and reliability assessments across\nvarious software engineering tasks.\n  We surveyed 91 software engineers, including 72 active GenAI users, to\nunderstand AI usage patterns throughout the development process. Our 14 key\nfindings show that while code generation is nearly universal, proficiency\nstrongly correlates with using AI for more nuanced tasks such as debugging and\ncode review, and that developers prefer iterative multi-turn conversations to\nsingle-shot prompting. Documentation tasks are perceived as most reliable,\nwhile complex code generation and debugging present sizable challenges. Our\ninsights provide an empirical baseline of current developer practices, from\nsimple code generation to deeper workflow integration, with actionable insights\nfor future improvements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8c03\u67e591\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u7cfb\u7edf\u5206\u6790\u4e86GenAI\u5de5\u5177\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6a21\u5f0f\uff0c\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u666e\u904d\u4f46\u719f\u7ec3\u7528\u6237\u66f4\u503e\u5411\u5c06\u5176\u7528\u4e8e\u8c03\u8bd5\u548c\u4ee3\u7801\u5ba1\u67e5\u7b49\u590d\u6742\u4efb\u52a1\uff0c\u5f00\u53d1\u8005\u504f\u597d\u591a\u8f6e\u5bf9\u8bdd\u800c\u975e\u5355\u6b21\u63d0\u793a\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e2a\u522b\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u800c\u975e\u8f6f\u4ef6\u5f00\u53d1\u8005\u5c06GenAI\u5de5\u5177\u6574\u5408\u5230\u4e13\u4e1a\u5b9e\u8df5\u4e2d\u7684\u6574\u4f53\u5de5\u4f5c\u6d41\u7a0b\uff0c\u9700\u8981\u7cfb\u7edf\u8c03\u67e5GenAI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u60c5\u51b5\u3002", "method": "\u901a\u8fc7\u5927\u89c4\u6a21\u8c03\u67e591\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff08\u5176\u4e2d72\u540d\u6d3b\u8dc3GenAI\u7528\u6237\uff09\uff0c\u5206\u6790\u63d0\u793a\u7b56\u7565\u3001\u5bf9\u8bdd\u6a21\u5f0f\u548c\u53ef\u9760\u6027\u8bc4\u4f30\u5728\u4e0d\u540c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "14\u4e2a\u5173\u952e\u53d1\u73b0\u663e\u793a\uff1a\u4ee3\u7801\u751f\u6210\u51e0\u4e4e\u666e\u53ca\uff0c\u4f46\u719f\u7ec3\u5ea6\u4e0e\u4f7f\u7528AI\u8fdb\u884c\u8c03\u8bd5\u548c\u4ee3\u7801\u5ba1\u67e5\u7b49\u590d\u6742\u4efb\u52a1\u5f3a\u76f8\u5173\uff1b\u5f00\u53d1\u8005\u504f\u597d\u8fed\u4ee3\u5f0f\u591a\u8f6e\u5bf9\u8bdd\uff1b\u6587\u6863\u4efb\u52a1\u88ab\u8ba4\u4e3a\u6700\u53ef\u9760\uff0c\u800c\u590d\u6742\u4ee3\u7801\u751f\u6210\u548c\u8c03\u8bd5\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f53\u524d\u5f00\u53d1\u8005\u5b9e\u8df5\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u57fa\u51c6\uff0c\u4ece\u7b80\u5355\u4ee3\u7801\u751f\u6210\u5230\u66f4\u6df1\u5c42\u6b21\u7684\u5de5\u4f5c\u6d41\u7a0b\u6574\u5408\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u53ef\u884c\u89c1\u89e3\u3002"}}
{"id": "2510.05254", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05254", "abs": "https://arxiv.org/abs/2510.05254", "authors": ["Filipp Sporykhin", "Holger Homann"], "title": "Performance of a high-order MPI-Kokkos accelerated fluid solver", "comment": "12 pages, 16 figures. submitted to Computer Physics Communications", "summary": "This work discusses the performance of a modern numerical scheme for fluid\ndynamical problems on modern high-performance computing architectures. Our code\nimplements a spatial nodal discontinuous Galerkin scheme that we test up to an\norder of convergence of eight. It is temporally coupled to a set of Runge-Kutta\nmethods of orders up to six. The code integrates the linear advection equations\nas well as the isothermal Euler equations in one, two, and three dimensions. In\norder to target modern hardware involving many-core Central Processing Units\nand accelerators such as Graphic Processing Units we use the Kokkos library in\nconjunction with the Message Passing Interface to run our single source code on\nvarious GPU systems. We find that the higher the order the faster is the code.\nEighth-order simulations attain a given global error with much less computing\ntime than third- or fourth-order simulations. The RK scheme has a smaller\nimpact on the code performance and a classical fourth-order scheme seems to\ngenerally be a good choice. The code performs very well on all considered GPUs.\nThe many-CPU performance is also very good and perfect weak scaling is observed\nup to many hundreds of CPU cores using MPI. We note that small grid-size\nsimulations are faster on CPUs than on GPUs while GPUs win significantly over\nCPUs for simulations involving more than $10^7$ degrees of freedom ($\\approx\n3100^2$ grid points). When it comes to the environmental impact of numerical\nsimulations we estimate that GPUs consume less energy than CPUs for large\ngrid-size simulations but more energy on small grids. We observe a tendency\nthat the more modern is the GPU the larger needs to be the grid in order to use\nit efficiently. This yields a rebound effect because larger simulations need\nlonger computing times and in turn more energy that is not compensated by the\nenergy efficiency gain of the newer GPUs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\u4e0a\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u7684\u6570\u503c\u65b9\u6848\u6027\u80fd\uff0c\u53d1\u73b0\u9ad8\u9636\uff08\u516b\u9636\uff09\u6a21\u62df\u6bd4\u4f4e\u9636\uff08\u4e09\u3001\u56db\u9636\uff09\u6a21\u62df\u5728\u76f8\u540c\u5168\u5c40\u8bef\u5dee\u4e0b\u8ba1\u7b97\u65f6\u95f4\u66f4\u5c11\uff0cGPU\u5728\u5927\u89c4\u6a21\u7f51\u683c\u6a21\u62df\u4e2d\u6027\u80fd\u4f18\u4e8eCPU\u4f46\u80fd\u8017\u66f4\u9ad8", "motivation": "\u7814\u7a76\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u67b6\u6784\uff08\u591a\u6838CPU\u548cGPU\uff09\u4e0a\u6d41\u4f53\u52a8\u529b\u5b66\u6570\u503c\u65b9\u6848\u7684\u6027\u80fd\u8868\u73b0\uff0c\u7279\u522b\u5173\u6ce8\u9ad8\u9636\u6570\u503c\u65b9\u6cd5\u5728\u4e0d\u540c\u786c\u4ef6\u5e73\u53f0\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\u548c\u80fd\u8017\u7279\u6027", "method": "\u4f7f\u7528Kokkos\u5e93\u548cMPI\u5b9e\u73b0\u7a7a\u95f4\u8282\u70b9\u95f4\u65adGalerkin\u65b9\u6848\uff08\u6700\u9ad8\u516b\u9636\uff09\u4e0eRunge-Kutta\u65f6\u95f4\u79ef\u5206\u65b9\u6cd5\uff08\u6700\u9ad8\u516d\u9636\uff09\u7684\u8026\u5408\uff0c\u5728CPU\u548cGPU\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\u7ebf\u6027\u5bf9\u6d41\u65b9\u7a0b\u548c\u7b49\u6e29Euler\u65b9\u7a0b", "result": "\u9ad8\u9636\u6a21\u62df\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u516b\u9636\u65b9\u6848\u6bd4\u4e09\u3001\u56db\u9636\u65b9\u6848\u83b7\u5f97\u76f8\u540c\u5168\u5c40\u8bef\u5dee\u6240\u9700\u8ba1\u7b97\u65f6\u95f4\u66f4\u5c11\uff1bGPU\u5728\u5927\u89c4\u6a21\u7f51\u683c\uff08\u8d85\u8fc710^7\u81ea\u7531\u5ea6\uff09\u6a21\u62df\u4e2d\u6027\u80fd\u663e\u8457\u4f18\u4e8eCPU\uff0c\u4f46\u5c0f\u7f51\u683c\u6a21\u62dfCPU\u66f4\u5feb\uff1b\u73b0\u4ee3GPU\u9700\u8981\u66f4\u5927\u7f51\u683c\u624d\u80fd\u9ad8\u6548\u5229\u7528\uff0c\u5b58\u5728\u53cd\u5f39\u6548\u5e94", "conclusion": "\u9ad8\u9636\u6570\u503c\u65b9\u6cd5\u5728\u73b0\u4ee3\u786c\u4ef6\u4e0a\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\uff0cGPU\u9002\u5408\u5927\u89c4\u6a21\u6a21\u62df\u4f46\u80fd\u8017\u8f83\u9ad8\uff0c\u786c\u4ef6\u9009\u62e9\u9700\u6839\u636e\u95ee\u9898\u89c4\u6a21\u6743\u8861\u6027\u80fd\u4e0e\u80fd\u8017"}}
{"id": "2510.06104", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2510.06104", "abs": "https://arxiv.org/abs/2510.06104", "authors": ["Elijah Kayode Adejumo", "Brittany Johnson"], "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations", "comment": null, "summary": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u9759\u6001\u5206\u6790\u6307\u6807\u8f6c\u5316\u4e3a\u53ef\u7406\u89e3\u7684\u98ce\u9669\u89e3\u91ca\uff0c\u5e2e\u52a9\u5f00\u6e90\u8f6f\u4ef6\u8d21\u732e\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u4fee\u6539\u4ee3\u7801", "motivation": "\u5f00\u6e90\u8f6f\u4ef6\u8d21\u732e\u8005\u5728\u4fee\u6539\u4ee3\u7801\u65f6\u96be\u4ee5\u7406\u89e3\u9759\u6001\u5206\u6790\u5de5\u5177\u4ea7\u751f\u7684\u590d\u6742\u6307\u6807\uff0c\u9700\u8981\u66f4\u76f4\u89c2\u7684\u98ce\u9669\u89e3\u91ca\u548c\u6307\u5bfc", "method": "\u63d0\u51fa\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u6545\u969c\u9884\u6d4b\u6307\u6807\u8f6c\u5316\u4e3a\u63cf\u8ff0\u6027\u3001\u4e0a\u4e0b\u6587\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u89e3\u91ca\uff0c\u8ba1\u5212\u901a\u8fc7\u4efb\u52a1\u7814\u7a76\u8bc4\u4f30\u6548\u679c", "result": "\u76ee\u524d\u5904\u4e8e\u7814\u7a76\u9636\u6bb5\uff0c\u63d0\u51fa\u4e86LLM\u751f\u6210\u89e3\u91ca\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u5c1a\u672a\u6709\u5b9e\u8bc1\u7ed3\u679c", "conclusion": "LLM\u6709\u6f5c\u529b\u6539\u5584\u5f00\u6e90\u8f6f\u4ef6\u8d21\u732e\u8005\u5bf9\u4ee3\u7801\u98ce\u9669\u7684\u7406\u89e3\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u9a8c\u8bc1\u5176\u5b9e\u9645\u6548\u679c"}}
{"id": "2510.05476", "categories": ["cs.DC", "cs.AR", "cs.NI"], "pdf": "https://arxiv.org/pdf/2510.05476", "abs": "https://arxiv.org/abs/2510.05476", "authors": ["Xi Wang", "Bin Ma", "Jongryool Kim", "Byungil Koh", "Hoshik Kim", "Dong Li"], "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications", "comment": null, "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.", "AI": {"tldr": "cMPI\u662f\u9996\u4e2a\u5728\u771f\u5b9eCXL\u5e73\u53f0\u4e0a\u4f7f\u7528CXL\u5185\u5b58\u5171\u4eab\u4f18\u5316MPI\u70b9\u5bf9\u70b9\u901a\u4fe1\u7684\u5de5\u4f5c\uff0c\u5c06\u8de8\u8282\u70b9\u901a\u4fe1\u8f6c\u6362\u4e3aCXL\u5185\u5b58\u5185\u7684\u5185\u5b58\u4e8b\u52a1\u548c\u6570\u636e\u62f7\u8d1d\uff0c\u7ed5\u8fc7\u4f20\u7edf\u7f51\u7edc\u534f\u8bae\u3002", "motivation": "\u4f20\u7edfMPI\u5e93\u4f7f\u7528\u590d\u6742\u7684\u7f51\u7edc\u4e92\u8fde\u548c\u534f\u8bae\u6808\u8fdb\u884c\u8de8\u8282\u70b9\u901a\u4fe1\uff0c\u5b58\u5728\u6027\u80fd\u74f6\u9888\u3002CXL\u5185\u5b58\u5171\u4eab\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u901a\u4fe1\u4f18\u5316\u673a\u4f1a\u3002", "method": "\u5229\u7528CXL\u5185\u5b58\u5171\u4eab\u6280\u672f\uff0c\u5c06MPI\u901a\u4fe1\u8f6c\u6362\u4e3aCXL\u5185\u5b58\u5185\u7684\u5185\u5b58\u4e8b\u52a1\u548c\u6570\u636e\u62f7\u8d1d\uff0c\u89e3\u51b3\u6570\u636e\u5bf9\u8c61\u7ba1\u7406\u3001\u7f13\u5b58\u4e00\u81f4\u6027\u548c\u539f\u5b50\u64cd\u4f5c\u7b49\u6311\u6218\u3002", "result": "CXL\u5185\u5b58\u5171\u4eab\u6bd4TCP\u4e92\u8fde\u5ef6\u8fdf\u964d\u4f4e7.2-8.1\u500d\uff0c\u5728\u5c0f\u578b\u6d88\u606f\u4f20\u8f93\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u4ee5\u592a\u7f51NIC\u548c\u9ad8\u7aefSmartNIC\uff0c\u5ef6\u8fdf\u548c\u5e26\u5bbd\u5206\u522b\u63d0\u534749\u500d\u548c72\u500d\u3002", "conclusion": "CXL\u5185\u5b58\u5171\u4eab\u4e3aMPI\u901a\u4fe1\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u662f\u9ad8\u6027\u80fd\u8ba1\u7b97\u901a\u4fe1\u4f18\u5316\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.06187", "categories": ["cs.SE", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.06187", "abs": "https://arxiv.org/abs/2510.06187", "authors": ["Griffin Pitts", "Aum Pandya", "Darsh Rank", "Tirth Bhatt", "Muntasir Hoq", "Bita Akram"], "title": "Automated Program Repair of Uncompilable Student Code", "comment": null, "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.", "AI": {"tldr": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u4fee\u590dCS1\u8bfe\u7a0b\u4e2d\u65e0\u6cd5\u7f16\u8bd1\u7684\u5b66\u751f\u4ee3\u7801\uff0c\u4ee5\u4fdd\u7559\u5b66\u751f\u7ed3\u6784\u610f\u56fe\u5e76\u7528\u4e8e\u5b66\u751f\u5efa\u6a21", "motivation": "CS1\u5b66\u4e60\u73af\u5883\u4e2d\u5927\u91cf\u5b66\u751f\u7f16\u7a0b\u63d0\u4ea4\u65e0\u6cd5\u7f16\u8bd1\uff0c\u9650\u5236\u4e86\u5728\u5b66\u751f\u5efa\u6a21\u548c\u77e5\u8bc6\u8ffd\u8e2a\u4e2d\u7684\u4f7f\u7528\uff0c\u4f20\u7edf\u5efa\u6a21\u6d41\u7a0b\u5f80\u5f80\u6392\u9664\u8fd9\u4e9b\u60c5\u51b5", "method": "\u8bc4\u4f30GPT-5\u3001Claude 3.5 Haiku\u548cGemini 2.5 Flash\u7b49LLM\u5728\u9ad8/\u4f4e\u4e0a\u4e0b\u6587\u63d0\u793a\u6761\u4ef6\u4e0b\u4f5c\u4e3a\u4fee\u590d\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u8bc4\u4f30\u7f16\u8bd1\u6027\u3001\u7f16\u8f91\u8ddd\u79bb\u3001\u7ed3\u6784\u4fdd\u7559\u548c\u903b\u8f91\u4fdd\u7559", "result": "\u6240\u6709\u4e09\u4e2aLLM\u90fd\u80fd\u4ea7\u751f\u53ef\u7f16\u8bd1\u4fee\u590d\uff0c\u4f46\u5728\u4fdd\u7559\u5b66\u751f\u63a7\u5236\u6d41\u548c\u4ee3\u7801\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u4e0d\u540c\uff0c\u5f71\u54cd\u5176\u6559\u5b66\u6548\u7528", "conclusion": "\u901a\u8fc7\u6062\u590d\u65e0\u6cd5\u7f16\u8bd1\u7684\u63d0\u4ea4\uff0c\u8fd9\u9879\u5de5\u4f5c\u80fd\u591f\u5bf9\u5b66\u4e60\u8005\u7684\u7f16\u7801\u8fc7\u7a0b\u548c\u53d1\u5c55\u8fdb\u884c\u66f4\u4e30\u5bcc\u5168\u9762\u7684\u5206\u6790"}}
{"id": "2510.05497", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05497", "abs": "https://arxiv.org/abs/2510.05497", "authors": ["Zhongkai Yu", "Yue Guan", "Zihao Yu", "Chenyang Zhou", "Shuyi Pei", "Yangwook Kang", "Yufei Ding", "Po-An Tsai"], "title": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting", "comment": null, "summary": "Large Language Models (LLMs) with Mixture of Experts (MoE) architectures\nachieve remarkable performance improvements, but their random expert selection\nmechanism introduces significant data movement overhead that becomes the\ndominant bottleneck in multi-unit serving systems. To forecast the patterns\nunderlying this data movement, we conduct comprehensive data-movement-centric\nprofiling across three state-of-the-art large-scale MoE models (200B- 671B)\nusing over 24,000 requests spanning diverse workloads. With the resulting\n150GB+ trace files, we perform systematic analysis from both temporal and\nspatial perspectives and distill six key insights to guide the design of\ndiverse future serving systems. Taking wafer-scale GPUs as a case study, we\ndemonstrate that minor architectural modifications leveraging our insights\nachieve substantial performance gains, delivering 6.3X and 4.0X average\nspeedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first\ncomprehensive data-centric analysis of MoE models at scale. Our profiling\ntraces and analysis results are publicly available at\n{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will\nalso release our simulation framework shortly to facilitate future research in\nthis area.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9MoE\u67b6\u6784LLM\u7684\u6570\u636e\u79fb\u52a8\u6a21\u5f0f\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u63d0\u51fa\u4e866\u4e2a\u5173\u952e\u6d1e\u5bdf\uff0c\u5e76\u5728\u6676\u5706\u7ea7GPU\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "MoE\u67b6\u6784LLM\u7684\u968f\u673a\u4e13\u5bb6\u9009\u62e9\u673a\u5236\u5728\u591a\u5355\u5143\u670d\u52a1\u7cfb\u7edf\u4e2d\u5f15\u5165\u4e86\u663e\u8457\u7684\u6570\u636e\u79fb\u52a8\u5f00\u9500\uff0c\u6210\u4e3a\u4e3b\u8981\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u6df1\u5165\u5206\u6790\u5176\u6a21\u5f0f\u4ee5\u4f18\u5316\u7cfb\u7edf\u8bbe\u8ba1\u3002", "method": "\u5bf9\u4e09\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u89c4\u6a21MoE\u6a21\u578b\uff08200B-671B\uff09\u8fdb\u884c\u6570\u636e\u79fb\u52a8\u4e3a\u4e2d\u5fc3\u7684\u5206\u6790\uff0c\u4f7f\u7528\u8d85\u8fc724,000\u4e2a\u8bf7\u6c42\uff0c\u6db5\u76d6\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u751f\u6210150GB+\u7684\u8ddf\u8e2a\u6587\u4ef6\uff0c\u4ece\u65f6\u95f4\u548c\u7a7a\u95f4\u89d2\u5ea6\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u57fa\u4e8e\u5206\u6790\u6d1e\u5bdf\u5bf9\u6676\u5706\u7ea7GPU\u8fdb\u884c\u5fae\u5c0f\u67b6\u6784\u4fee\u6539\uff0c\u5728DeepSeek V3\u548cQwen3\u4e0a\u5206\u522b\u5b9e\u73b0\u4e866.3\u500d\u548c4.0\u500d\u7684\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684\u5168\u9762\u6570\u636e\u4e3a\u4e2d\u5fc3\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u6307\u5bfc\u672a\u6765\u670d\u52a1\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5173\u952e\u6d1e\u5bdf\uff0c\u76f8\u5173\u5206\u6790\u6570\u636e\u548c\u6a21\u62df\u6846\u67b6\u5c06\u516c\u5f00\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2510.05556", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2510.05556", "abs": "https://arxiv.org/abs/2510.05556", "authors": ["Jiakai Xu", "Tianle Zhou", "Eugene Wu", "Kostis Kaffes"], "title": "Toward Systems Foundations for Agentic Exploration", "comment": null, "summary": "Agentic exploration, letting LLM-powered agents branch, backtrack, and search\nacross many execution paths, demands systems support well beyond today's\npass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that\ngeneric tools such as CRIU or container commits are not fast enough even in\nisolated testbeds, and they crumble entirely in real deployments where agents\nshare files, sockets, and cloud APIs with other agents and human users. In this\ntalk, we pinpoint three open fundamental challenges: fork semantics, which\nconcerns how branches reveal or hide tentative updates; external side-effects,\nwhere fork awareness must be added to services or their calls intercepted; and\nnative forking, which requires cloning databases and runtimes in microseconds\nwithout bulk copying.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86LLM\u667a\u80fd\u4f53\u63a2\u7d22\u4e2d\u5feb\u7167/\u6062\u590d\u673a\u5236\u7684\u4e0d\u8db3\uff0c\u6307\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u5206\u652f\u8bed\u4e49\u3001\u5916\u90e8\u526f\u4f5c\u7528\u548c\u539f\u751f\u5206\u53c9\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u7684\u5feb\u7167/\u6062\u590d\u5de5\u5177\uff08\u5982CRIU\u6216\u5bb9\u5668\u63d0\u4ea4\uff09\u5728\u667a\u80fd\u4f53\u63a2\u7d22\u573a\u666f\u4e2d\u6027\u80fd\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u90e8\u7f72\u73af\u5883\u4e2d\uff0c\u5f53\u667a\u80fd\u4f53\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u548c\u7528\u6237\u5171\u4eab\u6587\u4ef6\u3001\u5957\u63a5\u5b57\u548c\u4e91API\u65f6\uff0c\u8fd9\u4e9b\u673a\u5236\u5b8c\u5168\u5931\u6548\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u516d\u79cd\u5feb\u7167/\u6062\u590d\u673a\u5236\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u9694\u79bb\u6d4b\u8bd5\u73af\u5883\u548c\u771f\u5b9e\u90e8\u7f72\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u5de5\u5177\u5373\u4f7f\u5728\u9694\u79bb\u6d4b\u8bd5\u73af\u5883\u4e2d\u4e5f\u4e0d\u591f\u5feb\uff0c\u5728\u771f\u5b9e\u90e8\u7f72\u73af\u5883\u4e2d\u5b8c\u5168\u5931\u6548\u3002", "conclusion": "\u8bba\u6587\u8bc6\u522b\u4e86\u4e09\u4e2a\u5f00\u653e\u7684\u57fa\u7840\u6311\u6218\uff1a\u5206\u652f\u8bed\u4e49\u95ee\u9898\u3001\u5916\u90e8\u526f\u4f5c\u7528\u5904\u7406\u548c\u539f\u751f\u5206\u53c9\u9700\u6c42\uff0c\u8fd9\u4e9b\u6311\u6218\u9700\u8981\u5728\u5fae\u79d2\u7ea7\u522b\u514b\u9686\u6570\u636e\u5e93\u548c\u8fd0\u884c\u65f6\u800c\u4e0d\u8fdb\u884c\u6279\u91cf\u590d\u5236\u3002"}}
{"id": "2510.05621", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.05621", "abs": "https://arxiv.org/abs/2510.05621", "authors": ["Zhiyuan Ren", "Tao Zhang", "Wenchi Chen"], "title": "Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems", "comment": null, "summary": "In distributed multi-agent systems, correctness is often entangled with\noperational policies such as scheduling, batching, or routing, which makes\nsystems brittle since performance-driven policy evolution may break integrity\nguarantees. This paper introduces the Deterministic Causal Structure (DCS), a\nformal foundation that decouples correctness from policy. We develop a minimal\naxiomatic theory and prove four results: existence and uniqueness,\npolicy-agnostic invariance, observational equivalence, and axiom minimality.\nThese results show that DCS resolves causal ambiguities that value-centric\nconvergence models such as CRDTs cannot address, and that removing any axiom\ncollapses determinism into ambiguity. DCS thus emerges as a boundary principle\nof asynchronous computation, analogous to CAP and FLP: correctness is preserved\nonly within the expressive power of a join-semilattice. All guarantees are\nestablished by axioms and proofs, with only minimal illustrative constructions\nincluded to aid intuition. This work establishes correctness as a fixed,\npolicy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which\ndistributed intelligent systems can be built modularly, safely, and evolvably.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u56e0\u679c\u7ed3\u6784\uff08DCS\uff09\u4f5c\u4e3a\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u5c06\u6b63\u786e\u6027\u4e0e\u64cd\u4f5c\u7b56\u7565\u89e3\u8026\uff0c\u786e\u4fdd\u7b56\u7565\u6f14\u8fdb\u4e0d\u4f1a\u7834\u574f\u5b8c\u6574\u6027\u4fdd\u8bc1\u3002", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u6b63\u786e\u6027\u5e38\u4e0e\u8c03\u5ea6\u3001\u6279\u5904\u7406\u7b49\u64cd\u4f5c\u7b56\u7565\u7ea0\u7f20\uff0c\u5bfc\u81f4\u7cfb\u7edf\u8106\u5f31\uff0c\u6027\u80fd\u9a71\u52a8\u7684\u7b56\u7565\u6f14\u8fdb\u53ef\u80fd\u7834\u574f\u5b8c\u6574\u6027\u4fdd\u8bc1\u3002", "method": "\u5f00\u53d1\u4e86\u6700\u5c0f\u516c\u7406\u5316\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u56db\u4e2a\u7ed3\u679c\uff1a\u5b58\u5728\u6027\u4e0e\u552f\u4e00\u6027\u3001\u7b56\u7565\u65e0\u5173\u4e0d\u53d8\u6027\u3001\u89c2\u6d4b\u7b49\u4ef7\u6027\u548c\u516c\u7406\u6700\u5c0f\u6027\u3002", "result": "DCS\u89e3\u51b3\u4e86CRDT\u7b49\u503c\u4e2d\u5fc3\u6536\u655b\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u7684\u56e0\u679c\u6a21\u7cca\u6027\uff0c\u79fb\u9664\u4efb\u4f55\u516c\u7406\u90fd\u4f1a\u4f7f\u786e\u5b9a\u6027\u5d29\u6e83\u4e3a\u6a21\u7cca\u6027\u3002", "conclusion": "DCS\u786e\u7acb\u4e86\u6b63\u786e\u6027\u4f5c\u4e3a\u56fa\u5b9a\u3001\u7b56\u7565\u65e0\u5173\u7684\u57fa\u7840\uff0c\u5f62\u6210\"\u6b63\u786e\u6027\u5373\u5e95\u76d8\"\u8303\u5f0f\uff0c\u53ef\u5728\u5176\u4e0a\u6a21\u5757\u5316\u3001\u5b89\u5168\u3001\u53ef\u6f14\u8fdb\u5730\u6784\u5efa\u5206\u5e03\u5f0f\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2510.05711", "categories": ["cs.DC", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.05711", "abs": "https://arxiv.org/abs/2510.05711", "authors": ["Ailiya Borjigin", "Cong He"], "title": "Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium", "comment": "23 pages, 5 figures", "summary": "Time-bound stablecoins are DeFi assets that temporarily tokenize traditional\nsecurities during market off-hours, enabling continuous cross-market liquidity.\nWe introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of\nproviding liquidity when the primary market is closed. We build a no-arbitrage\npricing model that yields a band for fair values over different expiries, and a\ndynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real\ntime to keep TLP within a target range. Our analysis blends financial\nengineering (no-arbitrage conditions, option-style pricing) with empirical\nfinance (event studies on cross-listed stocks and futures) to measure TLP under\ntime-zone frictions. We define TLP formally, derive closed-form expressions for\nits term structure under idealized assumptions, and simulate scenarios that\nvary volatility and collateralization. We then propose an LTV policy that\nraises or lowers collateral to expand or curtail time-bound stablecoin supply,\nanalogous to a central bank adjusting rates to defend a peg. We outline\nempirical proxies for TLP, including ADR premiums, overseas index futures\nversus cash index divergence, and pre-market versus official close gaps.\nResults show that TLP grows with closure length and volatility, yet can be\ncontained by adaptive LTV. We provide backtests and figures (term-structure\ncurves, capital-efficiency versus tail-risk trade-offs, time-liquidity\nheatmaps) and discuss protocol design (vault structure, closing-price oracles,\non-chain auction liquidations). The findings position time-bound stablecoins as\na tool to reduce temporal market inefficiencies and inform future research and\ndeployment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u65f6\u95f4\u6d41\u52a8\u6027\u6ea2\u4ef7(TLP)\u6982\u5ff5\uff0c\u4e3a\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u5efa\u7acb\u65e0\u5957\u5229\u5b9a\u4ef7\u6a21\u578b\u548c\u52a8\u6001\u98ce\u9669\u63a7\u5236\u673a\u5236\uff0c\u901a\u8fc7\u8c03\u6574LTV\u6bd4\u7387\u6765\u7ba1\u7406\u8de8\u5e02\u573a\u6d41\u52a8\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bc1\u5238\u5728\u5e02\u573a\u95ed\u5e02\u671f\u95f4\u7684\u6d41\u52a8\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u5b9e\u73b0\u8de8\u5e02\u573a\u8fde\u7eed\u6d41\u52a8\u6027\uff0c\u51cf\u5c11\u65f6\u95f4\u7ef4\u5ea6\u7684\u5e02\u573a\u4f4e\u6548\u7387\u3002", "method": "\u7ed3\u5408\u91d1\u878d\u5de5\u7a0b\uff08\u65e0\u5957\u5229\u6761\u4ef6\u3001\u671f\u6743\u5f0f\u5b9a\u4ef7\uff09\u548c\u5b9e\u8bc1\u91d1\u878d\uff08\u4ea4\u53c9\u4e0a\u5e02\u80a1\u7968\u548c\u671f\u8d27\u7684\u4e8b\u4ef6\u7814\u7a76\uff09\uff0c\u6784\u5efa\u5b9a\u4ef7\u6a21\u578b\u548c\u52a8\u6001LTV\u8c03\u6574\u673a\u5236\u3002", "result": "TLP\u968f\u95ed\u5e02\u65f6\u95f4\u548c\u6ce2\u52a8\u7387\u589e\u957f\uff0c\u4f46\u53ef\u901a\u8fc7\u81ea\u9002\u5e94LTV\u63a7\u5236\uff1b\u63d0\u4f9b\u4e86\u56de\u6d4b\u7ed3\u679c\u548c\u591a\u79cd\u53ef\u89c6\u5316\u5206\u6790\u5de5\u5177\u3002", "conclusion": "\u65f6\u95f4\u7ed1\u5b9a\u7a33\u5b9a\u5e01\u662f\u51cf\u5c11\u65f6\u95f4\u7ef4\u5ea6\u5e02\u573a\u4f4e\u6548\u7387\u7684\u6709\u6548\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2510.05738", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2510.05738", "abs": "https://arxiv.org/abs/2510.05738", "authors": ["Ritesh Chandra", "Sonali Agarwal", "Navjot Singh", "Sadhana Tiwari"], "title": "A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications", "comment": null, "summary": "Exponential growth in heterogeneous healthcare data arising from electronic\nhealth records (EHRs), medical imaging, wearable sensors, and biomedical\nresearch has accelerated the adoption of data lakes and centralized\narchitectures capable of handling the Volume, Variety, and Velocity of Big Data\nfor advanced analytics. However, without effective governance, these\nrepositories risk devolving into disorganized data swamps. Ontology-driven\nsemantic data management offers a robust solution by linking metadata to\nhealthcare knowledge graphs, thereby enhancing semantic interoperability,\nimproving data discoverability, and enabling expressive, domain-aware access.\nThis review adopts a systematic research strategy, formulating key research\nquestions and conducting a structured literature search across major academic\ndatabases, with selected studies analyzed and classified into six categories of\nontology-driven healthcare analytics: (i) ontology-driven integration\nframeworks, (ii) semantic modeling for metadata enrichment, (iii)\nontology-based data access (OBDA), (iv) basic semantic data management, (v)\nontology-based reasoning for decision support, and (vi) semantic annotation for\nunstructured data. We further examine the integration of ontology technologies\nwith Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting\ntheir combined potential to deliver scalable and intelligent healthcare\nanalytics. For each category, recent techniques, representative case studies,\ntechnical and organizational challenges, and emerging trends such as artificial\nintelligence, machine learning, the Internet of Things (IoT), and real-time\nanalytics are reviewed to guide the development of sustainable, interoperable,\nand high-performance healthcare data ecosystems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u672c\u4f53\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u5728\u533b\u7597\u5927\u6570\u636e\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u5c06\u76f8\u5173\u7814\u7a76\u5206\u4e3a\u516d\u5927\u7c7b\u522b\uff0c\u63a2\u8ba8\u4e86\u672c\u4f53\u6280\u672f\u4e0e\u5927\u6570\u636e\u6846\u67b6\u7684\u7ed3\u5408\u6f5c\u529b\u3002", "motivation": "\u533b\u7597\u6570\u636e\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff08\u6765\u81ea\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u3001\u533b\u5b66\u5f71\u50cf\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u7b49\uff09\u63a8\u52a8\u4e86\u6570\u636e\u6e56\u548c\u96c6\u4e2d\u5f0f\u67b6\u6784\u7684\u91c7\u7528\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u6cbb\u7406\u4f1a\u5bfc\u81f4\u6570\u636e\u6cbc\u6cfd\u95ee\u9898\u3002\u672c\u4f53\u9a71\u52a8\u7684\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u80fd\u591f\u901a\u8fc7\u5c06\u5143\u6570\u636e\u94fe\u63a5\u5230\u533b\u7597\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7814\u7a76\u7b56\u7565\uff0c\u5236\u5b9a\u5173\u952e\u7814\u7a76\u95ee\u9898\uff0c\u5728\u4e3b\u8981\u5b66\u672f\u6570\u636e\u5e93\u4e2d\u8fdb\u884c\u7ed3\u6784\u5316\u6587\u732e\u68c0\u7d22\uff0c\u5c06\u9009\u5b9a\u7814\u7a76\u5206\u6790\u5e76\u5206\u7c7b\u4e3a\u516d\u5927\u7c7b\u672c\u4f53\u9a71\u52a8\u7684\u533b\u7597\u5206\u6790\u7c7b\u522b\u3002", "result": "\u8bc6\u522b\u4e86\u516d\u7c7b\u672c\u4f53\u9a71\u52a8\u7684\u533b\u7597\u5206\u6790\uff1a\u672c\u4f53\u9a71\u52a8\u96c6\u6210\u6846\u67b6\u3001\u8bed\u4e49\u5efa\u6a21\u5143\u6570\u636e\u4e30\u5bcc\u3001\u57fa\u4e8e\u672c\u4f53\u7684\u6570\u636e\u8bbf\u95ee\u3001\u57fa\u672c\u8bed\u4e49\u6570\u636e\u7ba1\u7406\u3001\u57fa\u4e8e\u672c\u4f53\u7684\u63a8\u7406\u51b3\u7b56\u652f\u6301\u3001\u975e\u7ed3\u6784\u5316\u6570\u636e\u8bed\u4e49\u6807\u6ce8\u3002", "conclusion": "\u672c\u4f53\u6280\u672f\u4e0e\u5927\u6570\u636e\u6846\u67b6\uff08\u5982Hadoop\u3001Spark\u3001Kafka\u7b49\uff09\u7684\u7ed3\u5408\u5177\u6709\u63d0\u4f9b\u53ef\u6269\u5c55\u667a\u80fd\u533b\u7597\u5206\u6790\u7684\u6f5c\u529b\uff0c\u9700\u8981\u5173\u6ce8\u4eba\u5de5\u667a\u80fd\u3001\u673a\u5668\u5b66\u4e60\u3001\u7269\u8054\u7f51\u548c\u5b9e\u65f6\u5206\u6790\u7b49\u65b0\u5174\u8d8b\u52bf\uff0c\u4ee5\u6307\u5bfc\u53ef\u6301\u7eed\u3001\u4e92\u64cd\u4f5c\u548c\u9ad8\u6027\u80fd\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.05943", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05943", "abs": "https://arxiv.org/abs/2510.05943", "authors": ["Zheyue Tan", "Mustapha Abdullahi", "Tuo Shi", "Huining Yuan", "Zelai Xu", "Chao Yu", "Boxun Li", "Bo Zhao"], "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length.", "AI": {"tldr": "EARL\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u9009\u62e9\u5668\u548c\u6570\u636e\u5206\u53d1\u5668\u6765\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u4e0a\u4e0b\u6587\u957f\u5ea6\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u7684\u5185\u5b58\u4f7f\u7528\u81a8\u80c0\u3001\u5ef6\u8fdf\u589e\u52a0\u548c\u5185\u5b58\u4e0d\u8db3\u6545\u969c\uff0c\u4ee5\u53ca\u4e2d\u95f4\u5f20\u91cf\u79ef\u7d2f\u9020\u6210\u7684\u8de8\u8bbe\u5907\u6570\u636e\u79fb\u52a8\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u5e76\u884c\u9009\u62e9\u5668\u6839\u636e\u5e8f\u5217\u957f\u5ea6\u548c\u7cfb\u7edf\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u6a21\u578b\u548c\u8bad\u7ec3\u5e76\u884c\u5ea6\uff0c\u4ee5\u53ca\u6570\u636e\u5206\u53d1\u5668\u6267\u884c\u5e03\u5c40\u611f\u77e5\u7684\u53bb\u4e2d\u5fc3\u5316\u4e2d\u95f4\u6570\u636e\u6279\u6b21\u4ea4\u6362\u3002", "result": "\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u51cf\u5c11\u4e86\u957f\u4e0a\u4e0b\u6587\u6545\u969c\uff0c\u5b9e\u73b0\u4e86\u4ee3\u7406\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u65e0\u9700\u4f9d\u8d56\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u786c\u9650\u5236\u6216\u60e9\u7f5a\u3002", "conclusion": "EARL\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u5e76\u884c\u5316\u548c\u9ad8\u6548\u6570\u636e\u7ba1\u7406\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002"}}
