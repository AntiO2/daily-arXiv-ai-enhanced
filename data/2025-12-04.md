<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 8]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity](https://arxiv.org/abs/2512.03416)
*Ruiqi Lai,Hongrui Liu,Chengzhi Lu,Zonghao Liu,Siyu Cao,Siyang Shao,Yixin Zhang,Luo Mai,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: TokenScale是一个用于解耦LLM服务的自动扩缩框架，通过Token Velocity指标和Convertible Decoders创新，显著提升SLO达成率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务的prefill/decode解耦架构虽然提高了资源利用率，但难以应对突发工作负载。现有的自动扩缩策略依赖滞后指标（如GPU利用率），导致对负载峰值反应缓慢，造成TTFT和TPOT SLO违规以及昂贵的过度配置。

Method: 1. 提出Token Velocity作为统一prefill、网络和decode阶段工作速率的新指标，作为系统背压的领先指标，实现主动扩缩。
2. 引入Convertible Decoders，允许解码GPU在流量高峰时动态执行prefill任务，创建快速响应缓冲区吸收突发流量，消除新prefiller的初始化延迟。

Result: 在GPU集群上的生产环境评估显示，TokenScale将SLO达成率从50-88%提升到80-96%，相比DistServe、BlitzScale和AIBrix等先进系统，成本降低4-14%。

Conclusion: 通过结合预测性指标和灵活的系统设计，TokenScale显著提升了解耦LLM服务基础设施的性能和效率。

Abstract: The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.

</details>


### [2] [Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks](https://arxiv.org/abs/2512.03487)
*Zhen Wang,Bin Lin,Qiang,Ye*

Main category: cs.DC

TL;DR: 提出了一种面向空天地海一体化网络的双边缘辅助计算卸载与资源分配方案，利用无人机和低轨卫星为海上自主船舶提供并行计算服务，通过联合优化降低系统能耗


<details>
  <summary>Details</summary>
Motivation: 空天地海一体化网络中海事应用的计算需求日益增长，但海上自主船舶的计算资源有限，需要利用无人机和卫星等边缘计算资源来满足低延迟、高能效的计算需求

Method: 提出双边缘辅助计算卸载方案，允许海上自主船舶同时向无人机和低轨卫星卸载部分计算任务；采用多接入方式；通过优化问题建模，联合优化卸载模式、卸载量和计算资源分配；使用交替优化和分层方法分解原始问题

Result: 通过仿真验证了所提方案的有效性和效率，相比基准算法在满足延迟约束的前提下显著降低了系统能耗

Conclusion: 该双边缘辅助计算卸载与资源分配方案能够有效解决空天地海一体化网络中的计算需求问题，在保证服务质量的同时优化系统能耗，为海事智能化应用提供了可行的技术方案

Abstract: In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.

</details>


### [3] [Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas](https://arxiv.org/abs/2512.03565)
*Luis Gall,Samuel James Newcome,Fabio Alexander Gratl,Markus Mühlhäußer,Manish Kumar Mishra,Hans-Joachim Bungartz*

Main category: cs.DC

TL;DR: 该论文探索了SIMD向量化技术来优化分子动力学模拟中的粒子对力计算，通过研究粒子值加载到向量寄存器的顺序，并扩展AutoPas的动态调优机制来选择运行时最优的向量化顺序。


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟在原子尺度上为物理过程提供有价值见解，但计算效率需要优化。先前研究表明最优MD算法可能在运行时变化，因此需要研究模拟特定参数（如粒子密度和邻居识别算法）对性能的影响，并开发动态调优机制。

Method: 研究各种SIMD向量化技术，特别是粒子值加载到向量寄存器的顺序优化。扩展AutoPas粒子模拟库的动态调优机制，使其能够在运行时选择最优的向量化顺序。通过基准测试评估不同粒子相互作用顺序对性能的影响。

Result: 基准测试表明，在运行时考虑不同的粒子相互作用顺序相比AutoPas先前的方法，能够显著提高力计算的性能。动态调优机制能够根据模拟参数选择最优的向量化策略。

Conclusion: 通过优化SIMD向量化中粒子值的加载顺序，并实现动态调优机制，可以显著提升分子动力学模拟中力计算的性能。这种方法能够适应运行时变化的模拟条件，为高性能计算提供有效解决方案。

Abstract: Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.
  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.
  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.

</details>


### [4] [FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management](https://arxiv.org/abs/2512.03644)
*Bohan Zhao,Yuanhong Wang,Chenglin Liu,Jiagi Pan,Guang Yang,Ruitao Liu,Tingrui Zhang,Kai Luo,Wei Xu*

Main category: cs.DC

TL;DR: FFTrainer利用网络剩余带宽快速保存和加载状态，减少LLM训练中的回滚和恢复时间


<details>
  <summary>Details</summary>
Motivation: 随着LLM集群规模扩大，节点故障、恢复时间长和大检查点等问题降低了训练效率。传统的异步检查点方法要么触发昂贵的回滚，要么增加过多开销。

Method: FFTrainer利用网络剩余容量快速保存和加载状态，防止回滚并加速恢复过程。

Result: 相比现有检查点方法，FFTrainer将恢复时间减少高达98%，GPU利用率损失降低高达68%，且不影响正常训练。

Conclusion: FFTrainer通过有效利用网络资源，为大规模LLM训练提供了高效且鲁棒的解决方案。

Abstract: Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.

</details>


### [5] [On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs](https://arxiv.org/abs/2512.03697)
*Rafael Ravedutti Lucio Machado,Jan Eitzinger,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 本文分析了在Fritz和Alex HPC集群上对合成基准测试和Gromacs软件包进行能效分析时遇到的挑战，使用MPI并行化在Intel Ice Lake/Sapphire Rapids CPU和Nvidia A40/A100 GPU上进行实验，展示测量结果并讨论实验中的陷阱，提出未来能效分析的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示在高性能计算集群上进行能效分析时面临的实际挑战和陷阱，特别是在使用不同硬件架构（CPU和GPU）和不同代际处理器时遇到的测量和分析困难。通过实际案例分析，帮助研究人员避免常见错误，提高能效研究的准确性和可靠性。

Method: 在Fritz和Alex HPC集群上，使用MPI并行化在完整的CPU插槽上运行实验，硬件包括Intel Ice Lake和Sapphire Rapids CPU，以及Nvidia A40和A100 GPU。使用Likwid（针对CPU）和Nvidia profiling工具（针对GPU）进行性能指标测量和能效分析。通过对比不同硬件配置下的实验结果，识别测量过程中的挑战和陷阱。

Result: 实验展示了在不同硬件平台（Intel Ice Lake/Sapphire Rapids CPU和Nvidia A40/A100 GPU）上运行合成基准测试和Gromacs应用时获得的能效测量结果。揭示了在使用不同性能分析工具时遇到的测量不一致性、工具兼容性问题以及跨平台比较的挑战。具体结果包括不同硬件配置下的能效表现对比。

Conclusion: 本文总结了在高性能计算环境中进行能效分析时面临的主要挑战，包括测量工具的限制、硬件差异带来的比较困难以及实验设置中的常见陷阱。基于这些经验，提出了未来能效分析研究的最佳实践建议，旨在提高此类研究的准确性和可重复性，为HPC能效优化提供更可靠的指导。

Abstract: This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.

</details>


### [6] [Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods](https://arxiv.org/abs/2512.03825)
*Aingeru Ramos,Jose A Pascual,Javier Navaridas,Ivan Coluzza*

Main category: cs.DC

TL;DR: 本文提出了一种基于OpenMP和CUDA的并行化Metropolis-Hastings并行回火算法实现，在CPU和GPU上分别获得了52倍和986倍的加速比。


<details>
  <summary>Details</summary>
Motivation: 传统MCMC方法在处理复杂构型空间时采样精度不足，而并行回火(Parallel Tempering)等改进技术虽然提高了精度，但显著增加了计算成本。需要通过并行化来抵消这种计算开销，使MCMC/PT技术能更有效地运行并研究更大规模的模型。

Method: 开发了Metropolis-Hastings并行回火算法的并行实现，使用OpenMP进行现代CPU的并行化，使用CUDA进行GPU的并行化。通过多个副本并行运行并定期交换状态来提高采样精度。

Result: 使用OpenMP在48核CPU上获得了最大52倍的加速比，使用CUDA在GPU上获得了986倍的加速比。这些结果也为未来量子实现的相同算法提供了基础基准。

Conclusion: 提出的并行实现显著提高了MCMC/PT算法的计算效率，使得能够更有效地研究复杂系统，并为未来量子实现提供了性能基准。

Abstract: Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.

</details>


### [7] [OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference](https://arxiv.org/abs/2512.03927)
*Liujianfu Wang,Yuyang Du,Yuchen Pan,Soung Chang Liew,Jiacheng Liu,Kexin Chen*

Main category: cs.DC

TL;DR: OD-MoE：一种无需专家缓存的分布式MoE推理框架，通过按需加载专家参数，在边缘设备上实现高效内存利用


<details>
  <summary>Details</summary>
Motivation: 混合专家模型在边缘设备部署时面临内存限制问题，现有的专家卸载方法虽然将专家参数存储在CPU内存中，但GPU内存中保留的专家缓存利用率仍然较低，无法在资源受限的边缘设备上有效运行。

Method: 提出OD-MoE框架，包含两个关键技术：1）在分布式边缘节点上并行化专家加载和专家计算；2）超准确的模拟预测器，在专家计算进行时提前多层预测专家激活。通过按需动态加载和及时驱逐专家参数，最大化GPU内存利用率。

Result: 实验显示：1）OD-MoE达到99.94%的专家激活预测准确率，远超现有方法；2）仅使用1/3 GPU内存即可实现完全GPU缓存MoE部署约75%的解码速度；3）使MoE推理能够在GPU内存小于1GB的边缘节点上运行。

Conclusion: OD-MoE通过消除专家缓存需求，实现了在资源受限边缘设备上的高效MoE推理，为LLM时代低成本物联网设备的MoE部署铺平了道路。

Abstract: Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.

</details>


### [8] [A Chronological Analysis of the Evolution of SmartNICs](https://arxiv.org/abs/2512.04054)
*Olasupo Ajayi,Ryan Grant*

Main category: cs.DC

TL;DR: 该论文对智能网卡(SNIC)进行了15年(2010-2024)的时序分析，基于370篇文献研究了SNIC的演变、制造商、用例和应用领域，旨在澄清其实际用途和适用性。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及和对更快网络访问需求的增长，传统网卡已升级为智能网卡(SNIC)，能够处理海量数据并实现近实时处理。然而，尽管SNIC日益流行，但其确切用途和适用性仍存在争议，特别是随着加速器集成到SNIC中，使其能够分担主机CPU的各种任务，这些争议更加复杂化。

Method: 采用时序分析方法，收集并分析了2010年至2024年期间发表的370篇相关文献，对SNIC的演变历程、制造商情况、具体用例和应用领域进行了系统性研究。

Result: 通过对15年间文献的系统分析，揭示了SNIC的技术演进路径、主要制造商格局、实际应用场景以及在不同领域的应用情况，为理解SNIC的实际价值和适用性提供了实证依据。

Conclusion: 该研究通过大规模文献分析，澄清了智能网卡的发展轨迹和应用现状，为学术界和工业界理解SNIC的技术演进、市场格局和实际应用提供了重要参考，有助于解决关于SNIC适用性的持续争议。

Abstract: Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks](https://arxiv.org/abs/2512.03262)
*Songwen Zhao,Danqing Wang,Kexun Zhang,Jiaxuan Luo,Zhuo Li,Lei Li*

Main category: cs.SE

TL;DR: 论文提出SU S VI B E S基准测试，评估LLM编程代理在真实世界软件工程任务中的安全性表现，发现所有代理在安全性方面表现都很差，即使功能正确的解决方案也只有少数是安全的。


<details>
  <summary>Details</summary>
Motivation: 随着vibe coding（人类工程师指导LLM代理完成复杂编码任务）的普及，需要评估其输出在生产环境中的安全性，特别是在安全敏感应用中。

Method: 构建包含200个真实开源项目中导致漏洞实现的特征请求任务的基准测试SU S VI B E S，评估多个广泛使用的编码代理（包括使用前沿模型的SWE-Agent等）。

Result: 所有编码代理在软件安全方面表现都很差：虽然SWE-Agent with Claude 4 Sonnet的61%解决方案功能正确，但只有10.5%是安全的。初步安全策略（如添加漏洞提示）无法缓解这些安全问题。

Conclusion: 研究结果对vibe coding在安全敏感应用中的广泛采用提出了严重关切，表明当前LLM编程代理存在显著的安全风险。

Abstract: Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.

</details>


### [10] [Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization](https://arxiv.org/abs/2512.03421)
*Hexiang Xu,Hengyuan Liu,Yonghao Wu,Xiaolan Kang,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 该研究评估了LLM在故障定位中的表现，发现具有推理能力的先进模型（如OpenAI o3和DeepSeekR1）在准确性上表现优异，但计算成本高且存在过度推理问题。


<details>
  <summary>Details</summary>
Motivation: 新手程序员由于经验有限，在故障定位上面临挑战。传统方法如SBFL和MBFL缺乏对代码上下文的理解能力，而LLM凭借其理解程序语法和语义的能力，有望克服这些限制。

Method: 使用Codeflaws、Condefects和新构建的BugT数据集，评估了6个闭源和7个开源LLM。BugT数据集专门设计用于缓解数据泄漏问题。研究比较了不同模型在故障定位任务中的表现。

Result: 具有推理能力的先进模型（如OpenAI o3和DeepSeekR1）在准确性上表现优异，对提示工程依赖最小。无推理能力的模型（如GPT-4）需要精心设计的提示才能保持性能。LLM在简单故障定位中表现良好，但随着问题难度增加准确性下降。过度推理和计算成本是主要挑战。

Conclusion: LLM在提高调试效率方面具有潜力，特别对新手程序员有显著帮助价值。但需要在推理能力和计算效率方面进一步改进，以实现实际应用。

Abstract: Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.

</details>


### [11] [Runnable Directories: The Solution to the Monorepo vs. Multi-repo Debate](https://arxiv.org/abs/2512.03815)
*Shayan Ghasemnezhad,Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith,Heanh Sok*

Main category: cs.SE

TL;DR: Causify Dev系统提出了一种混合方法，结合了单仓库和多仓库的优点，通过可运行目录实现可扩展、模块化的代码组织。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统对传统代码库组织策略提出了挑战：单仓库提供一致性但存在可扩展性和工具复杂性；多仓库提供模块化但面临协调和依赖管理困难。需要一种折中方案来解决这些权衡。

Method: 提出Causify Dev系统，核心概念是"可运行目录"——自包含、独立可执行的单元，具有自己的开发、测试和部署生命周期。系统包含统一的轻量环境、共享辅助工具和基于Docker的容器化工作流。

Result: 可运行目录能够实现一致的设置、隔离的依赖关系和高效的CI/CD流程。该系统在单仓库和多仓库策略之间提供了实用的中间地带。

Conclusion: Causify Dev方法为不断增长的复杂代码库提高了可靠性和可维护性，解决了传统代码组织策略的局限性。

Abstract: Modern software systems increasingly strain traditional codebase organization strategies. Monorepos offer consistency but often suffer from scalability issues and tooling complexity, while multi-repos provide modularity at the cost of coordination and dependency management challenges. As an answer to this trade-off, we present the Causify Dev system, a hybrid approach that integrates key benefits of both. Its central concept is the runnable directory -- a self-contained, independently executable unit with its own development, testing, and deployment lifecycles. Backed by a unified thin environment, shared helper utilities, and containerized Docker-based workflows, runnable directories enable consistent setups, isolated dependencies, and efficient CI/CD processes. The Causify Dev approach provides a practical middle ground between monorepo and multi-repo strategies, improving reliability and maintainability for growing, complex codebases.

</details>


### [12] [A Comprehensive Study on the Impact of Vulnerable Dependencies on Open-Source Software](https://arxiv.org/abs/2512.03868)
*Shree Hari Bittugondanahalli Indra Kumar,Lilia Rodrigues Sampaio,André Martin,Andrey Brito,Christof Fetzer*

Main category: cs.SE

TL;DR: 研究分析了1000多个开源项目的5万次发布，发现漏洞依赖主要是传递性的，关键漏洞平均需要一年多才能修复。


<details>
  <summary>Details</summary>
Motivation: 开源库广泛使用但可能引入安全漏洞（如Log4Shell），随着使用量增加，理解并解决这些依赖漏洞变得尤为重要。SCA工具能帮助分析项目依赖，但需要更广泛的研究来了解漏洞的普遍性和修复速度。

Method: 使用VODA（SCA工具）爬取1000多个GitHub项目从2013到2023年的版本历史，分析Java、Python、Rust、Go、Ruby、PHP、JavaScript等多种语言项目。收集库版本、依赖深度、已知漏洞等信息，并追踪其在软件开发周期中的演变。

Result: 1. 大多数编程语言中，漏洞依赖主要是传递性的（transitive）；2. 关键漏洞平均需要超过一年时间才能被修复；3. 数据集比先前研究更大更多样化，提供了更好的洞察力和结果泛化性；4. 回答了关于依赖深度和漏洞持续时间的多个研究问题。

Conclusion: 开源项目中的安全漏洞问题严重，特别是传递性依赖漏洞，且修复周期过长。需要更有效的漏洞管理机制来改善软件供应链安全。

Abstract: Open-source libraries are widely used by software developers to speed up the development of products, however, they can introduce security vulnerabilities, leading to incidents like Log4Shell. With the expanding usage of open-source libraries, it becomes even more imperative to comprehend and address these dependency vulnerabilities. The use of Software Composition Analysis (SCA) tools does greatly help here as they provide a deep insight on what dependencies are used in a project, enhancing the security and integrity in the software supply chain. In order to learn how wide spread vulnerabilities are and how quickly they are being fixed, we conducted a study on over 1k open-source software projects with about 50k releases comprising several languages such as Java, Python, Rust, Go, Ruby, PHP, and JavaScript. Our objective is to investigate the severity, persistence, and distribution of these vulnerabilities, as well as their correlation with project metrics such as team and contributors size, activity and release cycles. In order to perform such analysis, we crawled over 1k projects from github including their version history ranging from 2013 to 2023 using VODA, our SCA tool. Using our approach, we can provide information such as library versions, dependency depth, and known vulnerabilities, and how they evolved over the software development cycle. Being larger and more diverse than datasets used in earlier works and studies, ours provides better insights and generalizability of the gained results. The data collected answers several research questions about the dependency depth and the average time a vulnerability persists. Among other findings, we observed that for most programming languages, vulnerable dependencies are transitive, and a critical vulnerability persists in average for over a year before being fixed.

</details>


### [13] [Tunable Automation in Automated Program Verification](https://arxiv.org/abs/2512.03926)
*Alexander Y. Bai,Chris Hawblitzel,Andrea Lattuada*

Main category: cs.SE

TL;DR: 该论文提出了一种在SMT求解器验证工具中实现细粒度量化词实例化控制的方法，允许开发者在自动化程度和验证性能之间进行选择性调优。


<details>
  <summary>Details</summary>
Motivation: 现有基于SMT求解器的自动化验证工具在处理量化词实例化时面临自动化与性能之间的根本矛盾：激进实例化提供更多自动化但验证时间长，保守实例化响应快但需要更多手动证明提示。

Method: 提出了一种机制，允许在验证上下文中对量化事实的可用性进行细粒度控制。库作者可以提供预定义的自动化级别，同时让最终用户能够在模块、函数或证明上下文级别进一步自定义量化词可用性。

Result: 在基于Rust的验证工具Verus中实现了该技术，并在多个开源代码库上进行了评估。实证分析展示了自动化-性能权衡，并证明选择性量化管理使开发者能够在不同上下文中选择合适的自动化级别。

Conclusion: 通过提供细粒度的量化词实例化控制机制，该研究解决了SMT验证工具中自动化与性能的根本矛盾，使开发者能够根据具体验证需求灵活选择适当的自动化级别。

Abstract: Automated verification tools based on SMT solvers have made significant progress in verifying complex software systems. However, these tools face a fundamental tension between automation and performance when dealing with quantifier instantiation -- the primary source of incompleteness and verification slowdown in SMT-based verifiers. Tools choose between aggressive quantifier instantiation that provides more automation but longer verification times, or conservative instantiation that responds quickly but may require more manual proof hints.
  We present a mechanism that enables fine-grained control over the availability of quantified facts in verification contexts, allowing developers to selectively tune the level of automation. Our approach lets library authors provide different pre-defined automation levels while giving end-users the ability to further customize quantifier availability at the module, function, or proof context level.
  We implement our techniques in Verus, a Rust-based verification tool, and evaluate them on multiple openly available codebases. Our empirical analysis demonstrates the automation-performance tradeoff and that selective quantifier management enables developers to select the appropriate level of automation in different contexts.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [14] [Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases](https://arxiv.org/abs/2512.03278)
*Michael Theologitis,Dan Suciu*

Main category: cs.DB

TL;DR: Thucy：首个跨数据库、跨表格的多智能体声明验证系统，在TabFact基准上超越SOTA 5.6个百分点


<details>
  <summary>Details</summary>
Motivation: 当前社会存在大量可验证的声明冲突，现有验证系统仅能处理小型单表数据库，无法应对真实世界的复杂多源数据验证需求

Method: 构建完全数据源无关的多智能体系统，能够自主发现、检查并推理所有可用关系数据库，通过生成支持性SQL查询提供透明证据

Result: 在TabFact数据集上达到94.3%准确率，比之前最佳结果88.7%提升5.6个百分点

Conclusion: Thucy首次实现了跨数据库、跨表格的自动声明验证，通过SQL查询提供透明证据，显著提升了结构化数据事实验证的性能

Abstract: In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\unicode{x2014}$typically a few hundred rows$\unicode{x2014}$that conveniently fit within an LLM's context window.
  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\unicode{x2014}$the standard benchmark for fact verification over structured data$\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).

</details>


### [15] [Continuous Prompts: LLM-Augmented Pipeline Processing over Unstructured Streams](https://arxiv.org/abs/2512.03389)
*Shu Chen,Deepti Raghavan,Uğur Çetintemel*

Main category: cs.DB

TL;DR: Continuous Prompts (CPs) 框架将 LLM 推理引入连续流处理，通过语义感知的持续计算解决传统 LLM 框架在流式分析中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前 LLM 框架是状态无关的单次处理模式，无法满足非结构化数据流中需要持续语义感知计算的需求，限制了其在长期运行分析中的应用。

Method: 1. 提出 Continuous Prompts (CPs) 框架，将 RAG 扩展到流式场景；2. 定义连续语义算子；3. 提供多种实现（主要基于 LLM，也有嵌入变体）；4. 研究两种 LLM 优化：元组批处理和算子融合；5. 提出动态优化框架，使用轻量级影子执行和成本感知多目标贝叶斯优化来学习吞吐量-准确率边界。

Result: 在 VectraFlow 流处理系统中实现 CPs，通过算子级微基准测试和真实数据集上的流式管道实验，证明系统能够适应工作负载动态变化，在准确率与效率之间进行权衡，并维持对演化中非结构化流的持续语义查询。

Conclusion: CPs 框架成功将 LLM 推理引入连续流处理，通过动态优化机制有效解决了准确率与效率的权衡问题，为长期非结构化流分析提供了可行的解决方案。

Abstract: Monitoring unstructured streams increasingly requires persistent, semantics-aware computation, yet today's LLM frameworks remain stateless and one-shot, limiting their usefulness for long-running analytics. We introduce Continuous Prompts (CPs), the first framework that brings LLM reasoning into continuous stream processing. CPs extend RAG to streaming settings, define continuous semantic operators, and provide multiple implementations, primarily focusing on LLM-based approaches but also reporting one embedding-based variants. Furthermore, we study two LLM-centric optimizations, tuple batching and operator fusion, to significantly improve efficiency while managing accuracy loss.
  Because these optimizations inherently trade accuracy for speed, we present a dynamic optimization framework that uses lightweight shadow executions and cost-aware multi-objective Bayesian optimization (MOBO) to learn throughput-accuracy frontiers and adapt plans under probing budgets.
  We implement CPs in the VectraFlow stream processing system. Using operator-level microbenchmarks and streaming pipelines on real datasets, we show that VectraFlow can adapt to workload dynamics, navigate accuracy-efficiency trade-offs, and sustain persistent semantic queries over evolving unstructured streams.

</details>


### [16] [Enterprise Data Science Platform: A Unified Architecture for Federated Data Access](https://arxiv.org/abs/2512.03401)
*Ryoto Miyamoto,Akira Kasuga*

Main category: cs.DB

TL;DR: EDSP是一个基于数据湖仓架构的企业数据科学平台，采用"一次写入，随处读取"原则，实现多查询引擎环境下的联邦数据访问，消除数据复制和供应商锁定。


<details>
  <summary>Details</summary>
Motivation: 组织在不同部门使用不同数据分析平台时面临数据共享难题，传统数据仓库需要数据复制导致n*m副本问题，增加不一致性和成本，且跨平台访问困难。

Method: 提出EDSP平台，采用四层架构：数据准备层、数据存储层、访问接口层和查询引擎层，基于数据湖仓架构实现联邦数据访问，支持多查询引擎直接查询相同数据源。

Result: 实验证明主要云数据仓库和编程环境可直接查询EDSP管理的数据集，生产部署确认跨多查询引擎的互操作性，相比传统数据迁移方法减少33-44%操作步骤，查询延迟虽增加最多2.6倍但端到端完成时间仍在秒级。

Conclusion: EDSP为多查询引擎环境中的数据孤岛问题提供了实用设计指南，通过集中式数据管理和联邦访问机制，在保持实用性能的同时显著减少操作复杂性。

Abstract: Organizations struggle to share data across departments that have adopted different data analytics platforms. If n datasets must serve m environments, up to n*m replicas can emerge, increasing inconsistency and cost. Traditional warehouses copy data into vendor-specific stores; cross-platform access is hard. This study proposes the Enterprise Data Science Platform (EDSP), which builds on data lakehouse architecture and follows a Write-Once, Read-Anywhere principle. EDSP enables federated data access for multi-query engine environments, targeting data science workloads with periodic data updates and query response times ranging from seconds to minutes. By providing centralized data management with federated access from multiple query engines to the same data sources, EDSP eliminates data duplication and vendor lock-in inherent in traditional data warehouses. The platform employs a four-layer architecture: Data Preparation, Data Store, Access Interface, and Query Engines. This design enforces separation of concerns and reduces the need for data migration when integrating additional analytical environments. Experimental results demonstrate that major cloud data warehouses and programming environments can directly query EDSP-managed datasets. We implemented and deployed EDSP in production, confirming interoperability across multiple query engines. For data sharing across different analytical environments, EDSP achieves a 33-44% reduction in operational steps compared with conventional approaches requiring data migration. Although query latency may increase by up to a factor of 2.6 compared with native tables, end-to-end completion times remain on the order of seconds, maintaining practical performance for analytical use cases. Based on our production experience, EDSP provides practical design guidelines for addressing the data-silo problem in multi-query engine environments.

</details>


### [17] [ExOAR: Expert-Guided Object and Activity Recognition from Textual Data](https://arxiv.org/abs/2512.03790)
*Iris Beerepoot,Vinicius Stein Dani,Xixi Lu*

Main category: cs.DB

TL;DR: ExOAR是一个交互式方法，结合大语言模型和人工验证，从非结构化文本中识别对象和活动，为以对象为中心的过程挖掘提供结构化数据。


<details>
  <summary>Details</summary>
Motivation: 以对象为中心的过程挖掘需要结构化数据，但从非结构化文本中提取这种数据仍然是一个挑战。现有方法难以有效将文本数据转换为具有清晰语义的结构化日志。

Method: ExOAR采用交互式方法，结合大语言模型和人工验证。系统引导用户通过连续阶段：LLM基于上下文输入（如用户职业）和文本数据生成候选对象类型、活动和对象实例，用户审查并完善这些建议后再进入下一阶段。

Result: ExOAR作为实用工具实现，通过演示初步验证，然后使用五个用户的真实世界活动窗口跟踪数据进行评估。结果表明ExOAR能有效弥合非结构化文本数据与以对象为中心的过程分析所需的结构化日志之间的差距，同时保持灵活性和人工监督。

Conclusion: ExOAR成功解决了从非结构化文本中提取结构化数据用于对象中心过程挖掘的挑战，通过结合LLM能力和人工验证，提供了实用且有效的解决方案。

Abstract: Object-centric process mining requires structured data, but extracting it from unstructured text remains a challenge. We introduce ExOAR (Expert-Guided Object and Activity Recognition), an interactive method that combines large language models (LLMs) with human verification to identify objects and activities from textual data. ExOAR guides users through consecutive stages in which an LLM generates candidate object types, activities, and object instances based on contextual input, such as a user's profession, and textual data. Users review and refine these suggestions before proceeding to the next stage. Implemented as a practical tool, ExOAR is initially validated through a demonstration and then evaluated with real-world Active Window Tracking data from five users. Our results show that ExOAR can effectively bridge the gap between unstructured textual data and the structured log with clear semantics needed for object-centric process analysis, while it maintains flexibility and human oversight.

</details>


### [18] [IBM Multilevel Process Mining vs de facto Object-Centric Process Mining approaches](https://arxiv.org/abs/2512.03906)
*Alberto Ronzoni,Anina Antony,Anjana M R,Francesca De Leo,Jesna Jose,Mattia Freda,Nandini Narayanankutty,Rafflesia Khan,Raji RV,Thomas Diacci*

Main category: cs.DB

TL;DR: 本文比较了对象中心过程挖掘和IBM多级过程挖掘两种方法，并基于比较开发了结合两者优势的组织挖掘新功能


<details>
  <summary>Details</summary>
Motivation: 过程挖掘领域正在向对象中心过程挖掘发展，IBM开发了多级过程挖掘方法，需要比较这两种方法的优缺点来指导产品演进

Method: 对对象中心过程挖掘和IBM多级过程挖掘进行描述和比较分析，基于比较结果开发新的组织挖掘功能，并通过示例展示其潜力

Result: IBM基于比较分析开发了组织挖掘新功能，该功能结合了两种方法的优势，展示了这种创新方法的潜力

Conclusion: 通过比较两种过程挖掘方法，IBM成功开发了创新的组织挖掘功能，结合了两种方法的优势，为过程挖掘领域提供了新的方法论

Abstract: The academic evolution of process mining is moving toward object centric process mining, marking a significant shift in how processes are modeled and analyzed. IBM has developed its own distinctive approach called Multilevel Process Mining. This paper provides a description of the two approaches and presents a comparative analysis of their respective advantages and limitations. IBM leveraged this comparison to drive the evolution of IBM Process Mining product, creating the new Organizational Mining feature, an innovation that combines the best of the two approaches. Demonstrate the potential of this novel, innovative and distinct methodology with an example.

</details>
