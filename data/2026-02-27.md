<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 14]
- [cs.DC](#cs.DC) [Total: 18]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [EmpiRE-Compass: A Neuro-Symbolic Dashboard for Sustainable and Dynamic Knowledge Exploration, Synthesis, and Reuse](https://arxiv.org/abs/2602.22276)
*Oliver Karras,Amirreza Alasti,Lena John,Sushant Aggarwal,Yücel Celik*

Main category: cs.SE

TL;DR: EmpiRE-Compass是一个神经符号仪表板，通过语义结构化文献综述数据到研究知识图谱，并利用LLM实现动态访问、复制和重用，提升SE/RE领域文献综述的可持续性。


<details>
  <summary>Details</summary>
Motivation: 软件工程和需求工程领域文献综述数量激增，但生成式AI产生的综述质量、严谨性和透明度不足，且缺乏数据共享，限制了复制和重用。

Method: 开发EmpiRE-Compass神经符号仪表板，基于两个RE用例，采用模块化系统设计和工作流，将文献综述数据语义结构化到研究知识图谱，并利用LLM实现动态访问。

Result: EmpiRE-Compass提供三大核心功能：1) 探索性可视化分析；2) 神经符号合成支持自定义能力问题；3) 所有查询、分析和结果开放可用。仪表板在线免费提供，开源发布代码和文档。

Conclusion: EmpiRE-Compass通过统一研究知识图谱和LLM，降低了技术门槛，促进了透明度和可重复性，实现了协作、持续更新和可重用的文献综述，推动了SE/RE及其他领域的可持续文献综述发展。

Abstract: Software engineering (SE) and requirements engineering (RE) face a significant increase in secondary studies, particularly literature reviews (LRs), due to the ever-growing number of scientific publications. Generative artificial intelligence (GenAI) exacerbates this trend by producing LRs rapidly but often at the expense of quality, rigor, and transparency. At the same time, secondary studies often fail to share underlying data and artifacts, limiting replication and reuse. This paper introduces EmpiRE-Compass, a neuro-symbolic dashboard designed to lower barriers for accessing, replicating, and reusing LR data. Its overarching goal is to demonstrate how LRs can become more sustainable by semantically structuring their underlying data in research knowledge graphs (RKGs) and by leveraging large language models (LLMs) for easy and dynamic access, replication, and reuse. Building on two RE use cases, we developed EmpiRE-Compass with a modular system design and workflows for curated and custom competency questions. The dashboard is freely available online, accompanied by a demonstration video. To manage operational costs, a limit of 25 requests per IP address per day applies to the default LLM (GPT-4o mini). All source code and documentation are released as an open-source project to foster reuse, adoption, and extension. EmpiRE-Compass provides three core capabilities: (1) Exploratory visual analytics for curated competency questions; (2) Neuro-symbolic synthesis for custom competency questions; and (3) Reusable knowledge with all queries, analyses, and results openly available. By unifying RKGs and LLMs in a neuro-symbolic dashboard, EmpiRE-Compass advances sustainable LRs in RE, SE, and beyond. It lowers technical barriers, fosters transparency and reproducibility, and enables collaborative, continuously updated, and reusable LRs

</details>


### [2] [The Ethos of the PEERfect REVIEWer: Scientific Care and Collegial Welfare](https://arxiv.org/abs/2602.22292)
*Oliver Karras*

Main category: cs.SE

TL;DR: 本文提出PEERfect REVIEWer理念，强调同行评审应兼顾科学严谨与同事关怀，通过16条实践建议促进建设性、支持性的评审文化。


<details>
  <summary>Details</summary>
Motivation: 传统同行评审过于强调科学严谨性，缺乏对评审过程中所有参与者（作者、共同评审人、会议组织者、期刊编辑）的关怀与支持，导致评审过程难以促进共同进步和福祉。

Method: 基于作者十年学术经验，结合与同事的专业交流、文献考量、以及对自己撰写和收到的评审意见的分析，提出PEERfect REVIEWer理念及其核心价值（科学关怀与同事福祉），并制定了包含16条实践建议的指导方针。

Result: 提出的PEERfect REVIEWer理念和指导方针帮助评审人在保持高标准科学严谨性的同时，以建设性、支持性、尊重性和及时性的方式进行评审，证明科学严谨与同理心是互补的力量。

Conclusion: 同行评审需要同等重视科学严谨性和同理心，评审人不仅是质量把关者，更是学术旅程中的合作伙伴，应同时担当质量守护者和共同进步与福祉的促进者角色。

Abstract: Peer review remains a cornerstone in academia, yet it frequently falls short in fostering joint progress and well-being. While peer review primarily emphasizes scientific rigor, it often lacks the empathy essential for supporting and encouraging all peers involved. In this experience report, I aim to highlight that peer review is a practice that demands both scientific care for quality and collegial welfare for the joint progress and well-being of all peers involved, including authors, co-reviewers, workshop or conference organizers, and journal editors. Drawing on my ten years of experience in academia, I propose the ethos of the PEERfect REVIEWer, grounded in the two core values: Scientific care and collegial welfare. Through reflection shaped by professional exchanges with colleagues, consideration of literature, and an examination of both self-authored and received reviews, I formulated an accompanying guideline with 16 practical recommendations to guide reviewers in their actions to achieve these two values. The ethos of the PEERfect REVIEWer and its accompanying guideline help reviewers in upholding high scientific standards and conducting peer review in a constructive, supportive, respectful, and timely manner. They demonstrate that scientific rigor and empathy are complementary forces that promote impactful peer review practice. By placing scientific care and collegial welfare at the core of peer review, this experience report reaffirms the importance of scientific rigor while also advocating for greater attention to empathy. It invites reviewers to reconsider their role not merely as gatekeepers but as partners in the academic journey of each peer involved. The PEERfect REVIEWer is both a caretaker of quality and a steward of joint progress and well-being - as truly impactful peer review practice requires scientific rigor and empathy in equal measure.

</details>


### [3] [EyeLayer: Integrating Human Attention Patterns into LLM-Based Code Summarization](https://arxiv.org/abs/2602.22368)
*Jiahao Zhang,Yifan Zhang,Kevin Leach,Yu Huang*

Main category: cs.SE

TL;DR: EyeLayer：通过人类眼动模式增强LLM代码摘要的轻量级注意力模块


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在代码摘要任务上取得了显著进展，但人类在代码理解方面的专业知识是否能进一步指导和增强这些模型仍然是一个开放性问题。作者希望探索人类专业知识（以眼动模式为代理）能否提升LLM的代码摘要能力。

Method: 提出EyeLayer，一个轻量级的注意力增强模块，通过多模态高斯混合模型建模人类阅读代码时的眼动模式，学习捕捉开发者关注位置和强度的参数(μ_i, σ_i^2)，并基于这些参数重新分配token嵌入，将人类注意力先验无缝集成到LLM中而不干扰现有表示。

Result: 在多种模型家族（LLaMA-3.2、Qwen3、CodeBERT）上评估，EyeLayer始终优于强微调基线，在BLEU-4指标上提升高达13.17%，表明人类眼动模式编码了互补的注意力信号，能有效增强LLM的语义焦点并跨模型迁移。

Conclusion: 人类眼动模式作为人类专业知识的代理，能够有效指导和增强LLM的代码摘要能力，EyeLayer提供了一种将人类注意力先验无缝集成到LLM中的轻量级方法，显著提升了代码摘要性能。

Abstract: Code summarization is the task of generating natural language descriptions of source code, which is critical for software comprehension and maintenance. While large language models (LLMs) have achieved remarkable progress on this task, an open question remains: can human expertise in code understanding further guide and enhance these models? We propose EyeLayer, a lightweight attention-augmentation module that incorporates human eye-gaze patterns, as a proxy of human expertise, into LLM-based code summarization. EyeLayer models human attention during code reading via a Multimodal Gaussian Mixture, redistributing token embeddings based on learned parameters (μ_i, σ_i^2) that capture where and how intensively developers focus. This design enables learning generalizable attention priors from eye-tracking data and incorporating them into LLMs seamlessly, without disturbing existing representations. We evaluate EyeLayer across diverse model families (i.e., LLaMA-3.2, Qwen3, and CodeBERT) covering different scales and architectures. EyeLayer consistently outperforms strong fine-tuning baselines across standard metrics, achieving gains of up to 13.17% on BLEU-4. These results demonstrate that human gaze patterns encode complementary attention signals that enhance the semantic focus of LLMs and transfer effectively across diverse models for code summarization.

</details>


### [4] [Contextual Memory Virtualisation: DAG-Based State Management and Structurally Lossless Trimming for LLM Agents](https://arxiv.org/abs/2602.22402)
*Cosmo Santoni*

Main category: cs.SE

TL;DR: 提出Contextual Memory Virtualisation (CMV)系统，将LLM累积的理解视为版本控制状态，通过有向无环图管理会话历史，使用三遍无损修剪算法减少token数量20%-86%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在扩展推理任务中积累了大量状态信息，但当会话达到上下文限制时，这些理解会在压缩过程中丢失，需要一种方法来保存和重用这些累积的知识。

Method: 借鉴操作系统虚拟内存概念，将会话历史建模为有向无环图，定义快照、分支和修剪原语。提出三遍结构无损修剪算法，保留所有用户消息和助手响应，但去除机械冗余如原始工具输出、base64图像和元数据。

Result: 在76个真实世界编码会话的评估中，平均减少20%的token，最高可达86%。混合工具使用会话平均减少39%，在10轮内达到盈亏平衡。在提示缓存下仍保持经济可行性。

Conclusion: CMV系统能够有效管理LLM会话中的累积理解，通过版本控制和智能修剪实现上下文重用，显著减少token消耗，为扩展推理任务提供了实用的状态管理解决方案。

Abstract: As large language models engage in extended reasoning tasks, they accumulate significant state -- architectural mappings, trade-off decisions, codebase conventions -- within the context window. This understanding is lost when sessions reach context limits and undergo lossy compaction. We propose Contextual Memory Virtualisation (CMV), a system that treats accumulated LLM understanding as version-controlled state. Borrowing from operating system virtual memory, CMV models session history as a Directed Acyclic Graph (DAG) with formally defined snapshot, branch, and trim primitives that enable context reuse across independent parallel sessions. We introduce a three-pass structurally lossless trimming algorithm that preserves every user message and assistant response verbatim while reducing token counts by a mean of 20% and up to 86% for sessions with significant overhead by stripping mechanical bloat such as raw tool outputs, base64 images, and metadata. A single-user case-study evaluation across 76 real-world coding sessions demonstrates that trimming remains economically viable under prompt caching, with the strongest gains in mixed tool-use sessions, which average 39% reduction and reach break-even within 10 turns. A reference implementation is available at https://github.com/CosmoNaught/claude-code-cmv.

</details>


### [5] [XMENTOR: A Rank-Aware Aggregation Approach for Human-Centered Explainable AI in Just-in-Time Software Defect Prediction](https://arxiv.org/abs/2602.22403)
*Saumendu Roy,Banani Roy,Chanchal Roy,Richard Bassey*

Main category: cs.SE

TL;DR: XMENTOR是一个集成多种XAI解释的VS Code插件，通过聚合方法减少冲突解释带来的混淆，提升开发者对缺陷预测模型的信任


<details>
  <summary>Details</summary>
Motivation: 基于机器学习的缺陷预测模型虽然能提升软件质量，但其不透明的推理过程导致开发者难以信任。现有的XAI方法（如LIME、SHAP、BreakDown）经常产生相互冲突的解释，增加了开发者的困惑、沮丧和认知负担

Method: 提出XMENTOR方法，这是一个以人为中心、考虑排名感知的聚合方法，实现为VS Code插件。通过自适应阈值、排名和符号一致性以及回退策略，将多个后验解释统一为单一、连贯的视图

Result: 用户研究中近90%的参与者更喜欢聚合后的解释，认为这减少了混淆，并更好地支持日常的调试和缺陷审查任务

Conclusion: 将多种解释方法结合并嵌入到开发者工作流中，可以显著提升缺陷预测模型的可解释性、可用性和信任度

Abstract: Machine learning (ML)-based defect prediction models can improve software quality. However, their opaque reasoning creates an HCI challenge because developers struggle to trust models they cannot interpret. Explainable AI (XAI) methods such as LIME, SHAP, and BreakDown aim to provide transparency, but when used together, they often produce conflicting explanations that increase confusion, frustration, and cognitive load. To address this usability challenge, we introduce XMENTOR, a human-centered, rank-aware aggregation method implemented as a VS Code plugin. XMENTOR unifies multiple post-hoc explanations into a single, coherent view by applying adaptive thresholding, rank and sign agreement, and fallback strategies to preserve clarity without overwhelming users. In a user study, nearly 90% of the participants preferred aggregated explanations, citing reduced confusion and stronger support for daily tasks of debugging and review of defects. Our findings show how combining explanations and embedding them into developer workflows can enhance interpretability, usability, and trust.

</details>


### [6] [Automating the Detection of Requirement Dependencies Using Large Language Models](https://arxiv.org/abs/2602.22456)
*Ikram Darif,Feifei Niu,Manel Abdellatif,Lionel C. Briand,Ramesh S.,Arun Adiththan*

Main category: cs.SE

TL;DR: LEREDD：基于LLM的需求依赖检测方法，利用RAG和ICL技术，在识别需求依赖关系方面表现出色，准确率达0.93，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 需求之间存在多种依赖关系，识别这些依赖对软件开发至关重要。然而，由于现代软件系统需求量大、复杂度高、自然语言需求模糊且频繁变更，依赖检测通常被忽视或手动完成。LLM在自然语言处理方面表现出色，为需求相关任务提供了新可能，但其在需求依赖识别方面的有效性尚未被探索。

Method: 提出LEREDD方法，基于大语言模型（LLM），结合检索增强生成（RAG）和上下文学习（ICL）技术，直接从自然语言需求中识别多种依赖类型。

Result: LEREDD在依赖和非依赖需求分类上准确率达0.93，F1分数0.84（非依赖情况平均0.96）。在细粒度依赖类型检测上表现优异，特别是Requires依赖类型的F1分数相比基线平均提升94.87%和105.41%。同时提供了包含3个系统813个需求对的标注数据集。

Conclusion: LEREDD证明了LLM在需求依赖检测任务中的有效性，显著优于零样本LLM和现有基线方法，为自动化需求依赖识别提供了有前景的解决方案，并贡献了可复现的研究数据集。

Abstract: Requirements are inherently interconnected through various types of dependencies. Identifying these dependencies is essential, as they underpin critical decisions and influence a range of activities throughout software development. However, this task is challenging, particularly in modern software systems, given the high volume of complex, coupled requirements. These challenges are further exacerbated by the ambiguity of Natural Language (NL) requirements and their constant change. Consequently, requirement dependency detection is often overlooked or performed manually. Large Language Models (LLMs) exhibit strong capabilities in NL processing, presenting a promising avenue for requirement-related tasks. While they have shown to enhance various requirements engineering tasks, their effectiveness in identifying requirement dependencies remains unexplored. In this paper, we introduce LEREDD, an LLM-based approach for automated detection of requirement dependencies that leverages Retrieval-Augmented Generation (RAG) and In-Context Learning (ICL). It is designed to identify diverse dependency types directly from NL requirements. We empirically evaluate LEREDD against two state-of-the-art baselines. The results show that LEREDD provides highly accurate classification of dependent and non-dependent requirements, achieving an accuracy of 0.93, and an F1 score of 0.84, with the latter averaging 0.96 for non-dependent cases. LEREDD outperforms zero-shot LLMs and baselines, particularly in detecting fine-grained dependency types, where it yields average relative gains of 94.87% and 105.41% in F1 scores for the Requires dependency over the baselines. We also provide an annotated dataset of requirement dependencies encompassing 813 requirement pairs across three distinct systems to support reproducibility and future research.

</details>


### [7] [RepoMod-Bench: A Benchmark for Code Repository Modernization via Implementation-Agnostic Testing](https://arxiv.org/abs/2602.22518)
*Xuefeng Li,Nir Ben-Israel,Yotam Raz,Belal Ahmed,Doron Serebro,Antoine Raux*

Main category: cs.SE

TL;DR: RepoMod-Bench：一个用于代码现代化代理评估的基准框架，包含21个真实仓库、8种语言、160万行代码，采用黑盒测试防止测试驱动过拟合，发现代理在大规模项目上性能急剧下降。


<details>
  <summary>Details</summary>
Motivation: 现有AI编码代理评估存在两个问题：1）通用代码仓库生成缺乏确定性基准导致评估模糊；2）现有现代化基准局限于小规模仓库且依赖语言特定单元测试，容易导致测试驱动过拟合。

Method: 提出基于实现无关评估范式的基准框架，具体实现为RepoMod-Bench：包含21个真实仓库（14-211K LOC），8种编程语言，160万行代码，11616个测试。通过标准化接口仓库，使用实现无关测试套件验证功能等价性，采用黑盒方法隐藏所有测试以防止代理作弊。

Result: 评估4个SOTA代理配置发现显著的规模崩溃现象：在小于10K LOC项目上平均通过率91.3%，但在超过50K LOC项目上降至15.3%，表明大规模自主现代化仍是重大挑战。

Conclusion: 大规模代码现代化仍是开放挑战，RepoMod-Bench为评估AI编码代理提供了更严谨的基准，其实现无关的黑盒测试方法能更真实反映代理能力。

Abstract: The evolution of AI coding agents has shifted the frontier from simple snippet completion to autonomous repository-level engineering. However, evaluating these agents remains ill-posed in general code repository generation, where the lack of deterministic ground truth leads to ambiguous metrics. Code modernization via automated translation offers a more rigorous alternative by providing a fixed ground truth -- the source repository; yet existing benchmarks are limited to small-scale repositories and rely on language-specific unit tests visible to the agent, allowing test-driven overfitting.
  We address these limitations by introducing a benchmarking framework for repository-level code modernization built on an implementation-agnostic evaluation paradigm. This framework is instantiated through RepoMod-Bench: a benchmark of 21 real-world repositories with standardized interfaces, spanning 8 programming languages. The benchmark contains 1.6M lines of code (LOC) and 11,616 tests, with repository sizes ranging from 14 to 211K LOC. By targeting repositories with standardized interfaces, we utilize an implementation-agnostic test suite to verify functional equivalence between source and target implementations. This black-box approach ensures verification remains consistent across languages, and our environment hides all test suites from agents to prevent test-driven shortcuts. Evaluating four state-of-the-art agent configurations reveals a sharp scaling collapse: average pass rates drop from 91.3% on projects under 10K LOC to 15.3% on projects exceeding 50K LOC. These results demonstrate that autonomous modernization at scale remains a significant open challenge. Our benchmark and code are available at https://github.com/Modelcode-ai/mcode-benchmark.

</details>


### [8] [RandSet: Randomized Corpus Reduction for Fuzzing Seed Scheduling](https://arxiv.org/abs/2602.22729)
*Yuchong Xie,Kaikai Zhang,Yu Liu,Rundong Yang,Ping Chen,Shuai Wang,Dongdong She*

Main category: cs.SE

TL;DR: RandSet提出一种随机化语料库缩减技术，通过随机化子集覆盖所有特征来解决模糊测试中的种子爆炸问题，实现低开销的多样化种子选择。


<details>
  <summary>Details</summary>
Motivation: 模糊测试中的种子爆炸问题导致模糊器维护巨大语料库且无法选择有希望的种子。现有种子优先级调度方法仍受种子爆炸困扰，而传统语料库缩减技术（如cull_queue、AFL-Cmin、MinSet）存在多样性差或开销大的问题。

Method: 将语料库缩减建模为集合覆盖问题，计算一个随机化子集来覆盖整个语料库的所有特征。通过引入随机性获得随机化算法的两个优势：随机化输出（多样化种子选择）和低运行时成本。从这个小而随机的子集中调度种子而非整个语料库。

Result: 在AFL++、LibAFL和Centipede上实现RandSet，在独立程序、FuzzBench和Magma上评估。平均子集比例为4.03%和5.99%，在AFL++上实现16.58%的覆盖率提升，在FuzzBench上最高提升3.57%，在Magma上比最先进技术多触发7个真实漏洞，仅引入1.17%-3.93%的开销。

Conclusion: RandSet通过随机化语料库缩减有效缓解种子爆炸问题，在保持低开销的同时实现多样化的种子选择，显著提升覆盖率和漏洞发现能力。

Abstract: Seed explosion is a fundamental problem in fuzzing seed scheduling, where a fuzzer maintains a huge corpus and fails to choose promising seeds. Existing works focus on seed prioritization but still suffer from seed explosion since corpus size remains huge. We tackle this from a new perspective: corpus reduction, i.e., computing a seed corpus subset. However, corpus reduction could lead to poor seed diversity and large runtime overhead. Prior techniques like cull_queue, AFL-Cmin, and MinSet suffer from poor diversity or prohibitive overhead, making them unsuitable for high-frequency seed scheduling.
  We propose RandSet, a novel randomized corpus reduction technique that reduces corpus size and yields diverse seed selection simultaneously with minimal overhead. Our key insight is introducing randomness into corpus reduction to enjoy two benefits of a randomized algorithm: randomized output (diverse seed selection) and low runtime cost. Specifically, we formulate corpus reduction as a set cover problem and compute a randomized subset covering all features of the entire corpus. We then schedule seeds from this small, randomized subset rather than the entire corpus, effectively mitigating seed explosion.
  We implement RandSet on three popular fuzzers: AFL++, LibAFL, and Centipede, and evaluate it on standalone programs, FuzzBench, and Magma. Results show RandSet achieves significantly more diverse seed selection than other reduction techniques, with average subset ratios of 4.03% and 5.99% on standalone and FuzzBench programs. RandSet achieves a 16.58% coverage gain on standalone programs and up to 3.57% on FuzzBench in AFL++, triggers up to 7 more ground-truth bugs than the state-of-the-art on Magma, while introducing only 1.17%-3.93% overhead.

</details>


### [9] [Evaluating and Improving Automated Repository-Level Rust Issue Resolution with LLM-based Agents](https://arxiv.org/abs/2602.22764)
*Jiahong Xiang,Wenxiao He,Xihua Wang,Hongliang Tian,Yuqun Zhang*

Main category: cs.SE

TL;DR: 论文提出了Rust-SWE-bench基准测试，包含500个真实Rust仓库级任务，并开发了RUSTFORGER智能体，在Claude-Sonnet-3.7上实现了28.6%的任务解决率，比最强基线提升34.9%。


<details>
  <summary>Details</summary>
Motivation: Rust编程语言学习曲线陡峭，编码挑战大，需要自动化问题解决来促进其广泛采用。当前LLM驱动的代码智能体在解决复杂软件工程任务方面表现出色，但缺乏针对Rust的大规模仓库级基准测试。

Method: 1. 创建Rust-SWE-bench基准测试，包含来自34个流行Rust仓库的500个真实仓库级软件工程任务；2. 对四个代表性智能体和四个SOTA LLM进行全面研究；3. 提出RUSTFORGER智能体方法，集成自动化测试环境设置和Rust元编程驱动的动态追踪策略。

Result: 研究发现ReAct风格智能体最多解决21.2%的问题，但受限于代码结构理解和Rust类型/特质语义。RUSTFORGER使用Claude-Sonnet-3.7显著优于所有基线，解决28.6%的任务（比最强基线提升34.9%），独特解决了46个其他智能体无法解决的任务。

Conclusion: Rust-SWE-bench填补了Rust仓库级基准测试的空白，RUSTFORGER通过集成自动化测试环境设置和动态追踪策略，显著提升了Rust代码问题的解决能力，为Rust生态系统的AI辅助开发提供了重要工具。

Abstract: The Rust programming language presents a steep learning curve and significant coding challenges, making the automation of issue resolution essential for its broader adoption. Recently, LLM-powered code agents have shown remarkable success in resolving complex software engineering tasks, yet their application to Rust has been limited by the absence of a large-scale, repository-level benchmark. To bridge this gap, we introduce Rust-SWE-bench, a benchmark comprising 500 real-world, repository-level software engineering tasks from 34 diverse and popular Rust repositories. We then perform a comprehensive study on Rust-SWE-bench with four representative agents and four state-of-the-art LLMs to establish a foundational understanding of their capabilities and limitations in the Rust ecosystem. Our extensive study reveals that while ReAct-style agents are promising, i.e., resolving up to 21.2% of issues, they are limited by two primary challenges: comprehending repository-wide code structure and complying with Rust's strict type and trait semantics. We also find that issue reproduction is rather critical for task resolution. Inspired by these findings, we propose RUSTFORGER, a novel agentic approach that integrates an automated test environment setup with a Rust metaprogramming-driven dynamic tracing strategy to facilitate reliable issue reproduction and dynamic analysis. The evaluation shows that RUSTFORGER using Claude-Sonnet-3.7 significantly outperforms all baselines, resolving 28.6% of tasks on Rust-SWE-bench, i.e., a 34.9% improvement over the strongest baseline, and, in aggregate, uniquely solves 46 tasks that no other agent could solve across all adopted advanced LLMs.

</details>


### [10] [Productivity and Collaboration in Hybrid Agile Teams: An Interview Study](https://arxiv.org/abs/2602.22835)
*Elisabeth Mo,Jefferson Seide Molléri,Asle Fagerstrøm*

Main category: cs.SE

TL;DR: 混合工作模式影响敏捷团队：减少非正式互动、参与不均、工具依赖增加，敏捷仪式成为协调锚点，信任和沟通是关键中介因素


<details>
  <summary>Details</summary>
Motivation: 疫情后混合工作成为现实，改变了敏捷团队交付价值、协作和适应的方式，需要研究混合环境如何影响生产力和协作

Method: 通过对三个挪威敏捷团队的9次访谈进行定性研究，调查混合工作环境的影响

Result: 混合工作减少非正式互动、造成参与不均、增加对数字工具的依赖；敏捷仪式成为协调锚点；信任、沟通和工具支持是团队效能的中介因素

Conclusion: 混合敏捷工作是一个不断发展的领域，需要定制化结构来支持包容性、团队凝聚力和可持续绩效

Abstract: Hybrid work has become a reality post-pandemic, transforming how Agile teams deliver value, collaborate, and adapt. This study investigate how hybrid settings influence productivity and collaboration through nine interviews with three Norwegian Agile teams. Our findings show that hybrid work reduces informal interaction, creates uneven participation, and increases reliance on digital tools. Agile ceremonies became alignment anchors, while trust, communication, and tool support mediate team effectiveness. Hybrid Agile work is an evolving field that requires tailored structures to support inclusion, team cohesion, and sustainable performance.

</details>


### [11] [Managing Uncertainty in LLM-based Multi-Agent System Operation](https://arxiv.org/abs/2602.23005)
*Man Zhang,Tao Yue,Yihua He*

Main category: cs.SE

TL;DR: 该论文提出一个生命周期不确定性管理框架，用于管理LLM多智能体软件系统在安全关键领域中的系统级风险，通过表示、识别、演化和适应四种机制实现运行时治理。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如寿命超声心动图）应用基于LLM的多智能体软件系统时，仅提高模型准确性无法解决系统级风险。不确定性通过智能体协调、数据管道、人机交互和运行时控制逻辑传播，而现有工作主要关注模型层面的不确定性，而非将其作为软件工程的首要关注点。

Method: 首先区分LLM多智能体系统运行中的认识论和本体论不确定性，然后提出基于生命周期的四机制不确定性管理框架：表示、识别、演化和适应。该框架管理不确定性在架构层和执行阶段的出现、转化和缓解。

Result: 通过在真实临床合作开发的LLM多智能体超声心动图软件系统中演示框架可行性，显示在诊断推理中提高了可靠性和可诊断性。

Conclusion: 该方法可推广到其他安全关键的LLM多智能体软件系统，支持超越以模型为中心方法的原理性操作控制和运行时保证。

Abstract: Applying LLM-based multi-agent software systems in safety-critical domains such as lifespan echocardiography introduces system-level risks that cannot be addressed by improving model accuracy alone. During system operation, beyond individual LLM behavior, uncertainty propagates through agent coordination, data pipelines, human-in-the-loop interaction, and runtime control logic. Yet existing work largely treats uncertainty at the model level rather than as a first-class software engineering concern. This paper approaches uncertainty from both system-level and runtime perspectives. We first differentiate epistemological and ontological uncertainties in the context of LLM-based multi-agent software system operation. Building on this foundation, we propose a lifecycle-based uncertainty management framework comprising four mechanisms: representation, identification, evolution, and adaptation. The uncertainty lifecycle governs how uncertainties emerge, transform, and are mitigated across architectural layers and execution phases, enabling structured runtime governance and controlled adaptation. We demonstrate the feasibility of the framework using a real-world LLM-based multi-agent echocardiographic software system developed in clinical collaboration, showing improved reliability and diagnosability in diagnostic reasoning. The proposed approach generalizes to other safety-critical LLM-based multi-agent software systems, supporting principled operational control and runtime assurance beyond model-centric methods.

</details>


### [12] [CL4SE: A Context Learning Benchmark For Software Engineering Tasks](https://arxiv.org/abs/2602.23047)
*Haichuan Hu,Ye Shang,Guoqing Xie,Congqing He,Quanjun Zhang*

Main category: cs.SE

TL;DR: CL4SE提出了首个面向软件工程的上下文学习基准，包含4种SE专用上下文类型分类和超过13,000个样本的数据集，实验显示上下文学习平均提升SE任务性能24.7%。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对软件工程专用上下文类型的系统分类，也没有专门的基准来量化不同上下文在核心SE工作流中的异质效果，需要建立标准化的评估框架。

Method: 提出CL4SE基准，包含四种SE导向的上下文类型分类：可解释示例、项目特定上下文、过程决策上下文、正负上下文，对应四个代表性任务，构建了来自30多个开源项目的13,000多个样本的高质量数据集。

Result: 上下文学习在所有任务上平均提升性能24.7%，具体而言：过程上下文将代码审查性能提升33%，混合正负上下文改进补丁评估30%，项目特定上下文提高代码摘要BLEU分数14.78%，可解释示例增强代码生成PASS@1 5.72%。

Conclusion: CL4SE建立了首个SE上下文学习的标准化评估框架，提供了任务特定上下文设计的可操作经验见解，并发布了大规模数据集以促进该领域的可重复研究。

Abstract: Context engineering has emerged as a pivotal paradigm for unlocking the potential of Large Language Models (LLMs) in Software Engineering (SE) tasks, enabling performance gains at test time without model fine-tuning. Despite its success, existing research lacks a systematic taxonomy of SE-specific context types and a dedicated benchmark to quantify the heterogeneous effects of different contexts across core SE workflows. To address this gap, we propose CL4SE (Context Learning for Software Engineering), a comprehensive benchmark featuring a fine-grained taxonomy of four SE-oriented context types (interpretable examples, project-specific context, procedural decision-making context, and positive & negative context), each mapped to a representative task (code generation, code summarization, code review, and patch correctness assessment). We construct high-quality datasets comprising over 13,000 samples from more than 30 open-source projects and evaluate five mainstream LLMs across nine metrics. Extensive experiments demonstrate that context learning yields an average performance improvement of 24.7% across all tasks. Specifically, procedural context boosts code review performance by up to 33% (Qwen3-Max), mixed positive-negative context improves patch assessment by 30% (DeepSeek-V3), project-specific context increases code summarization BLEU by 14.78% (GPT-Oss-120B), and interpretable examples enhance code generation PASS@1 by 5.72% (DeepSeek-V3). CL4SE establishes the first standardized evaluation framework for SE context learning, provides actionable empirical insights into task-specific context design, and releases a large-scale dataset to facilitate reproducible research in this domain.

</details>


### [13] [LLM-Powered Silent Bug Fuzzing in Deep Learning Libraries via Versatile and Controlled Bug Transfer](https://arxiv.org/abs/2602.23065)
*Kunpeng Zhang,Dongwei Xiao,Daoyuan Wu,Jiali Zhao,Yuanyi Lin,Tongtong Xu,Shaohua Wang,Shuai Wang*

Main category: cs.SE

TL;DR: TransFuzz使用LLM从历史bug报告中提取模式，通过功能相似性匹配API，合成测试用例和定制化验证器，实现跨API的静默bug转移检测，在三大DL库中发现79个新bug


<details>
  <summary>Details</summary>
Motivation: 现有DL模糊测试技术主要检测崩溃，但难以检测静默bug，因为缺乏有效的测试程序和验证器。历史bug报告包含丰富的静默bug信息未被充分利用

Method: 利用LLM从历史问题中提取上下文感知的bug模式，基于功能嵌入匹配语义相关的API，合成带定制验证器的测试用例，实现高风险上下文和验证器设计从已知bug API向功能相似目标API的转移，并引入LLM驱动的自验证模块确保可靠性

Result: 在PyTorch、TensorFlow和MindSpore三大主流DL库中成功发现79个先前未知的bug（其中12个确认为CVE），涵盖10种bug类型，证明了方法的有效性和泛化能力

Conclusion: TransFuzz通过LLM驱动的上下文感知bug转移方法，能够有效检测DL库中的静默bug，显著提升了DL模糊测试在静默bug检测方面的能力，具有很好的泛化性和实际应用价值

Abstract: Deep learning (DL) libraries are widely used in critical applications, where even subtle silent bugs can lead to serious consequences. While existing DL fuzzing techniques have made progress in detecting crashes, they inherently struggle to detect silent bugs due to the lack of effective test programs and corresponding oracles.
  Building on the observation that historical bug reports contain rich, underutilized information about silent bugs, we leverage large language models (LLMs) to perform versatile yet controlled bug transfer for silent bug fuzzing. Specifically, our approach uses LLMs to extract context-aware bug patterns from historical issues, match semantically related Application Programming Interfaces (APIs) using functionality-based embeddings, and synthesize test cases with customized oracles. This enables proactive detection of silent bugs by transferring high-risk contexts and oracle designs from known buggy APIs to functionally similar target APIs. To ensure the reliability of our context-aware bug transfer, we introduce an LLM-powered self-validation module that systematically evaluates the validity of each transferred bug instance. We implement this methodology in a tool named TransFuzz and evaluate it on three mainstream DL libraries: PyTorch, TensorFlow, and MindSpore. TransFuzz successfully discovers 79 previously unknown bugs (12 confirmed as Common Vulnerabilities and Exposures (CVEs)) in 10 bug types, demonstrating its effectiveness and generalizability in migrating DL library bug discovery capabilities.

</details>


### [14] [Utilizing LLMs for Industrial Process Automation](https://arxiv.org/abs/2602.23331)
*Salim Fares*

Main category: cs.SE

TL;DR: 该研究探索将大语言模型应用于工业过程自动化领域，解决专用编程语言的软件开发问题，以加速制造系统开发周期。


<details>
  <summary>Details</summary>
Motivation: 当前LLM研究主要关注Python等通用编程语言，而工业过程自动化领域使用高度专业化的专有语言，这方面的LLM应用研究不足。需要探索LLM如何解决工业开发中的实际编程任务。

Method: 在工业开发过程中集成和利用LLM，解决实际编程任务（如生成机器人手臂运动例程），加速制造系统的开发周期。

Result: 从摘要中无法得知具体实验结果，但研究目标是展示LLM在工业自动化领域的应用潜力，解决专用语言的编程挑战。

Conclusion: LLM在工业过程自动化领域具有重要应用价值，能够解决专用编程语言的开发问题，加速制造系统的开发流程。

Abstract: A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Energy Efficient Federated Learning with Hyperdimensional Computing (HDC)](https://arxiv.org/abs/2602.22290)
*Yahao Ding,Yinchao Yang,Jiaxiang Wang,Zhonghao Liu,Zhaohui Yang,Mingzhe Chen,Mohammad Shikh-Bahaei*

Main category: cs.DC

TL;DR: 提出FL-HDC-DP框架，结合超维计算和差分隐私，通过联合优化HDC维度、发射功率和CPU频率，实现无线边缘网络中安全联邦学习的能耗最小化，相比基线方案节能达83.3%。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络处理大规模分布式数据时面临高计算成本和隐私挑战，需要在无线边缘网络中实现安全高效的联邦学习，同时最小化总能耗。

Method: 提出FL-HDC-DP框架：边缘设备使用超维计算进行轻量级本地训练，应用差分隐私保护传输的模型更新；通过联合优化HDC维度、发射功率和CPU频率来最小化总能耗；开发高效的混合算法，结合外层枚举搜索（HDC维度）和内层一维搜索（资源分配）。

Result: 仿真结果显示，所提框架相比基线方案实现高达83.3%的能耗降低，同时保持高精度和更快的收敛速度。

Conclusion: FL-HDC-DP框架有效解决了无线边缘网络中安全联邦学习的能耗优化问题，通过超维计算的轻量级特性和差分隐私的安全保护，实现了显著的节能效果和良好的性能平衡。

Abstract: This paper investigates the problem of minimizing total energy consumption for secure federated learning (FL) in wireless edge networks, a key paradigm for decentralized big data analytics. To tackle the high computational cost and privacy challenges of processing large-scale distributed data with conventional neural networks, we propose an FL with hyperdimensional computing and differential privacy (FL-HDC-DP) framework. Each edge device employs hyperdimensional computing (HDC) for lightweight local training and applies differential privacy (DP) noise to protect transmitted model updates. The total energy consumption is minimized through a joint optimization of the HDC dimension, transmit power, and CPU frequency. An efficient hybrid algorithm is developed, combining an outer enumeration search for HDC dimensions with an inner one-dimensional search for resource allocation. Simulation results show that the proposed framework achieves up to 83.3% energy reduction compared with baseline schemes, while maintaining high accuracy and faster convergence.

</details>


### [16] [Engineered Simultaneity: The Physical Impossibility of Consolidated Price Discovery Across Spacelike-Separated Exchanges](https://arxiv.org/abs/2602.22350)
*Paul Borrill*

Main category: cs.DC

TL;DR: 论文提出"工程化同时性"概念，指出美国NBBO报价系统因光速延迟导致帧依赖性问题，被高频交易利用，每年造成约50亿美元的信息不对称损失。


<details>
  <summary>Details</summary>
Motivation: 揭示美国证券交易委员会NMS规则611规定的国家最佳买卖报价(NBBO)系统存在根本性设计缺陷。该系统要求比较空间分离的交易所事件，但光速延迟导致无法建立帧独立的同时性概念，这被高频交易公司利用。

Method: 提出"工程化同时性"理论框架，分析NBBO系统的三个特征：需要比较空间分离事件、通过隐含同时性约定实现比较、将结果表示为客观而非约定。通过计算交易所间距离(43-1,180km)和光速延迟(143-3,940微秒)，证明NBBO的帧依赖性。对比直接数据馈送(~数十微秒)与统一证券信息处理器(~1,128微秒)的延迟差异。

Result: 证明NBBO是帧依赖的：其价值取决于定义"当前"价格的参考系。由于交易所间距离导致光速延迟，存在不可避免的窗口期，期间不存在帧独立的价格排序。高频交易公司通过直接数据馈送(延迟比超过50:1)利用这一窗口，每年从其他市场参与者提取约50亿美元。

Conclusion: NBBO系统犯了赖尔意义上的范畴错误：在"同时性"没有帧独立意义的领域应用了这一概念。这种工程化同时性设计导致了严重的信息不对称和市场不公平，需要重新考虑金融市场监管的技术基础。

Abstract: We introduce the concept of engineered simultaneity: a system design that (1) requires comparing events at spacelike-separated locations, (2) implements this comparison via an implicit simultaneity convention, and (3) represents the result as objective rather than conventional. The United States National Best Bid and Offer (NBBO), mandated by SEC Regulation NMS Rule 611, is shown to be an instance of engineered simultaneity. We prove that the NBBO is frame-dependent: its value depends on the reference frame in which "current" prices are defined. Since the exchanges that generate quote data are separated by distances of 43-1,180 km, light-travel times of 143-3,940 microseconds create unavoidable windows during which no frame-independent price ordering exists. High-frequency trading firms exploit this window by accessing exchange data via direct feeds (latency ~tens of microseconds) while the consolidated Securities Information Processor operates at ~1,128 microseconds -- a ratio exceeding 50:1. We demonstrate that this constitutes a category mistake in the sense of Ryle: the NBBO applies the concept of "simultaneity" in a domain where it has no frame-independent meaning. The resulting information asymmetry extracts approximately $5 billion annually from other market participants.

</details>


### [17] [DIAL: Decentralized I/O AutoTuning via Learned Client-side Local Metrics for Parallel File System](https://arxiv.org/abs/2602.22392)
*Md Hasanur Rashid,Xinyi Li,Youbiao He,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: DIAL采用去中心化方法，通过机器学习模型让每个I/O客户端仅基于本地可观测指标进行自主调优，实现全局I/O性能提升


<details>
  <summary>Details</summary>
Motivation: 现有并行文件系统客户端I/O调优方法依赖大量全局运行时指标和精确的I/O模式建模，开销过大，限制了在实际系统中实现细粒度动态调优的能力

Method: 提出DIAL框架，采用去中心化方法，将每个I/O客户端视为独立单元，仅使用本地可观测指标进行调优，通过机器学习模型使多个可调单元做出独立但协同的决策

Result: DIAL能够及时响应全局存储系统的变化，为应用程序实现更好的全局I/O性能

Conclusion: 去中心化的本地指标学习方法相比传统依赖全局指标的调优方法，能够在降低开销的同时实现更高效的动态I/O性能优化

Abstract: Enabling efficient, high-performance data access in parallel file systems (PFS) is critical for today's high-performance computing systems. PFS client-side I/O heavily impacts the final I/O performance delivered to individual applications and the entire system. Autotuning the key client-side I/O behaviors has been extensively studied and shows promising results. However, existing work has heavily relied on extensive number of global runtime metrics to monitor and accurate modeling of applications' I/O patterns. Such heavy overheads significantly limit the ability to enable fine-grained, dynamic tuning in practical systems. In this study, we propose DIAL (Decentralized I/O AutoTuning via Learned Client-side Local Metrics) which takes a drastically different approach. Instead of trying to extract the global I/O patterns of applications, DIAL takes a decentralized approach, treating each I/O client as an independent unit and tuning configurations using only its locally observable metrics. With the help of machine learning models, DIAL enables multiple tunable units to make independent but collective decisions, reacting to what is happening in the global storage systems in a timely manner and achieving better I/O performance globally for the application.

</details>


### [18] [AdapTBF: Decentralized Bandwidth Control via Adaptive Token Borrowing for HPC Storage](https://arxiv.org/abs/2602.22409)
*Md Hasanur Rashid,Dong Dai*

Main category: cs.DC

TL;DR: AdapTBF：一种基于自适应借贷的去中心化I/O带宽控制方法，用于解决HPC应用中存储带宽分配不公的问题，在保持高存储利用率的同时确保公平性。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算应用中，单个计算节点可能消耗不成比例的存储带宽，影响大规模作业的性能，造成资源浪费。传统的比例限制方法会降低整体I/O效率，因为HPC应用通常产生突发性I/O流量。

Method: 在并行文件系统（如Lustre）的Token Bucket Filter基础上，提出AdapTBF方法，采用去中心化的自适应借贷带宽控制机制，允许应用在空闲时借出带宽，在突发时借用带宽。

Result: 通过基于真实场景的合成工作负载评估，AdapTBF能有效管理I/O带宽，即使在极端条件下也能保持高存储利用率，同时确保应用间公平性。

Conclusion: AdapTBF通过自适应借贷机制解决了传统比例限制方法的不足，在最大化单应用性能和整体存储效率的同时，防止小作业阻塞大规模作业，实现了更智能的HPC存储带宽管理。

Abstract: Modern high-performance computing (HPC) applications run on compute resources but share global storage systems. This design can cause problems when applications consume a disproportionate amount of storage bandwidth relative to their allocated compute resources. For example, an application running on a single compute node can issue many small, random writes and consume excessive I/O bandwidth from a storage server. This can hinder larger jobs that write to the same storage server and are allocated many compute nodes, resulting in significant resource waste.
  A straightforward solution is to limit each application's I/O bandwidth on storage servers in proportion to its allocated compute resources. This approach has been implemented in parallel file systems using Token Bucket Filter (TBF). However, strict proportional limits often reduce overall I/O efficiency because HPC applications generate short, bursty I/O. Limiting bandwidth can waste server capacity when applications are idle or prevent applications from temporarily using higher bandwidth during bursty phases.
  We argue that I/O control should maximize per-application performance and overall storage efficiency while ensuring fairness (e.g., preventing small jobs from blocking large-scale ones). We propose AdapTBF, which builds on TBF in modern parallel file systems (e.g., Lustre) and introduces a decentralized bandwidth control approach using adaptive borrowing and lending. We detail the algorithm, implement AdapTBF in Lustre, and evaluate it using synthetic workloads modeled after real-world scenarios. Results show that AdapTBF manages I/O bandwidth effectively while maintaining high storage utilization, even under extreme conditions.

</details>


### [19] [CARAT: Client-Side Adaptive RPC and Cache Co-Tuning for Parallel File Systems](https://arxiv.org/abs/2602.22423)
*Md Hasanur Rashid,Nathan R. Tallent,Forrest Sheng Bao,Dong Dai*

Main category: cs.DC

TL;DR: CARAT是一个基于机器学习的框架，用于在线协同调优并行文件系统的客户端RPC和缓存参数，仅利用本地可观测指标，实现高达3倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 高性能计算系统中并行文件系统的调优面临复杂I/O路径、多样I/O模式和动态系统条件的挑战。现有自动调优框架缺乏可扩展性、自适应性和在线操作能力。

Method: 提出CARAT框架，采用机器学习指导，协同调优客户端RPC和缓存参数，仅依赖本地可观测指标，使每个客户端能够在线独立做出智能调优决策，实时响应应用I/O行为和系统状态变化。

Result: 在Lustre上原型实现CARAT，在动态I/O模式、真实HPC工作负载和多客户端部署中广泛评估，相比默认或静态配置实现高达3倍的性能提升。

Conclusion: CARAT具有可扩展性和轻量级特性，有潜力广泛部署到现有并行文件系统中，使各种数据密集型应用受益。

Abstract: Tuning parallel file system in High-Performance Computing (HPC) systems remains challenging due to the complex I/O paths, diverse I/O patterns, and dynamic system conditions. While existing autotuning frameworks have shown promising results in tuning PFS parameters based on applications' I/O patterns, they lack scalability, adaptivity, and the ability to operate online. In this work, focusing on scalable online tuning, we present CARAT, an ML-guided framework to co-tune client-side RPC and caching parameters of PFS, leveraging only locally observable metrics. Unlike global or pattern-dependent approaches, CARAT enables each client to make independent and intelligent tuning decisions online, responding to real-time changes in both application I/O behaviors and system states. We then prototyped CARAT using Lustre and evaluated it extensively across dynamic I/O patterns, real-world HPC workloads, and multi-client deployments. The results demonstrated that CARAT can achieve up to 3x performance improvement over the default or static configurations, validating the effectiveness and generality of our approach. Due to its scalability and lightweight, we believe CARAT has the potential to be widely deployed into existing PFS and benefit various data-intensive applications.

</details>


### [20] [GetBatch: Distributed Multi-Object Retrieval for ML Data Loading](https://arxiv.org/abs/2602.22434)
*Alex Aizman,Abhishek Gaikwad,Piotr Żelasko*

Main category: cs.DC

TL;DR: GetBatch：一种新的对象存储API，将批量检索提升为一流的存储操作，通过单个确定性、容错的流式执行替代独立的GET操作，显著提升小对象吞吐量并降低延迟


<details>
  <summary>Details</summary>
Motivation: 机器学习训练管道以批次方式消费数据，单个训练步骤可能需要从存储集群中分布的多个分片中提取数千个样本。发出数千个独立的GET请求会产生每个请求的开销，这种开销通常主导数据传输时间，导致效率低下

Method: 引入GetBatch API，将批量检索作为一等存储操作，用单个确定性、容错的流式执行替代独立的GET操作。该方法将多个小对象请求合并为批量操作，减少请求开销并优化数据传输

Result: GetBatch对小对象实现了高达15倍的吞吐量提升；在生产训练工作负载中，将P95批次检索延迟降低2倍，将P99每个对象尾部延迟降低3.7倍，相比独立的GET请求有显著改进

Conclusion: GetBatch通过将批量检索作为核心存储操作，有效解决了机器学习训练中大量小对象请求的开销问题，显著提升了系统性能和效率，为大规模分布式训练提供了更好的存储支持

Abstract: Machine learning training pipelines consume data in batches. A single training step may require thousands of samples drawn from shards distributed across a storage cluster. Issuing thousands of individual GET requests incurs per-request overhead that often dominates data transfer time. To solve this problem, we introduce GetBatch - a new object store API that elevates batch retrieval to a first-class storage operation, replacing independent GET operations with a single deterministic, fault-tolerant streaming execution. GetBatch achieves up to 15x throughput improvement for small objects and, in a production training workload, reduces P95 batch retrieval latency by 2x and P99 per-object tail latency by 3.7x compared to individual GET requests.

</details>


### [21] [veScale-FSDP: Flexible and High-Performance FSDP at Scale](https://arxiv.org/abs/2602.22437)
*Zezhou Wang,Youjie Li,Zhiqi Lin,Jiacheng Yang,Cong Xie,Guanyu Feng,Zheng Zhong,Ziyue Huang,Hongyu Zhu,Zhi Zhang,Yanghua Peng,Xin Liu*

Main category: cs.DC

TL;DR: veScale-FSDP是一个重新设计的FSDP系统，通过灵活的RaggedShard分片格式和结构感知规划算法，解决了现有FSDP在块状量化训练和非逐元素优化器支持上的局限性，实现了更高的吞吐量和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有FSDP系统在处理结构感知训练方法（如块状量化训练）和非逐元素优化器（如Shampoo、Muon）时存在困难，其固定的逐元素或逐行分片格式与块状结构计算冲突。此外，当前实现存在通信和内存效率不足的问题，限制了向数万GPU的扩展能力。

Method: 提出了veScale-FSDP系统，结合灵活的RaggedShard分片格式和结构感知规划算法。RaggedShard支持灵活的数据分片，而规划算法确保性能和灵活性。系统原生支持FSDP所需的高效数据布局，从而支持块状量化和非逐元素优化器。

Result: veScale-FSDP相比现有FSDP系统实现了5~66%的吞吐量提升和16~30%的内存使用降低，同时能够高效扩展到数万GPU规模。

Conclusion: veScale-FSDP通过重新设计的分片格式和规划算法，解决了现有FSDP在支持先进训练技术和扩展性方面的局限性，为大规模模型训练提供了更灵活高效的解决方案。

Abstract: Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs.

</details>


### [22] [Fault-tolerant Reduce and Allreduce operations based on correction](https://arxiv.org/abs/2602.22445)
*Martin Kuettler,Hermann Haertig*

Main category: cs.DC

TL;DR: 提出一种容错的Reduce算法，通过校正通信阶段和树形阶段结合，并扩展到Allreduce


<details>
  <summary>Details</summary>
Motivation: 基于信息传播算法（如gossip或树形通信）的广播实现通常后接校正算法，本文旨在将类似思想应用于Reduce操作，提供容错能力

Method: 采用校正式通信阶段在前、树形阶段在后的两阶段方法，先进行校正通信，再进行树形通信，确保容错性

Result: 开发出能容忍多个进程失败的Reduce算法，提供并证明了算法的语义正确性

Conclusion: 将提出的容错Reduce算法与广播结合，实现了容错的Allreduce操作，为分布式计算提供了可靠的集合通信原语

Abstract: Implementations of Broadcast based on some information dissemination algorithm -- e.g., gossip or tree-based communication -- followed by a correction algorithm has been proposed previously. This work describes an approach to apply a similar idea to Reduce. In it, a correction-like communication phase precedes a tree-based phase. This provides a Reduce algorithm which is tolerant to a number of failed processes. Semantics of the resulting algorithm are provided and proven.
  Based on these results, Broadcast and Reduce are combined to provide Allreduce.

</details>


### [23] [CCCL: Node-Spanning GPU Collectives with CXL Memory Pooling](https://arxiv.org/abs/2602.22457)
*Dong Xu,Han Meng,Xinyu Chen,Dengcheng Zhu,Wei Tang,Fei Liu,Liguang Xie,Wu Xiang,Rui Shi,Yue Li,Henry Hu,Hui Zhang,Jianping Jiang,Dong Li*

Main category: cs.DC

TL;DR: 提出名为\name的集体通信库，利用CXL共享内存池实现跨节点GPU操作，无需传统RDMA网络，在LLM训练中相比InfiniBand获得性能提升和成本节约。


<details>
  <summary>Details</summary>
Motivation: 多节点LLM训练/推理对GPU内存和互联带宽压力大，CXL共享内存池可提供可扩展解决方案，减少资源过度配置并提高利用率。

Method: 设计\name集体通信库，利用CXL共享内存池支持跨节点GPU操作，解决同步、数据交错和通信并行化等挑战。

Result: 在TITAN-II CXL交换机和六个Micron CZ120内存卡上评估，相比200Gbps InfiniBand的RDMA实现，AllGather提升1.34倍，Broadcast提升1.84倍，Gather提升1.94倍，Scatter提升1.04倍。LLM训练案例显示速度提升1.11倍，硬件成本节约2.75倍。

Conclusion: \name展示了CXL在可扩展、内存中心GPU通信方面的潜力，为多节点LLM训练提供了高效且成本效益高的解决方案。

Abstract: Large language models (LLMs) training or inference across multiple nodes introduces significant pressure on GPU memory and interconnect bandwidth. The Compute Express Link (CXL) shared memory pool offers a scalable solution by enabling memory sharing across nodes, reducing over-provisioning and improving resource utilization. We propose \name, a collective communication library, leveraging the CXL shared memory pool to support cross-node GPU operations without relying on traditional RDMA-based networking. Our design addresses the challenges on synchronization, data interleaving, and communication parallelization faced by using the CXL shared memory pool for collective communications. Evaluating on multiple nodes with a TITAN-II CXL switch and six Micron CZ120 memory cards, we show that \name achieves highly efficient collective operations across hosts, demonstrating CXL's potential for scalable, memory-centric GPU communication. Our evaluation demonstrates that \name achieves average performance improvements of 1.34$\times$ for AllGather, 1.84$\times$ for Broadcast, 1.94$\times$ for Gather, and 1.04$\times$ for Scatter, compared to the original RDMA-based implementation over 200 Gbps InfiniBand. \textcolor{dong}{In addition, the evaluation with a case of LLM training shows 1.11$\times$ speedup compared with the InfiniBand while saving production cost by $2.75\times$ in hardware.}

</details>


### [24] [FuxiShuffle: An Adaptive and Resilient Shuffle Service for Distributed Data Processing on Alibaba Cloud](https://arxiv.org/abs/2602.22580)
*Yuhao Lin,Zhipeng Tang,Jiayan Tong,Junqing Xiao,Bin Lu,Yuhang Li,Chao Li,Zhiguo Zhang,Junhua Wang,Hao Luo,James Cheng,Chuang Hu,Jiawei Jiang,Xiao Yan*

Main category: cs.DC

TL;DR: FuxiShuffle是一个为阿里巴巴MaxCompute平台设计的通用数据shuffle服务，通过动态模式选择、进度感知调度和主动容错机制，解决了传统shuffle系统在超大规模集群中适应性差和容错效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 在阿里巴巴MaxCompute平台的超大规模集群运行中，现有shuffle系统无法适应高度动态的作业特性和集群资源条件，且容错机制被动低效。shuffle通常成为分布式数据处理瓶颈，需要更好的适应性和容错能力。

Method: 1. 动态shuffle模式选择：基于运行时信息选择最佳shuffle模式
2. 进度感知调度：为下游worker进行智能调度
3. 自动备份策略：为每个shuffle数据块确定最合适的备份策略
4. 主动容错：多副本故障转移确保数据可用性
5. 内存管理：防止内存溢出
6. 增量恢复机制：不丢失计算进度

Result: 实验表明，与基线系统相比，FuxiShuffle显著减少了端到端作业完成时间和总体资源消耗。微观实验证实了设计在提高适应性和容错效率方面的有效性。

Conclusion: FuxiShuffle作为一个通用的数据shuffle服务，在超大规模生产环境中表现出良好的适应性和高效的故障恢复能力，解决了传统shuffle系统的局限性。

Abstract: Shuffle exchanges intermediate results between upstream and downstream operators in distributed data processing and is usually the bottleneck due to factors such as small random I/Os and network contention. Several systems have been designed to improve shuffle efficiency, but from our experiences of running ultra-large clusters at Alibaba Cloud MaxCompute platform, we observe that they can not adapt to highly dynamic job characteristics and cluster resource conditions, and their fault tolerance mechanisms are passive and inefficient when failures are inevitable. To tackle their limitations, we design and implement FuxiShuffle as a general data shuffle service for the ultra-large production environment of MaxCompute, featuring good adaptability and efficient failure resilience. Specifically, to achieve good adaptability, FuxiShuffle dynamically selects the shuffle mode based on runtime information, conducts progress-aware scheduling for the downstream workers, and automatically determines the most suitable backup strategy for each shuffle data chunk. To make failure resilience efficient, FuxiShuffle actively ensures data availability with multi-replica failover, prevents memory overflow with careful memory management, and employs an incremental recovery mechanism that does not lose computation progress. Our experiments show that, compared to baseline systems, FuxiShuffle significantly reduces not only end-to-end job completion time but also aggregate resource consumption. Micro experiments suggest that our designs are effective in improving adaptability and failure resilience.

</details>


### [25] [FLYING SERVING: On-the-Fly Parallelism Switching for Large Language Model Serving](https://arxiv.org/abs/2602.22593)
*Shouwei Gao,Junqi Yin,Feiyi Wang,Wenqian Dong*

Main category: cs.DC

TL;DR: Flying Serving是一个基于vLLM的LLM服务系统，支持在线DP-TP切换而无需重启工作进程，通过虚拟化状态实现动态并行配置适应，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务系统通常采用静态并行配置，无法适应非平稳流量、混合请求需求和长上下文推理的动态变化，导致性能受限

Method: 1) 零拷贝模型权重管理器按需提供TP分片视图；2) KV缓存适配器在DP/TP布局间保持请求KV状态；3) 预初始化通信池分摊集体设置开销；4) 无死锁调度器协调安全转换

Result: 在三个流行LLM和实际服务场景中，Flying Serving在高负载下性能提升达4.79倍，低负载下提升3.47倍，同时支持延迟和内存驱动的请求

Conclusion: Flying Serving通过在线DP-TP切换实现了动态并行配置适应，显著提升了LLM服务的吞吐量、延迟和上下文容量，解决了静态配置的局限性

Abstract: Production LLM serving must simultaneously deliver high throughput, low latency, and sufficient context capacity under non-stationary traffic and mixed request requirements. Data parallelism (DP) maximizes throughput by running independent replicas, while tensor parallelism (TP) reduces per-request latency and pools memory for long-context inference. However, existing serving stacks typically commit to a static parallelism configuration at deployment; adapting to bursts, priorities, or long-context requests is often disruptive and slow. We present Flying Serving, a vLLM-based system that enables online DP-TP switching without restarting engine workers. Flying Serving makes reconfiguration practical by virtualizing the state that would otherwise force data movement: (i) a zero-copy Model Weights Manager that exposes TP shard views on demand, (ii) a KV Cache Adaptor that preserves request KV state across DP/TP layouts, (iii) an eagerly initialized Communicator Pool to amortize collective setup, and (iv) a deadlock-free scheduler that coordinates safe transitions under execution skew. Across three popular LLMs and realistic serving scenarios, Flying Serving improves performance by up to $4.79\times$ under high load and $3.47\times$ under low load while supporting latency- and memory-driven requests.

</details>


### [26] [Distributed LLM Pretraining During Renewable Curtailment Windows: A Feasibility Study](https://arxiv.org/abs/2602.22760)
*Philipp Wiesner,Soeren Becker,Brett Cornick,Dominik Scheinert,Alexander Acker,Odej Kao*

Main category: cs.DC

TL;DR: 利用可再生能源弃电窗口进行分布式LLM训练，通过弹性调度在多个GPU集群间切换本地训练和联邦同步，显著降低碳排放


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练需要大量计算和能源，而可再生能源经常产生超过电网吸收能力的电力导致弃电。这些弃电窗口提供了使用既清洁又便宜的电力进行LLM预训练的机会

Method: 开发了一个系统，在区域弃电窗口期间跨地理分布式GPU集群进行全参数LLM训练，弹性地在本地单站点训练和联邦多站点同步之间切换。使用Flower联邦学习框架，基于真实边际碳强度轨迹确定弃电时段

Result: 原型系统训练了一个5.61亿参数的Transformer模型，横跨三个集群。初步结果显示，弃电感知调度在保持训练质量的同时，将运营排放降低到单站点基线的5-12%

Conclusion: 利用可再生能源弃电窗口进行分布式LLM训练是可行的，能显著降低碳排放，为可持续AI训练提供了有前景的方向

Abstract: Training large language models (LLMs) requires substantial compute and energy. At the same time, renewable energy sources regularly produce more electricity than the grid can absorb, leading to curtailment, the deliberate reduction of clean generation that would otherwise go to waste. These periods represent an opportunity: if training is aligned with curtailment windows, LLMs can be pretrained using electricity that is both clean and cheap. This technical report presents a system that performs full-parameter LLM training across geo-distributed GPU clusters during regional curtailment windows, elastically switching between local single-site training and federated multi-site synchronization as sites become available or unavailable. Our prototype trains a 561M-parameter transformer model across three clusters using the Flower federated learning framework, with curtailment periods derived from real-world marginal carbon intensity traces. Preliminary results show that curtailment-aware scheduling preserves training quality while reducing operational emissions to 5-12% of single-site baselines.

</details>


### [27] [An Artificial Intelligence Framework for Joint Structural-Temporal Load Forecasting in Cloud Native Platforms](https://arxiv.org/abs/2602.22780)
*Qingyuan Zhang*

Main category: cs.DC

TL;DR: 提出面向微服务拓扑的结构化时序联合负载预测框架，通过多粒度融合和结构先验增强，有效捕捉复杂调用关系中的负载传播与累积


<details>
  <summary>Details</summary>
Motivation: 云原生环境中微服务调用关系复杂、负载波动多尺度叠加、跨服务影响显著，传统方法难以有效建模这种复杂依赖关系

Method: 将系统表示为时间演化的服务调用图与多元负载序列的耦合实体，构建邻域聚合和全局汇总视图，引入轻量级结构先验到注意力计算，采用多目标回归策略联合优化服务级和集群级预测

Result: 通过单因素敏感性分析验证了多粒度融合和结构注入的必要性，明确了时间窗口长度、编码深度和正则化强度等关键参数的有效配置范围

Conclusion: 该框架为云环境中容量评估、资源编排和运行时态势理解提供了可重用的建模范式和实现路径

Abstract: This study targets cloud native environments where microservice invocation relations are complex, load fluctuations are multi-scale and superimposed, and cross-service impacts are significant. We propose a structured temporal joint load prediction framework oriented to microservice topology. The method represents the system as a coupled entity of a time-evolving service invocation graph and multivariate load sequences. It constructs neighborhood-aggregated and global summarized views based on service level observations. This forms layered load representations across instance, service, and cluster levels. A unified sequence encoder models multi-scale historical context. To strengthen the expression of invocation dependencies, the framework introduces a lightweight structural prior into attention computation. This enables more effective capture of load propagation and accumulation along invocation chains, while maintaining consistent modeling of local bursts and overall trends. The training objective adopts a multi-objective regression strategy that jointly optimizes service level and cluster level predictions to improve cross-granularity stability. We further conduct single-factor sensitivity analyses on key structural and training hyperparameters. We systematically examine the effects of time window length, encoding depth, and regularization strength. The results support the necessity of multi-granularity fusion and structural injection and clarify their effective configuration ranges. Overall, the framework provides a reusable modeling paradigm and implementation path for capacity assessment, resource orchestration, and runtime situational understanding in cloud environments.

</details>


### [28] [Workload Buoyancy: Keeping Apps Afloat by Identifying Shared Resource Bottlenecks](https://arxiv.org/abs/2602.22852)
*Oliver Larsson,Thijs Metsch,Cristian Klein,Erik Elmroth*

Main category: cs.DC

TL;DR: 本文提出了一种名为"浮力(buoyancy)"的新抽象概念，用于在多租户异构计算环境中表征工作负载性能，通过整合应用级指标和系统级资源争用洞察，提供比传统启发式方法更好的性能瓶颈指示。


<details>
  <summary>Details</summary>
Motivation: 现代多租户、硬件异构计算环境对有效的工作负载编排提出了重大挑战。传统的简单启发式方法（如CPU利用率或应用级指标）往往无法捕捉资源争用和"嘈杂邻居"效应带来的复杂性能动态。在这种环境中，性能瓶颈可能出现在任何共享系统资源中，导致难以诊断的性能下降。

Method: 本文引入了"浮力(buoyancy)"这一新颖抽象概念，用于表征多租户系统中的工作负载性能。与传统方法不同，浮力将应用级指标与共享资源争用的系统级洞察相结合，提供性能动态的整体视图。通过显式捕获多个资源的瓶颈和余量，浮力实现了直观、可扩展且可跨异构平台泛化的资源感知和应用感知编排。

Result: 使用代表性的多租户工作负载评估浮力，展示了其揭示性能限制性资源交互的能力。浮力相比传统启发式方法平均提供19.3%更好的瓶颈指示。此外，浮力可以作为传统性能指标的即插即用替代品，实现改进的可观测性和更明智的调度与优化决策。

Conclusion: 浮力抽象为多租户异构计算环境中的工作负载性能表征提供了一种有效的解决方案，能够更好地识别性能瓶颈，支持更智能的资源编排和优化决策。

Abstract: Modern multi-tenant, hardware-heterogeneous computing environments pose significant challenges for effective workload orchestration. Simple heuristics for assessing workload performance, such as CPU utilization or application-level metrics, are often insufficient to capture the complex performance dynamics arising from resource contention and noisy-neighbor effects. In such environments, performance bottlenecks may emerge in any shared system resource, leading to unexpected and difficult-to-diagnose degradation.
  This paper introduces buoyancy, a novel abstraction for characterizing workload performance in multi-tenant systems. Unlike traditional approaches, buoyancy integrates application-level metrics with system-level insights of shared resource contention to provide a holistic view of performance dynamics. By explicitly capturing bottlenecks and headroom across multiple resources, buoyancy facilitates resource-aware and application-aware orchestration in a manner that is intuitive, extensible, and generalizable across heterogeneous platforms. We evaluate buoyancy using representative multi-tenant workloads to illustrate its ability to expose performance-limiting resource interactions. Buoyancy provides a 19.3% better indication of bottlenecks compared to traditional heuristics on average. We additionally show how buoyancy can act as a drop-in replacement for conventional performance metrics, enabling improved observability and more informed scheduling and optimization decisions.

</details>


### [29] [A Simple Distributed Deterministic Planar Separator](https://arxiv.org/abs/2602.22916)
*Yaseen Abd-Elhaleem,Michal Dory,Oren Weimann*

Main category: cs.DC

TL;DR: 提出了一种更简单的确定性分布式算法，用于在平面图中计算O(D)大小的平衡分隔符，运行时间为近最优的Õ(D)轮。


<details>
  <summary>Details</summary>
Motivation: 平面图的平衡分隔符在分布式算法中很重要，但现有的确定性算法复杂，随机算法限制了应用范围。需要简单高效的确定性算法来支持各种经典问题的确定性解决方案。

Method: 采用简单的权重转移策略：每个顶点将其权重任意转移到一个它所在的面。基于Lipton-Tarjan算法框架，在分布式模型中实现确定性计算。

Result: 提出了比之前更简单的确定性分隔符算法，具有相同的Õ(D)轮复杂度，可以直接用于确定性化单源最短路径、最大流、有向全局最小割和可达性等问题的分布式算法。

Conclusion: 通过简单的权重转移策略成功实现了高效的确定性平衡分隔符算法，简化了之前复杂或随机的解决方案，为平面图上的经典分布式问题提供了确定性基础。

Abstract: A balanced separator of a graph $G$ is a set of vertices whose removal disconnects the graph into connected components that are a constant factor smaller than $G$. Lipton and Tarjan [FOCS'77] famously proved that every planar graph admits a balanced separator of size $O(\sqrt{n})$, as well as a balanced separator of size $O(D)$ that is a simple path (where $D$ is $G$'s diameter). In the centralized setting, both separators can be found in linear time. In the distributed setting, $D$ is a universal lower bound for the round complexity of solving many optimization problems, so, separators of size $O(D)$ are preferable.
  It was not until [DISC'17] that a distributed algorithm was devised by Ghaffari and Parter to compute such an $O(D)$-size separator in $\tilde O(D)$ rounds, by adapting the Lipton-Tarjan algorithm to the distributed model. Since then, this algorithm was used in several distributed algorithms for planar graphs, e.g., [GP, DISC'17], [LP, STOC'19], [AEDPW, PODC'25]. However, the algorithm is randomized, deeming the algorithms that use it to be randomized as well. Obtaining a deterministic algorithm remained an interesting open question until [PODC'25], when a (complex) deterministic separator algorithm was given by Jauregui, Montealegre and Rapaport.
  We present a much simpler deterministic separator algorithm with the same (near-optimal) $\tilde O(D)$-round complexity. While previous works devised either complicated or randomized ways of transferring weights from vertices to faces of $G$, we show that a straightforward way also works: Each vertex simply transfers its weight to one arbitrary face it lies on. That's it!
  We note that a deterministic separator algorithm directly derandomizes the state-of-the-art distributed algorithms for classical problems on planar graphs such as single-source shortest-paths, maximum-flow, directed global min-cut, and reachability.

</details>


### [30] [LLMServingSim 2.0: A Unified Simulator for Heterogeneous and Disaggregated LLM Serving Infrastructure](https://arxiv.org/abs/2602.23036)
*Jaehong Cho,Hyunmin Choi,Guseul Heo,Jongse Park*

Main category: cs.DC

TL;DR: LLMServingSim 2.0是一个统一的系统级模拟器，用于建模异构和分解式LLM服务基础设施中的硬件-软件运行时交互，支持性能、内存和功耗分析。


<details>
  <summary>Details</summary>
Motivation: 随着LLM服务基础设施向异构化和分解化发展，硬件多样性和系统组件分布式部署使得性能评估变得复杂。现有模拟器缺乏统一框架来联合建模异构硬件和分解式服务技术之间的运行时交互。

Method: 开发LLMServingSim 2.0模拟器，将服务决策和硬件行为嵌入单一运行时循环，支持基于配置文件的扩展性建模，能够捕获批处理、路由、卸载、内存和功耗等动态服务行为。

Result: 验证显示模拟器能准确复现关键性能、内存和功耗指标，平均误差仅0.97%，复杂配置下模拟时间约10分钟，显著优于现有方法。

Conclusion: LLMServingSim 2.0为硬件创新和服务系统设计提供了实用桥梁，支持下一代LLM服务基础设施的系统化探索和协同设计。

Abstract: Large language model (LLM) serving infrastructures are undergoing a shift toward heterogeneity and disaggregation. Modern deployments increasingly integrate diverse accelerators and near-memory processing technologies, introducing significant hardware heterogeneity, while system software increasingly separates computation, memory, and model components across distributed resources to improve scalability and efficiency. As a result, LLM serving performance is no longer determined by hardware or software choices in isolation, but by their runtime interaction through scheduling, data movement, and interconnect behavior. However, understanding these interactions remains challenging, as existing simulators lack the ability to jointly model heterogeneous hardware and disaggregated serving techniques within a unified, runtime-driven framework.
  This paper presents LLMServingSim 2.0, a unified system-level simulator designed to make runtime-driven hardware-software interactions in heterogeneous and disaggregated LLM serving infrastructures explicit and analyzable. LLMServingSim 2.0 embeds serving decisions and hardware behavior into a single runtime loop, enabling interaction-aware modeling of batching, routing, offloading, memory, and power. The simulator supports extensible integration of emerging accelerators and memory systems through profile-based modeling, while capturing dynamic serving behavior and system-level effects. We validate LLMServingSim 2.0 against real deployments, showing that it reproduces key performance, memory, and power metrics with an average error of 0.97%, while maintaining simulation times of around 10 minutes even for complex configurations. These results demonstrate that LLMServingSim 2.0 provides a practical bridge between hardware innovation and serving-system design, enabling systematic exploration and co-design for next-generation LLM serving infrastructures.

</details>


### [31] [STELLAR: Storage Tuning Engine Leveraging LLM Autonomous Reasoning for High Performance Parallel File Systems](https://arxiv.org/abs/2602.23220)
*Chris Egersdoerfer,Philip Carns,Shane Snyder,Robert Ross,Dong Dai*

Main category: cs.DC

TL;DR: STELLAR是一个基于大语言模型的自主并行文件系统调优器，能在前5次尝试中选出接近最优的参数配置，显著降低I/O调优的复杂性和人力成本。


<details>
  <summary>Details</summary>
Motivation: 大规模存储系统的I/O性能调优复杂、成本高且需要大量人力，这对大多数领域科学家来说难以实现。传统自动调优方法通常需要数十万次迭代才能收敛，效率低下。

Method: 基于大语言模型构建自主端到端代理调优系统，包括：1）从软件手册准确提取可调参数；2）分析应用生成的I/O跟踪日志；3）选择初始调优策略；4）在真实系统上重新运行应用并收集I/O性能反馈；5）调整调优策略并重复调优循环；6）将调优经验总结为可重用知识。系统整合了检索增强生成、工具执行、基于LLM的推理和多代理设计。

Result: STELLAR几乎总是能在前5次尝试中为并行文件系统选择接近最优的参数配置，即使对于未见过的应用也是如此。评估了各组件对优化结果的影响，为其他优化领域的类似系统提供了设计见解。

Conclusion: STELLAR的架构和实证结果展示了一种有前景的复杂系统优化方法，特别适用于搜索空间大、探索成本高的问题，同时使I/O调优对领域科学家更加可访问，且只需最少的额外资源。

Abstract: I/O performance is crucial to efficiency in data-intensive scientific computing; but tuning large-scale storage systems is complex, costly, and notoriously manpower-intensive, making it inaccessible for most domain scientists. To address this problem, we propose STELLAR, an autonomous tuner for high-performance parallel file systems. Our evaluations show that STELLAR almost always selects near-optimal parameter configurations for parallel file systems within the first five attempts, even for previously unseen applications.
  STELLAR differs fundamentally from traditional autotuning methods, which often require hundreds of thousands of iterations to converge. Powered by large language models (LLMs), STELLAR enables autonomous end-to-end agentic tuning by (1) accurately extracting tunable parameters from software manuals, (2) analyzing I/O trace logs generated by applications, (3) selecting initial tuning strategies, (4) rerunning applications on real systems and collecting I/O performance feedback, (5) adjusting tuning strategies and repeating the tuning cycle, and (6) reflecting on and summarizing tuning experiences into reusable knowledge for future optimizations. STELLAR integrates retrieval-augmented generation (RAG), tool execution, LLM-based reasoning, and a multiagent design to stabilize reasoning and combat hallucinations.
  We evaluate the impact of each component on optimization outcomes, providing design insights for similar systems in other optimization domains. STELLAR's architecture and empirical results highlight a promising approach to complex system optimization, especially for problems with large search spaces and high exploration costs, while making I/O tuning more accessible to domain scientists with minimal added resources.

</details>


### [32] [Exploiting network topology in brain-scale simulations of spiking neural networks](https://arxiv.org/abs/2602.23274)
*Melissa Lober,Markus Diesmann,Susanne Kunkel*

Main category: cs.DC

TL;DR: 论文提出了一种局部-全局混合通信架构，通过减少同步通信来优化大规模神经元网络模拟性能，挑战了传统认为通信库集体调用是瓶颈的观点。


<details>
  <summary>Details</summary>
Motivation: 大规模脉冲神经元网络模拟的瓶颈在于计算节点间的通信，传统认为受限于互连硬件和通信库，但实际瓶颈是等待最慢节点的同步时间。

Method: 提出基于大脑区域结构的映射策略：将短延迟的区域内连接映射到同一计算节点进行频繁通信，长延迟的区域间连接进行较少频率的全局通信，形成局部-全局混合通信架构。

Result: 在实际示例中展示了显著的性能提升，为传统计算系统提供了能效优化的指导方针，并为神经形态系统设定了更高标准。

Conclusion: 通过结构感知的映射策略减少同步通信，可以显著提升大规模神经元网络模拟性能，挑战了传统通信瓶颈观点，并为大脑结构到超级计算机结构的映射提供了初步方案。

Abstract: Simulation code for conventional supercomputers serves as a reference for neuromorphic computing systems. The present bottleneck of distributed large-scale spiking neuronal network simulations is the communication between compute nodes. Communication speed seems limited by the interconnect between the nodes and the software library orchestrating the data transfer. Profiling reveals, however, that the variability of the time required by the compute nodes between communication calls is large. The bottleneck is in fact the waiting time for the slowest node. A statistical model explains total simulation time on the basis of the distribution of computation times between communication calls. A fundamental cure is to avoid communication calls because this requires fewer synchronizations and reduces the variability of computation times across compute nodes. The organization of the mammalian brain into areas lends itself to such an optimization strategy. Connections between neurons within an area have short delays, but the delays of the long-range connections across areas are an order of magnitude longer. This suggests a structure-aware mapping of areas to compute nodes allowing for a partition into more frequent communication between nodes simulating a particular area and less frequent global communication. We demonstrate a substantial performance gain on a real-world example. This work proposes a local-global hybrid communication architecture for large-scale neuronal network simulations as a first step in mapping the structure of the brain to the structure of a supercomputer. It challenges the long-standing belief that the bottleneck of simulation is synchronization inherent in the collective calls of standard communication libraries. We provide guidelines for the energy efficient simulation of neuronal networks on conventional computing systems and raise the bar for neuromorphic systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [33] [Replacing Multi-Step Assembly of Data Preparation Pipelines with One-Step LLM Pipeline Generation for Table QA](https://arxiv.org/abs/2602.22721)
*Fengyu Li,Junhao Zhu,Kaishi Song,Lu Chen,Zhongming Yao,Tianyi Li,Christian S. Jensen*

Main category: cs.DB

TL;DR: Operation-R1：首个通过可验证奖励强化学习训练轻量级LLM的框架，可在单步推理中生成高质量表格问答数据准备管道，显著提升准确性同时降低成本


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的表格问答解决方案依赖多步操作生成管道，需要多次LLM调用，导致延迟高、计算成本昂贵。需要开发能够在单步推理中生成高质量数据准备管道的轻量级解决方案。

Method: 1. 使用可验证奖励的强化学习变体训练轻量级LLM；2. 引入自监督奖励机制自动获取细粒度管道级监督信号；3. 提出方差感知组重采样缓解训练不稳定性；4. 开发操作合并（多候选共识过滤虚假操作）和自适应回滚（运行时保护防止信息丢失）增强鲁棒性

Result: 在两个基准数据集上，与相同LLM骨干的多步准备基线相比，平均绝对准确率分别提升9.55和6.08个百分点，实现79%的表格压缩和2.2倍的货币成本降低

Conclusion: Operation-R1通过创新的强化学习框架和鲁棒性增强机制，成功实现了在单步推理中生成高质量数据准备管道，在显著提升表格问答准确性的同时大幅降低了计算成本和延迟

Abstract: Table Question Answering (TQA) aims to answer natural language questions over structured tables. Large Language Models (LLMs) enable promising solutions to this problem, with operator-centric solutions that generate table manipulation pipelines in a multi-step manner offering state-of-the-art performance. However, these solutions rely on multiple LLM calls, resulting in prohibitive latencies and computational costs.
  We propose Operation-R1, the first framework that trains lightweight LLMs (e.g., Qwen-4B/1.7B) via a novel variant of reinforcement learning with verifiable rewards to produce high-quality data-preparation pipelines for TQA in a single inference step. To train such an LLM, we first introduce a self-supervised rewarding mechanism to automatically obtain fine-grained pipeline-wise supervision signals for LLM training. We also propose variance-aware group resampling to mitigate training instability. To further enhance robustness of pipeline generation, we develop two complementary mechanisms: operation merge, which filters spurious operations through multi-candidate consensus, and adaptive rollback, which offers runtime protection against information loss in data transformation. Experiments on two benchmark datasets show that, with the same LLM backbone, Operation-R1 achieves average absolute accuracy gains of 9.55 and 6.08 percentage points over multi-step preparation baselines, with 79\% table compression and a 2.2$\times$ reduction in monetary cost.

</details>


### [34] [Optimizing SSD-Resident Graph Indexing for High-Throughput Vector Search](https://arxiv.org/abs/2602.22805)
*Weichen Zhao,Yuncheng Lu,Yao Tian,Hao Zhang,Jiehui Li,Minghao Zhao,Yakun Li,Weining Qian*

Main category: cs.DB

TL;DR: VeloANN：基于SSD的图ANN系统，通过局部性感知数据布局和协程异步运行时解决存储停顿问题，显著提升吞吐量和降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有基于SSD的图ANN系统存在CPU利用率低和读放大问题，主要由于图遍历时访问局部性有限导致的存储停顿

Method: 1) 层次压缩和亲和性数据放置方案，将相关向量共置同一页面；2) 记录级缓冲池，按向量邻居分组记录；3) 协程异步运行时减少I/O中断调度开销；4) 异步预取和波束感知搜索策略优先使用缓存数据

Result: 相比最先进的基于磁盘ANN系统，吞吐量提升5.8倍，延迟降低3.25倍，仅使用内存系统10%内存即达到其0.92倍吞吐量

Conclusion: VeloANN通过局部性优化和异步处理有效缓解存储停顿，显著提升基于SSD的图ANN系统性能，接近内存系统性能但内存占用大幅减少

Abstract: Graph-based approximate nearest neighbor search (ANNS) methods (e.g., HNSW) have become the de facto state of the art for their high precision and low latency. To scale beyond main memory, recent out-of-memory ANNS systems leverage SSDs to store large vector indexes. However, they still suffer from severe CPU underutilization and read amplification (i.e., storage stalls) caused by limited access locality during graph traversal. We present VeloANN, which mitigates storage stalls through a locality-aware data layout and a coroutine-based asynchronous runtime. VeloANN utilizes hierarchical compression and affinity-based data placement scheme to co-locate related vectors within the same page, effectively reducing fragmentation and over-fetching. We further design a record-level buffer pool, where each record groups the neighbors of a vector; by persistently retaining hot records in memory, it eliminates excessive page swapping under constrained memory budgets. To minimize CPU scheduling overheads during disk I/O interruptions, VeloANN employs a coroutine-based asynchronous runtime for lightweight task scheduling. On top of this, it incorporates asynchronous prefetching and a beam-aware search strategy to prioritize cached data, ultimately improving overall search efficiency. Extensive experiments show that VeloANN outperforms state-of-the-art disk-based ANN systems by up to 5.8x in throughput and 3.25x in latency reduction, while achieving 0.92x the throughput of in-memory systems using only 10% of their memory footprint.

</details>


### [35] [Workload-Aware Incremental Reclustering in Cloud Data Warehouses](https://arxiv.org/abs/2602.23289)
*Yipeng Liu,Renfei Zhou,Jiaqi Yan,Haunchen Zhang*

Main category: cs.DB

TL;DR: WAIR提出了一种工作负载感知的自动重聚类方法，通过只重聚类边界微分区来在查询性能和重聚类成本之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现代云数据仓库依赖元数据（如分区映射）进行高效数据剪枝，但大规模表中保持数据聚类对剪枝效果至关重要。现有自动聚类方法在动态云环境中缺乏灵活性，无法适应持续数据摄入和变化的工作负载。

Method: 提出重聚类策略与聚类键选择的清晰分离，引入边界微分区概念（位于查询范围边界的分区），并设计WAIR算法来识别和重聚类对剪枝效率最关键的分区。

Result: WAIR实现了接近完全排序表布局的查询性能，同时显著降低了重聚类成本，具有理论上的上界保证。在标准基准测试和真实工作负载中，相比现有方案提升了查询性能并降低了总体成本。

Conclusion: WAIR通过工作负载感知的智能重聚类策略，在动态云环境中有效平衡了查询性能和重聚类成本，为大规模数据仓库提供了实用的自动聚类解决方案。

Abstract: Modern cloud data warehouses store data in micro-partitions and rely on metadata (e.g., zonemaps) for efficient data pruning during query processing. Maintaining data clustering in a large-scale table is crucial for effective data pruning. Existing automatic clustering approaches lack the flexibility required in dynamic cloud environments with continuous data ingestion and evolving workloads. This paper advocates a clean separation between reclustering policy and clustering-key selection. We introduce the concept of boundary micro-partitions that sit on the boundary of query ranges. We then present WAIR, a workload-aware algorithm to identify and recluster only boundary micro-partitions most critical for pruning efficiency. WAIR achieves near-optimal (with respect to fully sorted table layouts) query performance but incurs significantly lower reclustering cost with a theoretical upper bound. We further implement the algorithm into a prototype reclustering service and evaluate on standard benchmarks (TPC-H, DSB) and a real-world workload. Results show that WAIR improves query performance and reduces the overall cost compared to existing solutions.

</details>


### [36] [AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search](https://arxiv.org/abs/2602.23342)
*Weijian Chen,Haotian Liu,Yangshen Deng,Long Xiang,Liang Huang,Gezi Li,Bo Tang*

Main category: cs.DB

TL;DR: AlayaLaser是一个针对大规模高维向量相似性搜索的高效磁盘图索引系统，通过SIMD指令和多种优化技术解决计算瓶颈问题，性能超越现有磁盘图索引系统，甚至媲美内存索引系统。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为磁盘图索引系统的性能受限于I/O成本，但作者发现随着向量维度增加（数百或数千维），系统实际上受计算瓶颈制约。现有系统普遍专注于减少I/O而忽略了计算开销，这为性能优化提供了重要机会。

Method: 1. 使用改进的roofline模型分析现有磁盘图索引系统性能；2. 设计新颖的磁盘数据布局以利用现代CPU的SIMD指令缓解计算瓶颈；3. 采用多种优化技术：基于度数的节点缓存、基于聚类的入口点选择和早期调度策略。

Result: 在大规模高维向量数据集上的广泛实验表明，AlayaLaser不仅超越了现有磁盘图索引系统，而且性能匹配甚至超过了内存索引系统。

Conclusion: AlayaLaser通过解决计算瓶颈而非传统I/O瓶颈，实现了磁盘图索引系统的显著性能提升，为大规模高维向量相似性搜索提供了高效解决方案。

Abstract: On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space.
  In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-based node cache, cluster-based entry point selection, and early dispatch strategy) to further improve the performance of AlayaLaser. We last conduct extensive experimental studies on a wide range of large-scale high-dimensional vector datasets to verify the superiority of AlayaLaser. Specifically, AlayaLaser not only surpasses existing on-disk graph-based index systems but also matches or even exceeds the performance of in-memory index systems.

</details>
