{"id": "2602.21411", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21411", "abs": "https://arxiv.org/abs/2602.21411", "authors": ["Marc Dufay", "Diana Ghinea", "Anton Paramonov"], "title": "General Convex Agreement with Near-Optimal Communication", "comment": "Working paper", "summary": "Convex Agreement (CA) strengthens Byzantine Agreement (BA) by requiring the output agreed upon to lie in the convex hull of the honest parties' inputs. This validity condition is motivated by practical aggregation tasks (e.g., robust learning or sensor fusion) where honest inputs need not coincide but should still constrain the decision. CA inherits BA lower bounds, and optimal synchronous round complexity is easy to obtain (e.g., via Byzantine Broadcast). The main challenge is \\emph{communication}: standard approaches for CA have a communication complexity of $\u0398(Ln^2)$ for large $L$-bit inputs, leaving a gap in contrast to BA's lower bound of $\u03a9(Ln)$ bits. While recent work achieves optimal communication complexity of $O(Ln)$ for sufficiently large $L$ [GLW,PODC'25], translating this result to general convexity spaces remained an open problem.\n  We investigate this gap for abstract convexity spaces, and we present deterministic synchronous CA protocols with near-optimal communication complexity: when $L = \u03a9(n \\cdot \u03ba)$, where $\u03ba$ is a security parameter, we achieve $O(L\\cdot n\\log n)$ communication for finite convexity spaces and $O(L\\cdot n^{1+o(1)})$ communication for Euclidean spaces $\\mathbb{R}^d$. Our protocols have asymptotically optimal round complexity $O(n)$ and, when a bound on the inputs' lengths $L$ is fixed a priori, we achieve near-optimal resilience $t < n/(\u03c9+\\varepsilon)$ for any constant $\\varepsilon>0$, where $\u03c9$ is the Helly number of the convexity space. If $L$ is unknown, we still achieve resilience $t<n/(\u03c9+\\varepsilon+1)$ for any constant $\\varepsilon > 0$. We further note that our protocols can be leveraged to efficiently solve parallel BA.\n  Our main technical contribution is the use of extractor graphs to obtain a deterministic assignment of parties to committees, which is resilient against adaptive adversaries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9488\u5bf9\u62bd\u8c61\u51f8\u7a7a\u95f4\u7684\u786e\u5b9a\u6027\u540c\u6b65\u51f8\u534f\u8bae\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u901a\u4fe1\u590d\u6742\u5ea6\uff1a\u5bf9\u4e8e\u6709\u9650\u51f8\u7a7a\u95f4\u4e3aO(L\u00b7n log n)\uff0c\u5bf9\u4e8e\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e3aO(L\u00b7n^{1+o(1)})\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f18\u8f6e\u590d\u6742\u5ea6O(n)\u3002", "motivation": "\u51f8\u534f\u8bae\uff08CA\uff09\u76f8\u6bd4\u62dc\u5360\u5ead\u534f\u8bae\uff08BA\uff09\u8981\u6c42\u8f93\u51fa\u4f4d\u4e8e\u8bda\u5b9e\u65b9\u8f93\u5165\u7684\u51f8\u5305\u5185\uff0c\u9002\u7528\u4e8e\u9700\u8981\u805a\u5408\u4f46\u8f93\u5165\u4e0d\u5fc5\u5b8c\u5168\u4e00\u81f4\u7684\u5b9e\u7528\u573a\u666f\uff08\u5982\u9c81\u68d2\u5b66\u4e60\u3001\u4f20\u611f\u5668\u878d\u5408\uff09\u3002\u73b0\u6709CA\u534f\u8bae\u901a\u4fe1\u590d\u6742\u5ea6\u4e3a\u0398(Ln\u00b2)\uff0c\u4e0eBA\u7684\u4e0b\u754c\u03a9(Ln)\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u4e3a\u62bd\u8c61\u51f8\u7a7a\u95f4\u8bbe\u8ba1\u901a\u4fe1\u9ad8\u6548\u7684\u534f\u8bae\u3002", "method": "\u4f7f\u7528\u63d0\u53d6\u5668\u56fe\u83b7\u5f97\u786e\u5b9a\u6027\u59d4\u5458\u4f1a\u5206\u914d\u65b9\u6848\uff0c\u80fd\u591f\u62b5\u6297\u81ea\u9002\u5e94\u654c\u624b\u653b\u51fb\u3002\u534f\u8bae\u8bbe\u8ba1\u9488\u5bf9\u62bd\u8c61\u51f8\u7a7a\u95f4\uff0c\u5f53L=\u03a9(n\u00b7\u03ba)\u65f6\uff0c\u5b9e\u73b0\u6709\u9650\u51f8\u7a7a\u95f4\u7684O(L\u00b7n log n)\u901a\u4fe1\u548c\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u7684O(L\u00b7n^{1+o(1)})\u901a\u4fe1\u3002", "result": "\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u901a\u4fe1\u590d\u6742\u5ea6\uff1a\u6709\u9650\u51f8\u7a7a\u95f4O(L\u00b7n log n)\uff0c\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4O(L\u00b7n^{1+o(1)})\uff0c\u8f6e\u590d\u6742\u5ea6\u4e3a\u6700\u4f18\u7684O(n)\u3002\u5f53\u8f93\u5165\u957f\u5ea6L\u5df2\u77e5\u65f6\uff0c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5bb9\u9519\u6027t < n/(\u03c9+\u03b5)\uff1bL\u672a\u77e5\u65f6\uff0c\u5bb9\u9519\u6027\u4e3at < n/(\u03c9+\u03b5+1)\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u62bd\u8c61\u51f8\u7a7a\u95f4\u51f8\u534f\u8bae\u901a\u4fe1\u590d\u6742\u5ea6\u7684\u7406\u8bba\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u53d6\u5668\u56fe\u6280\u672f\u5b9e\u73b0\u4e86\u786e\u5b9a\u6027\u59d4\u5458\u4f1a\u5206\u914d\uff0c\u4e3a\u51f8\u534f\u8bae\u63d0\u4f9b\u4e86\u901a\u4fe1\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u5e76\u884c\u62dc\u5360\u5ead\u534f\u8bae\u3002"}}
{"id": "2602.21548", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21548", "abs": "https://arxiv.org/abs/2602.21548", "authors": ["Yongtong Wu", "Shaoyuan Chen", "Yinmin Zhong", "Rilin Huang", "Yixuan Tan", "Wentao Zhang", "Liyue Zhang", "Shangyan Zhou", "Yuxuan Liu", "Shunfeng Zhou", "Mingxing Zhang", "Xin Jin", "Panpan Huang"], "title": "DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference", "comment": null, "summary": "The performance of multi-turn, agentic LLM inference is increasingly dominated by KV-Cache storage I/O rather than computation. In prevalent disaggregated architectures, loading the massive KV-Cache from external storage creates a fundamental imbalance: storage NICs on prefill engines become bandwidth-saturated, while those on decoding engines remain idle. This asymmetry severely constrains overall system throughput.\n  We present DualPath, an inference system that breaks this bottleneck by introducing dual-path KV-Cache loading. Beyond the traditional storage-to-prefill path, DualPath enables a novel storage-to-decode path, in which the KV-Cache is loaded into decoding engines and then efficiently transferred to prefill engines via RDMA over the compute network. DualPath combines this optimized data path -- which inherently avoids network congestion and avoids interference with latency-critical model execution communications -- with a global scheduler that dynamically balances load across prefill and decode engines.\n  Our evaluation on three models with production agentic workloads demonstrates that DualPath improves offline inference throughput by up to 1.87$\\times$ on our in-house inference system. It can also improve online serving throughput by an average factor of 1.96$\\times$ without violating SLO.", "AI": {"tldr": "DualPath\u901a\u8fc7\u5f15\u5165\u53cc\u8def\u5f84KV-Cache\u52a0\u8f7d\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u591a\u8f6e\u4ee3\u7406\u5f0fLLM\u63a8\u7406\u4e2dKV-Cache\u5b58\u50a8I/O\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u591a\u8f6e\u4ee3\u7406\u5f0fLLM\u63a8\u7406\u7684\u6027\u80fd\u8d8a\u6765\u8d8a\u53d7KV-Cache\u5b58\u50a8I/O\u800c\u975e\u8ba1\u7b97\u9650\u5236\u3002\u5728\u6d41\u884c\u7684\u89e3\u8026\u67b6\u6784\u4e2d\uff0c\u4ece\u5916\u90e8\u5b58\u50a8\u52a0\u8f7d\u5927\u91cfKV-Cache\u5bfc\u81f4\u5b58\u50a8NIC\u5e26\u5bbd\u9971\u548c\uff0c\u800c\u89e3\u7801\u5f15\u64ce\u7684NIC\u5374\u95f2\u7f6e\uff0c\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u4e25\u91cd\u9650\u5236\u4e86\u7cfb\u7edf\u6574\u4f53\u541e\u5410\u91cf\u3002", "method": "DualPath\u5f15\u5165\u53cc\u8def\u5f84KV-Cache\u52a0\u8f7d\u673a\u5236\uff1a\u9664\u4e86\u4f20\u7edf\u7684\u5b58\u50a8\u5230\u9884\u586b\u5145\u8def\u5f84\u5916\uff0c\u65b0\u589e\u5b58\u50a8\u5230\u89e3\u7801\u8def\u5f84\uff0c\u5c06KV-Cache\u52a0\u8f7d\u5230\u89e3\u7801\u5f15\u64ce\uff0c\u7136\u540e\u901a\u8fc7RDMA\u5728\u8ba1\u7b97\u7f51\u7edc\u4e0a\u9ad8\u6548\u4f20\u8f93\u5230\u9884\u586b\u5145\u5f15\u64ce\u3002\u8be5\u7cfb\u7edf\u7ed3\u5408\u4f18\u5316\u7684\u6570\u636e\u8def\u5f84\uff08\u907f\u514d\u7f51\u7edc\u62e5\u585e\u548c\u5ef6\u8fdf\u5173\u952e\u6a21\u578b\u6267\u884c\u901a\u4fe1\u7684\u5e72\u6270\uff09\u4ee5\u53ca\u5168\u5c40\u8c03\u5ea6\u5668\u52a8\u6001\u5e73\u8861\u9884\u586b\u5145\u548c\u89e3\u7801\u5f15\u64ce\u7684\u8d1f\u8f7d\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u578b\u7684\u751f\u4ea7\u4ee3\u7406\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0cDualPath\u5c06\u79bb\u7ebf\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe1.87\u500d\uff0c\u5728\u7ebf\u670d\u52a1\u541e\u5410\u91cf\u5e73\u5747\u63d0\u53471.96\u500d\uff0c\u4e14\u4e0d\u8fdd\u53cdSLO\u3002", "conclusion": "DualPath\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u8def\u5f84KV-Cache\u52a0\u8f7d\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6eLLM\u63a8\u7406\u4e2d\u7684\u5b58\u50a8I/O\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\uff0c\u4e3a\u4ee3\u7406\u5f0f\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21626", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21626", "abs": "https://arxiv.org/abs/2602.21626", "authors": ["Yifan Sun", "Gholamreza Haffar", "Minxian Xu", "Rajkumar Buyya", "Adel N. Toosi"], "title": "Multi-Layer Scheduling for MoE-Based LLM Reasoning", "comment": "12 pages, 10 figures", "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide range of tasks, but serving them efficiently at scale remains a critical challenge due to their substantial computational and latency demands. While most existing inference frameworks rely on simple scheduling strategies such as First-Come-First-Serve (FCFS) at the engine level and Round-Robin (RR) at the scheduler or coordinator level, they often fail to fully utilize system resources and may suffer from issues such as head-of-line blocking and load imbalance. Recent advances in Mixture-of-Experts (MoE) models have also introduced new challenges in scheduling arising from expert parallelism and routing complexity. This research proposes a multi-layer scheduling framework tailored for MoE-based LLM serving. It targets scheduling at three levels: request-level, enginelevel, and expert-level. At the request level, we explore algorithms such as Shortest-Job-First (SJF) and priority-aware aging to improve throughput and reduce latency. At the engine level, we design load-aware dispatching strategies that account for the current prefix token load, KV cache utilization, and user stickiness to achieve better resource matching. At the expert level, we focus on alleviating expert hotspots and strategically placing inter-layer expert dependencies to balance load and improve routing efficiency. Extensive experimental results from more than 100 experiments conducted under diverse workload distributions show that our approach consistently outperforms the state-of-theart inference framework vLLM, achieving up to 17.8% reduction in Time To First Token (TTFT) latency and 13.3% reduction in Time-Per-Output-Token (TPOT) latency.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9MoE\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u591a\u5c42\u8c03\u5ea6\u6846\u67b6\uff0c\u5728\u8bf7\u6c42\u3001\u5f15\u64ce\u548c\u4e13\u5bb6\u4e09\u4e2a\u5c42\u9762\u4f18\u5316\u8c03\u5ea6\u7b56\u7565\uff0c\u76f8\u6bd4vLLM\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u9762\u4e34\u8ba1\u7b97\u548c\u5ef6\u8fdf\u6311\u6218\uff0c\u73b0\u6709\u8c03\u5ea6\u7b56\u7565\u5982FCFS\u548cRR\u65e0\u6cd5\u5145\u5206\u5229\u7528\u7cfb\u7edf\u8d44\u6e90\uff0c\u5b58\u5728\u961f\u5934\u963b\u585e\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u800cMoE\u6a21\u578b\u53c8\u5e26\u6765\u4e86\u4e13\u5bb6\u5e76\u884c\u548c\u8def\u7531\u590d\u6742\u6027\u7684\u65b0\u6311\u6218", "method": "\u63d0\u51fa\u4e09\u5c42\u8c03\u5ea6\u6846\u67b6\uff1a1) \u8bf7\u6c42\u5c42\u91c7\u7528SJF\u548c\u4f18\u5148\u7ea7\u611f\u77e5\u8001\u5316\u7b97\u6cd5\uff1b2) \u5f15\u64ce\u5c42\u57fa\u4e8e\u524d\u7f00token\u8d1f\u8f7d\u3001KV\u7f13\u5b58\u5229\u7528\u7387\u548c\u7528\u6237\u7c98\u6027\u8bbe\u8ba1\u8d1f\u8f7d\u611f\u77e5\u5206\u53d1\u7b56\u7565\uff1b3) \u4e13\u5bb6\u5c42\u7f13\u89e3\u4e13\u5bb6\u70ed\u70b9\u5e76\u7b56\u7565\u6027\u653e\u7f6e\u5c42\u95f4\u4e13\u5bb6\u4f9d\u8d56", "result": "\u5728100\u591a\u4e2a\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u5206\u5e03\u7684\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4vLLM\u6846\u67b6\uff0cTTFT\u5ef6\u8fdf\u964d\u4f4e17.8%\uff0cTPOT\u5ef6\u8fdf\u964d\u4f4e13.3%", "conclusion": "\u9488\u5bf9MoE\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u591a\u5c42\u8c03\u5ea6\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316\u7cfb\u7edf\u8d44\u6e90\u5229\u7528\uff0c\u663e\u8457\u964d\u4f4e\u670d\u52a1\u5ef6\u8fdf\uff0c\u4e3a\u9ad8\u6548\u7684\u5927\u6a21\u578b\u670d\u52a1\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.21251", "categories": ["cs.SE", "cs.AI", "cs.MA", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.21251", "abs": "https://arxiv.org/abs/2602.21251", "authors": ["Clemens Pohle"], "title": "AgenticTyper: Automated Typing of Legacy Software Projects Using Agentic AI", "comment": "Accepted at ICSE 2026 Student Research Competition (SRC)", "summary": "Legacy JavaScript systems lack type safety, making maintenance risky. While TypeScript can help, manually adding types is expensive. Previous automated typing research focuses on type inference but rarely addresses type checking setup, definition generation, bug identification, or behavioral correctness at repository scale. We present AgenticTyper, a Large Language Model (LLM)-based agentic system that addresses these gaps through iterative error correction and behavior preservation via transpilation comparison. Evaluation on two proprietary repositories (81K LOC) shows that AgenticTyper resolves all 633 initial type errors in 20 minutes, reducing manual effort from one working day.", "AI": {"tldr": "AgenticTyper\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u9519\u8bef\u4fee\u6b63\u548c\u8f6c\u8bd1\u6bd4\u8f83\u6765\u4e3aJavaScript\u4ee3\u7801\u5e93\u81ea\u52a8\u6dfb\u52a0TypeScript\u7c7b\u578b\uff0c\u572820\u5206\u949f\u5185\u89e3\u51b3\u4e8681K\u884c\u4ee3\u7801\u4e2d\u7684633\u4e2a\u7c7b\u578b\u9519\u8bef\u3002", "motivation": "\u4f20\u7edfJavaScript\u7cfb\u7edf\u7f3a\u4e4f\u7c7b\u578b\u5b89\u5168\uff0c\u7ef4\u62a4\u98ce\u9669\u9ad8\u3002\u867d\u7136TypeScript\u53ef\u4ee5\u63d0\u4f9b\u5e2e\u52a9\uff0c\u4f46\u624b\u52a8\u6dfb\u52a0\u7c7b\u578b\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u81ea\u52a8\u5316\u7c7b\u578b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7c7b\u578b\u63a8\u65ad\uff0c\u5f88\u5c11\u89e3\u51b3\u7c7b\u578b\u68c0\u67e5\u8bbe\u7f6e\u3001\u5b9a\u4e49\u751f\u6210\u3001\u9519\u8bef\u8bc6\u522b\u6216\u4ed3\u5e93\u89c4\u6a21\u7684\u884c\u4e3a\u6b63\u786e\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fed\u4ee3\u9519\u8bef\u4fee\u6b63\u548c\u884c\u4e3a\u4fdd\u6301\uff08\u901a\u8fc7\u8f6c\u8bd1\u6bd4\u8f83\uff09\u6765\u586b\u8865\u4e0a\u8ff0\u7814\u7a76\u7a7a\u767d\u3002", "result": "\u5728\u4e24\u4e2a\u4e13\u6709\u4ee3\u7801\u5e93\uff0881K\u884c\u4ee3\u7801\uff09\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAgenticTyper\u572820\u5206\u949f\u5185\u89e3\u51b3\u4e86\u6240\u6709633\u4e2a\u521d\u59cb\u7c7b\u578b\u9519\u8bef\uff0c\u5c06\u624b\u52a8\u5de5\u4f5c\u91cf\u4ece\u4e00\u6574\u5929\u51cf\u5c11\u5230\u51e0\u4e4e\u4e3a\u96f6\u3002", "conclusion": "AgenticTyper\u6709\u6548\u5730\u89e3\u51b3\u4e86JavaScript\u5230TypeScript\u8fc1\u79fb\u4e2d\u7684\u81ea\u52a8\u5316\u7c7b\u578b\u6dfb\u52a0\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\u3002"}}
{"id": "2602.21730", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21730", "abs": "https://arxiv.org/abs/2602.21730", "authors": ["Paul Borrill"], "title": "Lamport's Arrow of Time: The Category Mistake in Logical Clocks", "comment": "14 pages, 32 references", "summary": "Lamport's 1978 paper introduced the happens-before relation and logical clocks, freeing distributed systems from dependence on synchronized physical clocks. This is widely understood as a move away from Newtonian absolute time. We argue that Lamport's formalism retains a deeper and largely unexamined assumption: that causality induces a globally well-defined directed acyclic graph (DAG) over events -- a forward-in-time-only (FITO) structure that functions as an arrow of time embedded at the semantic level. Following Ryle's analysis of category mistakes, we show that this assumption conflates an epistemic construct (the logical ordering of messages) with an ontic claim (that physical causality is globally acyclic and monotonic). We trace this conflation through Shannon's channel model, TLA+, Bell's theorem, and the impossibility results of Fischer-Lynch-Paterson and Brewer's CAP theorem. We then show that special and general relativity permit only local causal structure, and that recent work on indefinite causal order demonstrates that nature admits correlations with no well-defined causal ordering. We propose that mutual information conservation, rather than temporal precedence, provides a more fundamental primitive for distributed consistency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6279\u5224Lamport\u7684happens-before\u5173\u7cfb\u4fdd\u7559\u4e86\u672a\u68c0\u9a8c\u7684\u5047\u8bbe\uff1a\u56e0\u679c\u6027\u8bf1\u5bfc\u5168\u5c40\u6709\u5411\u65e0\u73af\u56fe\uff0c\u5c06\u8ba4\u8bc6\u8bba\u6784\u9020\u4e0e\u672c\u4f53\u8bba\u4e3b\u5f20\u6df7\u6dc6\u3002\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u4e92\u4fe1\u606f\u5b88\u6052\u800c\u975e\u65f6\u95f4\u4f18\u5148\u7684\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u539f\u8bed\u3002", "motivation": "\u63ed\u793aLamport\u7684happens-before\u5173\u7cfb\u4e2d\u9690\u542b\u7684\u6df1\u5c42\u5047\u8bbe\uff1a\u56e0\u679c\u6027\u5f62\u6210\u5168\u5c40\u6709\u5411\u65e0\u73af\u56fe\uff0c\u8fd9\u79cd\"\u4ec5\u5411\u524d\u65f6\u95f4\"\u7ed3\u6784\u5c06\u903b\u8f91\u6d88\u606f\u6392\u5e8f\u7684\u8ba4\u8bc6\u8bba\u6784\u9020\u4e0e\u7269\u7406\u56e0\u679c\u6027\u7684\u672c\u4f53\u8bba\u4e3b\u5f20\u6df7\u6dc6\u3002", "method": "\u8fd0\u7528Ryle\u7684\u8303\u7574\u9519\u8bef\u5206\u6790\uff0c\u8ffd\u6eaf\u8fd9\u4e00\u6df7\u6dc6\u5728\u9999\u519c\u4fe1\u9053\u6a21\u578b\u3001TLA+\u3001\u8d1d\u5c14\u5b9a\u7406\u3001FLP\u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u548cCAP\u5b9a\u7406\u4e2d\u7684\u4f53\u73b0\uff0c\u7ed3\u5408\u76f8\u5bf9\u8bba\u548c\u4e0d\u786e\u5b9a\u56e0\u679c\u987a\u5e8f\u7814\u7a76\uff0c\u8bba\u8bc1\u81ea\u7136\u53ea\u5141\u8bb8\u5c40\u90e8\u56e0\u679c\u7ed3\u6784\u3002", "result": "\u8bba\u8bc1Lamport\u7684\u5f62\u5f0f\u4e3b\u4e49\u4fdd\u7559\u4e86\u672a\u68c0\u9a8c\u7684\u5047\u8bbe\uff0c\u81ea\u7136\u53ea\u5141\u8bb8\u5c40\u90e8\u56e0\u679c\u7ed3\u6784\uff0c\u4e0d\u786e\u5b9a\u56e0\u679c\u987a\u5e8f\u7814\u7a76\u8868\u660e\u81ea\u7136\u5141\u8bb8\u6ca1\u6709\u660e\u786e\u5b9a\u4e49\u56e0\u679c\u987a\u5e8f\u7684\u5173\u8054\uff0c\u9700\u8981\u65b0\u7684\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u57fa\u7840\u3002", "conclusion": "\u63d0\u51fa\u4e92\u4fe1\u606f\u5b88\u6052\u800c\u975e\u65f6\u95f4\u4f18\u5148\u5e94\u4f5c\u4e3a\u5206\u5e03\u5f0f\u4e00\u81f4\u6027\u7684\u66f4\u57fa\u672c\u539f\u8bed\uff0c\u9700\u8981\u8d85\u8d8a\u5168\u5c40\u6709\u5411\u65e0\u73af\u56fe\u5047\u8bbe\u7684\u65b0\u5206\u5e03\u5f0f\u7cfb\u7edf\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.21568", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21568", "abs": "https://arxiv.org/abs/2602.21568", "authors": ["Yuvraj Agrawal", "Pallav Jain"], "title": "From Ad-Hoc Scripts to Orchestrated Pipelines: Architecting a Resilient ELT Framework for Developer Productivity Metrics", "comment": null, "summary": "Developer Productivity Dashboards are essential for visualizing DevOps performance metrics such as Deployment Frequency and Change Failure Rate (DORA). However, the utility of these dashboards is frequently undermined by data reliability issues. In early iterations of our platform, ad-hoc ingestion scripts (Cron jobs) led to \"silent failures,\" where data gaps went undetected for days, eroding organizational trust. This paper reports on our experience migrating from legacy scheduling to a robust Extract-Load-Transform (ELT) pipeline using Directed Acyclic Graph (DAG) orchestration and Medallion Architecture. We detail the operational benefits of decoupling data extraction from transformation, the necessity of immutable raw history for metric redefinition, and the implementation of state-based dependency management. Our experience suggests that treating the metrics pipeline as a production-grade distributed system is a prerequisite for sustainable engineering analytics.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4ece\u4e34\u65f6Cron\u4f5c\u4e1a\u8fc1\u79fb\u5230\u57fa\u4e8eDAG\u7f16\u6392\u548cMedallion\u67b6\u6784\u7684ELT\u7ba1\u9053\uff0c\u4ee5\u89e3\u51b3\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u4eea\u8868\u677f\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898", "motivation": "\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u4eea\u8868\u677f\u5bf9\u4e8e\u53ef\u89c6\u5316DevOps\u6027\u80fd\u6307\u6807\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898\u7ecf\u5e38\u524a\u5f31\u5176\u6548\u7528\u3002\u65e9\u671f\u5e73\u53f0\u4e2d\u7684\u4e34\u65f6\u6444\u53d6\u811a\u672c\uff08Cron\u4f5c\u4e1a\uff09\u5bfc\u81f4\"\u9759\u9ed8\u6545\u969c\"\uff0c\u6570\u636e\u7f3a\u53e3\u6570\u65e5\u672a\u88ab\u53d1\u73b0\uff0c\u4fb5\u8680\u4e86\u7ec4\u7ec7\u4fe1\u4efb\u3002", "method": "\u4ece\u4f20\u7edf\u8c03\u5ea6\u8fc1\u79fb\u5230\u4f7f\u7528\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7f16\u6392\u548cMedallion\u67b6\u6784\u7684\u7a33\u5065\u63d0\u53d6-\u52a0\u8f7d-\u8f6c\u6362\uff08ELT\uff09\u7ba1\u9053\u3002\u5177\u4f53\u5305\u62ec\uff1a\u89e3\u8026\u6570\u636e\u63d0\u53d6\u4e0e\u8f6c\u6362\u3001\u5b9e\u73b0\u4e0d\u53ef\u53d8\u539f\u59cb\u5386\u53f2\u8bb0\u5f55\u7528\u4e8e\u6307\u6807\u91cd\u5b9a\u4e49\u3001\u5b9e\u65bd\u57fa\u4e8e\u72b6\u6001\u7684\u4f9d\u8d56\u7ba1\u7406\u3002", "result": "\u901a\u8fc7\u5c06\u6307\u6807\u7ba1\u9053\u89c6\u4e3a\u751f\u4ea7\u7ea7\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u53ef\u6301\u7eed\u7684\u5de5\u7a0b\u5206\u6790\u3002ELT\u7ba1\u9053\u63d0\u4f9b\u4e86\u64cd\u4f5c\u4f18\u52bf\uff0c\u5305\u62ec\u66f4\u597d\u7684\u6570\u636e\u53ef\u9760\u6027\u3001\u6545\u969c\u68c0\u6d4b\u548c\u6307\u6807\u91cd\u5b9a\u4e49\u80fd\u529b\u3002", "conclusion": "\u5c06\u6307\u6807\u7ba1\u9053\u89c6\u4e3a\u751f\u4ea7\u7ea7\u5206\u5e03\u5f0f\u7cfb\u7edf\u662f\u53ef\u6301\u7eed\u5de5\u7a0b\u5206\u6790\u7684\u5148\u51b3\u6761\u4ef6\u3002\u4ece\u4f20\u7edf\u8c03\u5ea6\u8fc1\u79fb\u5230\u57fa\u4e8eDAG\u7f16\u6392\u7684ELT\u7ba1\u9053\u89e3\u51b3\u4e86\u6570\u636e\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5e76\u652f\u6301\u6307\u6807\u91cd\u5b9a\u4e49\u548c\u66f4\u597d\u7684\u4f9d\u8d56\u7ba1\u7406\u3002"}}
{"id": "2602.21213", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21213", "abs": "https://arxiv.org/abs/2602.21213", "authors": ["Bilge Senturk", "Faruk Alpay"], "title": "Topological Relational Theory: A Simplicial-Complex View of Functional Dependencies, Lossless Decomposition, and Acyclicity", "comment": "8 pages, 2 figures", "summary": "We develop a topological lens on relational schema design by encoding functional dependencies (FDs) as simplices of an abstract simplicial complex. This dependency complex exposes multi-attribute interactions and enables homological invariants (Betti numbers) to diagnose cyclic dependency structure. We define Simplicial Normal Form (SNF) as homological acyclicity of the dependency complex in positive dimensions, i.e., vanishing reduced homology for all $n \\ge 1$. SNF is intentionally weaker than contractibility and does not identify homology with homotopy. For decompositions, we give a topological reformulation of the classical binary lossless-join criterion: assuming dependency preservation, a decomposition is lossless exactly when the intersection attributes form a key for at least one component. Topologically, this yields a strong deformation retraction that trivializes the relevant Mayer--Vietoris boundary map. For multiway decompositions, we show how the nerve of a cover by induced subcomplexes provides a computable certificate: a 1-cycle in the nerve (detected by $H_1$) obstructs join-tree structure and aligns with cyclic join behavior in acyclic-scheme theory. Finally, we discuss an algorithmic consequence: Betti numbers of the dependency complex (or of a decomposition nerve) can be computed from boundary matrices and used as a lightweight schema diagnostic to localize \"unexplained\" dependency cycles, complementing standard FD-chase tests.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u5b66\u7684\u5173\u7cfb\u6a21\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5c06\u51fd\u6570\u4f9d\u8d56\u7f16\u7801\u4e3a\u62bd\u8c61\u5355\u7eaf\u590d\u5f62\u7684\u5355\u7eaf\u5f62\uff0c\u901a\u8fc7\u540c\u8c03\u4e0d\u53d8\u91cf\u8bca\u65ad\u5faa\u73af\u4f9d\u8d56\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u7684\u5173\u7cfb\u6a21\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u591a\u5c5e\u6027\u4ea4\u4e92\u548c\u5faa\u73af\u4f9d\u8d56\u7ed3\u6784\u7684\u7cfb\u7edf\u6027\u5206\u6790\u5de5\u5177\uff0c\u9700\u8981\u65b0\u7684\u6570\u5b66\u6846\u67b6\u6765\u8bca\u65ad\u548c\u4f18\u5316\u6a21\u5f0f\u8bbe\u8ba1\u3002", "method": "\u5c06\u51fd\u6570\u4f9d\u8d56\u7f16\u7801\u4e3a\u62bd\u8c61\u5355\u7eaf\u590d\u5f62\uff08\u4f9d\u8d56\u590d\u5f62\uff09\uff0c\u5229\u7528\u540c\u8c03\u7406\u8bba\uff08Betti\u6570\uff09\u5206\u6790\u4f9d\u8d56\u7ed3\u6784\uff1b\u5b9a\u4e49\u5355\u7eaf\u6b63\u89c4\u5f62\u5f0f\uff08SNF\uff09\u4f5c\u4e3a\u540c\u8c03\u65e0\u73af\u6027\u6761\u4ef6\uff1b\u5c06\u7ecf\u5178\u7684\u65e0\u635f\u8fde\u63a5\u51c6\u5219\u91cd\u65b0\u8868\u8ff0\u4e3a\u62d3\u6251\u53d8\u5f62\u6536\u7f29\u95ee\u9898\u3002", "result": "\u5efa\u7acb\u4e86\u4f9d\u8d56\u590d\u5f62\u7684\u540c\u8c03\u6027\u8d28\u4e0e\u6a21\u5f0f\u8bbe\u8ba1\u8d28\u91cf\u7684\u5173\u7cfb\uff1b\u8bc1\u660e\u4e86\u65e0\u635f\u8fde\u63a5\u51c6\u5219\u7684\u62d3\u6251\u7b49\u4ef7\u5f62\u5f0f\uff1b\u5c55\u793a\u4e86\u795e\u7ecf\u590d\u5f62\u4e2d\u76841-\u5faa\u73af\u5982\u4f55\u963b\u788d\u8fde\u63a5\u6811\u7ed3\u6784\uff1b\u63d0\u51fa\u4e86\u57fa\u4e8eBetti\u6570\u7684\u8f7b\u91cf\u7ea7\u6a21\u5f0f\u8bca\u65ad\u7b97\u6cd5\u3002", "conclusion": "\u62d3\u6251\u65b9\u6cd5\u4e3a\u5173\u7cfb\u6a21\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u8bca\u65ad\u5faa\u73af\u4f9d\u8d56\u7ed3\u6784\u5e76\u8865\u5145\u4f20\u7edf\u7684FD-chase\u6d4b\u8bd5\uff0c\u5177\u6709\u5b9e\u9645\u7684\u8ba1\u7b97\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.21788", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21788", "abs": "https://arxiv.org/abs/2602.21788", "authors": ["Yifan Niu", "Han Xiao", "Dongyi Liu", "Wei Zhou", "Jia Li"], "title": "DHP: Efficient Scaling of MLLM Training with Dynamic Hybrid Parallelism", "comment": null, "summary": "Scaling long-context capabilities is crucial for Multimodal Large Language Models (MLLMs). However, real-world multimodal datasets are extremely heterogeneous. Existing training frameworks predominantly rely on static parallelism strategies, which suffer from severe load imbalance, redundant communication, and suboptimal hardware utilization under data heterogeneity. In this work, we propose Dynamic Hybrid Parallelism (DHP), an efficient parallelism strategy that adaptively reconfigures communication groups and parallelism degrees during MLLM training. We generalize the non-power-of-two parallelism degrees and develop a polynomial-time algorithm to generate near-optimal parallelism strategies with only millisecond-level overhead per training batch. DHP is able to maintain high hardware efficiency even under extreme data variability. Experimental results demonstrate that DHP significantly outperforms Megatron-LM and DeepSpeed, achieving up to 1.36 $\\times$ speedup in training throughput while maintaining near-linear scaling efficiency across large-scale NPU clusters.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6df7\u5408\u5e76\u884c\uff08DHP\uff09\u7b56\u7565\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf1.36\u500d", "motivation": "\u73b0\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u6570\u636e\u96c6\u9ad8\u5ea6\u5f02\u6784\uff0c\u73b0\u6709\u9759\u6001\u5e76\u884c\u7b56\u7565\u5728\u6570\u636e\u5f02\u6784\u60c5\u51b5\u4e0b\u5b58\u5728\u4e25\u91cd\u8d1f\u8f7d\u4e0d\u5747\u8861\u3001\u5197\u4f59\u901a\u4fe1\u548c\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u4e0b\u95ee\u9898", "method": "\u63d0\u51fa\u52a8\u6001\u6df7\u5408\u5e76\u884c\uff08DHP\uff09\u7b56\u7565\uff0c\u81ea\u9002\u5e94\u5730\u91cd\u6784\u901a\u4fe1\u7ec4\u548c\u5e76\u884c\u5ea6\uff1b\u63a8\u5e7f\u975e2\u7684\u5e42\u6b21\u5e76\u884c\u5ea6\uff0c\u5f00\u53d1\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u751f\u6210\u8fd1\u4f3c\u6700\u4f18\u5e76\u884c\u7b56\u7565\uff0c\u6bcf\u6279\u6b21\u8bad\u7ec3\u4ec5\u6beb\u79d2\u7ea7\u5f00\u9500", "result": "DHP\u663e\u8457\u4f18\u4e8eMegatron-LM\u548cDeepSpeed\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u5347\u6700\u9ad8\u8fbe1.36\u500d\uff0c\u5728\u5927\u89c4\u6a21NPU\u96c6\u7fa4\u4e0a\u4fdd\u6301\u63a5\u8fd1\u7ebf\u6027\u7684\u6269\u5c55\u6548\u7387", "conclusion": "DHP\u80fd\u6709\u6548\u5e94\u5bf9\u6781\u7aef\u6570\u636e\u53d8\u5f02\u6027\uff0c\u4fdd\u6301\u9ad8\u786c\u4ef6\u6548\u7387\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\u63d0\u4f9b\u9ad8\u6548\u5e76\u884c\u8bad\u7ec3\u65b9\u6848"}}
{"id": "2602.21611", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21611", "abs": "https://arxiv.org/abs/2602.21611", "authors": ["Kangning Shen", "Jingyuan Zhang", "Chenxi Sun", "Wencong Zeng", "Yang Yue"], "title": "Structurally Aligned Subtask-Level Memory for Software Engineering Agents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated significant potential as autonomous software engineering (SWE) agents. Recent work has further explored augmenting these agents with memory mechanisms to support long-horizon reasoning. However, these approaches typically operate at a coarse instance granularity, treating the entire problem-solving episode as the atomic unit of storage and retrieval. We empirically demonstrate that instance-level memory suffers from a fundamental granularity mismatch, resulting in misguided retrieval when tasks with similar surface descriptions require distinct reasoning logic at specific stages. To address this, we propose Structurally Aligned Subtask-Level Memory, a method that aligns memory storage, retrieval, and updating with the agent's functional decomposition. Extensive experiments on SWE-bench Verified demonstrate that our method consistently outperforms both vanilla agents and strong instance-level memory baselines across diverse backbones, improving mean Pass@1 over the vanilla agent by +4.7 pp on average (e.g., +6.8 pp on Gemini 2.5 Pro). Performance gains grow with more interaction steps, showing that leveraging past experience benefits long-horizon reasoning in complex software engineering tasks.", "AI": {"tldr": "\u63d0\u51faStructurally Aligned Subtask-Level Memory\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b50\u4efb\u52a1\u7ea7\u8bb0\u5fc6\u5bf9\u9f50\uff0c\u89e3\u51b3LLM\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u4e2d\u5b9e\u4f8b\u7ea7\u8bb0\u5fc6\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728SWE-bench Verified\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u8bb0\u5fc6\u673a\u5236\u901a\u5e38\u91c7\u7528\u7c97\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u5b58\u50a8\u548c\u68c0\u7d22\uff0c\u5c06\u6574\u4e2a\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\u4f5c\u4e3a\u539f\u5b50\u5355\u4f4d\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u7c92\u5ea6\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u8bef\u5bfc\u6027\u68c0\u7d22\uff0c\u7279\u522b\u662f\u5f53\u8868\u9762\u63cf\u8ff0\u76f8\u4f3c\u7684\u4efb\u52a1\u5728\u7279\u5b9a\u9636\u6bb5\u9700\u8981\u4e0d\u540c\u7684\u63a8\u7406\u903b\u8f91\u65f6\u3002", "method": "\u63d0\u51faStructurally Aligned Subtask-Level Memory\u65b9\u6cd5\uff0c\u5c06\u8bb0\u5fc6\u7684\u5b58\u50a8\u3001\u68c0\u7d22\u548c\u66f4\u65b0\u4e0e\u4ee3\u7406\u7684\u529f\u80fd\u5206\u89e3\u5bf9\u9f50\u3002\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5b50\u4efb\u52a1\u7ea7\u8bb0\u5fc6\u673a\u5236\uff0c\u66f4\u597d\u5730\u652f\u6301\u957f\u65f6\u7a0b\u63a8\u7406\u3002", "result": "\u5728SWE-bench Verified\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u666e\u901a\u4ee3\u7406\u548c\u5f3a\u5927\u7684\u5b9e\u4f8b\u7ea7\u8bb0\u5fc6\u57fa\u7ebf\uff0c\u5e73\u5747\u5c06Pass@1\u63d0\u9ad8\u4e864.7\u4e2a\u767e\u5206\u70b9\uff08\u5982Gemini 2.5 Pro\u4e0a\u63d0\u9ad86.8\u4e2a\u767e\u5206\u70b9\uff09\u3002\u6027\u80fd\u589e\u76ca\u968f\u7740\u4ea4\u4e92\u6b65\u9aa4\u7684\u589e\u52a0\u800c\u589e\u957f\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7684\u5b50\u4efb\u52a1\u7ea7\u8bb0\u5fc6\u5bf9\u9f50\u80fd\u6709\u6548\u89e3\u51b3\u5b9e\u4f8b\u7ea7\u8bb0\u5fc6\u7684\u7c92\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5229\u7528\u8fc7\u53bb\u7684\u7ecf\u9a8c\u6709\u76ca\u4e8e\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u957f\u65f6\u7a0b\u63a8\u7406\uff0c\u4e3aLLM\u8f6f\u4ef6\u5de5\u7a0b\u4ee3\u7406\u7684\u8bb0\u5fc6\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8bbe\u8ba1\u65b9\u5411\u3002"}}
{"id": "2602.21237", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21237", "abs": "https://arxiv.org/abs/2602.21237", "authors": ["Il-Sun Chang"], "title": "Premature Dimensional Collapse and Tensor-based Execution Paths for High-Dimensional Relational Operations in Cost-Based Database Systems", "comment": "24 pages, 7 figures", "summary": "Modern cost-based DBMSs frequently exhibit execution instability and tail-latency amplification when high-dimensional relational operations trigger memory-regime transitions such as hash-table spilling and external materialization. We identify a structural failure mode in which intermediate representations are prematurely linearized under memory pressure, causing disproportionate I/O amplification and phase-transition-like latency behavior. To mitigate this, we propose a tensor-based execution path that delays premature linearization and preserves higher-dimensional locality through late materialization and structured intermediate layouts. Using a modified PostgreSQL-based prototype and controlled microbenchmarks, we show that under constrained memory settings (e.g., work_mem=1MB) conventional execution can spill hundreds of megabytes and exceed multi-second P99 latency, while the proposed path maintains stable execution and reduces P99 latency to sub-second levels. Our results suggest that representation timing is a first-class design variable for execution stability, complementing traditional optimization efforts focused on cardinality estimation and operator throughput.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5f20\u91cf\u6267\u884c\u8def\u5f84\u5ef6\u8fdf\u4e2d\u95f4\u8868\u793a\u7684\u7ebf\u6027\u5316\uff0c\u89e3\u51b3\u5185\u5b58\u53d7\u9650\u65f6\u6570\u636e\u5e93\u6267\u884c\u4e0d\u7a33\u5b9a\u548c\u5c3e\u90e8\u5ef6\u8fdf\u95ee\u9898", "motivation": "\u73b0\u4ee3\u57fa\u4e8e\u6210\u672c\u7684DBMS\u5728\u9ad8\u7ef4\u5173\u7cfb\u64cd\u4f5c\u89e6\u53d1\u5185\u5b58\u673a\u5236\u8f6c\u6362\uff08\u5982\u54c8\u5e0c\u8868\u6ea2\u51fa\u548c\u5916\u90e8\u7269\u5316\uff09\u65f6\uff0c\u7ecf\u5e38\u51fa\u73b0\u6267\u884c\u4e0d\u7a33\u5b9a\u548c\u5c3e\u90e8\u5ef6\u8fdf\u653e\u5927\u95ee\u9898\u3002\u4f5c\u8005\u53d1\u73b0\u7ed3\u6784\u6027\u95ee\u9898\u5728\u4e8e\u4e2d\u95f4\u8868\u793a\u5728\u5185\u5b58\u538b\u529b\u4e0b\u8fc7\u65e9\u7ebf\u6027\u5316\uff0c\u5bfc\u81f4\u4e0d\u6210\u6bd4\u4f8b\u7684I/O\u653e\u5927\u548c\u7c7b\u4f3c\u76f8\u53d8\u7684\u5ef6\u8fdf\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5f20\u91cf\u7684\u6267\u884c\u8def\u5f84\uff0c\u901a\u8fc7\u5ef6\u8fdf\u8fc7\u65e9\u7ebf\u6027\u5316\u548c\u7ed3\u6784\u5316\u4e2d\u95f4\u5e03\u5c40\u6765\u4fdd\u6301\u9ad8\u7ef4\u5c40\u90e8\u6027\u3002\u4f7f\u7528\u4fee\u6539\u7684PostgreSQL\u539f\u578b\u548c\u53d7\u63a7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u53d7\u9650\u5185\u5b58\u8bbe\u7f6e\u4e0b\uff08\u5982work_mem=1MB\uff09\uff0c\u4f20\u7edf\u6267\u884c\u53ef\u80fd\u6ea2\u51fa\u6570\u767e\u5146\u5b57\u8282\u5e76\u8d85\u8fc7\u6570\u79d2\u7684P99\u5ef6\u8fdf\uff0c\u800c\u63d0\u51fa\u7684\u8def\u5f84\u4fdd\u6301\u7a33\u5b9a\u6267\u884c\u5e76\u5c06P99\u5ef6\u8fdf\u964d\u4f4e\u5230\u4e9a\u79d2\u7ea7\u3002", "conclusion": "\u8868\u793a\u65f6\u673a\u662f\u6267\u884c\u7a33\u5b9a\u6027\u7684\u91cd\u8981\u8bbe\u8ba1\u53d8\u91cf\uff0c\u8865\u5145\u4e86\u4f20\u7edf\u4e13\u6ce8\u4e8e\u57fa\u6570\u4f30\u8ba1\u548c\u7b97\u5b50\u541e\u5410\u91cf\u7684\u4f18\u5316\u5de5\u4f5c\u3002"}}
{"id": "2602.21897", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21897", "abs": "https://arxiv.org/abs/2602.21897", "authors": ["Aleix Bon\u00e9", "Alejandro Aguirre", "David \u00c1lvarez", "Pedro J. Martinez-Ferrer", "Vicen\u00e7 Beltran"], "title": "A task-based data-flow methodology for programming heterogeneous systems with multiple accelerator APIs", "comment": "13 pages, 8 figures", "summary": "Heterogeneous nodes that combine multi-core CPUs with diverse accelerators are rapidly becoming the norm in both high-performance computing (HPC) and AI infrastructures. Exploiting these platforms, however, requires orchestrating several low-level accelerator APIs such as CUDA, SYCL, and Triton. In some occasions they can be combined with optimized vendor math libraries: e.g., cuBLAS and oneAPI. Each API or library introduces its own abstractions, execution semantics, and synchronization mechanisms. Combining them within a single application is therefore error-prone and labor-intensive. We propose reusing a task-based data-flow methodology together with Task-Aware APIs (TA-libs) to overcome these limitations and facilitate the seamless integration of multiple accelerator programming models, while still leveraging the best-in-class kernels offered by each API.\n  Applications are expressed as a directed acyclic graph (DAG) of host tasks and device kernels managed by an OpenMP/OmpSs-2 runtime. We introduce Task-Aware SYCL (TASYCL) and leverage Task-Aware CUDA (TACUDA), which elevate individual accelerator invocations to first-class tasks. When multiple native runtimes coexist on the same multi-core CPU, they contend for threads, leading to oversubscription and performance variability. To address this, we unify their thread management under the nOS-V tasking and threading library, to which we contribute a new port of the PoCL (Portable OpenCL) runtime.\n  These results demonstrate that task-aware libraries, coupled with the nOS-V library, enable a single application to harness multiple accelerator programming models transparently and efficiently. The proposed methodology is immediately applicable to current heterogeneous nodes and is readily extensible to future systems that integrate even richer combinations of CPUs, GPUs, FPGAs, and AI accelerators.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4efb\u52a1\u7684\u6570\u636e\u6d41\u65b9\u6cd5\u548c\u4efb\u52a1\u611f\u77e5API\uff08TA-libs\uff09\uff0c\u7ed3\u5408nOS-V\u7ebf\u7a0b\u5e93\uff0c\u5b9e\u73b0\u591a\u52a0\u901f\u5668\u7f16\u7a0b\u6a21\u578b\u7684\u65e0\u7f1d\u96c6\u6210\u4e0e\u9ad8\u6548\u534f\u540c", "motivation": "\u5f02\u6784\u8282\u70b9\uff08\u591a\u6838CPU+\u591a\u79cd\u52a0\u901f\u5668\uff09\u6210\u4e3aHPC\u548cAI\u57fa\u7840\u8bbe\u65bd\u5e38\u6001\uff0c\u4f46\u4e0d\u540c\u52a0\u901f\u5668API\uff08CUDA\u3001SYCL\u3001Triton\u7b49\uff09\u5404\u6709\u62bd\u8c61\u3001\u6267\u884c\u8bed\u4e49\u548c\u540c\u6b65\u673a\u5236\uff0c\u7ec4\u5408\u4f7f\u7528\u6613\u9519\u4e14\u8d39\u529b", "method": "1. \u91c7\u7528\u57fa\u4e8e\u4efb\u52a1\u7684\u6570\u636e\u6d41\u65b9\u6cd5\uff0c\u5c06\u5e94\u7528\u8868\u793a\u4e3aDAG\u56fe\uff1b2. \u5f15\u5165\u4efb\u52a1\u611f\u77e5API\uff08TASYCL\u3001TACUDA\uff09\u5c06\u52a0\u901f\u5668\u8c03\u7528\u63d0\u5347\u4e3a\u4e00\u7b49\u4efb\u52a1\uff1b3. \u4f7f\u7528nOS-V\u7edf\u4e00\u7ebf\u7a0b\u7ba1\u7406\uff0c\u907f\u514d\u591a\u8fd0\u884c\u65f6\u7ebf\u7a0b\u4e89\u7528", "result": "\u4efb\u52a1\u611f\u77e5\u5e93\u4e0enOS-V\u5e93\u4f7f\u5355\u4e2a\u5e94\u7528\u80fd\u900f\u660e\u9ad8\u6548\u5730\u5229\u7528\u591a\u79cd\u52a0\u901f\u5668\u7f16\u7a0b\u6a21\u578b\uff0c\u65b9\u6cd5\u9002\u7528\u4e8e\u5f53\u524d\u5f02\u6784\u8282\u70b9\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u672a\u6765\u66f4\u590d\u6742\u7684\u5f02\u6784\u7cfb\u7edf", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u52a0\u901f\u5668\u7f16\u7a0b\u6a21\u578b\u96c6\u6210\u96be\u9898\uff0c\u4e3a\u5f02\u6784\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7edf\u4e00\u9ad8\u6548\u7684\u7f16\u7a0b\u6846\u67b6\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2602.21641", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21641", "abs": "https://arxiv.org/abs/2602.21641", "authors": ["Man Zhang", "Yunyang Li", "Tao Yue"], "title": "Uncertainty Modeling for SysML v2", "comment": null, "summary": "Uncertainty is inherent in modern engineered systems, including cyber-physical systems, autonomous systems, and large-scale software-intensive infrastructures (such as microservice-based systems) operating in dynamic and partially observable environments. The recent publication of Precise Semantics for Uncertainty Modeling (PSUM) by the Object Management Group represents the first standardized specification for uncertainty modeling within the Model-Based Systems Engineering (MBSE) community, providing formally defined semantics for representing and reasoning about uncertainty in models. In parallel, the second version of Systems Modeling Language (SysML v2) was released as the next-generation systems modeling language, offering improved semantic rigor and reusability, yet lacking native constructs aligned with PSUM for first-class uncertainty representation. This paper proposes a systematic extension of SysML v2 that incorporates the PSUM metamodel into its modeling framework. The extension enables explicit specification of indeterminacy sources, structured characterization of uncertainties, and consistent propagation of uncertainty within system models, while preserving conformance with SysML v2 syntax and semantics. We validate the approach through seven case studies. Results demonstrate that the proposed extension (PSUM-SysMLv2) is expressive and applicable for uncertainty-aware MBSE, and potentially enables uncertainty and uncertainty propagation analyses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06PSUM\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u6807\u51c6\u96c6\u6210\u5230SysML v2\u4e2d\u7684\u7cfb\u7edf\u6269\u5c55\uff0c\u4f7f\u7cfb\u7edf\u5efa\u6a21\u80fd\u591f\u663e\u5f0f\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u5e76\u8fdb\u884c\u4f20\u64ad\u5206\u6790\u3002", "motivation": "\u73b0\u4ee3\u5de5\u7a0b\u7cfb\u7edf\uff08\u5982\u4fe1\u606f\u7269\u7406\u7cfb\u7edf\u3001\u81ea\u4e3b\u7cfb\u7edf\u3001\u5fae\u670d\u52a1\u7cfb\u7edf\uff09\u5728\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u5b58\u5728\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\u3002\u867d\u7136OMG\u53d1\u5e03\u4e86PSUM\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u6807\u51c6\uff0c\u4f46\u65b0\u4e00\u4ee3\u7cfb\u7edf\u5efa\u6a21\u8bed\u8a00SysML v2\u7f3a\u4e4f\u4e0ePSUM\u5bf9\u9f50\u7684\u539f\u751f\u6784\u9020\u6765\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u3002", "method": "\u63d0\u51faSysML v2\u7684\u7cfb\u7edf\u6269\u5c55\uff0c\u5c06PSUM\u5143\u6a21\u578b\u96c6\u6210\u5230\u5176\u5efa\u6a21\u6846\u67b6\u4e2d\u3002\u8be5\u6269\u5c55\u80fd\u591f\u663e\u5f0f\u6307\u5b9a\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u3001\u7ed3\u6784\u5316\u8868\u5f81\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u7cfb\u7edf\u6a21\u578b\u4e2d\u4fdd\u6301\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eSysML v2\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u7b26\u5408\u6027\u3002", "result": "\u901a\u8fc7\u4e03\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u6269\u5c55\uff08PSUM-SysMLv2\uff09\u5177\u6709\u8868\u8fbe\u529b\uff0c\u9002\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684MBSE\uff0c\u5e76\u53ef\u80fd\u652f\u6301\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5c06PSUM\u6807\u51c6\u96c6\u6210\u5230SysML v2\u4e2d\uff0c\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u7cfb\u7edf\u5de5\u7a0b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u5f62\u5f0f\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u80fd\u529b\uff0c\u586b\u8865\u4e86SysML v2\u5728\u4e0d\u786e\u5b9a\u6027\u8868\u793a\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2602.21247", "categories": ["cs.DB", "cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21247", "abs": "https://arxiv.org/abs/2602.21247", "authors": ["Tobias Rubel", "Richard Wen", "Laxman Dhulipala", "Lars Gottesb\u00fcren", "Rajesh Jayaram", "Jakub \u0141\u0105cki"], "title": "PiPNN: Ultra-Scalable Graph-Based Nearest Neighbor Indexing", "comment": null, "summary": "The fastest indexes for Approximate Nearest Neighbor Search today are also the slowest to build: graph-based methods like HNSW and Vamana achieve state-of-the-art query performance but have large construction times due to relying on random-access-heavy beam searches. We introduce PiPNN (Pick-in-Partitions Nearest Neighbors), an ultra-scalable graph construction algorithm that avoids this ``search bottleneck'' that existing graph-based methods suffer from.\n  PiPNN's core innovation is HashPrune, a novel online pruning algorithm which dynamically maintains sparse collections of edges. HashPrune enables PiPNN to partition the dataset into overlapping sub-problems, efficiently perform bulk distance comparisons via dense matrix multiplication kernels, and stream a subset of the edges into HashPrune. HashPrune guarantees bounded memory during index construction which permits PiPNN to build higher quality indices without the use of extra intermediate memory.\n  PiPNN builds state-of-the-art indexes up to 11.6x faster than Vamana (DiskANN) and up to 12.9x faster than HNSW. PiPNN is significantly more scalable than recent algorithms for fast graph construction. PiPNN builds indexes at least 19.1x faster than MIRAGE and 17.3x than FastKCNA while producing indexes that achieve higher query throughput. PiPNN enables us to build, for the first time, high-quality ANN indexes on billion-scale datasets in under 20 minutes using a single multicore machine.", "AI": {"tldr": "PiPNN\u662f\u4e00\u79cd\u8d85\u53ef\u6269\u5c55\u7684\u56fe\u6784\u5efa\u7b97\u6cd5\uff0c\u901a\u8fc7HashPrune\u5728\u7ebf\u526a\u679d\u6280\u672f\u907f\u514d\u4f20\u7edf\u56fe\u65b9\u6cd5\u7684\u641c\u7d22\u74f6\u9888\uff0c\u6784\u5efa\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb11-13\u500d\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u673a20\u5206\u949f\u5185\u6784\u5efa\u5341\u4ebf\u7ea7\u6570\u636e\u96c6\u7684\u9ad8\u8d28\u91cfANN\u7d22\u5f15\u3002", "motivation": "\u5f53\u524d\u6700\u5feb\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u7d22\u5f15\uff08\u5982HNSW\u548cVamana\uff09\u6784\u5efa\u901f\u5ea6\u6781\u6162\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u968f\u673a\u8bbf\u95ee\u5bc6\u96c6\u7684beam\u641c\u7d22\uff0c\u5b58\u5728\"\u641c\u7d22\u74f6\u9888\"\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u6784\u5efa\u9ad8\u8d28\u91cf\u56fe\u7d22\u5f15\u7684\u7b97\u6cd5\u3002", "method": "PiPNN\u7684\u6838\u5fc3\u521b\u65b0\u662fHashPrune\u5728\u7ebf\u526a\u679d\u7b97\u6cd5\uff0c\u5b83\u52a8\u6001\u7ef4\u62a4\u7a00\u758f\u8fb9\u96c6\u5408\u3002\u901a\u8fc7\u5c06\u6570\u636e\u96c6\u5212\u5206\u4e3a\u91cd\u53e0\u5b50\u95ee\u9898\uff0c\u4f7f\u7528\u5bc6\u96c6\u77e9\u9635\u4e58\u6cd5\u5185\u6838\u9ad8\u6548\u8fdb\u884c\u6279\u91cf\u8ddd\u79bb\u6bd4\u8f83\uff0c\u5e76\u5c06\u8fb9\u5b50\u96c6\u6d41\u5f0f\u8f93\u5165HashPrune\uff0c\u4fdd\u8bc1\u6784\u5efa\u8fc7\u7a0b\u4e2d\u7684\u5185\u5b58\u6709\u754c\u3002", "result": "PiPNN\u6784\u5efa\u901f\u5ea6\u6bd4Vamana\u5feb11.6\u500d\uff0c\u6bd4HNSW\u5feb12.9\u500d\uff0c\u6bd4MIRAGE\u5feb19.1\u500d\uff0c\u6bd4FastKCNA\u5feb17.3\u500d\u3002\u9996\u6b21\u5b9e\u73b0\u5355\u673a20\u5206\u949f\u5185\u6784\u5efa\u5341\u4ebf\u7ea7\u6570\u636e\u96c6\u7684\u9ad8\u8d28\u91cfANN\u7d22\u5f15\uff0c\u4e14\u4ea7\u751f\u7684\u7d22\u5f15\u5177\u6709\u66f4\u9ad8\u7684\u67e5\u8be2\u541e\u5410\u91cf\u3002", "conclusion": "PiPNN\u901a\u8fc7HashPrune\u7b97\u6cd5\u89e3\u51b3\u4e86\u56fe\u6784\u5efa\u4e2d\u7684\u641c\u7d22\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8d85\u53ef\u6269\u5c55\u7684\u56fe\u6784\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6784\u5efa\u901f\u5ea6\u800c\u4e0d\u727a\u7272\u7d22\u5f15\u8d28\u91cf\uff0c\u4e3a\u5927\u89c4\u6a21ANN\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21949", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21949", "abs": "https://arxiv.org/abs/2602.21949", "authors": ["Yahao Ding", "Yinchao Yang", "Jiaxiang Wang", "Zhaohui Yang", "Dusit Niyato", "Zhu Han", "Mohammad Shikh-Bahaei"], "title": "Energy Efficient Federated Learning with Hyperdimensional Computing over Wireless Communication Networks", "comment": "13 pages, 9 figures", "summary": "In this paper, we investigate a problem of minimizing total energy consumption for secure federated learning (FL) over wireless edge networks. To address the high computational cost and privacy challenges in conventional FL with neural networks (NN) for resource-constrained users, we propose a novel FL with hyperdimensional computing and differential privacy (FL-HDC-DP) framework. In the considered model, each edge user employs hyperdimensional computing (HDC) for local training, which replaces complex neural updates with simple hypervector operations, and applies differential privacy (DP) noise to protect transmitted model information. We optimize the total energy of computation and communication under both latency and privacy constraints. We formulate the problem as an optimization that minimizes the total energy of all users by jointly allocating HDC dimension, transmission time, system bandwidth, transmit power, and CPU frequency. To solve this problem, a sigmoid-variant function is proposed to characterize the relationship between the HDC dimension and the convergence rounds required to reach a target accuracy. Based on this model, we develop two alternating optimization algorithms, where closed-form expressions for time, frequency, bandwidth, and power allocations are derived at each iteration. Since the iterative algorithm requires a feasible initialization, we construct a feasibility problem and obtain feasible initial resource parameters by solving a per round transmission time minimization problem. Simulation results demonstrate that the proposed FL-HDC-DP framework achieves up to 83.3% total energy reduction compared with the baseline, while attaining about 90% accuracy in approximately 3.5X fewer communication rounds than the NN baseline.", "AI": {"tldr": "\u63d0\u51faFL-HDC-DP\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u7ef4\u8ba1\u7b97\u548c\u5dee\u5206\u9690\u79c1\u964d\u4f4e\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u8017\uff0c\u4f18\u5316\u8d44\u6e90\u5206\u914d\u5b9e\u73b083.3%\u7684\u8282\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u901a\u4fe1\u8f6e\u6570\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u5728\u65e0\u7ebf\u8fb9\u7f18\u7f51\u7edc\u4e2d\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u9690\u79c1\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7528\u6237\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u964d\u4f4e\u80fd\u8017\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faFL-HDC-DP\u6846\u67b6\uff1a1) \u4f7f\u7528\u8d85\u7ef4\u8ba1\u7b97(HDC)\u66ff\u4ee3\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u66f4\u65b0\uff0c\u7b80\u5316\u672c\u5730\u8bad\u7ec3\uff1b2) \u5e94\u7528\u5dee\u5206\u9690\u79c1(DP)\u4fdd\u62a4\u4f20\u8f93\u6a21\u578b\u4fe1\u606f\uff1b3) \u901a\u8fc7\u4f18\u5316HDC\u7ef4\u5ea6\u3001\u4f20\u8f93\u65f6\u95f4\u3001\u5e26\u5bbd\u3001\u53d1\u5c04\u529f\u7387\u548cCPU\u9891\u7387\u6765\u6700\u5c0f\u5316\u603b\u80fd\u8017\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u8fbe83.3%\u7684\u603b\u80fd\u8017\u964d\u4f4e\uff0c\u540c\u65f6\u8fbe\u5230\u7ea690%\u7684\u51c6\u786e\u7387\uff0c\u901a\u4fe1\u8f6e\u6570\u6bd4\u795e\u7ecf\u7f51\u7edc\u57fa\u7ebf\u51cf\u5c11\u7ea63.5\u500d\u3002", "conclusion": "FL-HDC-DP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u7ebf\u8fb9\u7f18\u8054\u90a6\u5b66\u4e60\u7684\u80fd\u8017\u548c\u9690\u79c1\u95ee\u9898\uff0c\u901a\u8fc7\u8d85\u7ef4\u8ba1\u7b97\u7b80\u5316\u8ba1\u7b97\uff0c\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u9690\u79c1\uff0c\u8054\u5408\u8d44\u6e90\u4f18\u5316\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.21681", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21681", "abs": "https://arxiv.org/abs/2602.21681", "authors": ["Renshuang Jiang", "Yichong Wang", "Pan Dong", "Xiaoxiang Fang", "Zhenling Duan", "Tinglue Wang", "Yuchen Hu", "Jie Yu", "Zhe Jiang"], "title": "AkiraRust: Re-thinking LLM-aided Rust Repair Using a Feedback-guided Thinking Switch", "comment": "7 pages, 11 figures, accepted to DAC", "summary": "Eliminating undefined behaviors (UBs) in Rust programs requires a deep semantic understanding to enable accurate and reliable repair. While existing studies have demonstrated the potential of LLMs to support Rust code analysis and repair, most frameworks remain constrained by inflexible templates or lack grounding in executable semantics, resulting in limited contextual awareness and semantic incorrectness. Here, we present AkiraRust, an LLM-driven repair and verification framework that incorporates a finite-state machine to dynamically adapt its detection and repair flow to runtime semantic conditions. AkiraRust introduces a dual-mode reasoning strategy that coordinates fast and slow thinking across multiple agents. Each agent is mapped to an FSM state, and a waveform-driven transition controller manages state switching, rollback decisions, and semantic check pointing, enabling context-aware and runtime-adaptive repair. Experimental results show that AkiraRust achieves about 92% semantic correctness and delivers a 2.2x average speedup compared to SOTA.", "AI": {"tldr": "AkiraRust\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684Rust\u7a0b\u5e8f\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u6709\u9650\u72b6\u6001\u673a\u548c\u53cc\u6a21\u5f0f\u63a8\u7406\u7b56\u7565\u5b9e\u73b0\u8fd0\u884c\u65f6\u8bed\u4e49\u81ea\u9002\u5e94\u7684\u672a\u5b9a\u4e49\u884c\u4e3a\u4fee\u590d\uff0c\u8fbe\u523092%\u7684\u8bed\u4e49\u6b63\u786e\u6027\u548c2.2\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u6709Rust\u4ee3\u7801\u4fee\u590d\u6846\u67b6\u5927\u591a\u53d7\u9650\u4e8e\u50f5\u5316\u7684\u6a21\u677f\u6216\u7f3a\u4e4f\u53ef\u6267\u884c\u8bed\u4e49\u57fa\u7840\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u611f\u77e5\u6709\u9650\u548c\u8bed\u4e49\u4e0d\u6b63\u786e\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6df1\u5ea6\u7406\u89e3\u8bed\u4e49\u5e76\u5b9e\u73b0\u51c6\u786e\u53ef\u9760\u4fee\u590d\u7684\u65b9\u6cd5\u3002", "method": "AkiraRust\u91c7\u7528\u6709\u9650\u72b6\u6001\u673a\u52a8\u6001\u8c03\u6574\u68c0\u6d4b\u548c\u4fee\u590d\u6d41\u7a0b\u4ee5\u9002\u5e94\u8fd0\u884c\u65f6\u8bed\u4e49\u6761\u4ef6\uff0c\u5f15\u5165\u53cc\u6a21\u5f0f\u63a8\u7406\u7b56\u7565\u534f\u8c03\u591a\u4e2a\u667a\u80fd\u4f53\u7684\u5feb\u6162\u601d\u8003\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u6620\u5c04\u5230FSM\u72b6\u6001\uff0c\u901a\u8fc7\u6ce2\u5f62\u9a71\u52a8\u8f6c\u6362\u63a7\u5236\u5668\u7ba1\u7406\u72b6\u6001\u5207\u6362\u3001\u56de\u6eda\u51b3\u7b56\u548c\u8bed\u4e49\u68c0\u67e5\u70b9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aAkiraRust\u8fbe\u5230\u7ea692%\u7684\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u6280\u672f\u5e73\u5747\u52a0\u901f2.2\u500d\u3002", "conclusion": "AkiraRust\u901a\u8fc7\u7ed3\u5408\u6709\u9650\u72b6\u6001\u673a\u548c\u53cc\u6a21\u5f0f\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u7684Rust\u7a0b\u5e8f\u4fee\u590d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u4e49\u6b63\u786e\u6027\u548c\u4fee\u590d\u6548\u7387\u3002"}}
{"id": "2602.21248", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21248", "abs": "https://arxiv.org/abs/2602.21248", "authors": ["Linus Baumg\u00e4rtner", "Adil Chhabra", "Marcelo Fonseca Faraj", "Christian Schulz"], "title": "BuffCut: Prioritized Buffered Streaming Graph Partitioning", "comment": null, "summary": "Streaming graph partitioners enable resource-efficient and massively scalable partitioning, but one-pass assignment heuristics are highly sensitive to stream order and often yield substantially higher edge cuts than in-memory methods. We present BuffCut, a buffered streaming partitioner that narrows this quality gap, particularly when stream ordering is adversarial, by combining prioritized buffering with batch-wise multilevel assignment. BuffCut maintains a bounded priority buffer to delay poorly informed decisions and regulate the order in which nodes are considered for assignment. It incrementally constructs high-locality batches of configurable size by iteratively inserting the highest-priority nodes from the buffer into the batch, effectively recovering locality structure from the stream. Each batch is then assigned via a multilevel partitioning algorithm. Experiments on diverse real-world and synthetic graphs show that BuffCut consistently outperforms state-of-the-art buffered streaming methods. Compared to the strongest prioritized buffering baseline, BuffCut achieves 20.8% fewer edge cuts while running 2.9 times faster and using 11.3 times less memory. Against the next-best buffered method, it reduces edge cut by 15.8% with only modest overheads of 1.8 times runtime and 1.09 times memory.", "AI": {"tldr": "BuffCut\uff1a\u4e00\u79cd\u5e26\u7f13\u51b2\u7684\u6d41\u5f0f\u56fe\u5212\u5206\u5668\uff0c\u901a\u8fc7\u4f18\u5148\u7ea7\u7f13\u51b2\u548c\u6279\u5904\u7406\u591a\u7ea7\u5206\u914d\uff0c\u663e\u8457\u51cf\u5c11\u8fb9\u5272\u6570\u91cf\uff0c\u5728\u5bf9\u6297\u6027\u6d41\u987a\u5e8f\u4e0b\u5c24\u5176\u6709\u6548\u3002", "motivation": "\u4f20\u7edf\u6d41\u5f0f\u56fe\u5212\u5206\u5668\u5bf9\u6570\u636e\u6d41\u987a\u5e8f\u9ad8\u5ea6\u654f\u611f\uff0c\u8fb9\u5272\u6570\u91cf\u901a\u5e38\u8fdc\u9ad8\u4e8e\u5185\u5b58\u65b9\u6cd5\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5728\u6d41\u5f0f\u5904\u7406\u4e2d\u4fdd\u6301\u9ad8\u8d28\u91cf\u5212\u5206\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6709\u754c\u4f18\u5148\u7ea7\u7f13\u51b2\u533a\u5ef6\u8fdf\u51b3\u7b56\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d2\u5165\u6700\u9ad8\u4f18\u5148\u7ea7\u8282\u70b9\u6784\u5efa\u9ad8\u5c40\u90e8\u6027\u6279\u6b21\uff0c\u7136\u540e\u4f7f\u7528\u591a\u7ea7\u5212\u5206\u7b97\u6cd5\u5206\u914d\u6bcf\u4e2a\u6279\u6b21\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u56fe\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cBuffCut\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7f13\u51b2\u6d41\u5f0f\u65b9\u6cd5\u3002\u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\uff0c\u8fb9\u5272\u51cf\u5c1120.8%\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb2.9\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1111.3\u500d\u3002", "conclusion": "BuffCut\u901a\u8fc7\u7ed3\u5408\u4f18\u5148\u7ea7\u7f13\u51b2\u548c\u6279\u5904\u7406\u591a\u7ea7\u5206\u914d\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u6d41\u5f0f\u4e0e\u5185\u5b58\u5212\u5206\u5668\u4e4b\u95f4\u7684\u8d28\u91cf\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u5bf9\u6297\u6027\u6d41\u987a\u5e8f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2602.22017", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22017", "abs": "https://arxiv.org/abs/2602.22017", "authors": ["Chris Egersdoerfer", "Arnav Sareen", "Jean Luca Bez", "Suren Byna", "Dongkuan", "Xu", "Dong Dai"], "title": "IOAgent: Democratizing Trustworthy HPC I/O Performance Diagnosis Capability via LLMs", "comment": "Published in the Proceedings of the 2025 IEEE International Parallel and Distributed Processing Symposium (IPDPS 2025)", "summary": "As the complexity of the HPC storage stack rapidly grows, domain scientists face increasing challenges in effectively utilizing HPC storage systems to achieve their desired I/O performance. To identify and address I/O issues, scientists largely rely on I/O experts to analyze their I/O traces and provide insights into potential problems. However, with a limited number of I/O experts and the growing demand for data-intensive applications, inaccessibility has become a major bottleneck, hindering scientists from maximizing their productivity. Rapid advances in LLMs make it possible to build an automated tool that brings trustworthy I/O performance diagnosis to domain scientists. However, key challenges remain, such as the inability to handle long context windows, a lack of accurate domain knowledge about HPC I/O, and the generation of hallucinations during complex interactions.In this work, we propose IOAgent as a systematic effort to address these challenges. IOAgent integrates a module-based pre-processor, a RAG-based domain knowledge integrator, and a tree-based merger to accurately diagnose I/O issues from a given Darshan trace file. Similar to an I/O expert, IOAgent provides detailed justifications and references for its diagnoses and offers an interactive interface for scientists to ask targeted follow-up questions. To evaluate IOAgent, we collected a diverse set of labeled job traces and released the first open diagnosis test suite, TraceBench. Using this test suite, we conducted extensive evaluations, demonstrating that IOAgent matches or outperforms state-of-the-art I/O diagnosis tools with accurate and useful diagnosis results. We also show that IOAgent is not tied to specific LLMs, performing similarly well with both proprietary and open-source LLMs. We believe IOAgent has the potential to become a powerful tool for scientists navigating complex HPC I/O subsystems in the future.", "AI": {"tldr": "IOAgent\uff1a\u57fa\u4e8eLLM\u7684HPC I/O\u6027\u80fd\u81ea\u52a8\u8bca\u65ad\u5de5\u5177\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u9884\u5904\u7406\u3001RAG\u9886\u57df\u77e5\u8bc6\u96c6\u6210\u548c\u6811\u72b6\u5408\u5e76\u6280\u672f\uff0c\u4e3a\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e13\u5bb6\u7ea7\u7684I/O\u95ee\u9898\u8bca\u65ad\u548c\u4ea4\u4e92\u5f0f\u5206\u6790\u3002", "motivation": "\u968f\u7740HPC\u5b58\u50a8\u7cfb\u7edf\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u79d1\u5b66\u5bb6\u9762\u4e34I/O\u6027\u80fd\u4f18\u5316\u6311\u6218\uff0c\u800cI/O\u4e13\u5bb6\u8d44\u6e90\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6570\u636e\u5bc6\u96c6\u578b\u5e94\u7528\u9700\u6c42\u3002LLM\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u6784\u5efa\u81ea\u52a8\u5316\u8bca\u65ad\u5de5\u5177\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u4f46\u5b58\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u3001\u9886\u57df\u77e5\u8bc6\u7f3a\u4e4f\u548c\u5e7b\u89c9\u751f\u6210\u7b49\u95ee\u9898\u3002", "method": "IOAgent\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a1) \u6a21\u5757\u5316\u9884\u5904\u7406\u5668\u5904\u7406Darshan\u8ddf\u8e2a\u6587\u4ef6\uff1b2) RAG\u9886\u57df\u77e5\u8bc6\u96c6\u6210\u5668\u6574\u5408HPC I/O\u4e13\u4e1a\u77e5\u8bc6\uff1b3) \u6811\u72b6\u5408\u5e76\u5668\u51c6\u786e\u8bca\u65adI/O\u95ee\u9898\u3002\u7cfb\u7edf\u63d0\u4f9b\u8be6\u7ec6\u8bca\u65ad\u4f9d\u636e\u548c\u4ea4\u4e92\u5f0f\u95ee\u7b54\u754c\u9762\u3002", "result": "\u5728\u9996\u4e2a\u5f00\u653e\u8bca\u65ad\u6d4b\u8bd5\u5957\u4ef6TraceBench\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cIOAgent\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u7684I/O\u8bca\u65ad\u5de5\u5177\uff0c\u63d0\u4f9b\u51c6\u786e\u6709\u7528\u7684\u8bca\u65ad\u7ed3\u679c\u3002\u7cfb\u7edf\u4e0d\u4f9d\u8d56\u7279\u5b9aLLM\uff0c\u5728\u4e13\u6709\u548c\u5f00\u6e90LLM\u4e0a\u8868\u73b0\u4e00\u81f4\u826f\u597d\u3002", "conclusion": "IOAgent\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86LLM\u5728HPC I/O\u8bca\u65ad\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u6709\u671b\u6210\u4e3a\u79d1\u5b66\u5bb6\u5e94\u5bf9\u590d\u6742HPC I/O\u5b50\u7cfb\u7edf\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u63d0\u9ad8\u79d1\u5b66\u5bb6\u7684\u751f\u4ea7\u529b\u548cI/O\u6027\u80fd\u4f18\u5316\u80fd\u529b\u3002"}}
{"id": "2602.21697", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21697", "abs": "https://arxiv.org/abs/2602.21697", "authors": ["Chenyan Liu", "Yun Lin", "Jiaxin Chang", "Jiawei Liu", "Binhang Qi", "Bo Jiang", "Zhiyong Huang", "Jin Song Dong"], "title": "EditFlow: Benchmarking and Optimizing Code Edit Recommendation Systems via Reconstruction of Developer Flows", "comment": "Accepted at OOPSLA 2026 (Proc. ACM Program. Lang., Vol. 10, OOPSLA1)", "summary": "Large language models (LLMs) for code editing have achieved remarkable progress, yet recent empirical studies reveal a fundamental disconnect between technical accuracy and developer productivity. Despite their strong benchmark performance, developers complete tasks 19% slower when using AI assistance, with over 68.81% of recommendations disrupting their mental flow. This misalignment stems from the use of static commit snapshots that lack temporal information, causing models to optimize for end results rather than the incremental, context-sensitive steps that align with developers' natural reasoning process.\n  To bridge this gap, we present EditFlow, which benchmarks and optimizes subsequent code edit recommendation systems through the reconstruction of developer editing flows. EditFlow addresses three key challenges. First, collecting edit-order data that reflects developers' flow is inherently difficult: manual annotation introduces prohibitive overhead, while development logs capture only single trajectories instead of all plausible editing flows. Second, benchmarking recommendation performance against developers' ongoing editing flow requires a digital-twin-like simulation that can faithfully simulate the editing process. Third, existing heterogeneous systems vary drastically in scale and architecture, posing challenges for developing a unified optimization strategy that endows all models with mental-flow awareness regardless of design or capability.\n  ......", "AI": {"tldr": "EditFlow\uff1a\u901a\u8fc7\u91cd\u6784\u5f00\u53d1\u8005\u7f16\u8f91\u6d41\u7a0b\u6765\u4f18\u5316\u4ee3\u7801\u7f16\u8f91\u63a8\u8350\u7cfb\u7edf\uff0c\u89e3\u51b3AI\u8f85\u52a9\u5f00\u53d1\u4e2d\u6280\u672f\u51c6\u786e\u6027\u4e0e\u5f00\u53d1\u6548\u7387\u8131\u8282\u7684\u95ee\u9898", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7f16\u8f91\u65b9\u9762\u867d\u7136\u6280\u672f\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5b9e\u9645\u4f7f\u7528\u4e2d\u5f00\u53d1\u8005\u5b8c\u6210\u4efb\u52a1\u901f\u5ea6\u53cd\u800c\u964d\u4f4e19%\uff0c\u8d85\u8fc768.81%\u7684\u63a8\u8350\u4f1a\u6253\u65ad\u5f00\u53d1\u8005\u7684\u601d\u7ef4\u6d41\u7a0b\u3002\u95ee\u9898\u6839\u6e90\u5728\u4e8e\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u63d0\u4ea4\u5feb\u7167\uff0c\u7f3a\u4e4f\u65f6\u95f4\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u4f18\u5316\u7684\u662f\u6700\u7ec8\u7ed3\u679c\u800c\u975e\u4e0e\u5f00\u53d1\u8005\u81ea\u7136\u63a8\u7406\u8fc7\u7a0b\u4e00\u81f4\u7684\u589e\u91cf\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u6b65\u9aa4\u3002", "method": "\u63d0\u51faEditFlow\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u6784\u5f00\u53d1\u8005\u7f16\u8f91\u6d41\u7a0b\u6765\u57fa\u51c6\u6d4b\u8bd5\u548c\u4f18\u5316\u540e\u7eed\u4ee3\u7801\u7f16\u8f91\u63a8\u8350\u7cfb\u7edf\u3002\u89e3\u51b3\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u6536\u96c6\u53cd\u6620\u5f00\u53d1\u8005\u6d41\u7a0b\u7684\u7f16\u8f91\u987a\u5e8f\u6570\u636e\uff1b2) \u521b\u5efa\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u6765\u57fa\u51c6\u6d4b\u8bd5\u63a8\u8350\u6027\u80fd\uff1b3) \u4e3a\u5f02\u6784\u7cfb\u7edf\u5f00\u53d1\u7edf\u4e00\u7684\u4f18\u5316\u7b56\u7565\uff0c\u4f7f\u6240\u6709\u6a21\u578b\u5177\u5907\u601d\u7ef4\u6d41\u7a0b\u610f\u8bc6\u3002", "result": "\u8bba\u6587\u672a\u5728\u6458\u8981\u4e2d\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63d0\u51fa\u4e86\u89e3\u51b3\u73b0\u6709\u95ee\u9898\u7684\u7cfb\u7edf\u6846\u67b6\u548c\u65b9\u6cd5\u8bba\u3002", "conclusion": "EditFlow\u901a\u8fc7\u5173\u6ce8\u5f00\u53d1\u8005\u7684\u7f16\u8f91\u6d41\u7a0b\u800c\u975e\u6700\u7ec8\u7ed3\u679c\uff0c\u65e8\u5728\u5f25\u5408\u6280\u672f\u51c6\u786e\u6027\u4e0e\u5f00\u53d1\u6548\u7387\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f7fAI\u4ee3\u7801\u7f16\u8f91\u63a8\u8350\u7cfb\u7edf\u66f4\u597d\u5730\u4e0e\u5f00\u53d1\u8005\u7684\u81ea\u7136\u601d\u7ef4\u8fc7\u7a0b\u4fdd\u6301\u4e00\u81f4\u3002"}}
{"id": "2602.21249", "categories": ["cs.DB", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.21249", "abs": "https://arxiv.org/abs/2602.21249", "authors": ["Markus Matoni", "Arno Kesper", "Gabriele Taentzer"], "title": "Quality of Descriptive Information on Cultural Heritage Objects: Definition and Empirical Evaluation", "comment": "preprint", "summary": "Effective data processing depends on the quality of the underlying data. However, quality issues such as inconsistencies and uncertainties, can significantly impede the processing and subsequent use of data. Despite the centrality of data quality to a wide range of computational tasks, there is currently no broadly accepted, domain-independent consensus on the definition of data quality. Existing frameworks primarily define data quality in ways that are tailored to specific domains, data types, or contexts of use. Although quality assessment frameworks exist for specific domains, such as electronic health record data and linked data, corresponding approaches for descriptive information about cultural heritage objects remain underdeveloped. Moreover, existing quality definitions are often theoretical in nature and lack empirical validation based on real-world data problems. In this paper, we address these limitations by first defining a set of quality dimensions specifically designed to capture the characteristics of descriptive information about cultural heritage objects. Our definition is based on an in-depth analysis of existing dimensions and is illustrated through domain-specific examples. We then evaluate the practical applicability of our proposed quality definition using a curated set of real-world data quality problems from the cultural heritage domain. This empirical evaluation substantiates our definition of data quality, resulting in a comprehensive definition of data quality in this domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u6587\u5316\u9057\u4ea7\u9886\u57df\u63cf\u8ff0\u6027\u4fe1\u606f\u7684\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e13\u95e8\u7684\u8d28\u91cf\u7ef4\u5ea6\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6570\u636e\u95ee\u9898\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "motivation": "\u6570\u636e\u8d28\u91cf\u5bf9\u6570\u636e\u5904\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5e7f\u6cdb\u63a5\u53d7\u3001\u9886\u57df\u65e0\u5173\u7684\u6570\u636e\u8d28\u91cf\u5b9a\u4e49\u3002\u73b0\u6709\u6846\u67b6\u591a\u4e3a\u7279\u5b9a\u9886\u57df\u5b9a\u5236\uff0c\u6587\u5316\u9057\u4ea7\u9886\u57df\u63cf\u8ff0\u6027\u4fe1\u606f\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u53d1\u5c55\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u5b9a\u4e49\u591a\u4e3a\u7406\u8bba\u6027\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u95ee\u9898\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "method": "\u9996\u5148\u57fa\u4e8e\u5bf9\u73b0\u6709\u8d28\u91cf\u7ef4\u5ea6\u7684\u6df1\u5165\u5206\u6790\uff0c\u5b9a\u4e49\u4e86\u4e00\u5957\u4e13\u95e8\u9488\u5bf9\u6587\u5316\u9057\u4ea7\u5bf9\u8c61\u63cf\u8ff0\u6027\u4fe1\u606f\u7684\u8d28\u91cf\u7ef4\u5ea6\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u793a\u4f8b\u8fdb\u884c\u8bf4\u660e\u3002\u7136\u540e\u4f7f\u7528\u6587\u5316\u9057\u4ea7\u9886\u57df\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8d28\u91cf\u95ee\u9898\u7684\u7cbe\u9009\u96c6\uff0c\u5bf9\u6240\u63d0\u51fa\u7684\u8d28\u91cf\u5b9a\u4e49\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6570\u636e\u8d28\u91cf\u5b9a\u4e49\uff0c\u5f62\u6210\u4e86\u6587\u5316\u9057\u4ea7\u9886\u57df\u5168\u9762\u7684\u6570\u636e\u8d28\u91cf\u5b9a\u4e49\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u6587\u5316\u9057\u4ea7\u9886\u57df\u6570\u636e\u8d28\u91cf\u5b9a\u4e49\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u7684\u3001\u4e13\u95e8\u9488\u5bf9\u6587\u5316\u9057\u4ea7\u63cf\u8ff0\u6027\u4fe1\u606f\u7684\u8d28\u91cf\u7ef4\u5ea6\u6846\u67b6\u3002"}}
{"id": "2602.22103", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.22103", "abs": "https://arxiv.org/abs/2602.22103", "authors": ["Mao Lin", "Hyeran Jeon", "Keren Zhou"], "title": "PASTA: A Modular Program Analysis Tool Framework for Accelerators", "comment": null, "summary": "The increasing complexity and diversity of hardware accelerators in modern computing systems demand flexible, low-overhead program analysis tools. We present PASTA, a low-overhead and modular Program AnalysiS Tool Framework for Accelerators. PASTA abstracts over low-level profiling APIs and diverse deep learning frameworks, offering users a unified interface to capture and analyze runtime events at multiple levels. Its extensible design enables researchers and practitioners to rapidly prototype custom tools with minimal overhead. We demonstrate the utility of PASTA by developing several analysis tools, including a deep learning workload characterization tool and a UVM optimization tool. Through extensive evaluation on mainstream deep learning workloads tested on NVIDIA and AMD GPUs under both single- and multi-GPU scenarios, we demonstrate PASTA's broad applicability. On NVIDIA GPUs, we further show that PASTA provides detailed performance insights with significantly lower overhead, up to 1.3*10^4 faster than conventional analysis tools, thanks to its GPU-accelerated backend. PASTA strikes a practical balance between usability, extensibility, and efficiency, making it well-suited for modern accelerator-based computing environments.", "AI": {"tldr": "PASTA\u662f\u4e00\u4e2a\u4f4e\u5f00\u9500\u3001\u6a21\u5757\u5316\u7684\u52a0\u901f\u5668\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u5e95\u5c42API\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\u8fdb\u884c\u591a\u7ea7\u8fd0\u884c\u65f6\u4e8b\u4ef6\u5206\u6790\uff0c\u652f\u6301\u5feb\u901f\u539f\u578b\u5f00\u53d1\u81ea\u5b9a\u4e49\u5de5\u5177\u3002", "motivation": "\u73b0\u4ee3\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u786c\u4ef6\u52a0\u901f\u5668\u65e5\u76ca\u590d\u6742\u591a\u6837\uff0c\u9700\u8981\u7075\u6d3b\u3001\u4f4e\u5f00\u9500\u7684\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\u6765\u5e94\u5bf9\u4e0d\u540c\u52a0\u901f\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u7684\u6311\u6218\u3002", "method": "PASTA\u6846\u67b6\u62bd\u8c61\u5e95\u5c42\u5206\u6790API\u548c\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\u6355\u83b7\u548c\u5206\u6790\u591a\u7ea7\u8fd0\u884c\u65f6\u4e8b\u4ef6\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u8bbe\u8ba1\u652f\u6301\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff0c\u5229\u7528GPU\u52a0\u901f\u540e\u7aef\u964d\u4f4e\u5f00\u9500\u3002", "result": "\u5728NVIDIA\u548cAMD GPU\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cPASTA\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5728NVIDIA GPU\u4e0a\u6bd4\u4f20\u7edf\u5206\u6790\u5de5\u5177\u5feb\u8fbe1.3\u00d710^4\u500d\uff0c\u663e\u8457\u964d\u4f4e\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u4f9b\u8be6\u7ec6\u6027\u80fd\u6d1e\u5bdf\u3002", "conclusion": "PASTA\u5728\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u5b9e\u7528\u5e73\u8861\uff0c\u975e\u5e38\u9002\u5408\u73b0\u4ee3\u57fa\u4e8e\u52a0\u901f\u5668\u7684\u8ba1\u7b97\u73af\u5883\u3002"}}
{"id": "2602.21734", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21734", "abs": "https://arxiv.org/abs/2602.21734", "authors": ["Selin Coban", "Miguel Perez", "Horst Lichter"], "title": "Proto-ML: An IDE for ML Solution Prototyping", "comment": "To be published at 3rd International Workshop on Integrated Development Environments (IDE '26), April 12--18, 2026, Rio de Janeiro, Brazil", "summary": "Prototyping plays a critical role in the development of machine learning (ML) solutions, yet existing tools often provide limited support for effective collaboration and knowledge reuse among stakeholders. This paper introduces Proto-ML, an IDE designed to strengthen ML prototyping workflows. By addressing key deficiencies such as insufficient stakeholder involvement, limited cross-project knowledge reuse, and fragmented tool support, Proto-ML offers a unified framework that enables structured documentation of prototyping activities and promotes knowledge sharing across projects.\n  The Proto-ML IDE consists of three extension bundles: prototype implementation, analysis, and knowledge management. These extensions support tasks ranging from evaluating prototype quality against defined criteria to incorporating stakeholder perspectives throughout the development process. Preliminary user feedback suggests that Proto-ML can increase prototyping efficiency and foster more transparent and reusable ML solution development.", "AI": {"tldr": "Proto-ML\u662f\u4e00\u4e2a\u4e13\u4e3a\u673a\u5668\u5b66\u4e60\u539f\u578b\u5f00\u53d1\u8bbe\u8ba1\u7684\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff0c\u901a\u8fc7\u4e09\u4e2a\u6269\u5c55\u5305\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u5728\u534f\u4f5c\u548c\u77e5\u8bc6\u590d\u7528\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8\u539f\u578b\u5f00\u53d1\u6548\u7387\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u539f\u578b\u5f00\u53d1\u5de5\u5177\u5728\u534f\u4f5c\u548c\u77e5\u8bc6\u590d\u7528\u65b9\u9762\u652f\u6301\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u3001\u8de8\u9879\u76ee\u77e5\u8bc6\u91cd\u7528\u548c\u7edf\u4e00\u5de5\u5177\u652f\u6301\uff0c\u5bfc\u81f4\u539f\u578b\u5f00\u53d1\u6548\u7387\u4f4e\u4e0b\u3002", "method": "Proto-ML IDE\u5305\u542b\u4e09\u4e2a\u6269\u5c55\u5305\uff1a\u539f\u578b\u5b9e\u73b0\u3001\u5206\u6790\u548c\u77e5\u8bc6\u7ba1\u7406\u3002\u8fd9\u4e9b\u6269\u5c55\u652f\u6301\u4ece\u8bc4\u4f30\u539f\u578b\u8d28\u91cf\u5230\u6574\u5408\u5229\u76ca\u76f8\u5173\u8005\u89c6\u89d2\u7684\u5168\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u6587\u6863\u5316\u548c\u77e5\u8bc6\u5171\u4eab\u6846\u67b6\u3002", "result": "\u521d\u6b65\u7528\u6237\u53cd\u9988\u8868\u660eProto-ML\u80fd\u591f\u63d0\u9ad8\u539f\u578b\u5f00\u53d1\u6548\u7387\uff0c\u4fc3\u8fdb\u66f4\u900f\u660e\u548c\u53ef\u590d\u7528\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u5f00\u53d1\u3002", "conclusion": "Proto-ML\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u4e86\u673a\u5668\u5b66\u4e60\u539f\u578b\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u7f3a\u9677\uff0c\u589e\u5f3a\u4e86\u534f\u4f5c\u548c\u77e5\u8bc6\u590d\u7528\u80fd\u529b\uff0c\u4e3a\u66f4\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u5f00\u53d1\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2602.21480", "categories": ["cs.DB", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.21480", "abs": "https://arxiv.org/abs/2602.21480", "authors": ["Germ\u00e1n T. Eizaguirre", "Lars Tissen", "Marc S\u00e1nchez-Artigas"], "title": "Both Ends Count! Just How Good are LLM Agents at \"Text-to-Big SQL\"?", "comment": "11 pages, 4 figures", "summary": "Text-to-SQL and Big Data are both extensively benchmarked fields, yet there is limited research that evaluates them jointly. In the real world, Text-to-SQL systems are often embedded with Big Data workflows, such as large-scale data processing or interactive data analytics. We refer to this as \"Text-to-Big SQL\". However, existing text-to-SQL benchmarks remain narrowly scoped and overlook the cost and performance implications that arise at scale. For instance, translation errors that are minor on small datasets lead to substantial cost and latency overheads as data scales, a relevant issue completely ignored by text-to-SQL metrics.\n  In this paper, we overcome this overlooked challenge by introducing novel and representative metrics for evaluating Text-to-Big SQL. Our study focuses on production-level LLM agents, a database-agnostic system adaptable to diverse user needs. Via an extensive evaluation of frontier models, we show that text-to-SQL metrics are insufficient for Big Data. In contrast, our proposed text-to-Big SQL metrics accurately reflect execution efficiency, cost, and the impact of data scale. Furthermore, we provide LLM-specific insights, including fine-grained, cross-model comparisons of latency and cost.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\"Text-to-Big SQL\"\u6982\u5ff5\uff0c\u9488\u5bf9\u73b0\u6709\u6587\u672c\u5230SQL\u57fa\u51c6\u6d4b\u8bd5\u5728\u5927\u6570\u636e\u573a\u666f\u4e0b\u7684\u4e0d\u8db3\uff0c\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4f20\u7edf\u6307\u6807\u5728\u5927\u6570\u636e\u73af\u5883\u4e2d\u4e0d\u591f\u5145\u5206\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230SQL\u7814\u7a76\u548c\u5927\u6570\u636e\u9886\u57df\u867d\u7136\u90fd\u6709\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f46\u7f3a\u4e4f\u8054\u5408\u8bc4\u4f30\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u6587\u672c\u5230SQL\u7cfb\u7edf\u5e38\u5d4c\u5165\u5927\u6570\u636e\u5de5\u4f5c\u6d41\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u72ed\u7a84\uff0c\u5ffd\u7565\u4e86\u6570\u636e\u89c4\u6a21\u6269\u5927\u65f6\u7684\u6210\u672c\u548c\u6027\u80fd\u5f71\u54cd\u3002\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u5fae\u5c0f\u7ffb\u8bd1\u9519\u8bef\u5728\u5927\u6570\u636e\u573a\u666f\u4e0b\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u6210\u672c\u548c\u5ef6\u8fdf\u5f00\u9500\uff0c\u800c\u73b0\u6709\u6307\u6807\u5b8c\u5168\u5ffd\u7565\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u65b0\u9896\u4e14\u5177\u6709\u4ee3\u8868\u6027\u7684Text-to-Big SQL\u8bc4\u4f30\u6307\u6807\uff0c\u91cd\u70b9\u5173\u6ce8\u751f\u4ea7\u7ea7LLM\u4ee3\u7406\uff08\u6570\u636e\u5e93\u65e0\u5173\u7684\u7cfb\u7edf\uff0c\u9002\u5e94\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\uff09\u3002\u901a\u8fc7\u5bf9\u524d\u6cbf\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5206\u6790\u4f20\u7edf\u6587\u672c\u5230SQL\u6307\u6807\u5728\u5927\u6570\u636e\u573a\u666f\u4e0b\u7684\u4e0d\u8db3\u3002", "result": "\u7814\u7a76\u8868\u660e\u6587\u672c\u5230SQL\u6307\u6807\u5728\u5927\u6570\u636e\u573a\u666f\u4e0b\u4e0d\u591f\u5145\u5206\uff0c\u800c\u63d0\u51fa\u7684Text-to-Big SQL\u6307\u6807\u80fd\u51c6\u786e\u53cd\u6620\u6267\u884c\u6548\u7387\u3001\u6210\u672c\u4ee5\u53ca\u6570\u636e\u89c4\u6a21\u7684\u5f71\u54cd\u3002\u540c\u65f6\u63d0\u4f9b\u4e86LLM\u7279\u5b9a\u6d1e\u5bdf\uff0c\u5305\u62ec\u7ec6\u7c92\u5ea6\u7684\u8de8\u6a21\u578b\u5ef6\u8fdf\u548c\u6210\u672c\u6bd4\u8f83\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5927\u6570\u636e\u573a\u666f\u7684\u6587\u672c\u5230SQL\u8bc4\u4f30\u6307\u6807\uff0c\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u6355\u6349\u6570\u636e\u89c4\u6a21\u6269\u5927\u65f6\u7684\u5b9e\u9645\u6210\u672c\u548c\u6027\u80fd\u5f71\u54cd\u3002\u63d0\u51fa\u7684Text-to-Big SQL\u6307\u6807\u4e3a\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.22158", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.22158", "abs": "https://arxiv.org/abs/2602.22158", "authors": ["Minqiu Sun", "Xin Huang", "Luanzheng Guo", "Nathan R. Tallent", "Kento Sato", "Dong Dai"], "title": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models", "comment": "9 pages, 3 figures, accepted at PDSW'25", "summary": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that selectively checkpointing only layers with significant updates could reduce overhead without harming training. Implementing such selective strategies requires fine-grained control over both weights and optimizer states, which no current tool provides. To address this gap, we propose \\texttt{LLMTailor}, a checkpoint-merging framework that filters and assembles layers from different checkpoints to form a composite checkpoint. Our evaluation indicates that LLMTailor can work with different selective checkpointing strategies and effectively reduce checkpoint size (e.g., 4.3 times smaller for Llama3.1-8B) and checkpoint time (e.g., 2.8 times faster for Qwen2.5-7B) while maintaining model quality.", "AI": {"tldr": "LLMTailor\u662f\u4e00\u4e2a\u68c0\u67e5\u70b9\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u4fdd\u5b58\u663e\u8457\u66f4\u65b0\u7684\u5c42\u6765\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u68c0\u67e5\u70b9\u5f00\u9500\uff0c\u53ef\u5c06\u68c0\u67e5\u70b9\u5927\u5c0f\u51cf\u5c114.3\u500d\uff0c\u65f6\u95f4\u52a0\u5feb2.8\u500d\u3002", "motivation": "\u73b0\u6709\u68c0\u67e5\u70b9\u65b9\u6cd5\u9700\u8981\u5b9a\u671f\u5b58\u50a8\u6574\u4e2a\u6a21\u578b\u548c\u4f18\u5316\u5668\u72b6\u6001\uff0c\u5bfc\u81f4\u5de8\u5927\u7684\u5b58\u50a8\u5f00\u9500\u548c\u8d44\u6e90\u4e89\u7528\u3002\u7814\u7a76\u53d1\u73b0LLM\u5404\u5c42\u7684\u66f4\u65b0\u9ad8\u5ea6\u4e0d\u5747\u5300\uff0c\u6709\u4e9b\u5c42\u53d8\u5316\u663e\u8457\uff0c\u6709\u4e9b\u5c42\u76f8\u5bf9\u7a33\u5b9a\u751a\u81f3\u4e0d\u53d8\uff0c\u8fd9\u4e3a\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faLLMTailor\u68c0\u67e5\u70b9\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u6ee4\u548c\u7ec4\u88c5\u4e0d\u540c\u68c0\u67e5\u70b9\u4e2d\u7684\u5c42\u6765\u5f62\u6210\u590d\u5408\u68c0\u67e5\u70b9\u3002\u8be5\u6846\u67b6\u652f\u6301\u4e0d\u540c\u7684\u9009\u62e9\u6027\u68c0\u67e5\u70b9\u7b56\u7565\uff0c\u80fd\u591f\u5bf9\u6743\u91cd\u548c\u4f18\u5316\u5668\u72b6\u6001\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u8bc4\u4f30\u663e\u793aLLMTailor\u80fd\u6709\u6548\u51cf\u5c11\u68c0\u67e5\u70b9\u5927\u5c0f\uff08\u5982Llama3.1-8B\u51cf\u5c114.3\u500d\uff09\u548c\u68c0\u67e5\u70b9\u65f6\u95f4\uff08\u5982Qwen2.5-7B\u52a0\u5feb2.8\u500d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u3002", "conclusion": "LLMTailor\u901a\u8fc7\u5229\u7528LLM\u5c42\u66f4\u65b0\u7684\u4e0d\u5747\u5300\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9009\u62e9\u6027\u68c0\u67e5\u70b9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u63a7\u5236\u7684\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u4e2d\u7684\u68c0\u67e5\u70b9\u5f00\u9500\u3002"}}
{"id": "2602.21800", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.21800", "abs": "https://arxiv.org/abs/2602.21800", "authors": ["Madhusudan Ghosh", "Rishabh Gupta"], "title": "An Evaluation of Context Length Extrapolation in Long Code via Positional Embeddings and Efficient Attention", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has led to a significant increase in automated tools in the software engineering, capable of performing various code-related tasks such as code generation, completion, and translation. Despite these advancements, its effectiveness is constrained by fixed context lengths, limiting its ability to generalize across long, domain-specific code sequences. To address this challenge, we investigate zero-shot, inference-only methods aimed at improving position encodings and optimizing attention mechanisms. Our goal is to provide a thorough analysis of current approaches that facilitate context length extrapolation in code, particularly in the context of long code completion tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4ee3\u7801\u5e8f\u5217\u4e0a\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u5916\u63a8\u80fd\u529b\uff0c\u7279\u522b\u9488\u5bf9\u4ee3\u7801\u8865\u5168\u4efb\u52a1\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u63a8\u7406\u65b9\u6cd5\u4f18\u5316\u4f4d\u7f6e\u7f16\u7801\u548c\u6ce8\u610f\u529b\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u81ea\u52a8\u5316\u5de5\u5177\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u591f\u6267\u884c\u4ee3\u7801\u751f\u6210\u3001\u8865\u5168\u548c\u7ffb\u8bd1\u7b49\u4efb\u52a1\uff0c\u4f46\u5176\u6709\u6548\u6027\u53d7\u5230\u56fa\u5b9a\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u9650\u5236\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u957f\u9886\u57df\u7279\u5b9a\u4ee3\u7801\u5e8f\u5217\u3002", "method": "\u7814\u7a76\u96f6\u6837\u672c\u3001\u4ec5\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u6539\u8fdb\u4f4d\u7f6e\u7f16\u7801\u548c\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u4fc3\u8fdb\u4ee3\u7801\u4e2d\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u5916\u63a8\u3002", "result": "\u8bba\u6587\u65e8\u5728\u5bf9\u5f53\u524d\u4fc3\u8fdb\u4ee3\u7801\u4e0a\u4e0b\u6587\u957f\u5ea6\u5916\u63a8\u7684\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u7279\u522b\u662f\u5728\u957f\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u80cc\u666f\u4e0b\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4f4d\u7f6e\u7f16\u7801\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4ee3\u7801\u5e8f\u5217\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u6539\u5584\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2602.21514", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21514", "abs": "https://arxiv.org/abs/2602.21514", "authors": ["Liang Li", "Shufeng Gong", "Yanan Yang", "Yiduo Wang", "Jie Wu"], "title": "I/O Optimizations for Graph-Based Disk-Resident Approximate Nearest Neighbor Search: A Design Space Exploration", "comment": null, "summary": "Approximate nearest neighbor (ANN) search on SSD-backed indexes is increasingly I/O-bound (I/O accounts for 70--90\\% of query latency). We present an I/O-first framework for disk-based ANN that organizes techniques along three dimensions: memory layout, disk layout, and search algorithm. We introduce a page-level complexity model that explains how page locality and path length jointly determine page reads, and we validate the model empirically. Using consistent implementations across four public datasets, we quantify both single-factor effects and cross-dimensional synergies. We find that (i) memory-resident navigation and dynamic width provide the strongest standalone gains; (ii) page shuffle and page search are weak alone but complementary together; and (iii) a principled composition, OctopusANN, substantially reduces I/O and achieves 4.1--37.9\\% higher throughput than the state-of-the-art system Starling and 87.5--149.5\\% higher throughput than DiskANN at matched Recall@10=90\\%. Finally, we distill actionable guidelines for selecting storage-centric or hybrid designs across diverse concurrency levels and accuracy constraints, advocating systematic composition rather than isolated tweaks when pushing the performance frontier of disk-based ANN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u9762\u5411SSD\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22I/O\u4f18\u5148\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5b58\u5e03\u5c40\u3001\u78c1\u76d8\u5e03\u5c40\u548c\u641c\u7d22\u7b97\u6cd5\u4e09\u4e2a\u7ef4\u5ea6\u7684\u7cfb\u7edf\u7ec4\u5408\uff0c\u663e\u8457\u51cf\u5c11I/O\u5f00\u9500\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u57fa\u4e8eSSD\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\u4e2d\uff0cI/O\u64cd\u4f5c\u5360\u67e5\u8be2\u5ef6\u8fdf\u768470-90%\uff0c\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9I/O\u4f18\u5316\u7684\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u9700\u8981\u4ece\u5b58\u50a8\u89d2\u5ea6\u91cd\u65b0\u601d\u8003ANN\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faI/O\u4f18\u5148\u6846\u67b6\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u7ec4\u7ec7\u6280\u672f\uff1a\u5185\u5b58\u5e03\u5c40\u3001\u78c1\u76d8\u5e03\u5c40\u548c\u641c\u7d22\u7b97\u6cd5\u3002\u5efa\u7acb\u9875\u9762\u7ea7\u590d\u6742\u5ea6\u6a21\u578b\u89e3\u91ca\u9875\u9762\u5c40\u90e8\u6027\u548c\u8def\u5f84\u957f\u5ea6\u5982\u4f55\u5171\u540c\u51b3\u5b9a\u9875\u9762\u8bfb\u53d6\u3002\u901a\u8fc7\u4e00\u81f4\u5b9e\u73b0\u9a8c\u8bc1\u6a21\u578b\uff0c\u5e76\u91cf\u5316\u5355\u56e0\u7d20\u6548\u5e94\u548c\u8de8\u7ef4\u5ea6\u534f\u540c\u4f5c\u7528\u3002", "result": "\u5185\u5b58\u9a7b\u7559\u5bfc\u822a\u548c\u52a8\u6001\u5bbd\u5ea6\u63d0\u4f9b\u6700\u5f3a\u7684\u72ec\u7acb\u589e\u76ca\uff1b\u9875\u9762\u6d17\u724c\u548c\u9875\u9762\u641c\u7d22\u5355\u72ec\u6548\u679c\u5f31\u4f46\u4e92\u8865\u6027\u5f3a\uff1b\u7cfb\u7edf\u5316\u7ec4\u5408OctopusANN\u663e\u8457\u51cf\u5c11I/O\uff0c\u5728Recall@10=90%\u65f6\u6bd4Starling\u541e\u5410\u91cf\u9ad84.1-37.9%\uff0c\u6bd4DiskANN\u9ad887.5-149.5%\u3002", "conclusion": "\u4e3a\u57fa\u4e8e\u78c1\u76d8\u7684ANN\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u539f\u5219\uff0c\u5f3a\u8c03\u5728\u4e0d\u540c\u5e76\u53d1\u6c34\u5e73\u548c\u7cbe\u5ea6\u7ea6\u675f\u4e0b\u9009\u62e9\u5b58\u50a8\u4e2d\u5fc3\u6216\u6df7\u5408\u8bbe\u8ba1\uff0c\u5021\u5bfc\u7cfb\u7edf\u5316\u7ec4\u5408\u800c\u975e\u5b64\u7acb\u8c03\u6574\u4ee5\u63a8\u52a8\u6027\u80fd\u8fb9\u754c\u3002"}}
{"id": "2602.21806", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21806", "abs": "https://arxiv.org/abs/2602.21806", "authors": ["Xinxue Zhu", "Jiacong Wu", "Xiaoyu Zhang", "Tianlin Li", "Yanzhou Mu", "Juan Zhai", "Chao Shen", "Yang Liu"], "title": "An Empirical Study of Bugs in Modern LLM Agent Frameworks", "comment": null, "summary": "LLM agents have been widely adopted in real-world applications, relying on agent frameworks for workflow execution and multi-agent coordination. As these systems scale, understanding bugs in the underlying agent frameworks becomes critical. However, existing work mainly focuses on agent-level failures, overlooking framework-level bugs. To address this gap, we conduct an empirical study of 998 bug reports from CrewAI and LangChain, constructing a taxonomy of 15 root causes and 7 observable symptoms across five agent lifecycle stages: 'Agent Initialization','Perception', 'Self-Action', 'Mutual Interaction' and 'Evolution'. Our findings show that agent framework bugs mainly arise from 'API misuse', 'API incompatibility', and 'Documentation Desync', largely concentrated in the 'Self-Action' stage. Symptoms typically appear as 'Functional Error', 'Crash', and 'Build Failure', reflecting disruptions to task progression and control flow.", "AI": {"tldr": "\u5bf9CrewAI\u548cLangChain\u7684998\u4e2abug\u62a5\u544a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u6784\u5efa\u4e8615\u4e2a\u6839\u672c\u539f\u56e0\u548c7\u4e2a\u53ef\u89c2\u5bdf\u75c7\u72b6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u53d1\u73b0\u4ee3\u7406\u6846\u67b6bug\u4e3b\u8981\u6e90\u4e8eAPI\u8bef\u7528\u3001API\u4e0d\u517c\u5bb9\u548c\u6587\u6863\u4e0d\u540c\u6b65\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u91c7\u7528\uff0c\u7406\u89e3\u5e95\u5c42\u4ee3\u7406\u6846\u67b6\u4e2d\u7684bug\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u4ee3\u7406\u7ea7\u6545\u969c\uff0c\u5ffd\u89c6\u4e86\u6846\u67b6\u7ea7bug\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5bf9CrewAI\u548cLangChain\u7684998\u4e2abug\u62a5\u544a\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b15\u4e2a\u6839\u672c\u539f\u56e0\u548c7\u4e2a\u53ef\u89c2\u5bdf\u75c7\u72b6\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u6db5\u76d6\u4ee3\u7406\u751f\u547d\u5468\u671f\u7684\u4e94\u4e2a\u9636\u6bb5\uff1a\u4ee3\u7406\u521d\u59cb\u5316\u3001\u611f\u77e5\u3001\u81ea\u6211\u884c\u52a8\u3001\u76f8\u4e92\u4ea4\u4e92\u548c\u6f14\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ee3\u7406\u6846\u67b6bug\u4e3b\u8981\u6e90\u4e8eAPI\u8bef\u7528\u3001API\u4e0d\u517c\u5bb9\u548c\u6587\u6863\u4e0d\u540c\u6b65\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\"\u81ea\u6211\u884c\u52a8\"\u9636\u6bb5\u3002\u75c7\u72b6\u901a\u5e38\u8868\u73b0\u4e3a\u529f\u80fd\u9519\u8bef\u3001\u5d29\u6e83\u548c\u6784\u5efa\u5931\u8d25\uff0c\u53cd\u6620\u4e86\u4efb\u52a1\u8fdb\u5c55\u548c\u63a7\u5236\u6d41\u7684\u4e2d\u65ad\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4ee3\u7406\u6846\u67b6\u7ea7bug\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdbLLM\u4ee3\u7406\u6846\u67b6\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86API\u8bbe\u8ba1\u3001\u517c\u5bb9\u6027\u548c\u6587\u6863\u7ef4\u62a4\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.21547", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21547", "abs": "https://arxiv.org/abs/2602.21547", "authors": ["Yuchong Wu", "Zihuan Xu", "Wangze Ni", "Peng Cheng", "Lei Chen", "Xuemin Lin", "Heng Tao Shen", "Kui Ren"], "title": "RAC: Relation-Aware Cache Replacement for Large Language Models", "comment": null, "summary": "The scaling of Large Language Model (LLM) services faces significant cost and latency challenges, making effective caching under tight capacity crucial. Existing cache replacement policies, from heuristics to learning-based methods, predominantly rely on limited-window statistics such as recency and frequency. We show these signals are not robust for real-world LLM workloads, which exhibit long reuse distances and sparse local recurrence.\n  To address these limitations, we propose Relation-Aware Cache (RAC), an online eviction strategy that leverages semantic relations among requests to guide eviction decisions. RAC synthesizes two relation-aware signals: (1) Topical Prevalence, which aggregates access evidence at the topic level to capture long-horizon reuse; and (2) Structural Importance, which leverages local intra-topic dependency structure to discriminate entries by their future reuse value. Extensive evaluations show that RAC maintains high effectiveness across diverse workloads, consistently surpassing state-of-the-art baselines by 20%--30% in cache hit ratio.", "AI": {"tldr": "RAC\u662f\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5173\u7cfb\u7684LLM\u670d\u52a1\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff0c\u5229\u7528\u4e3b\u9898\u6d41\u884c\u5ea6\u548c\u7ed3\u6784\u91cd\u8981\u6027\u4fe1\u53f7\uff0c\u5728\u6709\u9650\u5bb9\u91cf\u4e0b\u663e\u8457\u63d0\u5347\u7f13\u5b58\u547d\u4e2d\u7387\u3002", "motivation": "LLM\u670d\u52a1\u6269\u5c55\u9762\u4e34\u6210\u672c\u548c\u5ef6\u8fdf\u6311\u6218\uff0c\u73b0\u6709\u7f13\u5b58\u66ff\u6362\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u8fd1\u671f\u6027\u548c\u9891\u7387\u7b49\u6709\u9650\u7a97\u53e3\u7edf\u8ba1\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u5bf9\u73b0\u5b9eLLM\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5177\u6709\u957f\u91cd\u7528\u8ddd\u79bb\u548c\u7a00\u758f\u5c40\u90e8\u91cd\u590d\u6027\uff09\u4e0d\u591f\u9c81\u68d2\u3002", "method": "\u63d0\u51fa\u5173\u7cfb\u611f\u77e5\u7f13\u5b58(RAC)\uff0c\u4e00\u79cd\u5728\u7ebf\u9a71\u9010\u7b56\u7565\uff0c\u5229\u7528\u8bf7\u6c42\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u6307\u5bfc\u9a71\u9010\u51b3\u7b56\u3002RAC\u7efc\u5408\u4e24\u79cd\u5173\u7cfb\u611f\u77e5\u4fe1\u53f7\uff1a1) \u4e3b\u9898\u6d41\u884c\u5ea6\uff1a\u5728\u4e3b\u9898\u5c42\u9762\u805a\u5408\u8bbf\u95ee\u8bc1\u636e\u4ee5\u6355\u83b7\u957f\u65f6\u57df\u91cd\u7528\uff1b2) \u7ed3\u6784\u91cd\u8981\u6027\uff1a\u5229\u7528\u5c40\u90e8\u4e3b\u9898\u5185\u4f9d\u8d56\u7ed3\u6784\u533a\u5206\u6761\u76ee\u7684\u672a\u6765\u91cd\u7528\u4ef7\u503c\u3002", "result": "\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793aRAC\u5728\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u4fdd\u6301\u9ad8\u6709\u6548\u6027\uff0c\u5728\u7f13\u5b58\u547d\u4e2d\u7387\u65b9\u9762\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u57fa\u7ebf20%-30%\u3002", "conclusion": "RAC\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u5173\u7cfb\u4fe1\u53f7\u6709\u6548\u89e3\u51b3\u4e86LLM\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u4f20\u7edf\u7f13\u5b58\u7b56\u7565\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f13\u5b58\u6027\u80fd\u3002"}}
{"id": "2602.21566", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21566", "abs": "https://arxiv.org/abs/2602.21566", "authors": ["Yunhao Mao", "Harunari Takata", "Michail Bachras", "Yuqiu Zhang", "Shiquan Zhang", "Gengrui Zhang", "Hans-Arno Jacobsen"], "title": "Epoch-based Optimistic Concurrency Control in Geo-replicated Databases", "comment": "To appear at SIGMOD 2026", "summary": "Geo-distribution is essential for modern online applications to ensure service reliability and high availability. However, supporting high-performance serializable transactions in geo-replicated databases remains a significant challenge. This difficulty stems from the extensive over-coordination inherent in distributed atomic commitment, concurrency control, and fault-tolerance replication protocols under high network latency.\n  To address these challenges, we introduce Minerva, a unified distributed concurrency control designed for highly scalable multi-leader replication. Minerva employs a novel epoch-based asynchronous replication protocol that decouples data propagation from the commitment process, enabling continuous transaction replication. Optimistic concurrency control is used to allow any replicas to execute transactions concurrently and commit without coordination. In stead of aborting transactions when conflicts are detected, Minerva uses deterministic re-execution to resolve conflicts, ensuring serializability without sacrificing performance. To further enhance concurrency, we construct a conflict graph and use a maximum weight independent set algorithm to select the optimal subset of transactions for commitment, minimizing the number of re-executed transactions. Our evaluation demonstrates that Minerva significantly outperforms state-of-the-art replicated databases, achieving over $3\\times$ higher throughput in scalability experiments and $2.8\\times$ higher throughput during a high network latency simulation with the TPC-C benchmark.", "AI": {"tldr": "Minerva\u662f\u4e00\u4e2a\u7528\u4e8e\u5730\u7406\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u7684\u7edf\u4e00\u5206\u5e03\u5f0f\u5e76\u53d1\u63a7\u5236\u534f\u8bae\uff0c\u901a\u8fc7\u89e3\u8026\u6570\u636e\u4f20\u64ad\u4e0e\u63d0\u4ea4\u8fc7\u7a0b\u3001\u4f7f\u7528\u786e\u5b9a\u6027\u91cd\u6267\u884c\u89e3\u51b3\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5ef6\u8fdf\u7f51\u7edc\u4e0b\u7684\u53ef\u5e8f\u5217\u5316\u4e8b\u52a1\u6027\u80fd\u3002", "motivation": "\u5730\u7406\u5206\u5e03\u5f0f\u5e94\u7528\u9700\u8981\u9ad8\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\uff0c\u4f46\u5728\u9ad8\u5ef6\u8fdf\u7f51\u7edc\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u53ef\u5e8f\u5217\u5316\u4e8b\u52a1\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u4e3b\u8981\u6e90\u4e8e\u5206\u5e03\u5f0f\u539f\u5b50\u63d0\u4ea4\u3001\u5e76\u53d1\u63a7\u5236\u548c\u5bb9\u9519\u590d\u5236\u534f\u8bae\u4e2d\u7684\u8fc7\u5ea6\u534f\u8c03\u95ee\u9898\u3002", "method": "1. \u91c7\u7528\u57fa\u4e8eepoch\u7684\u5f02\u6b65\u590d\u5236\u534f\u8bae\uff0c\u89e3\u8026\u6570\u636e\u4f20\u64ad\u4e0e\u63d0\u4ea4\u8fc7\u7a0b\uff0c\u652f\u6301\u8fde\u7eed\u4e8b\u52a1\u590d\u5236\uff1b2. \u4f7f\u7528\u4e50\u89c2\u5e76\u53d1\u63a7\u5236\uff0c\u5141\u8bb8\u526f\u672c\u5e76\u884c\u6267\u884c\u4e8b\u52a1\u4e14\u65e0\u9700\u534f\u8c03\u63d0\u4ea4\uff1b3. \u901a\u8fc7\u786e\u5b9a\u6027\u91cd\u6267\u884c\u800c\u975e\u4e2d\u6b62\u4e8b\u52a1\u6765\u89e3\u51b3\u51b2\u7a81\uff1b4. \u6784\u5efa\u51b2\u7a81\u56fe\u5e76\u4f7f\u7528\u6700\u5927\u6743\u91cd\u72ec\u7acb\u96c6\u7b97\u6cd5\u9009\u62e9\u6700\u4f18\u4e8b\u52a1\u5b50\u96c6\u8fdb\u884c\u63d0\u4ea4\uff0c\u6700\u5c0f\u5316\u91cd\u6267\u884c\u4e8b\u52a1\u6570\u91cf\u3002", "result": "Minerva\u5728\u53ef\u6269\u5c55\u6027\u5b9e\u9a8c\u4e2d\u6bd4\u6700\u5148\u8fdb\u7684\u590d\u5236\u6570\u636e\u5e93\u5b9e\u73b0\u4e86\u8d85\u8fc73\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728TPC-C\u57fa\u51c6\u6d4b\u8bd5\u7684\u9ad8\u7f51\u7edc\u5ef6\u8fdf\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e862.8\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "Minerva\u901a\u8fc7\u521b\u65b0\u7684\u5f02\u6b65\u590d\u5236\u534f\u8bae\u548c\u786e\u5b9a\u6027\u51b2\u7a81\u89e3\u51b3\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5730\u7406\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u4e2d\u9ad8\u6027\u80fd\u53ef\u5e8f\u5217\u5316\u4e8b\u52a1\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u3002"}}
{"id": "2602.21833", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21833", "abs": "https://arxiv.org/abs/2602.21833", "authors": ["Norman Peitek", "Julia Hess", "Sven Apel"], "title": "From Restructuring to Stabilization: A Large-Scale Experiment on Iterative Code Readability Refactoring with Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly used for automated code refactoring tasks. Although these models can quickly refactor code, the quality may exhibit inconsistencies and unpredictable behavior. In this article, we systematically study the capabilities of LLMs for code refactoring with a specific focus on improving code readability.\n  We conducted a large-scale experiment using GPT5.1 with 230 Java snippets, each systematically varied and refactored regarding code readability across five iterations under three different prompting strategies. We categorized fine-grained code changes during the refactoring into implementation, syntactic, and comment-level transformations. Subsequently, we investigated the functional correctness and tested the robustness of the results with novel snippets.\n  Our results reveal three main insights: First, iterative code refactoring exhibits an initial phase of restructuring followed by stabilization. This convergence tendency suggests that LLMs possess an internalized understanding of an \"optimally readable\" version of code. Second, convergence patterns are fairly robust across different code variants. Third, explicit prompting toward specific readability factors slightly influences the refactoring dynamics.\n  These insights provide an empirical foundation for assessing the reliability of LLM-assisted code refactoring, which opens pathways for future research, including comparative analyses across models and a systematic evaluation of additional software quality dimensions in LLM-refactored code.", "AI": {"tldr": "GPT5.1\u7528\u4e8eJava\u4ee3\u7801\u91cd\u6784\u7684\u7cfb\u7edf\u7814\u7a76\u663e\u793a\uff0c\u8fed\u4ee3\u91cd\u6784\u5448\u73b0\u5148\u91cd\u6784\u540e\u7a33\u5b9a\u7684\u6536\u655b\u6a21\u5f0f\uff0c\u8868\u660eLLM\u5bf9\"\u6700\u4f18\u53ef\u8bfb\u6027\"\u4ee3\u7801\u6709\u5185\u5728\u7406\u89e3\uff0c\u4e14\u4e0d\u540c\u4ee3\u7801\u53d8\u4f53\u95f4\u6536\u655b\u6a21\u5f0f\u7a33\u5065\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u7528\u4e8e\u81ea\u52a8\u5316\u4ee3\u7801\u91cd\u6784\uff0c\u4f46\u5176\u91cd\u6784\u8d28\u91cf\u53ef\u80fd\u5b58\u5728\u4e0d\u4e00\u81f4\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u7814\u7a76LLM\u5728\u4ee3\u7801\u91cd\u6784\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u5173\u6ce8\u63d0\u9ad8\u4ee3\u7801\u53ef\u8bfb\u6027\u3002", "method": "\u4f7f\u7528GPT5.1\u5bf9230\u4e2aJava\u4ee3\u7801\u7247\u6bb5\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u6bcf\u4e2a\u7247\u6bb5\u5728\u4e09\u79cd\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u8fdb\u884c\u4e94\u6b21\u8fed\u4ee3\u91cd\u6784\u3002\u5c06\u91cd\u6784\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u4ee3\u7801\u53d8\u5316\u5206\u4e3a\u5b9e\u73b0\u3001\u8bed\u6cd5\u548c\u6ce8\u91ca\u7ea7\u8f6c\u6362\uff0c\u968f\u540e\u7814\u7a76\u529f\u80fd\u6b63\u786e\u6027\u5e76\u7528\u65b0\u4ee3\u7801\u7247\u6bb5\u6d4b\u8bd5\u7ed3\u679c\u7a33\u5065\u6027\u3002", "result": "1. \u8fed\u4ee3\u4ee3\u7801\u91cd\u6784\u5448\u73b0\u5148\u91cd\u6784\u540e\u7a33\u5b9a\u7684\u6536\u655b\u6a21\u5f0f\uff0c\u8868\u660eLLM\u5bf9\"\u6700\u4f18\u53ef\u8bfb\u6027\"\u4ee3\u7801\u7248\u672c\u6709\u5185\u5728\u7406\u89e3\uff1b2. \u6536\u655b\u6a21\u5f0f\u5728\u4e0d\u540c\u4ee3\u7801\u53d8\u4f53\u95f4\u76f8\u5f53\u7a33\u5065\uff1b3. \u9488\u5bf9\u7279\u5b9a\u53ef\u8bfb\u6027\u56e0\u7d20\u7684\u660e\u786e\u63d0\u793a\u5bf9\u91cd\u6784\u52a8\u6001\u7565\u6709\u5f71\u54cd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8bc4\u4f30LLM\u8f85\u52a9\u4ee3\u7801\u91cd\u6784\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u5305\u62ec\u8de8\u6a21\u578b\u6bd4\u8f83\u5206\u6790\u548c\u7cfb\u7edf\u8bc4\u4f30LLM\u91cd\u6784\u4ee3\u7801\u7684\u5176\u4ed6\u8f6f\u4ef6\u8d28\u91cf\u7ef4\u5ea6\u3002"}}
{"id": "2602.21997", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21997", "abs": "https://arxiv.org/abs/2602.21997", "authors": ["WeiZhe Xu", "Mengyu Liu", "Fanxin Kong"], "title": "Enhancing LLM-Based Test Generation by Eliminating Covered Code", "comment": "9 pages, 4 figures, supplementary material included", "summary": "Automated test generation is essential for software quality assurance, with coverage rate serving as a key metric to ensure thorough testing. Recent advancements in Large Language Models (LLMs) have shown promise in improving test generation, particularly in achieving higher coverage. However, while existing LLM-based test generation solutions perform well on small, isolated code snippets, they struggle when applied to complex methods under test. To address these issues, we propose a scalable LLM-based unit test generation method. Our approach consists of two key steps. The first step is context information retrieval, which uses both LLMs and static analysis to gather relevant contextual information associated with the complex methods under test. The second step, iterative test generation with code elimination, repeatedly generates unit tests for the code slice, tracks the achieved coverage, and selectively removes code segments that have already been covered. This process simplifies the testing task and mitigates issues arising from token limits or reduced reasoning effectiveness associated with excessively long contexts. Through comprehensive evaluations on open-source projects, our approach outperforms state-of-the-art LLM-based and search-based methods, demonstrating its effectiveness in achieving high coverage on complex methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u53ef\u6269\u5c55\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u4fe1\u606f\u68c0\u7d22\u548c\u8fed\u4ee3\u6d4b\u8bd5\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u65b9\u6cd5\u4e0a\u8986\u76d6\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u5728\u5904\u7406\u5c0f\u578b\u4ee3\u7801\u7247\u6bb5\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9762\u5bf9\u590d\u6742\u65b9\u6cd5\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4e0a\u4e0b\u6587\u4fe1\u606f\u68c0\u7d22\uff0c\u7ed3\u5408LLM\u548c\u9759\u6001\u5206\u6790\u83b7\u53d6\u590d\u6742\u65b9\u6cd5\u7684\u76f8\u5173\u4e0a\u4e0b\u6587\u4fe1\u606f\uff1b2) \u8fed\u4ee3\u6d4b\u8bd5\u751f\u6210\u4e0e\u4ee3\u7801\u6d88\u9664\uff0c\u53cd\u590d\u4e3a\u4ee3\u7801\u5207\u7247\u751f\u6210\u6d4b\u8bd5\uff0c\u8ddf\u8e2a\u8986\u76d6\u7387\uff0c\u5e76\u9009\u62e9\u6027\u79fb\u9664\u5df2\u8986\u76d6\u7684\u4ee3\u7801\u6bb5\u3002", "result": "\u5728\u5f00\u6e90\u9879\u76ee\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u65b9\u6cd5\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u4e8eLLM\u548c\u57fa\u4e8e\u641c\u7d22\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8986\u76d6\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u6269\u5c55LLM\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u65b9\u6cd5\u901a\u8fc7\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u8fed\u4ee3\u7b80\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u65b9\u6cd5\u6d4b\u8bd5\u751f\u6210\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8986\u76d6\u7387\u3002"}}
{"id": "2602.21604", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21604", "abs": "https://arxiv.org/abs/2602.21604", "authors": ["Qiange Wang", "Chaoyi Chen", "Jingqi Gao", "Zihan Wang", "Yanfeng Zhang", "Ge Yu"], "title": "Towards Autonomous Graph Data Analytics with Analytics-Augmented Generation", "comment": "8 pages, 7 figures", "summary": "This paper argues that reliable end-to-end graph data analytics cannot be achieved by retrieval- or code-generation-centric LLM agents alone. Although large language models (LLMs) provide strong reasoning capabilities, practical graph analytics for non-expert users requires explicit analytical grounding to support intent-to-execution translation, task-aware graph construction, and reliable execution across diverse graph algorithms. We envision Analytics-Augmented Generation (AAG) as a new paradigm that treats analytical computation as a first-class concern and positions LLMs as knowledge-grounded analytical coordinators. By integrating knowledge-driven task planning, algorithm-centric LLM-analytics interaction, and task-aware graph construction, AAG enables end-to-end graph analytics pipelines that translate natural-language user intent into automated execution and interpretable results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAnalytics-Augmented Generation (AAG)\u65b0\u8303\u5f0f\uff0c\u5c06\u5206\u6790\u8ba1\u7b97\u4f5c\u4e3a\u9996\u8981\u5173\u6ce8\u70b9\uff0c\u8ba9LLM\u4f5c\u4e3a\u77e5\u8bc6\u9a71\u52a8\u7684\u5206\u6790\u534f\u8c03\u8005\uff0c\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u5230\u81ea\u52a8\u5316\u6267\u884c\u7684\u53ef\u89e3\u91ca\u7aef\u5230\u7aef\u56fe\u6570\u636e\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u68c0\u7d22\u6216\u4ee3\u7801\u751f\u6210\u7684LLM\u667a\u80fd\u4f53\u65e0\u6cd5\u5b9e\u73b0\u53ef\u9760\u7684\u7aef\u5230\u7aef\u56fe\u6570\u636e\u5206\u6790\u3002\u867d\u7136LLM\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u9762\u5411\u975e\u4e13\u5bb6\u7528\u6237\u7684\u5b9e\u7528\u56fe\u5206\u6790\u9700\u8981\u660e\u786e\u7684\u5206\u6790\u57fa\u7840\u6765\u652f\u6301\u610f\u56fe\u5230\u6267\u884c\u7684\u8f6c\u6362\u3001\u4efb\u52a1\u611f\u77e5\u7684\u56fe\u6784\u5efa\u4ee5\u53ca\u8de8\u591a\u6837\u56fe\u7b97\u6cd5\u7684\u53ef\u9760\u6267\u884c\u3002", "method": "\u63d0\u51faAnalytics-Augmented Generation (AAG)\u65b0\u8303\u5f0f\uff1a1) \u5c06\u5206\u6790\u8ba1\u7b97\u4f5c\u4e3a\u9996\u8981\u5173\u6ce8\u70b9\uff1b2) \u5c06LLM\u5b9a\u4f4d\u4e3a\u77e5\u8bc6\u9a71\u52a8\u7684\u5206\u6790\u534f\u8c03\u8005\uff1b3) \u96c6\u6210\u77e5\u8bc6\u9a71\u52a8\u7684\u4efb\u52a1\u89c4\u5212\u3001\u7b97\u6cd5\u4e2d\u5fc3\u7684LLM-\u5206\u6790\u4ea4\u4e92\u548c\u4efb\u52a1\u611f\u77e5\u7684\u56fe\u6784\u5efa\u3002", "result": "AAG\u80fd\u591f\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u56fe\u5206\u6790\u6d41\u6c34\u7ebf\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u7528\u6237\u610f\u56fe\u8f6c\u5316\u4e3a\u81ea\u52a8\u5316\u6267\u884c\u548c\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\u3002", "conclusion": "\u9700\u8981\u8d85\u8d8a\u5355\u7eaf\u7684\u68c0\u7d22\u6216\u4ee3\u7801\u751f\u6210\u4e3a\u4e2d\u5fc3\u7684LLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7AAG\u65b0\u8303\u5f0f\u5c06\u5206\u6790\u8ba1\u7b97\u4f5c\u4e3a\u6838\u5fc3\u5173\u6ce8\u70b9\uff0c\u8ba9LLM\u5728\u77e5\u8bc6\u9a71\u52a8\u4e0b\u534f\u8c03\u5206\u6790\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u7aef\u5230\u7aef\u56fe\u6570\u636e\u5206\u6790\u3002"}}
{"id": "2602.22020", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.22020", "abs": "https://arxiv.org/abs/2602.22020", "authors": ["Andr\u00e9s Rodriguez", "Juan Cruz Gardey", "Alejandra Garrido"], "title": "Detecting UX smells in Visual Studio Code using LLMs", "comment": "4 pages, 2 figures, 1 table, 3rd International Workshop on Integrated Development Environments (IDE 2026)", "summary": "Integrated Development Environments shape developers' daily experience, yet the empirical study of their usability and user experience (UX) remains limited. This work presents an LLM-assisted approach to detecting UX smells in Visual Studio Code by mining and classifying user-reported issues from the GitHub repository. Using a validated taxonomy and expert review, we identified recurring UX problems that affect the developer experience. Our results show that the majority of UX smells are concentrated in informativeness, clarity, intuitiveness, and efficiency, qualities that developers value most.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6316\u6398GitHub\u7528\u6237\u53cd\u9988\u6765\u68c0\u6d4bVS Code\u4e2d\u7684UX\u5f02\u5473\uff0c\u53d1\u73b0\u4e3b\u8981\u95ee\u9898\u96c6\u4e2d\u5728\u4fe1\u606f\u6027\u3001\u6e05\u6670\u5ea6\u3001\u76f4\u89c2\u6027\u548c\u6548\u7387\u65b9\u9762", "motivation": "\u96c6\u6210\u5f00\u53d1\u73af\u5883\u5bf9\u5f00\u53d1\u8005\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u5176\u53ef\u7528\u6027\u548c\u7528\u6237\u4f53\u9a8c\u7684\u5b9e\u8bc1\u7814\u7a76\u4ecd\u7136\u6709\u9650\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u5206\u7c7bIDE\u4e2d\u7684UX\u95ee\u9898", "method": "\u91c7\u7528LLM\u8f85\u52a9\u65b9\u6cd5\uff0c\u4eceGitHub\u4ed3\u5e93\u6316\u6398VS Code\u7684\u7528\u6237\u53cd\u9988\u95ee\u9898\uff0c\u4f7f\u7528\u9a8c\u8bc1\u8fc7\u7684\u5206\u7c7b\u6cd5\u548c\u4e13\u5bb6\u8bc4\u5ba1\u6765\u8bc6\u522b\u548c\u5206\u7c7b\u91cd\u590d\u51fa\u73b0\u7684UX\u5f02\u5473", "result": "\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570UX\u5f02\u5473\u96c6\u4e2d\u5728\u4fe1\u606f\u6027\u3001\u6e05\u6670\u5ea6\u3001\u76f4\u89c2\u6027\u548c\u6548\u7387\u65b9\u9762\uff0c\u8fd9\u4e9b\u6b63\u662f\u5f00\u53d1\u8005\u6700\u770b\u91cd\u7684\u8d28\u91cf\u5c5e\u6027", "conclusion": "LLM\u8f85\u52a9\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4bIDE\u4e2d\u7684UX\u95ee\u9898\uff0c\u4e3a\u6539\u8fdb\u5f00\u53d1\u8005\u4f53\u9a8c\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u672a\u6765\u53ef\u6269\u5c55\u5e94\u7528\u5230\u5176\u4ed6\u5f00\u53d1\u5de5\u5177"}}
{"id": "2602.21766", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21766", "abs": "https://arxiv.org/abs/2602.21766", "authors": ["Mohamed Abdelmaksoud", "Sheng Ding", "Andrey Morozov", "Ziawasch Abedjan"], "title": "RAMSeS: Robust and Adaptive Model Selection for Time-Series Anomaly Detection Algorithms", "comment": null, "summary": "Time-series data vary widely across domains, making a universal anomaly detector impractical. Methods that perform well on one dataset often fail to transfer because what counts as an anomaly is context dependent. The key challenge is to design a method that performs well in specific contexts while remaining adaptable across domains with varying data complexities. We present the Robust and Adaptive Model Selection for Time-Series Anomaly Detection RAMSeS framework. RAMSeS comprises two branches: (i) a stacking ensemble optimized with a genetic algorithm to leverage complementary detectors. (ii) An adaptive model-selection branch identifies the best single detector using techniques including Thompson sampling, robustness testing with generative adversarial networks, and Monte Carlo simulations. This dual strategy exploits the collective strength of multiple models and adapts to dataset-specific characteristics. We evaluate RAMSeS and show that it outperforms prior methods on F1.", "AI": {"tldr": "RAMSeS\u6846\u67b6\u901a\u8fc7\u53cc\u5206\u652f\u7b56\u7565\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u8de8\u57df\u9002\u5e94\u6027\u95ee\u9898\uff1a\u4e00\u4e2a\u5206\u652f\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u5806\u53e0\u96c6\u6210\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u4f7f\u7528\u591a\u79cd\u6280\u672f\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u5355\u4e00\u68c0\u6d4b\u5668\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5728\u4e0d\u540c\u9886\u57df\u5dee\u5f02\u5f88\u5927\uff0c\u901a\u7528\u7684\u5f02\u5e38\u68c0\u6d4b\u5668\u4e0d\u5b9e\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4e00\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u96be\u4ee5\u8fc1\u79fb\u5230\u5176\u4ed6\u9886\u57df\uff0c\u56e0\u4e3a\u5f02\u5e38\u7684\u5b9a\u4e49\u662f\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u3002\u5173\u952e\u6311\u6218\u662f\u8bbe\u8ba1\u4e00\u4e2a\u5728\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u540c\u65f6\u80fd\u9002\u5e94\u4e0d\u540c\u9886\u57df\u6570\u636e\u590d\u6742\u6027\u7684\u65b9\u6cd5\u3002", "method": "RAMSeS\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a(1) \u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u7684\u5806\u53e0\u96c6\u6210\uff0c\u5229\u7528\u4e92\u8865\u68c0\u6d4b\u5668\u7684\u4f18\u52bf\uff1b(2) \u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u5206\u652f\uff0c\u4f7f\u7528\u6c64\u666e\u68ee\u91c7\u6837\u3001\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u6d4b\u8bd5\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u7b49\u6280\u672f\u8bc6\u522b\u6700\u4f73\u5355\u4e00\u68c0\u6d4b\u5668\u3002", "result": "\u8bc4\u4f30\u663e\u793aRAMSeS\u5728F1\u5206\u6570\u4e0a\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "RAMSeS\u7684\u53cc\u91cd\u7b56\u7565\u65e2\u5229\u7528\u4e86\u591a\u4e2a\u6a21\u578b\u7684\u96c6\u4f53\u4f18\u52bf\uff0c\u53c8\u80fd\u9002\u5e94\u6570\u636e\u96c6\u7279\u5b9a\u7684\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u7684\u8de8\u57df\u9002\u5e94\u6027\u95ee\u9898\u3002"}}
{"id": "2602.22076", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.22076", "abs": "https://arxiv.org/abs/2602.22076", "authors": ["Eduardo Miranda"], "title": "Visual Milestone Planning in a Hybrid Development Context", "comment": "15 pages, Presented at QUATIC 2023", "summary": "This paper explains the Visual Milestone Planning (VMP) method using an agile vocabulary to facilitate its adoption by agile practitioners as a front end for a hybrid development process. VMP is a visual and collaborative planning approach which promotes a shared understanding of the work approach and commitment through the direct manipulation by team members of the reified planning constructs involved in the development of the plan. Once the product backlog has been established and relevant milestones identified, a novel construct called the milestone planning matrix is used to document the allocation of product backlog items to milestones. The milestones due dates are later determined by grouping sticky notes representing the work to be performed into time-boxes called work packages and accommodating them on a resource and time scaled scheduling canvas very much as it would be done in a Tetris game.", "AI": {"tldr": "VMP\u662f\u4e00\u79cd\u53ef\u89c6\u5316\u91cc\u7a0b\u7891\u89c4\u5212\u65b9\u6cd5\uff0c\u4f7f\u7528\u654f\u6377\u672f\u8bed\u5e2e\u52a9\u654f\u6377\u5b9e\u8df5\u8005\u91c7\u7528\uff0c\u4f5c\u4e3a\u6df7\u5408\u5f00\u53d1\u6d41\u7a0b\u7684\u524d\u7aef\u3002\u5b83\u901a\u8fc7\u56e2\u961f\u76f4\u63a5\u64cd\u4f5c\u89c4\u5212\u6784\u4ef6\u6765\u4fc3\u8fdb\u5bf9\u5de5\u4f5c\u65b9\u6cd5\u7684\u5171\u540c\u7406\u89e3\u548c\u627f\u8bfa\u3002", "motivation": "\u4e3a\u654f\u6377\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e00\u79cd\u6613\u4e8e\u91c7\u7528\u7684\u6df7\u5408\u5f00\u53d1\u6d41\u7a0b\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u534f\u4f5c\u4fc3\u8fdb\u56e2\u961f\u5bf9\u5de5\u4f5c\u65b9\u6cd5\u7684\u5171\u540c\u7406\u89e3\u548c\u627f\u8bfa\u3002", "method": "1) \u5efa\u7acb\u4ea7\u54c1\u5f85\u529e\u4e8b\u9879\u5217\u8868\u548c\u8bc6\u522b\u76f8\u5173\u91cc\u7a0b\u7891\uff1b2) \u4f7f\u7528\u91cc\u7a0b\u7891\u89c4\u5212\u77e9\u9635\u8bb0\u5f55\u4ea7\u54c1\u5f85\u529e\u4e8b\u9879\u5230\u91cc\u7a0b\u7891\u7684\u5206\u914d\uff1b3) \u5c06\u4ee3\u8868\u5de5\u4f5c\u7684\u4fbf\u7b7e\u5206\u7ec4\u5230\u79f0\u4e3a\u5de5\u4f5c\u5305\u7684\u65f6\u95f4\u76d2\u4e2d\uff1b4) \u5728\u8d44\u6e90\u548c\u65f6\u95f4\u7f29\u653e\u7684\u8c03\u5ea6\u753b\u5e03\u4e0a\u5b89\u6392\u5de5\u4f5c\u5305\uff0c\u7c7b\u4f3c\u4fc4\u7f57\u65af\u65b9\u5757\u6e38\u620f\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u654f\u6377\u548c\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u7684\u53ef\u89c6\u5316\u534f\u4f5c\u89c4\u5212\u6846\u67b6\uff0c\u4f7f\u56e2\u961f\u80fd\u591f\u76f4\u89c2\u5730\u521b\u5efa\u548c\u7ba1\u7406\u5f00\u53d1\u8ba1\u5212\u3002", "conclusion": "VMP\u65b9\u6cd5\u4e3a\u654f\u6377\u56e2\u961f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u53ef\u89c6\u5316\u89c4\u5212\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u56e2\u961f\u534f\u4f5c\u548c\u5171\u540c\u7406\u89e3\uff0c\u53ef\u4f5c\u4e3a\u6df7\u5408\u5f00\u53d1\u6d41\u7a0b\u7684\u524d\u7aef\u89c4\u5212\u65b9\u6cd5\u3002"}}
{"id": "2602.21803", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21803", "abs": "https://arxiv.org/abs/2602.21803", "authors": ["Luisa Gerlach", "Tobias K\u00f6ppl", "Ren\u00e8 Zander", "Nicole Schweikardt", "Stefanie Scherzinger"], "title": "Quantum Computing for Query Containment of Conjunctive Queries", "comment": null, "summary": "We address the problem of checking query containment, a foundational problem in database research. Although extensively studied in theory research, optimization opportunities arising from query containment are not fully leveraged in commercial database systems, due to the high computational complexity and sometimes even undecidability of the underlying decision problem. In this article, we present the first approach to applying quantum computing to the query containment problem for conjunctive queries under set semantics. We propose a novel formulation as an optimization problem that can be solved on gate-based quantum hardware, and in some cases directly maps to quantum annealers. We formally prove this formulation to be correct and present a prototype implementation which we evaluate using simulator software as well as quantum devices. Our experiments successfully demonstrate that our approach is sound and scales within the current limitations of quantum hardware. In doing so, we show that quantum optimization can effectively address this problem. Thereby, we contribute a new computational perspective on the query containment problem.", "AI": {"tldr": "\u9996\u6b21\u5c06\u91cf\u5b50\u8ba1\u7b97\u5e94\u7528\u4e8e\u96c6\u5408\u8bed\u4e49\u4e0b\u5408\u53d6\u67e5\u8be2\u7684\u67e5\u8be2\u5305\u542b\u95ee\u9898\uff0c\u63d0\u51fa\u53ef\u8fd0\u884c\u4e8e\u95e8\u57fa\u91cf\u5b50\u786c\u4ef6\u548c\u91cf\u5b50\u9000\u706b\u5668\u7684\u4f18\u5316\u95ee\u9898\u5f62\u5f0f\u5316\u65b9\u6cd5", "motivation": "\u67e5\u8be2\u5305\u542b\u662f\u6570\u636e\u5e93\u7814\u7a76\u7684\u57fa\u7840\u95ee\u9898\uff0c\u867d\u7136\u7406\u8bba\u7814\u7a76\u5e7f\u6cdb\uff0c\u4f46\u7531\u4e8e\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u751a\u81f3\u4e0d\u53ef\u5224\u5b9a\uff0c\u5546\u4e1a\u6570\u636e\u5e93\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u4f18\u5316\u6f5c\u529b\u3002\u91cf\u5b50\u8ba1\u7b97\u4e3a\u6b64\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u5c06\u67e5\u8be2\u5305\u542b\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4f18\u5316\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u5728\u95e8\u57fa\u91cf\u5b50\u786c\u4ef6\u4e0a\u6c42\u89e3\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u76f4\u63a5\u6620\u5c04\u5230\u91cf\u5b50\u9000\u706b\u5668\u3002\u63d0\u4f9b\u4e86\u539f\u578b\u5b9e\u73b0\u5e76\u5728\u6a21\u62df\u5668\u548c\u91cf\u5b50\u8bbe\u5907\u4e0a\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u6210\u529f\u8bc1\u660e\u8be5\u65b9\u6cd5\u6b63\u786e\u4e14\u5728\u5f53\u524d\u91cf\u5b50\u786c\u4ef6\u9650\u5236\u5185\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u4f18\u5316\u80fd\u6709\u6548\u89e3\u51b3\u67e5\u8be2\u5305\u542b\u95ee\u9898\u3002", "conclusion": "\u9996\u6b21\u5c06\u91cf\u5b50\u8ba1\u7b97\u5e94\u7528\u4e8e\u67e5\u8be2\u5305\u542b\u95ee\u9898\uff0c\u4e3a\u8fd9\u4e00\u7ecf\u5178\u6570\u636e\u5e93\u95ee\u9898\u8d21\u732e\u4e86\u65b0\u7684\u8ba1\u7b97\u89c6\u89d2\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u4f18\u5316\u5728\u6b64\u9886\u57df\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.22124", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22124", "abs": "https://arxiv.org/abs/2602.22124", "authors": ["Patrick Tser Jern Kon", "Archana Pradeep", "Ang Chen", "Alexander P. Ellis", "Warren Hunt", "Zijian Wang", "John Yang", "Samuel Thompson"], "title": "SWE-Prot\u00e9g\u00e9: Learning to Selectively Collaborate With an Expert Unlocks Small Language Models as Software Engineering Agents", "comment": null, "summary": "Small language models (SLMs) offer compelling advantages in cost, latency, and adaptability, but have so far lagged behind larger models on long-horizon software engineering tasks such as SWE-bench, where they suffer from pervasive action looping and low resolution rates. We introduce SWE-Prot\u00e9g\u00e9, a post-training framework that reframes software repair as an expert-prot\u00e9g\u00e9 collaboration problem. In SWE-Prot\u00e9g\u00e9, an SLM remains the sole decision-maker while learning to selectively seek guidance from a strong expert model, recognize stalled states, and follow through on expert feedback. Our approach combines supervised fine-tuning on expert-augmented trajectories with agentic reinforcement learning that explicitly discourages degenerative looping and unproductive expert collaboration. We lightly post-train Qwen2.5-Coder-7B-Instruct to achieve 42.4% Pass@1 on SWE-bench Verified, a +25.4% improvement over the prior SLM state of the art, while using expert assistance sparsely (~4 calls per task and 11% of total tokens).", "AI": {"tldr": "SWE-Prot\u00e9g\u00e9\uff1a\u4e00\u4e2a\u901a\u8fc7\u4e13\u5bb6-\u5b66\u5f92\u534f\u4f5c\u6846\u67b6\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u6027\u80fd\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u6210\u672c\u548c\u5ef6\u8fdf\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5728SWE-bench\u7b49\u957f\u89c6\u91ce\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u52a8\u4f5c\u5faa\u73af\u548c\u4f4e\u89e3\u51b3\u7387\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9SLM\u65e2\u80fd\u4fdd\u6301\u51b3\u7b56\u81ea\u4e3b\u6027\uff0c\u53c8\u80fd\u6709\u6548\u5229\u7528\u4e13\u5bb6\u6a21\u578b\u7684\u6307\u5bfc\u3002", "method": "\u63d0\u51faSWE-Prot\u00e9g\u00e9\u6846\u67b6\uff0c\u5c06\u8f6f\u4ef6\u4fee\u590d\u91cd\u6784\u4e3a\u4e13\u5bb6-\u5b66\u5f92\u534f\u4f5c\u95ee\u9898\u3002SLM\u4f5c\u4e3a\u552f\u4e00\u51b3\u7b56\u8005\uff0c\u5b66\u4e60\u9009\u62e9\u6027\u5411\u4e13\u5bb6\u6a21\u578b\u5bfb\u6c42\u6307\u5bfc\u3001\u8bc6\u522b\u505c\u6ede\u72b6\u6001\u3001\u5e76\u9075\u5faa\u4e13\u5bb6\u53cd\u9988\u3002\u7ed3\u5408\u4e13\u5bb6\u589e\u5f3a\u8f68\u8ff9\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u660e\u786e\u6291\u5236\u9000\u5316\u5faa\u73af\u548c\u65e0\u6210\u6548\u7684\u4e13\u5bb6\u534f\u4f5c\u3002", "result": "\u5728Qwen2.5-Coder-7B-Instruct\u4e0a\u8f7b\u91cf\u540e\u8bad\u7ec3\uff0c\u5728SWE-bench Verified\u4e0a\u8fbe\u523042.4% Pass@1\uff0c\u6bd4\u4e4b\u524dSLM\u6700\u4f73\u7ed3\u679c\u63d0\u534725.4%\uff0c\u540c\u65f6\u7a00\u758f\u4f7f\u7528\u4e13\u5bb6\u534f\u52a9\uff08\u7ea64\u6b21\u8c03\u7528/\u4efb\u52a1\uff0c\u5360\u603btoken\u768411%\uff09\u3002", "conclusion": "SWE-Prot\u00e9g\u00e9\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86SLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u52a8\u4f5c\u5faa\u73af\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u7684\u4e13\u5bb6\u534f\u4f5c\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86SLM\u7684\u6210\u672c\u548c\u5ef6\u8fdf\u4f18\u52bf\u3002"}}
{"id": "2602.21955", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.21955", "abs": "https://arxiv.org/abs/2602.21955", "authors": ["Xiu Tang", "Sai Wu", "Dongxiang Zhang", "Feifei Li", "Gang Chen"], "title": "Detecting Logic Bugs of Join Optimizations in DBMS", "comment": null, "summary": "Generation-based testing techniques have shown their effectiveness in detecting logic bugs of DBMS, which are often caused by improper implementation of query optimizers. Nonetheless, existing generation-based debug tools are limited to single-table queries and there is a substantial research gap regarding multi-table queries with join operators. In this paper, we propose TQS, a novel testing framework targeted at detecting logic bugs derived by queries involving multi-table joins. Given a target DBMS, TQS achieves the goal with two key components: Data-guided Schema and Query Generation (DSG) and Knowledge-guided Query Space Exploration (KQE). DSG addresses the key challenge of multi-table query debugging: how to generate ground-truth (query, result) pairs for verification. It adopts the database normalization technique to generate a testing schema and maintains a bitmap index for result tracking. To improve debug efficiency, DSG also artificially inserts some noises into the generated data. To avoid repetitive query space search, KQE forms the problem as isomorphic graph set discovery and combines the graph embedding and weighted random walk for query generation. We evaluated TQS on four popular DBMSs: MySQL, MariaDB, TiDB and the gray release of an industry-leading cloud-native database, anonymized as X-DB. Experimental results show that TQS is effective in finding logic bugs of join optimization in database management systems. It successfully detected 115 bugs within 24 hours, including 31 bugs in MySQL, 30 in MariaDB, 31 in TiDB, and 23 in X-DB respectively.", "AI": {"tldr": "TQS\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u8868\u8fde\u63a5\u67e5\u8be2\u903b\u8f91bug\u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u5f15\u5bfc\u7684\u6a21\u5f0f\u67e5\u8be2\u751f\u6210\u548c\u77e5\u8bc6\u5f15\u5bfc\u7684\u67e5\u8be2\u7a7a\u95f4\u63a2\u7d22\uff0c\u572824\u5c0f\u65f6\u5185\u68c0\u6d4b\u5230115\u4e2abug\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u7684\u6d4b\u8bd5\u5de5\u5177\u4ec5\u9650\u4e8e\u5355\u8868\u67e5\u8be2\uff0c\u5bf9\u4e8e\u6d89\u53ca\u591a\u8868\u8fde\u63a5\u64cd\u4f5c\u7b26\u7684\u67e5\u8be2\u5b58\u5728\u7814\u7a76\u7a7a\u767d\uff0c\u800cDBMS\u4e2d\u7684\u903b\u8f91bug\u901a\u5e38\u7531\u67e5\u8be2\u4f18\u5316\u5668\u7684\u4e0d\u5f53\u5b9e\u73b0\u5f15\u8d77\u3002", "method": "TQS\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u6570\u636e\u5f15\u5bfc\u7684\u6a21\u5f0f\u548c\u67e5\u8be2\u751f\u6210(DSG)\uff0c\u91c7\u7528\u6570\u636e\u5e93\u89c4\u8303\u5316\u6280\u672f\u751f\u6210\u6d4b\u8bd5\u6a21\u5f0f\uff0c\u7ef4\u62a4\u4f4d\u56fe\u7d22\u5f15\u8fdb\u884c\u7ed3\u679c\u8ddf\u8e2a\uff0c\u5e76\u4eba\u4e3a\u63d2\u5165\u566a\u58f0\uff1b2) \u77e5\u8bc6\u5f15\u5bfc\u7684\u67e5\u8be2\u7a7a\u95f4\u63a2\u7d22(KQE)\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u540c\u6784\u56fe\u96c6\u53d1\u73b0\uff0c\u7ed3\u5408\u56fe\u5d4c\u5165\u548c\u52a0\u6743\u968f\u673a\u6e38\u8d70\u8fdb\u884c\u67e5\u8be2\u751f\u6210\u3002", "result": "\u5728MySQL\u3001MariaDB\u3001TiDB\u548c\u884c\u4e1a\u9886\u5148\u7684\u4e91\u539f\u751f\u6570\u636e\u5e93X-DB\u4e0a\u8bc4\u4f30\uff0cTQS\u572824\u5c0f\u65f6\u5185\u6210\u529f\u68c0\u6d4b\u5230115\u4e2a\u903b\u8f91bug\uff1aMySQL 31\u4e2a\u3001MariaDB 30\u4e2a\u3001TiDB 31\u4e2a\u3001X-DB 23\u4e2a\u3002", "conclusion": "TQS\u80fd\u6709\u6548\u68c0\u6d4b\u6570\u636e\u5e93\u7ba1\u7406\u7cfb\u7edf\u4e2d\u8fde\u63a5\u4f18\u5316\u7684\u903b\u8f91bug\uff0c\u586b\u8865\u4e86\u591a\u8868\u8fde\u63a5\u67e5\u8be2\u6d4b\u8bd5\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645DBMS\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
