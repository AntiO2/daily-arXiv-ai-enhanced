<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 22]
- [cs.DB](#cs.DB) [Total: 7]
- [cs.DC](#cs.DC) [Total: 11]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [SIADAFIX: issue description response for adaptive program repair](https://arxiv.org/abs/2510.16059)
*Xin Cao,Nan Yu*

Main category: cs.SE

TL;DR: SIADAFIX是一种基于快慢思维的自适应程序修复方法，通过问题描述响应来指导错误修复代理的工作流程编排，在SWE-bench Lite上达到60.67%的pass@1性能。


<details>
  <summary>Details</summary>
Motivation: 利用快慢思维来增强基于大语言模型的代理在复杂任务（如程序修复）上的能力，平衡修复效率和准确性。

Method: 设计自适应程序修复方法SIADAFIX，使用慢思维错误修复代理完成复杂任务，快思维工作流决策组件优化和分类问题描述，根据问题复杂度自适应选择简单、中等和困难三种修复模式。

Result: 在SWE-bench Lite上的实验结果显示，使用Claude-4 Sonnet模型达到60.67%的pass@1性能，在所有开源方法中达到最先进水平。

Conclusion: SIADAFIX有效平衡了修复效率和准确性，为自动化程序修复提供了新的思路。

Abstract: We propose utilizing fast and slow thinking to enhance the capabilities of
large language model-based agents on complex tasks such as program repair. In
particular, we design an adaptive program repair method based on issue
description response, called SIADAFIX. The proposed method utilizes slow
thinking bug fix agent to complete complex program repair tasks, and employs
fast thinking workflow decision components to optimize and classify issue
descriptions, using issue description response results to guide the
orchestration of bug fix agent workflows. SIADAFIX adaptively selects three
repair modes, i.e., easy, middle and hard mode, based on problem complexity. It
employs fast generalization for simple problems and test-time scaling
techniques for complex problems. Experimental results on the SWE-bench Lite
show that the proposed method achieves 60.67% pass@1 performance using the
Claude-4 Sonnet model, reaching state-of-the-art levels among all open-source
methods. SIADAFIX effectively balances repair efficiency and accuracy,
providing new insights for automated program repair. Our code is available at
https://github.com/liauto-siada/siada-cli.

</details>


### [2] [Code Contribution and Credit in Science](https://arxiv.org/abs/2510.16242)
*Eva Maxfield Brown,Isaac Slaughter,Nicholas Weber*

Main category: cs.SE

TL;DR: 研究发现软件贡献与学术认可之间存在脱节：近30%论文包含未获作者署名的代码贡献者，频繁编码的作者h指数更低，软件贡献未能在传统学术评价体系中获得充分认可。


<details>
  <summary>Details</summary>
Motivation: 理解软件开发活动在科学合作中如何影响学术认可分配，探究软件贡献与传统作者署名之间的关系。

Method: 构建包含14万篇研究论文与代码仓库配对的数据集，开发预测模型匹配论文作者与软件仓库开发者账户，分析软件贡献对学术认可的影响。

Result: 30%论文有非作者代码贡献者；代码贡献作者仅获得4.2%引用增长（控制变量后不显著）；第一作者更可能是代码贡献者；编码频率与h指数呈负相关。

Conclusion: 软件贡献与学术认可之间存在系统性脱节，这对机构奖励体系和科学政策具有重要启示意义。

Abstract: Software development has become essential to scientific research, but its
relationship to traditional metrics of scholarly credit remains poorly
understood. We develop a dataset of approximately 140,000 paired research
articles and code repositories, as well as a predictive model that matches
research article authors with software repository developer accounts. We use
this data to investigate how software development activities influence credit
allocation in collaborative scientific settings. Our findings reveal
significant patterns distinguishing software contributions from traditional
authorship credit. We find that nearly 30% of articles include non-author code
contributors- individuals who participated in software development but received
no formal authorship recognition. While code-contributing authors show a modest
$\sim$4.2% increase in article citations, this effect becomes non-significant
when controlling for domain, article type, and open access status. First
authors are significantly more likely to be code contributors than other author
positions. Notably, we identify a negative relationship between coding
frequency and scholarly impact metrics. Authors who contribute code more
frequently exhibit progressively lower h-indices than non-coding colleagues,
even when controlling for publication count, author position, domain, and
article type. These results suggest a disconnect between software contributions
and credit, highlighting important implications for institutional reward
structures and science policy.

</details>


### [3] [MLCPD: A Unified Multi-Language Code Parsing Dataset with Universal AST Schema](https://arxiv.org/abs/2510.16357)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy*

Main category: cs.SE

TL;DR: MLCPD是一个大规模、语言无关的代码解析数据集，统一了10种主流编程语言的语法和结构表示，包含700多万个解析后的源代码文件，采用统一的抽象语法树模式。


<details>
  <summary>Details</summary>
Motivation: 现有语料库主要关注词法级代码或孤立解析器，缺乏统一的跨语言结构表示，需要一种能够实现跨语言推理、结构学习和多语言软件分析的标准化数据集。

Method: 提出通用抽象语法树模式，对10种编程语言的源代码进行解析和规范化，提供层次化树表示和丰富元数据，确保无损语法覆盖和结构一致性。

Result: 经验分析显示存在强大的跨语言结构规律性，证明Python、Java、Go等不同语言的语法图可以在共享模式下对齐。数据集以Parquet格式存储，便于扩展检索。

Conclusion: MLCPD为跨语言表示学习和程序分析的未来研究建立了开放、可复现的基础，数据集和代码库已在Hugging Face和GitHub上公开发布。

Abstract: We introduce the MultiLang Code Parser Dataset (MLCPD), a large-scale,
language-agnostic dataset unifying syntactic and structural representations of
code across ten major programming languages. MLCPD contains over seven million
parsed source files normalized under our proposed universal Abstract Syntax
Tree (AST) schema, enabling consistent cross-language reasoning, structural
learning, and multilingual software analysis. Unlike existing corpora that
focus purely on token-level code or isolated parsers, MLCPD provides both
hierarchical tree representations and rich metadata for every file, ensuring
lossless syntactic coverage and structural uniformity. Each entry includes a
normalized schema, language-level metadata, and abstracted node semantics
stored in Parquet format for scalable retrieval. Empirical analyses reveal
strong cross-language structural regularities-demonstrating that syntactic
graphs from languages as diverse as Python, Java, and Go can be aligned under a
shared schema. We release the dataset publicly on Hugging Face and the
accompanying codebase on GitHub, which includes complete pipelines for dataset
reproduction, grammar compilation, and a visualization tool for exploring the
unified AST across languages. Together, these resources establish MLCPD as an
open, reproducible foundation for future research in cross-language
representation learning and program analysis.

</details>


### [4] [SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis](https://arxiv.org/abs/2510.16384)
*Yuwei Zhao,Yuan-An Xiao,Qianyu Xiao,Zhao Zhang,Yingfei Xiong*

Main category: cs.SE

TL;DR: SemOpt是一个基于静态程序分析和LLM的代码优化框架，通过构建策略库和生成静态分析规则来精确识别可优化代码段，相比现有方法显著提升了优化效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息检索的代码优化方法由于语义等价但语法不同的代码片段难以被检索到，导致优化性能不佳。

Method: SemOpt包含三个组件：策略库构建器从真实代码修改中提取和聚类优化策略；规则生成器生成Semgrep静态分析规则来捕获优化条件；优化器利用策略库生成优化代码。

Result: 在151个优化任务的基准测试中，SemOpt相比基线将成功优化数量提高了1.38到28倍；在大型C/C++项目中，单个性能指标提升了5.04%到218.07%。

Conclusion: SemOpt通过结合静态程序分析和LLM，有效解决了现有检索方法在代码优化中的局限性，具有实际应用价值。

Abstract: Automated code optimization aims to improve performance in programs by
refactoring code, and recent studies focus on utilizing LLMs for the
optimization. Typical existing approaches mine optimization commits from
open-source codebases to construct a large-scale knowledge base, then employ
information retrieval techniques such as BM25 to retrieve relevant optimization
examples for hotspot code locations, thereby guiding LLMs to optimize these
hotspots. However, since semantically equivalent optimizations can manifest in
syntactically dissimilar code snippets, current retrieval methods often fail to
identify pertinent examples, leading to suboptimal optimization performance.
This limitation significantly reduces the effectiveness of existing
optimization approaches.
  To address these limitations, we propose SemOpt, a novel framework that
leverages static program analysis to precisely identify optimizable code
segments, retrieve the corresponding optimization strategies, and generate the
optimized results. SemOpt consists of three key components: (1) A strategy
library builder that extracts and clusters optimization strategies from
real-world code modifications. (2) A rule generator that generates Semgrep
static analysis rules to capture the condition of applying the optimization
strategy. (3) An optimizer that utilizes the strategy library to generate
optimized code results. All the three components are powered by LLMs.
  On our benchmark containing 151 optimization tasks, SemOpt demonstrates its
effectiveness under different LLMs by increasing the number of successful
optimizations by 1.38 to 28 times compared to the baseline. Moreover, on
popular large-scale C/C++ projects, it can improve individual performance
metrics by 5.04% to 218.07%, demonstrating its practical utility.

</details>


### [5] [Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development](https://arxiv.org/abs/2510.16395)
*Xin Peng,Chong Wang*

Main category: cs.SE

TL;DR: 提出代码数字孪生框架，将AI能力与企业软件开发现实对齐，解决隐性知识流失问题，支持复杂系统的可持续智能演进。


<details>
  <summary>Details</summary>
Motivation: 企业软件开发主要依赖增量演进，挑战远超常规编码，关键取决于隐性知识（设计决策、历史权衡）。现有LLM能力与企业开发实践存在差距，需要有效整合。

Method: 提出代码数字孪生框架，建模软件的物理和概念层，保存隐性知识，与代码库共同演进。集成混合知识表示、多阶段提取管道、增量更新、LLM赋能应用和人机协同反馈。

Result: 将碎片化知识转化为明确可操作表示，为AI进步与企业软件现实搭建桥梁。

Conclusion: 代码数字孪生为实现超复杂系统的可持续、智能和弹性开发演进提供了具体路线图，是连接AI能力与企业开发需求的关键框架。

Abstract: Recent advances in large language models (LLMs) have demonstrated strong
capabilities in software engineering tasks, raising expectations of
revolutionary productivity gains. However, enterprise software development is
largely driven by incremental evolution, where challenges extend far beyond
routine coding and depend critically on tacit knowledge, including design
decisions at different levels and historical trade-offs. To achieve effective
AI-powered support for complex software development, we should align emerging
AI capabilities with the practical realities of enterprise development. To this
end, we systematically identify challenges from both software and LLM
perspectives. Alongside these challenges, we outline opportunities where AI and
structured knowledge frameworks can enhance decision-making in tasks such as
issue localization and impact analysis. To address these needs, we propose the
Code Digital Twin, a living framework that models both the physical and
conceptual layers of software, preserves tacit knowledge, and co-evolves with
the codebase. By integrating hybrid knowledge representations, multi-stage
extraction pipelines, incremental updates, LLM-empowered applications, and
human-in-the-loop feedback, the Code Digital Twin transforms fragmented
knowledge into explicit and actionable representations. Our vision positions it
as a bridge between AI advancements and enterprise software realities,
providing a concrete roadmap toward sustainable, intelligent, and resilient
development and evolution of ultra-complex systems.

</details>


### [6] [Large-Scale Empirical Analysis of Continuous Fuzzing: Insights from 1 Million Fuzzing Sessions](https://arxiv.org/abs/2510.16433)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Yasutaka Kamei,Hajimu Iida*

Main category: cs.SE

TL;DR: 本研究通过分析OSS-Fuzz的878个项目约112万次模糊测试会话，揭示了持续模糊测试在漏洞检测中的作用：早期检测率高、代码覆盖率持续增长、覆盖率变化有助于漏洞发现。


<details>
  <summary>Details</summary>
Motivation: 尽管持续模糊测试已被数千个项目采用，但其在漏洞检测中的具体贡献尚不清楚。本研究旨在阐明持续模糊测试在漏洞检测中的作用。

Method: 收集OSS-Fuzz的问题报告、覆盖率报告和模糊测试日志，对878个项目约112万次模糊测试会话进行实证研究。

Result: 发现：大量模糊测试漏洞在持续模糊测试集成前就已存在，导致早期检测率高；代码覆盖率随持续模糊测试进展持续增加；覆盖率变化有助于模糊测试漏洞的检测。

Conclusion: 本研究为持续模糊测试在漏洞检测中的贡献提供了实证见解，对未来持续模糊测试策略和工具开发具有实际意义。

Abstract: Software vulnerabilities are constantly being reported and exploited in
software products, causing significant impacts on society. In recent years, the
main approach to vulnerability detection, fuzzing, has been integrated into the
continuous integration process to run in short and frequent cycles. This
continuous fuzzing allows for fast identification and remediation of
vulnerabilities during the development process. Despite adoption by thousands
of projects, however, it is unclear how continuous fuzzing contributes to
vulnerability detection. This study aims to elucidate the role of continuous
fuzzing in vulnerability detection. Specifically, we investigate the coverage
and the total number of fuzzing sessions when fuzzing bugs are discovered. We
collect issue reports, coverage reports, and fuzzing logs from OSS-Fuzz, an
online service provided by Google that performs fuzzing during continuous
integration. Through an empirical study of a total of approximately 1.12
million fuzzing sessions from 878 projects participating in OSS-Fuzz, we reveal
that (i) a substantial number of fuzzing bugs exist prior to the integration of
continuous fuzzing, leading to a high detection rate in the early stages; (ii)
code coverage continues to increase as continuous fuzzing progresses; and (iii)
changes in coverage contribute to the detection of fuzzing bugs. This study
provides empirical insights into how continuous fuzzing contributes to fuzzing
bug detection, offering practical implications for future strategies and tool
development in continuous fuzzing.

</details>


### [7] [On the Use of Large Language Models for Qualitative Synthesis](https://arxiv.org/abs/2510.16502)
*Sebastián Pizard,Ramiro Moreira,Federico Galiano,Ignacio Sastre,Lorena Etcheverry*

Main category: cs.SE

TL;DR: 本文探讨了在系统评价中使用大语言模型进行定性综合的挑战，通过协作式自我民族志研究评估了LLMs在此过程中的方法论严谨性和实际有用性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在支持系统评价方面显示出潜力，特别是复杂的定性综合任务，但将其应用于报告不均衡且执行多变的阶段存在重要风险：误用可能放大现有弱点并削弱对系统评价结果的信心。

Method: 采用协作式自我民族志方法，进行了两项试验，从方法论严谨性和实际有用性角度评估每个试验，并通过技术视角解释结果。

Result: 研究揭示了使用LLMs进行定性综合的具体挑战，包括方法论严谨性和实际应用价值方面的问题。

Conclusion: 需要谨慎应用大语言模型于系统评价的定性综合阶段，以避免放大现有弱点并确保研究结果的可靠性。

Abstract: Large language models (LLMs) show promise for supporting systematic reviews
(SR), even complex tasks such as qualitative synthesis (QS). However, applying
them to a stage that is unevenly reported and variably conducted carries
important risks: misuse can amplify existing weaknesses and erode confidence in
the SR findings. To examine the challenges of using LLMs for QS, we conducted a
collaborative autoethnography involving two trials. We evaluated each trial for
methodological rigor and practical usefulness, and interpreted the results
through a technical lens informed by how LLMs are built and their current
limitations.

</details>


### [8] [Human-Aligned Code Readability Assessment with Large Language Models](https://arxiv.org/abs/2510.16579)
*Wendkûuni C. Ouédraogo,Yinghua Li,Xueqi Dang,Pawel Borsukiewicz,Xin Zhou,Anil Koyuncu,Jacques Klein,David Lo,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: CoReEval是首个大规模评估LLM代码可读性评估能力的基准，包含140万次模型-代码片段-提示评估，涵盖10个先进LLM、3种编程语言、2种代码类型、4种提示策略和9种解码设置。研究发现基于人类定义可读性维度的开发者引导提示能提高对齐度和解释质量。


<details>
  <summary>Details</summary>
Motivation: 代码可读性对软件理解和维护至关重要，但难以大规模评估。传统静态指标无法捕捉人类判断的主观性和上下文敏感性，而LLM作为可扩展替代方案的行为尚未充分探索。

Method: 构建CoReEval基准，包含140万次评估，涵盖10个LLM、3种编程语言(Java、Python、CUDA)、2种代码类型(功能代码和单元测试)、4种提示策略(ZSL、FSL、CoT、ToT)、9种解码设置，以及针对初级和高级开发者角色的开发者引导提示。将LLM输出与人类标注和验证的静态模型进行比较。

Result: 基于人类定义可读性维度的开发者引导提示在结构化环境中提高了对齐度，增强了解释质量，并通过角色框架实现轻量级个性化。但评分变异性增加揭示了对齐度、稳定性和可解释性之间的权衡。

Conclusion: CoReEval为提示工程、模型对齐研究和人机协作评估提供了坚实基础，在教育、入职培训和CI/CD管道中，LLM可作为可解释、适应性强的评审者。

Abstract: Code readability is crucial for software comprehension and maintenance, yet
difficult to assess at scale. Traditional static metrics often fail to capture
the subjective, context-sensitive nature of human judgments. Large Language
Models (LLMs) offer a scalable alternative, but their behavior as readability
evaluators remains underexplored. We introduce CoReEval, the first large-scale
benchmark for evaluating LLM-based code readability assessment, comprising over
1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs.
The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types
(functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT),
9 decoding settings, and developer-guided prompts tailored to junior and senior
personas. We compare LLM outputs against human annotations and a validated
static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and
justification quality (sentiment, aspect coverage, semantic clustering). Our
findings show that developer-guided prompting grounded in human-defined
readability dimensions improves alignment in structured contexts, enhances
explanation quality, and enables lightweight personalization through persona
framing. However, increased score variability highlights trade-offs between
alignment, stability, and interpretability. CoReEval provides a robust
foundation for prompt engineering, model alignment studies, and human in the
loop evaluation, with applications in education, onboarding, and CI/CD
pipelines where LLMs can serve as explainable, adaptable reviewers.

</details>


### [9] [Contrasting the Hyperparameter Tuning Impact Across Software Defect Prediction Scenarios](https://arxiv.org/abs/2510.16665)
*Mohamed Sami Rakha,Andriy Miranskyy,Daniel Alencar da Costa*

Main category: cs.SE

TL;DR: 该研究对比了超参数调优在两种软件缺陷预测场景（IVDP和CVDP）中的影响差异，发现IVDP场景中的性能提升显著大于CVDP场景，且小数据集更容易受到性能影响差异的影响。


<details>
  <summary>Details</summary>
Motivation: 虽然超参数调优可以提升软件缺陷预测性能，但其效果可能因不同的SDP场景而异。为了提供全面见解并增强SDP建模的鲁棒性和实用性，需要比较超参数调优在不同SDP场景中的影响。

Method: 使用28种机器学习算法、53个发布后软件数据集、两种调优算法和五种优化指标，通过统计分析方法对比IVDP和CVDP两种场景中超参数调优的整体影响、单算法影响以及不同数据集大小下的变化。

Result: IVDP场景中的SDP性能增益显著大于CVDP场景；28种ML算法中有24种算法的性能增益声明可能无法跨多个SDP场景成立；小数据集更容易出现较大的性能影响差异。

Conclusion: 研究建议软件工程研究者和从业者在期望从超参数调优获得性能增益时，应考虑所选SDP场景的影响。

Abstract: Software defect prediction (SDP) is crucial for delivering high-quality
software products. Recent research has indicated that prediction performance
improvements in SDP are achievable by applying hyperparameter tuning to a
particular SDP scenario. However, the positive impact resulting from the
hyperparameter tuning step may differ based on the targeted SDP scenario.
Comparing the impact of hyperparameter tuning across SDP scenarios is necessary
to provide comprehensive insights and enhance the robustness, generalizability,
and, eventually, the practicality of SDP modeling for quality assurance.
  Therefore, in this study, we contrast the impact of hyperparameter tuning
across two pivotal and consecutive SDP scenarios: (1) Inner Version Defect
Prediction (IVDP) and (2) Cross Version Defect Prediction (CVDP). The main
distinctions between the two scenarios lie in the scope of defect prediction
and the selected evaluation setups. This study's experiments use common
evaluation setups, 28 machine learning (ML) algorithms, 53 post-release
software datasets, two tuning algorithms, and five optimization metrics. We
apply statistical analytics to compare the SDP performance impact differences
by investigating the overall impact, the single ML algorithm impact, and
variations across different software dataset sizes.
  The results indicate that the SDP gains within the IVDP scenario are
significantly larger than those within the CVDP scenario. The results reveal
that asserting performance gains for up to 24 out of 28 ML algorithms may not
hold across multiple SDP scenarios. Furthermore, we found that small software
datasets are more susceptible to larger differences in performance impacts.
Overall, the study findings recommend software engineering researchers and
practitioners to consider the effect of the selected SDP scenario when
expecting performance gains from hyperparameter tuning.

</details>


### [10] [QuanBench: Benchmarking Quantum Code Generation with Large Language Models](https://arxiv.org/abs/2510.16779)
*Xiaoyu Guo,Minggu Wang,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了QuanBench基准测试，用于评估大语言模型在量子代码生成方面的能力，包含44个编程任务，涵盖量子算法、状态准备、门分解和量子机器学习等领域。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用代码生成方面表现良好，但在量子代码生成方面的能力尚未得到充分研究，需要专门的基准测试来评估。

Method: 构建包含44个编程任务的QuanBench基准，每个任务都有可执行的规范解决方案，通过功能正确性(Pass@K)和量子语义等价性(过程保真度)进行评估。

Result: 评估结果显示当前LLMs在生成正确量子代码方面能力有限，总体准确率低于40%，经常出现语义错误，常见失败案例包括过时的API使用、电路构建错误和算法逻辑错误。

Conclusion: QuanBench为未来改进LLMs在量子代码生成方面的工作提供了基础，当前模型在量子编程领域仍存在显著局限性。

Abstract: Large language models (LLMs) have demonstrated good performance in general
code generation; however, their capabilities in quantum code generation remain
insufficiently studied. This paper presents QuanBench, a benchmark for
evaluating LLMs on quantum code generation. QuanBench includes 44 programming
tasks that cover quantum algorithms, state preparation, gate decomposition, and
quantum machine learning. Each task has an executable canonical solution and is
evaluated by functional correctness (Pass@K) and quantum semantic equivalence
(Process Fidelity). We evaluate several recent LLMs, including general-purpose
and code-specialized models. The results show that current LLMs have limited
capability in generating the correct quantum code, with overall accuracy below
40% and frequent semantic errors. We also analyze common failure cases, such as
outdated API usage, circuit construction errors, and incorrect algorithm logic.
QuanBench provides a basis for future work on improving quantum code generation
with LLMs.

</details>


### [11] [More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents](https://arxiv.org/abs/2510.16786)
*Pengfei Gao,Chao Peng*

Main category: cs.SE

TL;DR: 本文研究了LLM编码代理的轮次控制策略，发现动态轮次策略在保持性能的同时能显著降低成本，为部署经济可行的编码代理提供了有效指导。


<details>
  <summary>Details</summary>
Motivation: LLM编码代理在实际部署中面临显著且不可预测的成本问题，主要由于轮次数量的指数级增长、模型价格高昂、现实任务需要大量轮次以及代理采取低效行动等因素。现有研究主要关注单轮优化，而对总轮次数的战略控制研究不足。

Method: 在SWE-bench上使用三种最先进模型进行实证研究，评估三种轮次控制策略：无限制基线、带提醒的固定轮次限制、以及按需扩展的新型动态轮次策略。

Result: 研究发现：1) 无限制设置中存在基本权衡，没有单一模型在性能、成本和轮次效率方面都表现优异；2) 固定轮次限制（基线第75百分位）是"最佳点"，可大幅降低成本（24%-68%）且对解决率影响最小；3) 动态轮次策略始终优于固定限制方法，在保持或提高解决率的同时，通过智能分配资源进一步降低成本12%-24%。

Conclusion: 动态资源分配是部署强大且经济可行的编码代理的优越且易于实施的方法，为开发者平衡成本与效能提供了简单有效的指导。

Abstract: LLM-powered coding agents, which operate in iterative loops (turns) to solve
software engineering tasks, are becoming increasingly powerful. However, their
practical deployment is hindered by significant and unpredictable costs. This
challenge arises from a combination of factors: quadratically growing token
counts with each turn, the high price of models, the large number of turns
required for real-world tasks, and the tendency of agents to take inefficient
or unnecessary actions. While existing research focuses on optimizing
individual turns, the strategic control of the total number of turns remains an
underexplored area for managing agent performance and cost. To address this
gap, we conduct a comprehensive empirical study on SWE-bench using three
state-of-the-art models and evaluate the impact of three distinct turn-control
strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a
novel dynamic-turn strategy that grants extensions on-demand. Our findings
first reveal a fundamental trade-off in the unrestricted setting, where no
single model excels across performance, cost, and turn efficiency. We then show
that a fixed-turn limit, specifically at the 75th percentile of the baseline,
serves as a "sweet spot", substantially reducing costs (by 24%-68%) with
minimal impact on solve rates. Most significantly, the dynamic-turn strategy
consistently outperforms fixed-limit approaches, achieving comparable or better
solve rates while further reducing costs by an additional 12%-24% by
intelligently allocating resources only to tasks that need them. This work
provides the first systematic analysis of turn-control strategies, offering
simple yet effective guidelines for developers to balance cost and efficacy. We
demonstrate that dynamic resource allocation is a superior, easy-to-implement
approach for deploying powerful yet economically viable coding agents.

</details>


### [12] [When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation](https://arxiv.org/abs/2510.16809)
*Amirkia Rafiei Oskooei,Kaan Baturalp Cosdan,Husamettin Isiktas,Mehmet S. Aktas*

Main category: cs.SE

TL;DR: 论文研究发现代码翻译任务中存在'多示例悖论'：虽然静态相似度指标随示例数量增加略有提升，但功能性正确性在少样本提示（5-25个示例）时达到峰值，过多示例反而会降低性能。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在代码翻译任务中，增加上下文示例数量（从零样本到多样本）对性能的影响，验证'示例越多越好'的假设是否适用于复杂任务。

Method: 通过大规模实证研究，分析超过90,000个翻译实例，系统评估从零样本到625个示例（约10万到80万tokens）的不同提示配置。

Result: 发现功能性正确性在5-25个示例时达到最佳，过多示例会降低性能；静态相似度指标随示例数量增加仅有轻微改善。

Conclusion: 对于代码翻译任务，少量精心选择的示例质量比数量更重要，挑战了ICL中'越多越好'的普遍有效性，强调了最优提示策略的任务依赖性。

Abstract: Large Language Models (LLMs) with vast context windows offer new avenues for
in-context learning (ICL), where providing many examples ("many-shot"
prompting) is often assumed to enhance performance. We investigate this
assumption for the complex task of code translation. Through a large-scale
empirical study of over 90,000 translations, we systematically evaluate the
impact of scaling in-context examples from zero-shot to many-shot
configurations of up to 625 examples, with prompts spanning from approximately
100,000 to 800,000 tokens. Our findings reveal a "many-shot paradox": while
static similarity metrics may modestly improve with more examples, functional
correctness consistently peaks with few-shot prompting (5-25 examples).
Providing substantially more examples often degrades this crucial functional
performance. This study highlights that for code translation, the quality of a
few well-chosen examples outweighs sheer quantity, challenging the universal
efficacy of "more is better" for ICL and underscoring the task-dependent nature
of optimal prompting strategies. Our results have significant implications for
effectively leveraging LLMs in software engineering.

</details>


### [13] [When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation](https://arxiv.org/abs/2510.16823)
*Yue Liu,Zhenchang Xing,Shidong Pan,Chakkrit Tantithamthavorn*

Main category: cs.SE

TL;DR: LLM生成的Chrome扩展程序存在严重安全漏洞，在认证与身份管理、Cookie管理等场景中漏洞率高达83%和78%，大多数漏洞会暴露敏感浏览器数据。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件开发中广泛应用，开发者可能只关注程序功能而忽略实现中的安全问题，需要研究LLM生成的框架约束程序的安全性。

Method: 构建ChromeSecBench数据集（140个基于已知漏洞扩展的提示），使用9个先进LLM生成完整Chrome扩展，从场景类型、模型差异和漏洞类别三个维度分析安全性。

Result: LLM生成易受攻击程序的比例惊人（18%-50%），在认证与身份管理场景漏洞率高达83%，Cookie管理场景达78%。高级推理模型表现更差，生成的漏洞更多。

Conclusion: LLM的编码能力与编写安全框架约束程序的能力存在关键差距，需要加强LLM的安全意识训练。

Abstract: In recent years, the AI wave has grown rapidly in software development. Even
novice developers can now design and generate complex framework-constrained
software systems based on their high-level requirements with the help of Large
Language Models (LLMs). However, when LLMs gradually "take the wheel" of
software development, developers may only check whether the program works. They
often miss security problems hidden in how the generated programs are
implemented.
  In this work, we investigate the security properties of framework-constrained
programs generated by state-of-the-art LLMs. We focus specifically on Chrome
extensions due to their complex security model involving multiple privilege
boundaries and isolated components. To achieve this, we built ChromeSecBench, a
dataset with 140 prompts based on known vulnerable extensions. We used these
prompts to instruct nine state-of-the-art LLMs to generate complete Chrome
extensions, and then analyzed them for vulnerabilities across three dimensions:
scenario types, model differences, and vulnerability categories. Our results
show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%),
particularly in Authentication & Identity and Cookie Management scenarios (up
to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser
data like cookies, history, or bookmarks to untrusted code. Interestingly, we
found that advanced reasoning models performed worse, generating more
vulnerabilities than simpler models. These findings highlight a critical gap
between LLMs' coding skills and their ability to write secure
framework-constrained programs.

</details>


### [14] [Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection](https://arxiv.org/abs/2510.17056)
*Luis F. G. Campos,Leonardo C. Marques,Walter T. Nakamura*

Main category: cs.SE

TL;DR: 本研究比较了生成式AI与人类专家在可用性检查中的表现，发现AI虽然能发现新缺陷但误报率高，AI与人类结合可获得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 可用性检查成本高且需要专业知识，随着AI技术的发展，探索AI在可用性检查中的潜力以提升效率和降低成本。

Method: 使用软件原型，由4名专家和2个AI模型(GPT-4o和Gemini 2.5 Flash)进行评估，采用精确率、召回率和F1分数等指标。

Result: 人类检查者在精确率和总体覆盖率方面表现最佳，AI表现出高个体性能并发现许多新缺陷，但误报率和冗余报告较高。AI与人类结合产生最佳结果。

Conclusion: AI目前无法替代人类检查者，但可作为有价值的增强工具来提高效率并扩大缺陷覆盖范围，两者具有互补性。

Abstract: Usability inspection is a well-established technique for identifying
interaction issues in software interfaces, thereby contributing to improved
product quality. However, it is a costly process that requires time and
specialized knowledge from inspectors. With advances in Artificial Intelligence
(AI), new opportunities have emerged to support this task, particularly through
generative models capable of interpreting interfaces and performing inspections
more efficiently. This study examines the performance of generative AIs in
identifying usability problems, comparing them to those of experienced human
inspectors. A software prototype was evaluated by four specialists and two AI
models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall,
and F1-score. While inspectors achieved the highest levels of precision and
overall coverage, the AIs demonstrated high individual performance and
discovered many novel defects, but with a higher rate of false positives and
redundant reports. The combination of AIs and human inspectors produced the
best results, revealing their complementarity. These findings suggest that AI,
in its current stage, cannot replace human inspectors but can serve as a
valuable augmentation tool to improve efficiency and expand defect coverage.
The results provide evidence based on quantitative analysis to inform the
discussion on the role of AI in usability inspections, pointing to viable paths
for its complementary use in software quality assessment contexts.

</details>


### [15] [M2QCode: A Model-Driven Framework for Generating Multi-Platform Quantum Programs](https://arxiv.org/abs/2510.17110)
*Xiaoyu Guo,Shinobu Saito,Jianjun Zhao*

Main category: cs.SE

TL;DR: 提出了一种基于模型驱动开发（MDD）的方法，用于支持量子系统的结构化设计和实现，能够自动生成多种量子编程语言的代码。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，量子编程语言不断涌现，但模型驱动开发在量子系统工程中的应用仍未被充分探索。

Method: 开发了一个MDD框架，支持自动为多种量子编程语言生成量子代码。

Result: 通过多个案例研究证明了该方法的有效性和实用性。

Conclusion: 该MDD方法能够提高量子系统开发效率，并确保在异构量子平台上的一致性。

Abstract: With the growing interest in quantum computing, the emergence of quantum
supremacy has marked a pivotal milestone in the field. As a result, numerous
quantum programming languages (QPLs) have been introduced to support the
development of quantum algorithms. However, the application of Model-Driven
Development (MDD) in quantum system engineering remains largely underexplored.
This paper presents an MDD-based approach to support the structured design and
implementation of quantum systems. Our framework enables the automatic
generation of quantum code for multiple QPLs, thereby enhancing development
efficiency and consistency across heterogeneous quantum platforms. The
effectiveness and practicality of our approach have been demonstrated through
multiple case studies.

</details>


### [16] [SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning](https://arxiv.org/abs/2510.17130)
*Shuzheng Gao,Chaozheng Wang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: SEER是一个自探索深度推理框架，将代码生成的思维链过程建模为决策问题，通过多样化推理路径探索、质量感知模型训练和自适应推理来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理方法在代码生成中存在三个关键局限：推理路径多样性不足、中间步骤质量评估缺失、以及'过度思考'可能导致复杂错误解决方案。

Method: SEER包含三个核心组件：多样化推理路径探索（无需专家或闭源模型）、推理质量感知模型训练（策略模型生成推理步骤，价值模型评估质量）、自适应思维链推理（根据问题动态切换直接生成与逐步推理）。

Result: 论文未提供具体实验结果，但从方法描述来看，SEER框架旨在提高代码生成的准确性和适应性。

Conclusion: SEER通过将思维链代码生成建模为决策问题，提供了一种更准确和自适应的推理方法，解决了现有方法在多样性、质量和效率方面的局限性。

Abstract: Code generation, the task of creating executable programs from natural
language requirements, has recently seen tremendous advances through
Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to
develop high-level reasoning plans before writing code. Recent research has
proposed various methods to enhance models' CoT reasoning for code generation
such as prompt engineering and supervised fine-tuning. However, existing
approaches still face three critical limitations: (1) limited exploration of
diverse reasoning paths, which constrains generalization across various
programming scenarios, (2) lack of quality assessment for intermediate
reasoning steps, which hampers the reliability of the generated plans and code,
and (3) the potential negative impact of "overthinking", potentially leading to
unnecessarily complex and incorrect solutions. To address these limitations, we
frame CoT code generation as a decision making problem and present SEER, a
SElf-Exploring deep Reasoning framework that enables accurate and adaptive
reasoning for code generation. SEER introduces three key components: (1)
Diverse reasoning path exploration, which aims at exploring diverse reasoning
paths and annotating intermediate steps without relying on manual experts or
closed-source proprietary models; (2) Reasoning quality-aware model training,
which trains a policy model for generating candidate reasoning steps and a
value model for assessing their quality; and (3) Adaptive CoT reasoning, which
dynamically switches between direct generation and step-by-step reasoning for
different problems.

</details>


### [17] [PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing](https://arxiv.org/abs/2510.17142)
*Xiaoxue Ren,Jun Wan,Yun Peng,Zhongxin Liu,Ming Liang,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: Peace是一个用于项目级代码效率优化的混合框架，通过自动代码编辑实现函数间交互优化，在真实项目中显著提升执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码优化方法仅关注函数级优化，忽视了函数间交互，无法适应真实开发场景。代码编辑技术虽具潜力，但面临无效编辑和次优内部函数的挑战。

Method: Peace框架包含三个关键阶段：依赖感知的优化函数序列构建、有效关联编辑识别、效率优化编辑迭代，确保项目整体正确性和完整性。

Result: 在PeacExec基准测试中，Peace达到69.2%正确率、+46.9%优化率、0.840执行效率加速，在复杂多函数优化任务中显著优于现有方法。

Conclusion: Peace证明了项目级代码效率优化的可行性，其混合框架设计有效解决了现有方法的局限性，为LLM在代码优化领域的应用提供了新方向。

Abstract: Large Language Models (LLMs) have demonstrated significant capability in code
generation, but their potential in code efficiency optimization remains
underexplored. Previous LLM-based code efficiency optimization approaches
exclusively focus on function-level optimization and overlook interaction
between functions, failing to generalize to real-world development scenarios.
Code editing techniques show great potential for conducting project-level
optimization, yet they face challenges associated with invalid edits and
suboptimal internal functions. To address these gaps, we propose Peace, a novel
hybrid framework for Project-level code Efficiency optimization through
Automatic Code Editing, which also ensures the overall correctness and
integrity of the project. Peace integrates three key phases: dependency-aware
optimizing function sequence construction, valid associated edits
identification, and efficiency optimization editing iteration. To rigorously
evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark
comprising 146 real-world optimization tasks from 47 high-impact GitHub Python
projects, along with highly qualified test cases and executable environments.
Extensive experiments demonstrate Peace's superiority over the state-of-the-art
baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and
0.840 speedup in execution efficiency. Notably, our Peace outperforms all
baselines by significant margins, particularly in complex optimization tasks
with multiple functions. Moreover, extensive experiments are also conducted to
validate the contributions of each component in Peace, as well as the rationale
and effectiveness of our hybrid framework design.

</details>


### [18] [TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework](https://arxiv.org/abs/2510.17163)
*Shuzheng Gao,Eric John Li,Man Ho Lam,Jingyu Xiao,Yuxuan Wan,Chaozheng Wang,Ng Man Tik,Michael R. Lyu*

Main category: cs.SE

TL;DR: 提出了TREAT评估框架，用于全面评估代码大模型在真实软件工程场景中的可信度，解决了现有基准测试任务范围有限、缺乏鲁棒性评估等问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估代码大模型可信度方面存在局限，任务范围有限且缺乏对模型鲁棒性和可靠性的评估，需要更全面的评估框架。

Method: 开发TREAT评估框架，包含四个主要改进：多任务整体评估、多语言多模态评估、鲁棒性评估（语义保持的代码转换）、严谨的评估方法（多样化提示和自适应解决方案提取）。

Result: 评估了26个最先进模型，发现：当前模型在不同编程任务中表现差异显著；多模态语言模型在UI代码生成和编辑方面存在特定性能限制。

Conclusion: TREAT框架为代码大模型的可信度评估提供了更全面的方法，揭示了当前模型的性能差异和特定限制，为未来模型改进提供了重要见解。

Abstract: Large foundation models are fundamentally transforming the software
engineering landscape, demonstrating exceptional capabilities across diverse
tasks such as code generation, debugging, and testing. Despite this rapid
progress, a significant gap remains in how to comprehensively evaluate these
models' trustworthiness in real-world software engineering scenarios. Existing
benchmarks suffer from limited task scope and fail to incorporate critical
evaluation aspects such as the robustness and reliability of models. To bridge
this gap, we present an evaluation framework called TREAT (Code LLMs
Trustworthiness / Reliability Evaluation And Testing) that provides a holistic
assessment of model performance in code intelligence tasks. Our evaluation
framework addresses key limitations in existing approaches with four main
improvements: (1) Multi-Task Holistic Evaluation that spans diverse software
engineering activities rather than limited coding tasks; (2) Multi-Language and
Multi-Modality Assessment that extends beyond traditional single-language,
text-only benchmarks to include multi-modality coding tasks; (3) Robustness
Assessment that evaluates model reliability under semantically-preserving code
transformations; and (4) Rigorous Evaluation Methodology that enhances the
trustworthiness of evaluation results through diverse evaluation prompts and
adaptive solution extraction. Based on this evaluation framework, we assess 26
state-of-the-art models and uncover both their strengths and limitations,
yielding several key insights:(1) Current models show substantial performance
variation across programming tasks; (2) Multi-modal language models demonstrate
specific performance limitations in UI code generation and edit;

</details>


### [19] [Software Testing with Large Language Models: An Interview Study with Practitioners](https://arxiv.org/abs/2510.17164)
*Maria Deolinda Santana,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本研究通过访谈15名软件测试专业人员，提出了在软件测试工作流中集成大语言模型的初步实践指南，强调迭代式提示工程和人工监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件测试中的快速应用，目前缺乏结构化指导，主要依赖非正式实验。本研究旨在了解测试专业人员如何实际使用LLMs，并提出初步的实践指导。

Method: 采用定性研究方法，对15名来自不同角色和领域的软件测试人员进行半结构化访谈，使用基于扎根理论的主题分析方法处理数据。

Result: 测试人员描述了包含定义测试目标、应用提示工程策略、优化提示、评估输出和持续学习的迭代反思过程，强调需要人工监督和仔细验证，特别是针对LLMs的幻觉和不一致推理等已知限制。

Conclusion: LLMs在软件测试中的应用正在增长，但仍受到实践演变和对风险谨慎态度的影响。本研究为在测试环境中结构化使用LLMs提供了起点，并邀请未来研究在不同团队、工具和任务中完善这些实践。

Abstract: \textit{Background:} The use of large language models in software testing is
growing fast as they support numerous tasks, from test case generation to
automation, and documentation. However, their adoption often relies on informal
experimentation rather than structured guidance. \textit{Aims:} This study
investigates how software testing professionals use LLMs in practice to propose
a preliminary, practitioner-informed guideline to support their integration
into testing workflows. \textit{Method:} We conducted a qualitative study with
15 software testers from diverse roles and domains. Data were collected through
semi-structured interviews and analyzed using grounded theory-based processes
focused on thematic analysis. \textit{Results:} Testers described an iterative
and reflective process that included defining testing objectives, applying
prompt engineering strategies, refining prompts, evaluating outputs, and
learning over time. They emphasized the need for human oversight and careful
validation, especially due to known limitations of LLMs such as hallucinations
and inconsistent reasoning. \textit{Conclusions:} LLM adoption in software
testing is growing, but remains shaped by evolving practices and caution around
risks. This study offers a starting point for structuring LLM use in testing
contexts and invites future research to refine these practices across teams,
tools, and tasks.

</details>


### [20] [OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development](https://arxiv.org/abs/2510.17184)
*Nicolas Robert,Fabien Gandon,Maxime Lefrançois*

Main category: cs.SE

TL;DR: OLIVAW是一个基于GitHub的工具，支持ACIMOV方法学，通过W3C标准协助模块化本体的开发，提供GitHub复合操作、预提交钩子和命令行界面。


<details>
  <summary>Details</summary>
Motivation: 敏捷协作的本体设计方法对于确保本体用户驱动、保持最新状态并随支持系统演进至关重要，因此需要适当的持续验证工具来确保本体在整个开发过程中符合开发者需求。

Method: 提出OLIVAW工具，基于GitHub平台实现ACIMOV方法学，利用W3C标准通过GitHub复合操作、预提交钩子和命令行界面来支持模块化本体的开发。

Result: OLIVAW在多个本体项目中进行了测试，验证了其有用性、通用性和可重用性，并提供了模板仓库以便快速启动。

Conclusion: OLIVAW是一个有效的工具，支持敏捷协作的本体开发，确保本体质量并促进其持续演进。

Abstract: Agile and collaborative approaches to ontologies design are crucial because
they contribute to making them userdriven, up-to-date, and able to evolve
alongside the systems they support, hence proper continuous validation tooling
is required to ensure ontologies match developers' requirements all along their
development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV
Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C
Standards to assist the development of modular ontologies through GitHub
Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was
tested on several ontology projects to ensure its usefulness, genericity and
reusability. A template repository is available for a quick start. OLIVAW is

</details>


### [21] [AdapTrack: Constrained Decoding without Distorting LLM's Output Intent](https://arxiv.org/abs/2510.17376)
*Yongmin Li,Jia Li,Ge Li,Zhi Jin*

Main category: cs.SE

TL;DR: AdapTrack通过引入回溯机制解决约束解码扭曲模型输出意图的问题，在API补全和代码生成任务中显著优于传统约束解码方法。


<details>
  <summary>Details</summary>
Motivation: 传统约束解码技术虽然能确保代码满足语法和API约束，但会扭曲模型的输出意图，导致生成的代码虽然满足约束但语义上不正确。

Method: AdapTrack在生成过程中引入回溯机制，避免扭曲模型的输出意图，确保生成的代码既满足约束又语义正确。

Result: 在API补全数据集上，AdapTrack相比约束解码提升高达360.87%；在真实世界API补全数据集上提升38.93%；在HumanEval和MBPP基准测试中分别提升7.84%和6.42%。

Conclusion: AdapTrack通过更好地遵循模型的输出意图，在保持约束合规的同时显著提升了代码生成质量，理论证明其输出分布与模型分布一致。

Abstract: Language model-based code generation and completion tools have been widely
adopted, but they may sometimes produce code that does not meet necessary
constraints, such as syntactic correctness or API existence. Constrained
decoding techniques are developed to help the model generate code adhering to
the constraints by greedily eliminating generation options that violate
constraints at each step of the generation process. However, there is a severe
limitation of constrained decoding, that it distorts the model's output intent,
forcing it to produce code that may satisfy the constraint but does not match
the development intent and is therefore incorrect. In response to this
challenge, we propose AdapTrack. By incorporating backtracking into the
generation process, AdapTrack avoids distorting the output intent of the model,
thereby producing results that are not only constraint-compliant but also more
semantically aligned with model's output intent. On our synthetic API
completion dataset, AdapTrack can achieve up to 360.87% improvement compared to
constrained decoding; on the real-world API completion dataset we collect that
exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over
constrained decoding; in general code genration benchmarks, compared to
constrained decoding, AdapTrack can achieve up to 7.84% improvement on
HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by
better adhering to the model's output intent, AdapTrack can achieve significant
improvements. We provide a theoretical proof that the distribution produced by
AdapTrack aligns with the model's distribution given the generated tokens,
thereby ensuring that the model's output intent is not distorted. Experiments
on DSL problems show that, compared to existing methods, our approach can
provide generation results that are more consistent with the language model's
distribution.

</details>


### [22] [Scalable CI/CD for Legacy Modernization: An Industrial Experience Addressing Internal Challenges Related to the 2025 Japan Cliff](https://arxiv.org/abs/2510.17430)
*Kuniaki Kudo,Sherine Devi*

Main category: cs.SE

TL;DR: 开发了可扩展的CI/CD流水线来解决日本2025年悬崖问题，通过动态创建隔离开发环境、使用GitHub、Jenkins、AWS和Docker等技术，降低维护成本并推动数字化转型。


<details>
  <summary>Details</summary>
Motivation: 解决日本2025年悬崖问题，即大量遗留核心IT系统服务寿命结束导致的维护成本激增和系统更新困难问题，避免每年高达12万亿日元的潜在损失。

Method: 设计和实现可扩展CI/CD流水线，集成GitHub进行源代码控制、Jenkins实现流水线自动化、AWS提供可扩展环境、Docker实现环境容器化，支持动态创建和删除隔离开发环境。

Result: 开发人员可以在自己的环境中自由安全地测试维护程序和新技术实验，显著降低了维护成本。

Conclusion: 可扩展CI/CD流水线有效解决了遗留系统现代化问题，减少了维护成本，推动了数字化转型进程。

Abstract: We have developed a Scalable CI/CD Pipeline to address internal challenges
related to Japan 2025 cliff problem, a critical issue where the mass end of
service life of legacy core IT systems threatens to significantly increase the
maintenance cost and black box nature of these system also leads to difficult
update moreover replace, which leads to lack of progress in Digital
Transformation (DX). If not addressed, Japan could potentially lose up to 12
trillion yen per year after 2025, which is 3 times more than the cost in
previous years. Asahi also faced the same internal challenges regarding legacy
system, where manual maintenance workflows and limited QA environment have left
critical systems outdated and difficult to update. Middleware and OS version
have remained unchanged for years, leading to now its nearing end of service
life which require huge maintenance cost and effort to continue its operation.
To address this problem, we have developed and implemented a Scalable CI/CD
Pipeline where isolated development environments can be created and deleted
dynamically and is scalable as needed. This Scalable CI/CD Pipeline incorporate
GitHub for source code control and branching, Jenkins for pipeline automation,
Amazon Web Services for scalable environment, and Docker for environment
containerization. This paper presents the design and architecture of the
Scalable CI/CD Pipeline, with the implementation along with some use cases.
Through Scalable CI/CD, developers can freely and safely test maintenance
procedures and do experiments with new technology in their own environment,
reducing maintenance cost and drive Digital Transformation (DX).
  key words: 2025 Japan Cliff, Scalable CI/CD, DevOps, Legacy IT Modernization.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [23] [Unified Peripartum Database with Natural-Language-to-SQL Capabilities at Udine University Hospital: Design and Prototype](https://arxiv.org/abs/2510.16388)
*Doriana Armenise,Ginevra Battello,Andrea Brunello,Lorenza Driul,Angelo Montanari,Elisa Rizzante,Nicola Saccomanno,Andrea Salvador,Serena Xodo,Silvia Zermano*

Main category: cs.DB

TL;DR: 提出了一个将产科异构数据转换为可计算、可查询资源的实用蓝图，通过设计具有自然语言转SQL功能的统一围产期关系数据库。


<details>
  <summary>Details</summary>
Motivation: 医院中产科信息分散在不同电子病历模块、设备存储库和实验室系统中，这阻碍了产时护理和可重复研究。

Method: 与临床医生共同定义需求并形式化为实体关系图，从中推导出逻辑模式和SQL实现，集成异构数据源，并添加自然语言转SQL功能。

Result: 成功构建了一个连接产妇既往史、纵向病史、当前妊娠发现、产时过程和分娩及新生儿结局的统一围产期数据库。

Conclusion: 该数据库通过自然语言查询功能降低了审计和探索性分析的门槛，为产科数据整合提供了实用解决方案。

Abstract: The fragmentation of obstetric information across electronic health record
modules, device repositories, and laboratory systems, as it is common in
hospitals, hinders both intrapartum care and reproducible research. In this
work, we present a practical blueprint for transforming heterogeneous
peripartum records into computable, queryable assets by designing and
prototyping a unified peripartum relational database with
natural-language-to-SQL (NL2SQL) capabilities at the Obstetrics Clinic of Udine
University Hospital. Requirements were co-defined with clinicians and
formalized as an Entity-Relationship diagram, from which the logical schema and
SQL implementation of the database were then derived. The latter integrates
heterogeneous sources to connect maternal anamnestic and longitudinal history,
current-pregnancy findings, intrapartum course, and delivery and neonatal
outcomes. The NL2SQL layer enables clinicians to pose natural-language queries
to the system, lowering barriers to audit and exploratory analysis.

</details>


### [24] [Declarative Techniques for NL Queries over Heterogeneous Data](https://arxiv.org/abs/2510.16470)
*Elham Khabiri,Jeffrey O. Kephart,Fenno F. Heath III,Srideepika Jayaraman,Fateh A. Tipu,Yingjie Li,Dhruv Shah,Achille Fokoue,Anu Bhamidipaty*

Main category: cs.DB

TL;DR: 该论文针对工业环境中异构数据源查询问题，扩展了Spider基准数据集，提出声明式方法处理数据库和API调用的组合查询，显著优于现有LLM代理系统。


<details>
  <summary>Details</summary>
Motivation: 解决工业环境中自然语言查询需要组合多种异构数据源（数据库和API）的实际问题，现有LLM应用无法有效处理这种数据源异质性。

Method: 扩展Spider基准数据集以模拟真实工业环境的数据异质性，提出声明式方法来处理数据库和API调用的组合查询。

Result: 声明式方法在处理数据源异质性方面显著优于最先进的基于LLM的代理或命令式代码生成系统。

Conclusion: 提出的声明式方法能有效应对工业环境中的数据源异质性挑战，扩展的基准数据集可供研究社区使用。

Abstract: In many industrial settings, users wish to ask questions in natural language,
the answers to which require assembling information from diverse structured
data sources. With the advent of Large Language Models (LLMs), applications can
now translate natural language questions into a set of API calls or database
calls, execute them, and combine the results into an appropriate natural
language response. However, these applications remain impractical in realistic
industrial settings because they do not cope with the data source heterogeneity
that typifies such environments. In this work, we simulate the heterogeneity of
real industry settings by introducing two extensions of the popular Spider
benchmark dataset that require a combination of database and API calls. Then,
we introduce a declarative approach to handling such data heterogeneity and
demonstrate that it copes with data source heterogeneity significantly better
than state-of-the-art LLM-based agentic or imperative code generation systems.
Our augmented benchmarks are available to the research community.

</details>


### [25] [AVOCADO: The Streaming Process Mining Challenge](https://arxiv.org/abs/2510.17089)
*Christian Imenkamp,Andrea Maldonado,Hendrik Reiter,Martin Werner,Wilhelm Hasselbring,Agnes Koschmider,Andrea Burattin*

Main category: cs.DB

TL;DR: 提出了AVOCADO框架，用于标准化流式过程挖掘算法的评估，通过分离概念层和实例层来系统化处理流式数据分析的复杂性。


<details>
  <summary>Details</summary>
Motivation: 流式过程挖掘需要能够增量处理数据的算法，但该领域缺乏系统化的评估标准。AVOCADO旨在通过标准化挑战框架推动该领域的创新和社区讨论。

Method: AVOCADO框架通过分离概念层和实例层来构建挑战，评估算法在准确性、MAE、RMSE、处理延迟和鲁棒性等流式特定指标上的表现。

Result: 提出了一个基础框架，邀请社区贡献新的挑战，如集成系统吞吐量和内存消耗指标，扩展处理乱序事件等现实流复杂性的能力。

Conclusion: AVOCADO为流式过程挖掘算法评估提供了标准化基础，有望促进该领域的创新和发展，并欢迎社区参与框架的演进。

Abstract: Streaming process mining deals with the real-time analysis of streaming data.
Event streams require algorithms capable of processing data incrementally. To
systematically address the complexities of this domain, we propose AVOCADO, a
standardized challenge framework that provides clear structural divisions:
separating the concept and instantiation layers of challenges in streaming
process mining for algorithm evaluation. The AVOCADO evaluates algorithms on
streaming-specific metrics like accuracy, Mean Absolute Error (MAE), Root Mean
Square Error (RMSE), Processing Latency, and robustness. This initiative seeks
to foster innovation and community-driven discussions to advance the field of
streaming process mining. We present this framework as a foundation and invite
the community to contribute to its evolution by suggesting new challenges, such
as integrating metrics for system throughput and memory consumption, and
expanding the scope to address real-world stream complexities like out-of-order
event arrival.

</details>


### [26] [Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models](https://arxiv.org/abs/2510.17301)
*Panos Kalnis. Shuo Shang,Christian S. Jensen*

Main category: cs.DB

TL;DR: MapMuse是一个基于故事叙述的时空数据解释框架，利用大语言模型和RAG技术将复杂的时空数据转化为引人入胜的叙事体验。


<details>
  <summary>Details</summary>
Motivation: 传统时空数据可视化复杂且需要专业知识，难以引起广泛受众共鸣，需要一种更易理解的数据呈现方式。

Method: 采用检索增强生成和基于代理的技术，结合电影叙事原则，将位置视为角色、移动视为情节来构建故事。

Result: 通过出租车轨迹案例研究展示了两种叙事视角：基于热图的宏观城市移动模式分析和单个出租车旅程的详细叙事。

Conclusion: 数据故事叙述能够弥合数据复杂性与人类理解之间的鸿沟，推动对时空数据的洞察、参与和行动。

Abstract: Spatio-temporal data captures complex dynamics across both space and time,
yet traditional visualizations are complex, require domain expertise and often
fail to resonate with broader audiences. Here, we propose MapMuse, a
storytelling-based framework for interpreting spatio-temporal datasets,
transforming them into compelling, narrative-driven experiences. We utilize
large language models and employ retrieval augmented generation (RAG) and
agent-based techniques to generate comprehensive stories. Drawing on principles
common in cinematic storytelling, we emphasize clarity, emotional connection,
and audience-centric design. As a case study, we analyze a dataset of taxi
trajectories. Two perspectives are presented: a captivating story based on a
heat map that visualizes millions of taxi trip endpoints to uncover urban
mobility patterns; and a detailed narrative following a single long taxi
journey, enriched with city landmarks and temporal shifts. By portraying
locations as characters and movement as plot, we argue that data storytelling
drives insight, engagement, and action from spatio-temporal information. The
case study illustrates how MapMuse can bridge the gap between data complexity
and human understanding. The aim of this short paper is to provide a glimpse to
the potential of the cinematic storytelling technique as an effective
communication tool for spatio-temporal data, as well as to describe open
problems and opportunities for future research.

</details>


### [27] [Approximate Nearest Neighbor Search of Large Scale Vectors on Distributed Storage](https://arxiv.org/abs/2510.17326)
*Kun Yu,Jiabao Jin,Xiaoyao Zhong,Peng Cheng,Lei Chen,Zhitao Shen,Jingkuan Song,Hengtao Shen,Xuemin Lin*

Main category: cs.DB

TL;DR: DSANN是一个支持分布式存储的近似最近邻搜索系统，通过图-聚类混合索引方法解决传统ANNS算法在单机存储中的扩展性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 传统近似最近邻搜索算法需要将索引存储在单机内存或磁盘中，存在存储成本高、规模受限和单点故障问题。分布式存储可以提供成本效益和鲁棒性解决方案，但缺乏有效的分布式索引算法。

Method: 采用图-聚类混合索引方法，使用并发索引构建显著降低构建复杂度，应用点聚合图利用图结构信息聚合相似向量，通过分布式存储中的异步I/O优化存储效率和查询吞吐量。

Result: 实验表明DSANN能够在分布式存储场景下高效地索引、存储和搜索大规模向量数据集。

Conclusion: DSANN提供了一个在分布式存储中实现高效近似最近邻搜索的有效解决方案，解决了传统方法的存储和扩展性限制。

Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional space is an
essential operator in many online services, such as information retrieval and
recommendation. Indices constructed by the state-of-the-art ANNS algorithms
must be stored in single machine's memory or disk for high recall rate and
throughput, suffering from substantial storage cost, constraint of limited
scale and single point of failure. While distributed storage can provide a
cost-effective and robust solution, there is no efficient and effective
algorithms for indexing vectors in distributed storage scenarios. In this
paper, we present a new graph-cluster hybrid indexing and search system which
supports Distributed Storage Approximate Nearest Neighbor Search, called DSANN.
DSANN can efficiently index, store, search billion-scale vector database in
distributed storage and guarantee the high availability of index service. DSANN
employs the concurrent index construction method to significantly reduces the
complexity of index building. Then, DSANN applies Point Aggregation Graph to
leverage the structural information of graph to aggregate similar vectors,
optimizing storage efficiency and improving query throughput via asynchronous
I/O in distributed storage. Through extensive experiments, we demonstrate DSANN
can efficiently and effectively index, store and search large-scale vector
datasets in distributed storage scenarios.

</details>


### [28] [DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework](https://arxiv.org/abs/2510.17586)
*Boyan Li,Chong Chen,Zhujun Xue,Yinan Mei,Yuyu Luo*

Main category: cs.DB

TL;DR: DeepEye-SQL是一个软件工程启发的Text-to-SQL框架，将SQL生成视为软件开发过程，通过SDLC对齐的工作流程实现系统级可靠性，在BIRD-Dev和Spider-Test上分别达到73.5%和89.8%的执行准确率。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL解决方案在系统级可靠性方面存在不足，问题不仅在于单个模块，更关键的是缺乏结构化编排来确保整个工作流程的正确性。

Method: 采用软件开发生命周期(SDLC)方法，包含四个协同阶段：语义值检索和模式链接、N版本SQL生成、确定性验证工具链、置信度感知选择。

Result: 使用约300亿参数的开源LLM且无需微调，在BIRD-Dev上达到73.5%执行准确率，在Spider-Test上达到89.8%，优于现有最优方案。

Conclusion: 原则性编排而非单纯LLM扩展是实现Text-to-SQL系统级可靠性的关键，将临时查询生成转变为有纪律的工程过程。

Abstract: Large language models (LLMs) have advanced Text-to-SQL, yet existing
solutions still fall short of system-level reliability. The limitation is not
merely in individual modules - e.g., schema linking, reasoning, and
verification - but more critically in the lack of structured orchestration that
enforces correctness across the entire workflow. This gap motivates a paradigm
shift: treating Text-to-SQL not as free-form language generation but as a
software-engineering problem that demands structured, verifiable orchestration.
We present DeepEye-SQL, a software-engineering-inspired framework that reframes
Text-to-SQL as the development of a small software program, executed through a
verifiable process guided by the Software Development Life Cycle (SDLC).
DeepEye-SQL integrates four synergistic stages: it grounds ambiguous user
intent through semantic value retrieval and robust schema linking; enhances
fault tolerance with N-version SQL generation using diverse reasoning
paradigms; ensures deterministic verification via a tool-chain of unit tests
and targeted LLM-guided revision; and introduces confidence-aware selection
that clusters execution results to estimate confidence and then takes a
high-confidence shortcut or runs unbalanced pairwise adjudication in
low-confidence cases, yielding a calibrated, quality-gated output. This
SDLC-aligned workflow transforms ad hoc query generation into a disciplined
engineering process. Using ~30B open-source LLMs without any fine-tuning,
DeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on
Spider-Test, outperforming state-of-the-art solutions. This highlights that
principled orchestration, rather than LLM scaling alone, is key to achieving
system-level reliability in Text-to-SQL.

</details>


### [29] [This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!](https://arxiv.org/abs/2510.17748)
*William Zhang,Wan Shen Lim,Andrew Pavlo*

Main category: cs.DB

TL;DR: Booster框架通过利用查询级历史洞察和LLM，帮助现有数据库调优器适应环境变化（如工作负载漂移、模式迁移），显著提升调优效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动调优器难以适应环境变化，无法利用查询级历史洞察，导致在环境变化时重新优化数据库管理系统效率低下。

Method: Booster将历史工件构建为查询-配置上下文，使用LLM为每个查询推荐配置，然后通过波束搜索将查询级建议组合成整体配置。

Result: 在多个OLAP工作负载上，Booster帮助不同调优器发现比传统方法快4.7倍、性能提升74%的配置。

Conclusion: Booster框架能有效辅助现有调优器适应环境变化，通过查询级洞察显著提升调优效果和效率。

Abstract: Tuning database management systems (DBMSs) is challenging due to trillions of
possible configurations and evolving workloads. Recent advances in tuning have
led to breakthroughs in optimizing over the possible configurations. However,
due to their design and inability to leverage query-level historical insights,
existing automated tuners struggle to adapt and re-optimize the DBMS when the
environment changes (e.g., workload drift, schema transfer).
  This paper presents the Booster framework that assists existing tuners in
adapting to environment changes (e.g., drift, cross-schema transfer). Booster
structures historical artifacts into query-configuration contexts, prompts
large language models (LLMs) to suggest configurations for each query based on
relevant contexts, and then composes the query-level suggestions into a
holistic configuration with beam search. With multiple OLAP workloads, we
evaluate Booster's ability to assist different state-of-the-art tuners (e.g.,
cost-/machine learning-/LLM-based) in adapting to environment changes. By
composing recommendations derived from query-level insights, Booster assists
tuners in discovering configurations that are up to 74% better and in up to
4.7x less time than the alternative approach of continuing to tune from
historical configurations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [30] [Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI](https://arxiv.org/abs/2510.16284)
*Di Zhang*

Main category: cs.DC

TL;DR: 提出了两种并行自助法策略：局部统计量聚合和同步伪随机数生成，显著降低了通信开销和内存使用，实现了大规模系统上的可扩展并行自助法


<details>
  <summary>Details</summary>
Motivation: 传统自助法在大数据集或大量重采样时计算成本过高，需要解决分布式环境中的高通信开销和内存限制问题

Method: 使用MPI开发并行自助法算法，包括局部统计量聚合（传输充分统计量而非完整重采样数据集）和同步伪随机数生成（支持分布式重采样）

Result: 分析模型显示所提方法在通信量和内存使用方面显著降低，实现了大规模系统的可扩展并行自助法

Conclusion: 提出的并行自助法策略有效解决了通信和内存瓶颈，为大规模数据分析提供了实用的解决方案

Abstract: Bootstrapping is a powerful statistical resampling technique for estimating
the sampling distribution of an estimator. However, its computational cost
becomes prohibitive for large datasets or a high number of resamples. This
paper presents a theoretical analysis and design of parallel bootstrapping
algorithms using the Message Passing Interface (MPI). We address two key
challenges: high communication overhead and memory constraints in distributed
environments. We propose two novel strategies: 1) Local Statistic Aggregation,
which drastically reduces communication by transmitting sufficient statistics
instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number
Generation, which enables distributed resampling when the entire dataset cannot
be stored on a single process. We develop analytical models for communication
and computation complexity, comparing our methods against naive baseline
approaches. Our analysis demonstrates that the proposed methods offer
significant reductions in communication volume and memory usage, facilitating
scalable parallel bootstrapping on large-scale systems.

</details>


### [31] [MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization](https://arxiv.org/abs/2510.16415)
*Rizhen Hu,Yutong He,Ran Yan,Mou Sun,Binghang Yuan,Kun Yuan*

Main category: cs.DC

TL;DR: MeCeFO是一种内存和计算高效容错优化算法，用于分布式大语言模型训练，通过跳过注意力模块、重计算和低秩梯度近似等技术，在节点故障时最小化额外开销。


<details>
  <summary>Details</summary>
Motivation: 随着分布式优化扩展到满足大语言模型训练需求，硬件故障变得不可忽视。现有容错训练方法通常引入显著的计算或内存开销，需要额外资源。

Method: MeCeFO采用三种关键技术：(i)跳过连接-在反向传播时丢弃多头注意力模块；(ii)重计算-减少前馈网络的激活内存；(iii)低秩梯度近似-高效估计前馈网络权重矩阵梯度。

Result: 理论上MeCeFO与传统分布式训练收敛率相同，为O(1/√nT)。实验表明在高故障率下保持稳健性能，吞吐量仅下降4.18%，比现有方法具有5.0-6.7倍的容错能力。

Conclusion: MeCeFO提供了一种内存和计算高效的容错优化方案，能够在分布式大语言模型训练中有效应对节点故障，同时保持训练效率和收敛性能。

Abstract: As distributed optimization scales to meet the demands of Large Language
Model (LLM) training, hardware failures become increasingly non-negligible.
Existing fault-tolerant training methods often introduce significant
computational or memory overhead, demanding additional resources. To address
this challenge, we propose Memory- and Computation-efficient Fault-tolerant
Optimization (MeCeFO), a novel algorithm that ensures robust training with
minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its
training task to a neighboring node while employing memory- and
computation-efficient algorithmic optimizations to minimize the extra workload
imposed on the neighboring node handling both tasks. MeCeFO leverages three key
algorithmic designs: (i) Skip-connection, which drops the multi-head attention
(MHA) module during backpropagation for memory- and computation-efficient
approximation; (ii) Recomputation, which reduces activation memory in
feedforward networks (FFNs); and (iii) Low-rank gradient approximation,
enabling efficient estimation of FFN weight matrix gradients. Theoretically,
MeCeFO matches the convergence rate of conventional distributed training, with
a rate of $\mathcal{O}(1/\sqrt{nT})$, where n is the data parallelism size and
T is the number of iterations. Empirically, MeCeFO maintains robust performance
under high failure rates, incurring only a 4.18% drop in throughput,
demonstrating 5.0$\times$ to 6.7$\times$ greater resilience than previous SOTA
approaches. Codes are available at https://github.com/pkumelon/MeCeFO.

</details>


### [32] [FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference](https://arxiv.org/abs/2510.16418)
*Jian Ma,Xinchen Lyu,Jun Jiang,Longhao Zou,Chenshan Ren,Qimei Cui,Xiaofeng Tao*

Main category: cs.DC

TL;DR: FourierCompress是一种基于傅里叶变换的LLM激活压缩框架，通过利用激活在频域的稀疏性，在边缘设备上实现高效压缩，显著减少通信开销，同时保持接近无损的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决协作式LLM推理中因传输高维中间激活导致的通信瓶颈问题，现有压缩方法难以同时实现高压缩比、低重构误差和计算效率。

Method: 利用FFT将激活转换到频域，保留低频系数块，利用共轭对称性在服务器端重构信号，实现硬件加速。

Result: 在Llama 3和Qwen2.5模型上的实验显示，平均减少7.6倍激活大小，准确率损失小于0.3%，压缩时间比Top-k减少32倍以上。

Conclusion: FourierCompress在通信效率、近无损推理和压缩速度之间取得了良好平衡，为边缘设备LLM推理提供了有效解决方案。

Abstract: Collaborative large language model (LLM) inference enables real-time,
privacy-preserving AI services on resource-constrained edge devices by
partitioning computational workloads between client devices and edge servers.
However, this paradigm is severely hindered by communication bottlenecks caused
by the transmission of high-dimensional intermediate activations, exacerbated
by the autoregressive decoding structure of LLMs, where bandwidth consumption
scales linearly with output length. Existing activation compression methods
struggle to simultaneously achieve high compression ratios, low reconstruction
error, and computational efficiency. This paper proposes FourierCompress, a
novel, layer-aware activation compression framework that exploits the
frequency-domain sparsity of LLM activations. We rigorously demonstrate that
activations from the first Transformer layer exhibit strong smoothness and
energy concentration in the low-frequency domain, making them highly amenable
to near-lossless compression via the Fast Fourier Transform (FFT).
FourierCompress transforms activations into the frequency domain, retains only
a compact block of low-frequency coefficients, and reconstructs the signal at
the server using conjugate symmetry, enabling seamless hardware acceleration on
DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10
commonsense reasoning datasets demonstrate that FourierCompress preserves
performance remarkably close to the uncompressed baseline, outperforming Top-k,
QR, and SVD. FourierCompress bridges the gap between communication efficiency
(an average 7.6x reduction in activation size), near-lossless inference (less
than 0.3% average accuracy loss), and significantly faster compression
(achieving over 32x reduction in compression time compared to Top-k via
hardware acceleration) for edge-device LLM inference.

</details>


### [33] [Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages](https://arxiv.org/abs/2510.16497)
*Pacome Simon Mbonimpa,Diane Tuyizere,Azizuddin Ahmed Biyabani,Ozan K. Tonguz*

Main category: cs.DC

TL;DR: 提出了一种用于基尼亚卢旺达语和斯瓦希里语的语音转录与合成框架，利用边缘-云并行处理提高处理速度和可访问性。


<details>
  <summary>Details</summary>
Motivation: 解决东非国家技术基础设施有限的情况下，广泛使用的基尼亚卢旺达语和斯瓦希里语缺乏强大语言处理工具的问题。

Method: 使用Whisper和SpeechT5预训练模型，采用级联机制在边缘设备和云端之间分配模型推理工作负载，减少延迟和资源使用。

Result: 在边缘设备上，SpeechT5模型内存使用压缩9.5%，Whisper模型压缩14%，最大内存使用149MB。在1.7GHz CPU和1MB/s网络带宽下，系统能在1分钟内处理270字符文本的语音转文本和文本转语音。

Conclusion: 提出的级联边缘-云架构可作为STT和TTS转录的优秀平台，具有良好的准确性和响应时间。

Abstract: This paper presents a novel framework for speech transcription and synthesis,
leveraging edge-cloud parallelism to enhance processing speed and accessibility
for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful
language processing tools for these widely spoken languages in East African
countries with limited technological infrastructure. The framework utilizes the
Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and
text-to-speech (TTS) translation. The architecture uses a cascading mechanism
that distributes the model inference workload between the edge device and the
cloud, thereby reducing latency and resource usage, benefiting both ends. On
the edge device, our approach achieves a memory usage compression of 9.5% for
the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage
of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with
a 1 MB/s network bandwidth, the system can process a 270-character text in less
than a minute for both speech-to-text and text-to-speech transcription. Using
real-world survey data from Kenya, it is shown that the cascaded edge-cloud
architecture proposed could easily serve as an excellent platform for STT and
TTS transcription with good accuracy and response time.

</details>


### [34] [Reimagining RDMA Through the Lens of ML](https://arxiv.org/abs/2510.16606)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: Celeris是一个针对机器学习工作负载优化的RDMA传输协议，通过移除重传和顺序交付机制，利用ML对数据丢失的容忍性来显著降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 随着分布式ML工作负载扩展到数千个GPU，集体通信中的尾延迟成为主要瓶颈。传统RDMA设计的严格可靠性和顺序交付机制在ML场景下引入不必要的复杂性和延迟。

Method: Celeris在RDMA NIC中移除重传和顺序交付，采用尽力而为传输，保留拥塞控制，通过软件级机制（自适应超时、数据优先级）管理通信，将丢失恢复转移到ML流水线（如使用Hadamard变换）。

Result: Celeris将第99百分位延迟降低高达2.3倍，BRAM使用减少67%，NIC容错能力几乎翻倍。

Conclusion: Celeris为集群规模的ML工作负载提供了一个弹性、可扩展的传输解决方案，通过利用ML对数据丢失的容忍性来优化性能。

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs
connected by ultra-high-speed inter-connects, tail latency in collective
communication has emerged as a primary bottleneck. Prior RDMA designs, like
RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying
on retransmissions and packet sequencing to ensure correctness. While effective
for general-purpose workloads, these mechanisms introduce complexity and
latency that scale poorly, where even rare packet losses or delays can
consistently degrade system performance. We introduce Celeris, a
domain-specific RDMA transport that revisits traditional reliability guarantees
based on ML's tolerance for lost or partial data. Celeris removes
retransmissions and in-order delivery from the RDMA NIC, enabling best-effort
transport that exploits the robustness of ML workloads. It retains congestion
control (e.g., DCQCN) and manages communication with software-level mechanisms
such as adaptive timeouts and data prioritization, while shifting loss recovery
to the ML pipeline (e.g., using the Hadamard Transform). Early results show
that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by
67%, and nearly doubles NIC resilience to faults -- delivering a resilient,
scalable transport tailored for ML at cluster scale.

</details>


### [35] [Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++](https://arxiv.org/abs/2510.16890)
*Jiří Klepl,Martin Kruliš,Matyáš Brabec*

Main category: cs.DC

TL;DR: 提出了一种基于C++ Noarr库的新型MPI抽象，支持布局无关的MPI应用设计，并通过分布式GEMM内核案例展示了其可用性和性能。


<details>
  <summary>Details</summary>
Motivation: MPI技术虽然成熟，但其纯C接口缺乏现代语言特性（如类型检查、泛型编程），限制了分布式应用的开发效率和灵活性。

Method: 将MPI抽象实现为C++ Noarr库的扩展，遵循Noarr范式（一等布局和遍历抽象），提供布局无关的MPI应用设计方法。

Result: 该抽象在保持与现有MPI C++绑定相当性能的同时，为分布式应用提供了更灵活的设计能力。

Conclusion: 提出的MPI抽象成功克服了传统MPI接口的局限性，为分布式高性能计算应用提供了现代化、灵活且高效的编程解决方案。

Abstract: Message Passing Interface (MPI) has been a well-established technology in the
domain of distributed high-performance computing for several decades. However,
one of its greatest drawbacks is a rather ancient pure-C interface. It lacks
many useful features of modern languages (namely C++), like basic type-checking
or support for generic code design. In this paper, we propose a novel
abstraction for MPI, which we implemented as an extension of the C++ Noarr
library. It follows Noarr paradigms (first-class layout and traversal
abstraction) and offers layout-agnostic design of MPI applications. We also
implemented a layout-agnostic distributed GEMM kernel as a case study to
demonstrate the usability and syntax of the proposed abstraction. We show that
the abstraction achieves performance comparable to the state-of-the-art MPI C++
bindings while allowing for a more flexible design of distributed applications.

</details>


### [36] [FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems](https://arxiv.org/abs/2510.16896)
*Yiming Hu*

Main category: cs.DC

TL;DR: 提出了一种用于互连多核系统的集成容错架构，通过稳定性指标识别可靠机器和定期诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。


<details>
  <summary>Details</summary>
Motivation: 解决传统双相三重模块冗余(TMR)在永久故障下失效的问题，以及反应式TMR(R-TMR)因依赖额外硬件而增加系统复杂性和降低容错能力的问题。

Method: 构建稳定性指标识别可靠机器，执行定期诊断，实现永久故障隔离和自适应任务调度，无需额外硬件。

Result: 相比基准TMR减少约30%的任务负载，实现了优越的故障覆盖率和隔离精度。

Conclusion: 该方法显著提高了系统的可靠性和能效，为互连多核系统提供了一种有效的集成容错解决方案。

Abstract: Two-Phase Triple Modular Redundancy TMR divides redundancy operations into
two stages, omitting part of the computation during fault-free operation to
reduce energy consumption. However, it becomes ineffective under permanent
faults, limiting its reliability in critical systems. To address this,
Reactive-TMR (R-TMR) introduces permanent fault isolation mechanisms for faulty
cores, tolerating both transient and permanent faults. Yet, its reliance on
additional hardware increases system complexity and reduces fault tolerance
when multiple cores or auxiliary modules fail. This paper proposes an
integrated fault-tolerant architecture for interconnected multicore systems. By
constructing a stability metric to identify reliable machines and performing
periodic diagnostics, the method enables permanent fault isolation and adaptive
task scheduling without extra hardware. Experimental results show that it
reduces task workload by approximately 30% compared to baseline TMR and
achieves superior fault coverage and isolation accuracy, significantly
improving both reliability and energy efficiency.

</details>


### [37] [Tutoring LLM into a Better CUDA Optimizer](https://arxiv.org/abs/2510.16933)
*Matyáš Brabec,Jiří Klepl,Michal Töpfer,Martin Kruliš*

Main category: cs.DC

TL;DR: 评估大型语言模型在生成优化CUDA代码方面的能力，研究其自主优化程度以及通过提示指导提升效果的方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在编程工具中的革命性应用，本研究旨在探索最新推理模型在生成优化CUDA代码方面的具体能力，特别是它们能自主完成哪些类型的代码优化和并行模式。

Method: 通过自动评估（正确性和加速比）和人工代码审查来评估生成的解决方案，尝试交互式方法让LLM在会话中修复之前的错误，并使用提示指导（提供更详细的提示和指南）来改进模型表现。

Result: 结果表明LLM是相当熟练的编码者，但需要指导才能达到并行计算专家提供的优化解决方案水平。

Conclusion: 大型语言模型在代码生成方面表现出色，但需要通过提示指导来达到专家级的优化水平，交互式方法有助于模型自我修正错误。

Abstract: Recent leaps in large language models (LLMs) caused a revolution in
programming tools (like GitHub Copilot) that can help with code generation,
debugging, and even performance optimization. In this paper, we focus on the
capabilities of the most recent reasoning models to generate optimized CUDA
code for predefined, well-known tasks. Our objective is to determine which
types of code optimizations and parallel patterns the LLMs can perform by
themselves and whether they can be improved by tutoring (providing more
detailed hints and guidelines in the prompt). The generated solutions were
evaluated both automatically (for correctness and speedup) and manually (code
reviews) to provide a more detailed perspective. We also tried an interactive
approach where the LLM can fix its previous mistakes within a session. The
results indicate that LLMs are quite skilled coders; however, they require
tutoring to reach optimized solutions provided by parallel computing experts.

</details>


### [38] [Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure](https://arxiv.org/abs/2510.16946)
*Erfan Darzi,Aldo Pareja,Shreeanant Bharadwaj*

Main category: cs.DC

TL;DR: 提出基于eBPF的GPU尾延迟监控系统，能在5秒内检测延迟尖峰，6-8秒完成根因分析，诊断准确率达81-88%，CPU开销仅1.21%。


<details>
  <summary>Details</summary>
Motivation: 现有监控工具在共享计算环境中缺乏细粒度根因分析能力，无法有效诊断GPU尾延迟尖峰问题。

Method: 使用eBPF技术构建统一的主机端监控系统，将eBPF获取的主机指标与GPU内部事件关联，实现系统级可观测性。

Result: 系统在分布式学习工作负载中成功识别出NIC争用、PCIe压力和CPU干扰等根因，无需集群范围插装即可进行运维调试。

Conclusion: 该eBPF监控系统为多租户GPU基础设施提供了有效的运维调试能力，实现了高性能的GPU尾延迟诊断。

Abstract: Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is
critical for maintaining performance predictability and resource utilization,
yet existing monitoring tools lack the granularity for root cause analysis in
shared computing environments. We introduce an eBPF-based telemetry system that
provides unified host-side monitoring of GPU workloads, correlating
eBPF-derived host metrics with GPU-internal events for holistic system
observability. The system achieves 81--88\% diagnostic accuracy, detects spikes
within 5 seconds, and completes root cause analysis in 6--8 seconds, operating
with 1.21\% CPU overhead at 100Hz sampling. Evaluated on distributed learning
workloads, the system identifies root causes including NIC contention, PCIe
pressure, and CPU interference, enabling operational debugging for multi-tenant
GPU infrastructure without requiring cluster-wide instrumentation.

</details>


### [39] [Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization](https://arxiv.org/abs/2510.17158)
*Daniel Nichols,Konstantinos Parasyris,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.DC

TL;DR: 提出了一种训练语言模型的方法，使其能够在推理过程中与性能工具交互，以解决代码性能优化任务。


<details>
  <summary>Details</summary>
Motivation: 语言模型在复杂软件工程任务中表现出色，但在代码性能相关任务（如优化）中表现不佳，因为这些任务依赖于环境、硬件等复杂数据，而这些数据并未直接体现在源代码中。

Method: 训练语言模型在推理过程中与性能工具交互，从而理解环境与代码性能的相互作用。

Result: 该方法被用于训练一个最先进的GPU内核优化模型。

Conclusion: 通过让语言模型在推理过程中与性能工具交互，可以显著提升其在代码性能优化任务中的表现。

Abstract: Language models are now prevalent in software engineering with many
developers using them to automate tasks and accelerate their development. While
language models have been tremendous at accomplishing complex software
engineering tasks, there are still many areas where they fail to deliver
desirable results, for instance code performance related tasks. Tasks like
optimization depend on many complex data from the environment, hardware, etc.
that are not directly represented in source code. Recent efforts have seen
large improvements in general code modeling tasks using chain-of-thought style
reasoning, but these models still fail to comprehend how the environment
interacts with code performance. In this paper we propose a methodology to
train language models that can interact with performance tools during their
reasoning process. We then demonstrate how this methodology can be used to
train a state-of-the-art GPU kernel optimization model.

</details>


### [40] [On the Universality of Round Elimination Fixed Points](https://arxiv.org/abs/2510.17639)
*Alkida Balliu,Sebastian Brandt,Ole Gabsdil,Dennis Olivetti,Jukka Suomela*

Main category: cs.DC

TL;DR: 该论文解决了分布式图算法中关于轮次消除固定点是否通用下界证明技术的问题，证明了某些同态问题确实可以通过轮次消除固定点证明下界，但也发现了该技术对带输入问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决分布式图算法中轮次消除固定点技术是否通用的问题，特别是针对那些目前只能通过Marks技术证明下界的同态问题。

Method: 使用三势输入(tripotent inputs)构建轮次消除下界，系统性地开发新的轮次消除下界构造技术。

Result: 证明了之前只能通过Marks技术证明的同态问题确实可以通过轮次消除固定点证明下界，但也发现轮次消除技术对带输入问题存在局限性。

Conclusion: 轮次消除固定点技术对于无输入问题可能是通用的，但对于带输入问题存在局限性，无法成为通用下界证明技术。

Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC
2020] has drawn attention to the following open question: are round elimination
fixed points a universal technique for proving lower bounds? That is, given a
locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds
in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of
$\Pi$ that is a nontrivial fixed point for the round elimination technique [see
STOC 2016, PODC 2019]? If yes, then a key part of distributed computational
complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems
[ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is
based on Marks' technique [J.AMS 2016].
  We develop a new technique for constructing round elimination lower bounds
systematically. Using so-called tripotent inputs we show that the
aforementioned homomorphism problems indeed admit a lower bound proof that is
based on round elimination fixed points. Hence we eliminate the only known
obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with
inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is
based on relaxations to nontrivial round elimination fixed points. Hence round
elimination cannot be a universal technique for problems with inputs (but it
might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable
to any problem, with or without inputs, that is a fixed point in round
elimination. Prior results of this form were only able to handle certain very
restricted inputs.

</details>
