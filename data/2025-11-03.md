<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 8]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Category-Aware Semantic Caching for Heterogeneous LLM Workloads](https://arxiv.org/abs/2510.26835)
*Chen Wang,Xunzhuo Liu,Yue Zhu,Alaa Youssef,Priya Nagpurkar,Huamin Chen*

Main category: cs.DB

TL;DR: 该论文提出了一种基于类别的语义缓存系统，通过为不同查询类别设置不同的相似度阈值、TTL和配额，解决了传统统一缓存策略在处理异构LLM查询工作负载时的问题。


<details>
  <summary>Details</summary>
Motivation: LLM服务系统处理异构查询工作负载时，不同类别的查询具有不同特征（如代码查询密集、对话查询稀疏），传统统一缓存策略导致缓存命中率分布不均，长尾查询类别无法有效缓存。

Method: 采用类别感知的语义缓存，相似度阈值、TTL和配额随查询类别变化；提出混合架构分离内存HNSW搜索和外部文档存储，将未命中成本从30ms降低到2ms。

Result: 降低未命中成本使得低命中率类别在经济上可行（盈亏平衡点从15-20%降至3-5%），实现整个工作负载分布的缓存覆盖；自适应负载策略理论上可将过载模型的流量减少9-17%。

Conclusion: 类别感知语义缓存能够有效解决LLM服务中异构查询工作负载的缓存问题，通过降低未命中成本和自适应策略显著提升缓存效率和系统性能。

Abstract: LLM serving systems process heterogeneous query workloads where different
categories exhibit different characteristics. Code queries cluster densely in
embedding space while conversational queries distribute sparsely. Content
staleness varies from minutes (stock data) to months (code patterns). Query
repetition patterns range from power-law (code) to uniform (conversation),
producing long tail cache hit rate distributions: high-repetition categories
achieve 40-60% hit rates while low-repetition or volatile categories achieve
5-15% hit rates. Vector databases must exclude the long tail because remote
search costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of
production traffic uncached. Uniform cache policies compound this problem:
fixed thresholds cause false positives in dense spaces and miss valid
paraphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This
paper presents category-aware semantic caching where similarity thresholds,
TTLs, and quotas vary by query category. We present a hybrid architecture
separating in-memory HNSW search from external document storage, reducing miss
cost from 30ms to 2ms. This reduction makes low-hit-rate categories
economically viable (break-even at 3-5% versus 15-20%), enabling cache coverage
across the entire workload distribution. Adaptive load-based policies extend
this framework to respond to downstream model load, dynamically adjusting
thresholds and TTLs to reduce traffic to overloaded models by 9-17% in
theoretical projections.

</details>


### [2] [SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification](https://arxiv.org/abs/2510.26840)
*Rocky Klopfenstein,Yang He,Andrew Tremante,Yuepeng Wang,Nina Narodytska,Haoze Wu*

Main category: cs.DB

TL;DR: 提出SpotIt评估方法，通过形式化等价验证引擎主动寻找能区分生成SQL与真实SQL的数据库，发现现有基于测试的评估方法过于乐观。


<details>
  <summary>Details</summary>
Motivation: 当前基于测试的Text-to-SQL评估方法存在缺陷，两个不同的SQL查询可能在测试数据库上产生相同结果，导致评估结果过于乐观。

Method: 开发SpotIt评估管道，扩展现有验证器以支持更丰富的SQL子集，通过形式化有界等价验证主动寻找区分查询的数据库。

Result: 在BIRD数据集上评估10种Text-to-SQL方法，发现基于测试的方法经常忽略生成查询与真实查询之间的差异。

Conclusion: 验证结果揭示了当前Text-to-SQL评估的复杂情况，SpotIt方法能更准确地评估模型性能。

Abstract: Community-driven Text-to-SQL evaluation platforms play a pivotal role in
tracking the state of the art of Text-to-SQL performance. The reliability of
the evaluation process is critical for driving progress in the field. Current
evaluation methods are largely test-based, which involves comparing the
execution results of a generated SQL query and a human-labeled ground-truth on
a static test database. Such an evaluation is optimistic, as two queries can
coincidentally produce the same output on the test database while actually
being different. In this work, we propose a new alternative evaluation
pipeline, called SpotIt, where a formal bounded equivalence verification engine
actively searches for a database that differentiates the generated and
ground-truth SQL queries. We develop techniques to extend existing verifiers to
support a richer SQL subset relevant to Text-to-SQL. A performance evaluation
of ten Text-to-SQL methods on the high-profile BIRD dataset suggests that
test-based methods can often overlook differences between the generated query
and the ground-truth. Further analysis of the verification results reveals a
more complex picture of the current Text-to-SQL evaluation.

</details>


### [3] [The Impact of Data Compression in Real-Time and Historical Data Acquisition Systems on the Accuracy of Analytical Solutions](https://arxiv.org/abs/2510.26868)
*Reham Faqehi,Haya Alhuraib,Hamad Saiari,Zyad Bamigdad*

Main category: cs.DB

TL;DR: 本文评估了工业物联网环境中数据压缩机制与分析准确性之间的关系，发现过度压缩会丢失关键模式、扭曲统计指标并降低预测准确性，提出了在分析完整性和压缩效率之间取得平衡的最佳实践。


<details>
  <summary>Details</summary>
Motivation: 工业物联网环境产生大量实时和历史过程数据，数据压缩虽能降低存储成本，但可能影响工程分析的准确性和可靠性，需要理解这种权衡关系以制定支持运营效率和准确分析的数据策略。

Method: 通过理论分析、模拟信号压缩和实证评估，研究常见压缩机制对统计分析、异常检测和机器学习模型准确性的影响。

Result: 研究表明过度压缩会丢失关键模式、扭曲统计指标并降低预测准确性。

Conclusion: 提出了在分析完整性和压缩效率之间取得平衡的最佳方法和实践建议。

Abstract: In industrial and IoT environments, massive amounts of real-time and
historical process data are continuously generated and archived. With sensors
and devices capturing every operational detail, the volume of time-series data
has become a critical challenge for storage and processing systems. Efficient
data management is essential to ensure scalability, cost-effectiveness, and
timely analytics. To minimize storage expenses and optimize performance, data
compression algorithms are frequently utilized in data historians and
acquisition systems. However, compression comes with trade-offs that may
compromise the accuracy and reliability of engineering analytics that depend on
this compressed data. Understanding these trade-offs is essential for
developing data strategies that support both operational efficiency and
accurate, reliable analytics. This paper assesses the relation of common
compression mechanisms used in real-time and historical data systems and the
accuracy of analytical solutions, including statistical analysis, anomaly
detection, and machine learning models. Through theoretical analysis, simulated
signal compression, and empirical assessment, we illustrate that excessive
compression can lose critical patterns, skew statistical measures, and diminish
predictive accuracy. The study suggests optimum methods and best practices for
striking a compromise between analytical integrity and compression efficiency.

</details>


### [4] [Unstructured Data Analysis using LLMs: A Comprehensive Benchmark](https://arxiv.org/abs/2510.27119)
*Qiyan Deng,Jianhui Li,Chengliang Chai,Jinqi Liu,Junzhi She,Kaisen Jin,Zhaoze Sun,Yuhao Deng,Jia Yuan,Ye Yuan,Guoren Wang,Lei Cao*

Main category: cs.DB

TL;DR: 提出了UDA-Bench，这是首个用于非结构化数据分析的全面基准测试，包含高质量、大规模、多样化的数据集和丰富的查询工作负载，用于评估LLM驱动的数据系统。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个全面的基准来评估各种非结构化数据分析系统，这些系统在查询接口、优化策略和操作实现等方面差异很大，难以确定哪种系统在何种场景下表现最佳。

Method: 组织30名研究生花费超过10,000小时策划5个不同领域的数据集，通过人工标注构建关系数据库视图作为真实基准，并设计多样化的查询来覆盖不同类型的分析操作符。

Result: 创建了UDA-Bench基准，包含5个领域的数据集和丰富的查询工作负载，能够全面评估非结构化数据分析系统的关键构建模块。

Conclusion: UDA-Bench填补了非结构化数据分析基准测试的空白，为系统评估提供了可靠的基础，并通过深入实验全面评估了现有系统的各个关键组件。

Abstract: Nowadays, the explosion of unstructured data presents immense analytical
value. Leveraging the remarkable capability of large language models (LLMs) in
extracting attributes of structured tables from unstructured data, researchers
are developing LLM-powered data systems for users to analyze unstructured
documents as working with a database. These unstructured data analysis (UDA)
systems differ significantly in all aspects, including query interfaces, query
optimization strategies, and operator implementations, making it unclear which
performs best in which scenario. Unfortunately, there does not exist a
comprehensive benchmark that offers high-quality, large-volume, and diverse
datasets as well as rich query workload to thoroughly evaluate such systems. To
fill this gap, we present UDA-Bench, the first benchmark for unstructured data
analysis that meets all the above requirements. Specifically, we organize a
team with 30 graduate students that spends over in total 10,000 hours on
curating 5 datasets from various domains and constructing a relational database
view from these datasets by manual annotation. These relational databases can
be used as ground truth to evaluate any of these UDA systems despite their
differences in programming interfaces. Moreover, we design diverse queries to
analyze the attributes defined in the database schema, covering different types
of analytical operators with varying selectivities and complexities. We conduct
in-depth analysis of the key building blocks of existing UDA systems: query
interface, query optimization, operator design, and data processing. We run
exhaustive experiments over the benchmark to fully evaluate these systems and
different techniques w.r.t. the above building blocks.

</details>


### [5] [Compass: General Filtered Search across Vector and Structured Data](https://arxiv.org/abs/2510.27141)
*Chunxiao Ye,Xiao Yan,Eric Lo*

Main category: cs.DB

TL;DR: Compass是一个统一框架，支持向量和结构化数据的通用过滤搜索，无需依赖新索引设计，通过协调不同模态的候选生成和谓词评估实现高效查询。


<details>
  <summary>Details</summary>
Motivation: 混合向量和关系数据的日益普及需要高效支持结合高维向量搜索和复杂关系过滤的查询，但现有过滤搜索解决方案受限于专用索引，限制了任意过滤功能并与通用DBMS集成困难。

Method: Compass利用现有索引结构（HNSW和IVF用于向量属性，B+树用于关系属性），实施原则性的协作查询执行策略，协调不同模态的候选生成和谓词评估。

Result: 综合实证评估表明，Compass在各种混合查询工作负载中始终优于现有唯一高性能通用框架NaviX，在单属性设置下匹配专用单属性索引的查询吞吐量，同时保持完全通用性和DBMS兼容性。

Conclusion: Compass为在向量数据库系统中实现真正通用的过滤搜索提供了一个实用且稳健的解决方案。

Abstract: The increasing prevalence of hybrid vector and relational data necessitates
efficient, general support for queries that combine high-dimensional vector
search with complex relational filtering. However, existing filtered search
solutions are fundamentally limited by specialized indices, which restrict
arbitrary filtering and hinder integration with general-purpose DBMSs. This
work introduces \textsc{Compass}, a unified framework that enables general
filtered search across vector and structured data without relying on new index
designs. Compass leverages established index structures -- such as HNSW and IVF
for vector attributes, and B+-trees for relational attributes -- implementing a
principled cooperative query execution strategy that coordinates candidate
generation and predicate evaluation across modalities. Uniquely, Compass
maintains generality by allowing arbitrary conjunctions, disjunctions, and
range predicates, while ensuring robustness even with highly-selective or
multi-attribute filters. Comprehensive empirical evaluations demonstrate that
Compass consistently outperforms NaviX, the only existing performant general
framework, across diverse hybrid query workloads. It also matches the query
throughput of specialized single-attribute indices in their favorite settings
with only a single attribute involved, all while maintaining full generality
and DBMS compatibility. Overall, Compass offers a practical and robust solution
for achieving truly general filtered search in vector database systems.

</details>


### [6] [ShapleyPipe: Hierarchical Shapley Search for Data Preparation Pipeline Construction](https://arxiv.org/abs/2510.27168)
*Jing Chang,Chang Liu,Jinbin Huang,Shuyuan Zheng,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: ShapleyPipe是一个基于博弈论Shapley值的数据准备管道自动构建框架，通过层次分解和智能搜索机制显著降低搜索复杂度，在保证高性能的同时提供完全可解释的算子贡献评估。


<details>
  <summary>Details</summary>
Motivation: 现有自动化数据准备管道构建方法存在两个根本限制：将管道构建视为黑盒优化而无法量化个体算子贡献，以及面临搜索空间组合爆炸问题（N^M配置）。

Method: 提出ShapleyPipe框架，利用Shapley值系统量化算子边际贡献；采用层次分解将搜索复杂度从指数级降至多项式级；开发多臂老虎机机制进行智能类别评估和置换Shapley值捕捉位置依赖的算子交互。

Result: 在18个多样化数据集上的评估显示，ShapleyPipe达到高预算基线性能的98.1%，同时减少24%的评估次数，比最先进的强化学习方法性能提升3.6%；提供可解释的算子估值（与经验性能相关性ρ=0.933）。

Conclusion: ShapleyPipe不仅实现了性能提升，还提供了可解释的算子估值，支持数据驱动的管道分析和系统化的算子库优化。

Abstract: Automated data preparation pipeline construction is critical for machine
learning success, yet existing methods suffer from two fundamental limitations:
they treat pipeline construction as black-box optimization without quantifying
individual operator contributions, and they struggle with the combinatorial
explosion of the search space ($N^M$ configurations for N operators and
pipeline length M). We introduce ShapleyPipe, a principled framework that
leverages game-theoretic Shapley values to systematically quantify each
operator's marginal contribution while maintaining full interpretability. Our
key innovation is a hierarchical decomposition that separates category-level
structure search from operator-level refinement, reducing the search complexity
from exponential to polynomial. To make Shapley computation tractable, we
develop: (1) a Multi-Armed Bandit mechanism for intelligent category evaluation
with provable convergence guarantees, and (2) Permutation Shapley values to
correctly capture position-dependent operator interactions. Extensive
evaluation on 18 diverse datasets demonstrates that ShapleyPipe achieves 98.1\%
of high-budget baseline performance while using 24\% fewer evaluations, and
outperforms the state-of-the-art reinforcement learning method by 3.6\%. Beyond
performance gains, ShapleyPipe provides interpretable operator valuations
($\rho$=0.933 correlation with empirical performance) that enable data-driven
pipeline analysis and systematic operator library refinement.

</details>


### [7] [DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries](https://arxiv.org/abs/2510.27238)
*Chuxuan Hu,Maxwell Yang,James Weiland,Yeji Lim,Suhas Palawala,Daniel Kang*

Main category: cs.DB

TL;DR: DRAMA是一个端到端的数据分析范式，通过自然语言查询在大规模开放域数据上进行自动分析，统一了数据收集、转换和分析流程。


<details>
  <summary>Details</summary>
Motivation: 手动进行真实世界数据分析劳动密集且低效，现有系统无法同时支持开放域数据收集、结构化数据转换和分析推理这三个关键能力。

Method: 提出DRAMA范式，开发DRAMA-Bot多智能体系统，包含数据检索器（协调子智能体执行数据收集和转换）和数据分析器（对检索数据进行结构化推理）。

Result: 在DRAMA-Bench基准测试中，DRAMA-Bot达到86.5%的任务准确率，成本仅0.05美元，优于所有基线方法，准确率最高提升6.9倍，成本不到1/6。

Conclusion: DRAMA提供了一个有效的端到端数据分析解决方案，在开放域数据上实现了高效准确的分析，显著优于现有方法。

Abstract: Manually conducting real-world data analyses is labor-intensive and
inefficient. Despite numerous attempts to automate data science workflows, none
of the existing paradigms or systems fully demonstrate all three key
capabilities required to support them effectively: (1) open-domain data
collection, (2) structured data transformation, and (3) analytic reasoning.
  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that
answers users' analytic queries in natural language on large-scale open-domain
data. DRAMA unifies data collection, transformation, and analysis as a single
pipeline. To quantitatively evaluate system performance on tasks representative
of DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories
of tasks: claim verification and question answering, each comprising 100
instances. These tasks are derived from real-world applications that have
gained significant public attention and require the retrieval and analysis of
open-domain data. We develop DRAMA-Bot, a multi-agent system designed following
DRAMA. It comprises a data retriever that collects and transforms data by
coordinating the execution of sub-agents, and a data analyzer that performs
structured reasoning over the retrieved data. We evaluate DRAMA-Bot on
DRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot
achieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines
with up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is
publicly available at https://github.com/uiuc-kang-lab/drama.

</details>


### [8] [Approximate Diverse $k$-nearest Neighbor Search in Vector Database](https://arxiv.org/abs/2510.27243)
*Jiachen Zhao,Xiao Yan,Eric Lo*

Main category: cs.DB

TL;DR: 提出了一种将结果多样化集成到近似k近邻搜索中的新方法，通过迭代搜索、多样化和验证阶段，在保持低延迟的同时获得接近最优的多样化结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于贪心的多样化方法经常产生次优结果，无法充分近似特定多样化水平下的最优相似度得分，且需要能适应不同用户定义结果大小和多样性需求的灵活算法。

Method: 采用渐进式搜索框架，包含迭代搜索、多样化和验证阶段，精心设计的多样化和验证步骤使方法能高效近似用户指定多样化水平下的最优多样化结果集，无需额外索引开销。

Result: 在三个百万级基准数据集上的实验表明，该方法在中等和高多样性设置下，能以最小延迟开销持续检索接近最优的多样化结果。

Conclusion: 该方法成功解决了现有多样化方法的局限性，在保持A$k$-NNS性能的同时提供了灵活且高效的多样化能力。

Abstract: Approximate $k$-nearest neighbor search (A$k$-NNS) is a core operation in
vector databases, underpinning applications such as retrieval-augmented
generation (RAG) and image retrieval. In these scenarios, users often prefer
diverse result sets to minimize redundancy and enhance information value.
However, existing greedy-based diverse methods frequently yield sub-optimal
results, failing to adequately approximate the optimal similarity score under
certain diversification level. Furthermore, there is a need for flexible
algorithms that can adapt to varying user-defined result sizes and diversity
requirements.
  To address these challenges, we propose a novel approach that seamlessly
integrates result diversification into state-of-the-art (SOTA) A$k$-NNS
methods. Our approach introduces a progressive search framework, consisting of
iterative searching, diversification, and verification phases. Carefully
designed diversification and verification steps enable our approach to
efficiently approximate the optimal diverse result set according to
user-specified diversification levels without additional indexing overhead.
  We evaluate our method on three million-scale benchmark datasets, LAION-art,
Deep1M, and Txt2img, using latency, similarity, and recall as performance
metrics across a range of $k$ values and diversification thresholds.
Experimental results demonstrate that our approach consistently retrieves
near-optimal diverse results with minimal latency overhead, particularly under
medium and high diversity settings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [Empirical Studies on Quantum Optimization for Software Engineering: A Systematic Analysis](https://arxiv.org/abs/2510.27113)
*Man Zhang,Yuechen Li,Tao Yue,Kai-Yuan Cai*

Main category: cs.SE

TL;DR: 对量子优化在软件工程领域实证研究的系统分析，识别当前实践中的关键差距，并提出改进建议


<details>
  <summary>Details</summary>
Motivation: 量子、量子启发和混合算法在解决软件工程优化问题中显示出潜力，但缺乏成熟的实证研究最佳实践

Method: 基于最新系统文献综述确定的主要研究，从实验设计、超参数设置、案例研究、基线、工具和指标等方面进行系统分析

Result: 识别出当前实践中的关键差距：重复次数和shots数量报告有限、噪声处理考虑不足、缺乏标准化评估协议（特别是量子特定指标）

Conclusion: 为设计实证研究提供见解，强调需要更多真实世界的开放案例研究来评估量子启发、量子、混合方法的成本效益和实际效用

Abstract: In recent years, quantum, quantum-inspired, and hybrid algorithms are
increasingly showing promise for solving software engineering optimization
problems. However, best-intended practices for conducting empirical studies
have not yet well established. In this paper, based on the primary studies
identified from the latest systematic literature review on quantum optimization
for software engineering problems, we conducted a systematic analysis on these
studies from various aspects including experimental designs, hyperparameter
settings, case studies, baselines, tooling, and metrics. We identify key gaps
in the current practices such as limited reporting of the number of
repetitions, number of shots, and inadequate consideration of noise handling,
as well as a lack of standardized evaluation protocols such as the adoption of
quality metrics, especially quantum-specific metrics. Based on our analysis, we
provide insights for designing empirical studies and highlight the need for
more real-world and open case studies to assess cost-effectiveness and
practical utility of the three types of approaches: quantum-inspired, quantum,
and hybrid. This study is intended to offer an overview of current practices
and serve as an initial reference for designing and conducting empirical
studies on evaluating and comparing quantum, quantum-inspired, and hybrid
algorithms in solving optimization problems in software engineering.

</details>


### [10] [MARIA: A Framework for Marginal Risk Assessment without Ground Truth in AI Systems](https://arxiv.org/abs/2510.27163)
*Jieshan Chen,Suyu Ma,Qinghua Lu,Sung Une Lee,Liming Zhu*

Main category: cs.SE

TL;DR: 提出了一个边际风险评估框架，避免依赖真实标签或绝对风险，专注于系统间的相对差异评估。


<details>
  <summary>Details</summary>
Motivation: 传统评估需要真实标签，但在延迟结果、高成本或不完整数据等情况下难以获取，特别是对于已被视为安全的长期系统。

Method: 采用边际风险评估框架，强调三种相对评估方法：可预测性、能力和交互主导性。

Result: 通过从绝对评估转向相对评估，为软件团队提供可操作的指导，识别AI改善结果、引入新风险的地方，以及如何负责任地采用此类系统。

Conclusion: 该框架为AI系统部署提供了更实用的风险评估方法，无需依赖难以获取的真实标签数据。

Abstract: Before deploying an AI system to replace an existing process, it must be
compared with the incumbent to ensure improvement without added risk.
Traditional evaluation relies on ground truth for both systems, but this is
often unavailable due to delayed or unknowable outcomes, high costs, or
incomplete data, especially for long-standing systems deemed safe by
convention. The more practical solution is not to compute absolute risk but the
difference between systems. We therefore propose a marginal risk assessment
framework, that avoids dependence on ground truth or absolute risk. It
emphasizes three kinds of relative evaluation methodology, including
predictability, capability and interaction dominance. By shifting focus from
absolute to relative evaluation, our approach equips software teams with
actionable guidance: identifying where AI enhances outcomes, where it
introduces new risks, and how to adopt such systems responsibly.

</details>


### [11] [On the Marriage of Theory and Practice in Data-Aware Business Processes via Low-Code](https://arxiv.org/abs/2510.27229)
*Ali Nour Eldin,Benjamin Dalmas,Walid Gaaloul*

Main category: cs.SE

TL;DR: BPMN-ProX是一个低代码测试框架，用于增强数据感知BPMN模型的验证，通过集成高级数据处理和最先进的模型检查器来桥接非技术专家与专业人士之间的差距。


<details>
  <summary>Details</summary>
Motivation: 由于业务流程模型缺乏形式化特征但被广泛采用，需要形式化其执行语义。数据和流程是同一枚硬币的两个方面，数据在流程模型执行中至关重要。

Method: 开发BPMN-ProX低代码测试框架，集成高级数据处理功能，并采用最先进的模型检查器进行稳健验证。

Result: 该框架显著增强了数据感知BPMN的验证能力，将理论验证与实际建模相结合。

Conclusion: 这种创新方法促进了更敏捷、可靠和以用户为中心的业务流程管理。

Abstract: In recent years, there has been a growing interest in the verification of
business process models. Despite their lack of formal characterization, these
models are widely adopted in both industry and academia. To address this issue,
formalizing the execution semantics of business process modeling languages is
essential. Since data and process are two facets of the same coin, and data are
critical elements in the execution of process models, this work introduces
Proving an eXecutable BPMN injected with data, BPMN-ProX. BPMN-ProX is a
low-code testing framework that significantly enhances the verification of
data-aware BPMN. This low-code platform helps bridge the gap between
non-technical experts and professionals by proposing a tool that integrates
advanced data handling and employs a robust verification mechanism through
state-of-the-art model checkers. This innovative approach combines theoretical
verification with practical modeling, fostering more agile, reliable, and
user-centric business process management.

</details>


### [12] [Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes](https://arxiv.org/abs/2510.27244)
*Ora Nova Fandina,Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky,Orna Raz*

Main category: cs.SE

TL;DR: SparseAlign是一个评估大型语言模型作为评判者(LaaJ)与人类判断对齐程度的框架，专门针对人类标注数据稀疏的场景，通过结合成对置信度和分数敏感对齐指标来可靠选择评估器。


<details>
  <summary>Details</summary>
Motivation: 传统语言(如COBOL、PL/I、REXX)应用现代化面临专家资源和高质量评估数据短缺问题。虽然LaaJ提供了可扩展的替代方案，但在高风险工作流中使用前需要验证其可靠性，避免形成不可靠的循环评估。

Method: 提出SparseAlign框架，结合新颖的成对置信度概念和分数敏感对齐指标，共同捕捉排名一致性和分数接近度，即使在有限标注样本下也能实现可靠的评估器选择。

Result: 在COBOL代码解释任务中应用SparseAlign选择最佳对齐的LaaJ，并将其集成到评估工作流中指导模型发布决策。通过四个LaaJ的案例研究展示了该框架在实际评估场景中的实用性。

Conclusion: SparseAlign为在人类标注数据有限的情况下可靠评估LaaJ对齐提供了有效解决方案，有助于在传统语言现代化等高风险领域做出更可靠的部署决策。

Abstract: Application modernization in legacy languages such as COBOL, PL/I, and REXX
faces an acute shortage of resources, both in expert availability and in
high-quality human evaluation data. While Large Language Models as a Judge
(LaaJ) offer a scalable alternative to expert review, their reliability must be
validated before being trusted in high-stakes workflows. Without principled
validation, organizations risk a circular evaluation loop, where unverified
LaaJs are used to assess model outputs, potentially reinforcing unreliable
judgments and compromising downstream deployment decisions. Although various
automated approaches to validating LaaJs have been proposed, alignment with
human judgment remains a widely used and conceptually grounded validation
strategy. In many real-world domains, the availability of human-labeled
evaluation data is severely limited, making it difficult to assess how well a
LaaJ aligns with human judgment. We introduce SparseAlign, a formal framework
for assessing LaaJ alignment with sparse human-labeled data. SparseAlign
combines a novel pairwise-confidence concept with a score-sensitive alignment
metric that jointly capture ranking consistency and score proximity, enabling
reliable evaluator selection even when traditional statistical methods are
ineffective due to limited annotated examples. SparseAlign was applied
internally to select LaaJs for COBOL code explanation. The top-aligned
evaluators were integrated into assessment workflows, guiding model release
decisions. We present a case study of four LaaJs to demonstrate SparseAlign's
utility in real-world evaluation scenarios.

</details>


### [13] [Efficient Integration of cross platform functions onto service-oriented architectures](https://arxiv.org/abs/2510.27344)
*Thomas Schulik,Viswanatha Reddy Batchu,Ramesh Kumar Dharmapuri,Saran Gundlapalli,Parthasarathy Nadarajan,Philipp Pelcz*

Main category: cs.SE

TL;DR: 提出了一种硬件和软件平台无关的应用程序开发与集成概念，通过标准化接口和机器可读描述，实现跨平台应用开发，并在AUTOSAR Adaptive和ROS 2平台上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 汽车行业E/E架构从分散式向集成式演进，导致软件平台异构化（如AUTOSAR Classic/Adaptive、ROS 2），需要开发硬件和平台无关的应用以提高开发和集成效率。

Method: 设计硬件和软件平台无关的应用架构，标准化应用接口，使用机器可读格式描述应用和中间件相关方面，开发工具支持开发和集成过程的半自动化。

Result: 开发了示例应用并在AUTOSAR Adaptive和ROS 2平台上成功集成，展示了方法的适用性，并提供了衡量整体概念效率的指标。

Conclusion: 提出的概念能够有效支持跨平台应用开发，实现开发与集成效率的提升，为软件定义车辆的发展提供了可行的解决方案。

Abstract: The automotive industry is currently undergoing a major transformation with
respect to the Electric/Electronic (E/E) and software architecture, driven by a
significant increase in the complexity of the technological stack within a
vehicle. This complexity acts as a driving force for Software-Defined Vehicles
(SDVs) leading to the evolution of the automotive E/E architectures from
decentralized configuration comprising multiple Electronic Control Units (ECUs)
towards a more integrated configuration comprising a smaller number of ECUs,
domain controllers, gateways, and High-Performance Computers (HPCs) [2]. This
transition along with several other reasons have resulted in heterogeneous
software platforms such as AUTOSAR Classic, AUTOSAR Adaptive, and prototypical
frameworks like ROS 2. It is therefore essential to develop applications that
are both hardware- and platform/middleware-agnostic to attain development and
integration efficiency. This work presents an application development and
integration concept to facilitate developing applications as Software as a
Product (SaaP), while simultaneously ensuring efficient integration onto
multiple software architecture platforms. The concept involves designing
applications in a hardware- and software platform-agnostic manner and
standardizing application interfaces [6]. It also includes describing the
relevant aspects of the application and corresponding middleware in a
machine-readable format to aid the integration of developed applications.
Additionally, tools are developed to facilitate semi-automation of the
development and integration processes. An example application has been
developed and integrated onto AUTOSAR Adaptive and ROS 2, demonstrating the
applicability of the approach. Finally, metrics are presented to show the
efficiency of the overall concept.

</details>


### [14] [Agentic LLMs for REST API Test Amplification: A Comparative Study Across Cloud Applications](https://arxiv.org/abs/2510.27417)
*Jarne Besjes,Robbe Nooyens,Tolgahan Bardakci,Mutlu Beyazit,Serge Demeyer*

Main category: cs.SE

TL;DR: 该研究评估了基于LLM的测试放大方法在REST API测试中的应用，比较了单代理和多代理配置在四个云应用中的表现，展示了在保持语义有效性的同时提高测试覆盖率和发现缺陷的能力。


<details>
  <summary>Details</summary>
Motivation: 确保REST API的可靠性需要自动化测试套件，但设计多样化的边界测试用例具有挑战性且资源密集，因此探索LLM驱动的测试放大方法。

Method: 扩展了基于LLM的测试放大研究，评估了单代理和多代理配置在四个额外云应用中的表现，通过最小人工干预保持测试语义有效性。

Result: 代理化LLM系统能够有效泛化到异构API架构，提高端点和参数覆盖率，同时发现缺陷。计算成本、运行时间和能耗分析揭示了准确性、可扩展性和效率之间的权衡。

Conclusion: LLM驱动的测试放大在复杂云环境中具有推进REST API测试自动化和可持续性的潜力。

Abstract: Representational State Transfer (REST) APIs are a cornerstone of modern cloud
native systems. Ensuring their reliability demands automated test suites that
exercise diverse and boundary level behaviors. Nevertheless, designing such
test cases remains a challenging and resource intensive endeavor. This study
extends prior work on Large Language Model (LLM) based test amplification by
evaluating single agent and multi agent configurations across four additional
cloud applications. The amplified test suites maintain semantic validity with
minimal human intervention. The results demonstrate that agentic LLM systems
can effectively generalize across heterogeneous API architectures, increasing
endpoint and parameter coverage while revealing defects. Moreover, a detailed
analysis of computational cost, runtime, and energy consumption highlights
trade-offs between accuracy, scalability, and efficiency. These findings
underscore the potential of LLM driven test amplification to advance the
automation and sustainability of REST API testing in complex cloud
environments.

</details>


### [15] [CodeAlignBench: Assessing Code Generation Models on Developer-Preferred Code Adjustments](https://arxiv.org/abs/2510.27565)
*Forough Mehralian,Ryan Shar,James R. Rae,Alireza Hashemi*

Main category: cs.SE

TL;DR: 提出了一个多语言基准测试，用于评估大语言模型在代码生成中的指令遵循能力，包括初始约束遵守和基于后续指令的细化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注功能正确性，忽视了现实世界编码任务的多样性和开发者期望，需要更全面的评估方法。

Method: 构建多语言基准测试框架，从LiveBench获取编程任务并自动翻译成Java和JavaScript，评估模型在初始约束遵守和后续指令细化两个关键场景的表现。

Result: 自动化基准测试显示不同模型在指令遵循的多个维度上表现出不同水平的性能，揭示了模型在语言和生成目标上的优势和局限性。

Conclusion: 该基准测试管道为代码生成模型提供了更全面的评估，能够更好地衡量模型在多样化编码任务中的实际表现。

Abstract: As large language models become increasingly capable of generating code,
evaluating their performance remains a complex and evolving challenge. Existing
benchmarks primarily focus on functional correctness, overlooking the diversity
of real-world coding tasks and developer expectations. To this end, we
introduce a multi-language benchmark that evaluates LLM instruction-following
capabilities and is extensible to operate on any set of standalone coding
problems. Our benchmark evaluates instruction following in two key settings:
adherence to pre-defined constraints specified with the initial problem, and
the ability to perform refinements based on follow-up instructions. For this
paper's analysis, we empirically evaluated our benchmarking pipeline with
programming tasks from LiveBench, that are also automatically translated from
Python into Java and JavaScript. Our automated benchmark reveals that models
exhibit differing levels of performance across multiple dimensions of
instruction-following. Our benchmarking pipeline provides a more comprehensive
evaluation of code generation models, highlighting their strengths and
limitations across languages and generation goals.

</details>


### [16] [Enhancing software product lines with machine learning components](https://arxiv.org/abs/2510.27640)
*Luz-Viviana Cobaleda,Julián Carvajal,Paola Vallejo,Andrés López,Raúl Mazo*

Main category: cs.SE

TL;DR: 提出一个结构化框架来扩展软件产品线工程，促进机器学习组件的集成，解决在包含ML组件的SPL中建模和管理可变性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统越来越多地集成机器学习，但在软件产品线中管理可变性和重用变得更加复杂，现有方法很少探索这两个领域的交叉点。

Method: 设计一个结构化框架来扩展软件产品线工程，通过VariaMos工具部分实现，支持系统化建模可变性和重用。

Result: 框架已部分实现，能够促进具有ML能力的SPL设计，系统化建模可变性和重用。

Conclusion: 该框架填补了在包含ML组件的SPL中建模和管理可变性的空白，为软件产品线工程提供了ML集成支持。

Abstract: Modern software systems increasingly integrate machine learning (ML) due to
its advancements and ability to enhance data-driven decision-making. However,
this integration introduces significant challenges for software engineering,
especially in software product lines (SPLs), where managing variability and
reuse becomes more complex with the inclusion of ML components. Although
existing approaches have addressed variability management in SPLs and the
integration of ML components in isolated systems, few have explored the
intersection of both domains. Specifically, there is limited support for
modeling and managing variability in SPLs that incorporate ML components. To
bridge this gap, this article proposes a structured framework designed to
extend Software Product Line engineering, facilitating the integration of ML
components. It facilitates the design of SPLs with ML capabilities by enabling
systematic modeling of variability and reuse. The proposal has been partially
implemented with the VariaMos tool.

</details>


### [17] [On Selecting Few-Shot Examples for LLM-based Code Vulnerability Detection](https://arxiv.org/abs/2510.27675)
*Md Abdul Hannan,Ronghao Ni,Chi Zhang,Limin Jia,Ravi Mangal,Corina S. Pasareanu*

Main category: cs.SE

TL;DR: 本文探讨了在代码漏洞检测任务中选择few-shot示例的两种标准：基于LLM错误率的样本选择和基于k近邻的相似性选择，评估了这些标准单独和组合使用时的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在代码任务中表现出色，但在代码漏洞检测方面仍面临挑战。通过ICL提供合适的few-shot示例可以提升模型性能，但如何选择这些示例至关重要。

Method: 提出了两种选择few-shot示例的标准：1) 基于LLM在样本上的错误率选择；2) 基于k近邻算法选择与查询程序相似的示例。在多个数据集上使用开源模型评估了这些标准的单独和组合效果。

Result: 通过实验评估了不同few-shot示例选择标准对代码漏洞检测性能的影响。

Conclusion: 合适的few-shot示例选择标准能够有效提升LLM在代码漏洞检测任务中的性能，两种标准各有优势，组合使用可能带来更好的效果。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities for
many coding tasks, including summarization, translation, completion, and code
generation. However, detecting code vulnerabilities remains a challenging task
for LLMs. An effective way to improve LLM performance is in-context learning
(ICL) - providing few-shot examples similar to the query, along with correct
answers, can improve an LLM's ability to generate correct solutions. However,
choosing the few-shot examples appropriately is crucial to improving model
performance. In this paper, we explore two criteria for choosing few-shot
examples for ICL used in the code vulnerability detection task. The first
criterion considers if the LLM (consistently) makes a mistake or not on a
sample with the intuition that LLM performance on a sample is informative about
its usefulness as a few-shot example. The other criterion considers similarity
of the examples with the program under query and chooses few-shot examples
based on the $k$-nearest neighbors to the given sample. We perform evaluations
to determine the benefits of these criteria individually as well as under
various combinations, using open-source models on multiple datasets.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [FlowMesh: A Service Fabric for Composable LLM Workflows](https://arxiv.org/abs/2510.26913)
*Junyi Shen,Noppanat Wadlom,Lingfeng Zhou,Dequan Wang,Xu Miao,Lei Fang,Yao Lu*

Main category: cs.DC

TL;DR: FlowMesh是一个多租户服务结构，将AI工作流作为共享服务执行和优化，通过分解工作流为细粒度算子、记录数据血缘、去重和批处理请求，在异构GPU上实现成本降低3.8倍、能耗降低2.0倍。


<details>
  <summary>Details</summary>
Motivation: AI部署越来越像数据转换、微调和智能体交互的流水线，而非单一的LLM任务。为应对这一转变，需要一种能够优化这些工作负载的共享服务结构。

Method: 将工作流分解为细粒度算子并记录数据血缘，实现跨用户工作去重和硬件上的请求批处理。全局控制平面维护集群范围内的算子池，使用单一效用函数选择批次和工作器，在异构GPU上平衡吞吐量、成本和数据局部性。数据平面是无状态工作器的弹性集群，支持快速自动扩展、安全重试和跨集群可移植性。

Result: 与基线解决方案相比，FlowMesh实现了高达3.8倍的成本降低和2.0倍的能耗降低，提供相似或更好的延迟性能，并在动态和易故障条件下保持高效。

Conclusion: FlowMesh通过将AI工作流作为共享服务执行和优化，显著提高了资源利用效率，降低了成本和能耗，同时保持了良好的性能表现。

Abstract: AI deployment increasingly resembles a pipeline of data transformation,
fine-tuning, and agent interactions rather than a monolithic LLM job; recent
examples include RLHF/RLAIF training and agentic workflows. To cope with this
shift, we propose FlowMesh, a multi-tenant service fabric that executes and
optimizes these workloads as one shared service instead of isolated pipelines.
It decomposes workflows into fine-grained operators with recorded lineage,
enabling de-duplication of work across users and batching requests on the same
hardware while preserving per-workflow provenance. A global control plane
maintains a cluster-wide pool of ready operators and uses a single utility
function to pick both the batch and the worker, balancing throughput, cost, and
data locality on heterogeneous GPUs. The data plane is an elastic fleet of
stateless workers backed by a content-addressable store, enabling rapid,
automatic scale-out, safe retry after preemption, and portability across
managed clusters such as Kubernetes and geo-distributed GPU marketplaces such
as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost
reduction and 2.0x lower energy usage, provides a similar or better latency
profile, and remains efficient under dynamic and failure-prone conditions.

</details>


### [19] [A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration](https://arxiv.org/abs/2510.27039)
*Zhuo Zheng,Lingran Meng,Ziyu Lin*

Main category: cs.DC

TL;DR: 提出了一种基于云的混合模型，结合时空图神经网络和Transformer架构，用于交通流量预测，在云平台上实现可扩展性和实时适应性。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以有效捕捉大规模路网中的复杂时空依赖关系，特别是在天气、节假日和交通事故等外部因素影响下。

Method: 集成时空图神经网络（ST-GNN）和Transformer架构，利用GNN建模空间相关性，Transformer捕捉长期时间依赖，并通过特征融合整合外部上下文特征。

Result: 在数据集上的实验评估显示，该模型优于基线方法（LSTM、TCN、GCN、纯Transformer），RMSE仅为17.92，MAE仅为10.53。

Conclusion: 混合GNN-Transformer方法为基于云的智能交通系统应用提供了有效且可扩展的解决方案，在交通流量预测方法上取得进展，并为缓解拥堵提供了实际意义。

Abstract: Accurate traffic flow forecasting is essential for the development of
intelligent transportation systems (ITS), supporting tasks such as traffic
signal optimization, congestion management, and route planning. Traditional
models often fail to effectively capture complex spatial-temporal dependencies
in large-scale road networks, especially under the influence of external
factors such as weather, holidays, and traffic accidents. To address this
challenge, this paper proposes a cloud-based hybrid model that integrates
Spatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture
for traffic flow prediction. The model leverages the strengths of GNNs in
modeling spatial correlations across road networks and the Transformers'
ability to capture long-term temporal dependencies. External contextual
features are incorporated via feature fusion to enhance predictive accuracy.
The proposed model is deployed on a cloud computing platform to achieve
scalability and real-time adaptability. Experimental evaluation of the dataset
shows that our model outperforms baseline methods (LSTM, TCN, GCN, pure
Transformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings
suggest that the hybrid GNN-Transformer approach provides an effective and
scalable solution for cloud-based ITS applications, offering methodological
advancements for traffic flow forecasting and practical implications for
congestion mitigation.

</details>


### [20] [Synergistic Tensor and Pipeline Parallelism](https://arxiv.org/abs/2510.27257)
*Mengshi Qi,Jiaxuan Peng,Jie Zhang,Juan Zhu,Yong Li,Huadong Ma*

Main category: cs.DC

TL;DR: 提出一种协同的张量和流水线并行调度方法，通过将前向和后向传播解耦为细粒度计算单元并交织编排，同时减少TP通信开销和PP流水线气泡，提升LLM和MLLM训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有混合并行方案中，张量并行(TP)带来显著通信开销，流水线并行(PP)存在同步效率低下的流水线气泡问题，而现有工作多从孤立角度解决这些问题。

Method: 将PP中的前向和后向传播解耦为细粒度计算单元，然后交织编排形成复合计算序列，在此基础上设计PP调度以最小化流水线气泡。

Result: 实验结果显示，相比现有调度方法，该方法将LLM训练吞吐量提升最高12%，MLLM提升最高16%。

Conclusion: 提出的协同调度方法能有效同时减少TP和PP相关的气泡，显著提升大规模模型训练效率。

Abstract: In the machine learning system, the hybrid model parallelism combining tensor
parallelism (TP) and pipeline parallelism (PP) has become the dominant solution
for distributed training of Large Language Models~(LLMs) and Multimodal LLMs
(MLLMs). However, TP introduces significant collective communication overheads,
while PP suffers from synchronization inefficiencies such as pipeline bubbles.
Existing works primarily address these challenges from isolated perspectives,
focusing either on overlapping TP communication or on flexible PP scheduling to
mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor
and pipeline parallelism schedule that simultaneously reduces both types of
bubbles. Our proposed schedule decouples the forward and backward passes in PP
into fine-grained computation units, which are then braided to form a composite
computation sequence. This compositional structure enables near-complete
elimination of TP-related bubbles. Building upon this structure, we further
design the PP schedule to minimize PP bubbles. Experimental results demonstrate
that our approach improves training throughput by up to 12% for LLMs and 16%
for MLLMs compared to existing scheduling methods. Our source code is avaiable
at https://github.com/MICLAB-BUPT/STP.

</details>


### [21] [A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination](https://arxiv.org/abs/2510.27289)
*Zhengchang Hua,Panagiotis Oikonomou,Karim Djemame,Nikos Tziritas,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 提出DT-MADDPG算法，将多智能体强化学习与协作数字孪生网络结合，在保护隐私的同时实现电动汽车V2G网络的协调控制。


<details>
  <summary>Details</summary>
Motivation: 大规模去中心化系统（如V2G网络）的协调控制面临挑战，需要在保护个体隐私的同时实现全局最优控制策略。

Method: 结合多智能体深度确定性策略梯度算法与协作数字孪生网络，通过预测性全局模型增强集中评论家，无需收集敏感原始数据。

Result: 在模拟V2G环境中，DT-MADDPG实现了与标准MADDPG相当的协调性能，同时在数据隐私和架构去中心化方面具有显著优势。

Conclusion: 该工作为复杂网络物理系统中部署智能、基于学习的协调提供了实用且鲁棒的框架。

Abstract: The coordination of large-scale, decentralised systems, such as a fleet of
Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a
significant challenge for modern control systems. While collaborative Digital
Twins have been proposed as a solution to manage such systems without
compromising the privacy of individual agents, deriving globally optimal
control policies from the high-level information they share remains an open
problem. This paper introduces Digital Twin Assisted Multi-Agent Deep
Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid
architecture that integrates a multi-agent reinforcement learning framework
with a collaborative DT network. Our core contribution is a simulation-assisted
learning algorithm where the centralised critic is enhanced by a predictive
global model that is collaboratively built from the privacy-preserving data
shared by individual DTs. This approach removes the need for collecting
sensitive raw data at a centralised entity, a requirement of traditional
multi-agent learning algorithms. Experimental results in a simulated V2G
environment demonstrate that DT-MADDPG can achieve coordination performance
comparable to the standard MADDPG algorithm while offering significant
advantages in terms of data privacy and architectural decentralisation. This
work presents a practical and robust framework for deploying intelligent,
learning-based coordination in complex, real-world cyber-physical systems.

</details>


### [22] [Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing](https://arxiv.org/abs/2510.27317)
*Shuyi Chen,Panagiotis Oikonomou,Zhengchang Hua,Nikos Tziritas,Karim Djemame,Nan Zhang,Georgios Theodoropoulos*

Main category: cs.DC

TL;DR: 提出了一种基于能量收集的MEC系统在线资源分配策略，通过动态调度计算任务和控制能耗来平衡可再生能源的间歇性与用户需求。


<details>
  <summary>Details</summary>
Motivation: MEC系统通过将应用部署在用户附近提供低延迟服务，结合可再生能源收集技术可提高可持续性。但收集能量的间歇性与动态用户需求之间存在资源分配挑战。

Method: 采用在线策略，动态调度具有依赖关系的计算任务，通过实时决策服务器频率调整和服务模块迁移来控制能耗。

Result: 使用真实世界数据集的实验表明，该算法能有效利用收集的能量，同时保持较低的服务延迟。

Conclusion: 提出的在线策略成功解决了能量收集MEC系统中资源分配的挑战，实现了高效能量利用和低延迟服务的平衡。

Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting
applications near end-users. To promote sustainability, these systems are
increasingly integrated with renewable Energy Harvesting (EH) technologies,
enabling operation where grid electricity is unavailable. However, balancing
the intermittent nature of harvested energy with dynamic user demand presents a
significant resource allocation challenge. This work proposes an online
strategy for an MEC system powered exclusively by EH to address this trade-off.
Our strategy dynamically schedules computational tasks with dependencies and
governs energy consumption through real-time decisions on server frequency
scaling and service module migration. Experiments using real-world datasets
demonstrate our algorithm's effectiveness in efficiently utilizing harvested
energy while maintaining low service latency.

</details>


### [23] [ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method](https://arxiv.org/abs/2510.27351)
*Milena Veneva*

Main category: cs.DC

TL;DR: 提出基于机器学习的启发式方法，用于确定并行分区算法CUDA实现的最佳子系统规模，并通过kNN分类方法构建预测模型。


<details>
  <summary>Details</summary>
Motivation: 为并行分区算法的CUDA实现寻找最优子系统规模，以提高计算效率。

Method: 使用kNN分类方法构建预测模型，通过计算实验确定不同线性代数方程组规模的最优子系统规模，并扩展至递归并行分区算法。

Result: 预测值与实际数据对比显示算法性能良好，成功构建了预测递归步骤数的kNN模型。

Conclusion: 基于机器学习的启发式方法能有效预测并行分区算法的最优参数配置，提升计算性能。

Abstract: This paper presents a machine learning (ML)-based heuristic for finding the
optimum sub-system size for the CUDA implementation of the parallel partition
algorithm. Computational experiments for different system of linear algebraic
equation (SLAE) sizes are conducted, and the optimum sub-system size for each
of them is found empirically. To estimate a model for the sub-system size, we
perform the k-nearest neighbors (kNN) classification method. Statistical
analysis of the results is done. By comparing the predicted values with the
actual data, the algorithm is deemed to be acceptably good. Next, the heuristic
is expanded to work for the recursive parallel partition algorithm as well. An
algorithm for determining the optimum sub-system size for each recursive step
is formulated. A kNN model for predicting the optimum number of recursive steps
for a particular SLAE size is built.

</details>


### [24] [RDMA Point-to-Point Communication for LLM Systems](https://arxiv.org/abs/2510.27656)
*Nandor Licker,Kevin Hu,Vladimir Zaytsev,Lequn Chen*

Main category: cs.DC

TL;DR: TransferEngine是一个统一的网络接口抽象层，为LLM系统提供可移植的点对点通信，支持多种NIC硬件，实现400Gbps峰值吞吐量，应用于分布式推理、RL微调和MoE路由等场景。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统模式（如分布式推理、MoE路由、异步RL微调）需要灵活的点对点通信，但现有实现被锁定在特定NIC上，阻碍了集成和跨硬件移植性。

Method: 设计TransferEngine桥接常见NIC功能，提供统一的WriteImm操作和ImmCounter完成通知原语，透明管理每个GPU的多个NIC，无需网络传输排序假设。

Result: 在NVIDIA ConnectX-7和AWS EFA上实现400Gbps峰值吞吐量；支持万亿参数模型1.3秒RL权重更新；MoE实现超越DeepEP解码延迟，在EFA上首次实现可行延迟。

Conclusion: TransferEngine的可移植点对点通信补充了集合通信，同时避免了硬件锁定，为LLM系统提供了跨硬件平台的通信解决方案。

Abstract: Emerging Large Language Model (LLM) system patterns, such as disaggregated
inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement
fine-tuning, require flexible point-to-point communication beyond simple
collectives. Existing implementations are locked to specific Network Interface
Controllers (NICs), hindering integration into inference engines and
portability across hardware providers. We present TransferEngine, which bridges
the functionality of common NICs to expose a uniform interface. TransferEngine
exposes one-sided WriteImm operations with a ImmCounter primitive for
completion notification, without ordering assumptions of network transport,
transparently managing multiple NICs per GPU. We demonstrate peak throughput of
400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We
showcase TransferEngine through three production systems: (1) KvCache transfer
for disaggregated inference with dynamic scaling, (2) RL weight updates
achieving 1.3 seconds for trillion-parameter models, and (3) MoE
dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,
with the first viable latencies on EFA. We demonstrate that our portable
point-to-point communication complements collectives while avoiding lock-in.

</details>
