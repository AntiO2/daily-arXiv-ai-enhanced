<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: METRO是一种针对内存受限场景的MoE模型专家并行路由算法，通过平衡GPU上的激活专家数量而非令牌数量来优化性能，相比现有方法显著降低解码延迟并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有专家并行方法通过平衡各GPU处理的令牌数量来解决负载不均衡问题，但在内存受限的MoE服务场景（特别是解码阶段）中，这种平衡反而会降低性能，因为它增加了激活专家数量，加剧了内存压力。

Method: 提出METRO（Minimum Expert Token ROuting）算法：1）平衡各GPU的激活专家数量而非令牌数量；2）通过联合优化算法效率和GPU并行处理能力实现接近最优的路由质量；3）采用新颖的allGather方案收集全局top-k信息，相比传统allToAll开销更小。

Result: 在真实系统（vLLM over 8 A100 GPUs）和专有模拟器（8-16 B200 GPUs）上评估，相比EPLB：1）解码延迟降低11-22%；2）Qwen3和DeepSeek-V3服务的总令牌吞吐量提高3-21%；3）通过延迟与吞吐量的权衡，在固定解码SLO下解码吞吐量最高提升4.11倍。

Conclusion: 在内存受限的MoE服务场景中，平衡激活专家数量而非令牌数量是更有效的负载均衡策略，METRO算法通过这一创新方法显著提升了专家并行MoE服务的性能，特别是在解码阶段。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [2] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 提出一种分布式分层卸载框架，用于Vision Transformers，通过将视觉数据分割到多个独立云服务器来保护隐私，防止单服务器重建完整图像。


<details>
  <summary>Details</summary>
Motivation: 视觉智能工具计算需求高，超出移动和可穿戴设备能力。传统云卸载方案在传输和服务器计算过程中存在严重的隐私漏洞，需要设计隐私保护的解决方案。

Method: 使用本地可信边缘设备（如手机或Nvidia Jetson）作为边缘协调器，将用户视觉数据分割成小部分，分发到多个独立云服务器。最终数据合并和聚合计算仅在用户可信边缘设备上进行，确保没有单个外部服务器拥有完整图像。

Result: 以Segment Anything Model (SAM)为案例研究，框架显著增强了内容隐私性，同时保持接近基准的分割性能，大幅降低了内容重建和用户数据暴露的风险。

Conclusion: 该框架为边缘-云连续体中的视觉任务提供了一个可扩展的隐私保护解决方案，通过设计防止单服务器数据重建，平衡了隐私保护和计算性能。

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [3] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: BatANN是一个分布式磁盘向量搜索系统，通过将查询状态完整传输到数据所在机器执行，在保持单机图对数搜索效率的同时实现近线性吞吐扩展。


<details>
  <summary>Details</summary>
Motivation: 随着数据集扩展到数十亿向量，磁盘向量搜索成为实用方案，但未来需要处理单个服务器无法容纳的超大规模数据集，因此需要分布式解决方案。

Method: 核心创新是当访问存储在另一台机器上的邻域时，将查询的完整状态发送到该机器继续执行，以提高局部性。系统基于标准TCP，在分布式环境中维护单一全局图。

Result: 在1亿和10亿点数据集上，使用10台服务器达到0.95召回率时，BatANN分别达到基线方法6.21-6.49倍和2.5-5.10倍的吞吐量，同时保持平均延迟低于6毫秒。

Conclusion: BatANN是首个开源分布式磁盘向量搜索系统，能够在分布式环境中操作单一全局图，实现了对数搜索效率和近线性吞吐扩展的良好平衡。

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [4] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: WarmServe是一个多LLM服务系统，通过通用GPU工作器和预知未来工作负载的预预热技术，显著改善首次令牌时间(TTFT)，同时提高GPU资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有多LLM服务系统在提高GPU利用率的同时牺牲了推理性能，特别是首次令牌时间(TTFT)。根本原因是它们缺乏对未来工作负载特征的认知，而实际工作负载具有高度周期性和长期可预测性。

Method: 提出通用GPU工作器实现一对多GPU预预热，基于此设计WarmServe系统：1)采用驱逐感知模型放置策略减少集群级预预热干扰；2)通过主动预预热提前准备通用GPU工作器；3)使用零开销内存切换机制管理GPU内存。

Result: 在真实数据集评估中，WarmServe相比最先进的自动扩缩系统将TTFT提升高达50.8倍，同时相比GPU共享系统能够服务多达2.5倍的请求。

Conclusion: 通过利用工作负载可预测性和通用GPU工作器，WarmServe在保持高GPU利用率的同时显著改善了多LLM服务的推理性能，特别是TTFT指标。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [5] [Scalable Construction of Spiking Neural Networks using up to thousands of GPUs](https://arxiv.org/abs/2512.09502)
*Bruno Golosio,Gianmarco Tiddia,José Villamar,Luca Pontisso,Luca Sergi,Francesco Simula,Pooja Babu,Elena Pastorelli,Abigail Morrison,Markus Diesmann,Alessandro Lonardo,Pier Stanislao Paolucci,Johanna Senk*

Main category: cs.DC

TL;DR: 提出一种用于多GPU集群和未来百亿亿次超级计算机的MPI网络构建方法，用于大规模脉冲神经网络模拟


<details>
  <summary>Details</summary>
Motivation: 大规模脉冲神经网络模拟在计算神经科学研究中至关重要，但需要高效管理通信和内存。人脑皮层包含约10^10个神经元，每个形成10^3-10^4个突触，模拟这种复杂系统对高性能计算集群提出了挑战。

Method: 使用消息传递接口(MPI)开发新颖的网络构建方法，每个进程构建本地连接性，并准备数据结构以在状态传播期间实现集群内高效脉冲交换。演示了点对点和集体通信两种皮层模型的扩展性能。

Result: 展示了两种皮层模型使用点对点和集体通信的扩展性能，验证了该方法在多GPU集群和未来百亿亿次超级计算机上的有效性。

Conclusion: 该方法为大规模脉冲神经网络模拟提供了高效的网络构建和通信解决方案，适用于高性能计算环境，有助于推进计算神经科学研究。

Abstract: Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\mathcal{O}(10^{10})$ neurons, each forming $\mathcal{O}(10^{3})$--$\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.

</details>


### [6] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: 提出基于帕累托的混合鲸鱼-海鸥优化算法(PHWSOA)，用于云任务调度，同时优化完工时间、虚拟机负载均衡和经济成本三个目标，相比基线方法取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有云任务调度方案大多只优化单一或有限指标（如执行时间或资源利用率），缺乏全面的多目标优化方法，需要同时考虑完工时间、负载均衡和经济成本等多个关键目标。

Method: 提出PHWSOA算法，融合鲸鱼优化算法(WOA)和海鸥优化算法(SOA)的优势，采用帕累托支配原则进行多目标优化。关键改进包括：Halton序列初始化增强种群多样性、帕累托引导的变异机制防止早熟收敛、并行处理加速收敛、动态虚拟机负载重分配机制改善负载均衡。

Result: 在CloudSim模拟器上使用NASA-iPSC和HPC2N真实工作负载进行实验，PHWSOA相比基线方法（WOA、GA、PEWOA、GCWOA）取得显著改进：完工时间最多减少72.1%，虚拟机负载均衡提升36.8%，成本节约23.5%。

Conclusion: PHWSOA算法通过融合WOA和SOA的优势，结合帕累托多目标优化和多种改进机制，在云任务调度中实现了全面的性能提升，为实际云环境中的高效资源管理提供了有前景的解决方案。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [7] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: SynthPix是一个基于JAX实现的高性能并行合成图像生成器，专门用于粒子图像测速（PIV），相比现有工具实现了几个数量级的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 开发SynthPix的主要动机是为了支持数据密集型强化学习方法在流场估计中的训练，并缩短快速流场估计方法的开发迭代时间，特别是在需要实时PIV反馈的主动流体控制研究中。

Method: SynthPix采用JAX框架实现，专注于在加速器（如GPU）上的性能和并行化。它支持与现有工具相同的配置参数，但通过并行化架构实现了更高的图像生成效率。

Result: SynthPix在每秒图像对生成吞吐量方面比现有工具高出几个数量级，显著提升了PIV合成图像的生成效率。

Conclusion: SynthPix对流体动力学社区具有实用价值，能够支持强化学习方法的训练和快速流场估计方法的开发，特别是在需要实时PIV反馈的主动流体控制研究中。

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [8] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: STAR系统通过新的同步模式和资源重分配策略，有效解决GPU深度学习训练中的straggler问题，显著降低训练时间同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 尽管GPU深度学习训练很流行，但straggler问题的普遍性、原因、影响以及现有缓解方法的有效性仍不清楚。研究发现straggler广泛存在，且现有方法（如同步转异步SGD）可能无法改善训练时间甚至产生更多straggler。

Method: 提出STAR系统，包含：1）新的同步模式，将worker分组进行参数更新；2）启发式和机器学习方法选择最优同步模式以最小化训练时间；3）资源重分配支持所选模式同时减少对共存作业的影响；4）主动预防straggler，避免CPU和带宽过载。

Result: 在AWS上的trace驱动评估显示，STAR在PS架构中降低训练时间48-84%，在all-reduce架构中降低51-70%，同时保持同步SGD的收敛精度。

Conclusion: STAR系统通过创新的同步模式和资源管理策略，有效解决了GPU深度学习训练中的straggler问题，显著提升了训练效率，且代码已开源。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [9] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 提出首个同时实现无锁和可恢复性的转换方法，将基于锁的实现转换为可恢复的无锁实现


<details>
  <summary>Details</summary>
Motivation: 现有系统通常需要在无锁性和可恢复性之间做出权衡，缺乏能同时提供这两种特性的转换方法

Method: 从基于锁的实现出发，提供对锁获取和锁释放操作的可恢复、无锁替代方案，支持嵌套锁以确保通用性

Result: 实现了首个同时引入无锁性和可恢复性的转换，在不影响原始基于锁实现正确性的前提下确保可恢复性

Conclusion: 该转换方法填补了同时实现无锁和可恢复性的技术空白，为并发系统提供了更可靠的解决方案

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning](https://arxiv.org/abs/2512.09006)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.SE

TL;DR: 研究探索使用Llama-3.1 8B大语言模型进行源代码漏洞检测，测试了多种微调和提示工程技术，发现微调对任务至关重要，其中提出的双重微调方法表现良好。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发周期加速，软件漏洞数量持续增加，自动化源代码漏洞检测变得至关重要。研究旨在探索当前性能最强的大语言模型在漏洞检测任务中的表现，并应用各种先进技术提升其效果。

Method: 使用Llama-3.1 8B开源模型，从BigVul和PrimeVul数据集中提取源代码样本。探索了多种微调设置（包括提出的双重微调方法和测试时微调）和提示工程技术，以及检索增强生成作为示例选择技术。

Result: 微调对解决漏洞检测任务至关重要；提出的双重微调方法表现良好；Llama模型在漏洞检测方面具有潜力；提示工程效果不佳；检索增强生成作为示例选择技术表现相对较好。

Conclusion: 研究部分回答了研究问题，但仍有许多问题有待解决，为未来工作提供了多个方向。大语言模型特别是经过适当微调的Llama模型在源代码漏洞检测中具有应用潜力。

Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.

</details>


### [11] [Evolving Excellence: Automated Optimization of LLM-based Agents](https://arxiv.org/abs/2512.09108)
*Paul Brookes,Vardan Voskanyan,Rafail Giavrimis,Matthew Truscott,Mina Ilieva,Chrystalla Pavlou,Alexandru Staicu,Manal Adham,Will Evers- Hood,Jingzhi Gong,Kejia Zhang,Matvey Fedoseev,Vishal Sharma,Roman Bauer,Zheng Wang,Hema Nair,Wei Jie,Tianhua Xu,Aurora Constantin,Leslie Kanthan,Michail Basios*

Main category: cs.SE

TL;DR: ARTEMIS是一个无需代码的进化优化平台，通过语义感知的遗传算子联合优化AI代理配置，在多个代理系统上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的AI代理系统在自动化复杂工作流方面潜力巨大，但由于提示词、工具描述和参数等配置需要数周手动调优，往往表现不佳。现有优化方法要么过于复杂，要么孤立处理组件，忽略了关键相互依赖关系。

Method: ARTEMIS是一个无需代码的进化优化平台，使用语义感知的遗传算子联合优化代理配置。给定基准脚本和自然语言目标后，它能自动发现可配置组件，从执行日志中提取性能信号，并在无需架构修改的情况下进化配置。

Result: 在四个代表性代理系统上评估：1) ALE代理在AtCoder Heuristic竞赛中接受率提升13.6%；2) Mini-SWE代理在SWE-Perf代码优化上获得10.1%性能提升；3) CrewAI代理在Math Odyssey数学推理上减少36.9%的token使用量；4) 基于Qwen2.5-7B小模型的MathTales-Teacher代理在GSM8K数学问题上准确率提升22%。

Conclusion: ARTEMIS能够有效优化基于商业和本地模型的AI代理配置，显著提升性能，无需手动调优或架构修改，为AI代理优化提供了实用解决方案。

Abstract: Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.
  We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.
  We evaluate ARTEMIS on four representative agent systems: the \emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \textbf{$13.6\%$ improvement} in acceptance rate; the \emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \textbf{10.1\% performance gain}; and the \emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \textbf{$36.9\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \textbf{22\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.

</details>


### [12] [TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization](https://arxiv.org/abs/2512.09196)
*Haonan Li,Keyu Man,Partha Kanuparthy,Hanning Chen,Wei Sun,Sreen Tallam,Chenguang Zhu,Kevin Zhu,Zhiyun Qian*

Main category: cs.SE

TL;DR: TritonForge是一个基于性能剖析的自动化Triton GPU内核优化框架，通过集成内核分析、运行时剖析和迭代代码转换，实现了高达5倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管Triton DSL简化了GPU内核开发，但要达到专家级性能仍需深入了解GPU架构和底层性能权衡，这是一个劳动密集型任务。需要自动化工具来降低优化门槛。

Method: TritonForge采用剖析引导的框架，集成内核分析、运行时剖析和迭代代码转换。系统利用剖析结果的数据驱动反馈，识别性能瓶颈，提出针对性代码修改并自动评估其影响。原型系统使用LLM辅助代码推理和转换，但框架保持模块化和模型无关性。

Result: 在不同内核类型和GPU架构上，TritonForge相比基线实现实现了高达5倍的性能提升，平均成功率为1.76倍（即76%的性能改进）。

Conclusion: TritonForge为自动化GPU性能优化提供了基础框架，展示了剖析引导方法在降低GPU内核优化门槛方面的潜力，为未来研究奠定了基础。

Abstract: High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.

</details>


### [13] [Bug Priority Change Prediction: An Exploratory Study on Apache Software](https://arxiv.org/abs/2512.09216)
*Guangzong Cai,Zengyang Li,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 提出基于缺陷修复演化特征和类别不平衡处理策略的两阶段缺陷优先级变更预测方法，在Apache项目数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 缺陷优先级在修复过程中可能变化，但人工评估依赖主观判断且繁琐，容易导致错误变更，影响及时修复。目前缺乏缺陷优先级变更预测的研究。

Method: 将缺陷生命周期分为报告阶段和修复阶段，为每个阶段构建预测模型。使用缺陷修复演化特征和类别不平衡处理策略来提升模型性能。

Result: 在32个Apache项目数据集上实验，报告阶段模型F1-score达0.798，修复阶段模型F1-weighted为0.712，F1-macro为0.613。跨项目适用性分析显示模型性能在不同项目间有较大差异，但总体表现良好，各优先级水平的预测性能相对一致且较高。

Conclusion: 提出的缺陷修复演化特征和类别不平衡处理策略能有效提升缺陷优先级变更预测模型的性能，为缺陷管理提供了自动化支持工具。

Abstract: Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.

</details>


### [14] [SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs](https://arxiv.org/abs/2512.09543)
*Arihant Tripathy,Ch Pavan Harshit,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 研究评估了四种基于SLM的自主代理框架在软件工程问题解决中的性能，发现当前框架架构是能耗的主要驱动因素，但SLM的有限推理能力导致能耗浪费，需要新的架构设计来管理SLM弱点。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自主代理依赖大型专有模型，难以本地部署，促使人们关注小语言模型。但SLM在复杂代理框架中的实际效果和效率，特别是在自动化问题解决任务中，仍不清楚。

Method: 在固定硬件上，对四种领先的代理框架（SWE-Agent、OpenHands、Mini SWE Agent、AutoCodeRover）使用两种SLM（Gemma-3 4B、Qwen-3 1.7B），在SWE-bench Verified Mini基准测试上进行控制评估，测量150次运行的能量、持续时间、令牌使用和内存消耗。

Result: 框架架构是能耗的主要驱动因素：能耗最高的AutoCodeRover（Gemma）比最低的OpenHands（Gemma）多消耗9.4倍能量。但任务解决率接近零，表明当前框架与SLM配对时，大量能量浪费在无生产力的推理循环上。SLM的有限推理是成功瓶颈，而框架设计是效率瓶颈。

Conclusion: 当前为强大LLM设计的代理框架无法与SLM高效协作。框架架构是能耗主因，但能量因SLM有限推理而浪费。可行的低能耗解决方案需要从被动编排转向主动管理SLM弱点的架构。

Abstract: Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.
  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.
  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.
  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.
  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.

</details>


### [15] [Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values](https://arxiv.org/abs/2512.09562)
*Radoslaw Klimek,Jakub Blazowski*

Main category: cs.SE

TL;DR: 将工作流挖掘转化为逻辑规范，结合Shapley值量化元素贡献，实现可解释的工作流分析


<details>
  <summary>Details</summary>
Motivation: 传统工作流挖掘生成层次化过程树，但无法解释模型为何满足或违反逻辑属性，以及各元素如何影响整体行为

Method: 将挖掘的工作流转化为逻辑规范，用自动定理证明器分析可满足性、活性和安全性等属性，并应用合作博弈论的Shapley值量化工作流元素的贡献

Result: 在基准数据集上的实验表明，该方法能识别关键节点、揭示冗余、暴露有害结构

Conclusion: 为可解释的工作流分析开辟了新方向，对软件工程实践有直接意义，支持合规检查、流程优化、冗余减少和下一代过程挖掘工具设计

Abstract: Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.

</details>


### [16] [Model management to support systems engineering workflows using ontology-based knowledge graphs](https://arxiv.org/abs/2512.09596)
*Arkadiusz Ryś,Lucas Lima,Joeri Exelmans,Dennis Janssens,Hans Vangheluwe*

Main category: cs.SE

TL;DR: 提出一个基于本体的框架来管理CPS工作流执行产生的建模工件，通过知识图谱存储和推理系统工程数据


<details>
  <summary>Details</summary>
Motivation: 系统工程从文档中心转向模型驱动，CPS涉及多领域专家使用不同形式化方法执行复杂工作流，需要有效管理这些工作流产生的建模工件以支持可重复性、可复制性和数据推理

Method: 使用OML（本体建模语言）定义工作流概念、形式化方法和工件的本体，构建包含系统工程数据的知识图谱，开发支持工作流设计、执行、工件存储、版本控制、查询和推理的工具集

Result: 在真实世界的传动系统智能传感器系统开发场景中应用该框架，结果显示不仅解决了存储和版本控制等基本困难，还减少了访问相关信息的时间，并能从知识图谱中推理出新知识

Conclusion: 提出的基于本体的框架能有效管理CPS工作流执行产生的建模工件，通过知识图谱支持系统工程数据的存储、版本控制、查询和推理，提高了系统开发效率

Abstract: System engineering has been shifting from document-centric to model-based approaches, where assets are becoming more and more digital. Although digitisation conveys several benefits, it also brings several concerns (e.g., storage and access) and opportunities. In the context of Cyber- Physical Systems (CPS), we have experts from various domains executing complex workflows and manipulating models in a plethora of different formalisms, each with their own methods, techniques and tools. Storing knowledge on these workflows can reduce considerable effort during system development not only to allow their repeatability and replicability but also to access and reason on data generated by their execution. In this work, we propose a framework to manage modelling artefacts generated from workflow executions. The basic workflow concepts, related formalisms and artefacts are formally defined in an ontology specified in OML (Ontology Modelling Language). This ontology enables the construction of a knowledge graph that contains system engineering data to which we can apply reasoning. We also developed several tools to support system engineering during the design of workflows, their enactment, and artefact storage, considering versioning, querying and reasoning on the stored data. These tools also hide the complexity of manipulating the knowledge graph directly. Finally, we have applied our proposed framework in a real-world system development scenario of a drivetrain smart sensor system. Results show that our proposal not only helped the system engineer with fundamental difficulties like storage and versioning but also reduced the time needed to access relevant information and new knowledge that can be inferred from the knowledge graph.

</details>


### [17] [LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection](https://arxiv.org/abs/2512.09627)
*Jingwei Ye,Zhi Wang,Chenbin Su,Jieshuai Yang,Jiayi Ding,Chunbo Liu,Ge Chu*

Main category: cs.SE

TL;DR: LogICL：一个将大语言模型推理能力蒸馏到轻量级编码器的框架，用于解决跨域日志异常检测中的冷启动问题，通过推理感知的演示选择和上下文学习实现准确且可解释的检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的模型需要大量资源和标注数据，在目标域日志稀缺时存在冷启动问题。现有跨域方法依赖表面词汇相似性，难以捕捉结构差异下的潜在语义等价性。

Method: 提出LogICL框架：1）训练时构建delta矩阵衡量演示对零样本推理的效用；2）使用多目标损失优化编码器（ICL引导项、最大均值差异、监督对比损失）；3）推理时编码器检索推理感知的演示，支持冻结LLM的思维链上下文学习。

Result: 在少样本和零样本跨域基准测试中达到最先进性能，有效捕捉潜在语义等价性，超越表面词汇相似性，实现快速部署。

Conclusion: LogICL通过将LLM推理能力蒸馏到轻量级编码器，解决了跨域日志异常检测中的语义鸿沟问题，实现了准确、可解释且资源高效的检测。

Abstract: Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.

</details>


### [18] [Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis](https://arxiv.org/abs/2512.09679)
*Naizhu Jin,Zhong Li,Guang Yang,Tian Zhang,Qingkai Zeng*

Main category: cs.SE

TL;DR: 该论文系统研究了思维链提示在代码生成中的有效性，发现结构化CoT方法平均提升5-12%的Pass@1，且比反思推理使用更少token，效果取决于语言类型系统和模型容量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成上表现优异，但思维链提示如何帮助代码生成的机制仍不清楚，需要系统性的实证和信息论研究来理解CoT在神经代码生成中的有效性。

Method: 使用条件互信息I(Y;C|X)作为概念框架，评估五种范式（零样本、零样本CoT、自规划、结构化CoT、推理CoT），在六个Python基准、一个包含12种编程语言的多语言基准以及6个7B到480B参数的模型上进行实验。

Result: 外部引导的CoT持续优于直接生成，结构化方法平均提升Pass@1 5-12%，同时比反思推理使用更少token；CoT效果取决于语言类型系统和模型容量；推理质量至关重要，高质量结构化CoT比轻量级替代方案准确率显著更高。

Conclusion: 研究结果为基于模型容量、语言特性和任务复杂度选择CoT策略提供了实用指导，强调高质量结构化CoT的重要性，并揭示了CoT效果与语言类型系统和模型规模的关系。

Abstract: Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.

</details>


### [19] [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](https://arxiv.org/abs/2512.09775)
*Vladimir Balditsyn,Philippe Lalanda,German Vega,Stéphanie Chollet*

Main category: cs.SE

TL;DR: 该论文提出了一种量化机器学习系统不确定性的方法，通过结合多种技术评估模型预测的相关性，并在人类活动识别领域进行了验证。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统与传统软件不同，它们通过训练而非手动编码实现，导致其运行边界不确定且无法保证绝对无错误。当前缺乏量化ML系统不确定性的方法，这在实际应用中存在风险。

Method: 提出量化ML系统不确定性的方法，通过调整和联合使用一组选定的技术来评估模型预测在运行时的相关性。在高度异构和动态变化的人类活动识别领域应用这些方法。

Result: 结果表明该方法具有相关性，能够有效评估ML系统的不确定性，并为领域专家提供详细的支持和帮助。

Conclusion: 提出的不确定性量化方法在ML系统中具有实际应用价值，特别是在需要可靠预测的领域如人类活动识别中，能够帮助专家更好地理解和信任ML系统的输出。

Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [20] [CUBE: A Cardinality Estimator Based on Neural CDF](https://arxiv.org/abs/2512.09622)
*Xiao Yan,Tiezheng Nie,Boyang Fang,Derong Shen,Kou Yue,Yu Ge*

Main category: cs.DB

TL;DR: 提出基于累积分布函数(CDF)的新型基数估计方法，无需采样或积分即可计算范围查询基数，实现准确、可预测的估计结果，推理速度比现有最优方法快10倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有基于概率模型的数据驱动方法无法同时保证低推理延迟和可扩展性，随着数据维度增加，优化时间可能超过实际查询执行时间。此外，基于采样或积分的概率模型推理会产生不可预测的估计结果，违反稳定性，导致查询执行性能不稳定，给数据库调优带来困难。

Method: 基于累积分布函数(CDF)的基数估计方法，通过合并计算实现推理加速，无需采样或积分即可计算范围查询基数，确保估计结果准确且可预测。

Result: 该方法实现了快速且接近恒定的推理速度，同时保持高准确性，即使维度增加也能保持性能，比当前最先进的数据驱动基数估计器快10倍以上，展现出优异的维度可扩展性。

Conclusion: 基于CDF的基数估计方法解决了现有数据驱动方法的延迟、可扩展性和稳定性问题，适合实际数据库应用场景，为数据库优化器提供了更可靠的基数估计方案。

Abstract: Modern database optimizer relies on cardinality estimator, whose accuracy directly affects the optimizer's ability to choose an optimal execution plan. Recent work on data-driven methods has leveraged probabilistic models to achieve higher estimation accuracy, but these approaches cannot guarantee low inference latency at the same time and neglect scalability. As data dimensionality grows, optimization time can even exceed actual query execution time. Furthermore, inference with probabilistic models by sampling or integration procedures unpredictable estimation result and violate stability, which brings unstable performance with query execution and make database tuning hard for database users. In this paper, we propose a novel approach to cardinality estimation based on cumulative distribution function(CDF), which calculates range query cardinality without sampling or integration, ensuring accurate and predictable estimation results. With inference acceleration by merging calculations, we can achieve fast and nearly constant inference speed while maintaining high accuracy, even as dimensionality increases, which is over 10x faster than current state-of-the-art data-driven cardinality estimator. This demonstrates its excellent dimensional scalability, making it well-suited for real-world database applications.

</details>


### [21] [Exqutor: Extended Query Optimizer for Vector-augmented Analytical Queries](https://arxiv.org/abs/2512.09695)
*Hyunjoon Kim,Chaerim Lim,Hyeonjun An,Rathijit Sen,Kwanghyun Park*

Main category: cs.DB

TL;DR: Exqutor是一个可插拔的基数估计框架，用于优化向量增强分析查询，通过精确基数查询优化和自适应采样技术提高向量搜索基数估计准确性，在pgvector、VBASE和DuckDB中实现了高达4个数量级的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着检索增强生成(RAG)扩展到表格增强生成以整合结构化数据，结合表格和向量搜索的工作负载日益普遍。然而，由于向量搜索组件的基数估计不准确导致查询计划次优，高效执行此类查询仍然具有挑战性。

Method: 提出Exqutor扩展查询优化器，采用可插拔的基数估计框架：当向量索引（如HNSW、IVF）可用时，利用精确基数查询优化技术；当缺乏索引时，采用基于采样的方法，通过自适应采样大小调整动态平衡估计精度和采样开销。

Result: 将Exqutor集成到pgvector、VBASE和DuckDB中，在向量增强分析查询上实现了高达四个数量级的性能改进。

Conclusion: Exqutor通过提高向量搜索基数估计的准确性，有效解决了向量增强分析查询的优化挑战，显著提升了查询性能，为数据科学管道中的RAG应用提供了高效的查询优化解决方案。

Abstract: Vector similarity search is becoming increasingly important for data science pipelines, particularly in Retrieval-Augmented Generation (RAG), where it enhances large language model inference by enabling efficient retrieval of relevant external knowledge. As RAG expands with table-augmented generation to incorporate structured data, workloads integrating table and vector search are becoming more prevalent. However, efficiently executing such queries remains challenging due to inaccurate cardinality estimation for vector search components, leading to suboptimal query plans. In this paper, we propose Exqutor, an extended query optimizer for vector-augmented analytical queries. Exqutor is a pluggable cardinality estimation framework designed to address this issue, leveraging exact cardinality query optimization techniques to enhance estimation accuracy when vector indexes (e.g., HNSW, IVF) are available. In scenarios lacking these indexes, we employ a sampling-based approach with adaptive sampling size adjustment, dynamically tuning the sample size to balance estimation accuracy and sampling overhead. This allows Exqutor to efficiently approximate vector search cardinalities while minimizing computational costs. We integrate our framework into pgvector, VBASE, and DuckDB, demonstrating performance improvements of up to four orders of magnitude on vector-augmented analytical queries.

</details>


### [22] [Baseline: Operation-Based Evolution and Versioning of Data](https://arxiv.org/abs/2512.09762)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.DB

TL;DR: Baseline是一个支持多维度变更的数据平台，采用操作差分技术实现细粒度的版本控制，能够处理模式变更等结构转换，并可将查询操作化为时间线进行推测执行。


<details>
  <summary>Details</summary>
Motivation: 传统数据版本控制难以处理模式变更等结构转换，需要一种能够支持时间维度、协作维度和设计维度变更的数据管理平台。

Method: 基于操作差分技术，将数据管理为包含重构和模式变更的高级操作序列，实现操作式版本控制，并将查询操作化为时间线进行推测执行。

Result: 开发了能够处理模式变更的细粒度差异比较和合并机制，简化了版本控制概念模型（无仓库、分支即复制），并解决了最近一篇论文中提出的八个模式演化挑战问题中的四个。

Conclusion: 操作差分技术为数据版本控制提供了新方法，能够有效处理结构转换，简化用户概念模型，并为查询重写以适应模式变更提供了"免费"支持。

Abstract: Baseline is a platform for richly structured data supporting change in multiple dimensions: mutation over time, collaboration across space, and evolution through design changes. It is built upon Operational Differencing, a new technique for managing data in terms of high-level operations that include refactorings and schema changes. We use operational differencing to construct an operation-based form of version control on data structures used in programming languages and relational databases.
  This approach to data version control does fine-grained diffing and merging despite intervening structural transformations like schema changes. It offers users a simplified conceptual model of version control for ad hoc usage: There is no repo; Branching is just copying. The informaton maintained in a repo can be synthesized more precisely from the append-only histories of branches. Branches can be flexibly shared as is commonly done with document files, except with the added benefit of diffing and merging.
  We conjecture that queries can be operationalized into a sequence of schema and data operations. We develop that idea on a query language fragment containing selects and joins.
  Operationalized queries are represented as a future timeline that is speculatively executed as a branch off of the present state, returning a value from its hypothetical future. Operationalized queries get rewritten to accommodate schema change "for free" by the machinery of operational differencing.
  Altogether we develop solutions to four of the eight challenge problems of schema evolution identified in a recent paper.

</details>


### [23] [Fast Factorized Learning: Powered by In-Memory Database Systems](https://arxiv.org/abs/2512.09836)
*Bernhard Stöckl,Maximilian E. Schüle*

Main category: cs.DB

TL;DR: 该研究实现了因子化学习在内存数据库系统中的应用，相比非因子化学习性能提升70%，相比磁盘数据库系统提升100倍


<details>
  <summary>Details</summary>
Motivation: 先前研究在传统磁盘数据库系统上探索了因子化学习的性能优势，但由于缺乏公开代码，无法在内存数据库系统上复现实验。本研究旨在填补这一空白，实现因子化学习在内存数据库系统中的实际应用。

Method: 实现了因子化学习的开源实现，使用PostgreSQL作为磁盘数据库系统和HyPer作为内存引擎进行基准测试，评估线性回归在因子化连接上的学习性能。

Result: 因子化学习在内存数据库系统上相比非因子化学习性能提升70%，相比磁盘数据库系统提升100倍。现代数据库引擎可以通过在数据提取前预计算聚合来加速机器学习训练。

Conclusion: 现代数据库引擎可以通过预计算聚合来加速机器学习训练流程，因子化学习在内存数据库系统中具有显著性能优势。

Abstract: Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.

</details>
