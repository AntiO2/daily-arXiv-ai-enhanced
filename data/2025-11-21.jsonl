{"id": "2511.15950", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.15950", "abs": "https://arxiv.org/abs/2511.15950", "authors": ["Michael V. DeBole", "Rathinakumar Appuswamy", "Neil McGlohon", "Brian Taba", "Steven K. Esser", "Filipp Akopyan", "John V. Arthur", "Arnon Amir", "Alexander Andreopoulos", "Peter J. Carlson", "Andrew S. Cassidy", "Pallab Datta", "Myron D. Flickner", "Rajamohan Gandhasri", "Guillaume J. Garreau", "Megumi Ito", "Jennifer L. Klamo", "Jeffrey A. Kusnitz", "Nathaniel J. McClatchey", "Jeffrey L. McKinstry", "Tapan K. Nayak", "Carlos Ortega Otero", "Hartmut Penner", "William P. Risk", "Jun Sawada", "Jay Sivagnaname", "Daniel F. Smith", "Rafael Sousa", "Ignacio Terrizzano", "Takanori Ueda", "Trent Gray-Donald", "David Cox", "Dharmendra S. Modha"], "title": "A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference", "comment": null, "summary": "A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model."}
{"id": "2511.15957", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.15957", "abs": "https://arxiv.org/abs/2511.15957", "authors": ["Nasit S Sony", "Xianzhong Ding"], "title": "Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT", "comment": "4", "summary": "Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\\langle n-f \\rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability."}
{"id": "2511.15977", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2511.15977", "abs": "https://arxiv.org/abs/2511.15977", "authors": ["Daniel Mas Montserrat", "Ray Verma", "Míriam Barrabés", "Francisco M. de la Vega", "Carlos D. Bustamante", "Alexander G. Ioannidis"], "title": "Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows", "comment": "Accepted at AAAI 2026", "summary": "Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows."}
{"id": "2511.16041", "categories": ["cs.DC", "cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.16041", "abs": "https://arxiv.org/abs/2511.16041", "authors": ["Chengyue Wang", "Wesley Pang", "Xinrui Wu", "Gregory Jun", "Luis Romero", "Endri Taka", "Diana Marculescu", "Tony Nowatzki", "Pranathi Vasireddy", "Joseph Melber", "Deming Chen", "Jason Cong"], "title": "Can Asymmetric Tile Buffering Be Beneficial?", "comment": null, "summary": "General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.\n  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE."}
{"id": "2511.16131", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16131", "abs": "https://arxiv.org/abs/2511.16131", "authors": ["Xuan-Quang Phan", "Tan-Ha Mai", "Thai-Duy Dinh", "Minh-Thuan Nguyen", "Lam-Son Lê"], "title": "AskDB: An LLM Agent for Natural Language Interaction with Relational Databases", "comment": "15 pages, 10 figures", "summary": "Interacting with relational databases remains challenging for users across different expertise levels, particularly when composing complex analytical queries or performing administrative tasks. Existing systems typically address either natural language querying or narrow aspects of database administration, lacking a unified and intelligent interface for general-purpose database interaction. We introduce AskDB, a large language model powered agent designed to bridge this gap by supporting both data analysis and administrative operations over SQL databases through natural language. Built on Gemini 2, AskDB integrates two key innovations: a dynamic schema-aware prompting mechanism that effectively incorporates database metadata, and a task decomposition framework that enables the agent to plan and execute multi-step actions. These capabilities allow AskDB to autonomously debug derived SQL, retrieve contextual information via real-time web search, and adaptively refine its responses. We evaluate AskDB on a widely used Text-to-SQL benchmark and a curated set of DBA tasks, demonstrating strong performance in both analytical and administrative scenarios. Our results highlight the potential of AskDB as a unified and intelligent agent for relational database systems, offering an intuitive and accessible experience for end users."}
{"id": "2511.15733", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.15733", "abs": "https://arxiv.org/abs/2511.15733", "authors": ["Eitan Farchi", "Kiran Nayak", "Papia Ghosh Majumdar", "Saritha Route"], "title": "Technique to Baseline QE Artefact Generation Aligned to Quality Metrics", "comment": null, "summary": "Large Language Models (LLMs) are transforming Quality Engineering (QE) by automating the generation of artefacts such as requirements, test cases, and Behavior Driven Development (BDD) scenarios. However, ensuring the quality of these outputs remains a challenge. This paper presents a systematic technique to baseline and evaluate QE artefacts using quantifiable metrics. The approach combines LLM-driven generation, reverse generation , and iterative refinement guided by rubrics technique for clarity, completeness, consistency, and testability. Experimental results across 12 projects show that reverse-generated artefacts can outperform low-quality inputs and maintain high standards when inputs are strong. The framework enables scalable, reliable QE artefact validation, bridging automation with accountability."}
{"id": "2511.16177", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.16177", "abs": "https://arxiv.org/abs/2511.16177", "authors": ["Thomas Collignon", "Kouds Halitim", "Raphaël Bleuse", "Sophie Cerf", "Bogdan Robu", "Éric Rutten", "Lionel Seinturier", "Alexandre van Kempen"], "title": "Mitigating Shared Storage Congestion Using Control Theory", "comment": null, "summary": "Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance."}
{"id": "2511.16134", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.16134", "abs": "https://arxiv.org/abs/2511.16134", "authors": ["Marijan Soric", "Cécile Gracianne", "Ioana Manolescu", "Pierre Senellart"], "title": "Benchmarking Table Extraction from Heterogeneous Scientific Extraction Documents", "comment": null, "summary": "Table Extraction (TE) consists in extracting tables from PDF documents, in a structured format which can be automatically processed. While numerous TE tools exist, the variety of methods and techniques makes it difficult for users to choose an appropriate one. We propose a novel benchmark for assessing end-to-end TE methods (from PDF to the final table). We contribute an analysis of TE evaluation metrics, and the design of a rigorous evaluation process, which allows scoring each TE sub-task as well as end-to-end TE, and captures model uncertainty. Along with a prior dataset, our benchmark comprises two new heterogeneous datasets of 37k samples. We run our benchmark on diverse models, including off-the-shelf libraries, software tools, large vision language models, and approaches based on computer vision. The results demonstrate that TE remains challenging: current methods suffer from a lack of generalizability when facing heterogeneous data, and from limitations in robustness and interpretability."}
{"id": "2511.15757", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15757", "abs": "https://arxiv.org/abs/2511.15757", "authors": ["Kareem Shehada", "Yifan Wu", "Wyatt D. Feng", "Adithya Iyer", "Gryphon Kumfert", "Yangruibo Ding", "Zhiyun Qian"], "title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling", "summary": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates."}
{"id": "2511.16193", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16193", "abs": "https://arxiv.org/abs/2511.16193", "authors": ["Rongxin Cheng", "Kai Zhou", "Xingda Wei", "Siyuan Liu", "Mingcong Han", "Mingjing Ai", "Yeju Zhou", "Baoquan Zhong", "Wencong Xiao", "Xin Liu", "Rong Chen", "Haibo Chen"], "title": "Fast LLM Post-training via Decoupled and Best-of-N Speculation", "comment": null, "summary": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \\emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \\emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\\sys} is {1.3--1.7}\\,$\\times$ faster than common post-training baselines, and is {1.3--1.5}\\,$\\times$ faster compared to naively adopting speculative decoding for rollout."}
{"id": "2511.16138", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.16138", "abs": "https://arxiv.org/abs/2511.16138", "authors": ["Weiping Yu", "Ye Jiarui", "He Mengke", "Junfeng Liu", "Siqiang Luo"], "title": "On 10x Better Scalability: KV Stores Scale Up KV Cache", "comment": null, "summary": "Large language models (LLMs) rely on Key-Value (KV) cache to reduce time- to-first-token (TTFT) latency, but existing disk-based KV cache systems using file-per-object layouts suffer from severe scalability bottlenecks due to file system metadata overhead, I/O inefficiency, and poor spatial locality. This paper presents SGLANG-LSM, a database-inspired system that leverages Log-Structured Merge- tree (LSM-tree) architectures for scalable KV cache management. SGLANG-LSM implements a layered system design with three coordinated components: (1) a prefix-preserving storage engine that maintains token sequence locality while efficiently storing large KV cache tensors through key-value separation, (2) an adaptive controller that dynamically optimizes LSM-tree configurations based on shifting workload characteristics, and (3) runtime services including batch opera- tions and automatic resource management for production deployment. Evaluation on large-scale dynamic workloads demonstrates that SGLANG-LSM significantly improves cache hits by up to 143% and reduces TTFT by up to 24% compared to state-of-the-art systems, representing the first systematic application of database storage architectures to large-scale LLM cache management."}
{"id": "2511.15817", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15817", "abs": "https://arxiv.org/abs/2511.15817", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in \\llm-Generated Code", "comment": null, "summary": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.\n  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code."}
{"id": "2511.16450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.16450", "abs": "https://arxiv.org/abs/2511.16450", "authors": ["Ziyue Xu", "Zhihong Zhang", "Holger R. Roth", "Chester Chen", "Yan Cheng", "Andrew Feng"], "title": "Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming", "comment": "FLLM 2025", "summary": "Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios."}
{"id": "2511.16366", "categories": ["cs.DB", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.16366", "abs": "https://arxiv.org/abs/2511.16366", "authors": ["Gustavo Laranja Thomaello", "Thomaz Yeiden Busnardo Aguena", "Eric Trevelato Costa", "Rafael Baságlia Rosante", "Thiago Rodrigo Ramos", "Daiane Aparecida Zuanetti", "Edgar Dutra Zanotto"], "title": "From Patents to Dataset: Scraping for Oxide Glass Compositions and Properties", "comment": null, "summary": "In this work, we present web scraping techniques to extract in- formation from patent tables, clean and structure them for future use in predictive machine learning models to develop new glasses. We extracted compositions and three properties relevant to the development of new glasses and structured them into a database to be used together with information from other available datasets. We also analyzed the consistency of the information obtained and what it adds to the existing databases. The extracted liquidus temperatures comprise 5,696 compositions; the second subset includes 4,298 refractive indexes and, finally, 1,771 compositions with Abbe numbers. The extraction performed here increases the available information by approximately 10.4% for liquidus temperature, 6.6% for refractive index, and 4.9% for Abbe number. The impact extends beyond quantity: the newly extracted data introduce compositions with property values that are more diverse than those in existing databases, thereby expanding the accessible compositional and property space for glass modeling applications. We emphasize that the compositions of the new database contain relatively more titanium, magnesium, zirconium, niobium, iron, tin, and yttrium oxides than those of the existing bases."}
{"id": "2511.15852", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15852", "abs": "https://arxiv.org/abs/2511.15852", "authors": ["Monu Sharma"], "title": "AI-Enabled Orchestration of Event-Driven Business Processes in Workday ERP for Healthcare Enterprises", "comment": "10 Pages, 6 figures , 2 Tables", "summary": "The adoption of cloud-based Enterprise Resource Planning (ERP) platforms such as Workday has transformed healthcare operations by integrating financial, supply-chain, and workforce processes into a unified ecosystem. However, traditional workflow logic in ERP systems often lacks the adaptability required to manage event-driven and data-intensive healthcare environments.\n  This study proposes an AI-enabled event-driven orchestration framework within Workday ERP that intelligently synchronizes financial and supply-chain workflows across distributed healthcare entities. The framework employs machine-learning triggers, anomaly detection, and process mining analytics to anticipate and automate responses to operational events such as inventory depletion, payment delays, or patient demand fluctuations. A multi-organization case analysis demonstrates measurable gains in process efficiency, cost visibility, and decision accuracy.\n  Results confirm that embedding AI capabilities into Workday's event-based architecture enhances operational resilience, governance, and scalability. The proposed model contributes to the broader understanding of intelligent ERP integration and establishes a reference for next-generation automation strategies in healthcare enterprises."}
{"id": "2511.16533", "categories": ["cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2511.16533", "abs": "https://arxiv.org/abs/2511.16533", "authors": ["Nithin Salevemula", "Shreyas Pai"], "title": "Distributed MIS Algorithms for Rational Agents using Games", "comment": null, "summary": "We study the problem of computing a Maximal Independent Set (MIS) in distributed networks where each node is a rational agent whose payoff depends on whether it joins the MIS. Classical distributed algorithms assume that nodes follow the prescribed protocol, but this assumption fails when nodes are strategic and may deviate if doing so increases their expected utility.\n  Standard MIS algorithms rely on honest randomness or unique identifiers to break symmetry. In rational settings, however, agents may manipulate randomness, and relying solely on identifiers can create unfairness, giving some nodes zero probability of joining the MIS and thus no incentive to participate. To address these issues, we propose two algorithms based on a utility model in which agents seek locally correct solutions while also having preferences over which solution is chosen. Randomness in our algorithms is generated through pairwise interactions between neighboring nodes, viewed as simple games in which no single node can unilaterally affect the outcome. This allows symmetry breaking while remaining compatible with rational behavior.\n  For both algorithms, we prove that at every stage of the execution, given any history, no agent can increase its expected utility through a unilateral deviation, assuming others follow the algorithm. This gives a stronger guarantee than Trembling-Hand Perfect Equilibrium. When all nodes follow the protocol, every node has a positive probability of joining the MIS, and the final output is a correct MIS. Under mild additional assumptions, both algorithms terminate in $O(\\log n)$ rounds with high probability."}
{"id": "2511.16455", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.16455", "abs": "https://arxiv.org/abs/2511.16455", "authors": ["Pei Mu", "Anderson Chaves Carniel", "Antonio Barbalace", "Amir Shaikhha"], "title": "[Experiment, Analysis, and Benchmark] Systematic Evaluation of Plan-based Adaptive Query Processing", "comment": null, "summary": "Unreliable cardinality estimation remains a critical performance bottleneck in database management systems (DBMSs). Adaptive Query Processing (AQP) strategies address this limitation by providing a more robust query execution mechanism. Specifically, plan-based AQP achieves this by incrementally refining cardinality using feedback from the execution of sub-plans. However, the actual reason behind the improvements of plan-based AQP, especially across different storage architectures (on-disk vs. in-memory DBMSs), remains unexplored.\n  This paper presents the first comprehensive analysis of state-of-the-art plan-based AQP. We implement and evaluate this strategy on both on-disk and in-memory DBMSs across two benchmarks. Our key findings reveal that while plan-based AQP provides overall speedups in both environments, the sources of improvement differ significantly. In the on-disk DBMS, PostgreSQL, performance gains primarily come from the query plan reorderings, but not the cardinality updating mechanism; in fact, updating cardinalities introduces measurable overhead. Conversely, in the in-memory DBMS, DuckDB, cardinality refinement drives significant performance improvements for most queries. We also observe significant performance benefits of the plan-based AQP compared to a state-of-the-art related-based AQP method. These observations provide crucial insights for researchers on when and why plan-based AQP is effective, and ultimately guide database system developers on the tradeoffs between the implementation effort and performance improvements."}
{"id": "2511.15859", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.15859", "abs": "https://arxiv.org/abs/2511.15859", "authors": ["Hina Saeeda", "Mazen Mohamad", "Eric Knauss", "Jennifer Horkoff", "Ali Nouri"], "title": "RE for AI in Practice: Managing Data Annotation Requirements for AI Autonomous Driving Systems", "comment": null, "summary": "High-quality data annotation requirements are crucial for the development of safe and reliable AI-enabled perception systems (AIePS) in autonomous driving. Although these requirements play a vital role in reducing bias and enhancing performance, their formulation and management remain underexplored, leading to inconsistencies, safety risks, and regulatory concerns. Our study investigates how annotation requirements are defined and used in practice, the challenges in ensuring their quality, practitioner-recommended improvements, and their impact on AIePS development and performance. We conducted $19$ semi-structured interviews with participants from six international companies and four research organisations. Our thematic analysis reveals five main key challenges: ambiguity, edge case complexity, evolving requirements, inconsistencies, and resource constraints and three main categories of best practices, including ensuring compliance with ethical standards, improving data annotation requirements guidelines, and embedded quality assurance for data annotation requirements. We also uncover critical interrelationships between annotation requirements, annotation practices, annotated data quality, and AIePS performance and development, showing how requirement flaws propagate through the AIePS development pipeline. To the best of our knowledge, this study is the first to offer empirically grounded guidance on improving annotation requirements, offering actionable insights to enhance annotation quality, regulatory compliance, and system reliability. It also contributes to the emerging fields of Software Engineering (SE for AI) and Requirements Engineering (RE for AI) by bridging the gap between RE and AI in a timely and much-needed manner."}
{"id": "2511.16004", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16004", "abs": "https://arxiv.org/abs/2511.16004", "authors": ["KeFan Li", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution", "comment": null, "summary": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode."}
{"id": "2511.16005", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.16005", "abs": "https://arxiv.org/abs/2511.16005", "authors": ["Qingao Dong", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution", "comment": null, "summary": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems."}
{"id": "2511.16092", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16092", "abs": "https://arxiv.org/abs/2511.16092", "authors": ["Xing Hu", "Raula Gaikovina Kula", "Christoph Treude"], "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report"}
{"id": "2511.16123", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16123", "abs": "https://arxiv.org/abs/2511.16123", "authors": ["Linyi Han", "Shidong Pan", "Zhenchang Xing", "Sofonias Yitagesu", "Xiaowang Zhang", "Zhiyong Feng", "Jiamou Sun", "Qing Huang"], "title": "Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions", "comment": null, "summary": "Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability."}
{"id": "2511.16224", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16224", "abs": "https://arxiv.org/abs/2511.16224", "authors": ["Francesco Salzano", "Simone Scalabrino", "Rocco Oliveto", "Simone Scalabrino"], "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts", "comment": "20 pages", "summary": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation."}
{"id": "2511.16410", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.16410", "abs": "https://arxiv.org/abs/2511.16410", "authors": ["Hina Saeeda", "Tommy Johansson", "Mazen Mohamad", "Eric Knauss"], "title": "Data Annotation Quality Problems in AI-Enabled Perception System Development", "comment": null, "summary": "Data annotation is essential but highly error-prone in the development of AI-enabled perception systems (AIePS) for automated driving, and its quality directly influences model performance, safety, and reliability. However, the industry lacks empirical insights into how annotation errors emerge and spread across the multi-organisational automotive supply chain. This study addresses this gap through a multi-organisation case study involving six companies and four research institutes across Europe and the UK. Based on 19 semi-structured interviews with 20 experts (50 hours of transcripts) and a six-phase thematic analysis, we develop a taxonomy of 18 recurring annotation error types across three data-quality dimensions: completeness (e.g., attribute omission, missing feedback loops, edge-case omissions, selection bias), accuracy (e.g., mislabelling, bounding-box inaccuracies, granularity mismatches, bias-driven errors), and consistency (e.g., inter-annotator disagreement, ambiguous instructions, misaligned hand-offs, cross-modality inconsistencies). The taxonomy was validated with industry practitioners, who reported its usefulness for root-cause analysis, supplier quality reviews, onboarding, and improving annotation guidelines. They described it as a failure-mode catalogue similar to FMEA. By conceptualising annotation quality as a lifecycle and supply-chain issue, this study contributes to SE4AI by offering a shared vocabulary, diagnostic toolset, and actionable guidance for building trustworthy AI-enabled perception systems."}
{"id": "2511.16593", "categories": ["cs.SE", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.16593", "abs": "https://arxiv.org/abs/2511.16593", "authors": ["Diaeddin Rimawi"], "title": "Green Resilience of Cyber-Physical Systems: Doctoral Dissertation", "comment": null, "summary": "Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS."}
