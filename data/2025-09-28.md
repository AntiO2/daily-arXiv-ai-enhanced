<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: FZModules是一个异构框架，用于通过高性能模块组装误差有界的自定义压缩流水线，支持快速实验并实现与融合内核GPU压缩器相当的端到端加速，同时保持与高保真CPU或混合压缩器相似的率失真性能。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器生成的数据量超过内存和存储容量，限制了可扩展性。虽然有损压缩通过控制误差来减小数据占用空间并提高吞吐量，但最佳流水线高度依赖于数据和目标，需要压缩专业知识。GPU压缩器提供原始吞吐量，但通常硬编码融合内核，阻碍快速实验，并且在率失真方面表现不佳。

Method: 提出FZModules异构框架，通过简洁可扩展的接口从高性能模块组装误差有界的自定义压缩流水线。利用异步任务支持的执行库推断数据依赖关系、管理内存移动，并为强大的异步压缩流水线暴露分支和阶段级并发。

Result: 在四个代表性科学数据集上评估三个使用FZModules构建的流水线，结果显示它们可以实现与融合内核GPU压缩器相当的端到端加速，同时达到与更高保真度的CPU或混合压缩器相似的率失真性能。

Conclusion: FZModules框架能够实现快速、针对特定领域定制的压缩流水线设计，解决了科学数据压缩中的可扩展性和性能瓶颈问题。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [2] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享了在高性能计算中心部署生成式AI工作负载的经验，讨论了HPC与云计算环境的集成，提出了融合计算架构，并通过Llama大语言模型的案例研究展示了跨平台部署。


<details>
  <summary>Details</summary>
Motivation: 虽然生成式AI应用通常部署在云环境中，但在高性能计算中心的部署能力仍在发展中，需要探索HPC与云计算的集成方案。

Method: 采用融合计算架构，集成HPC和Kubernetes平台运行容器化的生成式AI工作负载，使用多种容器运行时在Kubernetes和HPC平台上部署vLLM推理服务器。

Result: 成功部署了Llama大语言模型，展示了跨平台容器化部署的可行性，为HPC容器社区提供了实践经验。

Conclusion: 这项工作为HPC容器社区提供了实用的考虑因素和发展机会，指导未来的研究和工具开发。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [3] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出了可扩展的分布式内存算法用于稀疏矩阵的置换、提取和赋值操作，采用Identify-Exchange-Build策略减少通信开销，相比现有库获得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有分布式稀疏矩阵操作库如CombBLAS和PETSc在性能上存在不足，特别是在通信开销方面，需要更高效的算法来提升大规模稀疏矩阵处理的性能。

Method: 采用Identify-Exchange-Build(IEB)策略：识别本地非零元素、交换所需数据、从接收元素构建本地子矩阵；使用无同步多线程算法加速本地计算。

Result: 在两个大学集群和Perlmutter超级计算机上的实验表明，相比CombBLAS和PETSc，新算法在矩阵置换、子图提取、流图应用等场景下获得显著性能提升。

Conclusion: 本研究为稀疏矩阵置换、提取和赋值操作提供了全面的算法研究、软件实现、实验评估和应用案例，证明了所提方法的有效性和优越性。

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [4] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RADICAL-Pilot与Flux和Dragon运行时系统集成，显著提升了混合AI-HPC工作负载的性能，任务执行速率可达1500+任务/秒，相比传统srun有4倍以上的吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 科学工作流日益结合HPC和机器学习任务，但传统启动器如srun在并发性和吞吐量方面存在限制，不适合动态异构工作负载。

Method: 将RADICAL-Pilot与Flux和Dragon两个互补的运行时系统集成，实现分层资源管理和高吞吐量函数执行，在Frontier系统上使用合成和生产规模工作负载进行性能研究。

Result: RP+Flux可持续执行930任务/秒，RP+Flux+Dragon超过1500任务/秒，利用率超过99.6%；而srun峰值仅为152任务/秒，利用率低于50%。在IMPECCABLE.v2药物发现应用中，RP+Flux相比srun/Slurm缩短30-60%完成时间。

Conclusion: RP与运行时系统的混合集成为混合AI-HPC工作负载提供了可扩展的解决方案。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [5] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 本文提出了一种名为tail batching的新型同步强化学习调度策略，通过将长尾响应整合到少数专用步骤中，显著减少GPU空闲时间，并开发了RollPacker系统实现端到端训练时间2.03x-2.56x的加速。


<details>
  <summary>Details</summary>
Motivation: 同步RL后训练存在GPU利用率低的问题，由响应长度不平衡导致的bubbles造成。现有系统通过放宽同步性来缓解，但会牺牲训练精度。

Method: 提出tail batching策略，将导致长尾响应的提示整合到少量长轮次中，确保大多数步骤只涉及平衡的短rollout。开发RollPacker系统，在三个RL阶段进行全面优化：弹性并行适配、动态资源分配调度和基于流的训练。

Result: 在128个H800 GPU上对Qwen2.5系列LLMs进行测试，相比veRL实现2.03x-2.56x端到端训练时间减少，相比RLHFuse最高实现2.24x加速。

Conclusion: tail batching策略有效减少GPU空闲时间，显著加速RL训练而不牺牲精度，RollPacker系统充分利用了这一优势。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [6] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文提出了一种通过利用输入矩阵稀疏性来改进GPU上Schur补矩阵组装的方法，在FETI方法中实现了5.1倍的GPU部分加速和3.3倍的整体组装加速。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算集群主要依赖GPU，需要加速域分解方法中的Schur补矩阵计算。显式组装Schur补矩阵成本高昂，是加速方法的主要开销。

Method: 通过明智利用输入矩阵的稀疏性来改进GPU上的Schur补矩阵组装过程，减少显式组装的开销。

Result: 在FETI方法中，GPU部分代码实现了5.1倍加速，整体组装实现了3.3倍加速，使得从仅10次迭代开始就能获得加速效益。

Conclusion: 利用输入矩阵稀疏性可以显著改进GPU上的Schur补矩阵组装性能，使GPU加速在较少迭代次数下即具有实际效益。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [7] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: 评估Mojo语言在GPU科学计算中的性能和可移植性，与CUDA和HIP对比，发现Mojo在内存密集型任务上表现良好，但在原子操作和快速数学计算密集型任务上存在差距。


<details>
  <summary>Details</summary>
Motivation: 探索基于MLIR的新语言Mojo在科学计算工作负载中的表现，旨在弥合Python生态系统中性能和生产力之间的差距。

Method: 针对四种科学计算工作负载：七点模板（内存密集型）、BabelStream（内存密集型）、miniBUDE（计算密集型）和Hartree-Fock（带原子操作的计算密集型），在NVIDIA H100和AMD MI300A GPU上与供应商基线进行性能比较。

Result: Mojo在内存密集型内核上的性能与CUDA和HIP相当，但在AMD GPU上的原子操作以及AMD和NVIDIA GPU上的快速数学计算密集型内核存在性能差距。

Conclusion: 虽然学习曲线和编程要求仍然相对底层，但Mojo可以在科学计算和AI融合的碎片化Python生态系统中弥合重要差距。

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [8] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 该论文提出了一种针对RRAM阵列的分布式内存原始-对偶混合梯度方法，通过算法-硬件协同设计解决大规模优化问题，相比GPU加速求解器实现了三个数量级的能耗和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本物理限制，无法满足计算工作负载的指数增长需求。内存计算(RRAM)虽然提供了低延迟和低能耗的模拟计算，但现有算法不适用于IMC，特别是对于需要频繁矩阵重编程的约束优化问题。

Method: 开发了分布式内存原始-对偶混合梯度方法，最小化昂贵的写入周期，包含对设备非理想性的鲁棒性，并利用对称块矩阵公式在分布式交叉阵列中统一操作。使用MELISO+物理模拟框架评估实际设备条件下的性能。

Result: 在大规模线性程序上与GPU加速求解器进行基准测试，显示基于RRAM的求解器实现了相当的精度，同时能耗和延迟降低了三个数量级。

Conclusion: 这是首个在RRAM上实现的PDHG线性规划求解器，展示了通过分布式内存计算的算法-硬件协同设计解决大规模优化问题的变革潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [9] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 提出了弹性流水线并行（EPP）方法，通过协调令牌级和批次级流水线并行来适应资源和负载异质性，并构建了InfiniPipe系统，实现了1.69倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方案在长上下文训练中存在通信开销大、内存消耗高或硬件利用率低的问题，且静态调度方法忽略了序列长度分布的偏斜性，导致性能不佳。

Method: 提出EPP方法协调两种流水线并行粒度，构建InfiniPipe系统，包含资源感知和负载均衡的序列处理器，以及联合优化流水线调度和梯度检查点的协同优化方法。

Result: 综合实验表明，InfiniPipe相比最先进系统实现了1.69倍的加速。

Conclusion: EPP和InfiniPipe系统有效解决了长上下文训练中的流水线并行挑战，通过自适应粒度选择和协同优化显著提升了训练效率。

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>
