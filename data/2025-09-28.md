<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.SE](#cs.SE) [Total: 18]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines](https://arxiv.org/abs/2509.20563)
*Skyler Ruiter,Jiannan Tian,Fengguang Song*

Main category: cs.DC

TL;DR: FZModules是一个异构框架，用于通过高性能模块组装误差有界的自定义压缩流水线，支持快速实验和领域定制设计。


<details>
  <summary>Details</summary>
Motivation: 现代科学模拟和仪器生成的数据量超过了内存和存储容量，限制了可扩展性。虽然损失压缩通过控制误差来减少存储空间和提高吞吐量，但最优流水线高度依赖于数据和目标，需要压缩专业知识。

Method: FZModules提供了一个异构框架，通过简洁可扩展的接口从高性能模块组装误差有界的自定义压缩流水线。利用异步任务支持的执行库推断数据依赖、管理内存移动，并暴露分支和阶段级并发以实现强大的异步压缩流水线。

Result: 在四个代表性科学数据集上评估了三个使用FZModules构建的流水线，结果显示它们可以达到与融合内核GPU压缩器相当的端到端加速，同时实现与更高保真度的CPU或混合压缩器相似的率失真性能。

Conclusion: FZModules框架能够实现快速、领域定制的压缩流水线设计，在保持高性能的同时提供灵活的压缩方案。

Abstract: Modern scientific simulations and instruments generate data volumes that
overwhelm memory and storage, throttling scalability. Lossy compression
mitigates this by trading controlled error for reduced footprint and throughput
gains, yet optimal pipelines are highly data and objective specific, demanding
compression expertise. GPU compressors supply raw throughput but often
hard-code fused kernels that hinder rapid experimentation, and underperform in
rate-distortion. We present FZModules, a heterogeneous framework for assembling
error-bounded custom compression pipelines from high-performance modules
through a concise extensible interface. We further utilize an asynchronous
task-backed execution library that infers data dependencies, manages memory
movement, and exposes branch and stage level concurrency for powerful
asynchronous compression pipelines. Evaluating three pipelines built with
FZModules on four representative scientific datasets, we show they can compare
end-to-end speedup of fused-kernel GPU compressors while achieving similar
rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid,
domain-tailored design.

</details>


### [2] [Experience Deploying Containerized GenAI Services at an HPC Center](https://arxiv.org/abs/2509.20603)
*Angel M. Beltre,Jeff Ogden,Kevin Pedretti*

Main category: cs.DC

TL;DR: 本文分享了在高性能计算中心部署生成式AI工作负载的经验，讨论了HPC与云计算环境的集成，提出了融合计算架构，并通过Llama大语言模型的案例研究展示了跨平台部署。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用通常由专用组件组成并在云环境中部署，但在高性能计算中心的部署能力仍在发展中。本文旨在探索如何在HPC环境中有效部署和管理GenAI工作负载。

Method: 提出融合计算架构，集成HPC和Kubernetes平台运行容器化的GenAI工作负载；使用容器化推理服务器(vLLM)在Kubernetes和HPC平台上部署Llama大语言模型，支持多种容器运行时。

Result: 成功实现了GenAI工作负载在HPC环境中的部署，展示了跨平台容器化部署的可行性，为HPC容器社区提供了实践经验。

Conclusion: 本文的经验强调了HPC容器社区的实际考虑和发展机会，为未来的研究和工具开发提供了指导，促进了HPC与云计算环境的有效集成。

Abstract: Generative Artificial Intelligence (GenAI) applications are built from
specialized components -- inference servers, object storage, vector and graph
databases, and user interfaces -- interconnected via web-based APIs. While
these components are often containerized and deployed in cloud environments,
such capabilities are still emerging at High-Performance Computing (HPC)
centers. In this paper, we share our experience deploying GenAI workloads
within an established HPC center, discussing the integration of HPC and cloud
computing environments. We describe our converged computing architecture that
integrates HPC and Kubernetes platforms running containerized GenAI workloads,
helping with reproducibility. A case study illustrates the deployment of the
Llama Large Language Model (LLM) using a containerized inference server (vLLM)
across both Kubernetes and HPC platforms using multiple container runtimes. Our
experience highlights practical considerations and opportunities for the HPC
container community, guiding future research and tool development.

</details>


### [3] [Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment](https://arxiv.org/abs/2509.20776)
*Elaheh Hassani,Md Taufique Hussain,Ariful Azad*

Main category: cs.DC

TL;DR: 提出了一种用于稀疏矩阵置换、提取和赋值的可扩展分布式内存算法，采用Identify-Exchange-Build策略减少通信开销，通过无同步多线程算法加速本地计算，性能优于现有库


<details>
  <summary>Details</summary>
Motivation: 现有的分布式内存稀疏矩阵操作（如SpGEMM方法）通信开销较大，需要更高效的算法来提升性能

Method: 采用Identify-Exchange-Build策略：识别本地非零元素、交换所需数据、从接收元素构建本地子矩阵，结合无同步多线程算法

Result: 在多个集群上测试，性能显著优于CombBLAS和PETSc，适用于负载均衡、矩阵重排序、子图提取和流图应用等场景

Conclusion: 为稀疏矩阵置换、提取和赋值操作提供了全面的算法研究、软件实现和实验评估

Abstract: We present scalable distributed-memory algorithms for sparse matrix
permutation, extraction, and assignment. Our methods follow an
Identify-Exchange-Build (IEB) strategy where each process identifies the local
nonzeros to be sent, exchanges the required data, and then builds its local
submatrix from the received elements. This approach reduces communication
compared to SpGEMM-based methods in distributed memory. By employing
synchronization-free multithreaded algorithms, we further accelerate local
computations, achieving substantially better performance than existing
libraries such as CombBLAS and PETSc. We design efficient software for these
operations and evaluate their performance on two university clusters and the
Perlmutter supercomputer. Our experiments span a variety of application
scenarios, including matrix permutation for load balancing, matrix reordering,
subgraph extraction, and streaming graph applications. In all cases, we compare
our algorithms against CombBLAS, the most comprehensive distributed library for
these operations, and, in some scenarios, against PETSc. Overall, this work
provides a comprehensive study of algorithms, software implementations,
experimental evaluations, and applications for sparse matrix permutation,
extraction, and assignment.

</details>


### [4] [Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads](https://arxiv.org/abs/2509.20819)
*Andre Merzky,Mikhail Titov,Matteo Turilli,Shantenu Jha*

Main category: cs.DC

TL;DR: RADICAL-Pilot与Flux和Dragon运行时系统集成，显著提升了混合HPC和AI工作负载的性能，相比传统Slurm/srun在任务执行速率和资源利用率方面有巨大改进。


<details>
  <summary>Details</summary>
Motivation: 科学工作流日益结合HPC和机器学习任务，但传统的启动器如Slurm的srun在并发性和吞吐量方面存在限制，无法适应动态异构工作负载的需求。

Method: 将RADICAL-Pilot与Flux和Dragon两个互补的运行时系统集成，实现分层资源管理和高吞吐量函数执行，并在Frontier系统上使用合成和生产规模工作负载进行性能研究。

Result: RP+Flux可持续达到930任务/秒，RP+Flux+Dragon超过1,500任务/秒，利用率超过99.6%；而srun峰值仅为152任务/秒，利用率低于50%。在IMPECCABLE.v2药物发现活动中，RP+Flux相比srun/Slurm减少了30-60%的完成时间，吞吐量提高了四倍以上。

Conclusion: 在RADICAL-Pilot中集成混合运行时系统为混合AI-HPC工作负载提供了一种可扩展的方法。

Abstract: Scientific workflows increasingly involve both HPC and machine-learning
tasks, combining MPI-based simulations, training, and inference in a single
execution. Launchers such as Slurm's srun constrain concurrency and throughput,
making them unsuitable for dynamic and heterogeneous workloads. We present a
performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two
complementary runtime systems that enable hierarchical resource management and
high-throughput function execution. Using synthetic and production-scale
workloads on Frontier, we characterize the task execution properties of RP
across runtime configurations. RP+Flux sustains up to 930 tasks/s, and
RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast,
srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%.
For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60%
relative to srun/Slurm and increases throughput more than four times on up to
1,024. These results demonstrate hybrid runtime integration in RP as a scalable
approach for hybrid AI-HPC workloads.

</details>


### [5] [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)
*Wei Gao,Yuheng Zhao,Dakai An,Tianyuan Wu,Lunxi Cao,Shaopan Xiong,Ju Huang,Weixun Wang,Siran Yang,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: 本文提出了一种名为tail batching的同步强化学习调度策略，通过将长尾响应整合到少量长轮次中，显著减少GPU空闲时间，加速RL训练而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 同步RL后训练存在GPU利用率低的问题，这是由于rollout步骤中响应长度不平衡导致的。现有系统通过放松同步性来缓解，但会损害训练准确性。

Method: 引入tail batching策略，将导致长尾响应的提示整合到少量长轮次中，确保大多数步骤只涉及平衡的短rollout。同时提出RollPacker系统，在三个RL阶段进行全面优化。

Result: RollPacker在128个H800 GPU上为Qwen2.5系列LLM实现了2.03x-2.56x的端到端训练时间减少，相比veRL和RLHFuse分别达到2.24x加速。

Conclusion: tail batching策略和RollPacker系统有效解决了同步RL训练中的GPU利用率问题，显著加速训练过程且不损失准确性。

Abstract: Reinforcement Learning (RL) is a pivotal post-training technique for
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
synchronous RL post-training often suffers from significant GPU
underutilization, referred to as bubbles, caused by imbalanced response lengths
within rollout steps. Many RL systems attempt to alleviate this problem by
relaxing synchronization, but this can compromise training accuracy. In this
paper, we introduce tail batching, a novel rollout scheduling strategy for
synchronous RL that systematically consolidates prompts leading to long-tail
responses into a small subset of rollout steps (long rounds), while ensuring
that the majority of steps (short rounds) involve only balanced, short
rollouts. By excluding long responses from short rounds and rescheduling them
into a few designated long rounds, tail batching effectively reduces GPU idle
time during rollouts and significantly accelerates RL training without
sacrificing accuracy. We present RollPacker, a system that fully harnesses the
benefits of tail batching through holistic optimizations across all three RL
stages: elastic parallelism adaptation for rollout, dynamic resource allocation
and scheduling for reward, and stream-based training. Empirical results show
that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction
compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5
family of LLMs on up to 128 H800 GPUs.

</details>


### [6] [Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods](https://arxiv.org/abs/2509.21037)
*Jakub Homola,Ondřej Meca,Lubomír Říha,Tomáš Brzobohatý*

Main category: cs.DC

TL;DR: 本文展示了通过利用输入矩阵的稀疏性来进一步改进GPU上Schur补矩阵的组装，在FETI方法中实现了5.1倍的GPU部分加速和3.3倍的整体组装加速。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算集群性能主要依赖于GPU，需要加速域分解方法中的Schur补矩阵计算。显式组装Schur补矩阵成本高昂，是GPU加速方法的主要开销。

Method: 通过明智利用输入矩阵的稀疏性来改进GPU上的Schur补矩阵组装过程，在FETI方法中实施这一优化策略。

Result: 在GPU代码部分实现了5.1倍的加速，整体组装过程实现了3.3倍的加速，使得从仅10次迭代开始就能获得加速效益。

Conclusion: 利用输入矩阵的稀疏性可以显著改进GPU上Schur补矩阵的组装效率，使得GPU加速在较少的迭代次数下就能产生效益。

Abstract: Schur complement matrices emerge in many domain decomposition methods that
can solve complex engineering problems using supercomputers. Today, as most of
the high-performance clusters' performance lies in GPUs, these methods should
also be accelerated.
  Typically, the offloaded components are the explicitly assembled dense Schur
complement matrices used later in the iterative solver for multiplication with
a vector. As the explicit assembly is expensive, it represents a significant
overhead associated with this approach to acceleration. It has already been
shown that the overhead can be minimized by assembling the Schur complements
directly on the GPU.
  This paper shows that the GPU assembly can be further improved by wisely
utilizing the sparsity of the input matrices. In the context of FETI methods,
we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the
whole assembly, making the acceleration beneficial from as few as 10
iterations.

</details>


### [7] [Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem](https://arxiv.org/abs/2509.21039)
*William F. Godoy,Tatiana Melnichenko,Pedro Valero-Lara,Wael Elwasif,Philip Fackler,Rafael Ferreira Da Silva,Keita Teranishi,Jeffrey S. Vetter*

Main category: cs.DC

TL;DR: Mojo语言在GPU科学计算中的性能与可移植性评估，与CUDA/HIP对比，在内存密集型任务表现相当，但在原子操作和快速数学计算密集型任务存在差距


<details>
  <summary>Details</summary>
Motivation: 探索基于MLIR的新型Mojo语言在科学计算工作负载中的性能和可移植性，旨在弥合Python生态系统在科学计算与AI融合中的性能差距

Method: 针对四种科学计算工作负载（七点模板、BabelStream、miniBUDE、Hartree-Fock），在NVIDIA H100和AMD MI300A GPU上与CUDA和HIP进行性能对比

Result: Mojo在内存密集型内核上与CUDA和HIP性能相当，但在AMD GPU上的原子操作以及两个平台上的快速数学计算密集型内核存在性能差距

Conclusion: 虽然学习曲线和编程要求仍较底层，但Mojo能够显著弥合Python生态系统在科学计算与AI融合中的碎片化差距

Abstract: We explore the performance and portability of the novel Mojo language for
scientific computing workloads on GPUs. As the first language based on the
LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure,
Mojo aims to close performance and productivity gaps by combining Python's
interoperability and CUDA-like syntax for compile-time portable GPU
programming. We target four scientific workloads: a seven-point stencil
(memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and
Hartree-Fock (compute-bound with atomic operations); and compare their
performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We
show that Mojo's performance is competitive with CUDA and HIP for memory-bound
kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math
compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve
and programming requirements are still fairly low-level, Mojo can close
significant gaps in the fragmented Python ecosystem in the convergence of
scientific computing and AI.

</details>


### [8] [From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem](https://arxiv.org/abs/2509.21137)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Gozde Tutuncuoglu,Junchi Yang,Feng Qiu,Murat Yildirim*

Main category: cs.DC

TL;DR: 提出了一种针对RRAM阵列的分布式内存原始-对偶混合梯度方法，通过最小化写入周期、增强设备非理想性鲁棒性，在大型线性规划问题上实现比GPU加速求解器能耗和延迟降低三个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统架构受限于基本限制，无法满足计算工作负载的指数增长。内存计算虽然提供了有前景的替代方案，但现有算法不适用于内存计算，特别是在约束优化问题中频繁的矩阵重编程成本过高。

Method: 分布式内存原始-对偶混合梯度方法，专门为RRAM设备阵列共同设计，采用对称块矩阵公式统一分布式交叉开关操作，并使用MELISO+物理模拟框架评估实际设备条件下的性能。

Result: 在大型线性规划问题上，与GPU加速求解器相比，RRAM求解器达到相当精度，同时能耗和延迟降低高达三个数量级。

Conclusion: 这是首个在RRAM上实现的PDHG线性规划求解器，展示了算法-硬件共同设计通过分布式内存计算解决大规模优化问题的变革潜力。

Abstract: The exponential growth of computational workloads is surpassing the
capabilities of conventional architectures, which are constrained by
fundamental limits. In-memory computing (IMC) with RRAM provides a promising
alternative by providing analog computations with significant gains in latency
and energy use. However, existing algorithms developed for conventional
architectures do not translate to IMC, particularly for constrained
optimization problems where frequent matrix reprogramming remains
cost-prohibitive for IMC applications. Here we present a distributed in-memory
primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays
of RRAM devices. Our approach minimizes costly write cycles, incorporates
robustness against device non-idealities, and leverages a symmetric
block-matrix formulation to unify operations across distributed crossbars. We
integrate a physics-based simulation framework called MELISO+ to evaluate
performance under realistic device conditions. Benchmarking against
GPU-accelerated solvers on large-scale linear programs demonstrates that our
RRAM-based solver achieves comparable accuracy with up to three orders of
magnitude reductions in energy consumption and latency. These results
demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the
transformative potential of algorithm-hardware co-design for solving
large-scale optimization through distributed in-memory computing.

</details>


### [9] [Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training](https://arxiv.org/abs/2509.21275)
*Shiju Wang,Yujie Wang,Ao Sun,Fangcheng Fu,Zijian Zhu,Bin Cui,Xu Han,Kaisheng Ma*

Main category: cs.DC

TL;DR: 提出了弹性流水线并行(EPP)和InfiniPipe系统，通过自适应选择流水线并行粒度来优化长上下文训练，相比现有系统实现1.69倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行方案在长上下文训练中存在内存消耗与硬件利用率之间的权衡问题，且真实数据集序列长度分布不均导致负载不均衡，需要自适应解决方案。

Method: 提出弹性流水线并行(EPP)协调token级和batch级流水线并行，构建InfiniPipe系统，包含资源感知的序列处理器和联合优化流水线调度与梯度检查点的机制。

Result: 综合实验表明InfiniPipe相比最先进系统实现1.69倍加速。

Conclusion: EPP和InfiniPipe系统有效解决了长上下文训练中的流水线并行挑战，通过自适应粒度选择和负载均衡优化实现了显著性能提升。

Abstract: Long context training is crucial for LLM's context extension. Existing
schemes, such as sequence parallelism, incur substantial communication
overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness
hinges on partitioning granularity. Batch-level PP dividing input samples
exhibits high memory consumption in long-context scenario, whereas token-level
PP splitting sequences into slices alleviates memory overhead but may incur
hardware under-utilization. This trade-off motivates adaptively selecting PP
granularity to match resource and workload characteristics. Moreover, sequence
length distribution of the real-world dataset exhibits skewness, posing a
challenge on PP's workload balance and efficient scheduling. Current static PP
scheduling methods overlook the variance of sequence length, leading to
suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism
(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource
and workload heterogeneity. We build InfiniPipe, a distributed training system
that unleashes the potential of EPP via (1) a resource-aware and
workload-balanced sequence processor that splits long sequences and packs short
ones; and (2) a co-optimization methodology that jointly optimizes pipeline
schedule and gradient checkpointing via a mechanism named stage-aware
chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that
InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [ACCeLLiuM: Supervised Fine-Tuning for Automated OpenACC Pragma Generation](https://arxiv.org/abs/2509.20380)
*Samyak Jhaveri,Vanessa Klotzmann,Crista Lopes*

Main category: cs.SE

TL;DR: ACCeLLiuM是专门为生成OpenACC指令而微调的两个开源大语言模型，能够为数据并行循环自动生成专家级OpenACC指令，显著提升GPU编程效率。


<details>
  <summary>Details</summary>
Motivation: GPU硬件和并行编程框架日益复杂，虽然OpenACC等基于指令的编程标准简化了GPU编程，但仍需要相当的专业知识才能有效使用这些指令。

Method: 构建了包含4,033个OpenACC pragma-loop对的监督微调数据集，并基于此微调了两个大语言模型，专门用于为数据并行循环生成OpenACC指令。

Result: 在测试集上，基础LLMs无法一致生成有效指令，而微调后的模型能为87%的数据并行循环生成正确类型的指令，50%的情况下能生成完全匹配的指令。

Conclusion: ACCeLLiuM显著降低了自动GPU卸载的门槛，为LLM驱动的OpenACC指令生成建立了可复现的基准，并公开发布了代码、模型和数据集。

Abstract: The increasing ubiquity of GPUs is accompanied by the increasing complexity
of their hardware and parallel programming frameworks. Directive-based parallel
programming standards like OpenACC simplify GPU programming to some extent by
abstracting away low-level complexities, but a fair amount of expertise is
still required in order to use those directives effectively.
  We introduce ACCeLLiuM, two open weights Large Language Models specifically
fine-tuned for generating expert OpenACC directives for data-parallel loops,
along with the supervised fine-tuning dataset that was used to train them. The
ACCeLLiuM SFT dataset contains 4,033 OpenACC pragma-loop pairs mined from
public GitHub C/C++ repositories, with 3,223 pairs for training and 810 for
testing. Experimental evaluations show a pronounced performance gap in
generating correct OpenACC pragmas between base LLMs and our fine-tuned
versions. On the held-out test set, base LLMs fail to consistently generate
valid pragmas, whereas LLMs fine-tuned on the ACCeLLiuM dataset generate valid
pragmas with the correct directive type for $87\%$ of the data-parallel loops,
and exact pragmas--including directives, clauses, clause order, and clause
variables--for $50\%$ of the cases. Even when not exact, generated pragmas
frequently incorporate the correct clauses in a different order than the
ground-truth label, or include additional clauses that enable finer control
over parallel execution, data movement, and concurrency, offering practical
value beyond strict string-matching. By publicly releasing the code, models,
and dataset as ACCeLLiuM we hope to establish a reproducible benchmark for
LLM-powered OpenACC pragma generation, and lower the barrier to automated GPU
offloading of serially written programs.

</details>


### [11] [State-of-the-Art in Software Security Visualization: A Systematic Review](https://arxiv.org/abs/2509.20385)
*Ishara Devendra,Chaman Wijesiriwardana,Prasad Wimalaratne*

Main category: cs.SE

TL;DR: 本文系统综述了软件安全可视化领域的研究，提出了包含图基、符号基、矩阵基和隐喻基四种类型的综合分类法，分析了60多篇关键论文，强调了该领域的主要问题、最新进展和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统日益复杂和威胁环境不断演变，传统的基于文本和数值的安全分析方法变得越来越低效，需要将复杂的安全数据转化为易于理解的视觉格式。

Method: 通过文献系统综述方法，对60多篇关键研究论文进行分析，创建了软件安全可视化技术的综合分类法，将其分为图基、符号基、矩阵基和隐喻基四种类型。

Result: 识别出两个主要研究领域：广泛的软件开发可视化（关注软件架构描绘方法）和操作安全与网络安全可视化，强调了适应不断演变安全环境的创新可视化技术的必要性。

Conclusion: 软件安全可视化对于增强威胁检测、改进安全响应策略具有实际意义，并为未来研究提供了指导方向。

Abstract: Software security visualization is an interdisciplinary field that combines
the technical complexity of cybersecurity, including threat intelligence and
compliance monitoring, with visual analytics, transforming complex security
data into easily digestible visual formats. As software systems get more
complex and the threat landscape evolves, traditional text-based and numerical
methods for analyzing and interpreting security concerns become increasingly
ineffective. The purpose of this paper is to systematically review existing
research and create a comprehensive taxonomy of software security visualization
techniques through literature, categorizing these techniques into four types:
graph-based, notation-based, matrix-based, and metaphor-based visualization.
This systematic review explores over 60 recent key research papers in software
security visualization, highlighting its key issues, recent advancements, and
prospective future research directions. From the comprehensive analysis, the
two main areas were distinctly highlighted as extensive software development
visualization, focusing on advanced methods for depicting software
architecture: operational security visualization and cybersecurity
visualization. The findings highlight the necessity for innovative
visualization techniques that adapt to the evolving security landscape, with
practical implications for enhancing threat detection, improving security
response strategies, and guiding future research.

</details>


### [12] [Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments](https://arxiv.org/abs/2509.20386)
*Nishant Gaurav,Adit Akarsh,Ankit Ranjan,Manoj Bajaj*

Main category: cs.SE

TL;DR: Dynamic ReAct是一种新方法，使ReAct智能体能够在包含数百或数千个工具的MCP工具集中高效操作，通过渐进式工具选择架构减少50%的工具加载，同时保持任务完成准确性。


<details>
  <summary>Details</summary>
Motivation: 解决在包含大量可用工具的环境中，由于上下文内存限制无法同时加载所有工具的根本挑战。

Method: 提出并评估了五种渐进式改进工具选择过程的架构，最终实现具有最小计算开销的智能工具选择的搜索加载机制。

Result: 实验结果表明，该方法减少了高达50%的工具加载，同时保持了任务完成准确性。

Conclusion: 该方法推进了真正通用AI智能体的发展，使其能够动态适应多样化的任务环境。

Abstract: We present Dynamic ReAct, a novel approach for enabling ReAct agents to ef-
ficiently operate with extensive Model Control Protocol (MCP) tool sets that
exceed the contextual memory limitations of large language models. Our approach
addresses the fundamental challenge of tool selection in environments
containing hundreds or thousands of available tools, where loading all tools
simultaneously is computationally infeasible. We propose and evaluate five
distinct architectures that progressively refine the tool selection process,
culminating in a search-and-load mechanism that achieves intelligent tool
selection with minimal computational overhead. Our experimental results
demonstrate that the proposed approach reduces tool loading by up to 50% while
maintaining task completion accuracy, advancing the path towards truly
general-purpose AI agents capable of dynamically adapting to diverse task
environments.

</details>


### [13] [Towards Systematic Specification and Verification of Fairness Requirements: A Position Paper](https://arxiv.org/abs/2509.20387)
*Qusai Ramadan,Jukka Ruohonen,Abhishek Tiwari,Adam Alami,Zeyd Boukhers*

Main category: cs.SE

TL;DR: 提出基于知识图谱的公平性框架，用于形式化地指定和验证软件系统中的公平性需求，解决因缺乏明确公平性要求而导致的歧视问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究将软件系统中的歧视归因于算法设计缺陷或数据偏见，但忽略了缺乏明确可验证的公平性要求是歧视的主要原因。专家关于公平性的知识通常是隐性的，难以形式化。

Method: 借鉴安全工程领域的成功经验，提出开发基于知识图谱的框架来形式化公平性知识，辅助需求规范和验证。

Result: 提出了开发公平性知识图谱框架的挑战、研究问题和路线图。

Conclusion: 基于知识图谱的方法有望为软件系统公平性需求的形式化规范和验证提供有效机制，填补现有研究的空白。

Abstract: Decisions suggested by improperly designed software systems might be prone to
discriminate against people based on protected characteristics, such as gender
and ethnicity. Previous studies attribute such undesired behavior to flaws in
algorithmic design or biased data. However, these studies ignore that
discrimination is often the result of a lack of well-specified fairness
requirements and their verification. The fact that experts' knowledge about
fairness is often implicit makes the task of specifying precise and verifiable
fairness requirements difficult. In related domains, such as security
engineering, knowledge graphs have been proven to be effective in formalizing
knowledge to assist requirements specification and verification. To address the
lack of formal mechanisms for specifying and verifying fairness requirements,
we propose the development of a knowledge graph-based framework for fairness.
In this paper, we discuss the challenges, research questions, and a road map
towards addressing the research questions.

</details>


### [14] [Online-Optimized RAG for Tool Use and Function Calling](https://arxiv.org/abs/2509.20415)
*Yu Pan,Xiaocheng Li,Hanzhao Wang*

Main category: cs.SE

TL;DR: Online-Optimized RAG是一个部署时框架，通过实时交互持续优化检索嵌入，解决嵌入不对齐问题，提高工具选择和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中由于不完美的嵌入模型或噪声描述导致的嵌入不对齐问题，这种不对齐可能导致错误检索和任务失败。

Method: 使用轻量级在线梯度更新，从实时交互中持续适应检索嵌入，仅需最小反馈（如任务成功），无需改变底层LLM。

Result: 在多样化的工具使用和文档检索场景中，该方法持续提高了工具选择准确性和最终任务成功率。

Conclusion: Online-Optimized RAG为构建鲁棒、自我改进的RAG系统提供了一条简单实用的路径。

Abstract: In many applications, retrieval-augmented generation (RAG) drives tool use
and function calling by embedding the (user) queries and matching them to
pre-specified tool/function descriptions. In this paper, we address an
embedding misalignment issue that often arises in practical applications due to
imperfect embedding models or noisy descriptions; such misalignment may lead to
incorrect retrieval and task failure. We introduce Online-Optimized RAG, a
deployment-time framework that continually adapts retrieval embeddings from
live interactions using minimal feedback (e.g., task success). Online-Optimized
RAG applies lightweight online gradient updates with negligible per-query
latency and requires no changes to the underlying LLM. The method is
plug-and-play: it supports both single- and multi-hop tool use, dynamic tool
inventories, and $K$-retrieval with re-ranking. We provide a problem-dependent
theoretical analysis that quantifies how the method's performance depends on
the initialization quality of the embeddings and other related quantities.
Across diverse tool-use and document-retrieval scenarios, our Online-Optimized
RAG consistently improves tool selection accuracy and end-task success, thus
providing a simple, practical path to robust, self-improving RAG systems.

</details>


### [15] [Formal Verification of Legal Contracts: A Translation-based Approach](https://arxiv.org/abs/2509.20421)
*Reiner Hähnle,Cosimo Laneve,Adele Veschetti*

Main category: cs.SE

TL;DR: Stipula是一种用于建模法律合同的领域特定编程语言，本文提出通过将其转换为带有JML规范的Java代码，并使用KeY验证工具来自动验证合同正确性的方法。


<details>
  <summary>Details</summary>
Motivation: 为了确保涉及资产转移和义务的法律合同的可执行属性正确性，需要形式化验证方法来保证合同的可靠性。

Method: 将Stipula合同翻译为带有Java Modeling Language规范的Java代码，并使用KeY演绎验证工具作为验证后端，对具有不相交循环的Stipula合同子集进行部分和完全正确性验证。

Result: 翻译和验证过程完全自动化，成功证明通用演绎验证工具可以在翻译方法中有效使用。

Conclusion: 该方法展示了使用通用演绎验证工具成功验证领域特定语言合同的可行性，为法律合同的形式化验证提供了有效途径。

Abstract: Stipula is a domain-specific programming language designed to model legal
contracts with enforceable properties, especially those involving asset
transfers and obligations. This paper presents a methodology to formally verify
the correctness of Stipula contracts through translation into Java code
annotated with Java Modeling Language specifications. As a verification
backend, the deductive verification tool KeY is used. Both, the translation and
the verification of partial and total correctness for a large subset of Stipula
contracts, those with disjoint cycles, is fully automatic. Our work
demonstrates that a general-purpose deductive verification tool can be used
successfully in a translation approach.

</details>


### [16] [AI-Specific Code Smells: From Specification to Detection](https://arxiv.org/abs/2509.20491)
*Brahim Mahmoudi,Naouel Moha,Quentin Stievenert,Florent Avellaneda*

Main category: cs.SE

TL;DR: SpecDetect4AI是一个基于工具的方法，用于大规模规范和检测AI特定代码异味，结合了高级声明性领域特定语言和可扩展的静态分析工具，在826个AI系统中实现了88.66%的精确率和88.89%的召回率。


<details>
  <summary>Details</summary>
Motivation: AI系统的兴起带来了新的软件问题，现有检测工具经常错过AI特定的代码异味，这些异味可能导致不可重现性、静默故障或模型泛化能力差等深层问题。

Method: 结合高级声明性领域特定语言进行规则规范，以及可扩展的静态分析工具来解释和检测AI系统中的这些规则。

Result: 在826个AI系统（2000万行代码）上评估，实现了88.66%的精确率和88.89%的召回率，优于其他现有检测工具，系统可用性评分为81.7/100。

Conclusion: SpecDetect4AI通过专用规则有效支持AI特定代码异味的规范和检测，能够高效分析大型AI系统，展示了效率和可扩展性。

Abstract: The rise of Artificial Intelligence (AI) is reshaping how software systems
are developed and maintained. However, AI-based systems give rise to new
software issues that existing detection tools often miss. Among these, we focus
on AI-specific code smells, recurring patterns in the code that may indicate
deeper problems such as unreproducibility, silent failures, or poor model
generalization. We introduce SpecDetect4AI, a tool-based approach for the
specification and detection of these code smells at scale. This approach
combines a high-level declarative Domain-Specific Language (DSL) for rule
specification with an extensible static analysis tool that interprets and
detects these rules for AI-based systems. We specified 22 AI-specific code
smells and evaluated SpecDetect4AI on 826 AI-based systems (20M lines of code),
achieving a precision of 88.66% and a recall of 88.89%, outperforming other
existing detection tools. Our results show that SpecDetect4AI supports the
specification and detection of AI-specific code smells through dedicated rules
and can effectively analyze large AI-based systems, demonstrating both
efficiency and extensibility (SUS 81.7/100).

</details>


### [17] [PromptDebt: A Comprehensive Study of Technical Debt Across LLM Projects](https://arxiv.org/abs/2509.20497)
*Ahmed Aljohani,Hyunsook Do*

Main category: cs.SE

TL;DR: 首个大规模实证研究显示，LLM集成带来了特定形式的技术债务，其中OpenAI集成占54.49%，LangChain占12.35%，提示设计是主要债务来源。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs通过API嵌入软件，这些集成带来了自承认技术债务(SATD)，需要系统研究其起源、流行度和缓解策略。

Method: 分析了93,142个Python文件，涵盖主要LLM API，识别LLM特定SATD的来源和分布。

Result: 54.49%的SATD实例来自OpenAI集成，12.35%来自LangChain；提示设计是主要债务来源(6.61%)，指令提示(38.60%)和少样本提示(18.13%)最易产生债务。

Conclusion: LLM集成带来了显著的技术债务挑战，提示设计是主要问题来源，需要更好的管理和缓解策略。

Abstract: Large Language Models (LLMs) are increasingly embedded in software via APIs
like OpenAI, offering powerful AI features without heavy infrastructure. Yet
these integrations bring their own form of self-admitted technical debt (SATD).
In this paper, we present the first large-scale empirical study of LLM-specific
SATD: its origins, prevalence, and mitigation strategies. By analyzing 93,142
Python files across major LLM APIs, we found that 54.49% of SATD instances stem
from OpenAI integrations and 12.35% from LangChain use. Prompt design emerged
as the primary source of LLM-specific SATD, with 6.61% of debt related to
prompt configuration and optimization issues, followed by hyperparameter tuning
and LLM-framework integration. We further explored which prompt techniques
attract the most debt, revealing that instruction-based prompts (38.60%) and
few-shot prompts (18.13%) are particularly vulnerable due to their dependence
on instruction clarity and example quality. Finally, we release a comprehensive
SATD dataset to support reproducibility and offer practical guidance for
managing technical debt in LLM-powered systems.

</details>


### [18] [Enhancing Python Programming Education with an AI-Powered Code Helper: Design, Implementation, and Impact](https://arxiv.org/abs/2509.20518)
*Sayed Mahbub Hasan Amiri,Md Mainul Islam*

Main category: cs.SE

TL;DR: 开发了一个基于AI的Python聊天机器人，结合静态代码分析、动态执行追踪和大型语言模型，帮助学生解决编程问题，提高学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统IDE和静态分析工具缺乏交互式指导，而AI代码助手如GitHub Copilot主要关注代码完成而非学习过程。需要填补这一空白，提供教育导向的编程辅助工具。

Method: 采用混合架构：使用CodeLlama进行代码嵌入，GPT-4处理自然语言交互，Docker沙箱确保安全执行。结合静态分析和动态追踪技术。

Result: 在1500份学生提交中达到85%的错误解决成功率，优于pylint(62%)和GPT-4(73%)。调试时间减少59.3%，编程熟练度提高34%，尤其在递归和异常处理方面。

Conclusion: 该研究展示了AI工具如何在编程教育中平衡技术创新与教学同理心，优先考虑教育公平和长期技能保留，而不仅仅是代码完成。

Abstract: This is the study that presents an AI-Python-based chatbot that helps
students to learn programming by demonstrating solutions to such problems as
debugging errors, solving syntax problems or converting abstract theoretical
concepts to practical implementations. Traditional coding tools like Integrated
Development Environments (IDEs) and static analyzers do not give robotic help
while AI-driven code assistants such as GitHub Copilot focus on getting things
done. To close this gap, our chatbot combines static code analysis, dynamic
execution tracing, and large language models (LLMs) to provide the students
with relevant and practical advice, hence promoting the learning process. The
chatbots hybrid architecture employs CodeLlama for code embedding, GPT-4 for
natural language interactions, and Docker-based sandboxing for secure
execution. Evaluated through a mixed-methods approach involving 1,500 student
submissions, the system demonstrated an 85% error resolution success rate,
outperforming standalone tools like pylint (62%) and GPT-4 (73%). Quantitative
results revealed a 59.3% reduction in debugging time among users, with pre- and
post-test assessments showing a 34% improvement in coding proficiency,
particularly in recursion and exception handling. Qualitative feedback from 120
students highlighted the chatbots clarity, accessibility, and
confidence-building impact, though critiques included occasional latency and
restrictive code sanitization. By balancing technical innovation with
pedagogical empathy, this research provides a blueprint for AI tools that
prioritize educational equity and long-term skill retention over mere code
completion. The chatbot exemplifies how AI can augment human instruction,
fostering deeper conceptual understanding in programming education.

</details>


### [19] [Enhancing LLM-based Fault Localization with a Functionality-Aware Retrieval-Augmented Generation Framework](https://arxiv.org/abs/2509.20552)
*Xinyu Shi,Zhenhao Li,An Ran Chen*

Main category: cs.SE

TL;DR: FaR-Loc是一个基于LLM和检索增强生成(RAG)的故障定位框架，通过功能描述生成、语义密集检索和LLM重排序三个组件，显著提升了方法级故障定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的故障定位方法在处理复杂系统时存在项目特定知识缺乏和大型项目导航困难的问题，需要更有效的解决方案。

Method: FaR-Loc包含三个核心组件：LLM功能提取模块生成失败行为的自然语言描述；语义密集检索组件在共享语义空间中嵌入功能描述和覆盖方法；LLM重排序模块基于上下文相关性重新排序检索到的方法。

Result: 在Defects4J基准测试中，FaR-Loc在Top-1准确率上比SoapFL和AutoFL分别提升14.6%和9.1%，在Top-5准确率上分别提升19.2%和22.1%，超越了所有基于学习和基于频谱的基线方法。

Conclusion: FaR-Loc通过结合LLM和RAG技术，显著提升了故障定位性能，且无需重新训练，具有实际应用价值。结合代码结构的预训练嵌入模型可进一步提升性能。

Abstract: Fault localization (FL) is a critical but time-consuming task in software
debugging, aiming to identify faulty code elements. While recent advances in
large language models (LLMs) have shown promise for FL, they often struggle
with complex systems due to the lack of project-specific knowledge and the
difficulty of navigating large projects. To address these limitations, we
propose FaR-Loc, a novel framework that enhances method-level FL by integrating
LLMs with retrieval-augmented generation (RAG). FaR-Loc consists of three key
components: LLM Functionality Extraction, Semantic Dense Retrieval, and LLM
Re-ranking. First, given a failed test and its associated stack trace, the LLM
Functionality Extraction module generates a concise natural language
description that captures the failing behavior. Next, the Semantic Dense
Retrieval component leverages a pre-trained code-understanding encoder to embed
both the functionality description (natural language) and the covered methods
(code) into a shared semantic space, enabling the retrieval of methods with
similar functional behavior. Finally, the LLM Re-ranking module reorders the
retrieved methods based on their contextual relevance. Our experiments on the
widely used Defects4J benchmark show that FaR-Loc outperforms state-of-the-art
LLM-based baselines SoapFL and AutoFL, by 14.6% and 9.1% in Top-1 accuracy, by
19.2% and 22.1% in Top-5 accuracy, respectively. It also surpasses all
learning-based and spectrum-based baselines across all Top-N metrics without
requiring re-training. Furthermore, we find that pre-trained code embedding
models that incorporate code structure, such as UniXcoder, can significantly
improve fault localization performance by up to 49.0% in Top-1 accuracy.
Finally, we conduct a case study to illustrate the effectiveness of FaR-Loc and
to provide insights for its practical application.

</details>


### [20] [Design, Implementation and Evaluation of a Novel Programming Language Topic Classification Workflow](https://arxiv.org/abs/2509.20631)
*Michael Zhang,Yuan Tian,Mariam Guizani*

Main category: cs.SE

TL;DR: 提出了一种编程语言主题分类工作流，使用多标签SVM结合滑动窗口和投票策略，在IBM Project CodeNet数据集上实现了高精度分类。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模和复杂度的增长，理解源代码中编程语言主题分布对技术决策、新人培训、工具开发和教育具有重要意义。

Method: 结合多标签支持向量机(SVM)、滑动窗口和投票策略，实现核心语言概念的细粒度定位，如运算符重载、虚函数、继承和模板等。

Result: 在IBM Project CodeNet数据集上训练，模型在主题分类上平均F1得分为0.90，在代码-主题高亮上得分为0.75。

Conclusion: 为代码分析和数据驱动软件工程的研究者和实践者提供了实证见解和可复用的分析管道。

Abstract: As software systems grow in scale and complexity, understanding the
distribution of programming language topics within source code becomes
increasingly important for guiding technical decisions, improving onboarding,
and informing tooling and education. This paper presents the design,
implementation, and evaluation of a novel programming language topic
classification workflow. Our approach combines a multi-label Support Vector
Machine (SVM) with a sliding window and voting strategy to enable fine-grained
localization of core language concepts such as operator overloading, virtual
functions, inheritance, and templates. Trained on the IBM Project CodeNet
dataset, our model achieves an average F1 score of 0.90 across topics and 0.75
in code-topic highlight. Our findings contribute empirical insights and a
reusable pipeline for researchers and practitioners interested in code analysis
and data-driven software engineering.

</details>


### [21] [Exploring Engagement in Hybrid Meetings](https://arxiv.org/abs/2509.20780)
*Daniela Grassi,Fabio Calefato,Darja Smite,Nicole Novielli,Filippo Lanubile*

Main category: cs.SE

TL;DR: 本研究通过多模态方法测量混合会议中的参与度模式，发现现场和远程参与者的参与度相当，但远程参与者在长时间会议中参与度较低。主动角色与更高参与度正相关，而大型会议和下午时段则与较低参与度相关。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行后混合工作的广泛采用从根本上改变了软件开发实践，在传统办公室结构向灵活工作安排过渡的过程中引入了沟通和协作的新挑战。远程参与会议可能导致远程团队成员的孤立、疏离和参与度降低。

Method: 研究了三家软件公司的专业人士数周时间，采用多模态方法测量参与度。通过自我报告问卷和生物识别设备的生理测量在混合会议期间收集数据。

Result: 回归分析显示现场和远程参与者的参与度水平相当，但远程参与者在长时间会议中无论参与模式如何都表现出较低的参与度。主动角色与更高参与度正相关，而大型会议和下午时段与较低参与度相关。

Conclusion: 研究结果为混合会议中与参与度和脱离相关的因素提供了见解，以及潜在的会议改进建议。这些见解不仅对软件团队相关，也对面临类似混合协作挑战的各行业知识密集型组织具有潜在相关性。

Abstract: Background. The widespread adoption of hybrid work following the COVID-19
pandemic has fundamentally transformed software development practices,
introducing new challenges in communication and collaboration as organizations
transition from traditional office-based structures to flexible working
arrangements. This shift has established a new organizational norm where even
traditionally office-first companies now embrace hybrid team structures. While
remote participation in meetings has become commonplace in this new
environment, it may lead to isolation, alienation, and decreased engagement
among remote team members. Aims. This study aims to identify and characterize
engagement patterns in hybrid meetings through objective measurements, focusing
on the differences between co-located and remote participants. Method. We
studied professionals from three software companies over several weeks,
employing a multimodal approach to measure engagement. Data were collected
through self-reported questionnaires and physiological measurements using
biometric devices during hybrid meetings to understand engagement dynamics.
Results. The regression analyses revealed comparable engagement levels between
onsite and remote participants, though remote participants show lower
engagement in long meetings regardless of participation mode. Active roles
positively correlate with higher engagement, while larger meetings and
afternoon sessions are associated with lower engagement. Conclusions. Our
results offer insights into factors associated with engagement and
disengagement in hybrid meetings, as well as potential meeting improvement
recommendations. These insights are potentially relevant not only for software
teams but also for knowledge-intensive organizations across various sectors
facing similar hybrid collaboration challenges.

</details>


### [22] [Verification Limits Code LLM Training](https://arxiv.org/abs/2509.20837)
*Srishti Gureja,Elena Tommasone,Jingyi He,Sara Hooker,Matthias Gallé,Marzieh Fadaee*

Main category: cs.SE

TL;DR: 论文研究了代码生成中合成数据验证的瓶颈问题，发现当前验证方法过于严格，过滤了有价值的多样性。通过分析验证设计策略，提出了校准验证与多样化问题-解决方案对结合的方法来突破验证天花板。


<details>
  <summary>Details</summary>
Motivation: 代码生成模型越来越依赖合成数据，但合成验证器的能力限制了训练数据的质量和多样性，形成了验证天花板瓶颈。

Method: 系统研究验证设计和策略的影响：(i)分析测试复杂度和数量的影响；(ii)探索宽松通过阈值和LLM软验证；(iii)通过控制比较和人工评估验证必要性。

Result: 更丰富的测试套件平均提升3% pass@1性能；宽松验证阈值可恢复有价值训练数据，带来2-4点pass@1提升；保留多样化正确解决方案带来持续泛化收益。

Conclusion: 当前验证实践过于严格，但不能完全丢弃，需要重新校准。通过结合校准验证与多样化、具有挑战性的问题-解决方案对，可以突破验证天花板，开发更强的代码生成模型。

Abstract: Large language models for code generation increasingly rely on synthetic
data, where both problem solutions and verification tests are generated by
models. While this enables scalable data creation, it introduces a previously
unexplored bottleneck: the verification ceiling, in which the quality and
diversity of training data are fundamentally constrained by the capabilities of
synthetic verifiers. In this work, we systematically study how verification
design and strategies influence model performance. We investigate (i) what we
verify by analyzing the impact of test complexity and quantity: richer test
suites improve code generation capabilities (on average +3 pass@1), while
quantity alone yields diminishing returns, (ii) how we verify by exploring
relaxed pass thresholds: rigid 100% pass criteria can be overly restrictive. By
allowing for relaxed thresholds or incorporating LLM-based soft verification,
we can recover valuable training data, leading to a 2-4 point improvement in
pass@1 performance. However, this benefit is contingent upon the strength and
diversity of the test cases used, and (iii) why verification remains necessary
through controlled comparisons of formally correct versus incorrect solutions
and human evaluation: retaining diverse correct solutions per problem yields
consistent generalization gains. Our results show that Verification as
currently practiced is too rigid, filtering out valuable diversity. But it
cannot be discarded, only recalibrated. By combining calibrated verification
with diverse, challenging problem-solution pairs, we outline a path to break
the verification ceiling and unlock stronger code generation models.

</details>


### [23] [PseudoBridge: Pseudo Code as the Bridge for Better Semantic and Logic Alignment in Code Retrieval](https://arxiv.org/abs/2509.20881)
*Yixuan Li,Xinyi Liu,Weidong Yang,Ben Fei,Shuhao Li,Mingjie Zhou,Lipeng Ma*

Main category: cs.SE

TL;DR: 提出了PseudoBridge框架，通过引入伪代码作为中间模态来弥合自然语言查询与编程语言代码之间的语义鸿沟，提升代码检索的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练语言模型的代码检索方法仍面临语义鸿沟和代码风格多样性带来的挑战，需要更好的对齐机制来提升检索性能。

Method: 采用两阶段方法：1) 使用大语言模型生成伪代码实现自然语言与伪代码的显式对齐；2) 通过逻辑不变代码风格增强策略生成风格多样但逻辑等价的代码实现，增强模型对代码风格变化的鲁棒性。

Result: 在10种预训练模型和6种主流编程语言上的实验表明，PseudoBridge显著优于基线方法，在检索准确率和泛化能力上都有显著提升，特别是在零样本领域迁移场景下表现优异。

Conclusion: 通过伪代码实现显式逻辑对齐是有效的，PseudoBridge为代码检索提供了一个鲁棒且可泛化的解决方案。

Abstract: Code search aims to precisely find relevant code snippets that match natural
language queries within massive codebases, playing a vital role in software
development. Recent advances leverage pre-trained language models (PLMs) to
bridge the semantic gap between unstructured natural language (NL) and
structured programming languages (PL), yielding significant improvements over
traditional information retrieval and early deep learning approaches. However,
existing PLM-based methods still encounter key challenges, including a
fundamental semantic gap between human intent and machine execution logic, as
well as limited robustness to diverse code styles. To address these issues, we
propose PseudoBridge, a novel code retrieval framework that introduces
pseudo-code as an intermediate, semi-structured modality to better align NL
semantics with PL logic. Specifically, PseudoBridge consists of two stages.
First, we employ an advanced large language model (LLM) to synthesize
pseudo-code, enabling explicit alignment between NL queries and pseudo-code.
Second, we introduce a logic-invariant code style augmentation strategy and
employ the LLM to generate stylistically diverse yet logically equivalent code
implementations with pseudo-code, then align the code snippets of different
styles with pseudo-code, enhancing model robustness to code style variation. We
build PseudoBridge across 10 different PLMs and evaluate it on 6 mainstream
programming languages. Extensive experiments demonstrate that PseudoBridge
consistently outperforms baselines, achieving significant gains in retrieval
accuracy and generalization, particularly under zero-shot domain transfer
scenarios such as Solidity and XLCoST datasets. These results demonstrate the
effectiveness of explicit logical alignment via pseudo-code and highlight
PseudoBridge's potential as a robust, generalizable solution for code
retrieval.

</details>


### [24] [Designing for Novice Debuggers: A Pilot Study on an AI-Assisted Debugging Tool](https://arxiv.org/abs/2509.21067)
*Oka Kurniawan,Erick Chandra,Christopher M. Poskitt,Yannic Noller,Kenny Tsu Wei Choo,Cyrille Jegourel*

Main category: cs.SE

TL;DR: CodeHinter是一个结合传统调试工具和LLM技术的调试助手，旨在帮助新手程序员修复语义错误并促进他们在调试过程中的积极参与。


<details>
  <summary>Details</summary>
Motivation: 现有的AI调试工具往往导致学生对AI过度依赖，没有真正参与调试过程。本研究旨在设计一个既能有效修复错误又能促进学生主动参与的调试工具。

Method: 开发CodeHinter调试助手，结合传统调试工具和基于LLM的技术，通过第二版设计迭代，并在本科生群体中进行测试。

Result: 学生认为该工具在解决语义错误方面非常有效，使用起来比第一版显著更容易。错误定位是最有价值的功能。

Conclusion: 任何AI辅助调试工具都应该基于用户画像进行个性化，以优化与学生的互动效果。

Abstract: Debugging is a fundamental skill that novice programmers must develop.
Numerous tools have been created to assist novice programmers in this process.
Recently, large language models (LLMs) have been integrated with automated
program repair techniques to generate fixes for students' buggy code. However,
many of these tools foster an over-reliance on AI and do not actively engage
students in the debugging process. In this work, we aim to design an intuitive
debugging assistant, CodeHinter, that combines traditional debugging tools with
LLM-based techniques to help novice debuggers fix semantic errors while
promoting active engagement in the debugging process. We present findings from
our second design iteration, which we tested with a group of undergraduate
students. Our results indicate that the students found the tool highly
effective in resolving semantic errors and significantly easier to use than the
first version. Consistent with our previous study, error localization was the
most valuable feature. Finally, we conclude that any AI-assisted debugging tool
should be personalized based on user profiles to optimize their interactions
with students.

</details>


### [25] [An Improved Quantum Software Challenges Classification Approach using Transfer Learning and Explainable AI](https://arxiv.org/abs/2509.21068)
*Nek Dil Khan,Javed Ali Khan,Mobashir Husain,Muhammad Sohail Khan,Arif Ali Khan,Muhammad Azeem Akbar,Shahid Hussain*

Main category: cs.SE

TL;DR: 该研究通过分析Stack Overflow上的量子计算相关讨论，识别了量子软件工程的主要挑战类别，并使用Transformer模型实现了95%的分类准确率，优于传统深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 量子开发者面临优化量子计算和量子软件工程概念的挑战，他们使用Stack Overflow等平台讨论这些问题。通过分类量子相关帖子，可以帮助识别常见的量子软件工程挑战。

Method: 从问答平台提取2829个量子相关标签的问题，通过内容分析和扎根理论识别常见挑战类别，使用ChatGPT验证人工标注，并比较了BERT、DistilBERT、RoBERTa等Transformer模型与传统深度学习模型的分类性能。

Result: 识别出工具、理论、学习、概念、错误和API使用等六类主要挑战。Transformer模型平均准确率达到95%，优于传统深度学习模型（89%、86%、84%）。使用SHAP增强模型可解释性。

Conclusion: 基于Transformer的方法在分类量子软件工程挑战方面优于传统深度学习方法，准确率提高6%。这些发现有助于量子供应商和论坛更好地组织讨论，但需要与实际开发者和供应商进行实证评估。

Abstract: Quantum Software Engineering (QSE) is a research area practiced by tech
firms. Quantum developers face challenges in optimizing quantum computing and
QSE concepts. They use Stack Overflow (SO) to discuss challenges and label
posts with specialized quantum tags, which often refer to technical aspects
rather than developer posts. Categorizing questions based on quantum concepts
can help identify frequent QSE challenges. We conducted studies to classify
questions into various challenges. We extracted 2829 questions from Q&A
platforms using quantum-related tags. Posts were analyzed to identify frequent
challenges and develop a novel grounded theory. Challenges include Tooling,
Theoretical, Learning, Conceptual, Errors, and API Usage. Through content
analysis and grounded theory, discussions were annotated with common challenges
to develop a ground truth dataset. ChatGPT validated human annotations and
resolved disagreements. Fine-tuned transformer algorithms, including BERT,
DistilBERT, and RoBERTa, classified discussions into common challenges. We
achieved an average accuracy of 95% with BERT DistilBERT, compared to
fine-tuned Deep and Machine Learning (D&ML) classifiers, including Feedforward
Neural Networks (FNN), Convolutional Neural Networks (CNN), and Long Short-Term
Memory networks (LSTM), which achieved accuracies of 89%, 86%, and 84%,
respectively. The Transformer-based approach outperforms the D&ML-based
approach with a 6\% increase in accuracy by processing actual discussions,
i.e., without data augmentation. We applied SHAP (SHapley Additive
exPlanations) for model interpretability, revealing how linguistic features
drive predictions and enhancing transparency in classification. These findings
can help quantum vendors and forums better organize discussions for improved
access and readability. However,empirical evaluation studies with actual
developers and vendors are needed.

</details>


### [26] [Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach](https://arxiv.org/abs/2509.21170)
*Yongda Yu,Guohao Shi,Xianwei Wu,Haochuan He,XueMing Gu,Qianqian Zhao,Kui Liu,Qiushi Wang,Zhao Tian,Haifeng Shen,Guoping Rong*

Main category: cs.SE

TL;DR: MelcotCR是一种基于思维链的微调方法，通过长思维链技术为代码审查提供丰富的结构化信息，使LLM能够分析代码审查的多个维度。


<details>
  <summary>Details</summary>
Motivation: 现有基于代码审查数据微调LLM的方法受限于有限或模糊的信息，无法像人类审查者那样同时分析代码审查的多个维度来更好地识别问题。

Method: 结合最大熵建模原理和预定义推理路径，解决LLM处理长思维链提示时的上下文丢失和推理逻辑丢失问题，有效利用长思维链提示中的上下文知识并加强推理过程的逻辑严密性。

Result: 在MelcotCR数据集和CodeReviewer数据集上的实证评估显示，使用MelcotCR微调的14B Qwen2.5模型在检测和描述代码问题准确性方面超越了最先进方法，性能与671B DeepSeek-R1模型相当。

Conclusion: MelcotCR方法通过思维链微调显著提升了LLM在代码审查任务中的推理能力，使小参数模型能够达到甚至超越大参数模型的性能。

Abstract: Large Language Models (LLMs) have shown great potential in supporting
automated code review due to their impressive capabilities in context
understanding and reasoning. However, these capabilities are still limited
compared to human-level cognition because they are heavily influenced by the
training data. Recent research has demonstrated significantly improved
performance through fine-tuning LLMs with code review data. However, compared
to human reviewers who often simultaneously analyze multiple dimensions of code
review to better identify issues, the full potential of these methods is
hampered by the limited or vague information used to fine-tune the models. This
paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that
trains LLMs with an impressive reasoning ability to analyze multiple dimensions
of code review by harnessing long COT techniques to provide rich structured
information. To address context loss and reasoning logic loss issues that
frequently occur when LLMs process long COT prompts, we propose a solution that
combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning
pathways in MelcotCR to enable more effective utilization of in-context
knowledge within long COT prompts while strengthening the logical tightness of
the reasoning process. Empirical evaluations on our curated MelcotCR dataset
and the public CodeReviewer dataset reveal that a low-parameter base model,
such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art
methods in terms of the accuracy of detecting and describing code issues, with
its performance remarkably on par with that of the 671B DeepSeek-R1 model.

</details>


### [27] [Semantic Clustering of Civic Proposals: A Case Study on Brazil's National Participation Platform](https://arxiv.org/abs/2509.21292)
*Ronivaldo Ferreira,Guilherme da Silva,Carla Rocha,Gustavo Pinto*

Main category: cs.SE

TL;DR: 提出一种结合BERTopic、种子词和LLM自动验证的方法，用于大规模公民参与平台的内容分类，减少人工工作量，生成与官方分类一致的主题。


<details>
  <summary>Details</summary>
Motivation: 数字平台上的公民参与内容数量庞大，手动分类不可行，需要专家参与且需与官方分类保持一致，现有方法难以满足这些要求。

Method: 结合BERTopic主题建模、种子词引导和大型语言模型自动验证的方法。

Result: 生成的主题具有连贯性且与机构分类一致，仅需极少人工干预。

Conclusion: 该方法能将大量公民输入转化为可用于公共政策的可操作数据。

Abstract: Promoting participation on digital platforms such as Brasil Participativo has
emerged as a top priority for governments worldwide. However, due to the sheer
volume of contributions, much of this engagement goes underutilized, as
organizing it presents significant challenges: (1) manual classification is
unfeasible at scale; (2) expert involvement is required; and (3) alignment with
official taxonomies is necessary. In this paper, we introduce an approach that
combines BERTopic with seed words and automatic validation by large language
models. Initial results indicate that the generated topics are coherent and
institutionally aligned, with minimal human effort. This methodology enables
governments to transform large volumes of citizen input into actionable data
for public policy.

</details>
