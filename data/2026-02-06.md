<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 23]
- [cs.DB](#cs.DB) [Total: 9]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Reducing the Costs of Proof Synthesis on Rust Systems by Scaling Up a Seed Training Set](https://arxiv.org/abs/2602.04910)
*Nongyu Di,Tianyu Chen,Shan Lu,Shuai Lu,Yeyun Gong,Peng Cheng,Jacob R. Lorch,Yuan Yao,Xiaoxing Ma*

Main category: cs.SE

TL;DR: VeruSyn是一个用于Verus（Rust系统软件验证工具）的数据合成管道，通过自合成和教程合成生成了690万个带有形式规范和证明的Rust程序，用于训练代码证明生成模型。


<details>
  <summary>Details</summary>
Motivation: LLM生成的代码正确性存在问题，需要形式化证明来保证。但代码证明生成比代码生成需要更强的推理能力，且现有数据不足。Verus作为Rust系统软件的验证工具，缺乏大规模训练数据。

Method: VeruSyn采用三种合成方法：1) 自合成：基于现有程序生成新程序；2) 教程合成：从教程中提取代码和证明；3) 代理轨迹合成：生成长链推理数据。最终合成690万个Verus验证程序。

Result: 创建了最大的Verus验证程序数据集（690万个），基于此微调的Qwen2.5-Coder-32B-Instruct模型在成本-证明权衡上优于Claude Sonnet 4.5等商业模型，显著超越o4-mini和先前研究模型。

Conclusion: VeruSyn通过大规模数据合成解决了代码证明生成的数据稀缺问题，训练的模型在代码证明生成任务上表现出色，为LLM生成可验证代码提供了有效解决方案。

Abstract: Large Language Models (LLMs) are widely used for code generation. However, the correctness of code generated by LLMs remains a concern. A potential remedy to this concern is to have LLMs generate formal correctness proofs along with such code. However, compared with code generation, code-proof generation requires much higher reasoning capability and has much less existing data to learn from. In this paper, we present VeruSyn, a data synthesis pipeline for Verus, a state-of-the-art verification tool for system software written in Rust. Through self-synthesis and tutorial-based synthesis, VeruSyn achieves much larger scale and Verus-feature coverage than previous data-synthesis techniques designed for Verus; VeruSyn also supplements its dataset with long-chain-of-thought (CoT) data through agent trajectory synthesis. With VeruSyn, we synthesize the largest set of Verus verified programs: 6.9 million Rust programs, each with a formal specification and a proof that it meets that specification. This dataset lets us create a fine-tuned Qwen2.5-Coder-32B-Instruct model with appealing cost-proof tradeoff compared with state-of-the-art commercial models like Claude Sonnet 4.5. It also significantly outperforms models like o4-mini and previously proposed research models.

</details>


### [2] [ASA: Activation Steering for Tool-Calling Domain Adaptation](https://arxiv.org/abs/2602.04935)
*Youjin Wang,Run Zhou,Rong Fu,Shuaishuai Cao,Hongwei Zeng,Jiaxuan Lu,Sicheng Fan,Jiaqiao Zhao,Liangming Pan*

Main category: cs.SE

TL;DR: ASA是一种轻量级、无需训练的推理时适配机制，通过读取中间激活的路由信号，使用超轻量路由器产生自适应控制强度，实现精确领域对齐，在多领域工具生态系统中具有高效、可扩展的优势。


<details>
  <summary>Details</summary>
Motivation: 通用LLM智能体在现实部署中的核心挑战不是工具使用本身，而是在快速演变的工具集、API和协议下的高效领域适应。传统的LoRA或SFT方法在不同领域重复训练会导致训练和维护成本指数级增长，而提示或模式方法在分布偏移和复杂接口下表现脆弱。

Method: 提出激活导向适配器（ASA），这是一种轻量级、推理时、无需训练的机制。它从中间激活中读取路由信号，使用超轻量路由器产生自适应控制强度，实现精确的领域对齐。

Result: 在多个模型规模和领域上，ASA实现了与LoRA相当的适应效果，但开销显著降低，并具有强大的跨模型可转移性。

Conclusion: ASA非常适合具有频繁接口变化的稳健、可扩展和高效的多领域工具生态系统，为LLM智能体的实际部署提供了实用的解决方案。

Abstract: For real-world deployment of general-purpose LLM agents, the core challenge is often not tool use itself, but efficient domain adaptation under rapidly evolving toolsets, APIs, and protocols. Repeated LoRA or SFT across domains incurs exponentially growing training and maintenance costs, while prompt or schema methods are brittle under distribution shift and complex interfaces. We propose \textbf{Activation Steering Adapter (ASA}), a lightweight, inference-time, training-free mechanism that reads routing signals from intermediate activations and uses an ultra-light router to produce adaptive control strengths for precise domain alignment. Across multiple model scales and domains, ASA achieves LoRA-comparable adaptation with substantially lower overhead and strong cross-model transferability, making it ideally practical for robust, scalable, and efficient multi-domain tool ecosystems with frequent interface churn dynamics.

</details>


### [3] [Large Language Models in Software Documentation and Modeling: A Literature Review and Findings](https://arxiv.org/abs/2602.04938)
*Lukas Radosky,Ivan Polasek*

Main category: cs.SE

TL;DR: 对大型语言模型在软件工程文档和建模任务中应用的文献综述，分析相关研究、任务分类、提示技术、评估方法和数据集


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能特别是大型语言模型在软件工程领域受到广泛关注，它们能够理解自然语言和结构化语言，适合处理软件文档和模型。需要系统梳理LLM在软件工程文档和建模任务中的应用现状。

Method: 对四个主要相关领域的文献进行综述，按任务类型组织文章，分析使用的提示技术、评估指标、人工评估方法和主要数据集。

Result: 提供了LLM在软件工程文档和建模任务应用的系统性综述，包括任务分类框架、常用技术方法、评估体系等，为研究者提供该领域的全面概览。

Conclusion: 大型语言模型在软件工程文档和建模任务中具有重要应用价值，本文的综述为相关研究提供了系统性的参考框架和未来研究方向。

Abstract: Generative artificial intelligence attracts significant attention, especially with the introduction of large language models. Its capabilities are being exploited to solve various software engineering tasks. Thanks to their ability to understand natural language and generate natural language responses, large language models are great for processing various software documentation artifacts. At the same time, large language models excel at understanding structured languages, having the potential for working with software programs and models. We conduct a literature review on the usage of large language models for software engineering tasks related to documentation and modeling. We analyze articles from four major venues in the area, organize them per tasks they solve, and provide an overview of used prompt techniques, metrics, approaches to human-based evaluation, and major datasets.

</details>


### [4] [Applying a Requirements-Focused Agile Management Approach for Machine Learning-Enabled Systems](https://arxiv.org/abs/2602.05042)
*Lucas Romao,Luiz Xavier,Júlia Condé Araújo,Marina Condé Araújo,Ariane Rodrigues,Marcos Kalinowski*

Main category: cs.SE

TL;DR: RefineML是一个面向需求的持续敏捷方法，用于ML系统开发，集成了ML定制规范和敏捷管理，在工业-学术合作项目中验证有效。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统对传统需求工程和敏捷管理提出挑战，包括数据依赖性、实验性和模型行为不确定性。现有方法集成不足且未针对ML特性定制。

Method: 提出RefineML方法，集成ML定制规范、敏捷管理方法和系统映射研究的最佳实践。通过与巴西网络安全公司EXA的工业-学术合作项目进行应用评估，使用问卷调查和半结构化访谈收集数据，并进行主题分析。

Result: 问卷调查显示RefineML具有高感知有用性和使用意愿。访谈表明RefineML能改善沟通、促进早期可行性评估，并实现ML和软件工作的双轨治理，允许在演进软件项目的同时持续精炼模型。但仍存在将ML关注点操作化为敏捷需求以及估算ML工作量的困难。

Conclusion: RefineML为ML系统开发提供了有效的需求导向敏捷方法，改善了团队沟通和项目治理，但需要进一步解决ML需求操作化和工作量估算的挑战。

Abstract: Machine Learning (ML)-enabled systems challenge traditional Requirements Engineering (RE) and agile management due to data dependence, experimentation, and uncertain model behavior. Existing RE and agile practices remain poorly integrated and insufficiently tailored to these characteristics. This paper reports on the practical experience of applying RefineML, a requirements-focused approach for the continuous and agile refinement of ML-enabled systems, which integrates ML-tailored specification and agile management approaches with best practices derived from a systematic mapping study. The application context concerns an industry-academia collaboration project between PUC-Rio and EXA, a Brazilian cybersecurity company. For evaluation purposes, we applied questionnaires assessing RefineML's suitability and overall acceptance and semi-structured interviews. We applied thematic analysis to the collected qualitative data. Regarding suitability and acceptance, the results of the questionnaires indicated high perceived usefulness and intention to use. Based on the interviews, stakeholders perceived RefineML as improving communication and facilitating early feasibility assessments, as well as enabling dual-track governance of ML and software work, allowing continuous refinement of the model while evolving the overall software project. However, some limitations remain, particularly related to difficulties in operationalizing ML concerns into agile requirements and in estimating ML effort.

</details>


### [5] [Emergence-as-Code for Self-Governing Reliable Systems](https://arxiv.org/abs/2602.05458)
*Anatoly A. Krasnovsky*

Main category: cs.SE

TL;DR: 提出Emergence-as-Code (EmaC)框架，使微服务旅程可靠性可计算和可治理，通过意图声明和运行时推理来管理用户旅程的SLO


<details>
  <summary>Details</summary>
Motivation: 虽然SLO-as-code使单个服务可靠性可声明，但用户旅程的可靠性是微服务拓扑、路由、冗余、超时/回退、共享故障域和尾部放大的涌现属性。旅程目标通常维护在代码之外，随着系统演进而漂移，导致团队要么错过用户期望，要么过度配置并使用临时启发式方法控制发布

Method: 提出EmaC框架：1) EmaC规范声明旅程意图（目标、控制流操作符、允许的操作）并绑定到原子SLO和遥测数据；2) 运行时推理组件消费操作工件（如追踪和流量配置）合成具有来源和置信度的候选旅程模型；3) 从最后接受的模型中，EmaC编译器/控制器在显式相关性假设下推导有界的旅程SLO和预算；4) 生成控制平面工件（燃烧率警报、发布门控、操作防护）

Result: 提供匿名化工件仓库包含可运行的示例规范和生成输出，展示了如何使旅程可靠性可计算和可治理

Conclusion: EmaC为微服务架构中的旅程可靠性管理提供了系统化方法，通过意图加证据的方式使旅程可靠性可计算和可治理，解决了SLO-as-code在旅程层面的局限性

Abstract: SLO-as-code has made per-service} reliability declarative, but user experience is defined by journeys whose reliability is an emergent property of microservice topology, routing, redundancy, timeouts/fallbacks, shared failure domains, and tail amplification. As a result, journey objectives (e.g., "checkout p99 < 400 ms") are often maintained outside code and drift as the system evolves, forcing teams to either miss user expectations or over-provision and gate releases with ad-hoc heuristics. We propose Emergence-as-Code (EmaC), a vision for making journey reliability computable and governable via intent plus evidence. An EmaC spec declares journey intent (objective, control-flow operators, allowed actions) and binds it to atomic SLOs and telemetry. A runtime inference component consumes operational artifacts (e.g., tracing and traffic configuration) to synthesize a candidate journey model with provenance and confidence. From the last accepted model, the EmaC compiler/controller derives bounded journey SLOs and budgets under explicit correlation assumptions (optimistic independence vs. pessimistic shared fate), and emits control-plane artifacts (burn-rate alerts, rollout gates, action guards) that are reviewable in a Git workflow. An anonymized artifact repository provides a runnable example specification and generated outputs.

</details>


### [6] [Quality Model for Machine Learning Components](https://arxiv.org/abs/2602.05043)
*Grace A. Lewis,Rachel Brower-Sinning,Robert Edman,Ipek Ozkaya,Sebastián Echeverría,Alex Derr,Collin Beaudoin,Katherine R. Maffey*

Main category: cs.SE

TL;DR: 提出针对机器学习组件的质量模型，帮助开发者和系统利益相关者定义系统级需求并指导测试工作


<details>
  <summary>Details</summary>
Motivation: 当前机器学习测试主要关注模型性能等属性，而忽略了系统级需求（如吞吐量、资源消耗、鲁棒性），导致模型集成、部署和运维失败。虽然ISO 25059标准定义了AI系统质量模型，但它将系统属性和ML组件属性混在一起，对组件开发者不实用。

Method: 开发了一个专门针对机器学习组件的质量模型，作为需求获取和协商的指南，为ML组件开发者和系统利益相关者提供共同词汇。通过调查验证了该模型的相关性和价值，并将其集成到开源的ML组件测试和评估工具中。

Result: 参与者认可该质量模型的相关性和价值。该模型已成功集成到开源工具中，展示了其实际应用价值。

Conclusion: 提出的机器学习组件质量模型能够有效帮助开发者和利益相关者定义系统级需求，指导测试工作，解决当前ML测试过于局限的问题。

Abstract: Despite increased adoption and advances in machine learning (ML), there are studies showing that many ML prototypes do not reach the production stage and that testing is still largely limited to testing model properties, such as model performance, without considering requirements derived from the system it will be a part of, such as throughput, resource consumption, or robustness. This limited view of testing leads to failures in model integration, deployment, and operations. In traditional software development, quality models such as ISO 25010 provide a widely used structured framework to assess software quality, define quality requirements, and provide a common language for communication with stakeholders. A newer standard, ISO 25059, defines a more specific quality model for AI systems. However, a problem with this standard is that it combines system attributes with ML component attributes, which is not helpful for a model developer, as many system attributes cannot be assessed at the component level. In this paper, we present a quality model for ML components that serves as a guide for requirements elicitation and negotiation and provides a common vocabulary for ML component developers and system stakeholders to agree on and define system-derived requirements and focus their testing efforts accordingly. The quality model was validated through a survey in which the participants agreed with its relevance and value. The quality model has been successfully integrated into an open-source tool for ML component testing and evaluation demonstrating its practical application.

</details>


### [7] [Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems](https://arxiv.org/abs/2602.05486)
*Matteo Esposito,Lodovica Marchesi,Roberto Tonelli,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 论文提出将数字主权作为首要架构属性，而非单纯监管目标，并引入一个整合自主权身份、区块链信任、主权数据治理和受控生成式AI的参考架构。


<details>
  <summary>Details</summary>
Motivation: 数字主权已成为现代软件密集型系统的核心关切，主要驱动因素包括：非主权云基础设施的主导地位、生成式AI的快速采用、以及日益严格的监管要求。现有举措在治理、合规和安全性方面各自为政，缺乏在架构层面实现主权的具体指导。

Method: 提出将主权视为首要架构属性，引入主权参考架构，该架构整合了四个关键组件：自主权身份、基于区块链的信任与可审计性、主权数据治理、以及在明确架构控制下部署的生成式AI。

Result: 该架构明确捕捉了生成式AI的双重角色：既是治理风险的来源，又是合规性、问责制和持续保障的赋能者（当受到适当约束时）。通过将主权构建为架构质量属性，弥合了监管意图与具体系统设计之间的鸿沟。

Conclusion: 提出的参考架构为软件架构、生成式AI和数字主权交叉领域的未来研究和实践提供了原则性起点，为构建可审计、可演进和具备司法管辖意识的AI赋能系统奠定了连贯基础。

Abstract: Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.

</details>


### [8] [TestMigrationsInPy: A Dataset of Test Migrations from Unittest to Pytest](https://arxiv.org/abs/2602.05122)
*Altino Alves,Andre Hora*

Main category: cs.SE

TL;DR: TestMigrationsInPy是一个包含923个真实世界unittest到pytest迁移的数据集，用于支持Python测试框架迁移的自动化研究。


<details>
  <summary>Details</summary>
Motivation: pytest相比unittest有诸多优势，许多Python项目正在从unittest迁移到pytest，但迁移过程耗时且复杂，需要自动化工具支持。

Method: 收集了923个真实开发者的测试迁移案例，构建了包含迁移类型信息（如断言、fixture等）的数据集。

Result: 创建了TestMigrationsInPy数据集，包含详细的迁移信息，可作为未来研究的基础事实数据集。

Conclusion: TestMigrationsInPy为Python测试框架迁移研究提供了宝贵资源，支持从简单断言迁移到复杂fixture迁移的验证。

Abstract: Unittest and pytest are the most popular testing frameworks in Python. Overall, pytest provides some advantages, including simpler assertion, reuse of fixtures, and interoperability. Due to such benefits, multiple projects in the Python ecosystem have migrated from unittest to pytest. To facilitate the migration, pytest can also run unittest tests, thus, the migration can happen gradually over time. However, the migration can be time-consuming and take a long time to conclude. In this context, projects would benefit from automated solutions to support the migration process. In this paper, we propose TestMigrationsInPy, a dataset of test migrations from unittest to pytest. TestMigrationsInPy contains 923 real-world migrations performed by developers. Future research proposing novel solutions to migrate frameworks in Python can rely on TestMigrationsInPy as a ground truth. Moreover, as TestMigrationsInPy includes information about the migration type (e.g., changes in assertions or fixtures), our dataset enables novel solutions to be verified effectively, for instance, from simpler assertion migrations to more complex fixture migrations. TestMigrationsInPy is publicly available at: https://github.com/altinoalvesjunior/TestMigrationsInPy.

</details>


### [9] [Exceptional Behaviors: How Frequently Are They Tested?](https://arxiv.org/abs/2602.05123)
*Andre Hora,Gordon Fraser*

Main category: cs.SE

TL;DR: 对25个Python系统的实证研究发现，21.4%的执行方法会在运行时抛出异常，其中约20%的方法频繁抛出异常，挑战了异常是"罕见"行为的传统观念。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注传播到测试的异常，但忽略了未到达测试的其他异常。需要实证研究真实系统中异常行为的测试频率，以全面了解异常处理情况。

Method: 运行25个Python系统的测试套件，监控执行过程，收集运行时抛出的异常信息。覆盖5,372个执行方法、1,790万次调用和140万次抛出的异常。

Result: 21.4%的执行方法会在运行时抛出异常；在抛出异常的方法中，中位数显示每10次调用就有1次触发异常行为；约80%的方法很少抛出异常，但约20%的方法频繁抛出异常。

Conclusion: 异常抛出行为不一定是"异常"或罕见的。建议开发新工具支持异常行为测试和重构昂贵的try/except块，提醒研究者关注异常行为的实际频率。

Abstract: Exceptions allow developers to handle error cases expected to occur infrequently. Ideally, good test suites should test both normal and exceptional behaviors to catch more bugs and avoid regressions. While current research analyzes exceptions that propagate to tests, it does not explore other exceptions that do not reach the tests. In this paper, we provide an empirical study to explore how frequently exceptional behaviors are tested in real-world systems. We consider both exceptions that propagate to tests and the ones that do not reach the tests. For this purpose, we run an instrumented version of test suites, monitor their execution, and collect information about the exceptions raised at runtime. We analyze the test suites of 25 Python systems, covering 5,372 executed methods, 17.9M calls, and 1.4M raised exceptions. We find that 21.4% of the executed methods do raise exceptions at runtime. In methods that raise exceptions, on the median, 1 in 10 calls exercise exceptional behaviors. Close to 80% of the methods that raise exceptions do so infrequently, but about 20% raise exceptions more frequently. Finally, we provide implications for researchers and practitioners. We suggest developing novel tools to support exercising exceptional behaviors and refactoring expensive try/except blocks. We also call attention to the fact that exception-raising behaviors are not necessarily "abnormal" or rare.

</details>


### [10] [The Necessity of a Holistic Safety Evaluation Framework for AI-Based Automation Features](https://arxiv.org/abs/2602.05157)
*Alireza Abbaspour,Shabin Mahadevan,Kilian Zwirglmaier,Jeff Stafford*

Main category: cs.SE

TL;DR: QM组件传统上被排除在安全分析之外，但AI集成显示它们可能引发SOTIF相关风险，需要重新评估安全框架


<details>
  <summary>Details</summary>
Motivation: 传统驾驶自动化功能的安全分析将质量管理(QM)组件排除在严格的安全影响评估之外，但随着AI集成的发展，这些非安全相关组件可能引发SOTIF相关危险风险，需要重新评估安全考量

Method: 通过案例研究分析AI驱动的感知系统缺陷，展示即使在QM分类的组件中也可能出现缺陷，导致具有关键安全影响的意外功能行为；采用综合的FuSa、SOTIF和AI标准驱动方法

Result: 研究表明AI组件可能引入违反风险接受标准的危险，特别是在感知算法中；QM分类的组件中的缺陷可能导致关键安全影响

Conclusion: 需要采用全面的安全分析方法，修订现有安全框架以应对AI带来的挑战，确保跨所有组件分类的全面安全保证，涵盖多个安全标准

Abstract: The intersection of Safety of Intended Functionality (SOTIF) and Functional Safety (FuSa) analysis of driving automation features has traditionally excluded Quality Management (QM) components from rigorous safety impact evaluations. While QM components are not typically classified as safety-relevant, recent developments in artificial intelligence (AI) integration reveal that such components can contribute to SOTIF-related hazardous risks. Compliance with emerging AI safety standards, such as ISO/PAS 8800, necessitates re-evaluating safety considerations for these components. This paper examines the necessity of conducting holistic safety analysis and risk assessment on AI components, emphasizing their potential to introduce hazards with the capacity to violate risk acceptance criteria when deployed in safety-critical driving systems, particularly in perception algorithms. Using case studies, we demonstrate how deficiencies in AI-driven perception systems can emerge even in QM-classified components, leading to unintended functional behaviors with critical safety implications. By bridging theoretical analysis with practical examples, this paper argues for the adoption of comprehensive FuSa, SOTIF, and AI standards-driven methodologies to identify and mitigate risks in AI components. The findings demonstrate the importance of revising existing safety frameworks to address the evolving challenges posed by AI, ensuring comprehensive safety assurance across all component classifications spanning multiple safety standards.

</details>


### [11] [EGSS: Entropy-guided Stepwise Scaling for Reliable Software Engineering](https://arxiv.org/abs/2602.05242)
*Chenhui Mao,Yuanting Lei,Zhixiang Wei,Ming Liang,Zhixiang Wang,Jingxuan Xu,Dajun Chen,Wei Jiang,Yong Li*

Main category: cs.SE

TL;DR: EGSS框架通过熵引导自适应搜索和测试套件增强，在保持Agentic测试时扩展性能优势的同时，显著降低计算开销，实现效率与效果的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: Agentic测试时扩展在代码生成和bug修复等复杂软件工程任务中表现出色，但实际应用受限：1) 大规模集成部署成本过高；2) 缺乏可靠的最优候选方案选择机制，限制了性能提升潜力。

Method: 提出熵引导逐步扩展框架，通过熵引导的自适应搜索和鲁棒的测试套件增强，动态平衡效率与效果。

Result: 在SWE-Bench-Verified上，EGSS将性能提升5-10%，Kimi-K2-Intruct解决率从63.2%提升至72.2%，GLM-4.6从65.8%提升至74.6%。与GLM-4.6结合达到开源大模型SOTA，同时减少28%推理时token使用。

Conclusion: EGSS有效解决了Agentic测试时扩展的计算效率问题，在保持性能优势的同时显著降低开销，为实际部署提供了可行方案。

Abstract: Agentic Test-Time Scaling (TTS) has delivered state-of-the-art (SOTA) performance on complex software engineering tasks such as code generation and bug fixing. However, its practical adoption remains limited due to significant computational overhead, primarily driven by two key challenges: (1) the high cost associated with deploying excessively large ensembles, and (2) the lack of a reliable mechanism for selecting the optimal candidate solution, ultimately constraining the performance gains that can be realized. To address these challenges, we propose Entropy-Guided Stepwise Scaling (EGSS), a novel TTS framework that dynamically balances efficiency and effectiveness through entropy-guided adaptive search and robust test-suite augmentation. Extensive experiments on SWE-Bench-Verified demonstrate that EGSS consistently boosts performance by 5-10% across all evaluated models. Specifically, it increases the resolved ratio of Kimi-K2-Intruct from 63.2% to 72.2%, and GLM-4.6 from 65.8% to 74.6%. Furthermore, when paired with GLM-4.6, EGSS achieves a new state-of-the-art among open-source large language models. In addition to these accuracy improvements, EGSS reduces inference-time token usage by over 28% compared to existing TTS methods, achieving simultaneous gains in both effectiveness and computational efficiency.

</details>


### [12] [PatchGuru: Patch Oracle Inference from Natural Language Artifacts with Large Language Models](https://arxiv.org/abs/2602.05270)
*Thanh Le-Cong,Bach Le,Toby Murray,Michael Pradel,Cristian Cadar*

Main category: cs.SE

TL;DR: PatchGuru：首个从真实PR中自动推断可执行补丁规范的技术，利用LLM从自然语言描述中提取开发者意图，生成补丁预言（运行时断言），用于验证补丁行为并发现bug。


<details>
  <summary>Details</summary>
Motivation: 软件系统演化中，补丁可能无意改变程序行为。由于回归测试不完整且补丁意图描述是非正式的自然语言，验证补丁是否符合预期语义很困难。

Method: 给定PR，PatchGuru使用LLM从自然语言工件中提取开发者意图，合成补丁预言（比较程序中集成前后版本的运行时断言）。通过迭代精炼推断的预言，比较前后版本行为，识别违规，通过自审过滤不一致性，生成bug报告。

Result: 在4个广泛使用的开源Python项目的400个近期PR上评估，PatchGuru报告39个警告，精度0.62，产生24个确认的真阳性（包括12个先前未知的bug，其中11个被开发者修复）。相比最先进技术Testora，多检测17个bug（24 vs 7），精度从0.32提升到0.62。每个PR平均成本8.9分钟和0.07美元。

Conclusion: PatchGuru通过提供可执行文档和补丁意图的自动验证，补充了代码审查和回归测试。

Abstract: As software systems evolve, patches may unintentionally alter program behavior. Validating patches against their intended semantics is difficult due to incomplete regression tests and informal, non-executable natural language (NL) descriptions of patch intent. We present PatchGuru, the first automated technique that infers executable patch specifications from real-world pull requests (PRs). Given a PR, PatchGuru uses large language models (LLMs) to extract developer intent from NL artifacts and synthesizes patch oracles: under-approximate yet practical specifications expressed as runtime assertions in comparison programs that integrate pre- and post-patch versions. Patch oracles focus on patch-relevant behaviors, enable automated validation, and support cross-version properties. PatchGuru iteratively refines inferred oracles by comparing pre- and post-patch behaviors, identifies violations, filters inconsistencies via self-review, and generates bug reports. We evaluate PatchGuru on 400 recent PRs from four widely used open-source Python projects. PatchGuru reports 39 warnings with a precision of 0.62, yielding 24 confirmed true positives, including 12 previously unknown bugs, 11 of which were subsequently fixed by developers. Compared to the state-of-the-art technique Testora, PatchGuru detects 17 more bugs (24 vs. 7) while improving precision from 0.32 to 0.62. PatchGuru incurs an average cost of 8.9 minutes and USD 0.07 per PR. These results suggest that PatchGuru complements code review and regression testing by providing executable documentation and automated validation of patch intent.

</details>


### [13] [Does Programming Language Matter? An Empirical Study of Fuzzing Bug Detection](https://arxiv.org/abs/2602.05312)
*Tatsuya Shirai,Olivier Nourry,Yutaro Kashiwa,Kenji Fujiwara,Hajimu Iida*

Main category: cs.SE

TL;DR: 该研究对61,444个模糊测试漏洞和999,248次构建进行大规模跨语言分析，发现模糊测试效果因编程语言而异：C++和Rust检测频率高，Rust和Python漏洞比例低但严重性高，Go中不可复现漏洞多而Rust中少，Python补丁覆盖率高但检测时间长。


<details>
  <summary>Details</summary>
Motivation: 尽管模糊测试已广泛应用于持续集成流程，但先前研究未探讨不同编程语言下持续模糊测试的效果差异。本研究旨在分析模糊测试漏洞特征和检测效率在不同语言间的差异。

Method: 对559个OSS-Fuzz项目进行大规模跨语言分析，涵盖61,444个模糊测试漏洞和999,248次构建，按主要编程语言（C++、Rust、Python、Go等）分类研究。

Result: 1) C++和Rust的模糊测试漏洞检测频率更高；2) Rust和Python的漏洞比例较低但倾向于暴露更严重的安全漏洞；3) 崩溃类型因语言而异，Go中不可复现漏洞更常见而Rust中罕见；4) Python的补丁覆盖率更高但检测时间更长。

Conclusion: 模糊测试行为和效果受语言设计影响显著，研究结果为语言感知的模糊测试策略和工具开发提供了重要见解。

Abstract: Fuzzing has become a popular technique for automatically detecting vulnerabilities and bugs by generating unexpected inputs. In recent years, the fuzzing process has been integrated into continuous integration workflows (i.e., continuous fuzzing), enabling short and frequent testing cycles. Despite its widespread adoption, prior research has not examined whether the effectiveness of continuous fuzzing varies across programming languages. This study conducts a large-scale cross-language analysis to examine how fuzzing bug characteristics and detection efficiency differ among languages. We analyze 61,444 fuzzing bugs and 999,248 builds from 559 OSS-Fuzz projects categorized by primary language. Our findings reveal that (i) C++ and Rust exhibit higher fuzzing bug detection frequencies, (ii) Rust and Python show low vulnerability ratios but tend to expose more critical vulnerabilities, (iii) crash types vary across languages and unreproducible bugs are more frequent in Go but rare in Rust, and (iv) Python attains higher patch coverage but suffers from longer time-to-detection. These results demonstrate that fuzzing behavior and effectiveness are strongly shaped by language design, providing insights for language-aware fuzzing strategies and tool development.

</details>


### [14] [Can We Classify Flaky Tests Using Only Test Code? An LLM-Based Empirical Study](https://arxiv.org/abs/2602.05465)
*Alexander Berndt,Vekil Bekmyradov,Rainer Gemulla,Marcus Kessel,Thomas Bach,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLMs在仅基于测试代码的情况下难以有效分类flaky tests，效果仅略优于随机猜测，因为测试代码本身可能不包含足够的flakiness信息。


<details>
  <summary>Details</summary>
Motivation: Flaky tests会干扰代码变更的自动化质量保证并阻碍高效软件测试。先前基于测试代码标识符训练机器学习模型的方法缺乏泛化能力，而预训练LLMs在跨任务泛化方面表现出潜力，有望解决先前方法的泛化问题。

Method: 评估了三个LLMs（两个通用模型，一个代码专用模型），使用三种提示技术，在两个flaky test分类基准数据集上进行测试。同时手动调查了50个样本，以确定仅基于测试代码对人类是否可行。

Result: LLMs在仅基于测试代码的情况下难以有效分类flaky tests，最佳提示-模型组合的结果仅略优于随机猜测。手动分析发现测试代码不一定包含足够的flakiness分类信息。

Conclusion: 仅基于测试代码进行flakiness分类存在局限性，未来工作应评估LLMs在附加上下文（如检索增强生成或代理AI）下的flakiness分类能力。

Abstract: Flaky tests yield inconsistent results when they are repeatedly executed on the same code revision. They interfere with automated quality assurance of code changes and hinder efficient software testing. Previous work evaluated approaches to train machine learning models to classify flaky tests based on identifiers in the test code. However, the resulting classifiers have been shown to lack generalizability, hindering their applicability in practical environments. Recently, pre-trained Large Language Models (LLMs) have shown the capability to generalize across various tasks. Thus, they represent a promising approach to address the generalizability problem of previous approaches. In this study, we evaluated three LLMs (two general-purpose models, one code-specific model) using three prompting techniques on two benchmark datasets from prior studies on flaky test classification. Furthermore, we manually investigated 50 samples from the given datasets to determine whether classifying flaky tests based only on test code is feasible for humans. Our findings indicate that LLMs struggle to classify flaky tests given only the test code. The results of our best prompt-model combination were only marginally better than random guessing. In our manual analysis, we found that the test code does not necessarily contain sufficient information for a flakiness classification. Our findings motivate future work to evaluate LLMs for flakiness classification with additional context, for example, using retrieval-augmented generation or agentic AI.

</details>


### [15] [Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations](https://arxiv.org/abs/2602.05523)
*Shahin Honarvar,Amber Gorzynski,James Lee-Jones,Harry Coppock,Marek Rei,Joseph Ryan,Alastair F. Donaldson*

Main category: cs.SE

TL;DR: 论文提出CTF挑战家族概念，通过语义保留的程序变换生成等价挑战，用于评估LLM代理在网络安全任务中的鲁棒性和泛化能力，并开发了Evolve-CTF工具进行实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有CTF基准测试只能进行点状评估，无法深入分析智能体在不同代码变体上的鲁棒性和泛化能力，需要更系统的方法来评估LLM代理在网络安全任务中的表现。

Method: 提出CTF挑战家族概念，使用语义保留的程序变换生成等价挑战；开发Evolve-CTF工具，对Python挑战应用多种变换生成家族；在Cybench和Intercode挑战上评估13种LLM代理配置。

Result: 模型对重命名和代码插入变换表现出较强鲁棒性，但组合变换和深度混淆会影响性能；显式推理对成功率影响有限；提供了大规模数据集和评估工具。

Conclusion: CTF挑战家族为LLM评估提供了有价值的工具和方法，揭示了当前最先进模型在网络安全领域的性能特点和局限性，为未来研究提供了基准和方向。

Abstract: Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.

</details>


### [16] [ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval](https://arxiv.org/abs/2602.05550)
*Yulong He,Artem Ermakov,Sergey Kovalchuk,Artem Aliev,Dmitry Shalymov*

Main category: cs.SE

TL;DR: 本文构建了首个大规模ArkTS代码数据集，用于代码检索和评估任务，并评估了现有开源代码嵌入模型，通过微调得到了高性能的ArkTS代码理解模型。


<details>
  <summary>Details</summary>
Motivation: OpenHarmony生态系统中的核心编程语言ArkTS缺乏公开数据集和评估基准，阻碍了ArkTS代码智能研究的发展。

Method: 从GitHub和Gitee爬取ArkTS仓库，使用tree-sitter-arkts提取注释-函数对，进行跨平台去重和统计分析；设计单搜索任务，评估现有开源代码嵌入模型，并使用ArkTS和TypeScript训练数据集进行微调。

Result: 建立了首个ArkTS代码检索系统基准，发布了数据集和微调模型，为ArkTS代码智能研究提供了基础资源。

Conclusion: 这项工作填补了ArkTS代码智能研究的空白，为后续研究提供了数据集、基准和预训练模型，将促进OpenHarmony生态系统的发展。

Abstract: ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.

</details>


### [17] [SEAL: Symbolic Execution with Separation Logic (Competition Contribution)](https://arxiv.org/abs/2602.05703)
*Tomáš Brablec,Tomáš Dacík,Tomáš Vojnar*

Main category: cs.SE

TL;DR: SEAL是一个基于分离逻辑的静态分析工具，用于验证操作无界链表数据结构的程序，采用模块化架构和通用分离逻辑求解器Astral，在链表验证方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的分离逻辑方法通常缺乏模块化和可扩展性，难以与其他理论推理结合。SEAL旨在提供一个更易扩展、模块化的验证框架，专门针对无界链表数据结构的程序验证。

Method: 基于分离逻辑表示抽象内存状态，使用通用分离逻辑求解器Astral进行可满足性和蕴含检查，Astral本身基于SMT转换。采用模块化架构设计，便于扩展和与其他理论推理结合。

Result: 在LinkedLists基础类别中取得竞争性结果，是仅有的四个能够验证无界链表程序的分析器之一。虽然仍是原型，但展示了其有效性。

Conclusion: SEAL的模块化架构和可扩展性设计，结合进一步开发，有望在未来竞赛中取得显著改进。该工具为分离逻辑验证提供了更灵活、可扩展的框架。

Abstract: SEAL is a static analyser for the verification of programs that manipulate unbounded linked data structures. It is based on separation logic to represent abstract memory states and, unlike other separation-logic-based approaches, it employs a general-purpose separation logic solver Astral for satisfiability and entailment checking, which itself is based on translation to SMT. This design results in a modular architecture intended to be easier to extend and to combine with reasoning in other theories. Although still a prototype, SEAL achieved competitive results in the LinkedLists base category and was one of only four analysers capable of verifying programs with unbounded lists. We believe that the tool's extensibility, combined with further development, can lead to significant improvements in future competitions.

</details>


### [18] [Towards Green AI: Decoding the Energy of LLM Inference in Software Development](https://arxiv.org/abs/2602.05712)
*Lola Solovyeva,Fernando Castor*

Main category: cs.SE

TL;DR: 该研究分析了LLM推理的能耗模式，发现预填充阶段成本会影响解码阶段的能耗，且某些模型存在"废话"行为导致能耗浪费。通过抑制废话行为，可在不影响准确性的情况下实现44%-89%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 随着AI辅助工具在软件开发中的广泛应用，基于大语言模型的工具带来了巨大的计算和能源成本。理解并减少LLM推理的能源足迹对于可持续软件开发至关重要。

Method: 研究分析了6个6B-7B参数和4个3B-4B参数的transformer模型，在代码生成（HumanEval）和代码理解（LongBench）基准测试上进行评估。区分了预填充阶段（处理输入）和解码阶段（生成输出）的能耗。

Result: 研究发现：1）不同模型在能耗模式上存在差异；2）预填充成本的增加会放大解码阶段的每token能耗（放大1.3%-51.8%）；3）10个模型中有3个表现出"废话"行为，添加不必要内容增加能耗；4）通过抑制废话行为，可在不影响准确性的情况下节省44%-89%的能耗。

Conclusion: 预填充成本会影响解码阶段的能耗，而解码阶段主导整体能耗。抑制废话行为可显著节省能源（高达89%）。减少推理能耗需要同时抑制废话行为并限制预填充对解码的影响。

Abstract: Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.

</details>


### [19] [A Dual-Loop Agent Framework for Automated Vulnerability Reproduction](https://arxiv.org/abs/2602.05721)
*Bin Liu,Yanjie Zhao,Zhenpeng Chen,Guoai Xu,Haoyu Wang*

Main category: cs.SE

TL;DR: Cve2PoC是一个基于LLM的双循环代理框架，通过战略规划-战术执行-自适应优化的范式，自动从CVE描述生成可执行的PoC漏洞利用代码，显著提高了漏洞复现成功率。


<details>
  <summary>Details</summary>
Motivation: 从CVE描述自动复现漏洞需要生成可执行的PoC利用代码并在目标环境中验证，这一过程在软件安全研究和实践中至关重要，但手动操作耗时且需要专业知识。现有LLM代理方法在探索攻击方向和修复实现细节时常常混淆，导致复现失败时陷入无效的调试循环。

Method: 提出了Cve2PoC框架，采用计划-执行-评估范式，包含三个核心组件：战略规划器分析漏洞语义和目标代码生成结构化攻击计划；战术执行器生成PoC代码并通过渐进验证进行测试；自适应优化器评估执行结果并将失败路由到不同循环：战术循环用于代码级优化，战略循环用于攻击策略重新规划。这种双循环设计使框架能够根据失败类型匹配合适的修复方案，避免无效调试。

Result: 在两个包含617个真实漏洞的基准测试上，Cve2PoC在SecBench.js和PatchEval上分别达到了82.9%和54.3%的复现成功率，比最佳基线分别提高了11.3%和20.4%。人工评估确认生成的PoC在可读性和可重用性方面达到了与人工编写利用代码相当的质量。

Conclusion: Cve2PoC通过战略-战术分离的双循环设计，有效解决了现有LLM代理在漏洞复现中混淆攻击探索和实现细节的问题，显著提高了自动化漏洞复现的成功率和代码质量，为软件安全研究和实践提供了有效的自动化工具。

Abstract: Automated vulnerability reproduction from CVE descriptions requires generating executable Proof-of-Concept (PoC) exploits and validating them in target environments. This process is critical in software security research and practice, yet remains time-consuming and demands specialized expertise when performed manually. While LLM agents show promise for automating this task, existing approaches often conflate exploring attack directions with fixing implementation details, which leads to unproductive debugging loops when reproduction fails. To address this, we propose Cve2PoC, an LLM-based dual-loop agent framework following a plan-execute-evaluate paradigm. The Strategic Planner analyzes vulnerability semantics and target code to produce structured attack plans. The Tactical Executor generates PoC code and validates it through progressive verification. The Adaptive Refiner evaluates execution results and routes failures to different loops: the \textit{Tactical Loop} for code-level refinement, while the \textit{Strategic Loop} for attack strategy replanning. This dual-loop design enables the framework to escape ineffective debugging by matching remediation to failure type. Evaluation on two benchmarks covering 617 real-world vulnerabilities demonstrates that Cve2PoC achieves 82.9\% and 54.3\% reproduction success rates on SecBench.js and PatchEval, respectively, outperforming the best baseline by 11.3\% and 20.4\%. Human evaluation confirms that generated PoCs achieve comparable code quality to human-written exploits in readability and reusability.

</details>


### [20] [A Bayesian Optimization-Based AutoML Framework for Non-Intrusive Load Monitoring](https://arxiv.org/abs/2602.05739)
*Nazanin Siavash,Armin Moin*

Main category: cs.SE

TL;DR: 提出AutoML4NILM框架，将自动机器学习应用于非侵入式负荷监测领域，通过贝叶斯优化实现自动模型选择和超参数调优，降低领域专家应用机器学习的门槛。


<details>
  <summary>Details</summary>
Motivation: 非侵入式负荷监测（NILM）通过分析家庭总用电量来估计单个电器功耗，相比为每个电器安装专用智能电表更具成本效益。然而，应用机器学习技术需要数据科学专业知识，限制了领域专家的使用。

Method: 提出将自动机器学习（AutoML）引入NILM领域的新框架，利用贝叶斯优化进行自动模型选择和超参数调优。开发了AutoML4NILM开源工具包，目前支持11种算法及其不同超参数，且设计灵活可扩展。

Result: 开发了AutoML4NILM工具包，为NILM领域提供灵活可扩展的AutoML解决方案，使领域从业者无需高级数据科学专业知识也能有效应用机器学习技术。

Conclusion: 该框架通过自动化机器学习流程降低了NILM领域应用机器学习的门槛，促进了研究和工业应用，开源工具包的设计灵活性支持算法和超参数的进一步扩展。

Abstract: Non-Intrusive Load Monitoring (NILM), commonly known as energy disaggregation, aims to estimate the power consumption of individual appliances by analyzing a home's total electricity usage. This method provides a cost-effective alternative to installing dedicated smart meters for each appliance. In this paper, we introduce a novel framework that incorporates Automated Machine Learning (AutoML) into the NILM domain, utilizing Bayesian Optimization for automated model selection and hyperparameter tuning. This framework empowers domain practitioners to effectively apply machine learning techniques without requiring advanced expertise in data science or machine learning. To support further research and industry adoption, we present AutoML4NILM, a flexible and extensible open-source toolkit designed to streamline the deployment of AutoML solutions for energy disaggregation. Currently, this framework supports 11 algorithms, each with different hyperparameters; however, its flexible design allows for the extension of both the algorithms and their hyperparameters.

</details>


### [21] [Toward Quantum-Safe Software Engineering: A Vision for Post-Quantum Cryptography Migration](https://arxiv.org/abs/2602.05759)
*Lei Zhang*

Main category: cs.SE

TL;DR: 论文提出量子安全软件工程(QSSE)新研究方向，介绍自动化量子安全适应(AQuA)框架，包含PQC感知检测、语义重构和混合验证三大支柱，以应对后量子密码迁移的软件工程挑战。


<details>
  <summary>Details</summary>
Motivation: 量子计算对网络安全的威胁加速了后量子密码(PQC)标准化，但将遗留软件迁移到量子安全算法并非简单的库替换，而是一个新的软件工程挑战。现有漏洞检测、重构和测试工具无法处理PQC的概率行为、侧信道敏感性和复杂性能权衡。

Method: 提出自动化量子安全适应(AQuA)框架，包含三大支柱：1) PQC感知检测工具，2) 语义重构技术，3) 混合验证方法。这些工具专门设计用于处理PQC特有的挑战。

Result: 论文提出了量子安全软件工程(QSSE)作为一个独立的研究方向，并概述了AQuA框架的愿景，为后量子密码迁移提供系统化的软件工程解决方案。

Conclusion: 需要专门针对后量子密码特性的新工具和方法，AQuA框架为此提供了系统化方案，量子安全软件工程应成为一个独立的研究领域，以应对量子时代的软件安全挑战。

Abstract: The quantum threat to cybersecurity has accelerated the standardization of Post-Quantum Cryptography (PQC). Migrating legacy software to these quantum-safe algorithms is not a simple library swap, but a new software engineering challenge: existing vulnerability detection, refactoring, and testing tools are not designed for PQC's probabilistic behavior, side-channel sensitivity, and complex performance trade-offs. To address these challenges, this paper outlines a vision for a new class of tools and introduces the Automated Quantum-safe Adaptation (AQuA) framework, with a three-pillar agenda for PQC-aware detection, semantic refactoring, and hybrid verification, thereby motivating Quantum-Safe Software Engineering (QSSE) as a distinct research direction.

</details>


### [22] [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780)
*Ulrich Finkler,Irene Manotas,Wei Zhang,Geert Janssen,Octavian Popescu,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文提出基于代码语义范围的自动化LLM定制方法，通过RAG和微调策略提升模型在私有代码库上的代码补全性能，定制后中等规模模型表现优于未定制的大规模模型。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在公开基准测试中表现良好，但在处理未见过训练数据的私有代码库时，生成的代码难以与私有代码库对齐。定制化代码LLM可以提升模型在特定私有代码库上的性能。

Method: 提出基于代码语义范围的自动化LLM定制方法，通过语义范围机制处理代码库数据并构建训练数据对。评估两种定制策略：检索增强生成(RAG)和监督微调(FT)，在两个私有企业代码库上进行实验。

Result: 定制后的中等规模模型在代码补全任务上表现显著优于未定制的大规模模型。语义范围机制帮助模型学习特定代码库的底层模式，为开发者提供更精确的代码补全，提升生产力。

Conclusion: 基于语义范围的LLM定制方法能有效提升模型在私有代码库上的代码补全性能，定制化模型优于未定制的大规模模型。文中还包含对两个公开基准测试的分析，并提出了未来研究方向。

Abstract: Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.

</details>


### [23] [When Elo Lies: Hidden Biases in Codeforces-Based Evaluation of Large Language Models](https://arxiv.org/abs/2602.05891)
*Shenyu Zheng,Ximing Dong,Xiaoshuang Liu,Gustavo Oliva,Chong Chun Yong,Dayi Lin,Boyuan Chen,Shaowei Wang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: Codeforces Elo评分作为LLM编程能力评估指标存在严重偏差，研究发现提交顺序、比赛选择、模型随机性等因素可导致评分波动高达1122点，直接比较不可靠


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂推理方面取得突破，Codeforces Elo评分成为评估竞争性编程能力的主流指标，但现有报告缺乏关键实验细节，导致同一模型版本的评分波动近500点，需要系统研究评估偏差的隐藏因素

Method: 使用37个近期Codeforces比赛和13,691个生成测试用例的受控基准，系统实证研究三个关键偏差因素：提交时间顺序、比赛难度选择、LLM运行到运行的随机变异性

Result: Elo评分对这些参数高度敏感：不同提交顺序可导致评分偏移394点；比赛选择可造成同一模型评分差异高达1122点；运行到运行性能表现出显著不稳定性，相同比赛评估中平均评分最大差异达349点

Conclusion: 直接比较Elo评分不可靠且可能误导，必须在严格标准化和透明报告实验设置的条件下使用该评估方法

Abstract: As Large Language Models (LLMs) achieve breakthroughs in complex reasoning, Codeforces-based Elo ratings have emerged as a prominent metric for evaluating competitive programming capabilities. However, these ratings are often reported without critical experimental details, leading to significant discrepancies illustrated by recent reports where the score of the same model version fluctuated by nearly 500 points. This paper presents a systematic empirical study on the hidden factors biasing Elo evaluations: (1) the temporal ordering of submissions, (2) contest difficulty selection, and (3) run to run stochastic variability of LLMs. Utilizing a controlled benchmark of 37 recent Codeforces contests and 13,691 generated test cases, we demonstrate that Elo scores are highly sensitive to these parameters. Our findings reveal that varying submission orders can shift scores by 394 points, while contest selection can cause differences of up to 1,122 points for the same model. Run to run performance exhibits substantial instability, with a maximum difference of 349 points in mean scores observed when evaluating identical contests. We conclude that direct Elo comparisons are unreliable and potentially misleading without strict standardization and transparent reporting of experimental settings.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2602.04926)
*Ning Wang,Kuanyan Zhu,Daniel Yuehwoon Yee,Yitang Gao,Shiying Huang,Zirun Xu,Sainyam Galhotra*

Main category: cs.DB

TL;DR: AutoPrunedRetriever：基于图结构的RAG系统，通过持久化推理子图并增量扩展，减少重复检索和推理，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统对每个查询都重新检索长段落并从头推理，导致token使用、延迟和成本增加。需要一种能持久化推理结构并增量更新的系统来提高效率。

Method: 提出图式RAG系统，将实体和关系存储在紧凑的ID索引码本中，将问题、事实和答案表示为边序列。采用两层合并策略（快速ANN/KNN别名检测+选择性k-means）保持图紧凑，并修剪低价值结构。

Result: 在GraphRAG-Benchmark（医学和小说）上，两个变体都实现了最先进的复杂推理准确率，比HippoRAG2提高了约9-11个百分点。在STEM和TV基准测试中也排名第一，同时使用的token比基于图的基线少两个数量级。

Conclusion: AutoPrunedRetriever通过持久化和增量扩展推理子图，显著提高了RAG系统的效率和性能，为长期运行会话、演化语料库和多智能体管道提供了实用的基础。

Abstract: Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.

</details>


### [25] [DistillER: Knowledge Distillation in Entity Resolution with Large Language Models](https://arxiv.org/abs/2602.05452)
*Alexandros Zeakis,George Papadakis,Dimitrios Skoutas,Manolis Koubarakis*

Main category: cs.DB

TL;DR: DistillER是一个知识蒸馏框架，通过将大型LLM教师模型的知识转移到小型学生模型，在不依赖黄金标签的情况下实现高效且有效的实体解析。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的实体解析方法要么依赖计算成本高昂的大型模型，要么需要带标注的训练数据，在时间效率和有效性之间存在关键差距。需要一种更实用的方法来实现LLM驱动的实体解析。

Method: 提出DistillER框架，从三个维度系统解决知识蒸馏问题：1) 数据选择策略识别信息丰富的子集；2) 知识提取，比较单教师和多教师设置；3) 蒸馏算法，评估监督微调和强化学习方法。

Result: 实验表明，在LLM教师生成的噪声标签上对学生模型进行监督微调，始终优于其他知识蒸馏策略，同时还能生成高质量的解释。DistillER在效果和效率上都显著优于现有的监督和非监督ER方法。

Conclusion: 知识蒸馏是使LLM驱动的实体解析更实用的有效方法，DistillER框架在不依赖黄金标签的情况下，成功地将大型有效模型的知识转移到小型高效模型中。

Abstract: Recent advances in Entity Resolution (ER) have leveraged Large Language Models (LLMs), achieving strong performance but at the cost of substantial computational resources or high financial overhead. Existing LLM-based ER approaches operate either in unsupervised settings and rely on very large and costly models, or in supervised settings and require ground-truth annotations, leaving a critical gap between time efficiency and effectiveness. To make LLM-powered ER more practical, we investigate Knowledge Distillation (KD) as a means to transfer knowledge from large, effective models (Teachers) to smaller, more efficient models (Students) without requiring gold labels. We introduce DistillER, the first framework that systematically bridges this gap across three dimensions: (i) Data Selection, where we study strategies for identifying informative subsets of data; (ii) Knowledge Elicitation, where we compare single- and multi-teacher settings across LLMs and smaller language models (SLMs); and (iii) Distillation Algorithms, where we evaluate supervised fine-tuning and reinforcement learning approaches. Our experiments reveal that supervised fine-tuning of Students on noisy labels generated by LLM Teachers consistently outperforms alternative KD strategies, while also enabling high-quality explanation generation. Finally, we benchmark DistillER against established supervised and unsupervised ER methods based on LLMs and SLMs, demonstrating significant improvements in both effectiveness and efficiency.

</details>


### [26] [Repairing Property Graphs under PG-Constraints](https://arxiv.org/abs/2602.05503)
*Christopher Spinrath,Angela Bonifati,Rachid Echahed*

Main category: cs.DB

TL;DR: 该论文研究在PG-Constraints约束下修复属性图，提出包含ILP、朴素和LP引导贪心算法的修复流程，实验显示标签删除比节点/边删除减少59%删除量，LP引导贪心算法比ILP快97%且质量相当。


<details>
  <summary>Details</summary>
Motivation: 随着GQL和SQL/PGQ等图数据库标准查询语言以及PG-Constraints约束语言的出现，需要在PG-Constraints约束下修复属性图的问题变得重要。现有研究缺乏对这类约束下图修复的系统性方法。

Method: 识别PG-Constraints的重要子集（包含否定约束和递归），提出完整的图修复流程，涉及图拓扑结构变更和节点、边、标签删除。开发三种修复算法策略：整数线性规划(ILP)、朴素算法、LP引导贪心算法。

Result: 实验表明：1) 使用标签删除相比节点/边删除可减少59%的删除操作；2) LP引导贪心算法相比ILP策略运行时间减少高达97%，同时保持相同的修复质量。

Conclusion: 论文为PG-Constraints约束下的属性图修复提供了系统方法，LP引导贪心算法在效率和质量上取得良好平衡，标签删除策略显著减少修复代价，对图数据库约束维护有实际应用价值。

Abstract: Recent standardization efforts for graph databases lead to standard query languages like GQL and SQL/PGQ, and constraint languages like Property Graph Constraints (PG-Constraints). In this paper, we embark on the study of repairing property graphs under PG-Constraints. We identify a significant subset of PG-Constraints, encoding denial constraints and including recursion as a key feature, while still permitting automata-based structural analyses of errors. We present a comprehensive repair pipeline for these constraints to repair Property Graphs, involving changes in the graph topology and leading to node, edge and, optionally, label deletions. We investigate three algorithmic strategies for the repair procedure, based on Integer Linear Programming (ILP), a naive, and an LP-guided greedy algorithm. Our experiments on various real-world datasets reveal that repairing with label deletions can achieve a 59% reduction in deletions compared to node/edge deletions. Moreover, the LP-guided greedy algorithm offers a runtime advantage of up to 97% compared to the ILP strategy, while matching the same quality.

</details>


### [27] [Taking the Leap: Efficient and Reliable Fine-Grained NUMA Migration in User-space](https://arxiv.org/abs/2602.05540)
*Felix Schuhknecht,Nick Rassau*

Main category: cs.DB

TL;DR: 提出page_leap()用户空间页面迁移方法，解决NUMA架构下内存访问性能问题，相比现有方案具有更高性能和更完整功能


<details>
  <summary>Details</summary>
Motivation: 现代多插槽架构虽然提供单一虚拟地址空间，但物理内存分布在多个NUMA区域。当查询并行化跨多个区域核心执行时，数据应分布在相同区域以确保本地访问。现有迁移方案（Linux自动NUMA平衡和move_pages()系统调用）在功能集和性能方面存在显著缺陷。

Method: 提出page_leap()用户空间迁移方法，通过利用虚拟内存子系统特性实现异步高性能页面迁移。该方法具有以下特点：(a) 用户主动触发，(b) 确保所有页面最终迁移，(c) 正确处理并发写入，(d) 支持池化内存，(e) 根据工作负载自适应调整迁移粒度，(f) 支持小页面和大页面。

Result: 论文未提供具体实验结果，但从方法描述来看，page_leap()相比现有方案在功能和性能方面都有显著改进，能够更有效地优化NUMA架构下的内存访问性能。

Conclusion: page_leap()作为一种新的用户空间页面迁移方法，解决了现有NUMA页面迁移方案的局限性，为数据库等并行应用在NUMA架构上实现高效内存访问提供了更好的解决方案。

Abstract: Modern multi-socket architectures offer a single virtual address space, but physically divide main-memory across multiple regions, where each region is attached to a CPU and its cores. While this simplifies the usage, developers must be aware of non-uniform memory access (NUMA), where an access by a thread running on a core-local NUMA region is significantly cheaper than an access from a core-remote region. Obviously, if query answering is parallelized across the cores of multiple regions, then the portion of the database on which the query is operating should be distributed across the same regions to ensure local accesses. As the present data placement might not fit this, migrating pages from one NUMA region to another can be performed to improve the situation. To do so, different options exist: One option is to rely on automatic NUMA balancing integrated in Linux, which is steered by the observed access patterns and frequency. Another option is to actively trigger migration via the system call move_pages(). Unfortunately, both variants have significant downsides in terms of their feature set and performance. As an alternative, we propose a new user-space migration method called page_leap() that can perform page migration asynchronously at a high performance by exploiting features of the virtual memory subsystem. The method is (a) actively triggered by the user, (b) ensures that all pages are eventually migrated, (c) handles concurrent writes correctly, (d) supports pooled memory, (e) adaptively adjusts its migration granularity based on the workload, and (f) supports both small pages and huge pages.

</details>


### [28] [One Size Does NOT Fit All: On the Importance of Physical Representations for Datalog Evaluation](https://arxiv.org/abs/2602.05651)
*Nick Rassau,Felix Schuhknecht*

Main category: cs.DB

TL;DR: 该论文研究了Datalog查询引擎中物理表示选择的问题，通过实验分析工作负载特性与物理表示之间的相互作用，并设计了一个基于决策树的自动选择机制。


<details>
  <summary>Details</summary>
Motivation: 现有Datalog引擎在物理表示选择上过于限制，通常采用一刀切的解决方案。Datalog程序的执行计划涉及多种操作类型（插入、查找、包含检查），且不同工作负载特性（关系大小、多重性、递归性等）对操作需求不同，需要多样化的物理表示来满足不同场景。

Method: 1. 深入实验研究：分析潜在合适的物理表示与七个工作负载特性维度之间的相互作用；2. 设计自动选择机制：利用一组决策树来识别适合给定工作负载的物理表示。

Result: 实验揭示了哪些工作负载特性真正重要，为物理表示选择提供了实证依据。基于这些洞察设计的决策树机制能够自动为特定工作负载选择最优的物理表示。

Conclusion: Datalog查询引擎需要根据具体工作负载特性灵活选择物理表示，而不是采用一刀切的方案。通过实验分析和决策树机制，可以实现自动化的物理表示选择，从而优化Datalog程序的执行性能。

Abstract: Datalog is an increasingly popular recursive query language that is declarative by design, meaning its programs must be translated by an engine into the actual physical execution plan. When generating this plan, a central decision is how to physically represent all involved relations, an aspect in which existing Datalog engines are surprisingly restrictive and often resort to one-size-fits-all solutions. The reason for this is that the typical execution plan of a Datalog program not only performs a single type of operation against the physical representations, but a mixture of operations, such as insertions, lookups, and containment-checks. Further, the relevance of each operation type highly depends on the workload characteristics, which range from familiar properties such as the size, multiplicity, and arity of the individual relations to very specific Datalog properties, such as the "interweaving" of rules when relations occur multiple times, and in particular the recursiveness of the query which might generate new tuples on the fly during evaluation. This indicates that a variety of physical representations, each with its own strengths and weaknesses, is required to meet the specific needs of different workload situations. To evaluate this, we conduct an in-depth experimental study of the interplay between potentially suitable physical representations and seven dimensions of workload characteristics that vary across actual Datalog programs, revealing which properties actually matter. Based on these insights, we design an automatic selection mechanism that utilizes a set of decision trees to identify suitable physical representations for a given workload.

</details>


### [29] [Fast Private Adaptive Query Answering for Large Data Domains](https://arxiv.org/abs/2602.05674)
*Miguel Fuentes,Brett Mullins,Yingtai Xiao,Daniel Kifer,Cameron Musco,Daniel Sheldon*

Main category: cs.DB

TL;DR: 提出AIM+GReM机制，通过残差查询和优化策略改进差分隐私下的边际分布发布，显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私边际发布机制在从噪声测量重建边际估计时存在计算瓶颈，需要更高效的解决方案

Method: 提出基于多维数组的残差查询概念框架、惰性更新策略、自适应每轮隐私预算分配，并集成到AIM+GReM机制中

Result: AIM+GReM比原始框架快几个数量级，具有竞争性的误差和显著改善的可扩展性

Conclusion: 残差查询与自适应机制的有效集成能够大幅提升差分隐私边际发布的计算效率和可扩展性

Abstract: Privately releasing marginals of a tabular dataset is a foundational problem in differential privacy. However, state-of-the-art mechanisms suffer from a computational bottleneck when marginal estimates are reconstructed from noisy measurements. Recently, residual queries were introduced and shown to lead to highly efficient reconstruction in the batch query answering setting. We introduce new techniques to integrate residual queries into state-of-the-art adaptive mechanisms such as AIM. Our contributions include a novel conceptual framework for residual queries using multi-dimensional arrays, lazy updating strategies, and adaptive optimization of the per-round privacy budget allocation. Together these contributions reduce error, improve speed, and simplify residual query operations. We integrate these innovations into a new mechanism (AIM+GReM), which improves AIM by using fast residual-based reconstruction instead of a graphical model approach. Our mechanism is orders of magnitude faster than the original framework and demonstrates competitive error and greatly improved scalability.

</details>


### [30] [Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration](https://arxiv.org/abs/2602.05708)
*Chuangtao Ma,Zeyu Zhang,Arijit Khan,Sebastian Schelter,Paul Groth*

Main category: cs.DB

TL;DR: CE-RAG4EM：一种用于实体匹配的成本高效RAG架构，通过基于分块的批量检索和生成减少计算开销，在保持匹配质量的同时显著降低端到端运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有RAG管道在大规模实体匹配中产生大量检索和生成开销，需要更高效的解决方案来平衡性能与计算成本。

Method: 提出CE-RAG4EM架构，采用基于分块的批量检索和生成策略，并建立统一的RAG系统分析评估框架，重点关注分块感知优化和检索粒度。

Result: 实验表明CE-RAG4EM在保持或提升匹配质量的同时，显著减少端到端运行时间。分析揭示关键配置参数在性能和开销之间存在固有权衡。

Conclusion: CE-RAG4EM为实体匹配和数据集成提供了高效可扩展的RAG系统设计指导，通过优化配置参数平衡性能与计算开销。

Abstract: Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.

</details>


### [31] [Even Faster Geosocial Reachability Queries](https://arxiv.org/abs/2602.05928)
*Rick van der Heijden,Nikolay Yakovets,Thekla Hamm*

Main category: cs.DB

TL;DR: 2DReach：一种更简单的geosocial可达性查询方法，用2D R-tree替代3DReach的区间标签和3D R-tree，实现更快的索引构建和更稳定的查询性能。


<details>
  <summary>Details</summary>
Motivation: 现有3DReach方法使用区间标签和3D R-tree进行geosocial可达性查询，方法复杂。本文提出更简单的2DReach方法，避免区间标签的复杂性，同时保持或提升性能。

Method: 将强连通分量压缩为DAG，为每个分量直接存储2D R-tree覆盖所有可达空间顶点。查询简化为单次2D R-tree查找。提出压缩变体：排除空间汇点，在具有相同可达集的分量间共享R-tree。

Result: 在四个真实数据集上的实验显示：2DReach比3DReach索引构建更快；压缩变体在所有方法中索引尺寸最小；查询性能具有竞争力或更优，且响应时间在不同查询参数下更稳定。

Conclusion: 2DReach通过避免区间标签的复杂性，提供了一种更简单有效的geosocial可达性查询解决方案，在索引构建速度、存储效率和查询性能稳定性方面优于现有方法。

Abstract: Geosocial reachability queries (\textsc{RangeReach}) determine whether a given vertex in a geosocial network can reach any spatial vertex within a query region. The state-of-the-art 3DReach method answers such queries by encoding graph reachability through interval labelling and indexing spatial vertices in a 3D R-tree. We present 2DReach, a simpler approach that avoids interval labelling entirely. Like 3DReach, 2DReach collapses strongly connected components (SCCs) into a DAG, but instead of computing interval labels, it directly stores a 2D R-tree per component over all reachable spatial vertices. A query then reduces to a single 2D R-tree lookup. We further propose compressed variants that reduce storage by excluding spatial sinks and sharing R-trees between components with identical reachable sets. Experiments on four real-world datasets show that 2DReach achieves faster index construction than 3DReach, with the compressed variant yielding the smallest index size among all methods. 2DReach delivers competitive or superior query performance with more stable response times across varying query parameters.

</details>


### [32] ["Detective Work We Shouldn't Have to Do": Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems](https://arxiv.org/abs/2602.05944)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 该论文通过访谈欧盟数据从业者，研究机器学习系统中监管要求与数据质量实践之间的差距，发现法律原则与工程流程脱节、工具不足、责任不清等问题，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 随着欧盟GDPR和AI法案等监管框架对机器学习系统数据质量提出要求，需要了解数据从业者如何在实际工作中应对这些监管要求，以及监管原则与工程实践之间的差距。

Method: 采用定性访谈研究方法，对欧盟地区在受监管环境中从事机器学习系统的数据从业者进行半结构化访谈，探讨他们对监管要求下数据质量的理解、面临的挑战和所需支持。

Result: 研究发现：法律原则与工程流程存在持续差距；数据管道碎片化；现有工具存在局限性；技术团队与法律团队责任边界不清；数据质量实践倾向于被动、审计驱动的方式。

Conclusion: 从业者需要合规意识更强的工具、更清晰的治理结构，以及向主动数据治理的文化转变，以弥合监管要求与工程实践之间的差距。

Abstract: Ensuring data quality in machine learning (ML) systems has become increasingly complex as regulatory requirements expand. In the European Union (EU), frameworks such as the General Data Protection Regulation (GDPR) and the Artificial Intelligence Act (AI Act) articulate data quality requirements that closely parallel technical concerns in ML practice, while also extending to legal obligations related to accountability, risk management, and human rights protection. This paper presents a qualitative interview study with EU-based data practitioners working on ML systems in regulated contexts. Through semi-structured interviews, we investigate how practitioners interpret regulatory-aligned data quality, the challenges they encounter, and the supports they identify as necessary. Our findings reveal persistent gaps between legal principles and engineering workflows, fragmentation across data pipelines, limitations of existing tools, unclear responsibility boundaries between technical and legal teams, and a tendency toward reactive, audit-driven quality practices. We also identify practitioners' needs for compliance-aware tooling, clearer governance structures, and cultural shifts toward proactive data governance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [A novel scalable high performance diffusion solver for multiscale cell simulations](https://arxiv.org/abs/2602.05017)
*Jose-Luis Estragues-Muñoz,Carlos Alvarez,Arnau Montagud,Daniel Jimenez-Gonzalez,Alfonso Valencia*

Main category: cs.DC

TL;DR: 提出可扩展的HPC解决方案BioFVM库，用于分子扩散建模，相比现有方法实现200倍加速和36%内存减少


<details>
  <summary>Details</summary>
Motivation: 基于代理的细胞模型在模拟组织演化时需要处理个体细胞行为、细胞间相互作用和微环境响应。关键挑战是将细胞分辨率模型扩展到真实规模的肿瘤模拟，这对疾病数字孪生模型开发至关重要，需要利用高性能计算处理万亿级操作。

Method: 提出可扩展的HPC解决方案，采用高效实现的最先进有限体积法框架，系统评估新型可扩展生物有限体积法库，并对可用解决方案进行广泛的性能分析。

Result: HPC方案相比当前最先进解决方案实现近200倍加速和高达36%的内存使用减少。

Conclusion: 该工作为高效计算下一代生物问题铺平了道路。

Abstract: Agent-based cellular models simulate tissue evolution by capturing the behavior of individual cells, their interactions with neighboring cells, and their responses to the surrounding microenvironment. An important challenge in the field is scaling cellular resolution models to real-scale tumor simulations, which is critical for the development of digital twin models of diseases and requires the use of High-Performance Computing (HPC) since every time step involves trillions of operations. We hereby present a scalable HPC solution for the molecular diffusion modeling using an efficient implementation of state-of-the-art Finite Volume Method (FVM) frameworks. The paper systematically evaluates a novel scalable Biological Finite Volume Method (BioFVM) library and presents an extensive performance analysis of the available solutions. Results shows that our HPC proposal reach almost 200x speedup and up to 36% reduction in memory usage over the current state-of-the-art solutions, paving the way to efficiently compute the next generation of biological problems.

</details>


### [34] [Towards Advancing Research with Workflows: A perspective from the Workflows Community Summit -- Amsterdam, 2025](https://arxiv.org/abs/2602.05131)
*Irene Bonati,Silvina Caino-Lores,Tainã Coleman,Sagar Dolas,Sandro Fiore,Venkatesh Kannan,Marco Verdicchio,Sean R. Wilkinson,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 2025年科学工作流社区峰会识别了工作流采用的关键障碍，并提出了技术、政策和社区层面的解决方案，强调从计算性能评估转向科学影响衡量，建立标准化模式，培养国际社区，以及投资人力资本。


<details>
  <summary>Details</summary>
Motivation: 科学工作流在现代研究中对于协调分布式计算、管理大数据和确保可重复性至关重要，但面临采用障碍。2025年工作流社区峰会旨在召集国际专家，识别挑战并制定解决方案，以推动工作流在科学发现中的应用。

Method: 通过2025年6月6日在阿姆斯特丹举行的国际工作流社区峰会，汇集专家讨论，识别关键挑战，并提出跨技术、政策和社区维度的行动路线。

Result: 识别了四大关键障碍：系统通用性与领域特定性之间的张力、工作流系统长期可持续性问题、工作流开发者缺乏认可、标准化/资金/培训/跨学科合作不足。提出了四个行动路线：评估指标转向科学影响、建立标准化模式和基准、培养国际社区、投资人力资本和教育。

Conclusion: 科学工作流需要系统性变革，包括评估体系转型、标准化建设、社区发展和人才培养，以充分发挥其在推动科学发现中的潜力。峰会提出的多维度行动路线为工作流领域的未来发展提供了路线图。

Abstract: Scientific workflows have become essential for orchestrating complex computational processes across distributed resources, managing large datasets, and ensuring reproducibility in modern research. The Workflows Community Summit 2025, held in Amsterdam on June 6th, 2025, convened international experts to examine emerging challenges and opportunities in this domain. Participants identified key barriers to workflow adoption, including tensions between system generality and domain-specific utility, concerns over long-term sustainability of workflow systems and services, insufficient recognition for those who develop and maintain reproducible workflows, and gaps in standardization, funding, training, and cross-disciplinary collaboration. To address these challenges, the summit proposed action lines spanning technology, policy, and community dimensions: shifting evaluation metrics from raw computational performance toward measuring genuine scientific impact; formalizing workflow patterns and community-driven benchmarks to improve transparency, reproducibility, and usability; cultivating a cohesive international workflows community that engages funding bodies and research stakeholders; and investing in human capital through dedicated workflow engineering roles, career pathways, and integration of workflow concepts into educational curricula and long-term training initiatives. This document presents the summit's findings, beginning with an overview of the current computing ecosystem and the rationale for workflow-centric approaches, followed by a discussion of identified challenges and recommended action lines for advancing scientific discovery through workflows.

</details>


### [35] [ORACL: Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices](https://arxiv.org/abs/2602.05292)
*Haoyu Bai,Muhammed Tawfiqul Islam,Minxian Xu,Rajkumar Buyya*

Main category: cs.DC

TL;DR: ORACL使用大语言模型作为通用少样本资源分配器，通过思维链推理诊断微服务性能问题并推荐资源分配，无需部署特定训练即可提升根因识别准确率15%、训练加速24倍、服务质量提升6%。


<details>
  <summary>Details</summary>
Motivation: 随着应用从单体架构转向微服务和无服务器架构，现有的自动扩缩容策略存在局限性：基于学习的模型需要大量部署特定训练，而手动调优的规则又脆弱且难以泛化。需要一种能够适应快速演进的微服务部署的通用资源分配方案。

Method: 提出ORACL框架，利用大语言模型的先验知识和思维链推理能力。将运行时遥测数据（pods、副本、CPU/内存使用率、延迟、SLO、故障信号）转换为语义自然语言状态描述，调用LLM生成可解释的中间推理轨迹，识别根因、剪枝动作空间，并在策略约束下做出安全的分配决策。

Result: 在代表性开源微服务工作负载上的实验表明：ORACL将根因识别准确率提高15%，训练速度提升高达24倍，短期场景下服务质量提升6%，且无需部署特定的重新训练。

Conclusion: 大语言模型可以作为通用的少样本资源分配器，通过思维链推理适应快速演进的微服务部署，提供比现有方法更好的性能诊断和资源分配能力，同时保持可解释性和泛化性。

Abstract: Applications are moving away from monolithic designs to microservice and serverless architectures, where fleets of lightweight and independently deployable components run on public clouds. Autoscaling serves as the primary control mechanism for balancing resource utilization and quality of service, yet existing policies are either opaque learned models that require substantial per-deployment training or brittle hand-tuned rules that fail to generalize. We investigate whether large language models can act as universal few-shot resource allocators that adapt across rapidly evolving microservice deployments.
  We propose ORACL, Optimized Reasoning for Autoscaling via Chain of Thought with LLMs for Microservices, a framework that leverages prior knowledge and chain-of-thought reasoning to diagnose performance regressions and recommend resource allocations. ORACL transforms runtime telemetry, including pods, replicas, CPU and memory usage, latency, service-level objectives, and fault signals, into semantic natural-language state descriptions and invokes an LLM to produce an interpretable intermediate reasoning trace. This reasoning identifies likely root causes, prunes the action space, and issues safe allocation decisions under policy constraints. Experiments on representative open-source microservice workloads show that ORACL improves root-cause identification accuracy by 15 percent, accelerates training by up to 24x, and improves quality of service by 6 percent in short-term scenarios, without deployment-specific retraining.

</details>


### [36] [Proteus: Append-Only Ledgers for (Mostly) Trusted Execution Environments](https://arxiv.org/abs/2602.05346)
*Shubham Mishra,João Gonçalves,Chawinphat Tankuranand,Neil Giridharan,Natacha Crooks,Heidi Howard,Chris Jensen*

Main category: cs.DC

TL;DR: Proteus是一个分布式共识协议，通过在CFT协议中嵌入BFT协议，谨慎信任TEE硬件，在TEE可能被攻击的情况下仍能保证数据完整性。


<details>
  <summary>Details</summary>
Motivation: 分布式账本依赖硬件TEE提供信任，但TEE可能受到攻击，这会破坏分布式账本精心设计的保证。需要一种在TEE可能被攻破时仍能保证完整性的解决方案。

Method: 通过精心重构CFT和BFT协议使它们结构对齐，在CFT协议中嵌入BFT协议而不增加额外消息。谨慎信任TEE的保证，在TEE平台可能被攻破时仍能保证完整性。

Result: Proteus在保持与常规TEE增强共识协议相当性能的同时，能够在TEE平台被攻破的情况下保证数据完整性。

Conclusion: Proteus提供了一种平衡的方法，既利用TEE的性能优势，又通过嵌入BFT协议提供额外的安全保障，在硬件TEE可能不可靠的情况下仍能保证分布式账本的完整性。

Abstract: Distributed ledgers are increasingly relied upon by industry to provide trustworthy accountability, strong integrity protection, and high availability for critical data without centralizing trust. Recently, distributed append-only logs are opting for a layered approach, combining crash-fault-tolerant (CFT) consensus with hardware-based Trusted Execution Environments (TEEs) for greater resiliency. Unfortunately, hardware TEEs can be subject to (rare) attacks, undermining the very guarantees that distributed ledgers are carefully designed to achieve. In response, we present Proteus, a new distributed consensus protocol that cautiously trusts the guarantees of TEEs. Proteus carefully embeds a Byzantine fault-tolerant (BFT) protocol inside of a CFT protocol with no additional messages. This is made possible through careful refactoring of both the CFT and BFT protocols such that their structure aligns. Proteus achieves performance in line with regular TEE-enabled consensus protocols, while guaranteeing integrity in the face of TEE platform compromises.

</details>


### [37] [Reaching Univalency with Subquadratic Communication](https://arxiv.org/abs/2602.05356)
*Andrew Lewis-Pye*

Main category: cs.DC

TL;DR: 论文探讨Dolev-Reischuk下界（Ω(f²+n)消息）的本质，证明达到单值性（univalency）不需要二次通信，二次成本主要来自结果传播而非决策达成。


<details>
  <summary>Details</summary>
Motivation: 理解Dolev-Reischuk下界中二次通信成本的根本原因：是达成共识决策（单值性）需要二次通信，还是仅仅将结果传播给所有处理器需要二次通信？

Method: 提出ε-BA（允许ε比例正确处理器输出错误），证明在f < n(1/3 - ε)时可用O(n log n)通信复杂度确定性解决。任何ε-BA协议可作为完整BA协议的第一阶段，第二阶段只需一次全对全交换和多数投票。在认证设置中定义Extractable BA，证明可用O(f log f)通信解决。

Result: 达到单值性不需要二次通信，二次成本完全来自结果传播。ε-BA可在O(n log n)通信内解决，Extractable BA在认证设置中可在O(f log f)通信内解决。

Conclusion: Dolev-Reischuk下界的二次通信成本不是达成共识决策（单值性）所必需，而是将结果传播给所有处理器所需。这澄清了拜占庭协议通信复杂性的根本原因。

Abstract: The Dolev-Reischuk lower bound establishes that any deterministic Byzantine Agreement (BA) protocol for $n$ processors tolerating $f$ faults requires $Ω(f^2+n)$ messages. But what exactly does this quadratic cost pay for? Even the minimal requirement that every correct processor \emph{receive at least one message} already necessitates $Ω(f^2 + n)$ messages. This raises a fundamental question: is the Dolev-Reischuk bound about the difficulty of \emph{reaching univalency} -- the point at which the protocol's outcome is determined -- or merely about \emph{disseminating} the outcome to all processors afterward?
  We resolve this question by showing that reaching univalency does \emph{not} require quadratic communication. Specifically, we introduce $ε$-BA, a relaxation allowing an $ε$-fraction of correct processors to output incorrectly, and prove it can be solved deterministically with $O(n \log n)$ communication complexity when $f < n(1/3 - ε)$. Crucially, any $ε$-BA protocol can serve as the first phase of a full BA protocol: after $ε$-BA, a single all-to-all exchange and majority vote completes BA. Since the outcome is already determined after $ε$-BA, this demonstrates that the quadratic cost in Dolev-Reischuk stems entirely from dissemination, rather than from reaching univalency. We also define Extractable BA for authenticated settings, capturing when processors collectively hold enough signed messages to determine the agreed value, and show it can be solved with communication complexity $O(f \log f)$.

</details>


### [38] [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754)
*Seonghye Cho,Jaemin Han,Hyunjin Kim,Euisoo Jung,Jae-Gil Lee*

Main category: cs.DC

TL;DR: TimelyFreeze：一种通过建模流水线调度为有向无环图并求解线性规划问题来计算最优冻结比例的方法，在保证精度的前提下减少流水线气泡，提升训练吞吐量


<details>
  <summary>Details</summary>
Motivation: 流水线并行虽然能训练超出单设备内存的模型，但实际吞吐量受限于流水线气泡。现有的参数冻结方法虽然能通过自适应跳过反向计算来提高训练吞吐量，但往往过度冻结参数，导致不必要的精度下降。

Method: 将流水线调度建模为有向无环图，通过求解线性规划问题计算最优冻结比例，在精度约束下最小化批次执行时间。该方法能自适应地确定哪些参数应该冻结，避免过度冻结。

Result: 在LLaMA-8B模型上实现了高达40%的训练吞吐量提升，同时保持可比的精度。该方法在不影响收敛的情况下加速大规模模型训练，并能泛化到不同的流水线并行设置。

Conclusion: TimelyFreeze通过优化参数冻结策略，有效解决了流水线并行中的气泡问题，在保证模型精度的同时显著提升了训练效率，为大规模模型训练提供了实用的解决方案。

Abstract: Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.

</details>


### [39] [Location-Aware Dispersion on Anonymous Graphs](https://arxiv.org/abs/2602.05948)
*Himani,Supantha Pandit,Gokarna Sharma*

Main category: cs.DC

TL;DR: 论文提出了位置感知分散问题，这是经典分散问题的新推广，要求机器人根据颜色匹配移动到对应颜色的节点上，并设计了确定性算法，给出了时间与内存的界限，同时证明了不可能性和下界。


<details>
  <summary>Details</summary>
Motivation: 经典分散问题只要求机器人占据不同节点，不考虑节点属性。实际应用中，机器人可能需要移动到特定类型的节点（如充电站、工作站等）。位置感知分散问题引入了颜色标签，要求机器人移动到与其颜色匹配的节点，这更贴近实际机器人协调需求。

Method: 提出了位置感知分散问题的形式化定义，开发了多个确定性算法，这些算法保证在匿名、连通、无向图上工作。算法设计考虑了时间复杂度和内存需求的最小化。同时给出了不可能性证明和任意确定性算法的下界。

Result: 证明了位置感知分散问题在匿名网络中的算法可行性，给出了具有时间与内存保证界限的确定性算法。当颜色集合大小为1时，问题退化为经典分散问题。与经典分散相比，位置感知分散在效率上面临更大挑战。

Conclusion: 位置感知分散问题是分散问题的有意义推广，在匿名网络中算法可行但比经典分散更具挑战性。论文建立了该问题的理论基础，提供了算法解决方案，并揭示了其固有的复杂性限制。

Abstract: The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.

</details>
