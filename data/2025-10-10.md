<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 9]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Modeling Developer Burnout with GenAI Adoption](https://arxiv.org/abs/2510.07435)
*Zixuan Feng,Sadia Afroz,Anita Sarma*

Main category: cs.SE

TL;DR: 研究探讨生成式AI采用与开发者倦怠的关系，发现GenAI通过增加工作需求加剧倦怠，但工作资源和积极认知可缓解此效应。


<details>
  <summary>Details</summary>
Motivation: 虽然先前研究强调GenAI提高生产力，但其采用可能带来新压力影响开发者福祉，需要实证研究验证。

Method: 采用嵌入式混合方法研究设计，调查442名开发者，使用PLS-SEM和回归分析建模工作需求、资源与倦怠关系，辅以定性分析。

Result: GenAI采用通过增加工作需求加剧倦怠，但工作资源和积极认知能减轻这些负面影响。

Conclusion: GenAI采用既是挑战也是机遇，适当的工作资源和积极认知可帮助开发者更好应对技术变革。

Abstract: Generative AI (GenAI) is rapidly reshaping software development workflows.
While prior studies emphasize productivity gains, the adoption of GenAI also
introduces new pressures that may harm developers' well-being. In this paper,
we investigate the relationship between the adoption of GenAI and developers'
burnout. We utilized the Job Demands--Resources (JD--R) model as the analytic
lens in our empirical study. We employed a concurrent embedded mixed-methods
research design, integrating quantitative and qualitative evidence. We first
surveyed 442 developers across diverse organizations, roles, and levels of
experience. We then employed Partial Least Squares--Structural Equation
Modeling (PLS-SEM) and regression to model the relationships among job demands,
job resources, and burnout, complemented by a qualitative analysis of
open-ended responses to contextualize the quantitative findings. Our results
show that GenAI adoption heightens burnout by increasing job demands, while job
resources and positive perceptions of GenAI mitigate these effects, reframing
adoption as an opportunity.

</details>


### [2] [HotBugs.jar: A Benchmark of Hot Fixes for Time-Critical Bugs](https://arxiv.org/abs/2510.07529)
*Carol Hanna,Federica Sarro,Mark Harman,Justyna Petke*

Main category: cs.SE

TL;DR: HotBugs.jar是首个专门针对真实世界热修复的数据集，包含679个经过手动验证的热修复案例，其中110个可复现，为快速调试、自动修复等研究提供基准。


<details>
  <summary>Details</summary>
Motivation: 热修复是解决生产系统中紧急问题的关键变更，但现有评估基准缺乏专门针对热修复的数据集。

Method: 从10个Apache项目的19万次提交和15万份问题报告中识别746个候选热修复，经过手动评估确认679个真实热修复，其中110个可复现。

Result: 构建了HotBugs.jar数据集，包含679个手动验证的热修复和110个可复现案例，每个案例都包含错误版本、修复版本、测试套件和元数据。

Conclusion: HotBugs.jar已成为SBSE会议挑战赛的官方数据集，为快速调试、自动修复等研究提供了重要基准。

Abstract: Hot fixes are urgent, unplanned changes deployed to production systems to
address time-critical issues. Despite their importance, no existing evaluation
benchmark focuses specifically on hot fixes. We present HotBugs.jar, the first
dataset dedicated to real-world hot fixes. From an initial mining of 10 active
Apache projects totaling over 190K commits and 150K issue reports, we
identified 746 software patches that met our hot-fix criteria. After manual
evaluation, 679 were confirmed as genuine hot fixes, of which 110 are
reproducible using a test suite. Building upon the Bugs.jar framework,
HotBugs.jar integrates these 110 reproducible cases and makes available all 679
manually validated hot fixes, each enriched with comprehensive metadata to
support future research. Each hot fix was systematically identified using Jira
issue data, validated by independent reviewers, and packaged in a reproducible
format with buggy and fixed versions, test suites, and metadata. HotBugs.jar
has already been adopted as the official challenge dataset for the Search-Based
Software Engineering (SBSE) Conference Challenge Track, demonstrating its
immediate impact. This benchmark enables the study and evaluation of tools for
rapid debugging, automated repair, and production-grade resilience in modern
software systems to drive research in this essential area forward.

</details>


### [3] [RustAssure: Differential Symbolic Testing for LLM-Transpiled C-to-Rust Code](https://arxiv.org/abs/2510.07604)
*Yubo Bai,Tapti Palit*

Main category: cs.SE

TL;DR: RustAssure是一个使用大语言模型自动将C代码转换为Rust的系统，通过提示工程生成惯用安全的Rust代码，并使用差分符号测试验证语义等价性。


<details>
  <summary>Details</summary>
Motivation: 为了利用Rust的内存安全特性提升软件安全性，需要将现有用不安全内存语言（如C）编写的代码库转换为Rust。

Method: 使用大语言模型进行自动代码转换，通过提示工程技术优化生成的Rust代码质量，并采用差分符号测试来验证原始C代码与转换后Rust代码的语义相似性。

Result: 在五个真实应用和库的评估中，系统能为89.8%的C函数生成可编译的Rust函数，其中69.9%的函数在符号返回值上表现出等价性。

Conclusion: RustAssure能够有效自动化C到Rust的代码转换过程，生成大量可编译且语义等价的Rust代码，为利用Rust安全特性提供了可行路径。

Abstract: Rust is a memory-safe programming language that significantly improves
software security. Existing codebases written in unsafe memory languages, such
as C, must first be transpiled to Rust to take advantage of Rust's improved
safety guarantees. RustAssure presents a system that uses Large Language Models
(LLMs) to automatically transpile existing C codebases to Rust. RustAssure uses
prompt engineering techniques to maximize the chances of the LLM generating
idiomatic and safe Rust code. Moreover, because LLMs often generate code with
subtle bugs that can be missed under traditional unit or fuzz testing,
RustAssure performs differential symbolic testing to establish the semantic
similarity between the original C and LLM-transpiled Rust code. We evaluated
RustAssure with five real-world applications and libraries, and showed that our
system is able to generate compilable Rust functions for 89.8% of all C
functions, of which 69.9% produced equivalent symbolic return values for both
the C and Rust functions.

</details>


### [4] [AppForge: From Assistant to Independent Developer - Are GPTs Ready for Software Development?](https://arxiv.org/abs/2510.07740)
*Dezhi Ran,Yuan Cao,Mengzhou Wu,Simin Chen,Yuzhe Guo,Jun Ren,Zihe Song,Hao Yu,Jialei Wei,Linyi Li,Wei Yang,Baishakhi Ray,Tao Xie*

Main category: cs.SE

TL;DR: 提出了APPFORGE基准测试，包含101个真实Android应用开发问题，评估LLM从零构建完整软件系统的能力。测试结果显示当前模型表现不佳，最佳模型GPT-5仅能开发18.8%功能正确的应用。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估函数级代码生成，但无法评估LLM构建完整软件系统的能力。真实应用开发需要协调组件交互、维护状态一致性、处理生命周期和框架约束。

Method: 设计多智能体系统自动总结应用文档功能，通过导航应用合成测试用例验证功能正确性。经过Android专家手动验证后，构建包含自动化评估框架的基准测试。

Result: 评估12个主流LLM，所有模型效果都很低，最佳模型GPT-5仅能开发18.8%功能正确的应用。

Conclusion: 当前LLM在处理复杂、多组件的软件工程挑战方面存在根本性局限，无法有效构建完整软件系统。

Abstract: Large language models (LLMs) have demonstrated remarkable capability in
function-level code generation tasks. Unlike isolated functions, real-world
applications demand reasoning over the entire software system: developers must
orchestrate how different components interact, maintain consistency across
states over time, and ensure the application behaves correctly within the
lifecycle and framework constraints. Yet, no existing benchmark adequately
evaluates whether LLMs can bridge this gap and construct entire software
systems from scratch. To address this gap, we propose APPFORGE, a benchmark
consisting of 101 software development problems drawn from real-world Android
apps. Given a natural language specification detailing the app functionality, a
language model is tasked with implementing the functionality into an Android
app from scratch. Developing an Android app from scratch requires understanding
and coordinating app states, lifecycle management, and asynchronous operations,
calling for LLMs to generate context-aware, robust, and maintainable code. To
construct APPFORGE, we design a multi-agent system to automatically summarize
the main functionalities from app documents and navigate the app to synthesize
test cases validating the functional correctness of app implementation.
Following rigorous manual verification by Android development experts, APPFORGE
incorporates the test cases within an automated evaluation framework that
enables reproducible assessment without human intervention, making it easily
adoptable for future research. Our evaluation on 12 flagship LLMs show that all
evaluated models achieve low effectiveness, with the best-performing model
(GPT-5) developing only 18.8% functionally correct applications, highlighting
fundamental limitations in current models' ability to handle complex,
multi-component software engineering challenges.

</details>


### [5] [Interleaved Learning and Exploration: A Self-Adaptive Fuzz Testing Framework for MLIR](https://arxiv.org/abs/2510.07815)
*Zeyu Sun,Jingjing Liang,Weiyi Wang,Chenyao Suo,Junjie Chen,Fanjiang Xu*

Main category: cs.SE

TL;DR: FLEX是一个用于MLIR的自适应模糊测试框架，利用神经网络生成程序，通过扰动采样策略增强多样性，并采用反馈驱动的增强循环来改进模型。


<details>
  <summary>Details</summary>
Motivation: 确保MLIR的正确性和鲁棒性具有挑战性，现有的基于手动模板或基于规则突变的模糊测试方法难以生成足够多样且语义有效的测试用例，难以暴露MLIR复杂且不断演进的代码空间中的细微或深层错误。

Method: FLEX利用神经网络进行程序生成，采用扰动采样策略鼓励多样性，并通过反馈驱动的增强循环迭代改进模型，使用崩溃和非崩溃测试案例。

Result: 在30天的测试中，FLEX发现了80个先前未知的错误，包括多个新的根本原因和解析器错误；在24小时固定版本比较中，检测到53个错误（比最佳基线多3.5倍以上），并实现了28.2%的代码覆盖率，比次优工具高出42%。

Conclusion: 消融研究进一步证实了扰动生成和多样性增强在FLEX有效性中的关键作用。

Abstract: MLIR (Multi-Level Intermediate Representation) has rapidly become a
foundational technology for modern compiler frameworks, enabling extensibility
across diverse domains. However, ensuring the correctness and robustness of
MLIR itself remains challenging. Existing fuzzing approaches-based on manually
crafted templates or rule-based mutations-struggle to generate sufficiently
diverse and semantically valid test cases, making it difficult to expose subtle
or deep-seated bugs within MLIR's complex and evolving code space. In this
paper, we present FLEX, a novel self-adaptive fuzzing framework for MLIR. FLEX
leverages neural networks for program generation, a perturbed sampling strategy
to encourage diversity, and a feedback-driven augmentation loop that
iteratively improves its model using both crashing and non-crashing test cases.
Starting from a limited seed corpus, FLEX progressively learns valid syntax and
semantics and autonomously produces high-quality test inputs. We evaluate FLEX
on the upstream MLIR compiler against four state-of-the-art fuzzers. In a
30-day campaign, FLEX discovers 80 previously unknown bugs-including multiple
new root causes and parser bugs-while in 24-hour fixed-revision comparisons, it
detects 53 bugs (over 3.5x as many as the best baseline) and achieves 28.2%
code coverage, outperforming the next-best tool by 42%. Ablation studies
further confirm the critical role of both perturbed generation and diversity
augmentation in FLEX's effectiveness.

</details>


### [6] [Bug Histories as Sources of Compiler Fuzzing Mutators](https://arxiv.org/abs/2510.07834)
*Lingjun Liu,Feiran Qin,Owolabi Legunsen,Marcelo d'Amorim*

Main category: cs.SE

TL;DR: IssueMut：首个从编译器bug历史中提取变异算子的方法，通过分析bug报告中的程序元素来指导模糊测试发现类似bug，在GCC和LLVM中发现了65个新bug


<details>
  <summary>Details</summary>
Motivation: 编译器bug影响巨大，现有变异模糊测试器的效果取决于变异算子质量，但之前没有工作利用编译器bug历史作为变异算子来源

Method: 自动化从bug报告中挖掘变异算子，并将这些算子集成到现有变异编译器模糊测试器中

Result: 从1760个GCC和LLVM bug报告中挖掘587个变异算子，发现了28个GCC新bug和37个LLVM新bug，其中60个被确认或修复

Conclusion: bug历史包含丰富信息，编译器模糊测试器应该利用这些信息来发现更多bug

Abstract: Bugs in compilers, which are critical infrastructure today, can have outsized
negative impacts. Mutational fuzzers aid compiler bug detection by
systematically mutating compiler inputs, i.e., programs. Their effectiveness
depends on the quality of the mutators used. Yet, no prior work used compiler
bug histories as a source of mutators. We propose IssueMut, the first approach
for extracting compiler fuzzing mutators from bug histories. Our insight is
that bug reports contain hints about program elements that induced compiler
bugs; they can guide fuzzers towards similar bugs. IssueMut uses an automated
method to mine mutators from bug reports and retrofit such mutators into
existing mutational compiler fuzzers. Using IssueMut, we mine 587 mutators from
1760 GCC and LLVM bug reports. Then, we run IssueMut on these compilers, with
all their test inputs as seed corpora. We find that "bug history" mutators are
effective: they find new bugs that a state-of-the-art mutational compiler
fuzzer misses-28 in GCC and 37 in LLVM. Of these, 60 were confirmed or fixed,
validating our idea that bug histories have rich information that compiler
fuzzers should leverage.

</details>


### [7] [An AUTOSAR-Aligned Architectural Study of Vulnerabilities in Automotive SoC Software](https://arxiv.org/abs/2510.07941)
*Srijita Basu,Haraldsson Bengt,Miroslaw Staron,Christian Berger,Jennifer Horkoff,Magnus Almgren*

Main category: cs.SE

TL;DR: 该研究分析了180个公开报告的汽车SoC漏洞，识别了16个根本原因和56个受影响软件模块，揭示了主要漏洞模式和关键模块的补丁延迟问题，为汽车CPS平台安全提供了改进检测、优先级排序和定位策略的指导。


<details>
  <summary>Details</summary>
Motivation: 汽车SoC软件架构在实时安全关键环境中存在安全挑战，且缺乏对AUTOSAR对齐架构中SoC漏洞根本原因和影响的系统性分析。

Method: 分析180个公开报告的汽车SoC漏洞，映射到符合AUTOSAR原则的代表性SoC软件架构模型，识别根本原因和受影响模块，检查CWE类别和架构层的缓解延迟。

Result: 识别了16个根本原因和56个受影响软件模块，揭示了主导漏洞模式和补丁延迟较长的关键模块。

Conclusion: 为基于SoC的车辆平台SoC软件架构提供了改进检测、优先级排序和定位策略的可操作见解，有助于提升汽车CPS平台的安全性。

Abstract: Cooperative, Connected and Automated Mobility (CCAM) are complex
cyber-physical systems (CPS) that integrate computation, communication, and
control in safety-critical environments. At their core, System-on-Chip (SoC)
platforms consolidate processing units, communication interfaces, AI
accelerators, and security modules into a single chip. AUTOSAR (AUTomotive Open
System ARchitecture) standard was developed in the automotive domain to better
manage this complexity, defining layered software structures and interfaces to
facilitate reuse of HW/SW components. However, in practice, this integrated SoC
software architecture still poses security challenges, particularly in
real-time, safety-critical environments. Recent reports highlight a surge in
SoC-related vulnerabilities, yet systematic analysis of their root causes and
impact within AUTOSAR-aligned architectures is lacking. This study fills that
gap by analyzing 180 publicly reported automotive SoC vulnerabilities, mapped
to a representative SoC software architecture model that is aligned with
AUTOSAR principles for layered abstraction and service orientation. We identify
16 root causes and 56 affected software modules, and examine mitigation delays
across Common Weakness Enumeration (CWE) categories and architectural layers.
We uncover dominant vulnerability patterns and critical modules with prolonged
patch delays, and provide actionable insights for securing automotive CPS
platforms, including guides for improved detection, prioritization, and
localization strategies for SoC software architectures in SoC-based vehicle
platforms.

</details>


### [8] [Past, Present, and Future of Bug Tracking in the Generative AI Era](https://arxiv.org/abs/2510.08005)
*Utku Boran Torun,Mehmet Taha Demircan,Mahmut Furkan Gön,Eray Tüzün*

Main category: cs.SE

TL;DR: 提出基于大语言模型的AI驱动bug追踪框架，通过智能自动化减少修复时间和人工开销，改善传统bug追踪系统的效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统bug追踪系统依赖人工报告、复现、分类和解决，涉及多方协作，沟通成本高且响应延迟，导致修复周期长。

Method: 用户用自然语言报告问题，AI代理完善报告、尝试复现、请求补充信息，自动分类报告，无效报告通过无代码修复解决，有效报告定位并分配给开发者，LLM生成候选补丁，人工监督确保正确性。

Result: 框架在每个阶段集成自动化，加速响应时间，改善协作，增强软件维护实践。

Conclusion: AI驱动的bug追踪框架通过智能自动化实现更高效、以用户为中心的未来软件维护。

Abstract: Traditional bug tracking systems rely heavily on manual reporting,
reproduction, triaging, and resolution, each carried out by different
stakeholders such as end users, customer support, developers, and testers. This
division of responsibilities requires significant coordination and widens the
communication gap between non-technical users and technical teams, slowing the
process from bug discovery to resolution. Moreover, current systems are highly
asynchronous; users often wait hours or days for a first response, delaying
fixes and contributing to frustration. This paper examines the evolution of bug
tracking, from early paper-based reporting to today's web-based and SaaS
platforms. Building on this trajectory, we propose an AI-powered bug tracking
framework that augments existing tools with intelligent, large language model
(LLM)-driven automation. Our framework addresses two main challenges: reducing
time-to-fix and minimizing human overhead. Users report issues in natural
language, while AI agents refine reports, attempt reproduction, and request
missing details. Reports are then classified, invalid ones resolved through
no-code fixes, and valid ones localized and assigned to developers. LLMs also
generate candidate patches, with human oversight ensuring correctness. By
integrating automation into each phase, our framework accelerates response
times, improves collaboration, and strengthens software maintenance practices
for a more efficient, user-centric future.

</details>


### [9] [Building Whitespace-Sensitive Languages Using Whitespace-Insensitive Components](https://arxiv.org/abs/2510.08200)
*Alexander Hellwig,Nico Jansen,Bernhard Rumpe*

Main category: cs.SE

TL;DR: 提出了一种通过预处理语言构件来构建空格敏感语言的技术，解决了空格敏感和不敏感语言组件之间的集成差距问题。


<details>
  <summary>Details</summary>
Motivation: 软件语言工程中模块化语言组件的可重用性受到空格敏感与不敏感语言集成差距的限制，导致库无法重用且空格敏感语言需要从头开发。

Method: 通过预处理语言构件，使用模块化的空格不敏感语言模块来构建空格敏感语言。

Result: 通过重构简化版Python语言验证了该方法的有效性。

Conclusion: 该解决方案提高了现有语言组件的可重用性，减少了开发时间并提升了软件语言的整体质量。

Abstract: In Software Language Engineering, there is a trend towards reusability by
composing modular language components. However, this reusability is severely
inhibited by a gap in integrating whitespace-sensitive and
whitespace-insensitive languages. There is currently no consistent procedure
for seamlessly reusing such language components in both cases, such that
libraries often cannot be reused, and whitespacesensitive languages are
developed from scratch. This paper presents a technique for using modular,
whitespaceinsensitive language modules to construct whitespace sensitive
languages by pre-processing language artifacts before parsing. The approach is
evaluated by reconstructing a simplified version of the programming language
Python. Our solution aims to increase the reusability of existing language
components to reduce development time and increase the overall quality of
software languages.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [10] [TCDRM: A Tenant Budget-Aware Data Replication Framework for Multi-Cloud Computing](https://arxiv.org/abs/2510.07833)
*Santatra Hagamalala Bernardin,Riad Mokadem,Franck Morvan,Hasinarivo Ramanana,Hasimandimby Rakotoarivelo*

Main category: cs.DB

TL;DR: 提出了一种面向多云计算的租户预算感知数据复制框架TCDRM，该框架根据响应时间阈值、租户经济预算和数据流行度动态创建数据副本，在满足性能要求的同时不超出预算限制。


<details>
  <summary>Details</summary>
Motivation: 多云计算系统在确保可接受性能的同时遵守租户预算要求面临重大挑战，需要一种能够平衡性能和成本的智能数据复制策略。

Method: 采用启发式副本放置算法，利用多云提供商的不同定价结构，通过中间件在租户和云提供商之间进行智能副本放置决策，并定义严格的租户预算和响应时间阈值。

Result: 与无复制方法相比，带宽消耗减少高达78%，复杂查询的平均响应时间降低51%，同时遵守租户预算限制。

Conclusion: TCDRM策略有效满足了租户的性能目标，同时尊重其经济约束，证明了在多云环境中平衡性能和成本的可行性。

Abstract: Multi-cloud computing systems face significant challenges in ensuring
acceptable performance while adhering to tenant budget requirements. This paper
proposes a tenant budget-aware (tenant-centric) data replication framework for
Multi-Cloud Computing (TCDRM). The proposed strategy dynamically creates data
replicas based on predefined thresholds for response time, economic budget of
the tenant and data popularity. TCDRM employs a heuristic replica placement
algorithm that leverages the diverse pricing structures of multiple cloud
providers. The TCDRM strategy aims to maintain the required performance without
exceeding the tenant's budget by taking advantage of the capabilities offered
by multicloud environments. The middleware considered acts as an intermediary
between tenants and multiple cloud providers, facilitating intelligent replica
placement decisions. To achieve this, the proposed TCDRM strategy defines
strict thresholds for tenant budget and response time. A performance evaluation
is conducted to validate the effectiveness of the strategy. The results show
that our approach effectively meets tenant performance objectives while
respecting their economic constraints. Bandwidth consumption is reduced by up
to 78% compared to non-replicated approaches, and average response time for
complex queries is decreased by 51%, all while adhering to tenant budget
limitations.

</details>


### [11] [MobilityDuck: Mobility Data Management with DuckDB](https://arxiv.org/abs/2510.07963)
*Nhu Ngoc Hoang,Ngoc Hoa Pham,Viet Phuong Hoang,Esteban Zimányi*

Main category: cs.DB

TL;DR: MobilityDuck是一个基于DuckDB的轻量级时空数据分析扩展，通过集成MEOS库提供原生时空数据类型和连续轨迹操作符，相比MobilityDB具有更高的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的移动对象数据库系统要么过于复杂，要么不够轻量级，无法满足实际生产环境中的时空数据分析需求。

Method: 通过扩展DuckDB并集成MEOS库，在DuckDB的轻量级、列式、内存执行架构基础上提供时空数据支持。

Result: 在BerlinMOD-Hanoi基准数据集上的评估表明，MobilityDuck在保持时空查询表达能力的同时，受益于DuckDB的内存列式架构，性能优于MobilityDB。

Conclusion: MobilityDuck成功地将时空数据分析能力集成到轻量级的嵌入式分析系统中，为移动性分析提供了高效实用的解决方案。

Abstract: The analytics of spatiotemporal data is increasingly important for mobility
analytics. Despite extensive research on moving object databases (MODs), few
systems are ready on production or lightweight enough for analytics. MobilityDB
is a notable system that extends PostgreSQL with spatiotemporal data, but it
inherits complexity of the architecture as well. In this paper, we present
MobilityDuck, a DuckDB extension that integrates the MEOS library to provide
support spatiotemporal and other temporal data types in DuckDB. MobilityDuck
leverages DuckDB's lightweight, columnar, in-memory executable properties to
deliver efficient analytics. To the best of our knowledge, no existing
in-memory or embedded analytical system offers native spatiotemporal types and
continuous trajectory operators as MobilityDuck does. We evaluate MobilityDuck
using the BerlinMOD-Hanoi benchmark dataset and compare its performance to
MobilityDB. Our results show that MobilityDuck preserves the expressiveness of
spatiotemporal queries while benefiting from DuckDB's in-memory, columnar
architecture.

</details>


### [12] [ZeroCard: Cardinality Estimation with Zero Dependence on Target Databases -- No Data, No Query, No Retraining](https://arxiv.org/abs/2510.07983)
*Xianghong Xu,Rong Kang,Xiao He,Lei Zhang,Jianjun Chen,Tieying Zhang*

Main category: cs.DB

TL;DR: ZeroCard是首个基于语义的基数估计方法，无需访问原始数据、查询日志或在目标数据库上重新训练，通过模式语义预测数据分布，实现零依赖部署。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法严重依赖原始数据或查询，难以泛化到新数据集，限制了实际应用。利用模式语义可能缓解这些依赖问题。

Method: 使用模式语义预测数据分布以避免原始数据依赖；引入查询模板无关的表示方法减轻查询依赖；在大规模真实表查询数据集上预训练，从模式语义和谓词表示学习基数。

Result: 实验证明ZeroCard具有显著优势，其零依赖特性极大促进了实际部署。

Conclusion: ZeroCard通过语义驱动方法成功解决了基数估计中的数据依赖问题，为查询优化提供了实用的解决方案。

Abstract: Cardinality estimation is a fundamental task in database systems and plays a
critical role in query optimization. Despite significant advances in
learning-based cardinality estimation methods, most existing approaches remain
difficult to generalize to new datasets due to their strong dependence on raw
data or queries, thus limiting their practicality in real scenarios. To
overcome these challenges, we argue that semantics in the schema may benefit
cardinality estimation, and leveraging such semantics may alleviate these
dependencies. To this end, we introduce ZeroCard, the first semantics-driven
cardinality estimation method that can be applied without any dependence on raw
data access, query logs, or retraining on the target database. Specifically, we
propose to predict data distributions using schema semantics, thereby avoiding
raw data dependence. Then, we introduce a query template-agnostic
representation method to alleviate query dependence. Finally, we construct a
large-scale query dataset derived from real-world tables and pretrain ZeroCard
on it, enabling it to learn cardinality from schema semantics and predicate
representations. After pretraining, ZeroCard's parameters can be frozen and
applied in an off-the-shelf manner. We conduct extensive experiments to
demonstrate the distinct advantages of ZeroCard and show its practical
applications in query optimization. Its zero-dependence property significantly
facilitates deployment in real-world scenarios.

</details>


### [13] [Implementing Semantic Join Operators Efficiently](https://arxiv.org/abs/2510.08489)
*Immanuel Trummer*

Main category: cs.DB

TL;DR: 提出了一种基于块嵌套循环连接的语义连接算法，通过将批量行数据整合到单个提示中来优化LLM调用，显著降低了处理成本。


<details>
  <summary>Details</summary>
Motivation: 当前语义查询处理引擎通过嵌套循环实现语义连接，需要为每对行调用LLM，效率低下。本文旨在优化这一过程，减少LLM调用次数。

Method: 受传统数据库块嵌套循环连接启发，提出新算法将两个输入表的批量行整合到单个提示中，让LLM一次性识别所有匹配行对，并引入公式优化批次大小。

Result: 渐进处理成本分析和实证结果表明，该方法显著降低了成本，性能优于近期语义查询处理引擎使用的连接实现。

Conclusion: 提出的块式语义连接算法通过批量处理有效减少了LLM调用次数，在语义查询处理中实现了显著的成本优化。

Abstract: Semantic query processing engines often support semantic joins, enabling
users to match rows that satisfy conditions specified in natural language. Such
join conditions can be evaluated using large language models (LLMs) that solve
novel tasks without task-specific training.
  Currently, many semantic query processing engines implement semantic joins
via nested loops, invoking the LLM to evaluate the join condition on row pairs.
Instead, this paper proposes a novel algorithm, inspired by the block nested
loops join operator implementation in traditional database systems. The
proposed algorithm integrates batches of rows from both input tables into a
single prompt. The goal of the LLM invocation is to identify all matching row
pairs in the current input. The paper introduces formulas that can be used to
optimize the size of the row batches, taking into account constraints on the
size of the LLM context window (limiting both input and output size). An
adaptive variant of the proposed algorithm refers to cases in which the size of
the output is difficult to estimate. A formal analysis of asymptotic processing
costs, as well as empirical results, demonstrates that the proposed approach
reduces costs significantly and performs well compared to join implementations
used by recent semantic query processing engines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [Adaptive Execution Scheduler for DataDios SmartDiff](https://arxiv.org/abs/2510.07811)
*Aryan Poduri*

Main category: cs.DC

TL;DR: 提出了一种自适应调度器SmartDiff，通过动态调整批处理大小和线程/工作器数量，在固定CPU和内存预算下最小化p95延迟。


<details>
  <summary>Details</summary>
Motivation: 需要优化单差分引擎的性能，在资源受限环境下降低延迟并避免内存溢出。

Method: 使用轻量级预分析器估计字节/行和I/O速率；在线成本/内存模型修剪不安全操作；采用带保护的爬山策略优化延迟，包含背压和慢任务缓解机制；根据工作集估计选择执行后端。

Result: 在合成和公共表格基准测试中，相比调优的预热启发式方法，p95延迟降低23-28%；相比固定网格基线降低35-40%；峰值内存降低16-22%（相比固定基线降低25-32%），零内存溢出且吞吐量相当。

Conclusion: 该自适应调度器能有效优化差分引擎性能，在资源受限环境下显著降低延迟和内存使用，同时保持系统稳定性。

Abstract: We present an adaptive scheduler for a single differencing engine (SmartDiff)
with two execution modes: (i) in-memory threads and (ii) Dask based
parallelism. The scheduler continuously tunes batch size and worker/thread
count within fixed CPU and memory budgets to minimize p95 latency. A
lightweight preflight profiler estimates bytes/row and I/O rate; an online
cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors
lower latency with backpressure and straggler mitigation. Backend selection is
gated by a conservative working-set estimate so that in-memory execution is
chosen when safe, otherwise Dask is used. Across synthetic and public tabular
benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a
tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines),
while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed)
with zero OOMs and comparable throughput.

</details>


### [15] [A Multi-Simulation Bridge for IoT Digital Twins](https://arxiv.org/abs/2510.08164)
*Marco Picone,Samuele Burattini,Marco Melloni,Prasad Talasila,Davide Ziglioli,Matteo Martinelli,Nicola Bicocchi,Alessandro Ricci,Peter Gorm Larsen*

Main category: cs.DC

TL;DR: 本文提出了DT Simulation Bridge框架，支持数字孪生与仿真平台之间的双向交互，增强工业物联网系统的设计和实时操作能力。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生在物联网和工业物联网中能力的增强，需要与仿真平台无缝集成以支持系统设计、验证和实时操作。

Method: 设计了DT Simulation Bridge软件框架，通过双向数据交换实现数字孪生与仿真环境之间的动态交互，支持灵活的互操作性和可扩展部署。

Result: 实验结果表明，该框架提高了设计敏捷性，促进了虚拟调试，并在真实条件下支持实时行为分析，在多种工业场景中表现出有效性。

Conclusion: DT Simulation Bridge框架成功实现了数字孪生与仿真平台的有效集成，为工业物联网系统的全生命周期管理提供了有力支持。

Abstract: The increasing capabilities of Digital Twins (DTs) in the context of the
Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless
integration with simulation platforms to support system design, validation, and
real-time operation. This paper introduces the concept, design, and
experimental evaluation of the DT Simulation Bridge - a software framework that
enables diverse interaction patterns between active DTs and simulation
environments. The framework supports both the DT development lifecycle and the
incorporation of simulations during active operation. Through bidirectional
data exchange, simulations can update DT models dynamically, while DTs provide
real-time feedback to adapt simulation parameters. We describe the
architectural design and core software components that ensure flexible
interoperability and scalable deployment. Experimental results show that the DT
Simulation Bridge enhances design agility, facilitates virtual commissioning,
and supports live behavioral analysis under realistic conditions, demonstrating
its effectiveness across a range of industrial scenarios.

</details>


### [16] [Towards Energy-Efficient Serverless Computing with Hardware Isolation](https://arxiv.org/abs/2510.08180)
*Natalie Carl,Tobias Pfandzelter,David Bermbach*

Main category: cs.DC

TL;DR: 论文提出重新设计无服务器计算的硬件架构，使用硬件隔离而非软件隔离来运行无服务器函数，从而显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器平台在传统服务器硬件上运行大量函数，需要昂贵的软件隔离机制和高度的过度配置，导致能源效率低下。

Method: 采用硬件隔离方法，为每个函数分配独立处理器，构建只在应用实际工作时消耗能源的无服务器硬件堆栈。

Result: 初步评估显示，该方法可将能耗开销减少90.63%，平均节省70.8MW。

Conclusion: 重新设计无服务器硬件架构以匹配软件需求，能够显著提升能源效率，实现真正的按需能耗。

Abstract: Serverless computing provides just-in-time infrastructure provisioning with
rapid elasticity and a finely-grained pricing model. As full control of
resource allocation is in the hands of the cloud provider and applications only
consume resources when they actually perform work, we believe that serverless
computing is uniquely positioned to maximize energy efficiency.
  However, the focus of current serverless platforms is to run hundreds or
thousands of serverless functions from different tenants on traditional server
hardware, requiring expensive software isolation mechanisms and a high degree
of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared
caches, high clock frequencies, and many-core architectures, servers today are
optimized for large, singular workloads but not to run thousands of isolated
functions.
  We propose rethinking the serverless hardware architecture to align it with
the requirements of serverless software. Specifically, we propose using
hardware isolation with individual processors per function instead of software
isolation resulting in a serverless hardware stack that consumes energy only
when an application actually performs work. In preliminary evaluation with real
hardware and a typical serverless workload we find that this could reduce
energy consumption overheads by 90.63% or an average 70.8MW.

</details>


### [17] [Distributed Resource Selection for Self-Organising Cloud-Edge Systems](https://arxiv.org/abs/2510.08228)
*Quentin Renau,Amjad Ullah,Emma Hart*

Main category: cs.DC

TL;DR: 提出了一种分布式资源选择机制，用于云边环境中的动态资源分配，通过分布式决策避免集中式协调瓶颈，实现高效、可扩展和弹性的资源管理。


<details>
  <summary>Details</summary>
Motivation: 在高度动态的云边环境中，集中式协调成为瓶颈，需要分布式决策机制来满足复杂分布式应用的需求。

Method: 采用基于共识的机制，利用本地知识和代理间协作，不依赖中央控制器，实现分布式编排。

Result: 计算时间是影响分配决策的关键因素，该方法在保持最优性的同时实现快速分配，比集中式启发式方法快达30倍。

Conclusion: 该机制为分布式编排铺平了道路，在大规模场景下能够及时提供结果，而穷举搜索不可行。

Abstract: This paper presents a distributed resource selection mechanism for diverse
cloud-edge environments, enabling dynamic and context-aware allocation of
resources to meet the demands of complex distributed applications. By
distributing the decision-making process, our approach ensures efficiency,
scalability, and resilience in highly dynamic cloud-edge environments where
centralised coordination becomes a bottleneck. The proposed mechanism aims to
function as a core component of a broader, distributed, and self-organising
orchestration system that facilitates the intelligent placement and adaptation
of applications in real-time. This work leverages a consensus-based mechanism
utilising local knowledge and inter-agent collaboration to achieve efficient
results without relying on a central controller, thus paving the way for
distributed orchestration. Our results indicate that computation time is the
key factor influencing allocation decisions. Our approach consistently delivers
rapid allocations without compromising optimality or incurring additional cost,
achieving timely results at scale where exhaustive search is infeasible and
centralised heuristics run up to 30 times slower.

</details>


### [18] [Energy-Efficient Maximal Independent Sets in Radio Networks](https://arxiv.org/abs/2510.08244)
*Dominick Banasik,Varsha Dani,Fabien Dufoulon,Aayush Gupta,Thomas P. Hayes,Gopal Pandurangan*

Main category: cs.DC

TL;DR: 本文提出了在无线网络模型中更节能的最大独立集（MIS）分布式算法，针对有碰撞检测和无碰撞检测两种模型，分别实现了O(log n)和O(log²n log log n)的能量复杂度。


<details>
  <summary>Details</summary>
Motivation: 无线网络通常由电池供电，能量是宝贵资源。设计能量消耗尽可能少的分布式算法对延长网络寿命至关重要。

Method: 使用随机化分布式算法，在无线网络模型中考虑节点睡眠和唤醒状态，仅计算唤醒轮次作为能量复杂度。针对有碰撞检测和无碰撞检测两种模型分别设计算法。

Result: 1. 有碰撞检测模型：能量复杂度O(log n)，轮复杂度O(log² n)，失败概率1/poly(n)，能量复杂度被证明是最优的。2. 无碰撞检测模型：能量复杂度O(log²n log log n)，轮复杂度O(log³ n log Δ)，显著优于现有最佳算法的O(log³ n)复杂度。

Conclusion: 提出的算法在无线网络模型中实现了显著的能量效率改进，特别是在有碰撞检测模型中达到了最优能量复杂度，为能量受限的无线网络提供了实用的MIS解决方案。

Abstract: The maximal independent set (MIS) is one of the most fundamental problems in
distributed computing, and it has been studied intensively for over four
decades. This paper focuses on the MIS problem in the Radio Network model, a
standard model widely used to model wireless networks, particularly ad hoc
wireless and sensor networks. Energy is a premium resource in these networks,
which are typically battery-powered. Hence, designing distributed algorithms
that use as little energy as possible is crucial. We use the well-established
energy model where a node can be sleeping or awake in a round, and only the
awake rounds (when it can send or listen) determine the energy complexity of
the algorithm, which we want to minimize.
  We present new, more energy-efficient MIS algorithms in radio networks with
arbitrary and unknown graph topology. We present algorithms for two popular
variants of the radio model -- with collision detection (CD) and without
collision detection (no-CD). Specifically, we obtain the following results:
  1. CD model: We present a randomized distributed MIS algorithm with energy
complexity $O(\log n)$, round complexity $O(\log^2 n)$, and failure probability
$1 / poly(n)$, where $n$ is the network size. We show that our energy
complexity is optimal by showing a matching $\Omega(\log n)$ lower bound.
  2. no-CD model: In the more challenging no-CD model, we present a randomized
distributed MIS algorithm with energy complexity $O(\log^2n \log \log n)$,
round complexity $O(\log^3 n \log \Delta)$, and failure probability $1 /
poly(n)$. The energy complexity of our algorithm is significantly lower than
the round (and energy) complexity of $O(\log^3 n)$ of the best known
distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.

</details>


### [19] [Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver](https://arxiv.org/abs/2510.08536)
*Gregor Olenik,Marcel Koch,Hartwig Anzt*

Main category: cs.DC

TL;DR: 提出了一种重新分区策略来改进OpenFOAM中基于插件的GPU加速方法，通过平衡CPU矩阵组装和GPU线性求解来缓解资源过度订阅问题


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算越来越依赖GPU，但将GPU加速集成到复杂科学框架如OpenFOAM中仍然具有挑战性。现有方法要么完全重构代码库，要么使用基于插件的GPU求解器，在性能和开发工作量之间存在权衡

Method: 提出了重新分区策略，包括详细的计算模型、新颖的矩阵重新分区和更新过程，以更好地平衡CPU矩阵组装和基于GPU的线性求解

Result: 在大规模CFD模拟中评估性能，结果显示该方法显著缓解了过度订阅问题，提高了异构CPU-GPU环境中的求解器性能和资源利用率

Conclusion: 所提出的重新分区方法有效改进了OpenFOAM中基于插件的GPU加速，为异构计算环境提供了更好的性能平衡

Abstract: Modern high-performance computing (HPC) increasingly relies on GPUs, but
integrating GPU acceleration into complex scientific frameworks like OpenFOAM
remains a challenge. Existing approaches either fully refactor the codebase or
use plugin-based GPU solvers, each facing trade-offs between performance and
development effort. In this work, we address the limitations of plugin-based
GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better
balances CPU matrix assembly and GPU-based linear solves. We present a detailed
computational model, describe a novel matrix repartitioning and update
procedure, and evaluate its performance on large-scale CFD simulations. Our
results show that the proposed method significantly mitigates oversubscription
issues, improving solver performance and resource utilization in heterogeneous
CPU-GPU environments.

</details>
