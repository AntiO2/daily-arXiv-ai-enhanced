<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A Construction-Phase Digital Twin Framework for Quality Assurance and Decision Support in Civil Infrastructure Projects](https://arxiv.org/abs/2602.16748)
*Md Asiful Islam,Shanto Jouerder,Md Sabit As Sami,Afia Jahin Prema*

Main category: cs.SE

TL;DR: 提出一个施工阶段数字孪生框架，通过整合检查记录、材料数据、早期传感和强度预测模型，支持元素级质量保证和基于准备度的决策制定，实现从延迟文档审查向主动决策支持的转变。


<details>
  <summary>Details</summary>
Motivation: 传统施工质量保证依赖检查记录和实验室测试结果，这些信息通常在施工完成后数天或数周才能获得。在大型公路和桥梁项目中，这种延迟限制了早期干预，增加了返工风险、进度影响和文档碎片化问题。

Method: 开发一个施工阶段数字孪生框架，将检查记录、材料生产和放置数据、早期传感数据以及预测强度模型与单个施工元素相连接。通过整合这些数据流，系统能够表示每个元素不断变化的质量状态，并在标准年龄测试结果可用之前支持结构化的释放或保留决策。

Result: 该框架为施工质量保证提供了结构化路径，从延迟的文档驱动审查转向主动的、元素级的决策支持。它不取代既定的检查和测试程序，而是通过提高可追溯性和实现更早的数据驱动质量评估来补充现有工作流程。

Conclusion: 提出的数字孪生框架能够支持施工期间的元素级质量保证和基于准备度的决策制定。该框架考虑了数据集成、合同约束和实施挑战等实际问题，为改善施工质量管理提供了可行方案。

Abstract: Quality assurance (QA) during construction often relies on inspection records and laboratory test results that become available days or weeks after work is completed. On large highway and bridge projects, this delay limits early intervention and increases the risk of rework, schedule impacts, and fragmented documentation. This study presents a construction-phase digital twin framework designed to support element-level QA and readiness-based decision making during active construction. The framework links inspection records, material production and placement data, early-age sensing, and predictive strength models to individual construction elements. By integrating these data streams, the system represents the evolving quality state of each element and supports structured release or hold decisions before standard-age test results are available. The approach does not replace established inspection and testing procedures. Instead, it supplements existing workflows by improving traceability and enabling earlier, data-informed quality assessments. Practical considerations related to data integration, contractual constraints, and implementation challenges are also discussed. The proposed framework provides a structured pathway for transitioning construction QA from delayed, document-driven review toward proactive, element-level decision support during construction.

</details>


### [2] [Hybrid-Gym: Training Coding Agents to Generalize Across Tasks](https://arxiv.org/abs/2602.16819)
*Yiqing Xie,Emmy Liu,Gaokai Zhang,Nachiket Kotalwar,Shubham Gandhi,Sathwik Acharya,Xingyao Wang,Carolyn Rose,Graham Neubig,Daniel Fried*

Main category: cs.SE

TL;DR: 论文提出Hybrid-Gym训练环境，通过合成任务教授语言模型代码探索、测试、架构设计等可迁移技能，显著提升在真实代码任务上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如SWE-Bench）主要关注解决GitHub上的单一问题，而实际使用中代码代理需要处理更复杂多样的任务，包括代码库探索、软件测试和架构设计等技能。需要开发能够教授这些可迁移技能的训练方法。

Method: 通过分解任务轨迹为细粒度组件，识别可迁移技能，并设计辅助训练任务的原则。基于这些原则构建Hybrid-Gym训练环境，包含函数定位、依赖搜索等可扩展的合成任务。

Result: 在Hybrid-Gym上训练的代理能有效泛化到未见过的真实世界任务：在SWE-Bench Verified上相对基础模型提升25.4%，在SWT-Bench Verified上提升7.9%，在Commit-0 Lite上提升5.1%。还能补充下游任务数据集（如将SWE-Play在SWT-Bench Verified上的表现提升4.9%）。

Conclusion: 通过精心设计的合成任务训练语言模型学习可迁移技能，能够显著提升其在多样化真实代码任务上的泛化能力，Hybrid-Gym为代码代理训练提供了有效的补充方法。

Abstract: When assessing the quality of coding agents, predominant benchmarks focus on solving single issues on GitHub, such as SWE-Bench. In contrast, in real use, these agents solve more various and complex tasks that involve other skills such as exploring codebases, testing software, and designing architecture. In this paper, we first characterize some transferable skills that are shared across diverse tasks by decomposing trajectories into fine-grained components, and derive a set of principles for designing auxiliary training tasks to teach language models these skills. Guided by these principles, we propose a training environment, Hybrid-Gym, consisting of a set of scalable synthetic tasks, such as function localization and dependency search. Experiments show that agents trained on our synthetic tasks effectively generalize to diverse real-world tasks that are not present in training, improving a base model by 25.4% absolute gain on SWE-Bench Verified, 7.9% on SWT-Bench Verified, and 5.1% on Commit-0 Lite. Hybrid-Gym also complements datasets built for the downstream tasks (e.g., improving SWE-Play by 4.9% on SWT-Bench Verified). Code available at: https://github.com/yiqingxyq/Hybrid-Gym.

</details>


### [3] [Exploring LLMs for User Story Extraction from Mockups](https://arxiv.org/abs/2602.16997)
*Diego Firmenich,Leandro Antonelli,Bruno Pazos,Fabricio Lozada,Leonardo Morales*

Main category: cs.SE

TL;DR: LLMs结合高保真原型和语言扩展词典能自动生成高质量用户故事，提升需求工程效率


<details>
  <summary>Details</summary>
Motivation: 探索如何结合用户故事、高保真原型和大型语言模型，实现敏捷、自动化的用户故事生成，改善用户与开发者之间的沟通

Method: 通过案例研究分析LLMs从高保真原型提取用户故事的能力，比较是否包含语言扩展词典（LEL）词汇表对生成结果的影响

Result: 包含LEL词汇表显著提高了生成用户故事的准确性和适用性，证明该方法在需求工程中具有实际应用价值

Conclusion: 结合LLMs、高保真原型和LEL的方法为AI集成到需求工程迈出重要一步，有望改善用户与开发者的沟通效率

Abstract: User stories are one of the most widely used artifacts in the software industry to define functional requirements. In parallel, the use of high-fidelity mockups facilitates end-user participation in defining their needs. In this work, we explore how combining these techniques with large language models (LLMs) enables agile and automated generation of user stories from mockups. To this end, we present a case study that analyzes the ability of LLMs to extract user stories from high-fidelity mockups, both with and without the inclusion of a glossary of the Language Extended Lexicon (LEL) in the prompts. Our results demonstrate that incorporating the LEL significantly enhances the accuracy and suitability of the generated user stories. This approach represents a step forward in the integration of AI into requirements engineering, with the potential to improve communication between users and developers.

</details>


### [4] [Not Only for Developers: Exploring Plugin Maintenance for Knowledge-Centric Communities](https://arxiv.org/abs/2602.17018)
*Giovanni Rosa,David Moreno-Lumbreras,Raula Gaikovina Kula*

Main category: cs.SE

TL;DR: 研究Obsidian这个非开发者为中心的知识管理平台的插件生态系统，发现尽管社区以写作者和创意工作者为主，但仍能形成具有工程结构的插件生态，识别出六大主题类别，并观察到活跃的软件演化活动。


<details>
  <summary>Details</summary>
Motivation: 现代软件开发中第三方库生态系统（如PyPI、NPM、Maven）主要由开发者维护，但在非开发者为主的社区（如知识管理平台）中，插件生态系统的维护面临不同挑战。本研究旨在探索Obsidian这个以写作、组织和创意为中心的非开发者平台如何建立和维护其插件生态系统。

Method: 采用仓库挖掘和基于LLM的主题建模方法，对396个代表性插件样本进行分析，识别插件主题类别。同时分析这些插件的Pull Requests来了解软件演化情况。

Result: 识别出六大插件主题：(1)动态编辑与组织，(2)界面与布局，(3)创意写作与生产力，(4)知识同步解决方案，(5)链接与脚本工具，(6)工作流增强工具。Pull Requests分析显示这些生态系统中有大量的软件演化活动。

Conclusion: 即使在混合社区中，插件生态系统也能发展出可识别的工程结构。这为未来研究非开发者生态系统的健康性和可持续性提供了基础，提出了三个研究方向和相关研究问题。

Abstract: The adoption of third-party libraries has become integral to modern software development, leading to large ecosystems such as PyPI, NPM, and Maven, where contributors typically share the technical expertise to sustain extensions. In communities that are not exclusively composed of developers, however, maintaining plugin ecosystems can present different challenges. In this early results paper, we study Obsidian, a knowledge--centric platform whose community is focused on writing, organization, and creativity--has built a substantial plugin ecosystem despite not being developer--centric. We investigate what kinds of plugins exist within this hybrid ecosystem and establish a foundation for understanding how they are maintained. Using repository mining and LLM-based topic modeling on a representative sample of 396 plugins, we identify six topics related to knowledge management and tooling, which is (i) dynamic editing and organization, (ii) interface and layouts, (iii) creative writing and productivity, (iv) knowledge sync solutions, (v) linking and script tools, and (vi) workflow enhancements tools. Furthermore, analysis of the Pull Requests from these plugins show that much software evolution has been performed on these ecosystem. These findings suggest that even in mixed communities, plugin ecosystems can develop recognizable engineering structures, motivating future work that highlight three different research directions with six research questions related to the health and sustainability of these non-developer ecosystems.

</details>


### [5] [Wink: Recovering from Misbehaviors in Coding Agents](https://arxiv.org/abs/2602.17037)
*Rahul Nanda,Chandra Maddila,Smriti Jha,Euna Mehnaz Khan,Matteo Paltenghi,Satish Chandra*

Main category: cs.SE

TL;DR: Wink是一个轻量级异步自干预系统，用于自动恢复自主编码代理的异常行为，成功解决了90%需要单次干预的异常问题。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的自主编码代理在软件行业中被广泛采用，但这些代理容易出现各种异常行为，如偏离用户指令、陷入重复循环或工具使用错误，这些故障会中断开发流程并需要大量人工干预。

Method: 首先通过对生产流量分析建立异常行为分类法，识别出三类主要异常：规范漂移、推理问题和工具调用失败。然后开发了名为Wink的轻量级异步自干预系统，该系统观察代理轨迹并提供有针对性的纠偏指导，引导代理回到正确路径。

Result: 在超过10,000个真实世界代理轨迹上评估，系统成功解决了90%需要单次干预的异常行为。生产环境中的实时A/B测试显示，系统显著减少了工具调用失败、会话令牌数和工程师干预次数。

Conclusion: Wink系统有效解决了自主编码代理的异常行为问题，提供了构建大规模弹性代理系统的实践经验，展示了在真实生产环境中部署自干预系统的可行性。

Abstract: Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user's instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.

</details>


### [6] [What to Cut? Predicting Unnecessary Methods in Agentic Code Generation](https://arxiv.org/abs/2602.17091)
*Kan Watanabe,Tatsuya Shirai,Yutaro Kashiwa,Hajimu Iida*

Main category: cs.SE

TL;DR: 提出预测模型识别PR审查中可能被删除的函数，帮助评审者优先处理重要代码


<details>
  <summary>Details</summary>
Motivation: AI辅助编程工具（如GitHub Copilot）加速代码生成，但增加了PR代码量，将负担从实现者转移到评审者。大量AI生成的代码最终在审查中被删除，但评审者仍需检查这些代码才能决定是否删除。目前没有研究帮助评审者高效识别将被删除的代码。

Method: 提出预测模型来识别在PR审查中可能被删除的函数。研究发现因不同原因被删除的函数具有不同的特征。

Result: 模型实现了87.1%的AUC，表明预测方法能有效帮助评审者优先处理重要代码。

Conclusion: 预测模型能有效识别可能被删除的代码，帮助评审者提高审查效率，将精力集中在真正重要的代码上。

Abstract: Agentic Coding, powered by autonomous agents such as GitHub Copilot and Cursor, enables developers to generate code, tests, and pull requests from natural language instructions alone. While this accelerates implementation, it produces larger volumes of code per pull request, shifting the burden from implementers to reviewers. In practice, a notable portion of AI-generated code is eventually deleted during review, yet reviewers must still examine such code before deciding to remove it. No prior work has explored methods to help reviewers efficiently identify code that will be removed.In this paper, we propose a prediction model that identifies functions likely to be deleted during PR review. Our results show that functions deleted for different reasons exhibit distinct characteristics, and our model achieves an AUC of 87.1%. These findings suggest that predictive approaches can help reviewers prioritize their efforts on essential code.

</details>


### [7] [Multi-Ecosystem Modeling of OSS Project Sustainability](https://arxiv.org/abs/2602.17112)
*Arjun Ashok,Nafiz Imtiaz Khan,Swati Singhvi,Stefan Stanciulescu,Zhouhao Wang,Vladimir Filkov*

Main category: cs.SE

TL;DR: 基于Apache、Eclipse、OSGeo基金会孵化项目及GitHub项目的实证研究，开发了基于社会技术追踪特征的项目可持续性模型和分类方法，可跨基金会预测项目可持续性。


<details>
  <summary>Details</summary>
Motivation: 开源项目加入基金会（如Apache、Eclipse、OSGeo）可获得治理建议、孵化支持和社区建设机制，但不同基金会的政策、资助模式和支持策略各异。项目处于不同生命周期阶段，需求不同，难以确定合适的项目-基金会匹配和可持续发展计划。

Method: 对Apache、Eclipse、OSGeo基金会孵化项目及GitHub非基金会项目进行实证研究和定量分析，基于项目的社会技术追踪特征开发基金会特定的可持续性模型和项目分类方法。

Result: 开发的模型和分类方法不仅能有效预测基金会内项目的可持续性结果，还能跨基金会进行预测。该框架具有普适性，可应用于GitHub非基金会项目。结合先前工作的可操作恢复策略，应用于失败孵化项目的案例研究。

Conclusion: 研究强调了社会技术框架在表征和解决软件项目可持续性问题方面的价值，为项目选择合适基金会和制定可持续发展计划提供了量化工具。

Abstract: Many OSS projects join foundations such as Apache, Eclipse, and OSGeo, to aid their immediate plans and improve long-term prospects by getting governance advice, incubation support, and community-building mechanisms. But foundations differ in their policies, funding models, and support strategies. Moreover, since projects joining these foundations are diverse, coming at different lifecycle stages and having different needs, it can be challenging to decide on the appropriate project-foundation match and on the project-specific plan for sustainability.
  Here, we present an empirical study and quantitative analysis of the sustainability of incubator projects in the Apache, Eclipse, and OSGeo foundations, and, additionally, of OSS projects from GitHub outside of foundations. We develop foundation-specific sustainability models and a project triage, based on projects' sociotechnical trace profiles, and demonstrate their effectiveness across the foundations. Our results show that our models with triage can effectively forecast sustainability outcomes not only within but across foundations. In addition, the generalizability of the framework allows us to apply the approach to GitHub projects outside the foundations. We complement our findings with actionable recovery strategies from previous work and apply them to case studies of failed incubator projects. Our study highlights the value of sociotechnical frameworks in characterizing and addressing software project sustainability issues.

</details>


### [8] [Quantifying Competitive Relationships Among Open-Source Software Projects](https://arxiv.org/abs/2602.17131)
*Yuki Takei,Toshiaki Aoki,Chaiyong Ragkhitwetsagul*

Main category: cs.SE

TL;DR: 提出MIAO方法，使用结构向量自回归模型和脉冲响应函数量化开源软件项目间的竞争关系，能预测项目停止开发


<details>
  <summary>Details</summary>
Motivation: 开源软件在快速演进的领域（如Web开发和深度学习）中竞争加剧，但项目间竞争关系对生存的影响尚不明确，存在被竞争对手超越的风险

Method: 提出MIAO方法，采用宏观经济分析中常用的结构向量自回归模型和脉冲响应函数来分析OSS项目间的相互作用

Result: 对187个OSS项目组进行实证分析，MIAO识别因竞争影响而停止开发的项目准确率达81%，特征支持提前一年预测项目停止的准确率达77%

Conclusion: MIAO可作为OSS项目维护者理解生态系统动态、预测项目兴衰的宝贵工具

Abstract: Throughout the history of software, evolution has occurred in cycles of rise and fall driven by competition, and open-source software (OSS) is no exception. This cycle is accelerating, particularly in rapidly evolving domains such as web development and deep learning. However, the impact of competitive relationships among OSS projects on their survival remains unclear, and there are risks of losing a competitive edge to rivals. To address this, this study proposes a new automated method called ``Mutual Impact Analysis of OSS (MIAO)'' to quantify these competitive relationships. The proposed method employs a structural vector autoregressive model and impulse response functions, normally used in macroeconomic analysis, to analyze the interactions among OSS projects. In an empirical analysis involving mining and analyzing 187 OSS project groups, MIAO identified projects that were forced to cease development owing to competitive influences with up to 81\% accuracy, and the resulting features supported predictive experiments that anticipate cessation one year ahead with up to 77\% accuracy. This suggests that MIAO could be a valuable tool for OSS project maintainers to understand the dynamics of OSS ecosystems and predict the rise and fall of OSS projects.

</details>


### [9] [Robustness and Reasoning Fidelity of Large Language Models in Long-Context Code Question Answering](https://arxiv.org/abs/2602.17183)
*Kishan Maharaj,Nandakishore Menon,Ashita Saxena,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 该论文系统研究了LLM在长代码上下文问答中的鲁棒性，发现当前模型在答案格式变化、干扰信息和上下文规模变化时表现脆弱。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于需要处理长代码上下文的软件工程任务，但其在不同输入条件下的鲁棒性尚不清楚。作者旨在系统评估LLM在长代码上下文问答中的表现，特别是在答案格式、干扰信息和上下文规模变化时的敏感性。

Method: 通过受控消融实验研究LLM的敏感性：1) 打乱多项选择选项顺序；2) 开放式问题；3) 包含相关和对抗性无关信息的"大海捞针"上下文。扩展LongCodeBench Python数据集，新增COBOL和Java问答集，评估最先进模型。

Result: 结果显示：1) 打乱多项选择选项时性能显著下降；2) 开放式问题表现不佳；3) 存在无关信息时行为脆弱。这些发现突显了当前长上下文评估的局限性。

Conclusion: 当前LLM在长代码上下文推理中存在鲁棒性问题，特别是在输入条件变化时。研究为评估传统和现代系统中的代码推理提供了更全面的基准，强调了改进长上下文评估方法的必要性。

Abstract: Large language models (LLMs) increasingly assist software engineering tasks that require reasoning over long code contexts, yet their robustness under varying input conditions remains unclear. We conduct a systematic study of long-context code question answering using controlled ablations that test sensitivity to answer format, distractors, and context scale. Extending LongCodeBench Python dataset with new COBOL and Java question-answer sets, we evaluate state-of-the-art models under three settings: (i) shuffled multiple-choice options, (ii) open-ended questions and (iii) needle-in-a-haystack contexts containing relevant and adversarially irrelevant information. Results show substantial performance drops in both shuffled multiple-choice options and open-ended questions, and brittle behavior in the presence of irrelevant cues. Our findings highlight limitations of current long-context evaluations and provide a broader benchmark for assessing code reasoning in both legacy and modern systems.

</details>


### [10] [The Case for HTML First Web Development](https://arxiv.org/abs/2602.17193)
*Juho Vepsäläinen*

Main category: cs.SE

TL;DR: HTML First开发方法强调优先使用原生HTML，结合超媒体和服务器端逻辑，能显著减少代码量并提升开发维护效率。


<details>
  <summary>Details</summary>
Motivation: 随着Web框架的普及，HTML逐渐被边缘化，但HTML5等标准的发展缩小了Web平台与框架之间的差距。HTML First方法旨在重新重视原生HTML的优势，回归Web开发的简约理念。

Method: 采用HTML First开发原则：尽可能优先使用原生HTML，结合超媒体架构，将大量应用逻辑移至服务器端，保持代码简洁性和概念简单性。

Result: 基于htmx项目的实践显示，转向HTML First能大幅减少代码库规模，提升维护和开发效率。内容导向网站的对比研究显示性能优势，Yle网站的案例研究也证实了这些好处。

Conclusion: HTML First方法为Web开发者带来明显优势，但在效益程度以及与AI驱动Web开发趋势的契合度方面仍存在开放性问题。

Abstract: Since its introduction in the early 90s, the web has become the largest application platform available globally. HyperText Markup Language (HTML) has been an essential part of the web since the beginning, as it allows defining webpages in a tree-like manner, including semantics and content. Although the web was never meant to be an application platform, it evolved as such, especially since the early 2000s, as web application frameworks became available. While the emergence of frameworks made it easier than ever to develop complex applications, it also put HTML on the back burner. As web standards caught up, especially with milestones such as HTML5, the gap between the web platform and frameworks was reduced. HTML First development emphasizes this shift and puts focus on literally using HTML first when possible, while encouraging minimalism familiar from the early days of the web. It seems HTML-oriented web development can provide clear benefits to developers, especially when it is combined with comple- mentary approaches, such as embracing hypermedia and moving a large part of application logic to the server side. In the context of the htmx project, it was observed that moving towards HTML can reduce the size of a codebase greatly while leading to maintenance and development benefits due to the increased conceptual simplicity. Holotype-based comparisons for content-oriented websites show performance benefits, and the same observation was confirmed by a small case study where the Yle website was converted to follow HTML First principles. In short, the HTML First approach seems to have clear advantages for web developers, while there are open questions related to the magnitude of the benefits and the alignment with the recent trend of AI-driven web development.

</details>


### [11] [Disjunction Composition of BDD Transition Systems for Model-Based Testing](https://arxiv.org/abs/2602.17237)
*Tannaz Zameni,Petra van den Bos,Arend Rensink*

Main category: cs.SE

TL;DR: 本文提出了一种基于行为驱动开发（BDD）的模型驱动测试生成组合方法，通过析取组合将表示替代系统行为的BDD转换系统结合起来，在保持原始场景测试能力的同时实现集成行为的建模和测试。


<details>
  <summary>Details</summary>
Motivation: 在行为驱动开发中，系统行为通过文本场景描述，但缺乏将这些场景组合起来进行集成测试的正式方法。需要一种能够保持原始测试能力的组合机制来测试复杂的系统行为。

Method: 提出析取组合的形式化定义，将BDD文本场景转换为转换系统，通过符号语义证明组合后系统的等价性，确保测试能力保持不变。使用符号语义保证两个BDD转换系统的符号等价性意味着它们会失败相同的测试用例。

Result: 形式化定义了析取组合操作，证明了该组合能够保持原始场景的测试能力，并通过工业案例研究展示了该方法的实际应用潜力。

Conclusion: 析取组合为BDD中的模型驱动测试生成提供了一种有效的组合方法，能够在集成测试中保持原始场景的测试能力，具有实际工业应用价值。

Abstract: We introduce a compositional approach to model-based test generation in Behavior-Driven Development (BDD). BDD is an agile methodology in which system behavior is specified through textual scenarios that, in our approach, are translated into transition systems used for model-based testing. This paper formally defines disjunction composition, to combine BDD transition systems that represent alternative system behaviors. Disjunction composition allows for modeling and testing the integrated behavior while ensuring that the testing power of the original set of scenarios is preserved. This is proved using a symbolic semantics for BDD transition systems, with the property that the symbolic equivalence of two BDD transition systems guarantees that they fail the same test cases. Also, we demonstrate the potential of disjunction composition by applying the composition in an industrial case study.

</details>


### [12] [Socio-Technical Well-Being of Quantum Software Communities: An Overview on Community Smells](https://arxiv.org/abs/2602.17320)
*Stefano Lambiase,Manuel De Stefano,Fabio Palomba,Filomena Ferrucci,Andrea De Lucia*

Main category: cs.SE

TL;DR: 首次对量子开源社区的社会技术健康进行横断面研究，分析社区气味对量子项目质量和可持续性的影响


<details>
  <summary>Details</summary>
Motivation: 量子计算发展迅速，开源社区在量子系统开发中扮演重要角色，但面临社会技术挑战（社区气味），这些反模式可能影响产品质量和社区健康，而目前缺乏对量子开源社区社会技术因素的研究

Method: 采用横断面研究方法，分析量子开源社区的社会技术动态，识别社区气味等反模式

Result: 建立了量子开源社区社会技术健康分析的基础框架，为理解社区气味的影响提供了初步知识

Conclusion: 通过理解社会技术动态，可以建立基础知识来缓解社区气味带来的风险，确保开源量子计划的长期可持续性

Abstract: Quantum computing has gained significant attention due to its potential to solve computational problems beyond the capabilities of classical computers. With major corporations and academic institutions investing in quantum hardware and software, there has been a rise in the development of quantum-enabled systems, particularly within open-source communities. However, despite the promising nature of quantum technologies, these communities face critical socio-technical challenges, including the emergence of socio-technical anti-patterns known as community smells. These anti-patterns, prevalent in open-source environments, have the potential to negatively impact both product quality and community health by introducing technical debt and amplifying architectural and code smells. Despite the importance of these socio-technical factors, there remains a scarcity of research investigating their influence within quantum open-source communities. This work aims to address this gap by providing a first step in analyzing the socio-technical well-being of quantum communities through a cross-sectional study. By understanding the socio-technical dynamics at play, it is expected that foundational knowledge can be established to mitigate the risks associated with community smells and ensure the long-term sustainability of open-source quantum initiatives.

</details>


### [13] [Computer-Using World Model](https://arxiv.org/abs/2602.17365)
*Yiming Guan,Rui Yu,John Zhang,Lu Wang,Chaoyun Zhang,Liqun Li,Bo Qiao,Si Qin,He Huang,Fangkai Yang,Pu Zhao,Lukas Wutschitz,Samuel Kessler,Huseyin A Inan,Robert Sim,Saravan Rajmohan,Qingwei Lin,Dongmei Zhang*

Main category: cs.SE

TL;DR: CUWM是一个用于桌面软件的世界模型，通过两阶段分解预测UI状态变化，帮助代理在真实执行前模拟和比较候选操作，提高决策质量和执行鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在复杂软件环境中，单个错误的UI操作可能破坏长期工作流程。计算机使用场景不支持反事实探索，使得大规模试错学习和规划不切实际，尽管环境是完全数字化和确定性的。

Method: CUWM采用两阶段分解：首先预测代理相关状态变化的文本描述，然后可视化实现这些变化以合成下一张截图。模型在离线UI转换数据上训练，并通过轻量级强化学习阶段进一步优化，使文本转换预测与计算机使用环境的结构要求对齐。

Result: 通过测试时动作搜索评估，冻结代理使用世界模型在执行前模拟和比较候选动作。在多个Office任务中，世界模型引导的测试时扩展提高了决策质量和执行鲁棒性。

Conclusion: CUWM为桌面软件提供了一个有效的世界模型，能够预测UI状态变化，帮助代理在复杂软件环境中做出更好的决策，提高工作流程的可靠性和效率。

Abstract: Agents operating in complex software environments benefit from reasoning about the consequences of their actions, as even a single incorrect user interface (UI) operation can derail long, artifact-preserving workflows. This challenge is particularly acute for computer-using scenarios, where real execution does not support counterfactual exploration, making large-scale trial-and-error learning and planning impractical despite the environment being fully digital and deterministic. We introduce the Computer-Using World Model (CUWM), a world model for desktop software that predicts the next UI state given the current state and a candidate action. CUWM adopts a two-stage factorization of UI dynamics: it first predicts a textual description of agent-relevant state changes, and then realizes these changes visually to synthesize the next screenshot. CUWM is trained on offline UI transitions collected from agents interacting with real Microsoft Office applications, and further refined with a lightweight reinforcement learning stage that aligns textual transition predictions with the structural requirements of computer-using environments. We evaluate CUWM via test-time action search, where a frozen agent uses the world model to simulate and compare candidate actions before execution. Across a range of Office tasks, world-model-guided test-time scaling improves decision quality and execution robustness.

</details>


### [14] [The Runtime Dimension of Ethics in Self-Adaptive Systems](https://arxiv.org/abs/2602.17426)
*Marco Autili,Gianluca Filippone,Mashal Afzal Memon,Patrizio Pelliccione*

Main category: cs.SE

TL;DR: 论文主张从静态伦理规则转向运行时伦理推理的自适应系统，将伦理偏好作为运行时需求处理，需要基于伦理的协商来管理多方伦理权衡。


<details>
  <summary>Details</summary>
Motivation: 当前自适应系统通常将伦理编码为固定的规则约束或单一伦理理论，忽视了人类-系统交互环境的基本特性：伦理偏好因个体和群体而异，随情境演变，可能冲突，但仍需保持在法律和监管定义的硬伦理边界内。

Method: 提出将伦理偏好作为运行时需求，需要被获取、表示和持续修订。强调需要基于伦理的明确协商来管理多方伦理权衡，识别关键挑战包括伦理不确定性、伦理价值冲突以及多维/多方/多驱动协商。

Result: 论文提出了伦理自适应系统的研究方向和问题，包括如何实现运行时伦理推理、处理伦理不确定性、管理伦理价值冲突，以及设计有效的多维协商机制。

Conclusion: 自适应系统需要从静态伦理规则转向运行时伦理推理，将伦理偏好作为动态需求处理，并通过明确的伦理协商机制来管理多方伦理权衡，这是实现真正伦理自适应系统的关键方向。

Abstract: Self-adaptive systems increasingly operate in close interaction with humans, often sharing the same physical or virtual environments and making decisions with ethical implications at runtime. Current approaches typically encode ethics as fixed, rule-based constraints or as a single chosen ethical theory embedded at design time. This overlooks a fundamental property of human-system interaction settings: ethical preferences vary across individuals and groups, evolve with context, and may conflict, while still needing to remain within a legally and regulatorily defined hard-ethics envelope (e.g., safety and compliance constraints). This paper advocates a shift from static ethical rules to runtime ethical reasoning for self-adaptive systems, where ethical preferences are treated as runtime requirements that must be elicited, represented, and continuously revised as stakeholders and situations change. We argue that satisfying such requirements demands explicit ethics-based negotiation to manage ethical trade-offs among multiple humans who interact with, are represented by, or are affected by a system. We identify key challenges, ethical uncertainty, conflicts among ethical values (including human, societal, and environmental drivers), and multi-dimensional/multi-party/multi-driver negotiation, and outline research directions and questions toward ethically self-adaptive systems.

</details>


### [15] [Towards a Software Reference Architecture for Natural Language Processing Tools in Requirements Engineering](https://arxiv.org/abs/2602.17498)
*Julian Frattini,Quim Motger*

Main category: cs.SE

TL;DR: 论文提出从单体NLP4RE工具向可重用、可互操作模块生态系统的转型愿景，并制定了实现该愿景的软件参考架构研究路线图


<details>
  <summary>Details</summary>
Motivation: 当前NLP4RE工具存在功能重叠却重复开发、发布后即被废弃的问题，缺乏互操作性和维护性，导致开发效率低下、工具难以比较和基准测试、文档复杂化，以及工具长期可持续性不足

Method: 提出向模块化生态系统转型的愿景，制定软件参考架构研究路线图，采用标准SRA开发方法框架，通过利益相关者驱动的焦点小组会议收集了36个关键系统需求

Result: 通过焦点小组会议获得了36个NLP4RE工具的关键系统需求，验证了开发专用软件参考架构的必要性，为NLP4RE工具的改进开发、重用和长期维护奠定了基础

Conclusion: 提出的愿景、路线图和初步贡献为NLP4RE工具的开发、重用和长期维护提供了新方向，通过构建模块化生态系统可解决当前工具碎片化和不可持续的问题

Abstract: Natural Language Processing (NLP) tools support requirements engineering (RE) tasks like requirements elicitation, classification, and validation. However, they are often developed from scratch despite functional overlaps, and abandoned after publication. This lack of interoperability and maintenance incurs unnecessary development effort, impedes tool comparison and benchmarking, complicates documentation, and diminishes the long-term sustainability of NLP4RE tools. To address these issues, we postulate a vision to transition from monolithic NLP4RE tools to an ecosystem of reusable, interoperable modules. We outline a research roadmap towards a software reference architecture (SRA) to realize this vision, elaborated following a standard methodological framework for SRA development. As an initial step, we conducted a stakeholder-driven focus group session to elicit generic system requirements for NLP4RE tools. This activity resulted in 36 key system requirements, further motivating the need for a dedicated SRA. Overall, the proposed vision, roadmap, and initial contribution pave the way towards improved development, reuse, and long-term maintenance of NLP4RE tools.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [Read-Modify-Writable Snapshots from Read/Write operations](https://arxiv.org/abs/2602.16903)
*Armando Castañeda,Braulio Ramses Hernández Martínez*

Main category: cs.DC

TL;DR: 该论文提出了两种仅使用读/写操作实现RMWable快照的算法，分别在已知有限进程数和无限并发模型下工作，突破了以往需要强大原子操作的限制。


<details>
  <summary>Details</summary>
Motivation: 探索在仅使用简单读/写操作的情况下是否可能实现RMWable快照，因为现有算法严重依赖compare&swap或load-link/store-conditional等强大原子操作，而这些操作比简单读/写操作更强。

Method: 提出了两种仅使用读/写操作的RMWable快照算法：第一种在标准并发共享内存模型中，进程数n有限且已知；第二种在无限并发变体模型中，进程无限但任意时刻只有有限进程参与执行。

Result: 成功设计了两种仅使用读/写操作的RMWable快照算法，证明了在简单读/写操作下也能实现RMWable快照功能。

Conclusion: RMWable快照可以在仅使用简单读/写操作的情况下实现，无需依赖更强大的原子操作，这扩展了我们对分布式计算能力的理解。

Abstract: In the context of asynchronous concurrent shared-memory systems, a snapshot algorithm allows failure-prone processes to concurrently and atomically write on the entries of a shared array MEM , and also atomically read the whole array. Recently, Read-Modify-Writable (RMWable) snapshot was proposed, a variant of snapshot that allows processes to perform operations more complex than just read and write, specifically, each entry MEM[k] is an arbitrary readable object. The known RMWable snapshot algorithms heavily rely on powerful low-level operations such as compare&swap or load-link/store-conditional to correctly produce snapshots of MEM. Following the large body of research devoted to understand the limits of what can be solved using the simple read/write low-level operations, which are known to be strictly weaker than compare&swap and load-link/store-conditional, we explore if RMWable snapshots are possible using only read/write operations. We present two read/write RMWable snapshot algorithms, the first one in the standard concurrent shared-memory model where the number of processes n is finite and known in advance, and the second one in a variant of the standard model with unbounded concurrency, where there are infinitely many processes, but at any moment only finitely many processes participate in an execution.

</details>


### [17] [Heterogeneous Federated Fine-Tuning with Parallel One-Rank Adaptation](https://arxiv.org/abs/2602.16936)
*Zikai Zhang,Rui Hu,Jiahao Xu*

Main category: cs.DC

TL;DR: Fed-PLoRA：一种轻量级异构联邦微调框架，通过并行单秩适配模块和选择折叠策略，解决异构客户端资源下LoRA秩不同导致的初始化和聚合噪声问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中使用LoRA进行LLM微调时，客户端因异构资源采用不同LoRA秩，导致严重的初始化和聚合噪声，影响性能。

Method: 提出Fed-PLoRA框架：1) PLoRA（并行单秩适配）用多个并行单秩模块替代传统多秩LoRA；2) Select-N-Fold策略在本地训练前将未训练的PLoRA模块折叠到预训练权重中。

Result: 在多种LLM微调任务上的实验表明，Fed-PLoRA在准确性和效率上均优于现有方法。

Conclusion: Fed-PLoRA有效解决了异构联邦微调中的噪声问题，为资源受限环境下的LLM联邦学习提供了实用解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable effectiveness in adapting to downstream tasks through fine-tuning. Federated Learning (FL) extends this capability by enabling collaborative fine-tuning across distributed clients using Low-Rank Adaptation (LoRA), while preserving data privacy by avoiding raw data sharing. However, practical deployments face challenges when clients have heterogeneous resources and thus adopt different LoRA ranks, leading to substantial initialization and aggregation noise that undermines performance. To address these challenges, we propose Fed-PLoRA, a novel lightweight heterogeneous federated fine-tuning (FFT) framework. Fed-PLoRA introduces Parallel One-Rank Adaptation (PLoRA), a new LoRA variant that replaces the classic multi-rank LoRA module with multiple parallel one-rank modules, and a novel Select-N-Fold strategy that folds untrained PLoRA modules into the pre-trained weights before local training, thereby accommodating heterogeneous client resources. We provide a unified analysis of initialization and aggregation noise of Fed-PLoRA and demonstrate how it addresses the limitations of state-of-the-art methods. Extensive experiments on diverse LLM fine-tuning tasks demonstrate that Fed-PLoRA consistently outperforms existing methods in both accuracy and efficiency. The code is available at https://github.com/TNI-playground/Fed-PLoRA.

</details>


### [18] [Trivance: Latency-Optimal AllReduce by Shortcutting Multiport Networks](https://arxiv.org/abs/2602.17254)
*Anton Juerss,Vamsi Addanki,Stefan Schmid*

Main category: cs.DC

TL;DR: Trivance是一种新颖的AllReduce算法，在双向环网络上以log₃n步完成，相比Bruck算法减少3倍拥塞，保持带宽最优性，在多维环面网络中扩展良好。


<details>
  <summary>Details</summary>
Motivation: AllReduce是分布式计算中的关键操作，其完成时间由通信步数和通信距离决定。现有算法存在权衡：Bruck算法在log₃n步内完成但产生大通信距离和拥塞；Swing算法减少通信距离但需要log₂n步，牺牲延迟最优性。

Method: Trivance利用双向环的两个传输端口，在每个步骤中同时沿两个方向进行三倍通信距离的传输。通过执行联合归约，既减少步数又降低网络拥塞。算法可自然扩展到多维环面网络。

Result: Trivance在8MiB以下消息大小上比现有方法提升5-30%，在高带宽设置下可达32MiB，在3D环面中可达128MiB。在所有评估中，Trivance保持为性能最佳的延迟最优算法。

Conclusion: Trivance在保持延迟最优性(log₃n步)的同时，显著减少网络拥塞，为直接连接拓扑(如环面网络)中的AllReduce操作提供了更好的性能平衡。

Abstract: AllReduce is a fundamental collective operation in distributed computing and a key performance bottleneck for large-scale training and inference. Its completion time is determined by the number of communication steps, which dominates latency-sensitive workloads, and the communication distance affecting both latency- and bandwidth-bound regimes. Direct-connect topologies, such as torus networks used in Google's TPUv4, are particularly prone to large communication distances due to limited bisection bandwidth. Latency-optimal algorithms such as Bruck's complete AllReduce in $\log_3 n$ steps on a bidirectional ring, but incur large communication distances that result in substantial congestion. In contrast, recent approaches such as Swing reduce communication distance and congestion, but are inherently required to perform $\log_2 n$ steps to complete AllReduce, sacrificing latency-optimality.
  In this paper, we present Trivance, a novel AllReduce algorithm that completes within $\log_3 n$ steps, while reducing congestion compared to Bruck's algorithm by a factor of three and preserving bandwidth-optimality. Trivance exploits both transmission ports of a bidirectional ring within each step to triple the communication distance along both directions simultaneously. Furthermore, by performing joint reductions, Trivance improves both the number of steps and network congestion. We further show that Trivance extends naturally to multidimensional torus networks, retaining its latency advantage while achieving performance comparable to bandwidth-optimal algorithms for large messages.
  Our empirical evaluation shows that Trivance improves state-of-the-art approaches by 5-30% for message sizes up to 8\,MiB, in high-bandwidth settings up to 32MiB and for 3D tori up to 128MiB. Throughout the evaluation, Trivance remains the best-performing latency-optimal algorithm.

</details>


### [19] [Visual Insights into Agentic Optimization of Pervasive Stream Processing Services](https://arxiv.org/abs/2602.17282)
*Boris Sedlak,Víctor Casamayor Pujol,Schahram Dustdar*

Main category: cs.DC

TL;DR: 论文提出了一个用于边缘流处理服务的上下文感知自动扩展平台，通过智能代理探索服务动作空间并优化执行，解决资源波动、服务异构和资源竞争问题。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的流处理服务面临三个主要挑战：1) 应用需求和资源可用性波动，需要动态扩展以满足处理要求（如延迟）；2) 每个服务有不同的操作调整动作，需要个性化扩展策略；3) 缺乏高层协调器时，同一设备上的服务会相互竞争资源。

Method: 首先提出一个用于流处理服务上下文感知自动扩展的平台，允许开发者监控和调整跨多个服务特定参数的服务执行。然后连接一个扩展代理到这些接口，通过探索每个服务的动作空间逐步建立对处理环境的理解，并基于此知识优化服务执行。

Result: 该演示展示了平台和代理的实现，参与者可以通过视频摘要和介绍性海报回顾演示内容，或通过扩展工件库构建自定义代理。

Conclusion: 该平台通过智能代理驱动的上下文感知自动扩展，有效解决了边缘流处理服务面临的资源管理挑战，为智能城市等普适应用提供了低延迟处理解决方案。

Abstract: Processing sensory data close to the data source, often involving Edge devices, promises low latency for pervasive applications, like smart cities. This commonly involves a multitude of processing services, executed with limited resources; this setup faces three problems: first, the application demand and the resource availability fluctuate, so the service execution must scale dynamically to sustain processing requirements (e.g., latency); second, each service permits different actions to adjust its operation, so they require individual scaling policies; third, without a higher-level mediator, services would cannibalize any resources of services co-located on the same device. This demo first presents a platform for context-aware autoscaling of stream processing services that allows developers to monitor and adjust the service execution across multiple service-specific parameters. We then connect a scaling agent to these interfaces that gradually builds an understanding of the processing environment by exploring each service's action space; the agent then optimizes the service execution according to this knowledge. Participants can revisit the demo contents as video summary and introductory poster, or build a custom agent by extending the artifact repository.

</details>


### [20] [Evaluating Malleable Job Scheduling in HPC Clusters using Real-World Workloads](https://arxiv.org/abs/2602.17318)
*Patrick Zojer,Jonas Posner,Taylan Özden*

Main category: cs.DC

TL;DR: 该研究评估了HPC集群中资源弹性（可塑作业）对系统性能的改进，通过模拟实验发现可塑作业能显著降低作业周转时间、等待时间，提高节点利用率，即使只有20%的可塑作业也能带来实质性优势。


<details>
  <summary>Details</summary>
Motivation: 传统HPC集群的刚性作业调度导致资源利用率低下和作业等待时间增加，需要探索更灵活的调度机制来优化资源利用和提升用户满意度。

Method: 使用Cori、Eagle和Theta超级计算机的真实工作负载轨迹，通过ElastiSim软件模拟不同比例（0-100%）的可塑作业，评估了五种作业调度策略，包括一种新的策略（尽可能保持可塑作业在首选资源分配状态）。

Result: 相比完全刚性工作负载，可塑作业在所有关键指标上都有显著改进：作业周转时间减少37-67%，作业完成时间减少16-65%，作业等待时间减少73-99%，节点利用率提高5-52%。即使只有20%的可塑作业也能带来实质性优势。

Conclusion: 资源弹性（可塑作业）能有效解决当前HPC实践中的低效问题，即使有限的采用也能带来显著优势，值得集成到HPC资源管理中。研究还揭示了工作负载特征、可塑性比例和调度策略之间的重要相关性。

Abstract: Optimizing resource utilization in high-performance computing (HPC) clusters is essential for maximizing both system efficiency and user satisfaction. However, traditional rigid job scheduling often results in underutilized resources and increased job waiting times.
  This work evaluates the benefits of resource elasticity, where the job scheduler dynamically adjusts the resource allocation of malleable jobs at runtime. Using real workload traces from the Cori, Eagle, and Theta supercomputers, we simulate varying proportions (0-100%) of malleable jobs with the ElastiSim software.
  We evaluate five job scheduling strategies, including a novel one that maintains malleable jobs at their preferred resource allocation when possible. Results show that, compared to fully rigid workloads, malleable jobs yield significant improvements across all key metrics. Considering the best-performing scheduling strategy for each supercomputer, job turnaround times decrease by 37-67%, job makespan by 16-65%, job wait times by 73-99%, and node utilization improves by 5-52%. Although improvements vary, gains remain substantial even at 20% malleable jobs.
  This work highlights important correlations between workload characteristics (e.g., job runtimes and node requirements), malleability proportions, and scheduling strategies. These findings confirm the potential of malleability to address inefficiencies in current HPC practices and demonstrate that even limited adoption can provide substantial advantages, encouraging its integration into HPC resource management.

</details>


### [21] [Informative Trains: A Memory-Efficient Journey to a Self-Stabilizing Leader Election Algorithm in Anonymous Graphs](https://arxiv.org/abs/2602.17541)
*Lelia Blin,Sylvain Gay,Isabella Ziccardi*

Main category: cs.DC

TL;DR: 提出了一个在任意匿名网络中仅需O(log log n)比特内存的概率自稳定领导者选举算法


<details>
  <summary>Details</summary>
Motivation: 自稳定领导者选举算法在低内存复杂度方面存在挑战，现有解决方案要么需要Ω(log n)比特内存，要么只能在受限网络拓扑中实现低状态复杂度

Method: 在状态模型下使用概率方法，假设已知全局参数N=Θ(log n)，采用同步调度器，算法在收敛后继续传输信息（不满足静默性）

Result: 算法几乎必然收敛到具有唯一领导者的稳定配置，并以高概率在O(poly(n))轮内稳定，每个节点仅需O(log log n)比特内存

Conclusion: 首次为任意匿名网络实现了O(log log n)比特内存的自稳定领导者选举算法，突破了现有解决方案的内存限制

Abstract: We study the self-stabilizing leader election problem in anonymous $n$-nodes networks. Achieving self-stabilization with low space memory complexity is particularly challenging, and designing space-optimal leader election algorithms remains an open problem for general graphs. In deterministic settings, it is known that $Ω(\log \log n)$ bits of memory per node are necessary [Blin et al., Disc. Math. \& Theor. Comput. Sci., 2023], while in probabilistic settings the same lower bound holds for some values of $n$, but only for an unfair scheduler [Beauquier et al., PODC 1999]. Several deterministic and probabilistic protocols have been proposed in models ranging from the state model to the population protocols. However, to the best of our knowledge, existing solutions either require $Ω(\log n)$ bits of memory per node for general worst case graphs, or achieve low state complexity only under restricted network topologies such as rings, trees, or bounded-degree graphs.
  In this paper, we present a probabilistic self-stabilizing leader election algorithm for arbitrary anonymous networks that uses $O(\log \log n)$ bits of memory per node. Our algorithm operates in the state model under a synchronous scheduler and assumes knowledge of a global parameter $N = Θ(\log n)$. We show that, under our protocol, the system converges almost surely to a stable configuration with a unique leader and stabilizes within $O(\mathrm{poly}(n))$ rounds with high probability. To achieve $O(\log \log n)$ bits of memory, our algorithm keeps transmitting information after convergence, i.e. it does not verify the silence property. Moreover, like most works in the field, our algorithm does not provide explicit termination detection (i.e., nodes do not detect when the algorithm has converged).

</details>


### [22] [TopoSZp: Lightweight Topology-Aware Error-controlled Compression for Scientific Data](https://arxiv.org/abs/2602.17552)
*Tripti Agarwal,Sheng Di,Xin Liang,Zhaoyuan Su,Yuxiao Li,Ganesh Gopalakrishnan,Hanqi Guo,Franck Cappello*

Main category: cs.DC

TL;DR: TopoSZp是一个轻量级、拓扑感知的误差控制有损压缩器，在保持高压缩性能的同时，能有效保留科学数据中的关键拓扑结构（极值点和鞍点）。


<details>
  <summary>Details</summary>
Motivation: 现有有损压缩器（如SZ和ZFP）虽然提供数值误差保证，但无法有效保留对科学分析至关重要的拓扑结构（如极值点和鞍点）。现有的拓扑感知压缩器计算开销过大。

Method: 基于高性能SZp压缩器，集成高效关键点检测、局部顺序保持和针对性鞍点优化，在宽松但严格执行的误差边界内工作。

Result: 在真实科学数据集上，TopoSZp相比现有拓扑感知压缩器：非保留关键点减少3-100倍，无假阳性或错误关键点类型，压缩速度快100-10000倍，解压速度快10-500倍，同时保持有竞争力的压缩比。

Conclusion: TopoSZp实现了轻量级、高性能的拓扑感知压缩，在保持压缩性能的同时有效保护科学数据的关键拓扑特征，解决了现有方法的局限性。

Abstract: Error-bounded lossy compression is essential for managing the massive data volumes produced by large-scale HPC simulations. While state-of-the-art compressors such as SZ and ZFP provide strong numerical error guarantees, they often fail to preserve topological structures (example, minima, maxima, and saddle points) that are critical for scientific analysis. Existing topology-aware compressors address this limitation but incur substantial computational overhead. We present TopoSZp, a lightweight, topology-aware, error-controlled lossy compressor that preserves critical points and their relationships while maintaining high compression and decompression performance. Built on the high-throughput SZp compressor, TopoSZp integrates efficient critical point detection, local ordering preservation, and targeted saddle point refinement, all within a relaxed but strictly enforced error bound. Experimental results on real-world scientific datasets show that TopoSZp achieves 3 to 100 times fewer non-preserved critical points, introduces no false positives or incorrect critical point types, and delivers 100 to 10000 times faster compression and 10 to 500 times faster decompression compared to existing topology-aware compressors, while maintaining competitive compression ratios.

</details>


### [23] [Exploring Novel Data Storage Approaches for Large-Scale Numerical Weather Prediction](https://arxiv.org/abs/2602.17610)
*Nicolau Manubens Gil*

Main category: cs.DC

TL;DR: 评估DAOS和Ceph对象存储系统在数值天气预报和高性能计算应用中的性能表现，与传统的Lustre文件系统对比


<details>
  <summary>Details</summary>
Motivation: 随着HPC和AI应用（如数值天气预报）数据量激增，传统POSIX分布式文件系统在大规模I/O工作负载中存在性能限制，需要探索新的存储解决方案

Method: 开发软件适配器使ECMWF的NWP能够使用对象存储系统，在多个计算机系统上进行广泛的I/O基准测试，比较DAOS、Ceph与相同硬件上的Lustre文件系统性能

Result: DAOS和Ceph都表现出优异性能，但DAOS在可扩展性和灵活性方面优于Ceph和Lustre，特别适合大规模I/O应用

Conclusion: 对象存储（特别是DAOS）为HPC中心提供了有前景的存储方案，可能在未来得到更广泛采用，但这不一定意味着完全放弃POSIX类I/O

Abstract: Driven by scientific and industry ambition, HPC and AI applications such as operational Numerical Weather Prediction (NWP) require processing and storing ever-increasing data volumes as fast as possible. Whilst POSIX distributed file systems and NVMe SSDs are currently a common HPC storage configuration providing I/O to applications, new storage solutions have proliferated or gained traction over the last decade with potential to address performance limitations POSIX file systems manifest at scale for certain I/O workloads.
  This work has primarily aimed to assess the suitability and performance of two object storage systems -namely DAOS and Ceph- for the ECMWF's operational NWP as well as for HPC and AI applications in general. New software-level adapters have been developed which enable the ECMWF's NWP to leverage these systems, and extensive I/O benchmarking has been conducted on a few computer systems, comparing the performance delivered by the evaluated object stores to that of equivalent Lustre file system deployments on the same hardware. Challenges of porting to object storage and its benefits with respect to the traditional POSIX I/O approach have been discussed and, where possible, domain-agnostic performance analysis has been conducted, leading to insight also of relevance to I/O practitioners and the broader HPC community.
  DAOS and Ceph have both demonstrated excellent performance, but DAOS stood out relative to Ceph and Lustre, providing superior scalability and flexibility for applications to perform I/O at scale as desired. This sets a promising outlook for DAOS and object storage, which might see greater adoption at HPC centres in the years to come, although not necessarily implying a shift away from POSIX-like I/O.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [Guided Exploration of Sequential Rules](https://arxiv.org/abs/2602.16717)
*Wensheng Gan,Gengsen Huang,Junyu Ren,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出一种高效的用户中心化序列规则挖掘方法，通过设计紧致的上界和剪枝策略，针对特定查询规则进行挖掘，减少无关规则生成，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统序列规则挖掘方法虽然能发现时序关系，但计算量大且生成大量与用户无关的规则，增加了计算开销并复杂化下游分析，需要更高效的用户中心化方法。

Method: 1) 预处理数据库以判断目标查询规则是否存在；2) 设计紧致且可泛化的上界来剪枝无望项和避免不必要扩展；3) 提出基于频率和效用两种常见评估指标的挖掘算法；4) 设计两种规则相似度度量来发现最相关序列规则。

Result: 实验表明，所提算法在运行时间和内存使用上优于现有方法，能在灵活相似度设置下发现简洁的序列规则集合，能够处理具有个性化特征的序列数据并实现模式发现。

Conclusion: 提出的目标序列规则搜索方法解决了多个挑战，可应用于两种常见挖掘任务，实现了高效、用户中心化的序列规则发现，减少了无关规则生成并提高了计算效率。

Abstract: In pattern mining, sequential rules provide a formal framework to capture the temporal relationships and inferential dependencies between items. However, the discovery process is computationally intensive. To obtain mining results efficiently and flexibly, many methods have been proposed that rely on specific evaluation metrics (i.e., ensuring results meet minimum threshold requirements). A key issue with these methods, however, is that they generate many sequential rules that are irrelevant to users. Such rules not only incur additional computational overhead but also complicate downstream analysis. In this paper, we investigate how to efficiently discover user-centric sequential rules. The original database is first processed to determine whether a target query rule is present. To prune unpromising items and avoid unnecessary expansions, we design tight and generalizable upper bounds. We introduce a novel method for efficiently generating target sequential rules using the proposed techniques and pruning strategies. In addition, we propose the corresponding mining algorithms for two common evaluation metrics: frequency and utility. We also design two rule similarity metrics to help discover the most relevant sequential rules. Extensive experiments demonstrate that our algorithms outperform state-of-the-art approaches in terms of runtime and memory usage, while discovering a concise set of sequential rules under flexible similarity settings. Targeted sequential rule search can handle sequence data with personalized features and achieve pattern discovery. The proposed solution addresses several challenges and can be applied to two common mining tasks.

</details>


### [25] [UPER: Efficient Utility-driven Partially-ordered Episode Rule Mining](https://arxiv.org/abs/2602.16718)
*Hong Lin,Wensheng Gan,Junyu Ren,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出UPER算法挖掘高效用部分有序情节规则，通过NoList数据结构和剪枝策略提高效率


<details>
  <summary>Details</summary>
Motivation: 传统情节规则挖掘对事件顺序要求严格，部分有序情节规则(POERM)放宽了顺序约束，但未考虑规则的效用价值。本文旨在挖掘更有价值的高效用部分有序情节规则。

Method: 定义部分有序情节规则的效用，提出UPER算法，采用NoList数据结构存储必要信息，详细分析规则扩展，并提出WEUP、REUCSP和REEUP三种剪枝策略减少候选规则数量。

Result: 在多个数据集上进行实验，验证了UPER算法的有效性，能够发现高效用的部分有序情节规则。

Conclusion: UPER算法成功解决了高效用部分有序情节规则挖掘问题，通过创新的数据结构和剪枝策略提高了挖掘效率，为复杂事件序列分析提供了更有价值的规则发现方法。

Abstract: Episode mining is a fundamental problem in analyzing a sequence of numerous events. For discovering strong relationships between events in a complex event sequence, episode rule mining is proposed. However, both the episode and episode rules have strict requirements for the order of events. Hence, partially-ordered episode rule mining (POERM) is designed to loosen the constraints on the ordering, i.e., events in the antecedents and consequents of the rule can be unordered, and POERM has been applied to real-life event prediction. In this paper, we consider the utility of POERM, intending to discover more valuable rules. We define the utility of POERs and propose an algorithm called UPER to discover high-utility partially-ordered episode rules. In addition, we adopt a data structure named NoList to store the necessary information, analyze the expansion of POERs in detail, and propose several pruning strategies (namely WEUP, REUCSP, and REEUP) to reduce the number of candidate rules. Finally, we conduct experiments on several datasets to demonstrate the effectivene

</details>


### [26] [GPU-Accelerated Algorithms for Graph Vector Search: Taxonomy, Empirical Study, and Research Directions](https://arxiv.org/abs/2602.16719)
*Yaowen Liu,Xuejia Chen,Anxin Tian,Haoyang Li,Qinbin Li,Xin Zhang,Alexander Zhou,Chen Jason Zhang,Qing Li,Lei Chen*

Main category: cs.DB

TL;DR: 该论文对GPU加速的图基向量搜索算法进行了全面调查和实验研究，分析了GPU优化策略、硬件映射关系，并通过大规模基准测试评估了六种主要算法，发现距离计算是主要计算瓶颈，CPU-GPU数据传输是影响大规模实际延迟的主导因素。


<details>
  <summary>Details</summary>
Motivation: 尽管图基方法在近似最近邻搜索（ANNS）中代表了最先进的技术，但缺乏对现代GPU架构优化的系统理解以及在实际场景中的端到端有效性评估。随着数据集规模增长，高效检索越来越依赖于GPU加速。

Method: 1. 建立GPU优化策略的详细分类法；2. 阐明算法任务与GPU硬件执行单元之间的映射关系；3. 在八个大规模基准数据集上对六种领先算法进行全面评估；4. 分析图索引构建和查询搜索性能。

Result: 1. 距离计算仍然是主要计算瓶颈；2. CPU和GPU之间的数据传输成为大规模实际延迟的主导因素；3. 揭示了不同系统设计在可扩展性和内存使用方面的关键权衡；4. 提供了全面的基准测试结果。

Conclusion: 该研究为设计可扩展且稳健的GPU驱动的近似最近邻搜索系统提供了明确指导方针，并为知识发现和数据挖掘社区提供了全面的基准参考，强调了在实际部署中需要考虑数据传输优化的重要性。

Abstract: Approximate Nearest Neighbor Search (ANNS) underpins many large-scale data mining and machine learning applications, with efficient retrieval increasingly hinging on GPU acceleration as dataset sizes grow. Although graph-based approaches represent the state of the art in approximate nearest neighbor search, there is a lack of systematic understanding regarding their optimization for modern GPU architectures and their end-to-end effectiveness in practical scenarios. In this work, we present a comprehensive survey and experimental study of GPU-accelerated graph-based vector search algorithms. We establish a detailed taxonomy of GPU optimization strategies and clarify the mapping between algorithmic tasks and hardware execution units within GPUs. Through a thorough evaluation of six leading algorithms on eight large-scale benchmark datasets, we assess both graph index construction and query search performance. Our analysis reveals that distance computation remains the primary computational bottleneck, while data transfer between the host CPU and GPU emerges as the dominant factor influencing real-world latency at large scale. We also highlight key trade-offs in scalability and memory usage across different system designs. Our findings offer clear guidelines for designing scalable and robust GPU-powered approximate nearest neighbor search systems, and provide a comprehensive benchmark for the knowledge discovery and data mining community.

</details>


### [27] [APEX-SQL: Talking to the data via Agentic Exploration for Text-to-SQL](https://arxiv.org/abs/2602.16720)
*Bowen Cao,Weibin Liao,Yushi Sun,Dong Fang,Haitao Li,Wai Lam*

Main category: cs.DB

TL;DR: APEX-SQL是一个基于代理探索的Text-to-SQL框架，通过假设验证循环和数据探查来解决企业环境中复杂数据库的语义模糊问题，在BIRD和Spider 2.0-Snow基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的Text-to-SQL系统在学术基准上表现良好，但在复杂企业环境中存在局限，主要问题在于依赖静态模式表示，无法有效解决语义模糊性，也难以扩展到大型复杂数据库。

Method: 提出APEX-SQL代理式Text-to-SQL框架，采用假设验证循环将模型推理基于真实数据。在模式链接阶段使用逻辑规划生成假设、双路径剪枝减少搜索空间、并行数据剖析验证列角色，并通过全局合成确保拓扑连接性。在SQL生成阶段引入确定性机制检索探索指令，使代理能有效探索数据分布、精炼假设并生成语义准确的SQL。

Result: 在BIRD基准上达到70.65%的执行准确率，在Spider 2.0-Snow基准上达到51.01%的执行准确率，优于竞争基线且减少了token消耗。分析表明代理探索作为性能倍增器，能释放基础模型在企业环境中的潜在推理能力。

Conclusion: APEX-SQL通过从被动翻译转向代理探索的范式转变，解决了企业环境中Text-to-SQL的语义模糊和可扩展性问题。消融研究证实了各组件对确保鲁棒准确数据分析的关键贡献。

Abstract: Text-to-SQL systems powered by Large Language Models have excelled on academic benchmarks but struggle in complex enterprise environments. The primary limitation lies in their reliance on static schema representations, which fails to resolve semantic ambiguity and scale effectively to large, complex databases. To address this, we propose APEX-SQL, an Agentic Text-to-SQL Framework that shifts the paradigm from passive translation to agentic exploration. Our framework employs a hypothesis-verification loop to ground model reasoning in real data. In the schema linking phase, we use logical planning to verbalize hypotheses, dual-pathway pruning to reduce the search space, and parallel data profiling to validate column roles against real data, followed by global synthesis to ensure topological connectivity. For SQL generation, we introduce a deterministic mechanism to retrieve exploration directives, allowing the agent to effectively explore data distributions, refine hypotheses, and generate semantically accurate SQLs. Experiments on BIRD (70.65% execution accuracy) and Spider 2.0-Snow (51.01% execution accuracy) demonstrate that APEX-SQL outperforms competitive baselines with reduced token consumption. Further analysis reveals that agentic exploration acts as a performance multiplier, unlocking the latent reasoning potential of foundation models in enterprise settings. Ablation studies confirm the critical contributions of each component in ensuring robust and accurate data analysis.

</details>


### [28] [Multiple Index Merge for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2602.17099)
*Liuchang Jing,Mingyu Yang,Lei Li,Jianbin Qin,Wei Wang*

Main category: cs.DB

TL;DR: 提出RNSM方法高效合并近似k近邻搜索中的多图索引，通过反向邻居滑动合并和合并顺序选择，显著提升构建效率并保持搜索性能。


<details>
  <summary>Details</summary>
Motivation: 高维近似k近邻搜索中，基于邻近图的索引虽搜索效率最优，但构建时距离计算量大、内存开销高。处理大规模数据集时需构建多个子索引，但直接搜索这些分离索引会因缺乏跨图连接而严重损害搜索效率，因此需要高效的图索引合并方法。

Method: 提出反向邻居滑动合并(RNSM)方法，利用结构信息提升合并效率；研究合并顺序选择(MOS)以减少冗余合并操作；专注于两索引高效合并及多索引合并顺序优化。

Result: 实验显示，相比现有索引合并方法获得最高5.48倍加速，相比索引重建获得最高9.92倍加速，同时保持优越搜索性能；方法可扩展到1亿向量、50个分区，保持一致的加速效果。

Conclusion: RNSM和MOS方法能有效解决大规模高维向量数据库中图索引合并的效率问题，显著提升构建速度同时不损害搜索性能，具有良好的可扩展性。

Abstract: Approximate $k$ nearest neighbor (AKNN) search in high-dimensional space is a foundational problem in vector databases with widespread applications. Among the numerous AKNN indexes, Proximity Graph-based indexes achieve state-of-the-art search efficiency across various benchmarks. However, their extensive distance computations of high-dimensional vectors lead to slow construction and substantial memory overhead. The limited memory capacity often prevents building the entire index at once when handling large-scale datasets. A common practice is to build multiple sub-indexes separately. However, directly searching on these separated indexes severely compromises search efficiency, as queries cannot leverage cross-graph connections. Therefore, efficient graph index merging is crucial for multi-index searching. In this paper, we focus on efficient two-index merging and the merge order of multiple indexes for AKNN search. To achieve this, we propose a reverse neighbor sliding merge (RNSM) that exploits structural information to boost merging efficiency. We further investigate merge order selection (MOS) to reduce the merging cost by eliminating redundant merge operations. Experiments show that our approach yields up to a 5.48$\times$ speedup over existing index merge methods and 9.92$\times$ speedup over index reconstruction, while maintaining expected superior search performance. Moreover, our method scales efficiently to 100 million vectors with 50 partitions, maintaining consistent speedups.

</details>


### [29] [Do GPUs Really Need New Tabular File Formats?](https://arxiv.org/abs/2602.17335)
*Jigao Luo,Qi Chen,Carsten Binnig*

Main category: cs.DB

TL;DR: 研究显示Parquet文件在GPU上的扫描性能不佳并非格式本身问题，而是由于CPU导向的配置选择不当。通过采用GPU感知的配置，读取带宽可提升至125 GB/s。


<details>
  <summary>Details</summary>
Motivation: Parquet作为现代分析系统的列式文件格式，其配置指南主要基于CPU执行模型。随着GPU加速数据处理日益普及，采用CPU导向默认配置生成的Parquet文件会严重未充分利用GPU并行性，使GPU扫描成为性能瓶颈。

Method: 系统研究Parquet配置如何影响GPU扫描性能，分析不同配置参数对GPU并行性的影响，提出GPU感知的配置优化方案。

Result: 通过应用GPU感知配置，有效读取带宽可提升至125 GB/s，且无需修改Parquet规范本身。

Conclusion: Parquet在GPU上的性能问题并非格式固有缺陷，而是配置选择不当所致。通过优化配置参数可以显著提升GPU扫描性能，为GPU加速数据分析系统提供重要指导。

Abstract: Parquet is the de facto columnar file format in modern analytical systems, yet its configuration guidelines have largely been shaped by CPU-centric execution models. As GPU-accelerated data processing becomes increasingly prevalent, Parquet files generated with CPU-oriented defaults can severely underutilize GPU parallelism, turning GPU scans into a performance bottleneck.
  In this work, we systematically study how Parquet configurations affect GPU scan performance. We show that Parquet's poor GPU performance is not inherent to the format itself but rather a consequence of suboptimal configuration choices. By applying GPU-aware configurations, we increase effective read bandwidth up to 125 GB/s without modifying the Parquet specification.

</details>
