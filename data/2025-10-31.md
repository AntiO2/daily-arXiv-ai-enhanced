<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.SE](#cs.SE) [Total: 21]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration](https://arxiv.org/abs/2510.26495)
*Linzhuang Sun,Tianyu Guo,Hao Liang,Yuying Li,Qifeng Cai,Jingxuan Wei,Bihui Yu,Wentao Zhang,Bin Cui*

Main category: cs.DB

TL;DR: DySQL-Bench是一个用于评估Text-to-SQL模型在动态交互场景下性能的基准测试，通过自动化流水线生成多轮对话任务，模拟真实世界用户意图演变的SQL查询场景。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL系统在静态单轮任务中表现良好，但在真实交互场景中用户意图会演变，需要多轮查询精炼，现有系统无法满足这种动态需求。

Method: 采用自动化两阶段流水线：任务合成和验证。基于原始数据库表构建结构化树表示，指导LLM生成任务，然后进行交互导向过滤和专家验证。

Result: 构建了包含13个领域、1,072个任务的基准测试。GPT-4o仅达到58.34%整体准确率和23.81%的Pass@5指标，显示基准难度较高。

Conclusion: DySQL-Bench填补了动态交互Text-to-SQL评估的空白，揭示了当前模型在真实世界多轮交互场景中的局限性，为未来研究提供了重要基准。

Abstract: Recent advances in Text-to-SQL have achieved strong results in static,
single-turn tasks, where models generate SQL queries from natural language
questions. However, these systems fall short in real-world interactive
scenarios, where user intents evolve and queries must be refined over multiple
turns. In applications such as finance and business analytics, users
iteratively adjust query constraints or dimensions based on intermediate
results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a
benchmark assessing model performance under evolving user interactions. Unlike
previous manually curated datasets, DySQL-Bench is built through an automated
two-stage pipeline of task synthesis and verification. Structured tree
representations derived from raw database tables guide LLM-based task
generation, followed by interaction-oriented filtering and expert validation.
Human evaluation confirms 100% correctness of the synthesized data. We further
propose a multi-turn evaluation framework simulating realistic interactions
among an LLM-simulated user, the model under test, and an executable database.
The model must adapt its reasoning and SQL generation as user intents change.
DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling
1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the
Pass@5 metric, underscoring the benchmark's difficulty. All code and data are
released at https://github.com/Aurora-slz/Real-World-SQL-Bench .

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance](https://arxiv.org/abs/2510.25882)
*Wenhao Yang,Minghui Zhou,Daniel Izquierdo Cortázar,Yehui Wang*

Main category: cs.SE

TL;DR: 提出了一个开源风险治理框架OTVM，通过"目标->威胁->漏洞->缓解"的逻辑链，将不可控的外部威胁与可控的内部漏洞关联，帮助企业从战术风险管理转向整体风险治理。


<details>
  <summary>Details</summary>
Motivation: 传统风险管理仅关注技术工具，无法应对上游"静默修复"、社区冲突、许可证变更等系统性威胁，存在治理盲区。

Method: 基于扎根理论研究，通过对15位从业者的访谈，开发了包含战略目标矩阵、外部威胁和内部漏洞双重分类法、缓解框架的整体风险治理模型。

Result: 框架通过三个行业专家的回顾性案例研究验证了分析效用，提供了从被动"救火"到主动构建组织"免疫系统"的路径。

Conclusion: 该工作为企业提供了新颖的诊断视角和系统化路径，实现从战术风险管理到整体风险治理的转变。

Abstract: Enterprise engagement with open source has evolved from tactical adoption to
strategic deep integration, exposing them to a complex risk landscape far
beyond mere code. However, traditional risk management, narrowly focused on
technical tools, is structurally inadequate for systemic threats like upstream
"silent fixes", community conflicts, or sudden license changes, creating a
dangerous governance blind spot. To address this governance vacuum and enable
the necessary shift from tactical risk management to holistic risk governance,
we conducted a grounded theory study with 15 practitioners to develop a
holistic risk governance framework. Our study formalizes an analytical
framework built on a foundational risk principle: an uncontrollable External
Threat (e.g., a sudden license change in a key dependency) only becomes a
critical risk when it exploits a controllable Internal Vulnerability (e.g., an
undefined risk appetite for single-vendor projects), which then amplifies the
impact.The framework operationalizes this principle through a clear logical
chain: "Objectives -> Threats -> Vulnerabilities -> Mitigation" (OTVM). This
provides a holistic decision model that transcends mere technical checklists.
Based on this logic, our contributions are: (1) a "Strategic Objectives Matrix"
to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech,
Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and
(3) an actionable mitigation framework mapping capability-building to these
vulnerabilities. The framework's analytical utility was validated by three
industry experts through retrospective case studies on real-world incidents.
This work provides a novel diagnostic lens and a systematic path for
enterprises to shift from reactive "firefighting" to proactively building an
organizational "immune system".

</details>


### [3] [PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints](https://arxiv.org/abs/2510.25890)
*Tong Ma,Hui Lai,Hui Wang,Zhenhu Tian,Jizhou Wang,Haichao Wu,Yongfan Gao,Chaochao Li,Fengjie Xu,Ling Fang*

Main category: cs.SE

TL;DR: PRISM是一个将大语言模型与模型驱动工程相结合的系统，用于生成符合监管要求的工件和机器可检查的安全证据，特别适用于安全和合规关键领域。


<details>
  <summary>Details</summary>
Motivation: 解决在安全和合规关键领域中，使用大语言模型生成符合监管要求的工件时面临的挑战，包括异构模式整合、约束强制执行和可验证性保证。

Method: 采用三支柱架构：统一元模型整合异构模式，集成约束模型编译结构化和语义要求，约束引导的可验证生成通过两层强制执行机制确保合规性。

Result: 在汽车软件工程和跨境法律管辖两个领域进行评估，PRISM能够生成结构有效、可审计的工件，显著减少人工修复工作量。

Conclusion: PRISM为自动化工件生成提供了一条实用路径，具有内置保证机制，能够与现有工具链集成，提高安全关键系统开发的效率和可靠性。

Abstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate
regulator-ready artifacts and machine-checkable evidence for safety- and
compliance-critical domains. PRISM integrates three pillars: a Unified
Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a
single semantic space; an Integrated Constraint Model (ICM) compiles structural
and semantic requirements into enforcement artifacts including generation-time
automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and
Constraint-Guided Verifiable Generation (CVG) applies these through two-layer
enforcement - structural constraints drive prefix-safe decoding while
semantic/logical validation produces machine-checkable certificates. When
violations occur, PRISM performs audit-guided repair and records generation
traces for compliance review. We evaluate PRISM in automotive software
engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis).
PRISM produces structurally valid, auditable artifacts that integrate with
existing tooling and substantially reduce manual remediation effort, providing
a practical path toward automated artifact generation with built-in assurance.

</details>


### [4] [Environmental Impact of CI/CD Pipelines](https://arxiv.org/abs/2510.26413)
*Nuno Saavedra,Alexandra Mendes,João F. Ferreira*

Main category: cs.SE

TL;DR: 该研究分析了GitHub Actions CI/CD服务的碳足迹和水足迹，发现其环境影响相当可观。基于超过220万次工作流运行的数据，估计2024年碳足迹在150.5-994.9 MTCO2e之间，水足迹在1,989.6-37,664.5千升之间。


<details>
  <summary>Details</summary>
Motivation: 随着云计算环境影响日益增长，理解CI/CD服务的碳足迹和水足迹变得愈发重要，但CI服务提供商通常不披露此类信息。

Method: 基于Cloud Carbon Footprint框架的方法论，使用文献中最大的工作流运行数据集，包含超过220万次工作流运行和18,000多个仓库。

Result: GitHub Actions生态系统产生显著的环境足迹。最可能情景下碳足迹为456.9 MTCO2e，水足迹为5,738.2千升，分别相当于7,615棵城市树木一年的碳捕获量和美国家庭5,053年的用水量。

Conclusion: 建议通过减少浪费的计算资源来缓解环境影响，包括在法国和英国等环境影响低的地区部署运行器、实施更严格的定时运行停用政策，以及减小仓库大小。

Abstract: CI/CD pipelines are widely used in software development, yet their
environmental impact, particularly carbon and water footprints (CWF), remains
largely unknown to developers, as CI service providers typically do not
disclose such information. With the growing environmental impact of cloud
computing, understanding the CWF of CI/CD services has become increasingly
important.
  This work investigates the CWF of using GitHub Actions, focusing on
open-source repositories where usage is free and unlimited for standard
runners. We build upon a methodology from the Cloud Carbon Footprint framework
and we use the largest dataset of workflow runs reported in the literature to
date, comprising over 2.2 million workflow runs from more than 18,000
repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a
substantial CWF. Our estimates for the carbon footprint in 2024 range from
150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most
pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5
kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon
footprint and 5,738.2 kiloliters for water footprint. To provide perspective,
the carbon footprint in the most likely scenario is equivalent to the carbon
captured by 7,615 urban trees in a year, and the water footprint is comparable
to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted
computational resources. Key recommendations include deploying runners in
regions whose energy production has a low environmental impact such as France
and the United Kingdom, implementing stricter deactivation policies for
scheduled runs and aligning their execution with periods when the regional
energy mix is more environmentally favorable, and reducing the size of
repositories.

</details>


### [5] [A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows](https://arxiv.org/abs/2510.25935)
*Antía Dorado,Iván Folgueira,Sofía Martín,Gonzalo Martín,Álvaro Porto,Alejandro Ramos,John Wallace*

Main category: cs.SE

TL;DR: CodeSight是一个端到端系统，通过整合过程挖掘和机器学习来预测软件开发工作流中的截止期限合规性。


<details>
  <summary>Details</summary>
Motivation: 旨在主动识别软件开发过程中潜在的截止期限违规，实现更有效的项目管理。

Method: 从GitHub捕获开发和部署数据，转换为过程挖掘日志，使用LSTM模型基于序列活动轨迹和静态特征预测PR剩余解决时间。

Result: 系统在预测截止期限合规性方面表现出高精度和高F1分数。

Conclusion: 过程挖掘与机器学习的整合为主动式软件项目管理提供了重要价值。

Abstract: CodeSight is an end-to-end system designed to anticipate deadline compliance
in software development workflows. It captures development and deployment data
directly from GitHub, transforming it into process mining logs for detailed
analysis. From these logs, the system generates metrics and dashboards that
provide actionable insights into PR activity patterns and workflow efficiency.
Building on this structured representation, CodeSight employs an LSTM model
that predicts remaining PR resolution times based on sequential activity traces
and static features, enabling early identification of potential deadline
breaches. In tests, the system demonstrates high precision and F1 scores in
predicting deadline compliance, illustrating the value of integrating process
mining with machine learning for proactive software project management.

</details>


### [6] [Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation](https://arxiv.org/abs/2510.26130)
*Musfiqur Rahman,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: LLMs在函数级代码生成表现良好，但在真实软件项目中的类级实现能力有限，正确率仅为25%-34%，远低于合成基准测试的84%-89%。检索增强生成在部分文档情况下最有效，能提升4%-7%的正确率。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在真实软件项目中生成类级代码的能力，了解其在实践条件下的泛化表现，填补现有研究对类级工程能力理解的空白。

Method: 使用来自开源仓库的真实世界类构建新基准，分为已见和未见分区，评估多种LLM在不同输入规范、检索增强配置和文档完整性水平下的表现。

Result: LLMs在真实类任务中正确率仅为25%-34%，检索增强生成在部分文档时提升4%-7%正确率，完整文档仅带来1%-3%的微小提升。AttributeError、TypeError和AssertionError是主要失败模式（占84%）。

Conclusion: 当前LLMs在类级工程能力上存在严重局限，需要改进上下文建模、文档策略和检索集成，为生产代码辅助工具提供可操作的改进方向。

Abstract: Large language models (LLMs) have advanced code generation at the function
level, yet their ability to produce correct class-level implementations in
authentic software projects remains poorly understood. This work introduces a
novel benchmark derived from open-source repositories, comprising real-world
classes divided into seen and unseen partitions to evaluate generalization
under practical conditions. The evaluation examines multiple LLMs under varied
input specifications, retrieval-augmented configurations, and documentation
completeness levels.
  Results reveal a stark performance disparity: LLMs achieve 84% to 89%
correctness on established synthetic benchmarks but only 25% to 34% on
real-world class tasks, with negligible differences between familiar and novel
codebases. Comprehensive docstrings yield modest gains of 1% to 3% in
functional accuracy, though statistical significance is rare.
Retrieval-augmented generation proves most effective with partial
documentation, improving correctness by 4% to 7% by supplying concrete
implementation patterns absent from specifications. Error profiling identifies
AttributeError, TypeError, and AssertionError as dominant failure modes (84% of
cases), with synthetic tests overemphasizing assertion issues and real-world
scenarios highlighting type and attribute mismatches. Retrieval augmentation
reduces logical flaws but can introduce dependency conflicts.
  The benchmark and analysis expose critical limitations in current LLM
capabilities for class-level engineering, offering actionable insights for
enhancing context modelling, documentation strategies, and retrieval
integration in production code assistance tools.

</details>


### [7] [Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests](https://arxiv.org/abs/2510.26171)
*Hasnain Iqbal,Zerina Begum,Kazi Sakib*

Main category: cs.SE

TL;DR: 提出一种基于共享静态字段分析的方法来优先排序潜在的顺序依赖测试，以减少重复运行次数，提高检测效率。


<details>
  <summary>Details</summary>
Motivation: 顺序依赖测试会导致持续集成管道失败，但现有检测方法需要多次重复运行测试，成本高昂，因此需要优先识别潜在的顺序依赖测试。

Method: 通过分析测试类中的共享静态字段来识别更可能具有顺序依赖性的测试。

Result: 在27个项目模块的实验中，该方法在23个案例中成功优先排序了所有顺序依赖测试，平均减少测试执行65.92%，减少不必要重复运行72.19%。

Conclusion: 该方法通过显著降低执行成本，有效提高了顺序依赖测试检测的效率。

Abstract: Flaky tests can make automated software testing unreliable due to their
unpredictable behavior. These tests can pass or fail on the same code base on
multiple runs. However, flaky tests often do not refer to any fault, even
though they can cause the continuous integration (CI) pipeline to fail. A
common type of flaky test is the order-dependent (OD) test. The outcome of an
OD test depends on the order in which it is run with respect to other test
cases. Several studies have explored the detection and repair of OD tests.
However, their methods require re-runs of tests multiple times, that are not
related to the order dependence. Hence, prioritizing potential OD tests is
necessary to reduce the re-runs. In this paper, we propose a method to
prioritize potential order-dependent tests. By analyzing shared static fields
in test classes, we identify tests that are more likely to be order-dependent.
In our experiment on 27 project modules, our method successfully prioritized
all OD tests in 23 cases, reducing test executions by an average of 65.92% and
unnecessary re-runs by 72.19%. These results demonstrate that our approach
significantly improves the efficiency of OD test detection by lowering
execution costs.

</details>


### [8] [The "4W+1H" of Software Supply Chain Security Checklist for Critical Infrastructure](https://arxiv.org/abs/2510.26174)
*Liming Dong,Sung Une Lee,Zhenchang Xing,Muhammad Ejaz Ahmed,Stefan Avgoustakis*

Main category: cs.SE

TL;DR: 该论文通过多源文献综述分析了软件供应链安全实践，发现现有框架对关键基础设施领域针对性不足，提出了包含80个问题的结构化检查清单来帮助利益相关者评估和增强软件供应链安全。


<details>
  <summary>Details</summary>
Motivation: 软件供应链攻击日益频繁和复杂，对关键基础设施构成严重威胁，而现有安全实践分散且不足，缺乏针对关键基础设施领域的专门框架。

Method: 采用多源文献综述方法，分析国际框架、澳大利亚监管源和学术研究，使用"4W+1H"分析方法系统梳理软件供应链安全实践。

Result: 识别出10个核心类别的软件供应链安全实践，发现现有框架对关键基础设施领域覆盖不足，开发了包含80个问题的结构化检查清单。

Conclusion: 现有框架指导与行业特定需求存在差距，需要采用集成化、情境感知的方法来保护关键基础设施免受不断演变的软件供应链风险。

Abstract: The increasing frequency and sophistication of software supply chain attacks
pose severe risks to critical infrastructure sectors, threatening national
security, economic stability, and public safety. Despite growing awareness,
existing security practices remain fragmented and insufficient, with most
frameworks narrowly focused on isolated life cycle stages or lacking alignment
with the specific needs of critical infrastructure (CI) sectors. In this paper,
we conducted a multivocal literature review across international frameworks,
Australian regulatory sources, and academic studies to identify and analyze
security practices across the software supply chain, especially specific CI
sector. Our analysis found that few existing frameworks are explicitly tailored
to CI domains. We systematically leveraged identified software supply chain
security frameworks, using a "4W+1H" analytical approach, we synthesized ten
core categories (what) of software supply chain security practices, mapped them
across life-cycle phases (when), stakeholder roles (who), and implementation
levels (how), and examined their coverage across existing frameworks (where).
Building on these insights, the paper culminates in structured, multi-layered
checklist of 80 questions designed to relevant stakeholders evaluate and
enhance their software supply chain security. Our findings reveal gaps between
framework guidance and sector-specific needs, highlight the need for
integrated, context-aware approaches to safeguard critical infrastructure from
evolving software supply chain risks.

</details>


### [9] [A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI](https://arxiv.org/abs/2510.26275)
*Domenico Amalfitano,Andreas Metzger,Marco Autili,Tommaso Fulcini,Tobias Hey,Jan Keim,Patrizio Pelliccione,Vincenzo Scotti,Anne Koziolek,Raffaela Mirandola,Andreas Vogelsang*

Main category: cs.SE

TL;DR: 本文应用设计科学研究方法构建了GenAI增强软件工程的路线图，通过多轮证据整合和McLuhan四元组分析，识别了四种GenAI增强形式及其相关研究挑战，并提出了2030年软件工程的十个预测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在迅速改变软件工程实践，影响软件开发、运维和演进过程。需要系统性地构建路线图来指导GenAI在软件工程中的应用和发展。

Method: 采用设计科学研究方法，通过三个循环逐步整合多个证据源，包括FSE 2025研讨会讨论、快速文献综述和同行反馈。使用McLuhan四元组作为概念工具系统分析GenAI对软件工程的影响。

Result: 识别了四种GenAI增强软件工程的基本形式，系统描述了相关研究挑战和机遇，并整合为未来研究方向。通过多轮验证提供了透明可复现的分析基础。

Conclusion: 研究为分析GenAI如何影响软件工程过程、方法和工具提供了坚实基础，并基于发现提出了2030年软件工程的十个预测，为该快速演进领域的研究提供了框架。

Abstract: Generative AI (GenAI) is rapidly transforming software engineering (SE)
practices, influencing how SE processes are executed, as well as how software
systems are developed, operated, and evolved. This paper applies design science
research to build a roadmap for GenAI-augmented SE. The process consists of
three cycles that incrementally integrate multiple sources of evidence,
including collaborative discussions from the FSE 2025 "Software Engineering
2030" workshop, rapid literature reviews, and external feedback sessions
involving peers. McLuhan's tetrads were used as a conceptual instrument to
systematically capture the transforming effects of GenAI on SE processes and
software products.The resulting roadmap identifies four fundamental forms of
GenAI augmentation in SE and systematically characterizes their related
research challenges and opportunities. These insights are then consolidated
into a set of future research directions. By grounding the roadmap in a
rigorous multi-cycle process and cross-validating it among independent author
teams and peers, the study provides a transparent and reproducible foundation
for analyzing how GenAI affects SE processes, methods and tools, and for
framing future research within this rapidly evolving area. Based on these
findings, the article finally makes ten predictions for SE in the year 2030.

</details>


### [10] [Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search](https://arxiv.org/abs/2510.26287)
*Guochang Li,Yuchen Liu,Zhen Qin,Yunkun Wang,Jianping Zhong,Chen Zhi,Binhua Li,Fei Huang,Yongbin Li,Shuiguang Deng*

Main category: cs.SE

TL;DR: RepoSearch-R1是一个基于蒙特卡洛树搜索的强化学习框架，通过自训练生成高质量推理轨迹，无需模型蒸馏或外部监督，显著提升了代码库问答任务的答案完整性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在代码库级软件工程任务中存在局限：无训练方法难以有效指导工具使用和决策，基于训练的方法依赖昂贵的大模型蒸馏且存在数据合规问题。

Method: 提出RepoSearch-R1框架，结合蒙特卡洛树搜索进行智能体强化学习，通过自训练生成多样化高质量推理轨迹，并构建专门的RepoQA-Agent用于代码库问答。

Result: 在代码库问答任务中，RepoSearch-R1相比无检索方法提升16.0%答案完整性，相比迭代检索方法提升19.5%，训练效率比通用强化学习方法提高33%。

Conclusion: 冷启动训练方法消除了数据合规担忧，同时在代码库级推理任务中保持了强大的探索多样性和答案完整性。

Abstract: Repository-level software engineering tasks require large language models
(LLMs) to efficiently navigate and extract information from complex codebases
through multi-turn tool interactions. Existing approaches face significant
limitations: training-free, in-context learning methods struggle to guide
agents effectively in tool utilization and decision-making based on
environmental feedback, while training-based approaches typically rely on
costly distillation from larger LLMs, introducing data compliance concerns in
enterprise environments. To address these challenges, we introduce
RepoSearch-R1, a novel agentic reinforcement learning framework driven by
Monte-carlo Tree Search (MCTS). This approach allows agents to generate
diverse, high-quality reasoning trajectories via self-training without
requiring model distillation or external supervision. Based on RepoSearch-R1,
we construct a RepoQA-Agent specifically designed for repository
question-answering tasks. Comprehensive evaluation on repository
question-answering tasks demonstrates that RepoSearch-R1 achieves substantial
improvements of answer completeness: 16.0% enhancement over no-retrieval
methods, 19.5% improvement over iterative retrieval methods, and 33% increase
in training efficiency compared to general agentic reinforcement learning
approaches. Our cold-start training methodology eliminates data compliance
concerns while maintaining robust exploration diversity and answer completeness
across repository-level reasoning tasks.

</details>


### [11] [Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis](https://arxiv.org/abs/2510.26423)
*Dong Huang,Mingzhe Du,Jie M. Zhang,Zheng Lin,Meng Luo,Qianru Zhang,See-Kiong Ng*

Main category: cs.SE

TL;DR: Nexus是一个多智能体框架，通过专业代理的协作审议、验证和迭代自优化来生成测试预言，显著提升了非回归测试中的测试预言准确性。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程中长期存在的测试预言生成挑战，即如何为被测函数生成能够准确判断其行为是否符合预期的测试预言。

Method: 使用四个体现不同测试哲学的专业代理进行协作审议和优化测试预言，然后通过生成候选实现并在安全沙箱中执行验证，对失败的预言启动自动化自优化循环。

Result: 在七个不同基准测试中，Nexus显著优于现有最优方法，例如将LiveCodeBench上的测试预言准确率从46.30%提升到57.73%，并将HumanEval上的错误检测率从90.91%提升到95.45%。

Conclusion: Nexus框架通过多智能体协作和迭代自优化，有效解决了测试预言生成问题，显著提升了测试预言的质量和下游任务的性能。

Abstract: Test oracle generation in non-regression testing is a longstanding challenge
in software engineering, where the goal is to produce oracles that can
accurately determine whether a function under test (FUT) behaves as intended
for a given input. In this paper, we introduce Nexus, a novel multi-agent
framework to address this challenge. Nexus generates test oracles by leveraging
a diverse set of specialized agents that synthesize test oracles through a
structured process of deliberation, validation, and iterative self-refinement.
During the deliberation phase, a panel of four specialist agents, each
embodying a distinct testing philosophy, collaboratively critiques and refines
an initial set of test oracles. Then, in the validation phase, Nexus generates
a plausible candidate implementation of the FUT and executes the proposed
oracles against it in a secure sandbox. For any oracle that fails this
execution-based check, Nexus activates an automated selfrefinement loop, using
the specific runtime error to debug and correct the oracle before
re-validation. Our extensive evaluation on seven diverse benchmarks
demonstrates that Nexus consistently and substantially outperforms
state-of-theart baselines. For instance, Nexus improves the test-level oracle
accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The
improved accuracy also significantly enhances downstream tasks: the bug
detection rate of GPT4.1-Mini generated test oracles on HumanEval increases
from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of
automated program repair improves from 35.23% to 69.32%.

</details>


### [12] [CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses](https://arxiv.org/abs/2510.26431)
*Mihály Dobos-Kovács,Levente Bajczi,András Vörös*

Main category: cs.SE

TL;DR: CHCVERIF是一个基于组合策略的CHC求解器，采用软件验证方法来解决约束Horn子句问题，能够重用成熟的软件验证工具处理涉及位向量和低级语义的基准测试。


<details>
  <summary>Details</summary>
Motivation: 约束Horn子句(CHCs)被广泛用作各种验证任务的中间表示，包括安全检查、不变量合成和过程间分析。作者希望利用成熟的软件验证工具来解决CHC基准测试，特别是涉及位向量和低级语义的问题。

Method: 开发了CHCVERIF，这是一个基于组合策略的CHC求解器，采用软件验证方法，通过重用现有的软件验证工具来处理CHC基准测试。

Result: 在线性整数算术方面表现中等，在位向量基准测试上取得了适度的成功。结果表明使用软件验证工具作为CHC求解后端是可行的，特别是在精心构建的组合策略支持下。

Conclusion: 使用软件验证工具作为CHC求解后端具有可行性和潜力，特别是在精心构建的组合策略支持下，能够有效处理涉及位向量和低级语义的CHC问题。

Abstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate
representations for a variety of verification tasks, including safety checking,
invariant synthesis, and interprocedural analysis. This paper introduces
CHCVERIF, a portfolio-based CHC solver that adopts a software verification
approach for solving CHCs. This approach enables us to reuse mature software
verification tools to tackle CHC benchmarks, particularly those involving
bitvectors and low-level semantics. Our evaluation shows that while the method
enjoys only moderate success with linear integer arithmetic, it achieves modest
success on bitvector benchmarks. Moreover, our results demonstrate the
viability and potential of using software verification tools as backends for
CHC solving, particularly when supported by a carefully constructed portfolio.

</details>


### [13] [SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning](https://arxiv.org/abs/2510.26457)
*Fang Liu,Simiao Liu,Yinghao Zhu,Xiaoli Lian,Li Zhang*

Main category: cs.SE

TL;DR: SecureReviewer是一个专门用于增强LLM在代码审查中识别和解决安全问题的能力的新方法，通过构建专用数据集、安全感知微调策略和RAG技术，在安全检测准确性和评论质量方面优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动化代码审查方法主要关注通用代码审查，在识别和解决安全相关问题方面效果未充分探索，且面临数据稀缺和评估指标不足的挑战。

Method: 构建安全代码审查专用数据集，采用安全感知微调策略训练LLM生成安全审查评论，集成RAG技术减少幻觉并增强输出可靠性，引入SecureBLEU评估指标。

Result: 实验结果表明SecureReviewer在安全检测准确性和生成评论的整体质量及实用性方面均优于最先进的基线方法。

Conclusion: SecureReviewer有效解决了现有代码审查方法在安全方面的局限性，为自动化安全代码审查提供了可靠解决方案。

Abstract: Identifying and addressing security issues during the early phase of the
development lifecycle is critical for mitigating the long-term negative impacts
on software systems. Code review serves as an effective practice that enables
developers to check their teammates' code before integration into the codebase.
To streamline the generation of review comments, various automated code review
approaches have been proposed, where LLM-based methods have significantly
advanced the capabilities of automated review generation. However, existing
models primarily focus on general-purpose code review, their effectiveness in
identifying and addressing security-related issues remains underexplored.
Moreover, adapting existing code review approaches to target security issues
faces substantial challenges, including data scarcity and inadequate evaluation
metrics. To address these limitations, we propose SecureReviewer, a new
approach designed for enhancing LLMs' ability to identify and resolve
security-related issues during code review. Specifically, we first construct a
dataset tailored for training and evaluating secure code review capabilities.
Leveraging this dataset, we fine-tune LLMs to generate code review comments
that can effectively identify security issues and provide fix suggestions with
our proposed secure-aware fine-tuning strategy. To mitigate hallucination in
LLMs and enhance the reliability of their outputs, we integrate the RAG
technique, which grounds the generated comments in domain-specific security
knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric
designed to assess the effectiveness of review comments in addressing security
issues. Experimental results demonstrate that SecureReviewer outperforms
state-of-the-art baselines in both security issue detection accuracy and the
overall quality and practical utility of generated review comments.

</details>


### [14] [Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study](https://arxiv.org/abs/2510.26480)
*Sivajeet Chand,Melih Kilic,Roland Würsching,Sushant Kumar Pandey,Alexander Pretschner*

Main category: cs.SE

TL;DR: 评估5个开源LLM在Python代码提取方法重构任务上的表现，发现RCI提示策略优于单次提示，最佳模型在测试通过率和代码质量指标上表现优异，开发者调查显示70%以上的重构代码被接受。


<details>
  <summary>Details</summary>
Motivation: 自动化提取方法重构对提高代码可读性和可维护性很重要，但现有方法仍主要依赖人工。开源高效LLM为自动化此类高级任务提供了新可能。

Method: 系统评估5个3B-8B参数的开源LLM，使用自动指标评估功能正确性和代码质量，比较单次提示与递归批评改进(RCI)提示策略的影响。

Result: RCI提示在测试通过率和重构质量上持续优于单次提示。最佳模型Deepseek-Coder-RCI和Qwen2.5-Coder-RCI的测试通过率分别达0.829和0.808，显著降低代码行数和圈复杂度。开发者调查显示70%以上接受率。

Conclusion: RCI引导的自动化重构具有显著优势，但传统指标与人工判断存在差异，需要人机协同评估。开源基准为未来LLM自动化重构研究提供了基础。

Abstract: Automating the Extract Method refactoring (EMR) remains challenging and
largely manual despite its importance in improving code readability and
maintainability. Recent advances in open-source, resource-efficient Large
Language Models (LLMs) offer promising new approaches for automating such
high-level tasks. In this work, we critically evaluate five state-of-the-art
open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python
code. We systematically assess functional correctness and code quality using
automated metrics and investigate the impact of prompting strategies by
comparing one-shot prompting to a Recursive criticism and improvement (RCI)
approach. RCI-based prompting consistently outperforms one-shot prompting in
test pass rates and refactoring quality. The best-performing models,
Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP)
scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from
12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453
and 3.294, respectively. A developer survey on RCI-generated refactorings shows
over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation
criteria. In contrast, the original code scored below neutral, particularly in
readability and maintainability, underscoring the benefits of automated
refactoring guided by quality prompts. While traditional metrics like CC and
LOC provide useful signals, they often diverge from human judgments,
emphasizing the need for human-in-the-loop evaluation. Our open-source
benchmark offers a foundation for future research on automated refactoring with
LLMs.

</details>


### [15] [Envisioning Future Interactive Web Development: Editing Webpage with Natural Language](https://arxiv.org/abs/2510.26516)
*Truong Hai Dang,Jingyu Xiao,Yintong Huo*

Main category: cs.SE

TL;DR: 本文提出了一种自动化数据生成流程Instruct4Edit，用于训练LLMs进行网页代码编辑，通过指令生成、代码修改和视觉验证来创建高质量微调数据集，显著提升了模型在自然语言网页编辑任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 网页应用开发依赖迭代式代码修改，传统方法耗时且手动。虽然LLMs能生成UI代码，但根据新设计需求编辑现有代码仍具挑战，主要缺乏大规模高质量调优数据来对齐模型性能与人类期望。

Method: 开发自动化数据生成管道，使用LLMs合成高质量微调数据集Instruct4Edit，包括生成多样化指令、应用相应代码修改、进行视觉验证确保正确性。

Result: 在Instruct4Edit上微调模型后，在将人类意图转化为精确、结构连贯且视觉准确的代码更改方面表现出持续改进。

Conclusion: 为基于自然语言的网页编辑提供了可扩展且透明的基础，证明微调较小的开源模型可以达到与专有系统相竞争的性能。所有数据、代码实现和模型检查点均已发布用于复现。

Abstract: The evolution of web applications relies on iterative code modifications, a
process that is traditionally manual and time-consuming. While Large Language
Models (LLMs) can generate UI code, their ability to edit existing code from
new design requirements (e.g., "center the logo") remains a challenge. This is
largely due to the absence of large-scale, high-quality tuning data to align
model performance with human expectations. In this paper, we introduce a novel,
automated data generation pipeline that uses LLMs to synthesize a high-quality
fine-tuning dataset for web editing, named Instruct4Edit. Our approach
generates diverse instructions, applies the corresponding code modifications,
and performs visual verification to ensure correctness. By fine-tuning models
on Instruct4Edit, we demonstrate consistent improvement in translating human
intent into precise, structurally coherent, and visually accurate code changes.
This work provides a scalable and transparent foundation for natural language
based web editing, demonstrating that fine-tuning smaller open-source models
can achieve competitive performance with proprietary systems. We release all
data, code implementations, and model checkpoints for reproduction.

</details>


### [16] [Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models](https://arxiv.org/abs/2510.26538)
*David Williams,Max Hort,Maria Kechagia,Aldeida Aleti,Justyna Petke,Federica Sarro*

Main category: cs.SE

TL;DR: 该论文分析了软件工程领域基于大语言模型研究中存在的基准测试严谨性、污染、可复现性和可持续性等挑战，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 软件工程研究中大语言模型的使用带来了基准测试严谨性、污染、可复现性和可持续性等新挑战，需要研究社区反思这些问题的解决方式。

Method: 通过分析ICSE会议上基于LLM的软件工程研究，提供结构化概述，识别良好实践和持续存在的问题。

Result: 研究结果揭示了当前LLM-based SE研究在基准测试严谨性、可复现性以及财务和环境成本方面存在的不足。

Conclusion: 提出加强基准测试严谨性、改进可复现性、解决LLM-based SE财务和环境成本的建议。

Abstract: Software Engineering (SE) research involving the use of Large Language Models
(LLMs) has introduced several new challenges related to rigour in benchmarking,
contamination, replicability, and sustainability. In this paper, we invite the
research community to reflect on how these challenges are addressed in SE. Our
results provide a structured overview of current LLM-based SE research at ICSE,
highlighting both encouraging practices and persistent shortcomings. We
conclude with recommendations to strengthen benchmarking rigour, improve
replicability, and address the financial and environmental costs of LLM-based
SE.

</details>


### [17] ["Show Me You Comply... Without Showing Me Anything": Zero-Knowledge Software Auditing for AI-Enabled Systems](https://arxiv.org/abs/2510.26576)
*Filippo Scaramuzza,Renato Cordeiro Ferreira,Tomaz Maia Suller,Giovanni Quattrocchi,Damian Andrew Tamburri,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: ZKMLOps是一个基于零知识证明的MLOps验证框架，通过密码学协议在不泄露敏感信息的情况下证明AI系统的合规性，解决AI系统审计中透明度与隐私保护的矛盾。


<details>
  <summary>Details</summary>
Motivation: AI系统在关键领域的应用面临可信度挑战，传统验证方法成本高且不适合AI黑盒特性，而监管要求与保护机密数据/专有模型之间存在冲突。

Method: 将零知识证明协议集成到机器学习运维生命周期中，采用模块化可重复流程生成可验证的密码学合规证明。

Result: 通过金融风险审计案例验证了框架实用性，并对主流ZKP协议进行实证评估，分析了不同复杂度ML模型的性能权衡。

Conclusion: ZKMLOps框架为AI系统提供了既满足监管审计要求又保护敏感资产的可行解决方案，平衡了透明度与隐私保护的需求。

Abstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems
in critical domains has made trustworthiness concerns a paramount showstopper,
requiring verifiable accountability, often by regulation (e.g., the EU AI Act).
Classical software verification and validation techniques, such as procedural
audits, formal methods, or model documentation, are the mechanisms used to
achieve this. However, these methods are either expensive or heavily manual and
ill-suited for the opaque, "black box" nature of most AI models. An intractable
conflict emerges: high auditability and verifiability are required by law, but
such transparency conflicts with the need to protect assets being audited-e.g.,
confidential data and proprietary models-leading to weakened accountability. To
address this challenge, this paper introduces ZKMLOps, a novel MLOps
verification framework that operationalizes Zero-Knowledge Proofs
(ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a
statement is true without revealing additional information-within
Machine-Learning Operations lifecycles. By integrating ZKPs with established
software engineering patterns, ZKMLOps provides a modular and repeatable
process for generating verifiable cryptographic proof of compliance. We
evaluate the framework's practicality through a study of regulatory compliance
in financial risk auditing and assess feasibility through an empirical
evaluation of top ZKP protocols, analyzing performance trade-offs for ML models
of increasing complexity.

</details>


### [18] [Online and Interactive Bayesian Inference Debugging](https://arxiv.org/abs/2510.26579)
*Nathanael Nussbaumer,Markus Böck,Jürgen Cito*

Main category: cs.SE

TL;DR: 本文提出了一种新颖的贝叶斯推理调试方法，通过在开发环境中直接集成调试工具，显著减少了调试时间和所需专业知识。


<details>
  <summary>Details</summary>
Motivation: 概率编程虽然促进了贝叶斯模型的开发和推理，但推理调试过程耗时且需要深厚专业知识，这阻碍了该技术的广泛应用。

Method: 提出了满足贝叶斯推理调试框架需求的工具，直接在开发环境中实现在线和交互式调试功能。

Result: 通过18位有经验参与者的研究评估，证明该方法在推理调试任务上显著减少了时间和难度。

Conclusion: 这种在线交互式贝叶斯推理调试方法有效解决了概率编程中的调试难题，使该技术更易于使用。

Abstract: Probabilistic programming is a rapidly developing programming paradigm which
enables the formulation of Bayesian models as programs and the automation of
posterior inference. It facilitates the development of models and conducting
Bayesian inference, which makes these techniques available to practitioners
from multiple fields. Nevertheless, probabilistic programming is notoriously
difficult as identifying and repairing issues with inference requires a lot of
time and deep knowledge. Through this work, we introduce a novel approach to
debugging Bayesian inference that reduces time and required knowledge
significantly. We discuss several requirements a Bayesian inference debugging
framework has to fulfill, and propose a new tool that meets these key
requirements directly within the development environment. We evaluate our
results in a study with 18 experienced participants and show that our approach
to online and interactive debugging of Bayesian inference significantly reduces
time and difficulty on inference debugging tasks.

</details>


### [19] [Stitch: Step-by-step LLM Guided Tutoring for Scratch](https://arxiv.org/abs/2510.26634)
*Yuan Si,Kyle Qi,Daming Li,Hanyuan Shi,Jialu Zhang*

Main category: cs.SE

TL;DR: Stitch是一个交互式编程辅导系统，通过逐步脚手架式指导替代直接展示答案，帮助学习者修复Scratch等块编程环境中的语义错误。


<details>
  <summary>Details</summary>
Motivation: 现有调试方法直接展示正确答案，虽然能修复错误但削弱了问题解决能力的培养。需要一种能促进学习效果的教学方法。

Method: 系统通过Diff-Analyze模块对比学生项目与参考实现，识别关键差异，使用大语言模型解释差异重要性，学习者通过自定义渲染引擎检查高亮块，理解解释并选择性应用部分修复。

Result: 实证研究表明，Stitch比最先进的自动反馈生成工具更有效，逐步辅导显著提升了学习成果。

Conclusion: 直接展示正确答案的教学效果不佳，交互式逐步指导系统能提供更有效的学习体验，为块编程环境中的有效反馈提供了新证据。

Abstract: Block-based environments such as Scratch are increasingly popular in
programming education. While block syntax reduces surface errors, semantic bugs
remain common and challenging for novices to resolve. Existing debugging
workflows typically show the correct program directly to learners, a strategy
that may fix errors but undermines the development of problem-solving skills.
  We present Stitch, an interactive tutoring system that replaces "showing the
answer" with step-by-step scaffolding. The system's Diff-Analyze module
contrasts a student's project with a reference implementation, identifies the
most critical differences, and uses a large language model to explain why these
changes matter. Learners inspect highlighted blocks through a custom rendering
engine, understand the explanations, and selectively apply partial fixes. This
iterative process continues until the intended functionality is achieved.
  We evaluate Stitch in an empirical study, comparing it against a
state-of-the-art automated feedback generation tool for Scratch. Our key
insight is that simply presenting the correct program is pedagogically
ineffective. In contrast, our interactive, step-by-step guided system promotes
a more effective learning experience. More broadly, what constitutes effective
feedback in block-based programming remains an open question. Our evaluation
provides new evidence that step-by-step tutoring significantly enhances
learning outcomes, outperforming both direct-answer approaches and current
automated feedback generation tools.

</details>


### [20] [Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study](https://arxiv.org/abs/2510.26676)
*Samiha Shimmi,Nicholas M. Synovic,Mona Rahimi,George K. Thiruvathukal*

Main category: cs.SE

TL;DR: 该研究通过分析ImageMagick项目的76个漏洞重新引入案例，发现过程指标（如问题腐败率、问题密度）与代码变更共同揭示了漏洞重新引入的复杂机制，强调这不是孤立事件而是累积开发活动的结果。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞即使在修复后仍会持续存在或重新出现，现有研究很少探索过程指标是否能揭示随时间推移的风险开发活动，这对于预测和缓解软件漏洞至关重要。

Method: 在ImageMagick项目中进行案例研究，将纵向过程指标（总线因子、问题密度、问题腐败率）与漏洞重新引入活动相关联，分析提交级别的安全修复而非文件级别预测。

Result: 发现重新引入通常与增加的问题腐败率和波动的问题密度相一致，反映了问题管理和团队响应能力的短期低效。

Conclusion: 过程指标与代码指标结合能够预测风险修复并增强软件安全性，为更广泛研究提供了基础。

Abstract: Software vulnerabilities often persist or re-emerge even after being fixed,
revealing the complex interplay between code evolution and socio-technical
factors. While source code metrics provide useful indicators of
vulnerabilities, software engineering process metrics can uncover patterns that
lead to their introduction. Yet few studies have explored whether process
metrics can reveal risky development activities over time -- insights that are
essential for anticipating and mitigating software vulnerabilities. This work
highlights the critical role of process metrics along with code changes in
understanding and mitigating vulnerability reintroduction. We move beyond
file-level prediction and instead analyze security fixes at the commit level,
focusing not only on whether a single fix introduces a vulnerability but also
on the longer sequences of changes through which vulnerabilities evolve and
re-emerge. Our approach emphasizes that reintroduction is rarely the result of
one isolated action, but emerges from cumulative development activities and
socio-technical conditions. To support this analysis, we conducted a case study
on the ImageMagick project by correlating longitudinal process metrics such as
bus factor, issue density, and issue spoilage with vulnerability reintroduction
activities, encompassing 76 instances of reintroduced vulnerabilities. Our
findings show that reintroductions often align with increased issue spoilage
and fluctuating issue density, reflecting short-term inefficiencies in issue
management and team responsiveness. These observations provide a foundation for
broader studies that combine process and code metrics to predict risky fixes
and strengthen software security.

</details>


### [21] [Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment](https://arxiv.org/abs/2510.26699)
*Aylton Almeida,Laerte Xavier,Marco Tulio Valente*

Main category: cs.SE

TL;DR: 评估使用GitHub Copilot Agent Mode自动迁移SQLAlchemy库版本的效果，发现虽然API迁移覆盖率高(100%)，但应用功能测试通过率低(39.75%)。


<details>
  <summary>Details</summary>
Motivation: 软件系统更新对避免技术债务和安全漏洞至关重要，但传统更新过程耗时且易出错，需要探索利用LLM和智能体系统自动化维护任务的可能性。

Method: 使用GitHub Copilot Agent Mode作为自主AI系统，在10个客户端应用中执行SQLAlchemy库的多步骤迁移工作流，并引入迁移覆盖率指标评估效果。

Result: LLM智能体能够成功迁移功能性和API使用点(迁移覆盖率中位数100%)，但无法保持应用功能完整性(测试通过率中位数仅39.75%)。

Conclusion: 虽然LLM智能体在API迁移方面表现良好，但在保持应用功能完整性方面仍有不足，表明自动化库迁移需要更全面的功能验证机制。

Abstract: Keeping software systems up to date is essential to avoid technical debt,
security vulnerabilities, and the rigidity typical of legacy systems. However,
updating libraries and frameworks remains a time consuming and error-prone
process. Recent advances in Large Language Models (LLMs) and agentic coding
systems offer new opportunities for automating such maintenance tasks. In this
paper, we evaluate the update of a well-known Python library, SQLAlchemy,
across a dataset of ten client applications. For this task, we use the Github's
Copilot Agent Mode, an autonomous AI systema capable of planning and executing
multi-step migration workflows. To assess the effectiveness of the automated
migration, we also introduce Migration Coverage, a metric that quantifies the
proportion of API usage points correctly migrated. The results of our study
show that the LLM agent was capable of migrating functionalities and API usages
between SQLAlchemy versions (migration coverage: 100%, median), but failed to
maintain the application functionality, leading to a low test-pass rate
(39.75%, median).

</details>


### [22] [Optimized Log Parsing with Syntactic Modifications](https://arxiv.org/abs/2510.26793)
*Nafid Enan,Gias Uddin*

Main category: cs.SE

TL;DR: 该论文对语法和语义日志解析器进行实证研究，发现语义方法在模板识别上更准确，语法方法效率更高但模板识别不足。提出SynLog+作为两阶段解析架构的第二阶段，显著提升解析精度。


<details>
  <summary>Details</summary>
Motivation: 日志解析是自动化日志分析的第一步，但现有解析器采用不同技术，需要评估其特性和性能以指导选择。

Method: 通过实证研究比较语法和语义日志解析器，以及单阶段和两阶段解析架构。基于研究发现提出SynLog+模板识别模块。

Result: 语义方法模板识别更准确，语法方法效率高10-1000倍但模板识别不足。两阶段架构比单阶段精度更高。SynLog+使语法和语义解析器精度分别提升236%和20%。

Conclusion: 两阶段解析架构能显著提升日志解析精度，SynLog+模块在不增加运行时成本的情况下有效改善语法和语义解析器的性能。

Abstract: Logs provide valuable insights into system runtime and assist in software
development and maintenance. Log parsing, which converts semi-structured log
data into structured log data, is often the first step in automated log
analysis. Given the wide range of log parsers utilizing diverse techniques, it
is essential to evaluate them to understand their characteristics and
performance. In this paper, we conduct a comprehensive empirical study
comparing syntax- and semantic-based log parsers, as well as single-phase and
two-phase parsing architectures. Our experiments reveal that semantic-based
methods perform better at identifying the correct templates and syntax-based
log parsers are 10 to 1,000 times more efficient and provide better grouping
accuracy although they fall short in accurate template identification.
Moreover, two-phase architecture consistently improves accuracy compared to
single-phase architecture. Based on the findings of this study, we propose
SynLog+, a template identification module that acts as the second phase in a
two-phase log parsing architecture. SynLog+ improves the parsing accuracy of
syntax-based and semantic-based log parsers by 236\% and 20\% on average,
respectively, with virtually no additional runtime cost.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [23] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow是一个用于MoE推理的运行时系统，通过自适应专家预取和缓存感知路由来减少内存传输延迟，将模型停滞时间降低到基线的0.1%以下。


<details>
  <summary>Details</summary>
Motivation: 现代GPU内存容量有限限制了大型语言模型的扩展，传统MoE推理方法由于每层独立选择专家导致频繁参数传输，产生显著延迟。现有的跨层预测策略缺乏对不同硬件平台和工作负载的适应性。

Method: 结合自适应专家预取和缓存感知路由，利用传输带宽、参数维度和模型反馈信号等运行时统计信息动态调整预测范围，采用融合预门控信息和中间计算状态的混合跨层预测方案来预测未来专家需求。

Result: 评估显示ExpertFlow将模型停滞时间减少到基线的0.1%以下，有效优化了严格内存约束下的MoE推理性能。

Conclusion: ExpertFlow通过自适应预取决策和实际使用行为对齐，有效减少了缓存未命中和专家交换引入的延迟，为内存受限环境下的MoE推理提供了高效解决方案。

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>
