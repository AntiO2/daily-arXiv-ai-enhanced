<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.SE](#cs.SE) [Total: 15]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication](https://arxiv.org/abs/2601.09114)
*Yufan Xia,Marco De La Pierre,Amanda S. Barnard,Giuseppe Maria Junior Barca*

Main category: cs.DC

TL;DR: 使用机器学习动态选择最佳线程数以优化多线程GEMM性能，在两种HPC架构上实现25-40%加速


<details>
  <summary>Details</summary>
Motivation: 现代多核共享内存系统复杂，难以确定最小化多线程GEMM运行时的最佳线程数，需要智能优化方法

Method: 构建ADSALA软件库，使用机器学习模型实时根据训练数据为给定GEMM任务自动选择最优线程数

Result: 在Intel Cascade Lake和AMD Zen 3两种HPC节点架构上测试，内存使用100MB内的GEMM相比传统BLAS实现获得25-40%加速

Conclusion: 机器学习方法能有效优化多线程GEMM性能，ADSALA库为线性代数运算提供了智能化的运行时优化方案

Abstract: The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB.

</details>


### [2] [Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems](https://arxiv.org/abs/2601.09146)
*Lingkang Shangguan*

Main category: cs.DC

TL;DR: 提出基于拜占庭一致性广播的交易驱动动态重配置协议PDCC，避免全局交易排序以实现高性能支付系统


<details>
  <summary>Details</summary>
Motivation: 现代支付系统需要高性能的动态重配置能力，同时避免全局交易排序带来的性能瓶颈

Method: 结合用户nonce的交易排序与周期性系统范围共识机制，设计PDCC协议实现平滑重配置

Result: PDCC能够在不影响原始系统性能的情况下实现平滑的重配置过程

Conclusion: 交易驱动的动态重配置协议为现代支付系统提供了高性能且灵活的配置管理方案

Abstract: We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance.

</details>


### [3] [Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus](https://arxiv.org/abs/2601.09184)
*Yifei Xie,Btissam Er-Rahmadi,Xiao Chen,Tiejun Ma,Jane Hillston*

Main category: cs.DC

TL;DR: 提出基于混合整数规划的视图变更优化模型，通过优化领导者选择和追随者重新分配来提升并行BFT协议性能，在Azure云环境中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 并行BFT协议中的视图变更过程存在性能瓶颈，现有协议采用被动的视图变更机制和盲目的领导者轮换，经常选择不可用或缓慢节点作为领导者，导致性能下降。

Method: 提出基于混合整数规划的视图变更优化模型，考虑通信延迟和故障场景优化领导者选择和追随者重新分配；采用分解方法处理子问题并改进Benders割；基于分解结果提出高效的迭代备份领导者选择算法。

Result: 在Microsoft Azure云环境中的实验表明，VCO驱动的并行BFT在正常操作和故障条件下都优于现有配置方法，且随着网络规模增大效果更显著。

Conclusion: VCO模型能有效解决并行BFT中的视图变更性能瓶颈问题，为高性能并行BFT系统提供了合适的解决方案。

Abstract: The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems.

</details>


### [4] [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)
*Du Yin,Jiayi Ren,Xiayu Sun,Tianyao Zhou,Haizhu Zhou,Ruiyan Ma,Danyang Zhang*

Main category: cs.DC

TL;DR: LatencyPrism：首个零侵入多平台延迟剖析系统，用于LLM推理延迟监控、异常检测和SLO保障，无需代码修改或服务重启


<details>
  <summary>Details</summary>
Motivation: LLM推理延迟直接影响用户体验和运营成本，但现有AI性能分析方法存在不足：1）侵入式设计需要服务重启或暂停；2）硬件绑定实现无法适应异构推理环境；3）分布式推理环境（多样软件框架和XPU架构）加上动态工作负载使延迟分析困难

Method: LatencyPrism采用零侵入多平台延迟剖析系统设计，能够：1）跨流水线分解推理延迟；2）主动预警推理延迟异常；3）保证SLO遵守。系统支持批量级低开销实时监控，毫秒级触发警报，无需代码修改或服务重启

Result: 已在数千个XPU上部署超过6个月，能够区分工作负载驱动的延迟变化和指示潜在问题的异常，F1分数达0.98。通过广泛实验和根因分析展示了系统能力

Conclusion: LatencyPrism解决了现有AI性能分析方法在实时生产环境中的不足，提供了有效的零侵入延迟监控和异常检测解决方案，特别适用于异构分布式LLM推理环境

Abstract: LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.
  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability.

</details>


### [5] [High-Performance Serverless Computing: A Systematic Literature Review on Serverless for HPC, AI, and Big Data](https://arxiv.org/abs/2601.09334)
*Valerio Besozzi,Matteo Della Bartola,Patrizio Dazzi,Marco Danelutto*

Main category: cs.DC

TL;DR: 这篇论文对2018-2025年间122篇研究文章进行了系统性文献综述，探讨了无服务器计算在云计算、高性能计算和混合环境中处理计算密集型应用的使用情况，提出了包含8个研究方向、9个用例领域的分类法。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算、人工智能和大数据等计算密集型应用的广泛部署，云和高性能计算基础设施正在融合。云提供商正在集成高性能计算能力，而高性能计算社区开始探索云原生范式以提高可扩展性、弹性和资源利用率。在此背景下，无服务器计算成为处理高度动态、并行和分布式工作负载的有前景执行模型。

Method: 对2018年至2025年初发表的122篇研究文章进行全面的系统性文献综述，分析无服务器范式在云计算、高性能计算和混合环境中开发、部署和编排计算密集型应用的使用情况。提出了包含8个主要研究方向和9个目标用例领域的分类法，并分析了最近的发表趋势和作者合作网络。

Result: 研究展示了无服务器计算在处理计算密集型应用方面的应用现状，提出了系统的分类框架，揭示了该新兴研究领域日益增长的兴趣和内部联系。通过分析发表趋势和合作网络，突出了该领域的发展动态。

Conclusion: 这项工作为新手研究人员和经验丰富的从业者提供了有价值的基础，指导下一代无服务器解决方案的开发，以支持并行计算密集型应用。无服务器计算在融合云和高性能计算基础设施的背景下展现出巨大潜力。

Abstract: The widespread deployment of large-scale, compute-intensive applications such as high-performance computing, artificial intelligence, and big data is leading to convergence between cloud and high-performance computing infrastructures. Cloud providers are increasingly integrating high-performance computing capabilities in their infrastructures, such as hardware accelerators and high-speed interconnects, while researchers in the high-performance computing community are starting to explore cloud-native paradigms to improve scalability, elasticity, and resource utilization. In this context, serverless computing emerges as a promising execution model to efficiently handle highly dynamic, parallel, and distributed workloads. This paper presents a comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, exploring the use of the serverless paradigm to develop, deploy, and orchestrate compute-intensive applications across cloud, high-performance computing, and hybrid environments. From these, a taxonomy comprising eight primary research directions and nine targeted use case domains is proposed, alongside an analysis of recent publication trends and collaboration networks among authors, highlighting the growing interest and interconnections within this emerging research field. Overall, this work aims to offer a valuable foundation for both new researchers and experienced practitioners, guiding the development of next-generation serverless solutions for parallel compute-intensive applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [6] [Honesty-Aware Multi-Agent Framework for High-Fidelity Synthetic Data Generation in Digital Psychiatric Intake Doctor-Patient Interactions](https://arxiv.org/abs/2601.09216)
*Xinyuan Zhang,Zijian Wang,Chang Dao,Juexiao Zhou*

Main category: cs.DB

TL;DR: 提出一个多智能体合成框架，通过显式建模患者欺骗行为来生成高保真、可公开发布的合成精神病学入院记录，用于研究欺骗感知的精神病学评估和训练自适应对话系统。


<details>
  <summary>Details</summary>
Motivation: 精神病学入院和评估面临数据稀缺和不可靠自我报告（如隐瞒或夸大）的基本挑战，需要能够处理患者欺骗行为的合成数据生成方法。

Method: 基于DAIC-WOZ访谈构建丰富的患者档案，模拟四角色工作流程：患者（在主题相关的诚实状态下完成自评量表和半结构化访谈）、评估者（根据人口统计学和主诉选择工具）、评估员（基于评分员管理的量表进行访谈、跟踪怀疑并完成评分）、诊断师（整合所有证据形成诊断总结）。

Result: 通过四种互补评估验证框架：诊断一致性和严重程度分级、思维链消融、临床真实性和欺骗建模的人类评估、基于LLM的比较评估。生成的语料库涵盖多种障碍和严重程度水平。

Conclusion: 该框架能够生成包含患者欺骗行为的合成精神病学记录，支持对欺骗感知的精神病学评估进行受控研究，并为训练和评估自适应对话系统提供数据基础。

Abstract: Data scarcity and unreliable self-reporting -- such as concealment or exaggeration -- pose fundamental challenges to psychiatric intake and assessment. We propose a multi-agent synthesis framework that explicitly models patient deception to generate high-fidelity, publicly releasable synthetic psychiatric intake records. Starting from DAIC-WOZ interviews, we construct enriched patient profiles and simulate a four-role workflow: a \emph{Patient} completes self-rated scales and participates in a semi-structured interview under a topic-dependent honesty state; an \emph{Assessor} selects instruments based on demographics and chief complaints; an \emph{Evaluator} conducts the interview grounded in rater-administered scales, tracks suspicion, and completes ratings; and a \emph{Diagnostician} integrates all evidence into a diagnostic summary. Each case links the patient profile, self-rated and rater-administered responses, interview transcript, diagnostic summary, and honesty state. We validate the framework through four complementary evaluations: diagnostic consistency and severity grading, chain-of-thought ablations, human evaluation of clinical realism and dishonesty modeling, and LLM-based comparative evaluation. The resulting corpus spans multiple disorders and severity levels, enabling controlled study of dishonesty-aware psychiatric assessment and the training and evaluation of adaptive dialogue agents.

</details>


### [7] [TiInsight: A SQL-based Automated Exploratory Data Analysis System through Large Language Models](https://arxiv.org/abs/2601.09404)
*Jun-Peng Zhu,Boyan Niu,Peng Cai,Zheming Ni,Kai Xu,Jiajun Huang,Shengbo Ma,Bing Wang,Xuan Zhou,Guanglei Bao,Donghui Zhang,Liu Tang,Qi Liu*

Main category: cs.DB

TL;DR: TiInsight是一个基于SQL的自动化跨领域探索性数据分析系统，通过自然语言查询、分层数据上下文生成、问题澄清与分解、文本到SQL转换和数据可视化等功能，实现跨领域数据探索。


<details>
  <summary>Details</summary>
Motivation: 现有的SQL探索性数据分析方法通常缺乏跨领域分析能力，且对大型语言模型潜力的探索不足。需要开发一个能够支持跨领域自动探索的系统。

Method: TiInsight提供用户友好的GUI界面，支持自然语言查询，并构建了跨领域探索性数据分析管道：分层数据上下文生成、问题澄清与分解、文本到SQL转换（TiSQL）和数据可视化（TiChart）。

Result: TiInsight已在PingCAP生产环境中实现和部署，使用代表性数据集展示了其能力，提供了演示视频。

Conclusion: TiInsight成功实现了基于SQL的自动化跨领域探索性数据分析，解决了现有方法在跨领域分析方面的不足，并充分利用了大型语言模型的潜力。

Abstract: The SQL-based exploratory data analysis has garnered significant attention within the data analysis community. The emergence of large language models (LLMs) has facilitated the paradigm shift from manual to automated data exploration. However, existing methods generally lack the ability for cross-domain analysis, and the exploration of LLMs capabilities remains insufficient. This paper presents TiInsight, an SQL-based automated cross-domain exploratory data analysis system. First, TiInsight offers a user-friendly GUI enabling users to explore data using natural language queries. Second, TiInsight offers a robust cross-domain exploratory data analysis pipeline: hierarchical data context (i.e., HDC) generation, question clarification and decomposition, text-to-SQL (i.e., TiSQL), and data visualization (i.e., TiChart). Third, we have implemented and deployed TiInsight in the production environment of PingCAP and demonstrated its capabilities using representative datasets. The demo video is available at https://youtu.be/JzYFyYd-emI.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware DEsigns](https://arxiv.org/abs/2601.08856)
*Deeksha Nandal,Riccardo Revalor,Soham Dan,Debjit Pal*

Main category: cs.SE

TL;DR: LAUDE是一个基于大语言模型的硬件设计单元测试生成与调试统一框架，通过结合设计源代码语义理解和链式思维推理，显著提升测试生成准确性和代码可调试性。


<details>
  <summary>Details</summary>
Motivation: 硬件设计中单元测试对确保功能正确性至关重要，但开发测试需要深入理解设计功能和创造力，而调试设计失败是一个痛苦且密集的过程，需要自动化工具来提升效率。

Method: LAUDE框架结合提示工程和设计执行信息，将设计源代码的语义理解与大语言模型的链式思维推理能力交叉融合，支持闭源和开源LLM，应用于VerilogEval数据集的错误硬件设计代码。

Result: 在组合设计中，生成的单元测试检测到高达100%的错误，调试成功率达93%；在时序设计中，错误检测率达93%，调试成功率达84%。

Conclusion: LAUDE框架成功展示了LLM在硬件设计测试生成和调试中的有效性，能够显著提高硬件设计验证和调试的效率与准确性。

Abstract: Unit tests are critical in the hardware design lifecycle to ensure that component design modules are functionally correct and conform to the specification before they are integrated at the system level. Thus developing unit tests targeting various design features requires deep understanding of the design functionality and creativity. When one or more unit tests expose a design failure, the debugging engineer needs to diagnose, localize, and debug the failure to ensure design correctness, which is often a painstaking and intense process. In this work, we introduce LAUDE, a unified unit-test generation and debugging framework for hardware designs that cross-pollinates the semantic understanding of the design source code with the Chain-of-Thought (CoT) reasoning capabilities of foundational Large-Language Models (LLMs). LAUDE integrates prompt engineering and design execution information to enhance its unit test generation accuracy and code debuggability. We apply LAUDE with closed- and open-source LLMs to a large corpus of buggy hardware design codes derived from the VerilogEval dataset, where generated unit tests detected bugs in up to 100% and 93% of combinational and sequential designs and debugged up to 93% and 84% of combinational and sequential designs, respectively.

</details>


### [9] [Revisiting Software Engineering Education in the Era of Large Language Models: A Curriculum Adaptation and Academic Integrity Framework](https://arxiv.org/abs/2601.08857)
*Mustafa Degerli*

Main category: cs.SE

TL;DR: 论文提出理论框架分析生成式AI如何改变软件工程核心能力，并设计LLM集成教育模型，特别关注土耳其计算机工程教育面临的挑战。


<details>
  <summary>Details</summary>
Motivation: LLM工具（如ChatGPT、GitHub Copilot）正在重塑软件工程实践，降低了代码生成、解释和测试的成本，但传统软件工程教育仍强调手动语法生产作为技术能力标准，这种错配引发了评估有效性、学习成果和基础技能发展的担忧。

Method: 采用概念研究方法，提出理论框架分析生成式AI如何改变软件工程核心能力，并引入LLM集成教育的教学设计模型，特别关注土耳其计算机工程教育中集中监管、大班教学和考试导向评估实践加剧的挑战。

Result: 框架描述了问题分析、设计、实现和测试如何从构建转向批判、验证和人机协作管理，指出传统抄袭检测机制已不足，需要向过程透明度模型过渡。

Conclusion: 虽然提供了课程适应的结构化建议，但这仍是理论贡献，需要纵向实证研究来评估这些干预措施及其对学习的长期影响。

Abstract: The integration of Large Language Models (LLMs), such as ChatGPT and GitHub Copilot, into professional workflows is increasingly reshaping software engineering practices. These tools have lowered the cost of code generation, explanation, and testing, while introducing new forms of automation into routine development tasks. In contrast, most of the software engineering and computer engineering curricula remain closely aligned with pedagogical models that equate manual syntax production with technical competence. This growing misalignment raises concerns regarding assessment validity, learning outcomes, and the development of foundational skills. Adopting a conceptual research approach, this paper proposes a theoretical framework for analyzing how generative AI alters core software engineering competencies and introduces a pedagogical design model for LLM-integrated education. Attention is given to computer engineering programs in Turkey, where centralized regulation, large class sizes, and exam-oriented assessment practices amplify these challenges. The framework delineates how problem analysis, design, implementation, and testing increasingly shift from construction toward critique, validation, and human-AI stewardship. In addition, the paper argues that traditional plagiarism-centric integrity mechanisms are becoming insufficient, motivating a transition toward a process transparency model. While this work provides a structured proposal for curriculum adaptation, it remains a theoretical contribution; the paper concludes by outlining the need for longitudinal empirical studies to evaluate these interventions and their long-term impacts on learning.

</details>


### [10] [Adaptive Trust Metrics for Multi-LLM Systems: Enhancing Reliability in Regulated Industries](https://arxiv.org/abs/2601.08858)
*Tejaswini Bollikonda*

Main category: cs.SE

TL;DR: 该论文提出了一种用于多LLM生态系统的自适应信任度量框架，通过量化模型可靠性、分析系统行为、评估不确定性以及实施动态监控，为受监管行业中的AI部署提供信任保障。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在医疗、金融、法律等敏感领域的部署日益增多，围绕信任、问责制和可靠性的担忧日益凸显。这些模型在受监管环境中的集成需要解决信任度量和可靠性评估的问题。

Method: 提出了一个自适应信任度量框架，包括：分析系统行为、评估多个LLM之间的不确定性、实施动态监控管道，并在金融合规和医疗诊断等实际场景中进行案例研究。

Result: 研究展示了在现实世界环境中应用自适应信任度量的可行性，通过案例研究证明了该框架在金融合规和医疗诊断等受监管行业中的实际适用性。

Conclusion: 自适应信任度量是受监管行业中安全、可扩展AI采用的基础推动者，为多LLM生态系统提供了量化信任和可靠性的实用途径。

Abstract: Large Language Models (LLMs) are increasingly deployed in sensitive domains such as healthcare, finance, and law, yet their integration raises pressing concerns around trust, accountability, and reliability. This paper explores adaptive trust metrics for multi LLM ecosystems, proposing a framework for quantifying and improving model reliability under regulated constraints. By analyzing system behaviors, evaluating uncertainty across multiple LLMs, and implementing dynamic monitoring pipelines, the study demonstrates practical pathways for operational trustworthiness. Case studies from financial compliance and healthcare diagnostics illustrate the applicability of adaptive trust metrics in real world settings. The findings position adaptive trust measurement as a foundational enabler for safe and scalable AI adoption in regulated industries.

</details>


### [11] [EZInput: A Cross-Environment Python Library for Easy UI Generation in Scientific Computing](https://arxiv.org/abs/2601.08859)
*Bruno M. Saraiva,Iván Hidalgo-Cenalmor,António D. Brito,Damián Martínez,Tayla Shakespeare,Guillaume Jacquemet,Ricardo Henriques*

Main category: cs.SE

TL;DR: EZInput是一个Python库，让算法开发者只需定义一次输入需求，就能自动生成跨环境的GUI界面，使非编程用户也能使用计算工具，并支持参数持久化以提高可重复性。


<details>
  <summary>Details</summary>
Motivation: 研究人员在使用计算算法时面临参数配置需要编程技能、不同环境接口不一致、设置难以在会话间持久保存等问题，导致重复输入、迭代探索缓慢，且参数选择难以记录、共享和重用，损害了研究的可重复性。

Method: EZInput采用声明式规范系统，开发者只需定义一次输入需求和验证约束，库自动处理环境检测、界面渲染、参数验证和会话持久化。支持Jupyter notebooks、Google Colab和终端环境，通过轻量级YAML文件保存和恢复用户配置。

Result: EZInput实现了"一次编写，随处运行"的架构，使研究人员能在notebook中原型设计，并在远程系统上部署相同的参数配置而无需代码更改或手动转录。支持科学计算所需的各种输入类型，内置验证确保数据完整性。

Conclusion: EZInput通过自动生成跨环境GUI、参数持久化和内置验证，降低了算法使用的技术门槛，减少了重复输入，提高了研究效率和可重复性，特别适合科学计算工作流程。

Abstract: Researchers face a persistent barrier when applying computational algorithms with parameter configuration typically demanding programming skills, interfaces differing across environments, and settings rarely persisting between sessions. This fragmentation forces repetitive input, slows iterative exploration, and undermines reproducibility because parameter choices are difficult to record, share, and reuse. We present EZInput, a cross-runtime environment Python library enabling algorithm developers to automatically generate graphical user interfaces that make their computational tools accessible to end-users without programming expertise. EZInput employs a declarative specification system where developers define input requirements and validation constraints once; the library then handles environment detection, interface rendering, parameter validation, and session persistence across Jupyter notebooks, Google Colab, and terminal environments. This "write once, run anywhere" architecture enables researchers to prototype in notebooks and deploy identical parameter configurations for batch execution on remote systems without code changes or manual transcription. Parameter persistence, inspired by ImageJ/FIJI and adapted to Python workflows, saves and restores user configurations via lightweight YAML files, eliminating redundant input and producing shareable records that enhance reproducibility. EZInput supports diverse input types essential for scientific computing and it also includes built-in validation that ensures data integrity and clear feedback that reduces user friction.

</details>


### [12] [Bridging the Gap: Empowering Small Models in Reliable OpenACC-based Parallelization via GEPA-Optimized Prompting](https://arxiv.org/abs/2601.08884)
*Samyak Jhaveri,Cristina V. Lopes*

Main category: cs.SE

TL;DR: 使用遗传帕累托优化框架改进OpenACC并行代码生成的提示工程，使小型LLM在编译成功率和GPU加速性能上达到或超过大型模型水平


<details>
  <summary>Details</summary>
Motivation: OpenACC降低了GPU卸载的门槛，但编写高性能的pragma仍然复杂，需要深厚的领域专业知识。大语言模型为自动化并行代码生成提供了有前景的解决方案，但简单的提示通常会导致语法错误、无法编译或性能不佳的代码。

Method: 提出系统化的提示优化方法，利用GEPA（遗传-帕累托）框架，通过反思反馈循环迭代演化提示。该过程使用指令的交叉和变异，以专家策划的黄金示例为指导，并基于黄金和预测pragma之间的子句和子句参数级别不匹配提供结构化反馈。

Result: 在PolyBench套件评估中，使用优化提示生成的OpenACC pragma编译成功率显著提高，特别是对于"nano"规模模型：GPT-4.1 Nano从66.7%提升到93.3%，GPT-5 Nano从86.7%提升到100%，达到或超过更大更昂贵版本的能力。优化提示还使实现GPU加速的程序数量增加了21%。

Conclusion: 提示优化有效释放了更小、更便宜LLM在编写稳定有效GPU卸载指令方面的潜力，为HPC工作流中基于指令的自动化并行化建立了经济高效的途径。

Abstract: OpenACC lowers the barrier to GPU offloading, but writing high-performing pragma remains complex, requiring deep domain expertise in memory hierarchies, data movement, and parallelization strategies. Large Language Models (LLMs) present a promising potential solution for automated parallel code generation, but naive prompting often results in syntactically incorrect directives, uncompilable code, or performance that fails to exceed CPU baselines. We present a systematic prompt optimization approach to enhance OpenACC pragma generation without the prohibitive computational costs associated with model post-training. Leveraging the GEPA (GEnetic-PAreto) framework, we iteratively evolve prompts through a reflective feedback loop. This process utilizes crossover and mutation of instructions, guided by expert-curated gold examples and structured feedback based on clause- and clause parameter-level mismatches between the gold and predicted pragma. In our evaluation on the PolyBench suite, we observe an increase in compilation success rates for programs annotated with OpenACC pragma generated using the optimized prompts compared to those annotated using the simpler initial prompt, particularly for the "nano"-scale models. Specifically, with optimized prompts, the compilation success rate for GPT-4.1 Nano surged from 66.7% to 93.3%, and for GPT-5 Nano improved from 86.7% to 100%, matching or surpassing the capabilities of their significantly larger, more expensive versions. Beyond compilation, the optimized prompts resulted in a 21% increase in the number of programs that achieve functional GPU speedups over CPU baselines. These results demonstrate that prompt optimization effectively unlocks the potential of smaller, cheaper LLMs in writing stable and effective GPU-offloading directives, establishing a cost-effective pathway to automated directive-based parallelization in HPC workflows.

</details>


### [13] [Build Code is Still Code: Finding the Antidote for Pipeline Poisoning](https://arxiv.org/abs/2601.08995)
*Brent Pappas,Paul Gazzillo*

Main category: cs.SE

TL;DR: Foreman工具通过开发阶段隔离策略检测构建系统中毒，成功识别XZ Utils攻击中的恶意测试文件，旨在让构建系统安全检查像代码检查一样普及。


<details>
  <summary>Details</summary>
Motivation: C项目不仅包含C代码，还包含构建系统代码（如编译、测试、打包自动化）。现有安全技术要么只验证软件依赖忽略构建系统本身，要么只分析程序代码不检查构建系统代码。中毒的构建系统可以轻易绕过程序漏洞检测工具，XZ Utils和SolarWinds攻击就是近期案例。

Method: 提出"开发阶段隔离"策略，将构建自动化的信息和行为权限建模为程序代码。原型工具Foreman实现了这种方法，通过权限建模检测构建系统中毒。

Result: Foreman成功检测并警告了XZ Utils攻击中涉及的恶意测试文件。展示了开发阶段隔离策略在检测构建系统中毒方面的有效性。

Conclusion: 构建系统安全检查器应该像程序代码检查器一样普及。未来计划通过自动检查开发阶段隔离来防止流水线中毒，提升软件供应链安全。

Abstract: Open source C code underpins society's computing infrastructure. Decades of work has helped harden C code against attackers, but C projects do not consist of only C code. C projects also contain build system code for automating development tasks like compilation, testing, and packaging. These build systems are critcal to software supply chain security and vulnerable to being poisoned, with the XZ Utils and SolarWinds attacks being recent examples. Existing techniques try to harden software supply chains by verifying software dependencies, but such methods ignore the build system itself. Similarly, classic software security checkers only analyze and monitor program code, not build system code. Moreover, poisoned build systems can easily circumvent tools for detecting program code vulnerabilities by disabling such checks. We present development phase isolation, a novel strategy for hardening build systems against poisoning by modeling the information and behavior permissions of build automation as if it were program code. We have prototyped this approach as a tool called Foreman, which successfully detects and warns about the poisoned test files involved in the XZ Utils attack. We outline our future plans to protect against pipeline poisoning by automatically checking development phase isolation. We envision a future where build system security checkers are as prevalent as program code checkers.

</details>


### [14] [On the Flakiness of LLM-Generated Tests for Industrial and Open-Source Database Management Systems](https://arxiv.org/abs/2601.08998)
*Alexander Berndt,Thomas Bach,Rainer Gemulla,Marcus Kessel,Sebastian Baltes*

Main category: cs.SE

TL;DR: LLM生成的数据库测试中，不稳定测试比例略高于现有测试，主要原因是依赖未保证的顺序（无序集合），且LLM会通过提示上下文将现有测试的不稳定性传递到新测试中。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成的测试中不稳定测试的普遍性和根本原因，特别是在关系数据库管理系统环境中，因为现有研究已发现不稳定性是LLM生成测试的潜在问题，但其普遍性和原因尚不清楚。

Method: 在四个关系数据库管理系统（SAP HANA、DuckDB、MySQL、SQLite）中，使用GPT-4o和Mistral-Large-Instruct-2407两个LLM扩增测试套件，评估生成测试用例的不稳定性，并通过手动检查分析根本原因。

Result: 生成测试的不稳定测试比例略高于现有测试；主要根本原因是测试依赖未保证的顺序（"无序集合"），在115个不稳定测试中占72个（63%）；LLM通过提示上下文将现有测试的不稳定性传递到新测试中；在闭源系统（如SAP HANA）中不稳定性传递比开源系统更普遍。

Conclusion: 研究为开发者提供了关于LLM生成测试中可能遇到的不稳定性类型的见解，并强调了在使用LLM进行测试生成时提供定制化上下文的重要性，特别是要避免不稳定性从现有测试传递到新测试。

Abstract: Flaky tests are a common problem in software testing. They produce inconsistent results when executed multiple times on the same code, invalidating the assumption that a test failure indicates a software defect. Recent work on LLM-based test generation has identified flakiness as a potential problem with generated tests. However, its prevalence and underlying causes are unclear. We examined the flakiness of LLM-generated tests in the context of four relational database management systems: SAP HANA, DuckDB, MySQL, and SQLite. We amplified test suites with two LLMs, GPT-4o and Mistral-Large-Instruct-2407, to assess the flakiness of the generated test cases. Our results suggest that generated tests have a slightly higher proportion of flaky tests compared to existing tests. Based on a manual inspection, we found that the most common root cause of flakiness was the reliance of a test on a certain order that is not guaranteed ("unordered collection"), which was present in 72 of 115 flaky tests (63%). Furthermore, both LLMs transferred the flakiness from the existing tests to the newly generated tests via the provided prompt context. Our experiments suggest that flakiness transfer is more prevalent in closed-source systems such as SAP HANA than in open-source systems. Our study informs developers on what types of flakiness to expect from LLM-generated tests. It also highlights the importance of providing LLMs with tailored context when employing LLMs for test generation.

</details>


### [15] [AI-NativeBench: An Open-Source White-Box Agentic Benchmark Suite for AI-Native Systems](https://arxiv.org/abs/2601.09393)
*Zirui Wang,Guangba Yu,Michael R. Lyu*

Main category: cs.SE

TL;DR: AI-NativeBench：首个基于MCP和A2A标准的应用中心化白盒AI原生基准测试套件，揭示传统黑盒评估无法发现的系统级工程特性


<details>
  <summary>Details</summary>
Motivation: 从云原生向AI原生架构转型中，传统黑盒评估范式已不足够，现有基准测试仅衡量原始模型能力而忽视系统级执行动态，需要新的评估方法

Method: 引入AI-NativeBench基准套件，基于模型上下文协议(MCP)和代理到代理(A2A)标准，将代理跨度作为分布式跟踪中的一等公民，实现细粒度工程特性分析

Result: 在21个系统变体上发现传统指标无法揭示的关键工程现实：参数悖论（轻量模型在协议遵循上常优于旗舰模型）、普遍推理主导（协议开销次要）、昂贵失败模式（自愈机制在不可行工作流上成为成本倍增器）

Conclusion: 这项工作为从衡量模型能力转向工程可靠AI原生系统提供了首个系统性证据，开源基准和数据集促进可复现性和进一步研究

Abstract: The transition from Cloud-Native to AI-Native architectures is fundamentally reshaping software engineering, replacing deterministic microservices with probabilistic agentic services. However, this shift renders traditional black-box evaluation paradigms insufficient: existing benchmarks measure raw model capabilities while remaining blind to system-level execution dynamics. To bridge this gap, we introduce AI-NativeBench, the first application-centric and white-box AI-Native benchmark suite grounded in Model Context Protocol (MCP) and Agent-to-Agent (A2A) standards. By treating agentic spans as first-class citizens within distributed traces, our methodology enables granular analysis of engineering characteristics beyond simple capabilities. Leveraging this benchmark across 21 system variants, we uncover critical engineering realities invisible to traditional metrics: a parameter paradox where lightweight models often surpass flagships in protocol adherence, a pervasive inference dominance that renders protocol overhead secondary, and an expensive failure pattern where self-healing mechanisms paradoxically act as cost multipliers on unviable workflows. This work provides the first systematic evidence to guide the transition from measuring model capability to engineering reliable AI-Native systems. To facilitate reproducibility and further research, we have open-sourced the benchmark and dataset.

</details>


### [16] [SafePlanner: Testing Safety of the Automated Driving System Plan Model](https://arxiv.org/abs/2601.09171)
*Dohyun Kim,Sanggu Han,Sangmin Woo,Joonha Jang,Jaehoon Kim,Changhun Song,Yongdae Kim*

Main category: cs.SE

TL;DR: SafePlanner是一个系统化测试框架，用于检测自动驾驶系统规划模型中的安全关键缺陷，通过结构分析和引导式模糊测试生成有意义的测试场景并发现危险行为。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的规划模型存在安全关键缺陷，需要系统化的测试方法来生成有意义的测试场景并检测危险规划行为。

Method: 对规划模型实现进行结构分析，提取可行的场景转换，结合NPC行为组合测试场景，并应用引导式模糊测试来探索规划模型的行为空间。

Result: 在百度Apollo系统上生成20635个测试用例，检测到520个危险行为，分为15个根本原因，修复其中4个问题后未观察到明显副作用，函数覆盖率达83.63%，决策覆盖率达63.22%。

Conclusion: SafePlanner能有效发现自动驾驶系统规划模型中的安全缺陷，在错误发现和测试效率方面优于基线方法。

Abstract: In this work, we present SafePlanner, a systematic testing framework for identifying safety-critical flaws in the Plan model of Automated Driving Systems (ADS). SafePlanner targets two core challenges: generating structurally meaningful test scenarios and detecting hazardous planning behaviors. To maximize coverage, SafePlanner performs a structural analysis of the Plan model implementation - specifically, its scene-transition logic and hierarchical control flow - and uses this insight to extract feasible scene transitions from code. It then composes test scenarios by combining these transitions with non-player vehicle (NPC) behaviors. Guided fuzzing is applied to explore the behavioral space of the Plan model under these scenarios. We evaluate SafePlanner on Baidu Apollo, a production-grade level 4 ADS. It generates 20635 test cases and detects 520 hazardous behaviors, grouped into 15 root causes through manual analysis. For four of these, we applied patches based on our analysis; the issues disappeared, and no apparent side effects were observed. SafePlanner achieves 83.63 percent function and 63.22 percent decision coverage on the Plan model, outperforming baselines in both bug discovery and efficiency.

</details>


### [17] [DepRadar: Agentic Coordination for Context Aware Defect Impact Analysis in Deep Learning Libraries](https://arxiv.org/abs/2601.09440)
*Yi Gao,Xing Hu,Tongtong Xu,Jiali Zhao,Xiaohu Yang,Xin Xia*

Main category: cs.SE

TL;DR: DepRadar：基于多智能体协调的深度学习库缺陷影响分析框架，能够从库更新中提取缺陷语义并分析下游程序是否受影响


<details>
  <summary>Details</summary>
Motivation: 深度学习库（如Transformers、Megatron）的缺陷会以静默计算错误或性能回归等形式影响下游程序，但用户难以判断自己的程序是否受影响，因为需要理解缺陷语义并检查复杂的触发条件

Method: DepRadar协调四个专门智能体分三步工作：1) PR挖掘器和代码差异分析器从提交中提取结构化缺陷语义；2) 协调器智能体将这些信号合成为统一的缺陷模式及触发条件；3) 影响分析器检查下游程序是否能触发缺陷。框架结合静态分析和深度学习特定领域规则以提高准确性和可解释性

Result: 在两个代表性DL库的157个PR和70个提交上评估，缺陷识别精度达90%，生成高质量结构化字段（平均字段得分1.6）。在122个客户端程序上，识别受影响案例的召回率达90%，精度达80%，显著优于其他基线方法

Conclusion: DepRadar能够有效分析深度学习库更新中的缺陷及其对下游程序的影响，通过多智能体协调和领域特定规则实现了高精度和高召回率的缺陷影响分析

Abstract: Deep learning libraries like Transformers and Megatron are now widely adopted in modern AI programs. However, when these libraries introduce defects, ranging from silent computation errors to subtle performance regressions, it is often challenging for downstream users to assess whether their own programs are affected. Such impact analysis requires not only understanding the defect semantics but also checking whether the client code satisfies complex triggering conditions involving configuration flags, runtime environments, and indirect API usage. We present DepRadar, an agent coordination framework for fine grained defect and impact analysis in DL library updates. DepRadar coordinates four specialized agents across three steps: 1. the PR Miner and Code Diff Analyzer extract structured defect semantics from commits or pull requests, 2. the Orchestrator Agent synthesizes these signals into a unified defect pattern with trigger conditions, and 3. the Impact Analyzer checks downstream programs to determine whether the defect can be triggered. To improve accuracy and explainability, DepRadar integrates static analysis with DL-specific domain rules for defect reasoning and client side tracing. We evaluate DepRadar on 157 PRs and 70 commits across two representative DL libraries. It achieves 90% precision in defect identification and generates high quality structured fields (average field score 1.6). On 122 client programs, DepRadar identifies affected cases with 90% recall and 80% precision, substantially outperforming other baselines.

</details>


### [18] [Towards a Metadata Schema for Energy Research Software](https://arxiv.org/abs/2601.09456)
*Stephan Ferenz,Oliver Werth,Astrid Nieße*

Main category: cs.SE

TL;DR: 开发了能源研究软件的元数据模式，通过需求分析和用户测试评估，平衡了形式化、互操作性和领域特定需求


<details>
  <summary>Details</summary>
Motivation: 许多领域（包括能源研究）缺乏已建立的元数据模式，而领域特定的元数据模式对于提高研究软件的可发现性和可重用性以及遵循FAIR4RS原则至关重要

Method: 基于需求分析开发了能源研究软件的元数据模式，并通过用户测试进行评估

Result: 该模式在形式化、互操作性和满足能源研究人员特定需求之间取得了平衡；测试显示良好的信息呈现对于研究人员创建所需元数据至关重要

Conclusion: 本文提供了设计能源研究软件元数据模式的挑战和机遇的见解，强调了良好信息呈现的重要性

Abstract: Domain-specific metadata schemas are essential to improve the findability and reusability of research software and to follow the FAIR4RS principles. However, many domains, including energy research, lack established metadata schemas. To address this gap, we developed a metadata schema for energy research software based on a requirement analysis and evaluated it through user testing. Our results show that the schema balances the need for formalization and interoperability, while also meeting the specific needs of energy researchers. Meanwhile, the testing showed that a good presentation of the required information is key to enable researchers to create the required metadata. This paper provides insights into the challenges and opportunities of designing a metadata schema for energy research software.

</details>


### [19] [Analyzing GitHub Issues and Pull Requests in nf-core Pipelines: Insights into nf-core Pipeline Repositories](https://arxiv.org/abs/2601.09612)
*Khairul Alam,Banani Roy*

Main category: cs.SE

TL;DR: 对nf-core科学工作流管道的25,173个issue和PR进行实证研究，识别出13个关键挑战，分析解决动态，发现标签和代码片段能显著提高解决概率。


<details>
  <summary>Details</summary>
Motivation: 尽管Nextflow和nf-core在数据密集型科学领域被广泛采用，但用户在这些管道开发和维护过程中面临的具体挑战尚不清楚。本研究旨在通过实证分析揭示这些挑战和管理实践。

Method: 使用BERTopic主题建模分析25,173个issue和pull request，识别关键挑战类别。通过统计分析issue解决动态，包括解决时间、解决概率与标签、代码片段等因素的关系。

Result: 识别出13个关键挑战：管道开发与集成、bug修复、基因组数据集成、CI配置管理、版本更新等。89.38%的issue和PR最终被关闭，半数在3天内解决。标签（大效应，δ=0.94）和代码片段（中效应，δ=0.50）显著提高解决可能性。工具开发和仓库维护是最具挑战性的任务。

Conclusion: 研究为nf-core管道的协作开发和维护提供了可操作的见解，强调了增强其可用性、可持续性和可重复性的机会。标签系统和代码片段的使用能显著改善问题解决效率。

Abstract: Scientific Workflow Management Systems (SWfMSs) such as Nextflow have become essential software frameworks for conducting reproducible, scalable, and portable computational analyses in data-intensive fields like genomics, transcriptomics, and proteomics. Building on Nextflow, the nf-core community curates standardized, peer-reviewed pipelines that follow strict testing, documentation, and governance guidelines. Despite its broad adoption, little is known about the challenges users face during the development and maintenance of these pipelines. This paper presents an empirical study of 25,173 issues and pull requests from these pipelines to uncover recurring challenges, management practices, and perceived difficulties. Using BERTopic modeling, we identify 13 key challenges, including pipeline development and integration, bug fixing, integrating genomic data, managing CI configurations, and handling version updates. We then examine issue resolution dynamics, showing that 89.38\% of issues and pull requests are eventually closed, with half resolved within three days. Statistical analysis reveals that the presence of labels (large effect, $δ$ = 0.94) and code snippets (medium effect, $δ$ = 0.50) significantly improve resolution likelihood. Further analysis reveals that tool development and repository maintenance poses the most significant challenges, followed by testing pipelines and CI configurations, and debugging containerized pipelines. Overall, this study provides actionable insights into the collaborative development and maintenance of nf-core pipelines, highlighting opportunities to enhance their usability, sustainability, and reproducibility.

</details>


### [20] [SysPro: Reproducing System-level Concurrency Bugs from Bug Reports](https://arxiv.org/abs/2601.09616)
*Tarannum Shaila Zaman,Zhihui Yan,Chen Wang,Chadni Islam,Jiangfan Shi,Tingting Yu*

Main category: cs.SE

TL;DR: SysPro：一种从bug报告中自动提取系统调用信息并生成输入数据以复现系统级并发bug的新方法


<details>
  <summary>Details</summary>
Motivation: 系统级并发bug的复现需要输入数据和精确的系统调用交错顺序，但bug报告通常缺乏这些详细信息，且现有工具无法有效处理系统调用级别的交错问题

Method: SysPro自动从bug报告中提取相关系统调用名称并在源代码中定位，通过信息检索、正则表达式匹配和类别划分方法生成输入数据，最后通过动态源代码插桩复现bug

Result: 在真实世界基准测试上的实证研究表明，SysPro在从bug报告中定位和复现系统级并发bug方面既有效又高效

Conclusion: SysPro为解决系统级并发bug复现的挑战提供了一种有效方法，能够自动处理自然语言bug报告中的信息提取和bug复现

Abstract: Reproducing system-level concurrency bugs requires both input data and the precise interleaving order of system calls. This process is challenging because such bugs are non-deterministic, and bug reports often lack the detailed information needed. Additionally, the unstructured nature of reports written in natural language makes it difficult to extract necessary details. Existing tools are inadequate to reproduce these bugs due to their inability to manage the specific interleaving at the system call level. To address these challenges, we propose SysPro, a novel approach that automatically extracts relevant system call names from bug reports and identifies their locations in the source code. It generates input data by utilizing information retrieval, regular expression matching, and the category-partition method. This extracted input and interleaving data are then used to reproduce bugs through dynamic source code instrumentation. Our empirical study on real-world benchmarks demonstrates that SysPro is both effective and efficient at localizing and reproducing system-level concurrency bugs from bug reports.

</details>


### [21] [How well LLM-based test generation techniques perform with newer LLM versions?](https://arxiv.org/abs/2601.09695)
*Michael Konstantinou,Renzo Degiovanni,Mike Papadakis*

Main category: cs.SE

TL;DR: 研究发现，在单元测试生成任务中，使用最新LLM的简单方法可以超越现有的复杂技术，在代码覆盖率等指标上表现更好，且成本相当。建议采用先类后方法的策略进一步减少LLM调用次数。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试生成技术通常基于较弱的LLM版本进行评估，可能夸大了这些技术的优势。随着LLM的快速发展，需要重新评估这些复杂技术相对于简单使用最新LLM的效果。

Method: 复制了四种最先进的LLM测试生成工具（HITS、SymPrompt、TestSpark、CoverUp），并将它们与简单的LLM测试生成方法进行比较。在393个类和3,657个方法上进行实验，使用最新的LLM版本，评估测试效果和效率。

Result: 简单LLM方法在所有测试效果指标上都优于现有技术：行覆盖率提高17.72%，分支覆盖率提高19.80%，变异测试分数提高20.92%，且LLM查询成本相当。采用先类后方法的策略可以减少约20%的LLM请求。

Conclusion: 最新LLM的简单使用已经能够超越复杂的测试生成技术，建议优先使用最新LLM进行测试生成，并采用先类后方法的策略来优化成本效益。

Abstract: The rapid evolution of Large Language Models (LLMs) has strongly impacted software engineering, leading to a growing number of studies on automated unit test generation. However, the standalone use of LLMs without post-processing has proven insufficient, often producing tests that fail to compile or achieve high coverage. Several techniques have been proposed to address these issues, reporting improvements in test compilation and coverage. While important, LLM-based test generation techniques have been evaluated against relatively weak baselines (for todays' standards), i.e., old LLM versions and relatively weak prompts, which may exacerbate the performance contribution of the approaches. In other words, stronger (newer) LLMs may obviate any advantage these techniques bring. We investigate this issue by replicating four state-of-the-art LLM-based test generation tools, HITS, SymPrompt, TestSpark, and CoverUp that include engineering components aimed at guiding the test generation process through compilation and execution feedback, and evaluate their relative effectiveness and efficiency over a plain LLM test generation method. We integrate current LLM versions in all approaches and run an experiment on 393 classes and 3,657 methods. Our results show that the plain LLM approach can outperform previous state-of-the-art approaches in all test effectiveness metrics we used: line coverage (by 17.72%), branch coverage (by 19.80%) and mutation score (by 20.92%), and it does so at a comparable cost (LLM queries). We also observe that the granularity at which the plain LLM is applied has a significant impact on the cost. We therefore propose targeting first the program classes, where test generation is more efficient, and then the uncovered methods to reduce the number of LLM requests. This strategy achieves comparable (slightly higher) effectiveness while requiring about 20% fewer LLM requests.

</details>


### [22] [ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation](https://arxiv.org/abs/2601.09703)
*Sicong Liu,Yanxian Huang,Mingwei Liu,Jiachi Chen,Ensheng Shi,Yuchi Ma,Hongyu Zhang,Yin Zhang,Yanlin Wang*

Main category: cs.SE

TL;DR: ShortCoder是一个知识注入框架，通过代码语法级简化、混合数据合成和微调策略，在保持语义等价性和可读性的同时，显著提升代码生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成时存在效率问题：每个token生成都需要完整推理过程，需要持续保留上下文信息，导致资源消耗大。当前研究主要关注推理阶段优化，而生成阶段研究不足。

Method: 1) 提出10个Python语法级简化规则，基于AST保持转换，实现18.1%的token缩减；2) 设计混合数据合成管道，结合规则重写和LLM引导精炼，创建ShorterCodeBench语料库；3) 开发微调策略，将简洁性意识注入基础LLMs。

Result: 在HumanEval基准测试中，ShortCoder始终优于最先进方法，代码生成效率比先前方法提升18.1%-37.8%，同时保持代码生成性能。

Conclusion: ShortCoder通过优化代码生成效率，解决了LLMs在代码生成中的资源消耗问题，在保持语义等价性和可读性的同时显著提升效率，为代码生成任务提供了有效的解决方案。

Abstract: Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.

</details>
