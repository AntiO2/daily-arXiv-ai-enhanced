<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 25]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Maple: A Multi-agent System for Portable Deep Learning across Clusters](https://arxiv.org/abs/2510.08842)
*Molang Wu,Zhao Zhang*

Main category: cs.DC

TL;DR: Maple是一个多智能体系统，通过自然语言输入生成正确的深度学习命令行，解决了在异构GPU集群上训练深度学习模型时命令行配置的复杂性。


<details>
  <summary>Details</summary>
Motivation: 在GPU集群上训练深度学习模型面临技术挑战，用户需要适应异构启动器、调度器、亲和性选项、框架参数和环境变量。手动组合命令行容易出错，阻碍研究并浪费资源。

Method: Maple包含四个智能体：信息提取、模板检索、命令行验证和错误纠正。系统利用多个语言模型（总计100亿参数）来生成正确的命令行。

Result: 在9个GPU集群、5个深度学习模型家族和4种并行训练范式的567个测试案例中，Maple实现了92.0%的命令行生成准确率，性能与GPT-5、Claude和Gemini等最先进模型相当。

Conclusion: Maple在异构高性能计算环境中实现了可移植和可扩展的分布式深度学习训练，具有重要的实用价值。

Abstract: Training deep learning (DL) models across Graphics Processing Unit (GPU)
clusters is technically challenging. One aspect is that users have to compose
command lines to adapt to the heterogeneous launchers, schedulers, affinity
options, DL framework arguments, and environment variables. Composing correct
command lines is error-prone and can easily frustrate users, impeding research
or wasting resources. In this work, we present Maple, a multi-agent system that
generates correct DL command lines with users' natural language input. Maple
consists of four agents with the functionalities of information extraction,
template retrieval, command line verification, and error correction. We
evaluate Maple on nine GPU clusters across national computing centers in the
U.S., five representative deep learning model families, and four commonly used
parallel DL training paradigms. Our experiments also cover schedulers of SLURM
and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and
Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command
lines across the 567 test cases. Leverage multiple language models with an
aggregated size of 10B parameters, Maple delivers comparable performance to the
state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results
highlight Maple's practical value in enabling portable and scalable distributed
DL across heterogeneous HPC environments.

</details>


### [2] [Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication](https://arxiv.org/abs/2510.08874)
*Benjamin Brock,Renato Golin*

Main category: cs.DC

TL;DR: 提出了一种通用的单边分布式矩阵乘法算法，支持所有分区和复制因子组合，通过切片计算重叠图块相乘，性能与PyTorch DTensor相当。


<details>
  <summary>Details</summary>
Motivation: 现有分布式矩阵乘法算法仅支持部分分区方式，需要多个算法实现来覆盖所有可能的分区组合，否则需要重新分布操作数增加通信成本。

Method: 使用切片（索引算术）计算需要相乘的重叠图块集合，然后直接执行或重新排序并降级为优化IR以最大化重叠。基于高级C++ PGAS编程框架实现，使用节点内互连进行GPU到GPU直接通信。

Result: 在各种分区和复制因子下评估性能，发现与针对AI模型优化的PyTorch DTensor性能相当。

Conclusion: 提出的通用单边算法支持所有分区组合，避免了操作数重新分布，性能与现有优化库相当。

Abstract: Many important applications across science, data analytics, and AI workloads
depend on distributed matrix multiplication. Prior work has developed a large
array of algorithms suitable for different problem sizes and partitionings
including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is
that existing algorithms are limited to a subset of partitionings. Multiple
algorithm implementations are required to support the full space of possible
partitionings. If no algorithm implementation is available for a particular set
of partitionings, one or more operands must be redistributed, increasing
communication costs. This paper presents a universal one-sided algorithm for
distributed matrix multiplication that supports all combinations of
partitionings and replication factors. Our algorithm uses slicing (index
arithmetic) to compute the sets of overlapping tiles that must be multiplied
together. This list of local matrix multiplies can then either be executed
directly, or reordered and lowered to an optimized IR to maximize overlap. We
implement our algorithm using a high-level C++-based PGAS programming framework
that performs direct GPU-to-GPU communication using intra-node interconnects.
We evaluate performance for a wide variety of partitionings and replication
factors, finding that our work is competitive with PyTorch DTensor, a highly
optimized distributed tensor library targeting AI models.

</details>


### [3] [Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors](https://arxiv.org/abs/2510.09163)
*Alessandro Ottaviano,Andrino Meli,Paul Scheffler,Giovanni Bambini,Robert Balas,Davide Rossi,Andrea Bartolini,Luca Benini*

Main category: cs.DC

TL;DR: 提出了一种基于硬件-软件协同设计的轻量级MPC控制器，用于多核HPC处理器的能量和热管理，相比单核基线实现了33倍延迟降低和7.9倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 传统MPC方法在PE上执行控制器会因操作系统开销产生抖动并限制控制带宽，而专用片上控制器又面临面积和功耗开销的担忧。

Method: 采用算子分裂二次规划求解器和嵌入式多核RISC-V控制器，通过剪枝弱热耦合减少模型内存，并采用提前调度技术高效并行执行优化问题中的稀疏三角系统。

Result: 在500MHz频率下控制144个PE时实现亚毫秒延迟，内存占用小于1MiB，功耗仅325mW，占用典型HPC处理器芯片面积小于1.5%。

Conclusion: 该硬件-软件协同设计方法成功解决了MPC控制器在计算、内存和功耗方面的挑战，为大规模多核处理器的热管理提供了高效解决方案。

Abstract: Managing energy and thermal profiles is critical for many-core HPC processors
with hundreds of application-class processing elements (PEs). Advanced model
predictive control (MPC) delivers state-of-the-art performance but requires
solving an online optimization problem over a thousand times per second (1 kHz
control bandwidth), with computational and memory demands scaling with PE
count. Traditional MPC approaches execute the controller on the PEs, but
operating system overheads create jitter and limit control bandwidth. Running
MPC on dedicated on-chip controllers enables fast, deterministic control but
raises concerns about area and power overhead. In this work, we tackle these
challenges by proposing a hardware-software codesign of a lightweight MPC
controller, based on an operator-splitting quadratic programming solver and an
embedded multi-core RISC-V controller. Key innovations include pruning weak
thermal couplings to reduce model memory and ahead-of-time scheduling for
efficient parallel execution of sparse triangular systems arising from the
optimization problem. The proposed controller achieves sub-millisecond latency
when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x
higher energy efficiency than a single-core baseline. Operating within a
compact less than 1 MiB memory footprint, it consumes as little as 325 mW while
occupying less than 1.5% of a typical HPC processor's die area.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [4] [Comparative Analysis of Large Language Models for the Machine-Assisted Resolution of User Intentions](https://arxiv.org/abs/2510.08576)
*Justus Flerlage,Alexander Acker,Odej Kao*

Main category: cs.SE

TL;DR: 评估开源大语言模型作为本地部署意图解析系统的可行性，与GPT-4进行性能对比分析


<details>
  <summary>Details</summary>
Motivation: 现有基于云端专有模型的实现存在隐私、自主性和可扩展性限制，本地部署是实现真正鲁棒可信语言交互范式的必要条件

Method: 对多个开源和开放访问模型进行能力评估，与OpenAI的GPT-4系统进行对比分析，评估其在生成用户意图工作流方面的性能

Result: 提供了关于开源LLMs作为下一代操作系统自主本地可操作组件的实际可行性、性能权衡和潜力的实证见解

Conclusion: 研究结果为AI基础设施去中心化和民主化提供了参考，指向通过本地嵌入智能实现更无缝、自适应和隐私保护的用户设备交互未来

Abstract: Large Language Models (LLMs) have emerged as transformative tools for natural
language understanding and user intent resolution, enabling tasks such as
translation, summarization, and, increasingly, the orchestration of complex
workflows. This development signifies a paradigm shift from conventional,
GUI-driven user interfaces toward intuitive, language-first interaction
paradigms. Rather than manually navigating applications, users can articulate
their objectives in natural language, enabling LLMs to orchestrate actions
across multiple applications in a dynamic and contextual manner. However,
extant implementations frequently rely on cloud-based proprietary models, which
introduce limitations in terms of privacy, autonomy, and scalability. For
language-first interaction to become a truly robust and trusted interface
paradigm, local deployment is not merely a convenience; it is an imperative.
This limitation underscores the importance of evaluating the feasibility of
locally deployable, open-source, and open-access LLMs as foundational
components for future intent-based operating systems. In this study, we examine
the capabilities of several open-source and open-access models in facilitating
user intention resolution through machine assistance. A comparative analysis is
conducted against OpenAI's proprietary GPT-4-based systems to assess
performance in generating workflows for various user intentions. The present
study offers empirical insights into the practical viability, performance
trade-offs, and potential of open LLMs as autonomous, locally operable
components in next-generation operating systems. The results of this study
inform the broader discussion on the decentralization and democratization of AI
infrastructure and point toward a future where user-device interaction becomes
more seamless, adaptive, and privacy-conscious through locally embedded
intelligence.

</details>


### [5] [Which Is Better For Reducing Outdated and Vulnerable Dependencies: Pinning or Floating?](https://arxiv.org/abs/2510.08609)
*Imranur Rahman,Jill Marley,William Enck,Laurie Williams*

Main category: cs.SE

TL;DR: 本研究通过实证分析npm、PyPI和Cargo生态系统中依赖版本约束的使用情况，发现浮动次要版本约束是最可能导致依赖过时或存在漏洞的约束类型，而浮动主版本约束最不容易导致依赖过时，浮动次要版本约束最不容易导致漏洞。


<details>
  <summary>Details</summary>
Motivation: 开发者使用版本约束来指定可接受的依赖版本，但固定依赖版本(pinning)虽然能减少破坏性变更的风险，却容易导致依赖过时；而浮动版本(floating)能自动获取修复，但存在破坏性变更风险。目前尚不清楚不同版本约束类型导致依赖过时或存在漏洞的可能性差异。

Method: 首先识别npm、PyPI和Cargo生态系统中依赖版本约束的使用趋势和开发者修改约束类型的模式，然后使用生存分析对依赖状态转换进行建模，估计使用固定约束相对于其他约束类型导致依赖过时或存在漏洞的可能性变化。

Result: 在过时和存在漏洞的依赖中，最常用的版本约束类型是浮动次要版本，其次是固定约束。浮动主版本约束最不容易导致依赖过时，浮动次要版本约束最不容易导致依赖存在漏洞。

Conclusion: 研究结果有助于开发者在选择依赖版本约束类型时做出明智决策，了解不同约束类型在防止依赖过时和漏洞方面的权衡。

Abstract: Developers consistently use version constraints to specify acceptable
versions of the dependencies for their project. \emph{Pinning} dependencies can
reduce the likelihood of breaking changes, but comes with a cost of manually
managing the replacement of outdated and vulnerable dependencies. On the other
hand, \emph{floating} can be used to automatically get bug fixes and security
fixes, but comes with the risk of breaking changes. Security practitioners
advocate \emph{pinning} dependencies to prevent against software supply chain
attacks, e.g., malicious package updates. However, since \emph{pinning} is the
tightest version constraint, \emph{pinning} is the most likely to result in
outdated dependencies. Nevertheless, how the likelihood of becoming outdated or
vulnerable dependencies changes across version constraint types is unknown. The
goal of this study is to aid developers in making an informed dependency
version constraint choice by empirically evaluating the likelihood of
dependencies becoming outdated or vulnerable across version constraint types at
scale. In this study, we first identify the trends in dependency version
constraint usage and the patterns of version constraint type changes made by
developers in the npm, PyPI, and Cargo ecosystems. We then modeled the
dependency state transitions using survival analysis and estimated how the
likelihood of becoming outdated or vulnerable changes when using \emph{pinning}
as opposed to the rest of the version constraint types. We observe that among
outdated and vulnerable dependencies, the most commonly used version constraint
type is \emph{floating-minor}, with \emph{pinning} being the next most common.
We also find that \emph{floating-major} is the least likely to result in
outdated and \emph{floating-minor} is the least likely to result in vulnerable
dependencies.

</details>


### [6] [Relative Positioning Based Code Chunking Method For Rich Context Retrieval In Repository Level Code Completion Task With Code Language Model](https://arxiv.org/abs/2510.08610)
*Imranur Rahman,Md Rayhanur Rahman*

Main category: cs.SE

TL;DR: 提出了一种有效的上下文收集策略，通过代码分块和相对定位来提升LLM在代码补全任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有IDE中的代码补全功能缺乏对良好上下文定义的研究，需要确定基于IDE可用信息的最佳上下文策略来提升大语言模型的代码补全性能。

Method: 将代码仓库预处理为更小的代码块，然后使用基于语法和语义相似度的代码块检索方法，并结合相对定位技术。

Result: 研究发现代码分块和代码块在最终上下文中的相对定位能够显著提升代码补全任务的性能。

Conclusion: 提出的上下文收集策略通过代码分块和相对定位有效提升了LLM在代码补全任务中的表现。

Abstract: Code completion can help developers improve efficiency and ease the
development lifecycle. Although code completion is available in modern
integrated development environments (IDEs), research lacks in determining what
makes a good context for code completion based on the information available to
the IDEs for the large language models (LLMs) to perform better. In this paper,
we describe an effective context collection strategy to assist the LLMs in
performing better at code completion tasks. The key idea of our strategy is to
preprocess the repository into smaller code chunks and later use syntactic and
semantic similarity-based code chunk retrieval with relative positioning. We
found that code chunking and relative positioning of the chunks in the final
context improve the performance of code completion tasks.

</details>


### [7] [Impact of LLMs on Team Collaboration in Software Development](https://arxiv.org/abs/2510.08612)
*Devang Dhanuka*

Main category: cs.SE

TL;DR: 该论文研究了大型语言模型(LLMs)在软件开发生命周期(SDLC)中对团队协作的影响，发现LLMs能显著提高效率、改善沟通，但也带来了模型局限性和隐私等新挑战。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地集成到软件开发过程中，需要了解它们如何影响团队工作流程和生产力，特别是在协作方面。

Method: 通过文献综述、行业案例、团队调查和两个案例研究，评估LLM辅助工具对协作软件工程实践的影响。

Result: LLMs能够显著提高效率（通过自动化重复任务和文档）、增强沟通清晰度、促进跨职能协作，但同时也引入了模型局限性和隐私问题等新挑战。

Conclusion: LLMs在软件团队协作中具有重要价值，未来研究方向包括领域特定模型定制、改进开发工具集成以及确保信任和安全性的稳健策略。

Abstract: Large Language Models (LLMs) are increasingly being integrated into software
development processes, with the potential to transform team workflows and
productivity. This paper investigates how LLMs affect team collaboration
throughout the Software Development Life Cycle (SDLC). We reframe and update a
prior study with recent developments as of 2025, incorporating new literature
and case studies. We outline the problem of collaboration hurdles in SDLC and
explore how LLMs can enhance productivity, communication, and decision-making
in a team context. Through literature review, industry examples, a team survey,
and two case studies, we assess the impact of LLM-assisted tools (such as code
generation assistants and AI-powered project management agents) on
collaborative software engineering practices. Our findings indicate that LLMs
can significantly improve efficiency (by automating repetitive tasks and
documentation), enhance communication clarity, and aid cross-functional
collaboration, while also introducing new challenges like model limitations and
privacy concerns. We discuss these benefits and challenges, present research
questions guiding the investigation, evaluate threats to validity, and suggest
future research directions including domain-specific model customization,
improved integration into development tools, and robust strategies for ensuring
trust and security.

</details>


### [8] [Automating Android Build Repair: Bridging the Reasoning-Execution Gap in LLM Agents with Domain-Specific Tools](https://arxiv.org/abs/2510.08640)
*Ha Min Son,Huan Ren,Xin Liu,Zhe Zhao*

Main category: cs.SE

TL;DR: 提出了GradleFixer，一个专门用于修复Android构建错误的LLM代理，通过领域特定工具显著提升了构建错误修复成功率。


<details>
  <summary>Details</summary>
Motivation: Android作为最大的移动平台，自动构建应用仍面临挑战。虽然LLM在代码修复方面有潜力，但在修复Android构建错误方面的应用尚未充分探索。

Method: 首先创建AndroidBuildBench基准测试集，包含1,019个构建失败案例；然后提出GradleFixer，这是一个配备领域特定工具的LLM代理，用于检查和操作Gradle构建环境。

Result: GradleFixer实现了81.4%的解决率（pass@1），显著优于依赖通用shell的最先进编码代理。

Conclusion: 研究表明，虽然LLM具备解决构建失败的高级知识，但使用通用shell将这些知识转化为有效的低级操作存在困难。工具桥接策略通过提供API格式的领域感知抽象，成功弥合了模型高级推理与有效低级执行之间的差距。

Abstract: Android is the largest mobile platform, yet automatically building
applications remains a practical challenge. While Large Language Models (LLMs)
show promise for code repair, their use for fixing Android build errors remains
underexplored. To address this gap, we first introduce AndroidBuildBench, a
benchmark of 1,019 build failures curated from the commit histories of 43
open-source Android projects. Each problem is paired with a verified solution
from a subsequent commit, ensuring that fixes are feasible. Second, we propose
GradleFixer, an LLM agent with domain-specific tools for inspecting and
manipulating the Gradle build environment. GradleFixer achieves a resolve rate
of 81.4% (pass@1), significantly outperforming a state-of-the-art coding agent
that relies on a general-purpose shell. GradleFixer's success suggests that
while LLMs possess the high-level knowledge to solve these failures, they
struggle to translate this knowledge into effective low-level actions using a
general-purpose shell. We demonstrate the effectiveness of a strategy we term
Tool Bridging, which replaces general-purpose shell commands with domain-aware
abstractions. We hypothesize this approach works through two mechanisms: 1) it
provides tools in an API-like format that LLMs use more reliably, and 2) it
constrains the action space to relevant operations. This approach bridges the
gap between the model's high-level reasoning and effective low-level execution.

</details>


### [9] [Faver: Boosting LLM-based RTL Generation with Function Abstracted Verifiable Middleware](https://arxiv.org/abs/2510.08664)
*Jianan Mu,Mingyu Shi,Yining Wang,Tianmeng Yang,Bin Sun,Xing Hu,Jing Ye,Huawei Li*

Main category: cs.SE

TL;DR: 提出了一种名为Faver的函数抽象可验证中间件，通过在LLM友好的代码结构中混合基于规则的模板，将电路验证细节解耦，使LLM能够专注于功能本身，在实验中最高提升了14%的生成准确率。


<details>
  <summary>Details</summary>
Motivation: 由于高层次规范与RTL之间存在巨大语义鸿沟，加上训练数据有限，现有模型在RTL生成准确性方面存在困难。借鉴人类经验，设计验证有助于提高准确性，但RTL测试平台数据更加稀缺，对LLM不友好。

Method: 提出函数抽象可验证中间件(Faver)，通过混合LLM友好的代码结构和基于规则的模板，将电路验证细节解耦，让LLM专注于功能实现。

Result: 在SFT模型和开源模型的实验中，Faver将模型的生成准确率最高提升了14%。

Conclusion: Faver通过解耦验证细节，使LLM能够更专注于功能实现，有效提升了RTL生成的准确性。

Abstract: LLM-based RTL generation is an interesting research direction, as it holds
the potential to liberate the least automated stage in the current chip design.
However, due to the substantial semantic gap between high-level specifications
and RTL, coupled with limited training data, existing models struggle with
generation accuracy. Drawing on human experience, design with verification
helps improving accuracy. However, as the RTL testbench data are even more
scarce, it is not friendly for LLMs. Although LLMs excel at higher-level
languages like Python/C, they have a huge semantic gap from RTL. When
implementing the same functionality, Python/C code and hardware code differ
significantly in the spatiotemporal granularity, requiring the LLM not only to
consider high-level functional semantics but also to ensure the low-level
details align with the circuit code. It is not an easy task. In this paper, we
propose a function abstracted verifiable middleware (Faver) that streamlines
RTL verification in LLM-based workflows. By mixing LLM-friendly code structures
with a rule-based template, Faver decouples the details of circuit
verification, allowing the LLM to focus on the functionality itself. In our
experiments on the SFT model and open-source models, Faver improved the model's
generation accuracy by up to 14%.

</details>


### [10] [RA-Gen: A Controllable Code Generation Framework Using ReAct for Multi-Agent Task Execution](https://arxiv.org/abs/2510.08665)
*Aofan Liu,Haoxuan Li,Bin Wang,Ao Yang,Hui Li*

Main category: cs.SE

TL;DR: 提出了一种基于ReAct范式的可控代码生成多智能体框架，通过四个专业化智能体的协作实现高效、精确和可解释的代码生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有代码生成模型在安全性、准确性和可控性方面的不足，特别是缺乏外部工具的动态集成、透明推理和用户安全控制的问题。

Method: 采用多智能体系统架构，包含规划器、基于ReAct的搜索器、代码生成器和提取器四个专业智能体，通过动态交互实现推理与工具执行的交替进行。

Result: 在SVEN数据集上使用CodeQL实现了94.8%的安全率，优于现有方法，并在多种编程语言中表现出有效性。

Conclusion: 该框架通过透明推理过程增强了用户信任，提高了代码生成的可控性，为复杂任务的代码生成提供了有效的解决方案。

Abstract: Code generation models based on large language models (LLMs) have gained wide
adoption, but challenges remain in ensuring safety, accuracy, and
controllability, especially for complex tasks. Existing methods often lack
dynamic integration of external tools, transparent reasoning, and user control
over safety. To address these issues, we propose a controllable code generation
framework utilizing the ReAct paradigm for multi-agent task execution. This
framework is a multi-agent system designed to enable efficient, precise, and
interpretable code generation through dynamic interactions between LLMs and
external resources. The framework adopts a collaborative architecture
comprising four specialized agents: a Planner for task decomposition, a
Searcher that leverages the ReAct framework for reasoning and tool integration,
a CodeGen agent for accurate code generation, and an Extractor for structured
data retrieval. The ReAct-based Searcher alternates between generating
reasoning traces and executing actions, facilitating seamless integration of
internal knowledge with external tools (such as search engines) to enhance
accuracy and user control. Experimental results show the framework's
effectiveness across multiple languages, achieving a 94.8% security rate on the
SVEN dataset with CodeQL, outperforming existing approaches. Its transparent
reasoning process fosters user trust and improves controllability.

</details>


### [11] [RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data](https://arxiv.org/abs/2510.08667)
*Mohammad Baqar*

Main category: cs.SE

TL;DR: 提出基于检索增强生成(RAG)的框架，整合JIRA工单和GitHub数据，通过语义嵌入和向量搜索为软件团队提供上下文感知的问题解决建议。


<details>
  <summary>Details</summary>
Motivation: 解决现代软件团队因知识分散在JIRA工单、开发者讨论和GitHub PR中而导致的重复或相关问题解决延迟问题。

Method: 使用Sentence-Transformers生成语义嵌入，结合FAISS向量搜索检索相似历史案例，然后用大语言模型合成基于检索证据的解决方案建议。

Result: 实验评估显示，该系统在解决准确性、修复质量和知识重用方面显著改进，减少了解决时间并提高了开发者接受度。

Conclusion: 该框架通过统一JIRA和GitHub数据管道、异构软件工件的嵌入索引策略以及基于检索证据的解决方案生成模块，有效提升了DevOps环境中的问题解决效率。

Abstract: Modern software teams frequently encounter delays in resolving recurring or
related issues due to fragmented knowledge scattered across JIRA tickets,
developer discussions, and GitHub pull requests (PRs). To address this
challenge, we propose a Retrieval-Augmented Generation (RAG) framework that
integrates Sentence-Transformers for semantic embeddings with FAISS-based
vector search to deliver context-aware ticket resolution recommendations. The
approach embeds historical JIRA tickets, user comments, and linked PR metadata
to retrieve semantically similar past cases, which are then synthesized by a
Large Language Model (LLM) into grounded and explainable resolution
suggestions. The framework contributes a unified pipeline linking JIRA and
GitHub data, an embedding and FAISS indexing strategy for heterogeneous
software artifacts, and a resolution generation module guided by retrieved
evidence. Experimental evaluation using precision, recall, resolution time
reduction, and developer acceptance metrics shows that the proposed system
significantly improves resolution accuracy, fix quality, and knowledge reuse in
modern DevOps environments.

</details>


### [12] [BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution](https://arxiv.org/abs/2510.08697)
*Terry Yue Zhuo,Xiaolong Jin,Hange Liu,Juyong Jiang,Tianyang Liu,Chen Gong,Bhupesh Bishnoi,Vaisakhi Mishra,Marek Suppa,Noah Ziems,Saiteja Utpala,Ming Xu,Guangyu Song,Kaixin Li,Yuhan Cao,Bo Liu,Zheng Liu,Sabina Abdurakhmanova,Wenhao Yu,Mengzhao Jia,Jihan Yao,Kenneth Hamilton,Kumar Shridhar,Minh Chien Vu,Dingmin Wang,Jiawei Liu,Zijian Wang,Qian Liu,Binyuan Hui,Meg Risdal,Ahsen Khaliq,Atin Sood,Zhenchang Xing,Wasi Uddin Ahmad,John Grundy,David Lo,Banghua Zhu,Xiaoning Du,Torsten Scholak,Leandro von Werra*

Main category: cs.SE

TL;DR: BigCodeArena是一个基于Chatbot Arena构建的代码生成人类评估平台，提供实时执行环境来评估LLM生成的代码质量。该平台收集了超过14,000个代码对话，识别了4,700多个带有人类偏好的多轮样本，并创建了BigCodeReward和AutoCodeArena两个基准来系统评估LLM的代码能力。


<details>
  <summary>Details</summary>
Motivation: 在代码生成领域，手动评估LLM生成内容的质量极具挑战性，因为需要理解冗长的原始代码并模拟代码执行过程。

Method: 构建BigCodeArena平台，支持LLM生成代码的执行，允许人类与执行过程和结果交互。收集14,000+代码对话，涵盖10种语言和8种执行环境，识别4,700+带人类偏好多轮样本。

Result: 发现大多数LLM在有执行结果时可更好地判断代码偏好。专有LLM（如GPT-5、Claude-Sonnet-4、Claude-Opus-4）在代码生成性能上领先。

Conclusion: BigCodeArena为代码生成评估提供了有效的平台，AutoCodeArena基准可在无需人类参与的情况下自动评估LLM的代码质量，专有模型在代码生成方面仍保持领先地位。

Abstract: Crowdsourced model evaluation platforms, such as Chatbot Arena, enable
real-time evaluation from human perspectives to assess the quality of model
responses. In the coding domain, manually examining the quality of
LLM-generated content is extremely challenging, as it requires understanding
long chunks of raw code and deliberately simulating code execution. To this
end, we introduce BigCodeArena, an open human evaluation platform for code
generation backed by a comprehensive and on-the-fly execution environment.
Built on top of Chatbot Arena, BigCodeArena enables the execution of
LLM-generated code and allows humans to interact with the execution process and
outcomes. We collected over 14,000 raw code-centric conversation sessions
across 10 widely used LLMs, spanning 10 languages and 8 types of execution
environments. Among these conversations, we identified more than 4,700
multi-turn samples with pairwise human preferences. Further analysis uncovers
underexplored preferences of LLMs in fine-grained domains characterized by
tasks, languages, and frameworks. To systematically examine code understanding
and generation capabilities of frontier LLMs, we curated two benchmarks based
on the collected data, namely BigCodeReward and AutoCodeArena. For
BigCodeReward, we post-processed the 4,700 conversations and evaluated the
consistency between reward models and human preferences. The evaluation shows
that most LLMs have superior performance in judging coding preferences when the
execution results are available. Inspired by these findings, we propose
AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding
quality of LLMs without human involvement. We find that proprietary LLMs like
GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation
performance among recent emerging models.

</details>


### [13] [Search-based Hyperparameter Tuning for Python Unit Test Generation](https://arxiv.org/abs/2510.08716)
*Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: 使用差分进化算法调优DynaMOSA和MIO多目标搜索算法的超参数，在Pynguin框架中显著提高了测试套件的覆盖率，且比网格搜索更高效。


<details>
  <summary>Details</summary>
Motivation: 搜索式测试生成算法有众多配置选项，用户通常使用默认值，但这些默认值可能无法达到最佳效果。超参数调优可以找到更好的参数值，但通常资源需求高。

Method: 探索使用差分进化算法来调优DynaMOSA和MIO多目标搜索算法的超参数，并与基本的网格搜索方法进行比较。

Result: 调优后的DynaMOSA算法显著提高了测试套件的覆盖率，且差分进化算法比网格搜索更高效。

Conclusion: 差分进化算法是调优搜索式测试生成算法超参数的有效方法，能够显著提升测试覆盖效果。

Abstract: Search-based test-generation algorithms have countless configuration options.
Users rarely adjust these options and usually stick to the default values,
which may not lead to the best possible results. Tuning an algorithm's
hyperparameters is a method to find better hyperparameter values, but it
typically comes with a high demand of resources. Meta-heuristic search
algorithms -- that effectively solve the test-generation problem -- have been
proposed as a solution to also efficiently tune parameters. In this work we
explore the use of differential evolution as a means for tuning the
hyperparameters of the DynaMOSA and MIO many-objective search algorithms as
implemented in the Pynguin framework. Our results show that significant
improvement of the resulting test suite's coverage is possible with the tuned
DynaMOSA algorithm and that differential evolution is more efficient than basic
grid search.

</details>


### [14] [PyMigTool: a tool for end-to-end Python library migration](https://arxiv.org/abs/2510.08810)
*Mohayeminul Islam,Ajay Kumar Jha,May Mahmoud,Sarah Nadi*

Main category: cs.SE

TL;DR: 开发了PyMigTool，一个结合大语言模型、静态分析和动态分析的端到端Python库迁移工具，能够自动迁移任意功能相似的Python库之间的代码。


<details>
  <summary>Details</summary>
Motivation: 手动库迁移耗时且容易出错，现有自动化技术大多停留在API映射阶段或仅支持有限的库和代码转换。

Method: 使用大语言模型作为主要迁移引擎，结合静态分析和动态分析进行后处理，开发了PyMigTool命令行应用。

Result: 在717个真实Python应用上评估，PyMigTool能完全正确迁移32%的案例，剩余迁移中超过一半项目只有14%的迁移相关变更需要开发者修复。

Conclusion: LLMs能有效执行库迁移，结合后处理步骤可进一步提高性能，PyMigTool为Python库迁移提供了实用的端到端解决方案。

Abstract: Library migration is the process of replacing a library with a similar one in
a software project. Manual library migration is time consuming and error prone,
as it requires developers to understand the Application Programming Interfaces
(API) of both libraries, map equivalent APIs, and perform the necessary code
transformations. Due to the difficulty of the library migration process, most
of the existing automated techniques and tooling stop at the API mapping stage
or support a limited set of libraries and code transformations. In this paper,
we develop an end-to-end solution that can automatically migrate code between
any arbitrary pair of Python libraries that provide similar functionality. Due
to the promising capabilities of Large Language Models (LLMs) in code
generation and transformation, we use LLMs as the primary engine for migration.
Before building the tool, we first study the capabilities of LLMs for library
migration on a benchmark of 321 real-world library migrations. We find that
LLMs can effectively perform library migration, but some post-processing steps
can further improve the performance. Based on this, we develop PyMigTool, a
command line application that combines the power of LLMs, static analysis, and
dynamic analysis to provide accurate library migration. We evaluate PyMigTool
on 717 real-world Python applications that are not from our benchmark. We find
that PyMigTool can migrate 32% of the migrations with complete correctness. Of
the remaining migrations, only 14% of the migration-related changes are left
for developers to fix for more than half of the projects.

</details>


### [15] [McMining: Automated Discovery of Misconceptions in Student Code](https://arxiv.org/abs/2510.08827)
*Erfan Al-Hossami,Razvan Bunescu*

Main category: cs.SE

TL;DR: 提出了McMining任务，用于从学生代码样本中挖掘编程误解，并开发了基准数据集和两种基于LLM的McMiner方法。


<details>
  <summary>Details</summary>
Motivation: 学生在学习编程时经常产生对编程概念的误解，这些误解不仅会导致bug和低效代码，还会阻碍相关概念的学习。

Method: 开发了可扩展的误解基准数据集和代码样本集，引入了两种基于LLM的McMiner方法。

Result: 通过广泛评估表明，Gemini、Claude和GPT系列模型在发现学生代码中的误解方面表现有效。

Conclusion: LLM模型能够有效挖掘学生编程中的误解，为编程教育提供了新的工具。

Abstract: When learning to code, students often develop misconceptions about various
programming language concepts. These can not only lead to bugs or inefficient
code, but also slow down the learning of related concepts. In this paper, we
introduce McMining, the task of mining programming misconceptions from samples
of code from a student. To enable the training and evaluation of McMining
systems, we develop an extensible benchmark dataset of misconceptions together
with a large set of code samples where these misconceptions are manifested. We
then introduce two LLM-based McMiner approaches and through extensive
evaluations show that models from the Gemini, Claude, and GPT families are
effective at discovering misconceptions in student code.

</details>


### [16] [Identifying Video Game Debugging Bottlenecks: An Industry Perspective](https://arxiv.org/abs/2510.08834)
*Carlos Pinto Gomez,Fabio Petrillo*

Main category: cs.SE

TL;DR: 该论文研究了游戏开发中的调试实践，分析了20名资深游戏开发者的调试过程，发现开发者36.6%的时间用于检查游戏工件，35.1%的时间用于本地重现bug。


<details>
  <summary>Details</summary>
Motivation: 传统软件调试技术在游戏开发中应用有限，游戏开发需要独特的调试技术，如屏幕控制台、调试绘制、调试相机等，但目前缺乏对这些实践的系统研究。

Method: 通过记录游戏工作室中20名资深开发者在处理崩溃、对象行为和对象持久性等关键bug时的调试会话，进行主题分析。

Result: 识别出调试过程中的瓶颈活动，分析了使用的调试工具，展示了不同专业角色在调试中的协作方式，发现技术角色在调试中处于核心地位。

Conclusion: 游戏开发者大部分时间花费在检查游戏工件和重现bug上，需要针对游戏开发特点优化调试工具和流程。

Abstract: Conventional debugging techniques used in traditional software are similarly
used when debugging video games. However, the reality of video games require
its own set of unique debugging techniques such as On-Screen Console, Debug
Draws, Debug Camera, Cheats and In-Game Menus, and Data Scrubbing. In this
article, we provide insights from a video game studio on how 20 seasoned
industry game developers debug during the production of a game. Our experiments
rely on the recordings of debugging sessions for the most critical bugs
categorized as Crashes, Object Behaviors, and Object Persistence. In this
paper, we focus on identifying the debugging activities that bottleneck bug
resolution. We also identify the debugging tools used to perform debugging
techniques. Lastly, we present how different disciplines collaborate during
debugging and how technical roles are at the core of debugging. Our thematic
analysis has identified game developers spend 36.6\% of their time inspecting
game artifacts and 35.1\% of their time reproducing the bug locally.

</details>


### [17] [Repository-Aware File Path Retrieval via Fine-Tuned LLMs](https://arxiv.org/abs/2510.08850)
*Vasudha Yanuganti,Ishaan Puri,Swapnil Chhatre,Mantinder Singh,Ashok Jallepalli,Hritvik Shrivastava,Pradeep Kumar Sharma*

Main category: cs.SE

TL;DR: 提出了一种基于微调LLM的文件路径检索方法，通过代码感知策略生成训练数据，在Python项目中实现高精度的文件路径检索。


<details>
  <summary>Details</summary>
Motivation: 传统代码搜索方法缺乏语义上下文和跨文件链接理解，而大语言模型虽然理解自然语言但缺乏仓库特定细节，需要结合两者优势来改进代码文件检索。

Method: 使用QLoRA和Unsloth优化对Qwen3-8B模型进行微调，通过六种基于AST结构和仓库内容的代码感知策略生成训练数据，包括单文件提示到分层仓库摘要等多种方法。

Result: 在Python项目（Flask、Click、Jinja、FastAPI、PyTorch）上获得高达91%的精确匹配率和93%的召回率，在PyTorch这样的大型代码库（约4000个Python文件）中达到59%的召回率。

Conclusion: 多级代码信号有助于LLM推理跨文件上下文，该方法展示了良好的可扩展性，未来可进一步整合检索与基于LLM的代码智能。

Abstract: Modern codebases make it hard for developers and AI coding assistants to find
the right source files when answering questions like "How does this feature
work?" or "Where was the bug introduced?" Traditional code search (keyword or
IR based) often misses semantic context and cross file links, while large
language models (LLMs) understand natural language but lack repository specific
detail. We present a method for file path retrieval that fine tunes a strong
LLM (Qwen3-8B) with QLoRA and Unsloth optimizations to predict relevant file
paths directly from a natural language query. To build training data, we
introduce six code aware strategies that use abstract syntax tree (AST)
structure and repository content to generate realistic question-answer pairs,
where answers are sets of file paths. The strategies range from single file
prompts to hierarchical repository summaries, providing broad coverage. We fine
tune on Python projects including Flask, Click, Jinja, FastAPI, and PyTorch,
and obtain high retrieval accuracy: up to 91\% exact match and 93\% recall on
held out queries, clearly beating single strategy training. On a large codebase
like PyTorch (about 4,000 Python files), the model reaches 59\% recall, showing
scalability. We analyze how multi level code signals help the LLM reason over
cross file context and discuss dataset design, limits (for example, context
length in very large repos), and future integration of retrieval with LLM based
code intelligence.

</details>


### [18] [Vector Graph-Based Repository Understanding for Issue-Driven File Retrieval](https://arxiv.org/abs/2510.08876)
*Kostiantyn Bevziuk,Andrii Fatula,Svetozar Lashin Yaroslav Opanasenko,Anna Tukhtarova,Ashok Jallepalli Pradeepkumar Sharma,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 将大型软件仓库转换为向量化知识图谱的系统，通过捕获语义关系实现仓库开发的自动化


<details>
  <summary>Details</summary>
Motivation: 解决大型软件仓库难以理解和维护的问题，通过结构化表示来提升开发效率

Method: 构建包含语法关系和LLM生成摘要的知识图谱，结合语义检索和图感知扩展的混合检索管道

Result: 系统能够编码仓库的架构和语义结构，支持自动化的进一步开发

Conclusion: 该方法为软件仓库提供了结构化的知识表示，显著提升了开发自动化水平

Abstract: We present a repository decomposition system that converts large software
repositories into a vectorized knowledge graph which mirrors project
architectural and semantic structure, capturing semantic relationships and
allowing a significant level of automatization of further repository
development. The graph encodes syntactic relations such as containment,
implementation, references, calls, and inheritance, and augments nodes with
LLM-derived summaries and vector embeddings. A hybrid retrieval pipeline
combines semantic retrieval with graph-aware expansion, and an LLM-based
assistant formulates constrained, read-only graph requests and produces
human-oriented explanations.

</details>


### [19] [SEER: Sustainability Enhanced Engineering of Software Requirements](https://arxiv.org/abs/2510.08981)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 提出了SEER框架，在软件开发早期阶段解决可持续性问题，使用LLM推理能力和代理RAG方法，通过识别、评估和优化可持续性需求来提升软件可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为高层指导，实施耗时且依赖团队适应性，而可持续性评估应从需求工程阶段开始，但现有方法主要关注设计或实现阶段。

Method: SEER框架分三个阶段：1) 从通用分类法中识别特定软件产品的可持续性需求；2) 基于识别的SR评估系统需求的可持续性；3) 优化未能满足任何SR的系统需求。使用LLM推理能力和代理RAG方法实现。

Result: 在四个不同领域的软件项目上实验，使用Gemini 2.5推理模型，结果表明该方法能准确识别跨领域的广泛可持续性问题。

Conclusion: SEER框架有效解决了软件开发早期阶段的可持续性问题，能够准确识别和优化可持续性需求，为可持续软件开发提供了实用工具。

Abstract: The rapid expansion of software development has significant environmental,
technical, social, and economic impacts. Achieving the United Nations
Sustainable Development Goals by 2030 compels developers to adopt sustainable
practices. Existing methods mostly offer high-level guidelines, which are
time-consuming to implement and rely on team adaptability. Moreover, they focus
on design or implementation, while sustainability assessment should start at
the requirements engineering phase. In this paper, we introduce SEER, a
framework which addresses sustainability concerns in the early software
development phase. The framework operates in three stages: (i) it identifies
sustainability requirements (SRs) relevant to a specific software product from
a general taxonomy; (ii) it evaluates how sustainable system requirements are
based on the identified SRs; and (iii) it optimizes system requirements that
fail to satisfy any SR. The framework is implemented using the reasoning
capabilities of large language models and the agentic RAG (Retrieval Augmented
Generation) approach. SEER has been experimented on four software projects from
different domains. Results generated using Gemini 2.5 reasoning model
demonstrate the effectiveness of the proposed approach in accurately
identifying a broad range of sustainability concerns across diverse domains.

</details>


### [20] [Towards a Taxonomy of Sustainability Requirements for Software Design](https://arxiv.org/abs/2510.08990)
*Mandira Roy,Novarun Deb,Nabendu Chaki,Agostino Cortesi*

Main category: cs.SE

TL;DR: 该研究通过系统文献综述构建了一个全面的可持续性需求分类法，涵盖环境、技术、社会和经济四个维度，为软件开发者和研究人员提供系统化参考。


<details>
  <summary>Details</summary>
Motivation: 现有可持续性需求研究存在碎片化、局限于特定维度或应用领域的问题，缺乏统一的综合分类法，无法系统指导可持续软件开发。

Method: 采用系统文献综述方法，从最新研究中提取和组织可持续性需求，构建跨四个维度的分类法，并建立相关性矩阵。

Result: 开发了一个包含明确定义、相关指标和度量的可持续性需求综合分类法，以及展示不同维度类别间正负影响的相关性矩阵。

Conclusion: 该分类法为软件开发者和研究人员提供了系统化参考，有助于有效制定、管理和协调可持续软件开发中的权衡关系。

Abstract: Software systems are a significant contributor to global sustainability
concerns, demanding that environmental, social, technical, and economic factors
be systematically addressed from the initial requirements engineering phase.
Although existing research provides various sustainability requirements (SRs),
these contributions are often fragmented, specific to certain dimensions, or
limited to particular application domains, resulting in a critical lack of a
unified, comprehensive taxonomy for the software engineering community. To
address this gap, this research conducts a Systematic Literature Review (SLR)
to extract and organize sustainability requirements from the state-of-the-art.
The primary contribution is a comprehensive taxonomy of SRs across the four
dimensions of sustainability (environmental, technical, social, and economic).
For each identified category, we provide clear definitions, associated metrics,
and measures. Furthermore, we depict a correlation matrix that projects the
positive and negative influences (synergies and conflicts) among categories
across different dimensions. This systematized reference assists both software
developers and researchers in effectively formulating, managing, and
reconciling trade-offs within sustainable software development.

</details>


### [21] [Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent Evaluation](https://arxiv.org/abs/2510.08996)
*Spandan Garg,Ben Steenhoek,Yufan Huang*

Main category: cs.SE

TL;DR: 提出新的基准测试框架，将现有正式基准转换为真实用户查询，发现现有基准显著高估了聊天式编程助手在真实场景中的能力


<details>
  <summary>Details</summary>
Motivation: 当前基于GitHub问题的基准测试无法准确反映开发者在IDE中与聊天式编程助手的真实交互方式，导致对代理能力的系统性高估

Method: 通过分析开发者与聊天式代理的交互模式，将现有正式基准转换为真实用户查询的基准测试框架

Result: 发现现有基准显著高估代理能力，在公共基准上高估超过50%，在内部基准上高估10-16%

Conclusion: 这项工作通过基准变异技术建立了评估交互式聊天式软件工程代理的新范式

Abstract: Current benchmarks for evaluating software engineering agents, such as
SWE-Bench Verified, are predominantly derived from GitHub issues and fail to
accurately reflect how developers interact with chat-based coding assistants in
integrated development environments (IDEs). We posit that this mismatch leads
to a systematic overestimation of agent's capabilities in real-world scenarios,
especially bug fixing. We introduce a novel benchmarking framework that
transforms existing formal benchmarks into realistic user queries through
systematic analysis of developer interaction patterns with chat-based agents.
Our methodology is flexible and can be easily extended to existing benchmarks.
In this paper, we apply our testing framework to SWE-Bench Verified, the
TypeScript subset of Multi-SWE-Bench and a private benchmark, SWE-Bench C# and
transform formal GitHub issue descriptions into realistic user-style queries
based on telemetry analysis of a popular chat-based agent interactions. Our
findings reveal that existing benchmarks significantly overestimate agent
capabilities for some models by >50% over baseline performance for public
benchmarks and ~10-16% for our internal benchmark. This work establishes a new
paradigm for evaluating interactive chat-based software engineering agents
through benchmark mutation techniques.

</details>


### [22] [Cost-Efficient Long Code Translation using LLMs while Leveraging Identifier Replacements](https://arxiv.org/abs/2510.09045)
*Manojit Chakraborty,Madhusudan Ghosh,Rishabh Gupta*

Main category: cs.SE

TL;DR: 提出一种零样本代码翻译方法，通过标识符替换减少token数量，提高长代码翻译的效率和准确性


<details>
  <summary>Details</summary>
Motivation: LLM在翻译长代码时受限于上下文窗口，导致翻译不准确，需要解决长代码翻译的效率和成本问题

Method: 使用标识符替换技术，将用户给定的长标识符替换为通用占位符，让LLM专注于代码逻辑结构

Result: 实验结果表明该方法能保留语法和层次信息，并显著减少翻译所需的token数量

Conclusion: 提出的标识符替换方法有效提升了长代码翻译的效率和成本效益

Abstract: In the domain of software development, LLMs have been utilized to automate
tasks such as code translation, where source code from one programming language
is translated to another while preserving its functionality. However, LLMs
often struggle with long source codes that don't fit into the context window,
which produces inaccurate translations. To address this, we propose a novel
zero-shot code translation method that incorporates identifier replacement. By
substituting user-given long identifiers with generalized placeholders during
translation, our method allows the LLM to focus on the logical structure of the
code, by reducing token count and memory usage, which improves the efficiency
and cost-effectiveness of long code translation. Our empirical results
demonstrate that our approach preserves syntactical and hierarchical
information and produces translation results with reduced tokens.

</details>


### [23] [Model-Assisted and Human-Guided: Perceptions and Practices of Software Professionals Using LLMs for Coding](https://arxiv.org/abs/2510.09058)
*Italo Santos,Cleyton Magalhaes,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 对131名软件从业者的全球调查显示，LLM主要用于编码任务，能提高生产力但存在输出不准确、上下文理解有限等风险，开发者多将其视为辅助工具而非独立解决方案。


<details>
  <summary>Details</summary>
Motivation: 了解LLM在实际软件开发中的使用情况和专业人士对其优缺点的认知，填补对LLM实际应用理解不足的空白。

Method: 通过全球调查收集131名软件从业者的反馈数据。

Result: LLM被用于各种编码任务，带来生产力提升、认知负荷减少和学习加速等益处，但也存在输出不准确、上下文意识有限和伦理风险等问题。

Conclusion: 开发者普遍将LLM视为辅助工具，体现了谨慎而实用的整合态度，为未来研究和LLM在软件工程中的负责任使用提供了重要参考。

Abstract: Large Language Models have quickly become a central component of modern
software development workflows, and software practitioners are increasingly
integrating LLMs into various stages of the software development lifecycle.
Despite the growing presence of LLMs, there is still a limited understanding of
how these tools are actually used in practice and how professionals perceive
their benefits and limitations. This paper presents preliminary findings from a
global survey of 131 software practitioners. Our results reveal how LLMs are
utilized for various coding-specific tasks. Software professionals report
benefits such as increased productivity, reduced cognitive load, and faster
learning, but also raise concerns about LLMs' inaccurate outputs, limited
context awareness, and associated ethical risks. Most developers treat LLMs as
assistive tools rather than standalone solutions, reflecting a cautious yet
practical approach to their integration. Our findings provide an early,
practitioner-focused perspective on LLM adoption, highlighting key
considerations for future research and responsible use in software engineering.

</details>


### [24] [Literate Tracing](https://arxiv.org/abs/2510.09073)
*Matthew Sotoudeh*

Main category: cs.SE

TL;DR: 本文提出了一种称为'文学追踪'的程序文档范式，通过带注释的具体执行追踪来解释软件系统，并开发了TReX工具来创建交互式、可视化的文学追踪。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统变得越来越庞大复杂，系统专家需要向新手解释程序工作原理。现有代码注释缺乏全局上下文，设计文档又缺乏与代码的具体连接。

Method: 使用文学追踪范式，通过带注释的具体执行追踪来解释软件系统。开发了TReX工具来创建交互式、可视化且保证与程序语义一致的追踪文档。

Result: 已使用TReX为大型系统软件（包括Linux内核、Git源代码控制系统和GCC编译器）的组件编写了文学追踪文档。

Conclusion: 文学追踪通过结合具体执行追踪和注释，有效补充了传统代码注释和设计文档的不足，为程序理解提供了新的文档方法。

Abstract: As computer systems grow ever larger and more complex, a crucial task in
software development is for one person (the system expert) to communicate to
another (the system novice) how a certain program works. This paper reports on
the author's experiences with a paradigm for program documentation that we call
literate tracing. A literate trace explains a software system using annotated,
concrete execution traces of the system. Literate traces complement both
in-code comments (which often lack global context) and out-of-band design docs
(which often lack a concrete connection to the code). We also describe TReX,
our tool for making literate traces that are interactive, visual, and
guaranteed by construction to be faithful to the program semantics. We have
used TReX to write literate traces explaining components of large systems
software including the Linux kernel, Git source control system, and GCC
compiler.

</details>


### [25] [Constraint-Guided Unit Test Generation for Machine Learning Libraries](https://arxiv.org/abs/2510.09108)
*Lukas Krodinger,Altin Hajdari,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: PynguinML通过从官方API文档中提取约束条件，改进Pynguin测试生成器，为机器学习API生成合规输入，显著提高代码覆盖率。


<details>
  <summary>Details</summary>
Motivation: 机器学习库如PyTorch和TensorFlow的API有严格的输入约束，涉及张量等复杂数据结构。现有自动化测试工具如Pynguin不了解这些约束，常生成不合规输入，导致测试早期失败和代码覆盖率有限。

Method: 基于从官方API文档中提取的约束条件，改进Pynguin测试生成器，使其能够生成符合ML API要求的输入数据。

Result: 在PyTorch和TensorFlow的165个模块上评估，PynguinML相比Pynguin显著提高测试效果，代码覆盖率最高提升63.9%。

Conclusion: PynguinML通过利用API约束生成合规输入，能够更彻底地测试机器学习库，实现更高的代码覆盖率。

Abstract: Machine learning (ML) libraries such as PyTorch and TensorFlow are essential
for a wide range of modern applications. Ensuring the correctness of ML
libraries through testing is crucial. However, ML APIs often impose strict
input constraints involving complex data structures such as tensors. Automated
test generation tools such as Pynguin are not aware of these constraints and
often create non-compliant inputs. This leads to early test failures and
limited code coverage. Prior work has investigated extracting constraints from
official API documentation. In this paper, we present PynguinML, an approach
that improves the Pynguin test generator to leverage these constraints to
generate compliant inputs for ML APIs, enabling more thorough testing and
higher code coverage. Our evaluation is based on 165 modules from PyTorch and
TensorFlow, comparing PynguinML against Pynguin. The results show that
PynguinML significantly improves test effectiveness, achieving up to 63.9 %
higher code coverage.

</details>


### [26] [A Semantic Framework for Patient Digital Twins in Chronic Care](https://arxiv.org/abs/2510.09134)
*Amal Elgammal,Bernd J. Krämer,Michael P. Papazoglou,Mira Raheem*

Main category: cs.SE

TL;DR: 本文提出了患者医疗数字孪生(PMDT)框架，通过本体论驱动的方法整合多模态健康数据，为个性化慢性病护理提供统一、隐私保护的数字化基础。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生应用多为器官特异性或局限于孤立数据类型，缺乏统一且隐私保护的框架来支持精确、自适应和预防性的慢性病管理决策。

Method: 采用OWL 2.0实现的本体驱动框架，围绕模块化蓝图(患者、疾病诊断、治疗随访、轨迹、安全、路径和不良事件)构建，通过专家工作坊、问卷调查和真实世界免疫治疗患者试点研究进行迭代精化和验证。

Result: 评估确认了本体覆盖度、推理正确性、可用性和GDPR合规性。PMDT能够统一异构数据，操作能力问题，支持描述性、预测性和规范性分析，以联邦化、隐私保护的方式实现。

Conclusion: PMDT通过弥合数据碎片化和语义标准化方面的差距，为下一代数字健康生态系统提供了经过验证的基础，将慢性病护理转变为主动、持续优化和公平的管理模式。

Abstract: Personalized chronic care requires the integration of multimodal health data
to enable precise, adaptive, and preventive decision-making. Yet most current
digital twin (DT) applications remain organ-specific or tied to isolated data
types, lacking a unified and privacy-preserving foundation. This paper
introduces the Patient Medical Digital Twin (PMDT), an ontology-driven in
silico patient framework that integrates physiological, psychosocial,
behavioral, and genomic information into a coherent, extensible model.
Implemented in OWL 2.0, the PMDT ensures semantic interoperability, supports
automated reasoning, and enables reuse across diverse clinical contexts. Its
ontology is structured around modular Blueprints (patient, disease and
diagnosis, treatment and follow-up, trajectories, safety, pathways, and adverse
events), formalized through dedicated conceptual views. These were iteratively
refined and validated through expert workshops, questionnaires, and a pilot
study in the EU H2020 QUALITOP project with real-world immunotherapy patients.
Evaluation confirmed ontology coverage, reasoning correctness, usability, and
GDPR compliance. Results demonstrate the PMDT's ability to unify heterogeneous
data, operationalize competency questions, and support descriptive, predictive,
and prescriptive analytics in a federated, privacy-preserving manner. By
bridging gaps in data fragmentation and semantic standardization, the PMDT
provides a validated foundation for next-generation digital health ecosystems,
transforming chronic care toward proactive, continuously optimized, and
equitable management.

</details>


### [27] [A Model-Driven Engineering Approach to AI-Powered Healthcare Platforms](https://arxiv.org/abs/2510.09308)
*Mira Raheem,Amal Elgammal,Michael Papazoglou,Bernd Krämer,Neamat El-Tazi*

Main category: cs.SE

TL;DR: 提出基于模型驱动工程的医疗AI框架，使用图形化领域特定语言MILA和联邦学习架构，实现跨机构协作的医疗数据分析，在癌症免疫治疗研究中取得高准确率。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI应用中的数据碎片化、隐私保护和系统复杂性等挑战，促进医疗AI在临床实践中的采用。

Method: 开发基于形式化元模型、领域特定语言和自动化转换的模型驱动工程框架，核心是图形化医疗互操作性语言MILA，结合联邦学习架构实现隐私保护下的跨机构协作。

Result: 在多中心癌症免疫治疗研究中，生成的机器学习管道在关键任务中达到98.5%和98.3%的高准确率，同时显著减少手动编码工作量。

Conclusion: 模型驱动工程原则（元建模、语义集成和自动化代码生成）为实现可互操作、可复现和可信赖的数字健康平台提供了实用路径。

Abstract: Artificial intelligence (AI) has the potential to transform healthcare by
supporting more accurate diagnoses and personalized treatments. However, its
adoption in practice remains constrained by fragmented data sources, strict
privacy rules, and the technical complexity of building reliable clinical
systems. To address these challenges, we introduce a model driven engineering
(MDE) framework designed specifically for healthcare AI. The framework relies
on formal metamodels, domain-specific languages (DSLs), and automated
transformations to move from high level specifications to running software. At
its core is the Medical Interoperability Language (MILA), a graphical DSL that
enables clinicians and data scientists to define queries and machine learning
pipelines using shared ontologies. When combined with a federated learning
architecture, MILA allows institutions to collaborate without exchanging raw
patient data, ensuring semantic consistency across sites while preserving
privacy. We evaluate this approach in a multi center cancer immunotherapy
study. The generated pipelines delivered strong predictive performance, with
support vector machines achieving up to 98.5 percent and 98.3 percent accuracy
in key tasks, while substantially reducing manual coding effort. These findings
suggest that MDE principles metamodeling, semantic integration, and automated
code generation can provide a practical path toward interoperable,
reproducible, and trustworthy digital health platforms.

</details>


### [28] [TIT: A Tree-Structured Instruction Tuning Approach for LLM-Based Code Translation](https://arxiv.org/abs/2510.09400)
*He Jiang,Yufu Wang,Hao Lin,Peiyu Zou,Zhide Zhou,Ang Jia,Xiaochen Li,Zhilei Ren*

Main category: cs.SE

TL;DR: TIT是一种基于树结构指令调优的代码翻译方法，通过集成语言无关的句法特征和细粒度并行数据增强，显著提升了LLM在代码翻译中的准确性和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代码翻译方法存在两个关键问题：对语言特定特征敏感导致句法混淆，以及过度依赖函数级并行数据导致语义对齐不足。

Method: TIT包含三个模块：句法信息表示模块集成语言无关句法特征，细粒度并行数据集增强模块通过语句级分割和对齐匹配生成高质量数据，双阶段树指令调优模块通过句法感知微调和代码生成微调减轻LLM处理负担。

Result: 实验结果表明该方法在多个LLM中显著优于现有方法，代码翻译成功率提高1.22-1.75倍，同时显著减少句法混淆。

Conclusion: TIT通过树结构指令调优有效解决了LLM代码翻译中的句法混淆和语义对齐问题，为代码翻译任务提供了新的解决方案。

Abstract: Large Language Models (LLMs) have shown strong performance in automated
source-to-target code translation through pretraining on extensive code
corpora. However, mainstream LLM-based code translation methods suffer from two
critical limitations. First, they are highly sensitive to language-specific
features, which often introduce source-language syntax or lexicon into the
output, leading to syntactic confusion. Second, they lack fine-grained semantic
alignment due to an over-reliance on function-level parallel datasets,
resulting in semantic misalignment between the translated code and the original
source. To overcome these limitations, we propose TIT, a Tree-structured
Instruction Tuning paradigm for LLM-based code translation. Specifically, TIT
consists of three modules. First, to mitigate syntactic confusion, the
syntactic information representation module integrates language-agnostic
syntactic features via structured parsing. Then, to generate high-quality
fine-grained parallel data, the fine-grained parallel dataset augmentation
module aligns nodes with code segments through statement-level segmentation and
contrastive matching. Finally, we leverage the dual-stage tree instruction
tuning module to alleviate the contextual processing burden on the LLM caused
by the introduction of syntactic information. The first stage employs
syntax-aware fine-tuning to enable the LLM to autonomously comprehend
structured syntactic information, while the second stage utilizes code
generation fine-tuning to guide the model in generating accurate target code
based on function-level syntactic dependencies. The experimental results
demonstrate that the proposed method significantly outperforms existing
approaches in multiple LLMs, achieving a success rate 1.22x-1.75x higher in
code translation while markedly reducing syntactic confusion.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [29] [Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly](https://arxiv.org/abs/2510.08863)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DB

TL;DR: 对Redis、Aerospike和Dragonfly三种NoSQL键值存储系统进行性能评估，使用YCSB基准测试框架，测试不同工作负载模式下的延迟、吞吐量和内存特性。


<details>
  <summary>Details</summary>
Motivation: 分布式应用和云计算的发展对可扩展、高性能的键值存储系统提出了需求，需要评估不同NoSQL系统的性能表现。

Method: 使用Yahoo! Cloud Serving Benchmark (YCSB)框架，在三种不同工作负载模式（读密集、写密集、平衡）下，系统性地改变客户端并发数（1到32个客户端）进行广泛实验。

Result: 评估方法捕获了实际运行条件下的延迟、吞吐量和内存特性，揭示了各系统的性能权衡和可扩展性行为。

Conclusion: 该研究为不同NoSQL键值存储系统的性能特征提供了深入见解，有助于在实际应用场景中选择合适的存储解决方案。

Abstract: The rise of distributed applications and cloud computing has created a demand
for scalable, high-performance key-value storage systems. This paper presents a
performance evaluation of three prominent NoSQL key-value stores: Redis,
Aerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB)
framework. We conducted extensive experiments across three distinct workload
patterns (read-heavy, write-heavy), and balanced while systematically varying
client concurrency from 1 to 32 clients. Our evaluation methodology captures
both latency, throughput, and memory characteristics under realistic
operational conditions, providing insights into the performance trade-offs and
scalability behaviour of each system

</details>


### [30] [HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance](https://arxiv.org/abs/2510.08896)
*Suming Qiu,Jing Li,Zhicheng Zhou,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.DB

TL;DR: HES-SQL是一个新颖的混合训练框架，通过思维模式融合的监督微调与组相对策略优化的结合，提升了Text-to-SQL生成性能，在保持语义准确性的同时优化查询执行效率。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL系统在平衡语义准确性和计算效率方面存在挑战，需要开发能够同时优化查询正确性和执行性能的新方法。

Method: 提出三个关键创新：骨架完整性评分机制增强查询与最优SQL结构的对齐；查询延迟感知奖励系统激励生成计算高效的SQL查询；思维模式完成的自蒸馏过程防止模型推理能力退化。

Result: 在MySQL 8.0和SQLite 3.42上的实验显示，HES-SQL在BIRD和KaggleDBQA基准测试上分别达到79.14%和54.9%的执行准确率，效率提升11%-20%。

Conclusion: 该框架为Text-to-SQL系统建立了新范式，通过执行感知的强化学习有效平衡语义准确性和计算效率，对开发稳健的自然语言数据库接口具有重要意义。

Abstract: We present HES-SQL, a novel hybrid training framework that advances
Text-to-SQL generation through the integration of thinking-mode-fused
supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO).
Our approach introduces three key innovations: (1) a skeleton-completeness
scoring mechanism that enhances preference alignment between generated queries
and optimal SQL structures; (2) a query-latency-aware reward system that
incentivizes the generation of computationally efficient SQL queries; (3) a
self-distillation process for thinking-mode completion that prevents
degradation of the model's reasoning capabilities. This framework enables
hybrid thinking models to switch between reasoning and non-reasoning modes
while improving SQL query accuracy and execution efficiency.
  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under
controlled single-user conditions, demonstrates that HES-SQL achieves
competitive performance with execution accuracies of 79.14\% and 54.9\% on the
BIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the
end-to-end execution time of generated queries on the DBMS, averaged over
multiple runs to mitigate variance. Efficiency gains range from 11\% to 20\%
relative to supervised baselines. Our results establish a new paradigm for
Text-to-SQL systems that effectively balances semantic accuracy with
computational efficiency through execution-informed reinforcement learning
(RL). The proposed methodology has significant implications for developing
robust natural language interfaces to databases and can be extended to broader
structured generation tasks requiring both correctness and efficiency
optimization.

</details>
