<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.SE](#cs.SE) [Total: 19]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Trajectory-Driven Multi-Product Influence Maximization in Billboard Advertising](https://arxiv.org/abs/2601.14737)
*Dildar Ali,Suman Banerjee,Rajibul Islam*

Main category: cs.DB

TL;DR: 该论文研究多产品广告牌投放问题，提出两种变体：共同槽位选择和互斥槽位选择，分别建模为多子模覆盖问题及其推广，并设计相应的近似算法。


<details>
  <summary>Details</summary>
Motivation: 传统广告牌广告关注单个产品的影响力最大化，但商业公司通常需要同时推广多个产品，每个产品有特定的影响力需求。现有研究缺乏考虑多产品推广场景下的广告牌选择问题。

Method: 1. 共同槽位变体：建模为多子模覆盖问题，设计基于连续贪心框架和随机舍入的双准则近似算法
2. 互斥槽位变体：建模为多子模覆盖问题的推广，提出基于采样的近似方法和高效的原对偶贪心算法，确保槽位互斥性

Result: 在真实轨迹和广告牌数据集上的大量实验表明，所提出的解决方案在效果和效率方面表现优异，能够有效满足多产品推广的需求。

Conclusion: 该研究填补了多产品广告牌投放问题的空白，提出的算法框架能够有效解决共同槽位和互斥槽位两种变体，为实际广告投放提供了理论支持和实用工具。

Abstract: Billboard Advertising has emerged as an effective out-of-home advertising technique, where the goal is to select a limited number of slots and play advertisement content there, with the hope that it will be observed by many people and, effectively, a significant number of them will be influenced towards the brand. Given a trajectory and a billboard database and a positive integer $k$, how can we select $k$ highly influential slots to maximize influence? In this paper, we study a variant of this problem where a commercial house wants to make a promotion of multiple products, and there is an influence demand for each product. We have studied two variants of the problem. In the first variant, our goal is to select $k$ slots such that the respective influence demand of each product is satisfied. In the other variant of the problem, we are given with $\ell$ integers $k_1,k_2, \ldots, k_{\ell}$, the goal here is to search for $\ell$ many set of slots $S_1, S_2, \ldots, S_{\ell}$ such that for all $i \in [\ell]$, $|S_{i}| \leq k_i$ and for all $i \neq j$, $S_i \cap S_j=\emptyset$ and the influence demand of each of the products gets satisfied. We model the first variant of the problem as a multi-submodular cover problem and the second variant as its generalization. To solve the common-slot variant, we formulate the problem as a multi-submodular cover problem and design a bi-criteria approximation algorithm based on the continuous greedy framework and randomized rounding. For the disjoint-slot variant, we proposed a sampling-based approximation approach along with an efficient primal-dual greedy algorithm that enforces disjointness naturally. Extensive experiments with real-world trajectory and billboard datasets highlight the effectiveness and efficiency of the proposed solution approaches.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [JAXMg: A multi-GPU linear solver in JAX](https://arxiv.org/abs/2601.14466)
*Roeland Wiersema*

Main category: cs.DC

TL;DR: JAXMg为JAX提供多GPU稠密线性代数功能，通过集成NVIDIA cuSOLVERMg实现跨GPU内存限制的Cholesky线性求解和对称特征分解。


<details>
  <summary>Details</summary>
Motivation: 现代科学计算中需要处理大规模稠密线性系统和特征值问题，但现有编程框架难以在单GPU之外扩展。虽然有多GPU求解器库，但难以集成到可组合的JIT编译Python工作流中。

Method: 通过XLA外部函数接口将JAX与NVIDIA cuSOLVERMg连接，将分布式GPU求解器暴露为JIT兼容的JAX原语，使可扩展线性代数能直接嵌入JAX程序中。

Result: 实现了多GPU稠密线性代数功能，支持超过单GPU内存限制的矩阵的Cholesky线性求解和对称特征分解，保持与JAX变换的可组合性。

Conclusion: JAXMg成功将分布式GPU求解器集成到JAX生态系统中，使可扩展线性代数能在端到端科学工作流中实现多GPU执行，解决了大规模稠密线性代数在JIT编译工作流中的扩展挑战。

Abstract: Solving large dense linear systems and eigenvalue problems is a core requirement in many areas of scientific computing, but scaling these operations beyond a single GPU remains challenging within modern programming frameworks. While highly optimized multi-GPU solver libraries exist, they are typically difficult to integrate into composable, just-in-time (JIT) compiled Python workflows. JAXMg provides multi-GPU dense linear algebra for JAX, enabling Cholesky-based linear solves and symmetric eigendecompositions for matrices that exceed single-GPU memory limits. By interfacing JAX with NVIDIA's cuSOLVERMg through an XLA Foreign Function Interface, JAXMg exposes distributed GPU solvers as JIT-compatible JAX primitives. This design allows scalable linear algebra to be embedded directly within JAX programs, preserving composability with JAX transformations and enabling multi-GPU execution in end-to-end scientific workflows.

</details>


### [3] [Exploring Performance-Productivity Trade-offs in AMT Runtimes: A Task Bench Study of Itoyori, ItoyoriFBC, HPX, and MPI](https://arxiv.org/abs/2601.14608)
*Torben R. Lahnor,Mia Reitz,Jonas Posner,Patrick Diehl*

Main category: cs.DC

TL;DR: 该研究将Itoyori和ItoyoriFBC两种异步多任务运行时集成到Task Bench框架中，与MPI和HPX进行综合性能与生产力对比评估。


<details>
  <summary>Details</summary>
Motivation: 异步多任务运行时作为MPI的替代方案具有优势，但多样化的AMT生态系统使得公平比较变得困难。需要系统性地评估不同并行编程系统的性能与程序员生产力。

Method: 使用Task Bench参数化框架，集成Itoyori（基于PGAS和RDMA工作窃取）和ItoyoriFBC（扩展了基于future的同步）两种AMT运行时。通过应用效率、最小有效任务粒度评估性能，通过代码行数和库构造数量评估程序员生产力。

Result: MPI在规则、通信轻量级工作负载中效率最高但代码冗长；HPX在负载不均衡时保持稳定效率但生产力指标最差；Itoyori在通信密集型配置中效率最高且生产力最佳；ItoyoriFBC效率略低于Itoyori但为不规则工作负载提供表达潜力。

Conclusion: 不同系统存在明显的权衡取舍：AMT运行时并不天然保证比MPI更高的生产力，Itoyori在通信密集型场景中表现最佳，而future-based同步为不规则工作负载提供了有价值的扩展能力。

Abstract: Asynchronous Many-Task (AMT) runtimes offer a productive alternative to the Message Passing Interface (MPI). However, the diverse AMT landscape makes fair comparisons challenging. Task Bench, proposed by Slaughter et al., addresses this challenge through a parameterized framework for evaluating parallel programming systems. This work integrates two recent cluster AMTs, Itoyori and ItoyoriFBC, into Task Bench for comprehensive evaluation against MPI and HPX. Itoyori employs a Partitioned Global Address Space (PGAS) model with RDMA-based work stealing, while ItoyoriFBC extends it with futurebased synchronization.
  We evaluate these systems in terms of both performance and programmer productivity. Performance is assessed across various configurations, including compute-bound kernels, weak scaling, and both imbalanced and communication-intensive patterns. Performance is quantified using application efficiency, i.e., the percentage of maximum performance achieved, and the Minimum Effective Task Granularity (METG), i.e., the smallest task duration before runtime overheads dominate. Programmer productivity is quantified using Lines of Code (LOC) and the Number of Library Constructs (NLC).
  Our results reveal distinct trade-offs. MPI achieves the highest efficiency for regular, communication-light workloads but requires verbose, lowlevel code. HPX maintains stable efficiency under load imbalance across varying node counts, yet ranks last in productivity metrics, demonstrating that AMTs do not inherently guarantee improved productivity over MPI. Itoyori achieves the highest efficiency in communication-intensive configurations while leading in programmer productivity. ItoyoriFBC exhibits slightly lower efficiency than Itoyori, though its future-based synchronization offers potential for expressing irregular workloads.

</details>


### [4] [Exploiting Spot Instances for Time-Critical Cloud Workloads Using Optimal Randomized Strategies](https://arxiv.org/abs/2601.14612)
*Neelkamal Bhuyan,Randeep Bhatia,Murali Kodialam,TV Lakshman*

Main category: cs.DC

TL;DR: 提出随机调度算法ROSS，在混合云环境中实现最优竞争比√K，显著改进现有方法，实际评估显示成本节省提升30%


<details>
  <summary>Details</summary>
Motivation: 解决混合云环境中具有硬截止时间的作业调度挑战，现有确定性策略存在Ω(K)的最坏情况竞争比限制，需要更优的调度算法来平衡成本优化和截止时间保证

Method: 提出随机调度算法ROSS，针对混合云环境中的作业调度问题，在合理的截止时间约束下，通过随机化策略实现最优调度性能

Result: 理论证明ROSS算法达到最优竞争比√K，显著优于现有方法的Ω(K)下界；在Azure和AWS真实追踪数据上的评估显示，ROSS比现有最优方法节省高达30%的成本

Conclusion: ROSS算法在混合云调度中实现了理论最优性能，有效平衡了成本优化和截止时间保证，在不同现货市场条件下均表现优异

Abstract: This paper addresses the challenge of deadline-aware online scheduling for jobs in hybrid cloud environments, where jobs may run on either cost-effective but unreliable spot instances or more expensive on-demand instances, under hard deadlines. We first establish a fundamental limit for existing (predominantly-) deterministic policies, proving a worst-case competitive ratio of $Ω(K)$, where $K$ is the cost ratio between on-demand and spot instances. We then present a novel randomized scheduling algorithm, ROSS, that achieves a provably optimal competitive ratio of $\sqrt{K}$ under reasonable deadlines, significantly improving upon existing approaches. Extensive evaluations on real-world trace data from Azure and AWS demonstrate that ROSS effectively balances cost optimization and deadline guarantees, consistently outperforming the state-of-the-art by up to $30\%$ in cost savings, across diverse spot market conditions.

</details>


### [5] [Specifying and Verifying RDMA Synchronisation (Extended Version)](https://arxiv.org/abs/2601.14642)
*Guillaume Ambal,Max Stupple,Brijesh Dongol,Azalea Raad*

Main category: cs.DC

TL;DR: 本文提出了RDMA⁽ᴛˢᴏ⁾ᴿᴹᵂ语义，首次形式化远程RMW指令，解决了RDMA程序验证中远程同步缺失的问题，并构建了可组合的同步抽象库和三类远程锁。


<details>
  <summary>Details</summary>
Motivation: 现有RDMA⁽ᴛˢᴏ⁾语义缺乏远程同步的形式化描述，导致无法验证锁等常见抽象的实现正确性。需要填补这一空白，为RDMA程序提供完整的验证基础。

Method: 1. 提出RDMA⁽ᴛˢᴏ⁾ᴿᴹᵂ语义，形式化远程RMW指令；2. 构建RDMA⁽ᴡᴀɪᴛ⁾ᴿᴹᵂ库作为可组合同步抽象基础；3. 基于此库设计、实现并验证三类适用于不同场景的远程锁；4. 提出类似顺序一致性的强RDMA模型RDMA⁽ˢᶜ⁾ᴿᴹᵂ。

Result: 1. 发现远程RMW操作较弱，仅保证对其他远程RMW的原子性；2. 成功构建了可组合的同步抽象库；3. 验证了三类远程锁的正确性；4. 提出的库与现有高性能LOCO库兼容，确保可组合性和可验证性。

Conclusion: 本文填补了RDMA语义中远程同步的空白，为RDMA程序验证提供了完整基础，使锁等同步原语的实现验证成为可能，推动了RDMA编程模型的形式化发展。

Abstract: Remote direct memory access (RDMA) allows a machine to directly read from and write to the memory of remote machine, enabling high-throughput, low-latency data transfer. Ensuring correctness of RDMA programs has only recently become possible with the formalisation of $\text{RDMA}^\text{TSO}$ semantics (describing the behaviour of RDMA networking over a TSO CPU). However, this semantics currently lacks a formalisation of remote synchronisation, meaning that the implementations of common abstractions such as locks cannot be verified. In this paper, we close this gap by presenting $\text{RDMA}^{\text{TSO}}_{\text{RMW}}$, the first semantics for remote `read-modify-write' (RMW) instructions over TSO. It turns out that remote RMW operations are weak and only ensure atomicity against other remote RMWs. We therefore build a set of composable synchronisation abstractions starting with the $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$ library. Underpinned by $\text{RDMA}^{\text{WAIT}}_{\text{RMW}}$, we then specify, implement and verify three classes of remote locks that are suitable for different scenarios. Additionally, we develop the notion of a strong RDMA model, $\text{RDMA}^{\text{SC}}_{\text{RMW}}$, which is akin to sequential consistency in shared memory architectures. Our libraries are built to be compatible with an existing set of high-performance libraries called LOCO, which ensures compositionality and verifiability.

</details>


### [6] [Optimizing FaaS Platforms for MCP-enabled Agentic Workflows](https://arxiv.org/abs/2601.14735)
*Varad Kulkarni,Vaibhav Jha,Nikhil Reddy,Yogesh Simmhan*

Main category: cs.DC

TL;DR: FAME是一个基于FaaS的架构，用于编排支持MCP的智能体工作流，通过将智能体模式分解为可组合的FaaS函数，解决服务器部署和状态管理问题，显著降低延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 基于LLM和MCP的自主AI智能体工作流快速增长，但面临可扩展云部署和状态管理的挑战。传统VM托管资源密集且缺乏弹性，而FaaS平台虽然模块化、可自动扩展且成本效益高，但本质上是无状态的。

Method: FAME将智能体模式（如ReAct）分解为可组合的智能体：规划器、执行器和评估器，每个都是使用LangGraph构建的FaaS函数，并编排为FaaS工作流。使用DynamoDB实现跨用户请求的上下文持久化，通过AWS Lambda包装器优化MCP服务器部署，在S3中缓存工具输出，并提出函数融合策略。

Result: 在两个代表性应用（研究论文摘要和日志分析）上评估，结果显示延迟降低高达13倍，输入令牌减少88%，成本节省66%，工作流完成率提高。

Conclusion: FAME证明了无服务器平台在规模化托管复杂多智能体AI工作流方面的可行性，解决了传统部署方法的资源密集性和状态管理问题。

Abstract: Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88% fewer input tokens and 66% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.

</details>


### [7] [AlertGuardian: Intelligent Alert Life-Cycle Management for Large-scale Cloud Systems](https://arxiv.org/abs/2601.14912)
*Guangba Yu,Genting Mai,Rui Wang,Ruipeng Li,Pengfei Chen,Long Pan,Ruijie Xu*

Main category: cs.DC

TL;DR: AlertGuardian框架利用LLM和图模型优化云系统告警生命周期管理，通过降噪、摘要和规则优化三阶段显著减少告警疲劳并提升诊断效率。


<details>
  <summary>Details</summary>
Motivation: 大规模云系统中告警数量过多导致告警疲劳，降低运维效率，需要优化告警生命周期管理来解决这一问题。

Method: 提出AlertGuardian框架，结合大语言模型和轻量级图模型，包含三个阶段：告警降噪（使用带虚拟噪声的图学习模型）、告警摘要（基于RAG的LLM生成可操作摘要）、告警规则优化（多智能体迭代反馈改进规则质量）。

Result: 在四个真实数据集上评估，显著缓解告警疲劳（94.8%告警减少率），加速故障诊断（90.5%诊断准确率），改进1174条告警规则，其中375条被SRE接受（32%接受率）。

Conclusion: AlertGuardian成功优化了云系统告警生命周期管理，分享了部署后的成功经验和教训，为类似系统提供了有效解决方案。

Abstract: Alerts are critical for detecting anomalies in large-scale cloud systems, ensuring reliability and user experience. However, current systems generate overwhelming volumes of alerts, degrading operational efficiency due to ineffective alert life-cycle management. This paper details the efforts of Company-X to optimize alert life-cycle management, addressing alert fatigue in cloud systems. We propose AlertGuardian, a framework collaborating large language models (LLMs) and lightweight graph models to optimize the alert life-cycle through three phases: Alert Denoise uses graph learning model with virtual noise to filter noise, Alert Summary employs Retrieval Augmented Generation (RAG) with LLMs to create actionable summary, and Alert Rule Refinement leverages multi-agent iterative feedbacks to improve alert rule quality. Evaluated on four real-world datasets from Company-X's services, AlertGuardian significantly mitigates alert fatigue (94.8\% alert reduction ratios) and accelerates fault diagnosis (90.5\% diagnosis accuracy). Moreover, AlertGuardian improves 1,174 alert rules, with 375 accepted by SREs (32% acceptance rate). Finally, we share success stories and lessons learned about alert life-cycle management after the deployment of AlertGuardian in Company-X.

</details>


### [8] [Application-level observability for adaptive Edge to Cloud continuum systems](https://arxiv.org/abs/2601.14923)
*Kaddour Sidi,Daniel Balouek,Baptiste Jonglez*

Main category: cs.DC

TL;DR: 提出了一个应用级可观测性框架，用于现代边缘到云系统，通过开发者驱动的仪器化和SLO感知反馈实现自主适应，在视频处理用例中展示了自动调整以维持性能目标的能力。


<details>
  <summary>Details</summary>
Motivation: 现代边缘到云系统需要细粒度的可观测性，以确保在异构和动态环境中实现自适应行为并满足性能目标。现有系统缺乏有效的应用级监控和自适应控制机制。

Method: 开发了一个应用级可观测性框架，集成了OpenTelemetry、Prometheus、K3s和Chaos Mesh等技术。该框架结合开发者驱动的仪器化和SLO感知反馈，实现实时监控和自适应控制。

Result: 通过视频处理用例验证，应用级指标能够指导自动调整，在可变工作负载和注入故障的情况下维持目标帧率、延迟和检测精度。初步结果显示提高了可扩展性、容错性和响应性。

Conclusion: 该框架为自适应、符合SLO的边缘到云应用提供了实用基础，通过应用级可观测性实现了更好的系统适应性和性能保证。

Abstract: Modern Edge-to-Cloud (E2C) systems require fine-grained observability to ensure adaptive behavior and compliance with performance objectives across heterogeneous and dynamic environments. This work introduces an application-level observability framework that integrates developer-driven instrumentation and SLO-aware feedback for autonomous adaptation. By combining OpenTelemetry, Prometheus, K3s, and Chaos Mesh, the framework enables real-time monitoring and adaptive control across the continuum. A video processing use case demonstrates how application-level metrics guide automatic adjustments to maintain target frame rate, latency, and detection accuracy under variable workloads and injected faults. Preliminary results highlight improved scalability, fault tolerance, and responsiveness, providing a practical foundation for adaptive, SLO-compliant E2C applications.

</details>


### [9] [Parallel Collaborative ADMM Privacy Computing and Adaptive GPU Acceleration for Distributed Edge Networks](https://arxiv.org/abs/2601.14980)
*Mengchun Xia,Zhicheng Dong,Donghong Cai,Fang Fang,Lisheng Fan,Pingzhi Fan*

Main category: cs.DC

TL;DR: 提出3P-ADMM-PC2算法，结合并行ADMM、Paillier同态加密和GPU加速，用于边缘网络分布式计算中的隐私保护


<details>
  <summary>Details</summary>
Motivation: 边缘网络分布式计算面临单节点计算能力有限、协作计算导致信息泄露和通信开销过大的问题，需要隐私保护的高效分布式计算方案

Method: 设计并行协作分布式ADMM，结合Paillier同态加密保护交互数据隐私，引入量化方法将实数映射到正整数区间，利用GPU加速实现并行加密解密计算

Result: 3P-ADMM-PC2具有优异的均方误差性能，接近无隐私保护的分布式ADMM，相比CPU实现的集中式和分布式ADMM有显著加速比

Conclusion: 提出的GPU加速3P-ADMM-PC2算法能有效解决边缘网络分布式计算中的隐私保护和计算效率问题

Abstract: Distributed computing has been widely applied in distributed edge networks for reducing the processing burden of high-dimensional data centralization, where a high-dimensional computational task is decomposed into multiple low-dimensional collaborative processing tasks or multiple edge nodes use distributed data to train a global model. However, the computing power of a single-edge node is limited, and collaborative computing will cause information leakage and excessive communication overhead. In this paper, we design a parallel collaborative distributed alternating direction method of multipliers (ADMM) and propose a three-phase parallel collaborative ADMM privacy computing (3P-ADMM-PC2) algorithm for distributed computing in edge networks, where the Paillier homomorphic encryption is utilized to protect data privacy during interactions. Especially, a quantization method is introduced, which maps the real numbers to a positive integer interval without affecting the homomorphic operations. To address the architectural mismatch between large-integer and Graphics Processing Unit (GPU) computing, we transform high-bitwidth computations into low-bitwidth matrix and vector operations. Thus the GPU can be utilized to implement parallel encryption and decryption computations with long keys. Finally, a GPU-accelerated 3P-ADMM-PC2 is proposed to optimize the collaborative computing tasks. Meanwhile, large-scale computational tasks are conducted in network topologies with varying numbers of edge nodes. Experimental results demonstrate that the proposed 3P-ADMM-PC2 has excellent mean square error performance, which is close to that of distributed ADMM without privacy-preserving. Compared to centralized ADMM and distributed ADMM implemented with Central Processing Unit (CPU) computation, the proposed scheme demonstrates a significant speedup ratio.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [CMind: An AI Agent for Localizing C Memory Bugs](https://arxiv.org/abs/2601.14434)
*Chia-Yi Su,Collin McMillan*

Main category: cs.SE

TL;DR: CMind是一个基于人类程序员行为模式的AI代理，用于定位C语言内存错误，结合LLM推理和编码的决策指导来模拟人类调试过程。


<details>
  <summary>Details</summary>
Motivation: 现有工具在定位C内存错误时往往缺乏人类程序员的系统性思维过程。通过实证研究观察人类程序员如何查找内存错误，作者希望构建一个能模拟这种人类行为的AI代理，提高内存错误定位的效率和准确性。

Method: CMind通过读取bug报告找到程序入口点，然后导航源代码进行分析，生成符合模板的假设位置和理由。工具结合大型语言模型推理与编码的决策指导，模拟人类程序员在查找内存错误时的行为模式。

Result: CMind能够接收C程序源代码和bug报告作为输入，输出关于bug原因和位置的假设。工具展示了结合LLM推理与人类行为模拟的方法在内存错误定位任务上的可行性。

Conclusion: 通过模拟人类程序员查找内存错误的行为模式，CMind展示了AI代理在软件调试任务中的潜力，为结合人类专业知识与AI推理的调试工具开发提供了新思路。

Abstract: This demonstration paper presents CMind, an artificial intelligence agent for localizing C memory bugs. The novel aspect to CMind is that it follows steps that we observed human programmers perform during empirical study of those programmers finding memory bugs in C programs. The input to the tool is a C program's source code and a bug report describing the problem. The output is the tool's hypothesis about the reason for the bug and its location. CMind reads the bug report to find potential entry points to the program, then navigates the program's source code, analyzes that source code, and generates a hypothesis location and rationale that fit a template. The tool combines large language model reasoning with guided decision making we encoded to mimic human behavior. The video demonstration is available at https://youtu.be/_vVd0LRvVHI.

</details>


### [11] [Unpacking Security Scanners for GitHub Actions Workflows](https://arxiv.org/abs/2601.14455)
*Madjda Fares,Yogya Gamage,Benoit Baudry*

Main category: cs.SE

TL;DR: 本文首次系统比较了9款GitHub Actions工作流安全扫描器，建立了10类安全弱点的分类法，并在596个工作流上测试了这些工具的范围、检测能力和可用性。


<details>
  <summary>Details</summary>
Motivation: GitHub Actions作为广泛使用的自动化平台，已成为软件供应链攻击的主要目标。攻击者利用权限过高、版本模糊或缺乏完整性检查等弱点来破坏工作流。虽然出现了多种安全扫描器来帮助开发者加固工作流，但缺乏对这些工具的系统性比较。

Method: 1. 建立包含10类安全弱点的GitHub Actions工作流安全弱点分类法；2. 收集9款安全扫描器；3. 使用596个精选的工作流作为测试集；4. 从三个维度比较扫描器：范围（针对哪些安全弱点）、检测能力（检测多少弱点）、可用性（扫描时间）。

Result: 研究发现：1. GitHub Actions工作流安全扫描器生态多样，既有范围广泛的工具，也有非常专注的工具；2. 不同扫描器对安全弱点的解释存在显著差异，导致报告弱点的类型和数量有很大不同；3. 基于实证证据，为开发者提供了可操作的建议来加固他们的GitHub Actions工作流。

Conclusion: 这是对GitHub Actions工作流安全扫描器的首次系统性比较研究，揭示了当前工具生态的多样性和不一致性。研究结果为开发者选择和使用安全扫描器提供了重要参考，并提出了加固工作流的具体建议。

Abstract: GitHub Actions is a widely used platform that allows developers to automate the build and deployment of their projects through configurable workflows. As the platform's popularity continues to grow, it has become a target of choice for recent software supply chain attacks. These attacks exploit excessive permissions, ambiguous versions, or the absence of artifact integrity checks to compromise workflows. In response to these attacks, several security scanners have emerged to help developers harden their workflows.
  In this paper, we perform the first systematic comparison of 9 GitHub Actions workflow security scanners. We compare them in terms of scope (which security weaknesses they target), detection capabilities (how many weaknesses they detect), and usability (how long they take to scan a workflow). To compare scanners on a common ground, we first establish a taxonomy of 10 security weaknesses that can occur in GitHub Actions workflows. Then, we run the scanners against a curated set of 596 workflows.
  Our study reveals that the landscape of GitHub Actions workflow security scanners is diverse, with both broad-scope tools and very focused ones. More importantly, we show that scanners interpret security weaknesses differently, leading to significant differences in the type and number of reported weaknesses. Based on this empirical evidence, we make actionable recommendations for developers to harden their GitHub Actions workflows.

</details>


### [12] [Tokenomics: Quantifying Where Tokens Are Used in Agentic Software Engineering](https://arxiv.org/abs/2601.14470)
*Mohamad Salim,Jasmine Latendresse,SayedHassan Khatoonabadi,Emad Shihab*

Main category: cs.SE

TL;DR: 分析LLM多智能体系统在软件开发生命周期中的token消耗模式，发现代码审查阶段消耗最多token（平均59.4%），输入token占比最大（平均53.9%），表明自动化精炼和验证是主要成本来源。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统在软件工程任务中的应用日益广泛，但其运行效率和资源消耗情况尚不明确，这阻碍了实际应用，因为不可预测的成本和环境影响成为主要障碍。

Method: 使用ChatDev框架和GPT-5推理模型，分析30个软件开发任务的执行轨迹，将内部阶段映射到标准开发阶段（设计、编码、代码完成、代码审查、测试、文档），创建标准化评估框架，量化比较各阶段的token分布（输入、输出、推理）。

Result: 代码审查阶段消耗了大部分token（平均59.4%），输入token始终是最大的消耗部分（平均53.9%），表明智能体协作存在显著低效性，主要成本来自自动化精炼和验证而非初始代码生成。

Conclusion: 该研究为预测LLM多智能体系统成本和优化工作流程提供了方法论，指导未来研究开发更token高效的智能体协作协议，重点关注减少代码审查阶段的token消耗。

Abstract: LLM-based Multi-Agent (LLM-MA) systems are increasingly applied to automate complex software engineering tasks such as requirements engineering, code generation, and testing. However, their operational efficiency and resource consumption remain poorly understood, hindering practical adoption due to unpredictable costs and environmental impact. To address this, we conduct an analysis of token consumption patterns in an LLM-MA system within the Software Development Life Cycle (SDLC), aiming to understand where tokens are consumed across distinct software engineering activities. We analyze execution traces from 30 software development tasks performed by the ChatDev framework using a GPT-5 reasoning model, mapping its internal phases to distinct development stages (Design, Coding, Code Completion, Code Review, Testing, and Documentation) to create a standardized evaluation framework. We then quantify and compare token distribution (input, output, reasoning) across these stages.
  Our preliminary findings show that the iterative Code Review stage accounts for the majority of token consumption for an average of 59.4% of tokens. Furthermore, we observe that input tokens consistently constitute the largest share of consumption for an average of 53.9%, providing empirical evidence for potentially significant inefficiencies in agentic collaboration. Our results suggest that the primary cost of agentic software engineering lies not in initial code generation but in automated refinement and verification. Our novel methodology can help practitioners predict expenses and optimize workflows, and it directs future research toward developing more token-efficient agent collaboration protocols.

</details>


### [13] [AQUA: an Agile Process to Develop Quantum Annealing Applications](https://arxiv.org/abs/2601.14501)
*Lodovica Marchesi,Amal Nasharti,Michele Marchesi*

Main category: cs.SE

TL;DR: AQUA是一个为QUBO/量子退火开发定制的敏捷生命周期框架，通过行业-学术合作创建，采用设计科学研究方法，将工作分为四个阶段，并在真实信用评分案例中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: QUBO领域虽然因量子硬件的发展而受到关注，但实际应用受到数学复杂性、硬件限制以及缺乏健全的QUBO开发软件工程流程的阻碍。需要系统化的开发框架来促进实际应用。

Method: 采用设计科学研究方法，通过行业-学术合作，将Scrum定制为适合QUBO/量子退火开发的AQUA框架。工作分为四个阶段：初始评估与形式建模、原型驱动的算法选择、敏捷实现、部署与持续维护，每个阶段都有里程碑控制。

Result: 在真实的信用评分案例中验证了AQUA的可行性，提供了一个明确、系统的量子退火工程框架，展示了该框架在实际应用中的有效性。

Conclusion: AQUA为QUBO/量子退火开发提供了专门的软件流程，通过设计科学研究方法创建和设计，并在真实案例中进行了实证验证，有助于促进量子计算在实际问题中的应用。

Abstract: Quadratic unconstrained binary optimization (QUBO) is a field of operations research that is attracting growing interest due to the recent availability of quantum hardware targeted at solving QUBO problems. However, practical adoption is hindered by mathematical intricacy, hardware constraints, and a lack of sound software engineering processes for QUBO development. This work presents AQUA (Agile QUantum Annealing), an agile lifecycle for QUBO/QA development created through an industry-academia partnership between NetService S.p.A and the University of Cagliari. Using the Design Science Research (DSR) approach, AQUA customizes Scrum to the needs of QUBO/QA development, structuring work into four stages: initial assessment with formal modeling, prototype-driven algorithm selection, agile implementation, and deployment with ongoing maintenance, each gated by milestones. Validated on a real credit-scoring case, AQUA shows feasibility and offers an explicit, systematic QA engineering framework. Key contributions of our work are: a dedicated QUBO/QA software process, its creation and design using DSR approach, and its empirical validation on a simple yet real case study.

</details>


### [14] [HELIOS: Hierarchical Graph Abstraction for Structure-Aware LLM Decompilation](https://arxiv.org/abs/2601.14598)
*Yonatan Gizachew Achamyeleh,Harsh Thomare,Mohammad Abdullah Al Faruque*

Main category: cs.SE

TL;DR: HELIOS框架将LLM反编译重构为结构化推理任务，通过控制流层次化文本表示和编译器反馈，显著提升反编译代码的可编译性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM反编译方法将代码视为纯文本，忽略了程序控制流图，导致输出在语法上脆弱且逻辑不一致，特别是在优化二进制文件的情况下。

Method: HELIOS将二进制文件控制流和函数调用总结为层次化文本表示，包含基本块、后继块以及循环和条件等高级模式，将此表示与原始反编译器输出一起提供给通用LLM，并可选择结合编译器反馈循环。

Result: 在x86_64上，HELIOS将平均目标文件可编译性从45.0%提升至85.2%（Gemini 2.0）和从71.4%提升至89.6%（GPT-4.1 Mini）。使用编译器反馈后，可编译性超过94%，功能正确性比纯文本提示提升高达5.6个百分点。

Conclusion: HELIOS无需微调即可在不同硬件架构上保持高语法正确性并减少功能正确性差异，成为安全逆向工程工作流程中实用的构建模块，能够生成可重新编译且语义忠实的代码。

Abstract: Large language models (LLMs) have recently been applied to binary decompilation, yet they still treat code as plain text and ignore the graphs that govern program control flow. This limitation often yields syntactically fragile and logically inconsistent output, especially for optimized binaries. This paper presents \textsc{HELIOS}, a framework that reframes LLM-based decompilation as a structured reasoning task. \textsc{HELIOS} summarizes a binary's control flow and function calls into a hierarchical text representation that spells out basic blocks, their successors, and high-level patterns such as loops and conditionals. This representation is supplied to a general-purpose LLM, along with raw decompiler output, optionally combined with a compiler-in-the-loop that returns error messages when the generated code fails to build.
  On HumanEval-Decompile for \texttt{x86\_64}, \textsc{HELIOS} raises average object file compilability from 45.0\% to 85.2\% for Gemini~2.0 and from 71.4\% to 89.6\% for GPT-4.1~Mini. With compiler feedback, compilability exceeds 94\% and functional correctness improves by up to 5.6 percentage points over text-only prompting. Across six architectures drawn from x86, ARM, and MIPS, \textsc{HELIOS} reduces the spread in functional correctness while keeping syntactic correctness consistently high, all without fine-tuning. These properties make \textsc{HELIOS} a practical building block for reverse engineering workflows in security settings where analysts need recompilable, semantically faithful code across diverse hardware targets.

</details>


### [15] [ARFT-Transformer: Modeling Metric Dependencies for Cross-Project Aging-Related Bug Prediction](https://arxiv.org/abs/2601.14731)
*Shuning Ge,Fangyun Qin,Xiaohui Wan,Yang Liu,Qian Dai,Zheng Zheng*

Main category: cs.SE

TL;DR: 提出ARFT-Transformer框架，使用Transformer的注意力机制捕捉度量指标间的依赖关系，并结合Focal Loss处理类别不平衡，显著提升跨项目ARB预测性能。


<details>
  <summary>Details</summary>
Motivation: 软件老化相关缺陷(ARB)预测面临数据稀缺问题，跨项目预测存在两大挑战：1)源项目与目标项目间的分布差异导致的领域适应问题；2)ARB易发样本与无ARB样本间的严重类别不平衡。现有方法将输入度量指标独立处理，忽略了丰富的指标间依赖关系，且使用交叉熵损失无法区分样本分类难度。

Method: 提出ARFT-Transformer框架，基于Transformer架构引入度量级多头注意力机制来捕捉度量指标间的交互关系，并采用Focal Loss函数有效处理类别不平衡问题。

Result: 在三个大型开源项目上的实验表明，ARFT-Transformer在单源和多源情况下均优于现有最先进的跨项目ARB预测方法，Balance指标平均提升最高达29.54%和19.92%。

Conclusion: ARFT-Transformer通过捕捉度量指标间依赖关系和有效处理类别不平衡，显著提升了跨项目ARB预测性能，为解决软件老化相关缺陷预测中的数据稀缺问题提供了有效方案。

Abstract: Software systems that run for long periods often suffer from software aging, which is typically caused by Aging-Related Bugs (ARBs). To mitigate the risk of ARBs early in the development phase, ARB prediction has been introduced into software aging research. However, due to the difficulty of collecting ARBs, within-project ARB prediction faces the challenge of data scarcity, leading to the proposal of cross-project ARB prediction. This task faces two major challenges: 1) domain adaptation issue caused by distribution difference between source and target projects; and 2) severe class imbalance between ARB-prone and ARB-free samples. Although various methods have been proposed for cross-project ARB prediction, existing approaches treat the input metrics independently and often neglect the rich inter-metric dependencies, which can lead to overlapping information and misjudgment of metric importance, potentially affecting the model's performance. Moreover, they typically use cross-entropy as the loss function during training, which cannot distinguish the difficulty of sample classification. To overcome these limitations, we propose ARFT-Transformer, a transformer-based cross-project ARB prediction framework that introduces a metric-level multi-head attention mechanism to capture metric interactions and incorporates Focal Loss function to effectively handle class imbalance. Experiments conducted on three large-scale open-source projects demonstrate that ARFT-Transformer on average outperforms state-of-the-art cross-project ARB prediction methods in both single-source and multi-source cases, achieving up to a 29.54% and 19.92% improvement in Balance metric.

</details>


### [16] [ARISE - Adaptive Refinement and Iterative Scenario Engineering](https://arxiv.org/abs/2601.14743)
*Konstantin Poddubnyy,Igor Vozniak,Nils Lipp,Ivan Burmistrov,Davit Hovhannisyan,Christian Mueller,Philipp Slusallek*

Main category: cs.SE

TL;DR: ARISE是一个多阶段工具，通过迭代式LLM引导的优化，将自然语言提示转换为可执行的Scenic脚本，显著减少了手动干预需求，在生成语义准确且可执行的交通场景方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前碰撞自由轨迹规划器的有效性依赖于训练数据的质量和多样性，特别是对于罕见场景。虽然生成合成交通场景是常用方法，但手动脚本编写或单次生成需要高精度，而现有的文本到模拟管道存在静态片段检索、有限语法、单次解码或缺乏鲁棒可执行性检查等限制。

Method: ARISE采用自适应优化和迭代场景工程的多阶段方法，通过迭代式LLM引导的优化将自然语言提示转换为可执行的Scenic脚本。每次生成后，ARISE在模拟软件中测试脚本可执行性，并将结构化诊断反馈给LLM，直到满足语法和功能要求。

Result: 通过广泛评估，ARISE在生成语义准确且可执行的交通场景方面优于基线方法，具有更高的可靠性和鲁棒性。

Conclusion: ARISE通过迭代式LLM引导的优化和可执行性检查，显著减少了生成合成交通场景时的手动干预需求，为轨迹规划提供了更高质量和多样性的训练数据。

Abstract: The effectiveness of collision-free trajectory planners depends on the quality and diversity of training data, especially for rare scenarios. A widely used approach to improve dataset diversity involves generating realistic synthetic traffic scenarios. However, producing such scenarios remains difficult due to the precision required when scripting them manually or generating them in a single pass. Natural language offers a flexible way to describe scenarios, but existing text-to-simulation pipelines often rely on static snippet retrieval, limited grammar, single-pass decoding, or lack robust executability checks. Moreover, they depend heavily on constrained LLM prompting with minimal post-processing. To address these limitations, we introduce ARISE - Adaptive Refinement and Iterative Scenario Engineering, a multi-stage tool that converts natural language prompts into executable Scenic scripts through iterative LLM-guided refinement. After each generation, ARISE tests script executability in simulation software, feeding structured diagnostics back to the LLM until both syntactic and functional requirements are met. This process significantly reduces the need for manual intervention. Through extensive evaluation, ARISE outperforms the baseline in generating semantically accurate and executable traffic scenarios with greater reliability and robustness.

</details>


### [17] [FastFI: Enhancing API Call-Site Robustness in Microservice-Based Systems with Fault Injection](https://arxiv.org/abs/2601.14800)
*Yuzhen Tan,Jian Wang,Shuaiyu Xie,Bing Li,Yunqing Yong,Neng Zhang,Shaolin Tan*

Main category: cs.SE

TL;DR: FastFI是一个故障注入引导的框架，用于增强微服务系统中API调用站点的鲁棒性，通过动态故障注入和DFS求解器高效发现组合故障，并提供可操作的API加固指导。


<details>
  <summary>Details</summary>
Motivation: 微服务架构的复杂性导致故障注入空间呈指数增长，传统随机注入效率低下。现有基于谱系驱动的方法存在两个主要限制：1) 组合故障发现受限于通用SAT求解器，无法利用CNF公式的单调低重叠结构；2) 现有技术提供的注入后指导有限。

Method: FastFI采用DFS-based求解器配合动态故障注入来发现所有有效的组合故障，并利用故障注入结果识别需要加固的关键API调用站点。

Result: 在四个代表性微服务基准测试中，FastFI相比最先进的基线方法平均减少了76.12%的端到端故障注入时间，同时保持可接受的资源开销。能够准确识别高影响API并提供可操作的调用站点加固指导。

Conclusion: FastFI通过高效的组合故障发现和实用的API加固指导，有效解决了微服务系统中故障注入的效率和实用性问题，显著提升了软件可靠性评估的效果。

Abstract: Fault injection is a key technique for assessing software reliability, enabling proactive detection of system defects before they manifest in production. However, the increasing complexity of microservice architectures leads to exponential growth in the fault-injection space, rendering traditional random injection inefficient. Recent lineage-driven approaches mitigate this problem through heuristic pruning, but they face two limitations. First, combinatorial-fault discovery remains bottlenecked by general-purpose SAT solvers, which fail to exploit the monotone and low-overlap structure of derived CNF formulas and typically rely on a static upper bound on fault size. Second, existing techniques provide limited post-injection guidance beyond reporting detected faults. To address these challenges, we propose FastFI, a fault-injection-guided framework to enhance the robustness of API call sites in microservice-based systems. FastFI features a DFS-based solver with dynamic fault injection to discover all valid combinatorial faults, and it leverages fault-injection results to identify critical APIs whose call sites should be hardened for robustness. Experiments on four representative microservice benchmarks show that FastFI reduces end-to-end fault-injection time by an average of 76.12\% compared to state-of-the-art baselines while maintaining acceptable resource overhead. Moreover, FastFI accurately identifies high-impact APIs and provides actionable guidance for call-site hardening.

</details>


### [18] [Reclaiming Software Engineering as the Enabling Technology for the Digital Age](https://arxiv.org/abs/2601.14861)
*Tanja E. J. Vos,Tijs van der Storm,Alexander Serebrenik,Lionel Briand,Roberto Di Cosmo,J. -M Bruel,Benoît Combemale*

Main category: cs.SE

TL;DR: 本文主张软件工程应被视为战略性学科而非辅助性技术，呼吁投资软件工程研究以确保数字技术的可持续性、可靠性和自主性。


<details>
  <summary>Details</summary>
Motivation: 软件工程在人工智能、量子计算、光子学和网络安全等数字技术突破中发挥着关键作用，但在政策框架中常被视为辅助性技术而非战略性学科。欧洲主要项目中将软件作为其他技术的组成部分，而软件工程作为科学学科的地位被忽视。

Method: 本文采用立场论文的形式，通过论证软件工程在数字基础设施中的核心地位，呼吁政策制定者和研究机构重新认识软件工程的战略价值。

Result: 提出软件工程应被视为战略性、赋能性学科，而非仅仅是支持性数字组件。强调数字技术的长期可持续性、可靠性和主权依赖于对软件工程研究的投资。

Conclusion: 软件工程需要重新确立其学科身份，成为政策框架中的战略性投资领域，以确保欧洲在数字技术领域的可持续发展和技术主权。

Abstract: Software engineering is the invisible infrastructure of the digital age. Every breakthrough in artificial intelligence, quantum computing, photonics, and cybersecurity relies on advances in software engineering, yet the field is too often treated as a supportive digital component rather than as a strategic, enabling discipline. In policy frameworks, including major European programmes, software appears primarily as a building block within other technologies, while the scientific discipline of software engineering remains largely absent. This position paper argues that the long-term sustainability, dependability, and sovereignty of digital technologies depend on investment in software engineering research. It is a call to reclaim the identity of software engineering.

</details>


### [19] [Understanding Usefulness in Developer Explanations on Stack Overflow](https://arxiv.org/abs/2601.14865)
*Martin Obaidi,Kushtrim Qengaj,Hannah Deters,Jakob Droste,Marc Herrmann,Kurt Schneider,Jil Klünder*

Main category: cs.SE

TL;DR: 研究通过分析Stack Overflow上的59,398个回答，发现解释长度、代码包含、时机和作者声誉对解释的有用性有正向影响，而情感极性影响甚微，表明技术交流中清晰度和实质内容比语气更重要。


<details>
  <summary>Details</summary>
Motivation: 在线问答论坛如Stack Overflow提供了大规模的解释产生和评估场景，但现有研究主要关注答案接受和投票行为，对于哪些具体特征使解释真正有用知之甚少。结构、上下文和语言因素（如内容丰富度、时机和情感）的相对影响尚不清楚。

Method: 分析了Stack Overflow上的3,323个问题和59,398个答案，结合文本分析和统计建模，研究解释属性如何与感知有用性（标准化点赞数）相关。

Result: 结构和上下文因素，特别是解释长度、代码包含、时机和作者声誉，显示出小到中度的正向影响。情感极性影响可忽略不计，表明在技术交流中清晰度和实质内容比语气更重要。

Conclusion: 研究提供了关于开发者解释中感知有用性驱动因素的实证分析，通过开放数据和复制材料提供方法透明度，并将观察到的沟通模式与需求沟通原则联系起来。研究结果为开发者和需求工程从业者如何构建更清晰有效的解释提供了基于证据的启示。

Abstract: Explanations are essential in software engineering (SE) and requirements communication, helping stakeholders clarify ambiguities, justify design choices, and build shared understanding. Online Q&A forums such as Stack Overflow provide large-scale settings where such explanations are produced and evaluated, offering valuable insights into what makes them effective. While prior work has explored answer acceptance and voting behavior, little is known about which specific features make explanations genuinely useful. The relative influence of structural, contextual, and linguistic factors, such as content richness, timing, and sentiment, remains unclear. We analyzed 3,323 questions and 59,398 answers from Stack Overflow, combining text analysis and statistical modeling to examine how explanation attributes relate to perceived usefulness (normalized upvotes). Structural and contextual factors, especially explanation length, code inclusion, timing, and author reputation, show small to moderate positive effects. Sentiment polarity has negligible influence, suggesting that clarity and substance outweigh tone in technical communication. This study provides an empirical account of what drives perceived usefulness in developer explanations. It contributes methodological transparency through open data and replication materials, and conceptual insight by relating observed communication patterns to principles of requirements communication. The findings offer evidence-based implications for how developers and RE practitioners can craft clearer and more effective explanations, potentially supporting fairer communication in both open and organizational contexts. From an RE perspective, these determinants can be interpreted as practical signals for ambiguity reduction and rationale articulation in day-to-day requirements communication.

</details>


### [20] [LLM-Based Repair of C++ Implicit Data Loss Compiler Warnings: An Industrial Case Study](https://arxiv.org/abs/2601.14936)
*Chansong You,Hyun Deok Choi,Jingun Hong*

Main category: cs.SE

TL;DR: 使用LLM自动修复C++项目中隐式数据丢失警告的方法，通过LSP收集上下文、Tree-sitter提取代码、LLM决策生成修复，在大型C++项目中达到92.73%的代码审查接受率。


<details>
  <summary>Details</summary>
Motivation: 减少手动修复编译器警告的工作量，同时保持代码质量和性能，特别是在大型C++项目中处理隐式数据丢失警告的挑战。

Method: 结合语言服务器协议(LSP)收集上下文信息，使用Tree-sitter提取相关代码，利用大型语言模型(LLM)进行决策并生成修复方案，评估范围检查的性能影响。

Result: 在大型C++项目中测试，修复方案在代码审查中获得92.73%的接受率；相比基准修复策略，LLM生成的修复减少了39.09%因范围检查和异常处理引入的额外指令；比人工开发者最优方案低13.56%。

Conclusion: LLM方法能有效减少修复编译器警告的手动工作量，同时保持代码质量和性能，有望集成到现有开发流程中，改善复杂C++软件项目的代码维护实践。

Abstract: This paper presents a method to automatically fix implicit data loss warnings in large C++ projects using Large Language Models (LLMs). Our approach uses the Language Server Protocol (LSP) to gather context, Tree-sitter to extract relevant code, and LLMs to make decisions and generate fixes. The method evaluates the necessity of range checks concerning performance implications and generates appropriate fixes. We tested this method in a large C++ project, resulting in a 92.73% acceptance rate of the fixes by human developers during the code review. Our LLM-generated fixes reduced the number of warning fix changes that introduced additional instructions due to range checks and exception handling by 39.09% compared to a baseline fix strategy. This result was 13.56% behind the optimal solutions created by human developers. These findings demonstrate that our LLM-based approach can reduce the manual effort to address compiler warnings while maintaining code quality and performance in a real-world scenario. Our automated approach shows promise for integration into existing development workflows, potentially improving code maintenance practices in complex C++ software projects.

</details>


### [21] [SmartOracle - An Agentic Approach to Mitigate Noise in Differential Oracles](https://arxiv.org/abs/2601.15074)
*Srinath Srinivasan,Tim Menzies,Marcelo D'Amorim*

Main category: cs.SE

TL;DR: SmartOracle使用LLM代理架构自动化JavaScript引擎差分模糊测试的验证过程，将手动工作流程分解为专门子代理，显著提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 差分模糊测试需要手动构建验证预言机，成本高、耗时长、易产生误报，且规范变化时需要重复工作。需要自动化解决方案来改进这一过程。

Method: 将手动分类工作流程分解为专门的LLM子代理，这些代理综合从终端运行和针对性规范查询中独立收集的证据，做出最终判断。

Result: 在历史基准测试中达到0.84召回率和18%误报率；相比Gemini 2.5 Pro基线，提高分类准确性，分析时间减少4倍，API成本降低10倍；在主动模糊测试活动中成功识别并报告了V8、JavaScriptCore和GraalJS等主要引擎中先前未知的规范级问题。

Conclusion: SmartOracle的代理架构在JavaScript差分测试中取得成功，表明该方法可能适用于其他软件系统，这是未来研究的方向。

Abstract: Differential fuzzers detect bugs by executing identical inputs across distinct implementations of the same specification, such as JavaScript interpreters. Validating the outputs requires an oracle and for differential testing of JavaScript, these are constructed manually, making them expensive, time-consuming, and prone to false positives. Worse, when the specification evolves, this manual effort must be repeated.
  Inspired by the success of agentic systems in other SE domains, this paper introduces SmartOracle. SmartOracle decomposes the manual triage workflow into specialized Large Language Model (LLM) sub-agents. These agents synthesize independently gathered evidence from terminal runs and targeted specification queries to reach a final verdict.
  For historical benchmarks, SmartOracle achieves 0.84 recall with an 18% false positive rate. Compared to a sequential Gemini 2.5 Pro baseline, it improves triage accuracy while reducing analysis time by 4$\times$ and API costs by 10$\times$. In active fuzzing campaigns, SmartOracle successfully identified and reported previously unknown specification-level issues across major engines, including bugs in V8, JavaScriptCore, and GraalJS.
  The success of SmartOracle's agentic architecture on Javascript suggests it might be useful other software systems- a research direction we will explore in future work.

</details>


### [22] [DeLog: An Efficient Log Compression Framework with Pattern Signature Synthesis](https://arxiv.org/abs/2601.15084)
*Siyu Yu,Yifan Wu,Junjielong Xu,Ying Fu,Ning Wang,Maoyin Liu,Pancheng Jiang,Xiang Zhang,Tong Jia,Pinjia He,Ying Li*

Main category: cs.SE

TL;DR: 论文研究发现日志解析精度与压缩率无必然正相关，提出基于模式签名合成的DeLog压缩器，在压缩率和速度上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 基于解析器的日志压缩方法在生产环境复杂日志上表现不佳，这与日志解析组件精度下降有关。作者研究一个基础但未验证的问题：更高的解析精度是否必然带来更好的压缩率？

Method: 首先进行实证研究量化解析精度与压缩率的关系，发现两者无必然联系。基于此设计DeLog压缩器，采用模式签名合成机制实现高效的模式分组。

Result: 在16个公开数据集和10个生产数据集上，DeLog在压缩率和速度方面都达到了最先进的性能。

Conclusion: 压缩率的关键在于实现有效的基于模式的分组和编码，而非单纯提高解析精度。DeLog通过模式签名合成机制实现了这一目标。

Abstract: Parser-based log compression, which separates static tem- plates from dynamic variables, is a promising approach to exploit the unique structure of log data. However, its perfor- mance on complex production logs is often unsatisfactory. This performance gap coincides with a known degradation in the accuracy of its core log parsing component on such data, motivating our investigation into a foundational yet unverified question: does higher parsing accuracy necessarily lead to better compression ratio?
  To answer this, we conduct the first empirical study quanti- fying this relationship and find that a higher parsing accuracy does not guarantee a better compression ratio. Instead, our findings reveal that compression ratio is dictated by achiev- ing effective pattern-based grouping and encoding, i.e., the partitioning of tokens into low entropy, highly compressible groups.
  Guided by this insight, we design DeLog, a novel log com- pressor that implements a Pattern Signature Synthesis mecha- nism to achieve efficient pattern-based grouping. On 16 public and 10 production datasets, DeLog achieves state-of-the-art compression ratio and speed.

</details>


### [23] [Parameter-Efficient Multi-Task Fine-Tuning in Code-Related Tasks](https://arxiv.org/abs/2601.15094)
*Md Zahidul Haque,Saima Afrin,Antonio Mastropaolo*

Main category: cs.SE

TL;DR: 该论文研究了在多任务场景下使用QLoRA对大型代码模型进行参数高效微调的效果，发现多任务QLoRA能够有效利用迁移学习，在代码生成、翻译和总结三个任务上取得与单任务QLoRA和多任务全微调相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在软件工程任务中表现出色，但全微调计算成本高昂。虽然QLoRA等参数高效微调方法能在单任务上取得良好效果，但多任务QLoRA微调的效果以及迁移学习对生成代码正确性和质量的影响尚未得到充分研究。

Method: 采用多任务QLoRA微调方法，在代码生成、代码翻译和代码总结三个代表性任务上进行实验。通过执行正确性和相似性指标评估功能正确性，并进行全面的代码质量分析。

Result: 多任务QLoRA能够有效利用迁移学习，在正确性和质量方面达到或超过单任务QLoRA和多任务全微调的性能。较大模型在正确性和质量之间保持更一致的平衡，而较小模型虽然能保持功能正确性，但出现质量相关问题的频率更高。

Conclusion: 多任务QLoRA微调是一种有效的参数高效微调策略，能够在多个代码相关任务上实现竞争性性能，同时大幅降低计算成本。模型规模对正确性和质量之间的平衡有重要影响。

Abstract: Large Language Models (LLMs) have proven highly effective in automating software engineering tasks, bridging natural language and code semantics to achieve notable results in code generation and summarization. However, their scale incurs substantial computational costs, making full fine-tuning impractical. Parameter-Efficient Fine-Tuning (PEFT) methods like QLoRA enable efficient specialization with lower resource demands. Recent studies show QLoRA-optimized Large Code Models (LCMs) perform strongly across diverse tasks, yet it remains unclear whether this effectiveness persists when a single model is QLoRA fine-tuned for multiple code-related tasks. The interaction between Multi-task fine-tuning and QLoRA optimization, and how transfer learning affects correctness and quality of generated artifacts, remains largely unexplored. We investigate Multi-task QLoRA fine-tuning across three representative tasks: code generation, translation, and summarization. We evaluate functional correctness through execution-based and similarity-based metrics, complemented by comprehensive code quality analysis--an aspect largely overlooked in prior work. Our findings show that Multi-task QLoRA effectively leverages transfer learning, achieving competitive or superior performance relative to both Single-task QLoRA and Multi-task full fine-tuning. Larger models demonstrate more consistent balance between correctness and quality, whereas smaller models preserve functionality but exhibit a higher incidence of quality-related issues.

</details>


### [24] [Why Authors and Maintainers Link (or Don't Link) Their PyPI Libraries to Code Repositories and Donation Platforms](https://arxiv.org/abs/2601.15139)
*Alexandros Tsakpinis,Nicolas Raube,Alexander Pretschner*

Main category: cs.SE

TL;DR: 对PyPI库元数据（源码仓库链接和捐赠平台链接）缺失原因的大规模实证研究，通过5万开发者调查和LLM主题建模分析，揭示了链接动机、障碍及改进建议。


<details>
  <summary>Details</summary>
Motivation: PyPI库的元数据（源码仓库链接和捐赠平台链接）对开源库的透明度、信任和可持续性至关重要，但许多包缺乏此类元数据，且缺乏对其背后原因的了解。

Method: 大规模实证研究，向5万名PyPI作者和维护者发送针对性调查，收集1400多份回复，使用基于大语言模型的主题建模分析关键动机和障碍。

Result: 源码仓库链接主要用于促进协作、增加透明度和问题追踪，缺失原因包括疏忽、懒惰或认为与项目无关；捐赠链接用于支持开源工作或获得财务贡献，但受到怀疑、技术摩擦和组织约束的阻碍；共同挑战包括链接过时、缺乏意识和指导不清晰。主题建模管道在30次运行中表现出84%词汇和89%语义相似度的稳健性。

Conclusion: 研究提供了PyPI元数据实践的实证见解，提出了改进建议，同时证明了基于LLM的主题建模方法在分析短文本调查回复中的有效性。

Abstract: Metadata of libraries on the Python Package Index (PyPI)-including links to source code repositories and donation platforms-plays a critical role in supporting the transparency, trust, and sustainability of open-source libraries. Yet, many packages lack such metadata, and little is known about the underlying reasons. This paper presents a large-scale empirical study combining two targeted surveys sent to 50,000 PyPI authors and maintainers. We analyze more than 1,400 responses using large language model (LLM)-based topic modeling to uncover key motivations and barriers related to linking repositories and donation platforms. While repository URLs are often linked to foster collaboration, increase transparency, and enable issue tracking, some maintainers omit them due to oversight, laziness, or the perceived irrelevance to their project. Donation platform links are reported to support open source work or receive financial contributions, but are hindered by skepticism, technical friction, and organizational constraints. Cross-cutting challenges-such as outdated links, lack of awareness, and unclear guidance-affect both types of metadata. We further assess the robustness of our topic modeling pipeline across 30 runs (84% lexical and 89% semantic similarity) and validate topic quality with 23 expert raters (Randolph's kappa = 0.55). The study contributes empirical insights into PyPI's metadata practices and provides recommendations for improving them, while also demonstrating the effectiveness of our topic modeling approach for analyzing short-text survey responses.

</details>


### [25] [SAGA: Detecting Security Vulnerabilities Using Static Aspect Analysis](https://arxiv.org/abs/2601.15154)
*Yoann Marquer,Domenico Bianculli,Lionel C. Briand*

Main category: cs.SE

TL;DR: SAGA是一种用于检测Python源代码漏洞的静态分析方法，通过符号控制流图和领域特定语言实现高精度检测，在108个漏洞数据集上达到100%敏感度和99.15%特异度。


<details>
  <summary>Details</summary>
Motivation: Python作为流行编程语言，其项目包含越来越多的安全漏洞，但现有分析工具仅支持少数漏洞类型，需要能够检测多种Python漏洞的方法。

Method: 提出SAGA方法，包括：1）源代码解析器提取控制流和数据流信息，表示为符号控制流图；2）领域特定语言定义静态属性及其在图遍历中的演化；3）基于该语言构建完整性、机密性等安全属性的静态属性库。

Result: 在108个漏洞数据集上评估，获得100%敏感度和99.15%特异度，仅产生1个误报，优于4个常用安全分析工具。分析时间少于31秒，比基线工具快2.5-512.1倍。

Conclusion: SAGA能够以高精度和高效的方式检测Python源代码中的多种安全漏洞，填补了现有工具仅支持有限漏洞类型的空白。

Abstract: Python is one of the most popular programming languages; as such, projects written in Python involve an increasing number of diverse security vulnerabilities. However, existing state-of-the-art analysis tools for Python only support a few vulnerability types. Hence, there is a need to detect a large variety of vulnerabilities in Python projects.
  In this paper, we propose the SAGA approach to detect and locate vulnerabilities in Python source code in a versatile way. SAGA includes a source code parser able to extract control- and data-flow information and to represent it as a symbolic control-flow graph, as well as a domain-specific language defining static aspects of the source code and their evolution during graph traversals. We have leveraged this language to define a library of static aspects for integrity, confidentiality, and other security-related properties.
  We have evaluated SAGA on a dataset of 108 vulnerabilities, obtaining 100% sensitivity and 99.15% specificity, with only one false positive, while outperforming four common security analysis tools. This analysis was performed in less than 31 seconds, i.e., between 2.5 and 512.1 times faster than the baseline tools.

</details>


### [26] [Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback](https://arxiv.org/abs/2601.15188)
*Stephan Wallraven,Tim Köhne,Hartmut Westenberger,Andreas Moser*

Main category: cs.SE

TL;DR: LLMs在生成ABAP代码方面表现出显著性能差异，强大模型通过迭代编译反馈可达75%成功率，而小模型表现较差


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在许多编程语言中应用成功，但ABAP代码生成的系统性分析几乎空白，需要实证研究LLM生成ABAP代码的能力

Method: 使用包含180个任务的基准测试，包括改编的HumanEval任务和实际SAP场景，评估LLM生成语法正确和功能性ABAP代码的能力，并测试它们利用编译器反馈进行迭代改进的效果

Result: 模型间存在显著性能差异：强大LLM经过多次迭代后成功率约75%，并能从编译器反馈中大幅受益；而较小模型表现明显较弱

Conclusion: 研究表明强大LLM在ABAP开发流程中具有高潜力，特别是在迭代错误修正方面，为ABAP代码生成提供了实证基础

Abstract: This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.

</details>


### [27] [Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub](https://arxiv.org/abs/2601.15195)
*Ramtin Ehsani,Sakshi Pathak,Shriya Rawal,Abdullah Al Mujahid,Mia Mohammad Imran,Preetha Chatterjee*

Main category: cs.SE

TL;DR: 对GitHub上33k个AI编码代理提交的PR进行大规模研究，分析合并与未合并PR的特征，并构建拒绝模式的分类体系。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理在真实软件项目中作为自主贡献者提交PR的数量快速增长，但对其实际行为以及为何许多PR未能合并的原因了解甚少。

Method: 1) 对33k个由五个编码代理提交的PR进行定量分析，从四个维度比较合并与未合并PR：任务类型、代码变更、CI构建结果、评审动态；2) 对600个PR进行定性分析，构建拒绝模式的层次分类体系。

Result: 文档、CI和构建更新相关任务合并成功率最高，性能和bug修复任务表现最差；未合并PR通常涉及更大的代码变更、修改更多文件，且经常无法通过CI/CD流水线验证；定性分析揭示了缺乏有意义的评审参与、重复PR、不需要的功能实现和代理错位等拒绝原因。

Conclusion: 研究强调了改进未来代理工作流成功的关键社会技术和人机协作因素，这些因素对于提高AI编码代理的贡献质量至关重要。

Abstract: AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project's CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.

</details>


### [28] [When Agents Fail: A Comprehensive Study of Bugs in LLM Agents with Automated Labeling](https://arxiv.org/abs/2601.15232)
*Niful Islam,Ragib Shahriar Ayon,Deepak George Thomas,Shibbir Ahmed,Mohammad Wardat*

Main category: cs.SE

TL;DR: 本文首次全面研究LLM智能体开发中的bug类型、根本原因和影响，分析了1187个bug相关帖子，并构建BugReAct智能体实现自动化bug识别。


<details>
  <summary>Details</summary>
Motivation: LLM智能体通过集成工具扩展了LLM能力，但调试困难且成本高，该领域仍处于早期阶段，社区发展不成熟，需要系统研究开发中遇到的bug问题。

Method: 从Stack Overflow、GitHub和Hugging Face论坛收集1187个bug相关帖子和代码片段，分析7个主流LLM框架及自定义实现的bug；构建BugReAct智能体（基于ReAct架构，配备外部工具）测试自动化bug识别的可行性。

Result: 研究发现BugReAct智能体（使用Gemini 2.5 Flash）在标注bug特征方面表现优异，平均每个帖子/代码片段的成本仅为0.01美元。

Conclusion: 这是首个全面研究LLM智能体bug的工作，为开发者提供了bug分类和根本原因的深入理解，并展示了自动化bug识别的可行性，有助于降低LLM智能体开发调试成本。

Abstract: Large Language Models (LLMs) have revolutionized intelligent application development. While standalone LLMs cannot perform any actions, LLM agents address the limitation by integrating tools. However, debugging LLM agents is difficult and costly as the field is still in it's early stage and the community is underdeveloped. To understand the bugs encountered during agent development, we present the first comprehensive study of bug types, root causes, and effects in LLM agent-based software. We collected and analyzed 1,187 bug-related posts and code snippets from Stack Overflow, GitHub, and Hugging Face forums, focused on LLM agents built with seven widely used LLM frameworks as well as custom implementations. For a deeper analysis, we have also studied the component where the bug occurred, along with the programming language and framework. This study also investigates the feasibility of automating bug identification. For that, we have built a ReAct agent named BugReAct, equipped with adequate external tools to determine whether it can detect and annotate the bugs in our dataset. According to our study, we found that BugReAct equipped with Gemini 2.5 Flash achieved a remarkable performance in annotating bug characteristics with an average cost of 0.01 USD per post/code snippet.

</details>
