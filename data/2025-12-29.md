<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 17]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey](https://arxiv.org/abs/2512.21347)
*Vítor Mateus de Brito,Kleinner Farias*

Main category: cs.SE

TL;DR: 该研究通过调查46名行业专业人士，实证分析了LLM在软件工程中的采用情况，发现开发者对LLM持积极态度，但同时也担忧认知依赖、安全风险和技术自主性侵蚀等问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在软件工程中的快速发展和深度融入开发者日常工作流程，理解其实际使用情况变得至关重要。研究旨在填补学术讨论与真实软件开发实践之间的鸿沟。

Method: 采用实证研究方法，对46名具有不同教育背景和经验水平的行业专业人士进行问卷调查，收集关于LLM在软件工程中采用情况的数据。

Result: 调查结果显示：1）开发者对LLM持积极态度，特别是在快速解决技术问题、改进文档支持和增强源代码标准化方面；2）同时存在对认知依赖、安全风险和技术自主性侵蚀的担忧；3）需要批判性和监督性地使用LLM工具。

Conclusion: 研究强调需要以批判和监督的方式使用LLM工具，为开发者和研究者提供了实用见解，同时推动了关于LLM认知、伦理和组织影响的未来研究。

Abstract: The rapid advancement of Large Language Models (LLMs) is reshaping software engineering by profoundly influencing coding, documentation, and system maintenance practices. As these tools become deeply embedded in developers' daily workflows, understanding how they are used has become essential. This paper reports an empirical study of LLM adoption in software engineering, based on a survey of 46 industry professionals with diverse educational backgrounds and levels of experience. The results reveal positive perceptions of LLMs, particularly regarding faster resolution of technical questions, improved documentation support, and enhanced source code standardization. However, respondents also expressed concerns about cognitive dependence, security risks, and the potential erosion of technical autonomy. These findings underscore the need for critical and supervised use of LLM-based tools. By grounding the discussion in empirical evidence from industry practice, this study bridges the gap between academic discourse and real-world software development. The results provide actionable insights for developers and researchers seeking to adopt and evolve LLM-based technologies in a more effective, responsible, and secure manner, while also motivating future research on their cognitive, ethical, and organizational implications.

</details>


### [2] [Fairness Is Not Just Ethical: Performance Trade-Off via Data Correlation Tuning to Mitigate Bias in ML Software](https://arxiv.org/abs/2512.21348)
*Ying Xiao,Shangwen Wang,Sicen Liu,Dingyuan Xue,Xian Zhan,Yepang Liu,Jie M. Zhang*

Main category: cs.SE

TL;DR: CoT是一种新颖的预处理方法，通过调整数据相关性来缓解偏见，在单属性和多属性场景下均优于现有方法，显著提高了弱势群体的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统软件公平性研究主要关注伦理和社会责任，忽视了公平性本质上是一个软件质量问题，源于不同敏感用户群体间的性能差异。将公平性明确作为软件质量维度，除了伦理考量外，还能带来实际效益，如提高弱势群体的预测性能、增强分布外泛化能力和地理可迁移性。现有偏见缓解方法面临困境：预处理方法适用性广但效果不如后处理方法。

Method: 提出Correlation Tuning (CoT)方法，这是一种新颖的预处理方法，通过调整数据相关性来缓解偏见。具体包括：1) 引入Phi系数作为直观的相关性度量，系统量化敏感属性与标签之间的相关性；2) 使用多目标优化来解决代理偏见问题。

Result: CoT将弱势群体的真阳性率平均提高17.5%，并将三个关键偏见指标（统计奇偶差异SPD、平均几率差异AOD、平等机会差异EOD）平均降低50%以上。在单属性和多属性场景下，CoT分别比最先进方法高出3个和10个百分点。

Conclusion: CoT作为一种有效的预处理方法，成功解决了现有偏见缓解方法的困境，在保持广泛适用性的同时显著提高了公平性效果。该方法将公平性作为软件质量维度来处理，为实际部署中的公平性改进提供了实用解决方案。

Abstract: Traditional software fairness research typically emphasizes ethical and social imperatives, neglecting that fairness fundamentally represents a core software quality issue arising directly from performance disparities across sensitive user groups. Recognizing fairness explicitly as a software quality dimension yields practical benefits beyond ethical considerations, notably improved predictive performance for unprivileged groups, enhanced out-of-distribution generalization, and increased geographic transferability in real-world deployments. Nevertheless, existing bias mitigation methods face a critical dilemma: while pre-processing methods offer broad applicability across model types, they generally fall short in effectiveness compared to post-processing techniques. To overcome this challenge, we propose Correlation Tuning (CoT), a novel pre-processing approach designed to mitigate bias by adjusting data correlations. Specifically, CoT introduces the Phi-coefficient, an intuitive correlation measure, to systematically quantify correlation between sensitive attributes and labels, and employs multi-objective optimization to address the proxy biases. Extensive evaluations demonstrate that CoT increases the true positive rate of unprivileged groups by an average of 17.5% and reduces three key bias metrics, including statistical parity difference (SPD), average odds difference (AOD), and equal opportunity difference (EOD), by more than 50% on average. CoT outperforms state-of-the-art methods by three and ten percentage points in single attribute and multiple attributes scenarios, respectively. We will publicly release our experimental results and source code to facilitate future research.

</details>


### [3] [CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation](https://arxiv.org/abs/2512.21351)
*Santhosh Kumar Ravindran*

Main category: cs.SE

TL;DR: CosmoCore-Evo 扩展了 CosmoCore 框架，通过引入进化算法增强代码生成任务的适应性和新颖性，将RL轨迹视为"基因组"进行突变和选择，在分布偏移环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架在代码生成任务中容易陷入训练模式，缺乏对分布偏移环境（如API变更、新库引入）的适应能力，需要增强新颖性和适应性。

Method: 基于CosmoCore的affective dream-replay框架，引入进化算法：将RL轨迹视为"基因组"，在夜间回放阶段进行突变和选择；增强Dream Queue，包含高适应度轨迹的突变和企业调优的适应度函数（效率、合规性、可扩展性）。

Result: 在HumanEval变体、BigCodeBench和自定义PySpark管道模拟等扩展基准测试中，CosmoCore-Evo相比原始CosmoCore和PPO、REAMER等基线，解决方案新颖性提高35%，适应速度加快25%。

Conclusion: 进化组件在弥合LLM代理的感知差距方面发挥关键作用，CosmoCore-Evo通过进化算法有效增强了代码生成任务的适应性和新颖性，特别是在分布偏移环境中。

Abstract: Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.

</details>


### [4] [Multi-Agent LLM Committees for Autonomous Software Beta Testing](https://arxiv.org/abs/2512.21352)
*Sumanth Bharadwaj Hachalli Karanam,Dhiwahar Adhithya Kennady*

Main category: cs.SE

TL;DR: 提出多智能体委员会框架，通过视觉增强的LLM协作进行软件测试，相比单智能体基线显著提升任务成功率


<details>
  <summary>Details</summary>
Motivation: 手动软件测试成本高、耗时长，单智能体LLM方法存在幻觉和不一致行为问题，需要更可靠的自动化测试方案

Method: 采用多智能体委员会框架，包含多样化视觉增强LLM，通过三轮投票协议达成共识，结合模型多样性、角色驱动行为变化和视觉界面理解

Result: 多智能体委员会达到89.5%总体任务成功率，2-4智能体配置达到91.7-100%成功率，相比单智能体基线提升13.7-22.0个百分点；在WebShop上达到74.7%成功率，OWASP安全测试达到82.0%成功率，覆盖8/10 OWASP Top 10漏洞类别

Conclusion: 多智能体委员会框架显著提升软件测试效果，支持实时持续集成测试，开源实现促进可重复研究和实际部署

Abstract: Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.

</details>


### [5] [AInsteinBench: Benchmarking Coding Agents on Scientific Repositories](https://arxiv.org/abs/2512.21373)
*Titouan Duston,Shuo Xin,Yang Sun,Daoguang Zan,Aoyan Li,Shulin Xin,Kai Shen,Yixiao Chen,Qiming Sun,Ge Zhang,Jiashuo Liu,Huan Zhou,Jingkai Liu,Zhichen Pu,Yuanheng Wang,Bo-Xuan Ge,Xin Tong,Fei Ye,Zhi-Chao Zhao,Wen-Biao Han,Zhoujian Cao,Yueran Zhao,Weiluo Ren,Qingshen Long,Yuxiao Liu,Anni Huang,Yidi Du,Yuanyuan Rong,Jiahao Peng*

Main category: cs.SE

TL;DR: AInsteinBench是一个用于评估LLM代理在真实科研软件生态系统中作为科学计算开发代理能力的大规模基准测试，基于生产级科学代码库的实际维护者PR任务


<details>
  <summary>Details</summary>
Motivation: 现有科学推理基准侧重于概念知识，软件工程基准强调通用功能实现和问题解决，缺乏评估LLM在真实科研开发环境中端到端能力的基准

Method: 从六个广泛使用的科学代码库（量子化学、量子计算、分子动力学、数值相对论、流体动力学、化学信息学）中提取维护者编写的PR任务，通过多阶段筛选和专家评审确保科学挑战性、充分测试覆盖和校准难度

Result: 通过可执行环境评估、科学有意义的失败模式和测试驱动验证，基准能够测量模型超越表面代码生成、掌握计算科学研究核心能力的能力

Conclusion: AInsteinBench填补了现有基准的空白，为评估LLM在真实科研软件生态系统中的科学计算开发能力提供了重要工具

Abstract: We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.

</details>


### [6] [What Makes a GitHub Issue Ready for Copilot?](https://arxiv.org/abs/2512.21426)
*Mohammed Sayagh*

Main category: cs.SE

TL;DR: 论文分析了GitHub issue质量对AI代理（如Copilot）实现成功率的影响，提出了32条详细标准评估issue质量，并构建了可解释的机器学习模型预测issue能否产生被合并的PR。


<details>
  <summary>Details</summary>
Motivation: AI代理（如Copilot）在编码任务中表现依赖于输入质量，但现有GitHub Copilot的最佳实践建议有限且抽象。不清楚或范围不明确的issue可能导致实现失败，需要更系统的方法评估issue质量以提高AI代理成功率。

Method: 1. 构建32条详细标准评估GitHub issue质量；2. 比较导致合并PR与关闭PR的issue差异；3. 建立可解释的机器学习模型预测issue产生合并PR的可能性。

Result: 成功合并的PR通常来自更简短、范围明确、提供清晰指导和实现提示的issue。包含外部引用（配置、环境设置、依赖或外部API）的issue合并率较低。构建的模型AUC中位数为72%，能帮助用户改进issue质量。

Conclusion: 在AI协作时代，编写高质量的GitHub issue应成为软件工程的一等公民活动。研究揭示了issue质量的关键指标，为未来进一步研究提供了基础，并提供了实用工具帮助开发者优化issue以提高AI代理成功率。

Abstract: AI-agents help developers in different coding tasks, such as developing new features, fixing bugs, and reviewing code. Developers can write a Github issue and assign it to an AI-agent like Copilot for implementation. Based on the issue and its related discussion, the AI-agent performs a plan for the implementation, and executes it. However, the performance of AI-agents and LLMs heavily depends on the input they receive. For instance, a GitHub issue that is unclear or not well scoped might not lead to a successful implementation that will eventually be merged. GitHub Copilot provides a set of best practice recommendations that are limited and high-level. In this paper, we build a set of 32 detailed criteria that we leverage to measure the quality of GitHub issues to make them suitable for AI-agents. We compare the GitHub issues that lead to a merged pull request versus closed pull request. Then, we build an interpretable machine learning model to predict the likelihood of a GitHub issue resulting in a merged pull request. We observe that pull requests that end up being merged are those originating from issues that are shorter, well scoped, with clear guidance and hints about the relevant artifacts for an issue, and with guidance on how to perform the implementation. Issues with external references including configuration, context setup, dependencies or external APIs are associated with lower merge rates. We built an interpretable machine learning model to help users identify how to improve a GitHub issue to increase the chances of the issue resulting in a merged pull request by Copilot. Our model has a median AUC of 72\%. Our results shed light on quality metrics relevant for writing GitHub issues and motivate future studies further investigate the writing of GitHub issues as a first-class software engineering activity in the era of AI-teammates.

</details>


### [7] [Cerberus: Multi-Agent Reasoning and Coverage-Guided Exploration for Static Detection of Runtime Errors](https://arxiv.org/abs/2512.21431)
*Hridya Dhulipala,Xiaokai Rong,Tien N. Nguyen*

Main category: cs.SE

TL;DR: Cerberus是一个无需执行的预测性覆盖引导测试框架，使用LLM生成触发运行时错误的输入，并通过两阶段反馈循环提高代码覆盖率和错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 在软件开发中，需要在无需实际执行的情况下检测代码片段中的运行时错误和异常，特别是在将在线代码片段集成到代码库之前进行检测。

Method: Cerberus使用LLM生成触发运行时错误的输入，并进行代码覆盖预测和错误检测，无需代码执行。采用两阶段反馈循环：第一阶段同时提高代码覆盖率和检测运行时错误；第二阶段当覆盖率达到100%或最大值时，专注于检测运行时错误。

Result: 经验评估表明，Cerberus在（不）完整代码片段上比传统和基于学习的测试框架表现更好，能更高效地生成高覆盖率测试用例，从而发现更多运行时错误。

Conclusion: Cerberus通过LLM驱动的无执行测试框架，有效解决了无需实际执行即可检测运行时错误的问题，在代码覆盖率和错误检测方面优于现有方法。

Abstract: In several software development scenarios, it is desirable to detect runtime errors and exceptions in code snippets without actual execution. A typical example is to detect runtime exceptions in online code snippets before integrating them into a codebase. In this paper, we propose Cerberus, a novel predictive, execution-free coverage-guided testing framework. Cerberus uses LLMs to generate the inputs that trigger runtime errors and to perform code coverage prediction and error detection without code execution. With a two-phase feedback loop, Cerberus first aims to both increasing code coverage and detecting runtime errors, then shifts to focus only detecting runtime errors when the coverage reaches 100% or its maximum, enabling it to perform better than prompting the LLMs for both purposes. Our empirical evaluation demonstrates that Cerberus performs better than conventional and learning-based testing frameworks for (in)complete code snippets by generating high-coverage test cases more efficiently, leading to the discovery of more runtime errors.

</details>


### [8] [Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing](https://arxiv.org/abs/2512.21440)
*Hridya Dhulipala,Xiaokai Rong,Aashish Yadavally,Tien N. Nguyen*

Main category: cs.SE

TL;DR: FuzzWise：基于LLM多智能体框架的灰盒模糊测试初始种子生成方法，通过预测性代码覆盖评估，在单流程中生成高质量初始种子集


<details>
  <summary>Details</summary>
Motivation: 传统模糊测试中，生成大规模初始种子集然后进行最小化的两阶段方法效率低下。需要一种能够直接生成高质量初始种子集的方法，节省计算资源和时间，特别是在程序执行不可行或不希望执行的情况下。

Method: FuzzWise采用基于大语言模型的多智能体框架：第一个LLM智能体为目标程序生成测试用例；第二个LLM智能体作为预测性代码覆盖模块，评估每个生成的测试用例是否能提升当前种子集的整体覆盖率。这种流线化流程允许每个新生成的测试种子立即被评估其对整体覆盖率的贡献。

Result: 实验评估表明，FuzzWise生成的测试用例数量显著少于基线方法。尽管测试用例数量较少，但FuzzWise实现了更高的代码覆盖率，并触发了更多的运行时错误。此外，在生成初始种子集方面，FuzzWise更加时间高效和覆盖高效，能够捕获更多错误。

Conclusion: FuzzWise通过整合种子生成和最小化过程，利用LLM的预测能力，能够在不需要实际程序执行的情况下，高效生成高质量的初始种子集，为模糊测试提供了更有效的初始种子生成方法。

Abstract: In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.

</details>


### [9] [Code Clone Refactoring in C# with Lambda Expressions](https://arxiv.org/abs/2512.21511)
*Takuto Kawamoto,Yoshiki Higo*

Main category: cs.SE

TL;DR: 提出一种针对C#语言的代码克隆重构方法，使用lambda表达式参数化行为差异，评估显示35%的克隆对适合重构，其中28.9%成功重构


<details>
  <summary>Details</summary>
Motivation: 现有"提取方法"重构技术主要针对Java程序，使用参数化方法处理代码克隆中的差异，但不同编程语言的特性（特别是lambda表达式规范）会影响该技术的适用性，需要针对特定语言（如C#）开发优化的重构方法

Method: 提出C#特定的重构技术，使用lambda表达式分析和合并代码克隆。首先使用NiCad克隆检测器检测代码克隆，然后应用提出的方法分析克隆对，判断是否适合重构，并对可重构的克隆对实际执行重构操作

Result: 评估包含22个项目的2,217个克隆对：35.0%的克隆对被判定适合重构，其中28.9%成功完成重构。表明提出的C#特定方法能有效处理代码克隆重构问题

Conclusion: 不同编程语言需要针对性的重构方法，提出的C#特定技术能有效利用lambda表达式参数化行为差异，成功重构相当比例的代码克隆，验证了语言特性对重构技术适用性的重要影响

Abstract: "Extract Method" refactoring is a technique for consolidating code clones. Parameterization approaches are used to extract a single method from multiple code clones that contain differences. This approach parameterizes expressions and behaviors within a method. In particular, behavior parameterization has been extensively studied in Java programs, but little research has been conducted on other programming languages.
  Lambda expressions can be used to parameterize behaviors, but the specifications of each programming language significantly affect the applicability of this technique. Therefore, the optimal "Extract Method" approach may vary depending on the programming language.
  In this study, we propose a C#-specific technique that uses lambda expressions to analyze and consolidate code clones. We evaluated our proposed method by applying it to code clones detected by the NiCad clone detector and measuring how many of them could be successfully consolidated.
  In total, 2,217 clone pairs from 22 projects were included in our evaluation. For the clone pairs determined to be refactorable, we also attempted refactoring actually. The proposed approach determined that 35.0% of all clone pairs were suitable for refactoring. Among these, 28.9% were successfully refactored.

</details>


### [10] [XTrace: A Non-Invasive Dynamic Tracing Framework for Android Applications in Production](https://arxiv.org/abs/2512.21555)
*Qi Hu,Jiangchao Liu,Xin Yu,Lin Zhang,Edward Jiang*

Main category: cs.SE

TL;DR: XTrace是一个Android动态追踪框架，通过非侵入式代理和优化ART虚拟机的内置插桩机制，实现生产环境中任意方法的高性能拦截和追踪，解决了传统方法无法捕获"幽灵bug"的问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动应用复杂性指数级增长和设备环境碎片化加剧，确保在线应用稳定性面临前所未有的挑战。传统静态日志记录和崩溃后分析方法缺乏实时上下文信息，无法有效应对只在特定场景出现的"幽灵bug"，迫切需要动态运行时可观测性解决方案。

Method: XTrace提出了一种新颖的非侵入式代理范式，避免直接修改虚拟机底层数据结构。它通过利用和优化Android ART虚拟机内置的高度稳定的插桩机制，实现高性能方法拦截。该框架支持在生产环境中拦截和追踪任意方法，无需发布新版本应用。

Result: 在拥有数亿日活用户的字节跳动应用中评估显示，XTrace具备生产级稳定性和性能。大规模在线A/B实验证实其稳定性，对崩溃用户率和ANR率无显著影响(p>0.05)，同时保持极低开销(<7ms启动延迟，<0.01ms每次方法调用)和广泛兼容性(Android 5.0-15+)。成功诊断了11个以上严重在线崩溃和多个性能瓶颈，将根因定位效率提升超过90%。

Conclusion: XTrace提供了一个生产级解决方案，成功调和了Android动态追踪中长期存在的稳定性与全面覆盖之间的冲突，为移动应用稳定性保障提供了有效的动态运行时可观测性工具。

Abstract: As the complexity of mobile applications grows exponentially and the fragmentation of user device environments intensifies, ensuring online application stability faces unprecedented challenges. Traditional methods, such as static logging and post-crash analysis, lack real-time contextual information, rendering them ineffective against "ghost bugs" that only manifest in specific scenarios. This highlights an urgent need for dynamic runtime observability: intercepting and tracing arbitrary methods in production without requiring an app release. We propose XTrace, a novel dynamic tracing framework. XTrace introduces a new paradigm of non-invasive proxying, which avoids direct modification of the virtual machine's underlying data structures. It achieves high-performance method interception by leveraging and optimizing the highly stable, built-in instrumentation mechanism of the Android ART virtual machine. Evaluated in a ByteDance application with hundreds of millions of daily active users, XTrace demonstrated production-grade stability and performance. Large-scale online A/B experiments confirmed its stability, showing no statistically significant impact (p > 0.05) on Crash User Rate or ANR rate, while maintaining minimal overhead (<7 ms startup latency, <0.01 ms per-method call) and broad compatibility (Android 5.0-15+). Critically, XTrace diagnosed over 11 severe online crashes and multiple performance bottlenecks, improving root-cause localization efficiency by over 90%. This confirms XTrace provides a production-grade solution that reconciles the long-standing conflict between stability and comprehensive coverage in Android dynamic tracing.

</details>


### [11] [Co-Evolution of Types and Dependencies: Towards Repository-Level Type Inference for Python Code](https://arxiv.org/abs/2512.21591)
*Shuo Sun,Shixin Zhang,Jiwei Yan,Jun Yan,Jian Zhang*

Main category: cs.SE

TL;DR: 提出基于LLM的仓库级Python类型推断方法，通过类型与依赖的协同演化实现高精度推断


<details>
  <summary>Details</summary>
Motivation: Python动态类型机制导致运行时类型错误，现有工具难以处理仓库级别的复杂跨过程依赖关系

Method: 构建实体依赖图(EDG)建模仓库级类型依赖，采用迭代式类型推断，类型与依赖协同演化，结合类型检查器实时验证

Result: 在12个复杂Python仓库上，TypeSim得分0.89，TypeExact得分0.84，相比最强基线分别提升27%和40%，减少92.7%的工具引入新类型错误

Conclusion: 该方法显著推进了真实世界Python开发的自动化、可靠类型标注

Abstract: Python's dynamic typing mechanism, while promoting flexibility, is a significant source of runtime type errors that plague large-scale software, which inspires the automatic type inference techniques. Existing type inference tools have achieved advances in type inference within isolated code snippets. However, repository-level type inference remains a significant challenge, primarily due to the complex inter-procedural dependencies that are difficult to model and resolve. To fill this gap, we present \methodName, a novel approach based on LLMs that achieves repository-level type inference through the co-evolution of types and dependencies. \methodName~constructs an Entity Dependency Graph (EDG) to model the objects and type dependencies across the repository. During the inference process, it iteratively refines types and dependencies in EDG for accurate type inference. Our key innovations are: (1) an EDG model designed to capture repository-level type dependencies; (2) an iterative type inference approach where types and dependencies co-evolve in each iteration; and (3) a type-checker-in-the-loop strategy that validates and corrects inferences on-the-fly, thereby reducing error propagation. When evaluated on 12 complex Python repositories, \methodName~significantly outperformed prior works, achieving a \textit{TypeSim} score of 0.89 and a \textit{TypeExact} score of 0.84, representing a 27\% and 40\% relative improvement over the strongest baseline. More importantly, \methodName~removed new type errors introduced by the tool by 92.7\%. This demonstrates a significant leap towards automated, reliable type annotation for real-world Python development.

</details>


### [12] [How Do Agents Perform Code Optimization? An Empirical Study](https://arxiv.org/abs/2512.21757)
*Huiyun Peng,Antonio Zhong,Ricardo Andrés Calvo Méndez,Kelechi G. Kalu,James C. Davis*

Main category: cs.SE

TL;DR: 首个比较AI代理与人类在性能优化提交方面的实证研究，发现AI生成的PR在性能验证方面不如人类充分，但优化模式相似。


<details>
  <summary>Details</summary>
Motivation: 尽管AI编码代理在代码生成和错误修复方面取得了进展，但它们在真实世界性能优化任务上的表现尚不清楚，需要实证研究来评估AI代理在性能优化方面的能力。

Method: 使用AIDev数据集，分析了324个AI生成的PR和83个人类编写的PR，从采纳率、可维护性、优化模式和验证实践四个维度进行比较研究。

Result: AI生成的性能优化PR包含明确性能验证的比例显著低于人类（45.7% vs. 63.6%，p=0.007），但两者使用的优化模式基本相同。

Conclusion: AI代理在性能优化方面与人类有相似的优化模式，但在性能验证方面存在不足，这为改进AI代码优化代理提供了方向和机会。

Abstract: Performance optimization is a critical yet challenging aspect of software development, often requiring a deep understanding of system behavior, algorithmic tradeoffs, and careful code modifications. Although recent advances in AI coding agents have accelerated code generation and bug fixing, little is known about how these agents perform on real-world performance optimization tasks. We present the first empirical study comparing agent- and human-authored performance optimization commits, analyzing 324 agent-generated and 83 human-authored PRs from the AIDev dataset across adoption, maintainability, optimization patterns, and validation practices. We find that AI-authored performance PRs are less likely to include explicit performance validation than human-authored PRs (45.7\% vs. 63.6\%, $p=0.007$). In addition, AI-authored PRs largely use the same optimization patterns as humans. We further discuss limitations and opportunities for advancing agentic code optimization.

</details>


### [13] [The State of the SBOM Tool Ecosystems: A Comparative Analysis of SPDX and CycloneDX](https://arxiv.org/abs/2512.21781)
*Abdul Ali Bangash,Tongxu Ge,Zhimin Zhao,Arshdeep Singh,Zitao Wang,Bram Adams*

Main category: cs.SE

TL;DR: 该研究对SBOM（软件物料清单）的两种主流格式SPDX和CycloneDX的工具生态系统进行了量化比较，分析了170个公开工具、36,990个问题报告以及顶级开源项目，揭示了两个生态系统的不同特点和互补优势。


<details>
  <summary>Details</summary>
Motivation: SBOM的采用依赖于工具生态系统，但目前SPDX和CycloneDX两种主流格式的生态系统在成熟度、工具支持和社区参与方面存在显著差异，需要系统性的比较研究来指导开发者和实践者。

Method: 1. 对170个公开宣传的SBOM工具进行定量比较；2. 分析171个CycloneDX工具和470个SPDX工具的健康指标；3. 比较36,990个开源工具的问题报告；4. 调查每个工具生态系统中的前250个开源项目并比较其健康指标。

Result: CycloneDX工具项目显示出更高的开发者参与度和某些健康指标，而SPDX工具则受益于更成熟的生态系统、更广泛的工具可用性和已建立的行业采用。两个生态系统各有优势，存在互补性。

Conclusion: 研究为开发者、贡献者和实践者提供了关于这两个生态系统互补优势的见解，并指出了相互增强的机会。CycloneDX在开发者参与方面表现更好，而SPDX在生态系统成熟度和行业采用方面更有优势。

Abstract: A Software Bill of Materials (SBOM) provides transparency by documenting software component metadata and dependencies. However, SBOM adoption depends on tool ecosystems. With two dominant formats: SPDX and CycloneDX - the ecosystems vary significantly in maturity, tool support, and community engagement. We conduct a quantitative comparison of use cases for 170 publicly advertised SBOM tools, identifying enhancement areas for each format. We compare health metrics of both ecosystems (171 CycloneDX versus 470 SPDX tools) to evaluate robustness and maturity. We quantitatively compare 36,990 issue reports from open-source tools to identify challenges and development opportunities. Finally, we investigate the top 250 open-source projects using each tool ecosystem and compare their health metrics. Our findings reveal distinct characteristics: projects using CycloneDX tools demonstrate higher developer engagement and certain health indicators, while SPDX tools benefit from a more mature ecosystem with broader tool availability and established industry adoption. This research provides insights for developers, contributors, and practitioners regarding complementary strengths of these ecosystems and identifies opportunities for mutual enhancement.

</details>


### [14] [A Story About Cohesion and Separation: Label-Free Metric for Log Parser Evaluation](https://arxiv.org/abs/2512.21811)
*Qiaolin Qin,Jianchen Zhao,Heng Li,Weiyi Shang,Ettore Merlo*

Main category: cs.SE

TL;DR: 提出PMSS，一种无需标签的日志解析器评估指标，通过聚类质量分析替代传统依赖标注数据的方法


<details>
  <summary>Details</summary>
Motivation: 现有日志解析器评估指标严重依赖标注数据，限制了评估范围，且不同版本的真实标签会导致不一致的评估结论

Method: 提出PMSS指标，使用中心点轮廓分析和Levenshtein距离评估解析器的分组质量和模板质量，具有近线性时间复杂度

Result: PMSS与标签指标FGA/FTA显著正相关，PMSS最佳解析器与FGA最佳解析器性能差异仅2.1%，提供了无标签评估的有效替代方案

Conclusion: PMSS为日志解析器评估提供了无需标注数据的有效指标，特别适用于真实标签不一致或不可用的情况

Abstract: Log parsing converts log messages into structured event templates, allowing for automated log analysis and reducing manual inspection effort. To select the most compatible parser for a specific system, multiple evaluation metrics are commonly used for performance comparisons. However, existing evaluation metrics heavily rely on labeled log data, which limits prior studies to a fixed set of datasets and hinders parser evaluations and selections in the industry. Further, we discovered that different versions of ground-truth used in existing studies can lead to inconsistent performance conclusions. Motivated by these challenges, we propose a novel label-free template-level metric, PMSS (parser medoid silhouette score), to evaluate log parser performance. PMSS evaluates both parser grouping and template quality with medoid silhouette analysis and Levenshtein distance within a near-linear time complexity in general. To understand its relationship with label-based template-level metrics, FGA and FTA, we compared their evaluation outcomes for six log parsers on the standard corrected Loghub 2.0 dataset. Our results indicate that log parsers achieving the highest PMSS or FGA exhibit comparable performance, differing by only 2.1% on average in terms of the FGA score; the difference is 9.8% for FTA. PMSS is also significantly (p<1e-8) and positively correlated to both FGA and FTA: the Spearman's rho correlation coefficient of PMSS-FGA and PMSS-FTA are respectively 0.648 and 0.587, close to the coefficient between FGA and FTA (0.670). We further extended our discussion on how to interpret the conclusions from different metrics, identifying challenges in using PMSS, and provided guidelines on conducting parser selections with our metric. PMSS provides a valuable evaluation alternative when ground-truths are inconsistent or labels are unavailable.

</details>


### [15] [Analyzing Code Injection Attacks on LLM-based Multi-Agent Systems in Software Development](https://arxiv.org/abs/2512.21818)
*Brian Bowers,Smita Khapre,Jugal Kalita*

Main category: cs.SE

TL;DR: 该论文分析了多智能体系统在软件工程中的安全漏洞，发现coder-reviewer-tester架构比coder和coder-tester架构更安全但效率较低，通过添加安全分析智能体可提高效率同时增强安全性，但安全分析智能体本身仍易受高级代码注入攻击。


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI和多智能体系统即将主导产业和社会，这些由目标驱动自主性的系统代表了生成式AI的强大形式，从被动内容生成转向主动多任务能力。然而，由于其自主设计和缺乏人工监督，这些系统无法自行识别和应对攻击，因此需要分析其安全漏洞。

Method: 提出一个用于软件工程实现阶段的多智能体系统架构，并建立全面的威胁模型。分析不同架构（coder、coder-tester、coder-reviewer-tester）的安全性和效率，通过添加安全分析智能体来改进系统。演示安全分析智能体对高级代码注入攻击的脆弱性。

Result: coder-reviewer-tester架构比coder和coder-tester架构更具弹性但编码效率较低。添加安全分析智能体可缓解效率损失同时实现更好的弹性。然而，安全分析智能体本身易受高级代码注入攻击，在注入代码中嵌入有毒few-shot示例可将攻击成功率从0%提高到71.95%。

Conclusion: 多智能体系统虽然能准确生成代码，但存在安全漏洞，特别是代码注入攻击。coder-reviewer-tester架构更安全但效率较低，安全分析智能体能改善这一平衡但仍存在漏洞。需要进一步研究保护多智能体系统免受高级攻击的方法。

Abstract: Agentic AI and Multi-Agent Systems are poised to dominate industry and society imminently. Powered by goal-driven autonomy, they represent a powerful form of generative AI, marking a transition from reactive content generation into proactive multitasking capabilities. As an exemplar, we propose an architecture of a multi-agent system for the implementation phase of the software engineering process. We also present a comprehensive threat model for the proposed system. We demonstrate that while such systems can generate code quite accurately, they are vulnerable to attacks, including code injection. Due to their autonomous design and lack of humans in the loop, these systems cannot identify and respond to attacks by themselves. This paper analyzes the vulnerability of multi-agent systems and concludes that the coder-reviewer-tester architecture is more resilient than both the coder and coder-tester architectures, but is less efficient at writing code. We find that by adding a security analysis agent, we mitigate the loss in efficiency while achieving even better resiliency. We conclude by demonstrating that the security analysis agent is vulnerable to advanced code injection attacks, showing that embedding poisonous few-shot examples in the injected code can increase the attack success rate from 0% to 71.95%.

</details>


### [16] [HALF: Process Hollowing Analysis Framework for Binary Programs with the Assistance of Kernel Modules](https://arxiv.org/abs/2512.22043)
*Zhangbo Long,Letian Sha,Jiaye Pan,Dongpeng Xu,Yifei Huang,Fu Xiao*

Main category: cs.SE

TL;DR: 提出一个新的二进制程序分析框架，通过内核模块扩展传统动态二进制插桩能力，采用进程空洞技术和容器环境实现解耦分析，提高细粒度分析的可用性和性能。


<details>
  <summary>Details</summary>
Motivation: 二进制程序分析在系统安全中仍然非常重要，但细粒度分析（如动态污点分析）存在部署困难、内存使用高、性能开销大等问题，难以适应新的分析场景（如内存破坏利用和沙箱逃逸恶意软件）。

Method: 1. 使用内核模块扩展传统动态二进制插桩的分析能力；2. 基于解耦分析思想，通过进程空洞技术在容器进程中构建分析环境；3. 在Windows平台上实现原型系统。

Result: 通过大量基准测试和实际程序的实验验证了框架的有效性和性能，并通过分析实际利用程序和恶意代码验证了框架的实用性价值。

Conclusion: 该框架能够复用现有动态二进制插桩平台的功能，同时减少对目标程序执行的影响，提高了细粒度二进制程序分析的可用性和性能。

Abstract: Binary program analysis is still very important in system security. There are many practical achievements in binary code analysis, but fine-grained analysis such as dynamic taint analysis, is constantly studied due to the problem of deployability, high memory usage, and performance overhead, so as to better adapt to the new analysis scenario, such as memory corruption exploits and sandbox evasion malware. This paper presents a new binary program analysis framework, in order to improve the usability and performance of fine-grained analysis. The framework mainly uses the kernel module to further expand the analysis capability of the traditional dynamic binary instrumentation. Then, based on the idea of decoupling analysis, the analysis environment is constructed in the container process through process hollowing techniques in a new way. It can reuse the functions of the existing dynamic binary instrumentation platforms and also reduce the impact on the execution of the target program. The prototype is implemented on the Windows platform. The validity and performance of the framework are verified by a large number of experiments with benchmark and actual programs. The effectiveness of the framework is also verified by the analysis of actual exploit programs and malicious code, demonstrating the value of the practical application.

</details>


### [17] [Proceedings First Workshop on Adaptable Cloud Architectures](https://arxiv.org/abs/2512.22054)
*Giuseppe De Palma,Saverio Giallorenzo*

Main category: cs.SE

TL;DR: WACA 2025研讨会论文集，聚焦可适应云架构，与DisCoTec 2025联合举办


<details>
  <summary>Details</summary>
Motivation: 随着云计算的快速发展，需要探索能够适应动态变化需求、提高资源利用效率和系统可靠性的云架构设计方法

Method: 通过研讨会形式汇集学术界和工业界专家，分享最新研究成果和实践经验，包括论文征集、同行评审、会议报告和论文集出版

Result: 成功举办了WACA 2025研讨会，收集并出版了关于可适应云架构的最新研究成果，促进了该领域的学术交流

Conclusion: WACA 2025为可适应云架构研究提供了重要交流平台，推动了该领域的发展，论文集记录了当前研究进展和未来方向

Abstract: This volume contains the post-proceedings of the Workshop on Adaptable Cloud Architectures (WACA 2025), held on June 20, 2025, in Lille, France, co-located with DisCoTec 2025 - 20th International Federated Conference on Distributed Computing Techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum](https://arxiv.org/abs/2512.21340)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Nikolaos Tsironis,Souvik Sengupta,Kostas Ramantas,Jhofre Ojeda*

Main category: cs.DC

TL;DR: 智慧城市采用数据驱动架构提升城市服务效率、可持续性和韧性


<details>
  <summary>Details</summary>
Motivation: 智慧城市需要更高效、可持续和韧性的城市服务，传统方法难以满足现代城市需求

Method: 采用数据中心的架构设计，整合城市各系统数据

Result: 城市服务效率、可持续性和韧性得到提升

Conclusion: 数据驱动架构是智慧城市发展的关键方向

Abstract: Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.

</details>


### [19] [Demystifying ARM SME to Optimize General Matrix Multiplications](https://arxiv.org/abs/2512.21473)
*Chencheng Deng,Weiling Yang,Jianbin Fang,Dezun Dong*

Main category: cs.DC

TL;DR: MpGEMM是一个针对ARM SME架构优化的开源GEMM库，通过系统化架构特性分析，采用缓存感知分区、高效数据打包和专用微内核等技术，在Apple M4 Pro上相比Apple Accelerate库平均加速1.23倍。


<details>
  <summary>Details</summary>
Motivation: 现代架构如ARM SME引入了专门的矩阵运算硬件，但现有线性代数库未能充分利用其潜力，特别是在处理大矩阵时。需要开发能充分利用SME架构特性的GEMM库来提升性能。

Method: 1. 对SME架构进行系统化特性分析，推导优化指导原则；2. 采用缓存感知的分区策略；3. 实现高效数据打包和实时转置；4. 开发专用微内核，利用多向量加载和所有可用瓦片寄存器。

Result: 在Apple M4 Pro上使用DeepSeek和LLaMA的真实工作负载进行评估，MpGEMM相比厂商优化的Apple Accelerate库平均加速1.23倍，显著优于其他开源替代方案。

Conclusion: MpGEMM通过充分利用ARM SME架构的关键特性，成功优化了GEMM性能，为高性能计算和深度学习中的矩阵乘法提供了高效的解决方案。

Abstract: General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.

</details>


### [20] [Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism](https://arxiv.org/abs/2512.21487)
*Xinglin Pan,Shaohuai Shi,Wenxiang Lin,Yuxin Wang,Zhenheng Tang,Wei Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: FinDEP：针对MoE模型推理的细粒度任务调度算法，通过优化计算/通信任务划分和调度，在DEP架构上提升推理吞吐量


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能以亚线性计算增长扩展模型规模，但推理时由于KV缓存和稀疏专家激活导致内存密集。现有的DEP方法虽然将注意力机制和专家分配到专用GPU组，但缺乏对共享专家的支持且任务调度效率低下，限制了性能提升。

Method: 1) 将计算和通信划分为更小的任务以实现细粒度流水线；2) 制定支持可变粒度和顺序的调度优化问题；3) 开发针对这个大型搜索空间的高效求解器

Result: 在四个GPU系统上使用DeepSeek-V2和Qwen3-MoE进行实验，FinDEP相比现有方法将吞吐量提升最高达1.61倍，在32-GPU系统上实现最高1.24倍的加速

Conclusion: FinDEP通过细粒度任务调度有效解决了MoE推理中的内存瓶颈和调度效率问题，显著提升了DEP架构上的推理性能

Abstract: The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.
  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.
  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.

</details>


### [21] [nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures](https://arxiv.org/abs/2512.21571)
*Hui Guo,Qihang Zheng,Chenghai Huo,Dongliang Guo,Haoqi Yang,Yang Zhang*

Main category: cs.DC

TL;DR: nncase是一个开源端到端编译框架，通过基于e-graph的项重写引擎解决内存架构异构性问题，统一优化跨不同目标平台，在Qwen3系列模型上超越主流框架，CPU性能接近手工优化的llama.cpp。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的高效部署受到内存架构异构性的阻碍，传统编译器存在工作流碎片化和高适配成本的问题。

Method: 提出nncase编译框架，核心是基于e-graph的项重写引擎解决阶段排序问题，包含三个关键模块：Auto Vectorize适配异构计算单元，Auto Distribution搜索并行策略并优化通信成本，Auto Schedule最大化片上缓存局部性，以及缓冲区感知的代码生成阶段。

Result: 在Qwen3系列模型上超越MLC LLM和Intel IPEX等主流框架，在CPU上达到与手工优化的llama.cpp相当的性能。

Conclusion: nncase证明了自动化编译在高性能LLM部署中的可行性，为异构内存架构下的LLM部署提供了有效的解决方案。

Abstract: The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.

</details>


### [22] [Embedding Samples Dispatching for Recommendation Model Training in Edge Environments](https://arxiv.org/abs/2512.21615)
*Guopeng Li,Haisheng Tan,Chi Zhang,Hongqiu Ni,Zilong Wang,Xinyue Zhang,Yang Xu,Han Tian*

Main category: cs.DC

TL;DR: ESD机制通过优化边缘工作者之间的嵌入样本分发，减少嵌入传输成本，加速DLRM训练


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上训练深度推荐模型（DLRM）能保护数据隐私、降低延迟并实现个性化，但巨大的嵌入表需要参数服务器维护，导致边缘工作者与服务器间嵌入传输成本高昂，成为训练瓶颈

Method: 提出ESD机制，通过HybridDis分发决策方法优化输入嵌入样本到边缘工作者的分发，HybridDis结合资源密集型最优算法和启发式算法，平衡决策质量与资源消耗

Result: 实验表明ESD减少嵌入传输成本高达36.76%，端到端DLRM训练加速达1.74倍

Conclusion: ESD机制有效解决了边缘DLRM训练中的嵌入传输瓶颈问题，通过智能样本分发显著提升了训练效率

Abstract: Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.

</details>


### [23] [Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference](https://arxiv.org/abs/2512.21730)
*Linyi Jiang,Yifei Zhu,Hao Yin,Bo Li*

Main category: cs.DC

TL;DR: Hyperion是一个云-端协同框架，通过识别关键区域、动态调整传输质量和加权融合结果，实现了在动态网络条件下对超高清视频的低延迟Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 阵列相机视频技术能够实时捕捉超高清视频，但使用Transformer视觉基础模型处理这些数据面临计算或传输开销大的挑战。在设备端计算有计算瓶颈，在云端计算有传输瓶颈。

Method: Hyperion框架包含三个核心组件：1) 协作感知重要性评分器，在补丁级别识别关键区域；2) 动态调度器，根据网络条件自适应调整补丁传输质量以平衡延迟和精度；3) 加权集成器，融合边缘和云端结果以提高精度。

Result: 实验结果表明，Hyperion在各种网络环境下，与最先进的基线方法相比，帧处理率最高提升1.61倍，精度最高提升20.2%。

Conclusion: Hyperion通过利用视觉Transformer模型的固有特性，有效解决了超高清视觉Transformer的计算和传输瓶颈，实现了云-端协同的低延迟推理框架。

Abstract: Recent advancements in array-camera videography enable real-time capturing of ultra-high-definition (Ultra-HD) videos, providing rich visual information in a large field of view. However, promptly processing such data using state-of-the-art transformer-based vision foundation models faces significant computational overhead in on-device computing or transmission overhead in cloud computing. In this paper, we present Hyperion, the first cloud-device collaborative framework that enables low-latency inference on Ultra-HD vision data using off-the-shelf vision transformers over dynamic networks. Hyperion addresses the computational and transmission bottleneck of Ultra-HD vision transformers by exploiting the intrinsic property in vision Transformer models. Specifically, Hyperion integrates a collaboration-aware importance scorer that identifies critical regions at the patch level, a dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under dynamic network conditions, and a weighted ensembler that fuses edge and cloud results to improve accuracy. Experimental results demonstrate that Hyperion enhances frame processing rate by up to 1.61 times and improves the accuracy by up to 20.2% when compared with state-of-the-art baselines under various network environments.

</details>


### [24] [LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices](https://arxiv.org/abs/2512.21835)
*Mingyu Sun,Xiao Zhang,Shen Qu,Yan Li,Mengbai Xiao,Yuan Yuan,Dongxiao Yu*

Main category: cs.DC

TL;DR: LIME是一个支持多内存受限边缘设备上无损大模型推理的协作系统，通过交错流水线并行和模型卸载技术，在有限网络带宽下实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在边缘设备上部署面临巨大挑战：参数规模大、资源需求高，而边缘设备计算能力和内存有限，网络带宽瓶颈限制了分布式部署和实时响应。现有轻量化优化技术往往导致模型精度显著下降。

Method: 提出LIME系统，采用交错流水线并行结合模型卸载技术，动态平衡计算和通信。引入细粒度离线分配调度器和在线内存适应策略，优化设备计算和存储资源，最小化推理延迟。

Result: 在四个异构Nvidia Jetson边缘设备上部署LLaMA3.3-70B-Instruct模型，相比最先进基线，在零星和突发请求模式下分别实现1.7倍和3.7倍的加速，且不损失模型精度。

Conclusion: LIME系统成功解决了边缘设备上大模型推理的资源约束问题，通过协作式架构和优化策略，在保持模型精度的同时显著提升推理效率，为边缘智能提供了可行的解决方案。

Abstract: Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\times$ and 3.7$\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.

</details>


### [25] [Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models](https://arxiv.org/abs/2512.21884)
*Tingyang Sun,Ting He,Bo Ji,Parimal Parag*

Main category: cs.DC

TL;DR: 本文系统研究了分布式LLM推理中的资源分配问题，提出了性能预测模型、优化算法和在线适应方案，显著降低了推理时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理需要高端GPU，成本高昂。PETALS等分布式系统通过跨多个低端GPU服务器分割模型块来降低部署门槛，但其性能严重依赖资源分配，而如何最优分配仍未知。

Method: 1) 实验验证的性能模型预测给定块放置和请求路由决策下的推理性能；2) 将块放置和请求路由离线优化建模为混合整数线性规划问题，证明其NP-hard并设计多项式复杂度算法；3) 将离线算法适应在线设置，在负载有界时保持相同性能保证。

Result: 通过实验和实验验证的仿真表明，所提解决方案在多种地理分布式服务器设置下，相比最先进方案能显著减少推理时间。还开发了轻量级CPU-only模拟器，可在有限GPU访问下评估大规模部署。

Conclusion: 本文首次系统研究了分布式LLM推理的资源分配问题，提供了理论分析和实用算法，为高效分布式LLM部署提供了解决方案，并通过模拟器降低了未来研究的门槛。

Abstract: Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.

</details>


### [26] [BLEST: Blazingly Efficient BFS using Tensor Cores](https://arxiv.org/abs/2512.21967)
*Deniz Elbek,Kamer Kaya*

Main category: cs.DC

TL;DR: BLEST是一个利用GPU Tensor Core加速BFS的框架，通过位图结构、负载均衡、图重排序和批处理SpMSpV等技术，在多种真实图上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的Tensor Core虽然计算吞吐量极高，但专为密集矩阵运算设计，难以直接应用于BFS等不规则、非结构化的图计算。需要解决负载不均衡、冗余计算和同步开销等问题。

Method: 1. 基于位图的结构和执行布局重构pull-based BFS流水线；2. 引入二值化虚拟切片集(BVSS)实现warp级负载均衡；3. 两种图重排序策略：社交图用压缩导向排序，非社交图用带宽减少排序；4. 批处理SpMSpV乘法模式利用位运算TC瓦片；5. 内核融合和惰性顶点更新减少同步和原子开销。

Result: 在多种真实图上，BLEST平均比BerryBees快3.58倍，比Gunrock快4.64倍，比GSWITCH快4.9倍。

Conclusion: BLEST成功将GPU Tensor Core应用于BFS计算，通过创新的负载均衡、图重排序和计算模式设计，显著提升了不规则图计算的性能。

Abstract: Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\times$, $4.64\times$ and $4.9\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.

</details>


### [27] [Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View](https://arxiv.org/abs/2512.22035)
*Yanmeng Wang,Zhiwen Dai,Shuai Wang,Jian Zhou,Fu Xiao,Tony Q. S. Quek,Tsung-Hui Chang*

Main category: cs.DC

TL;DR: FedAuto：一种无需先验网络知识的自适应聚合联邦微调框架，有效应对连接失败和数据异构问题，具有严格的收敛保证


<details>
  <summary>Details</summary>
Motivation: 现有联邦微调方法通常假设同质网络条件或需要连接失败先验知识，但在现实网络中（有线、Wi-Fi、4G、5G混合）这些假设不切实际，导致性能下降

Method: 提出FedAuto框架，通过自适应聚合缓解连接失败和数据异构的联合影响，无需网络条件先验知识或基础设施修改，支持即插即用部署

Result: FedAuto在各种连接失败场景下始终优于最先进基线（包括全参数和部分参数微调如LoRA），甚至超越依赖复杂通信资源优化的策略

Conclusion: FedAuto为联邦微调提供了实用解决方案，通过自适应聚合和严格的每轮收敛保证，解决了现实网络中的连接失败和数据异构问题

Abstract: Federated Fine-Tuning (FFT) has attracted growing interest as it leverages both server- and client-side data to enhance global model generalization while preserving privacy, and significantly reduces the computational burden on edge devices by avoiding training from scratch. Despite these advantages, FFT performance is often degraded by unreliable server-client connections and heterogeneous client data distributions. Most existing methods assume homogeneous network conditions or require prior knowledge of connection failures. However, these assumptions are impractical in real-world networks characterized by diverse communication standards (e.g., wired, Wi-Fi, 4G, and 5G) and heterogeneous failure patterns. To address these limitations, we propose FedAuto, a novel FFT framework that mitigates the combined effects of connection failures and data heterogeneity via adaptive aggregation. FedAuto operates without prior knowledge of network conditions or modifications to existing infrastructure, enabling seamless plug-and-play deployment. Moreover, we establish a rigorous convergence guarantee for FedAuto. By adopting a novel per-round aggregation perspective, our analysis removes the need for assumptions on connection failures probabilities or client selection strategies commonly imposed in prior work, and guarantees convergence of FedAuto for each individual realization, providing a stronger theoretical assurance. Extensive experiments demonstrate that FedAuto consistently outperforms state-of-the-art baselines under diverse connection failure scenarios for both full-parameter and partial-parameter fine-tuning (e.g., LoRA), and even surpasses strategies that rely on complex communication resource optimization.

</details>


### [28] [FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion](https://arxiv.org/abs/2512.22036)
*Zhuoran Zhu,Chunyang Zhu,Hao Lin,Xu Fu,Yiming Zhou,Quanlu Zhang,Zhenhua Li,Feng Qian,Chao Yu,Boxun Li,Guohao Dai,Yu Wang*

Main category: cs.DC

TL;DR: FUSCO是一个专为MoE模型设计的通信库，通过融合数据转换和通信来解决专家并行中的数据传输瓶颈，相比现有方案显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型依赖专家并行进行高效训练和推理，但现有通信库处理数据混洗时效率低下，其开销可占端到端运行时间的一半以上。MoE的专家主数据布局与通信操作期望的设备主布局存在冲突，导致通信效率低下。

Method: FUSCO通过融合数据转换和通信实现高效轻量的数据混洗。它捕获细粒度数据布局，由流水线通信引擎在通信路径上高效执行混洗操作。轻量级规划和负载均衡机制消除冗余通信并分散流量。

Result: 在代表性基准测试中，FUSCO相比NCCL和DeepEP分别实现最高3.84倍和2.01倍的加速。在端到端MoE任务中，相比NCCL和DeepEP，FUSCO分别降低训练延迟1.17-1.39倍和1.10-1.19倍，降低推理中首令牌生成延迟1.09-1.25倍和1.06-1.16倍。

Conclusion: FUSCO通过专门针对MoE通信模式优化的设计，显著提升了大规模MoE模型的训练和推理效率，解决了现有通信库在处理专家并行数据混洗时的性能瓶颈。

Abstract: Large-scale Mixture-of-Experts (MoE) models rely on \emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\times$ and 2.01$\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\times$ and 1.10-1.19$\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\times$ and 1.06-1.16$\times$.

</details>


### [29] [Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications](https://arxiv.org/abs/2512.22113)
*Shengkun Cui,Rahul Krishna,Saurabh Jha,Ravishankar K. Iyer*

Main category: cs.DC

TL;DR: PRAXIS是一个用于诊断云事故的智能编排器，通过LLM驱动的图遍历方法分析服务依赖和代码依赖，显著提升根因分析准确性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 云事故造成巨大经济损失（平均每小时超过200万美元），而代码和配置问题是云事故的主要根因。现有方法在诊断这类事故时效果有限，需要更有效的自动化诊断工具。

Method: PRAXIS采用LLM驱动的结构化图遍历方法，结合两种依赖图：1）服务依赖图（SDG）捕获微服务级依赖；2）程序依赖图（PDG）捕获代码级依赖。LLM作为遍历策略在这些图上移动，定位和解释故障。

Result: 相比最先进的ReAct基线方法，PRAXIS将根因分析准确性提升最高达3.1倍，同时将token消耗降低3.8倍。在30个真实世界云事故数据集上进行了验证，这些数据正在被编译为RCA基准。

Conclusion: PRAXIS通过智能编排LLM驱动的图遍历，有效诊断代码和配置引起的云事故，显著提升诊断准确性并降低计算成本，为云事故根因分析提供了实用解决方案。

Abstract: Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [30] [Query Carefully: Detecting the Unanswerables in Text-to-SQL Tasks](https://arxiv.org/abs/2512.21345)
*Jasmin Saxer,Isabella Maria Aigner,Luise Linzmeier,Andreas Weiler,Kurt Stockinger*

Main category: cs.DB

TL;DR: 提出Query Carefully管道，整合LLM-based SQL生成与不可回答输入的显式检测处理，在生物医学领域构建OncoMX-NAQ数据集评估不可回答检测能力


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL系统对模糊、超出范围或不可回答的查询仍会生成可执行SQL，存在被误认为正确的风险，这在需要精确性的生物医学领域尤为严重

Method: 使用llama3.3:70b模型，结合schema-aware提示、显式No-Answer Rules和包含可回答与不可回答问题的few-shot示例，构建OncoMX-NAQ数据集（80个不可回答问题，8个类别）

Result: 在OncoMX开发集上，few-shot提示提高结果准确率；在OncoMX-NAQ上，平衡提示达到最高不可回答检测准确率（0.8），结构定义类别表现优异但缺失值查询（0.5）和列模糊（0.3）仍有挑战

Conclusion: Query Carefully管道通过显式检测和处理不可回答查询，支持透明可靠的生物医学Text-to-SQL应用，轻量级用户界面展示中间SQL、执行结果和弃权信息

Abstract: Text-to-SQL systems allow non-SQL experts to interact with relational databases using natural language. However, their tendency to generate executable SQL for ambiguous, out-of-scope, or unanswerable queries introduces a hidden risk, as outputs may be misinterpreted as correct. This risk is especially serious in biomedical contexts, where precision is critical. We therefore present Query Carefully, a pipeline that integrates LLM-based SQL generation with explicit detection and handling of unanswerable inputs. Building on the OncoMX component of ScienceBenchmark, we construct OncoMX-NAQ (No-Answer Questions), a set of 80 no-answer questions spanning 8 categories (non-SQL, out-of-schema/domain, and multiple ambiguity types). Our approach employs llama3.3:70b with schema-aware prompts, explicit No-Answer Rules (NAR), and few-shot examples drawn from both answerable and unanswerable questions. We evaluate SQL exact match, result accuracy, and unanswerable-detection accuracy. On the OncoMX dev split, few-shot prompting with answerable examples increases result accuracy, and adding unanswerable examples does not degrade performance. On OncoMX-NAQ, balanced prompting achieves the highest unanswerable-detection accuracy (0.8), with near-perfect results for structurally defined categories (non-SQL, missing columns, out-of-domain) but persistent challenges for missing-value queries (0.5) and column ambiguity (0.3). A lightweight user interface surfaces interim SQL, execution results, and abstentions, supporting transparent and reliable text-to-SQL in biomedical applications.

</details>
