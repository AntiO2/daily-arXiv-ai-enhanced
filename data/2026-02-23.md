<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [Multi-Attribute Group Fairness in $k$-NN Queries on Vector Databases](https://arxiv.org/abs/2602.17858)
*Thinh On,Senjuti Basu Roy,Baruch Schieber*

Main category: cs.DB

TL;DR: 提出首个多属性群体公平性k近邻搜索框架，解决向量数据库中跨多个受保护属性的比例表示问题，通过LSH加速和轻量级索引实现效率与公平性的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有向量搜索方法主要关注效率或查询过滤，缺乏对多属性群体公平性的考虑。当公平性要求跨多个受保护属性时，需要同时满足比例表示约束，这带来了计算上的挑战。

Method: 提出计算框架：1) 使用局部敏感哈希(LSH)加速候选生成；2) 在受保护属性值的笛卡尔积上构建轻量级索引；3) 检索满足联合计数约束的候选；4) 后处理阶段构建公平的k-NN结果。针对2属性提出基于流的多项式时间精确算法，针对3+属性提出基于整数线性规划(ILP)的精确解。

Result: 理论保证和实验评估表明：1) 现有向量搜索方法无法直接适应公平性要求；2) 提出的框架具有通用性和可扩展性；3) 在搜索时间、内存/索引成本和召回率之间实现了良好权衡。

Conclusion: 这是首个研究向量数据库中多属性群体公平性k-NN搜索的工作，提出了有效的计算框架，解决了效率与公平性的权衡问题，为公平性感知的向量搜索提供了理论基础和实践方案。

Abstract: We initiate the study of multi-attribute group fairness in $k$-nearest neighbor ($k$-NN) search over vector databases. Unlike prior work that optimizes efficiency or query filtering, fairness imposes count constraints to ensure proportional representation across groups defined by protected attributes. When fairness spans multiple attributes, these constraints must be satisfied simultaneously, making the problem computationally hard. To address this, we propose a computational framework that produces high-quality approximate nearest neighbors with good trade-offs between search time, memory/indexing cost, and recall. We adapt locality-sensitive hashing (LSH) to accelerate candidate generation and build a lightweight index over the Cartesian product of protected attribute values. Our framework retrieves candidates satisfying joint count constraints and then applies a post-processing stage to construct fair $k$-NN results across all attributes. For 2 attributes, we present an exact polynomial-time flow-based algorithm; for 3 or more, we formulate ILP-based exact solutions with higher computational cost. We provide theoretical guarantees, identify efficiency--fairness trade-offs, and empirically show that existing vector search methods cannot be directly adapted for fairness. Experimental evaluations demonstrate the generality of the proposed framework and scalability.

</details>


### [2] [From Lossy to Verified: A Provenance-Aware Tiered Memory for Agents](https://arxiv.org/abs/2602.17913)
*Qiming Zhu,Shunian Chen,Rui Yu,Zhehao Wu,Benyou Wang*

Main category: cs.DB

TL;DR: TierMem是一个两层级记忆框架，通过推理时证据分配解决长时程智能体中的摘要压缩与原始日志查询权衡问题，在保持高准确率的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 长时程智能体通常将交互历史压缩为写入时摘要，但这造成了"先写后查"的障碍：压缩决策在不知道未来查询需求的情况下做出，可能导致关键信息被遗漏。虽然保留原始日志能提供权威真相来源，但默认基于原始日志查询成本高昂，许多查询本可从摘要中回答。

Method: 提出TierMem框架，采用两层级记忆架构：默认查询快速摘要索引，当摘要证据不足时，运行时充分性路由器将升级查询到不可变的原始日志存储。系统将验证后的发现作为新的摘要单元写回，并与原始来源链接。

Result: 在LoCoMo基准测试中，TierMem达到0.851准确率（原始日志方法为0.873），同时减少54.1%的输入token和60.7%的延迟。

Conclusion: TierMem通过推理时证据分配和两层级记忆架构，在保持接近原始日志方法准确率的同时，显著降低了计算开销，为长时程智能体提供了高效的记忆管理解决方案。

Abstract: Long-horizon agents often compress interaction histories into write-time summaries. This creates a fundamental write-before-query barrier: compression decisions are made before the system knows what a future query will hinge on. As a result, summaries can cause unverifiable omissions -- decisive constraints (e.g., allergies) may be dropped, leaving the agent unable to justify an answer with traceable evidence. Retaining raw logs restores an authoritative source of truth, but grounding on raw logs by default is expensive: many queries are answerable from summaries, yet raw grounding still requires processing far longer contexts, inflating token consumption and latency.
  We propose TierMem, a provenance-linked framework that casts retrieval as an inference-time evidence allocation problem. TierMem uses a two-tier memory hierarchy to answer with the cheapest sufficient evidence: it queries a fast summary index by default, and a runtime sufficiency router Escalates to an immutable raw-log store only when summary evidence is insufficient. TierMem then writes back verified findings as new summary units linked to their raw sources. On LoCoMo, TierMem achieves 0.851 accuracy (vs.0.873 raw-only) while reducing input tokens by 54.1\% and latency by 60.7%.

</details>


### [3] [Efficient Filtered-ANN via Learning-based Query Planning](https://arxiv.org/abs/2602.17914)
*Zhuocheng Gan,Yifan Wang*

Main category: cs.DB

TL;DR: 提出基于学习的查询规划框架，动态选择最有效的执行计划（预过滤或后过滤），利用轻量级预测提升过滤ANN搜索性能


<details>
  <summary>Details</summary>
Motivation: 过滤ANN搜索面临执行顺序的权衡：预过滤需要昂贵的按谓词索引构建，后过滤可能在低选择性下浪费计算并损失召回率

Method: 基于学习的查询规划框架，利用数据集和查询统计（维度、语料大小、分布特征、谓词统计）进行轻量级预测，动态选择最有效的执行计划，支持多种过滤类型，可与任何后端ANN索引通用

Result: 实验显示该方法相比强基线实现高达4倍加速，同时保持≥90%的召回率

Conclusion: 提出的学习型查询规划框架有效解决了过滤ANN搜索中的执行顺序权衡问题，显著提升性能

Abstract: Filtered ANN search is an increasingly important problem in vector retrieval, yet systems face a difficult trade-off due to the execution order: Pre-filtering (filtering first, then ANN over the passing subset) requires expensive per-predicate index construction, while post-filtering (ANN first, then filtering candidates) may waste computation and lose recall under low selectivity due to insufficient candidates after filtering. We introduce a learning-based query planning framework that dynamically selects the most effective execution plan for each query, using lightweight predictions derived from dataset and query statistics (e.g., dimensionality, corpus size, distribution features, and predicate statistics). The framework supports diverse filter types, including categorical/keyword and range predicates, and is generic to use any backend ANN index. Experiments show that our method achieves up to 4x acceleration with >= 90% recall comparing to the strong baselines.

</details>


### [4] [Seasoning Data Modeling Education with GARLIC: A Participatory Co-Design Framework](https://arxiv.org/abs/2602.18274)
*Viktoriia Makovska,Ihor Michurin,Mariia Tokhtamysh,George Fletcher,Julia Stoyanovich*

Main category: cs.DB

TL;DR: GARLIC是一种教学参与式ER建模的方法论，通过角色扮演、协作综合、指导性批判和迭代改进的研讨会形式，培养技术建模技能和数据表示的社会伦理意识。


<details>
  <summary>Details</summary>
Motivation: 当前数据库教育缺乏教授参与式ER建模的结构化教学方法。传统的ER建模教学过于技术化，忽视了数据系统如何表示人、流程和机构的社会影响。参与式设计研究表明，让多元利益相关者参与建模可以发掘隐性知识、挑战隐含假设，并产生更具包容性的数据表示。

Method: GARLIC方法基于Makowska等人的ONION参与式ER建模框架，将其扩展为研讨会式学习形式。该方法结合了角色扮演、协作综合、指导性批判和迭代改进等元素，旨在降低参与式ER建模的门槛。

Result: GARLIC方法能够培养学生技术建模技能，同时增强他们对数据表示的社会和伦理维度的批判意识。该方法为学生提供了协作、包容性数据模型设计的实践技能。

Conclusion: GARLIC为数据库教育提供了教授参与式ER建模的结构化教学方法，填补了当前教育中的空白，使学生能够在技术建模的同时考虑社会伦理因素，设计更包容的数据系统。

Abstract: Entity-Relationship (ER) modeling is commonly taught as a primarily technical activity, despite its central role in shaping how data systems represent people, processes, and institutions. Prior research in participatory design demonstrates that involving diverse stakeholders in modeling can surface tacit knowledge, challenge implicit assumptions, and produce more inclusive data representations. However, database education currently lacks structured pedagogical approaches for teaching participatory ER modeling in practice.
  We introduce the GARLIC methodology for teaching and learning participatory ER modeling. GARLIC adapts and extends the ONION participatory ER modeling framework of Makovska et al.(HILDA 2025) into a workshop-based learning format that combines role-playing, collaborative synthesis, guided critique, and iterative refinement. GARLIC is designed to develop both technical modeling skills and critical awareness of the social and ethical dimensions of data representation. GARLIC lowers the barrier to participatory ER modeling and equips students with practical skills for collaborative, inclusive data model design.

</details>


### [5] [Dichotomy for Axiomatising Inclusion Dependencies on K-Databases](https://arxiv.org/abs/2602.18390)
*Miika Hannula,Teymur Ismikhanov,Jonni Virtema*

Main category: cs.DB

TL;DR: 本文研究了K-数据库上包含依赖的蕴含问题，建立了基于幺半群K性质的二分法：若K弱可消则标准公理完备；若K弱吸收则需添加弱对称公理；若要求K-关系为分布还需添加平衡公理。


<details>
  <summary>Details</summary>
Motivation: 研究带权数据库（K-数据库）上包含依赖的蕴含问题，其中K是正交换幺半群。旨在建立完备的公理化系统，为带权数据库的约束推理提供理论基础。

Method: 基于幺半群K的代数性质（弱可消性和弱吸收性）建立二分法。分析不同性质下包含依赖蕴含问题的公理完备性，包括标准公理、弱对称公理和平衡公理。

Result: 1. 若K弱可消，标准包含依赖公理对蕴含问题完备；2. 若K弱吸收，需添加弱对称公理才完备；3. 若要求K-关系为分布（联合权重相同），还需添加平衡公理，此时弱对称退化为对称性。

Conclusion: K-数据库上包含依赖的蕴含问题公理化完全由幺半群K的代数性质决定，建立了清晰的二分法，为带权数据库的约束理论提供了完整的理论基础。

Abstract: A relation consisting of tuples annotated by an element of a monoid K is called a K-relation. A K-database is a collection of K-relations. In this paper, we study entailment of inclusion dependencies over K-databases, where K is a positive commutative monoid. We establish a dichotomy regarding the axiomatisation of the entailment of inclusion dependencies over K-databases, based on whether the monoid K is weakly absorptive or weakly cancellative. We establish that, if the monoid is weakly cancellative then the standard axioms of inclusion dependencies are sound and complete for the implication problem. If the monoid is not weakly cancellative, it is weakly absorptive and the standard axioms of inclusion dependencies together with the weak symmetry axiom are sound and complete for the implication problem. In addition, we establish that the so-called balance axiom is further required, if one stipulates that the joint weights of each K-relation of a K-database need to be the same; this generalises the notion of a K-relation being a distribution. In conjunction with the balance axiom, weak symmetry axiom boils down to symmetry.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [Mind the Boundary: Stabilizing Gemini Enterprise A2A via a Cloud Run Hub Across Projects and Accounts](https://arxiv.org/abs/2602.17675)
*Takao Morita*

Main category: cs.DC

TL;DR: 论文实现了一个A2A Hub编排器，用于在跨项目和账户边界的安全可复现环境中协调异构后端代理和工具，解决了Gemini Enterprise UI约束和身份验证问题。


<details>
  <summary>Details</summary>
Motivation: 企业对话UI需要在跨项目和账户边界的安全可复现方式下协调异构后端代理和工具，但实际互操作性受到Gemini Enterprise UI约束和边界相关身份验证的限制。

Method: 在Cloud Run上实现A2A Hub编排器，将查询路由到四个路径：跨项目的公共A2A代理、跨账户的IAM保护Cloud Run A2A代理、结合Discovery Engine和Vertex AI Search的检索增强生成路径，以及通过Vertex AI的通用问答路径。为处理UI约束，在JSON-RPC端点强制执行纯文本兼容模式，同时将结构化输出和调试信号分离到REST工具API。

Result: 在涵盖费用政策、项目管理协助、通用知识和事件响应截止时间提取的四查询基准测试中，确认了确定性路由和稳定的UI响应。对于检索路径，授予存储对象读取权限能够实现基于证据的15分钟截止时间提取。所有实验均可通过存储库快照复现。

Conclusion: 实际的企业代理间互操作性不仅需要协议合规性，还必须考虑UI约束和边界相关身份验证。通过分离纯文本UI响应和结构化数据，可以在保持UI稳定性的同时实现复杂编排功能。

Abstract: Enterprise conversational UIs increasingly need to orchestrate heterogeneous backend agents and tools across project and account boundaries in a secure and reproducible way. Starting from Gemini Enterprise Agent-to-Agent (A2A) invocation, we implement an A2A Hub orchestrator on Cloud Run that routes queries to four paths: a public A2A agent deployed in a different project, an IAM-protected Cloud Run A2A agent in a different account, a retrieval-augmented generation path combining Discovery Engine and Vertex AI Search with direct retrieval of source text from Google Cloud Storage, and a general question answering path via Vertex AI. We show that practical interoperability is governed not only by protocol compliance but also by Gemini Enterprise UI constraints and boundary-dependent authentication. Real UI requests arrive as text-only inputs and include empty accepted output mode lists, so mixing structured data into JSON-RPC responses can trigger UI errors. To address this, we enforce a text-only compatibility mode on the JSON-RPC endpoint while separating structured outputs and debugging signals into a REST tool API. On a four-query benchmark spanning expense policy, project management assistance, general knowledge, and incident response deadline extraction, we confirm deterministic routing and stable UI responses. For the retrieval path, granting storage object read permissions enables evidence-backed extraction of the fifteen minute deadline. All experiments are reproducible using the repository snapshot tagged a2a-hub-gemini-ui-stable-paper.

</details>


### [7] [It's Not Just Timestamps: A Study on Docker Reproducibility](https://arxiv.org/abs/2602.17678)
*Oreofe Solarin*

Main category: cs.DC

TL;DR: Docker容器构建的可重现性研究：对2000个GitHub仓库的分析显示，仅有56%的Dockerfile能成功构建，其中仅2.7%能在不修改基础设施配置的情况下实现比特级可重现。通过调整配置可将可重现性提升18.6%，但仍有78.7%的构建不可重现，主要原因为时间戳、缓存、日志、文档和浮动版本等开发者可控因素。


<details>
  <summary>Details</summary>
Motivation: 研究Docker容器构建的可重现性，验证"从Dockerfile重建镜像并比较哈希值"这一软件供应链完整性检查方法的实际可行性，识别影响可重现性的主要因素。

Method: 构建Docker测量流水线，对2000个包含Dockerfile的GitHub仓库进行分层抽样分析。首先测试原始配置下的可重现性，然后修改基础设施配置重新测试，分析剩余差异的根本原因。

Result: 仅56%的Dockerfile能成功构建镜像，其中仅2.7%能在原始配置下实现比特级可重现。修改基础设施配置后，可重现性提升18.6%，但仍有78.7%的构建不可重现。主要问题包括时间戳、元数据、未清理的缓存、日志、文档和浮动版本等开发者可控因素。

Conclusion: Docker容器构建的可重现性现状不容乐观，大部分构建无法实现比特级重现。研究提出了具体的Dockerfile指导原则，这些发现可为未来开发可重现容器相关的代码检查工具和持续集成检查提供参考。

Abstract: Reproducible container builds promise a simple integrity check for software supply chains: rebuild an image from its Dockerfile and compare hashes. We build a Docker measurement pipeline and apply it to a stratified sample of 2,000 GitHub repositories that contained a Dockerfile. We found that only 56% produce any buildable image, and just 2.7% of those are bitwise reproducible without any infrastructure configurations. After modifying infrastructure configurations, we raise bitwise reproducibility by 18.6%, but 78.7% of buildable Dockerfiles remain non-reproducible. We analyze the root causes of the remaining differences, and find that beyond timestamps and metadata, developer-controlled choices such as uncleaned caches, logs, documentation, and floating versions are dominant causes of non-reproducibility. We derive concrete Dockerfile guidelines from these patterns and discuss how they can inform future linters and Continuous Integration (CI) checks for reproducible containers.

</details>


### [8] [Message-Oriented Middleware Systems: Technology Overview](https://arxiv.org/abs/2602.17774)
*Wael Al-Manasrah,Zuhair AlSader,Tim Brecht,Ahmed Alquraan,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: 对10个开源消息中间件系统进行全面的特征分析，涵盖42个特征共134个选项，发现MOM系统已演变为现代云应用提供高灵活性和可配置性的框架，并创建了可公开访问的标注数据集。


<details>
  <summary>Details</summary>
Motivation: 对开源消息中间件系统进行全面特征分析，帮助开发者和实践者理解不同系统的特性，促进社区在更少的开源项目上集中努力。

Method: 采用严谨方法学选择并研究10个流行且多样化的MOM系统，为每个系统检查42个特征共134个不同选项，创建标注数据集。

Result: 发现MOM系统已演变为现代云应用提供高灵活性和可配置性的框架，提供核心构建块如事务支持、主动消息、资源管理、流量控制和对多租户的原生支持。识别出社区有机会在更少的开源项目上集中努力。

Conclusion: 通过全面的特征分析揭示了MOM系统的发展趋势，创建了可验证研究结果的标注数据集，为开发者和实践者提供比较不同系统特性的工具，并公开数据集以扩大影响力。

Abstract: We present a comprehensive characterization study of open-source message-oriented middleware (MOM) systems. We followed a rigorous methodology to select and study ten popular and diverse MOM systems. For each system, we examine 42 features with a total of 134 different options. We found that MOM systems have evolved to provide a framework for modern cloud applications through high flexibility and configurability and by offering core building blocks for complex applications including transaction support, active messaging, resource management, flow control, and native support for multi-tenancy. We also identify that there is an opportunity for the community to consolidate its efforts on fewer open-source projects.
  We have also created an annotated data set that makes it easy to verify our findings, which can also be used to help practitioners and developers understand and compare the features of different systems. For a wider impact, we make our data set publicly available.

</details>


### [9] [Collaborative Processing for Multi-Tenant Inference on Memory-Constrained Edge TPUs](https://arxiv.org/abs/2602.17808)
*Nathan Ng,Walid A. Hanafy,Prashanthi Kadambi,Balachandra Sunil,Ayush Gupta,David Irwin,Yogesh Simmhan,Prashant Shenoy*

Main category: cs.DC

TL;DR: SwapLess是一个用于内存受限Edge TPU的自适应多租户TPU-CPU协同推理系统，通过分析排队模型动态调整分区点和CPU核心分配，显著降低端到端延迟。


<details>
  <summary>Details</summary>
Motivation: 物联网应用依赖设备端AI加速器，但片上内存有限导致推理时需要频繁在主机和加速器内存间交换模型片段，显著增加延迟。现有协同处理方法要么将过多计算转移到CPU，要么无法有效减少交换，在多租户动态环境中问题更严重。

Method: SwapLess使用分析排队模型，捕捉分区相关的CPU/TPU服务时间以及不同工作负载混合和请求率下的模型间和模型内交换开销。基于此模型，系统在线动态调整分区点和CPU核心分配，以最小化端到端响应时间。

Result: 在配备Edge TPU的平台上的实现表明，相比默认Edge TPU编译器，SwapLess将单租户工作负载的平均延迟降低高达63.8%，多租户工作负载降低高达77.4%。

Conclusion: SwapLess通过自适应协同推理有效解决了内存受限Edge TPU的延迟问题，特别是在多租户动态环境中显著提升了推理性能。

Abstract: IoT applications are increasingly relying on on-device AI accelerators to ensure high performance, especially in limited connectivity and safety-critical scenarios. However, the limited on-chip memory of these accelerators forces inference runtimes to swap model segments between host and accelerator memory, substantially inflating latency. While collaborative processing by partitioning the model processing between CPU and accelerator resources can reduce accelerator memory pressure and latency, naive partitioning may worsen end-to-end latency by either shifting excessive computation to the CPU or failing to sufficiently curb swapping, a problem that is further amplified in multi-tenant and dynamic environments.
  To address these issues, we present SwapLess, a system for adaptive, multi-tenant TPU-CPU collaborative inference for memory-constrained Edge TPUs. SwapLess utilizes an analytic queueing model that captures partition-dependent CPU/TPU service times as well as inter- and intra-model swapping overheads across different workload mixes and request rates. Using this model, SwapLess continuously adjusts both the partition point and CPU core allocation online to minimize end-to-end response time with low decision overhead. An implementation on Edge TPU-equipped platforms demonstrates that SwapLess reduces mean latency by up to 63.8% for single-tenant workloads and up to 77.4% for multi-tenant workloads relative to the default Edge TPU compiler.

</details>


### [10] [Faster Parallel Batch-Dynamic Algorithms for Low Out-Degree Orientation](https://arxiv.org/abs/2602.17811)
*Guy Blelloch,Andrew Brady,Laxman Dhulipala,Jeremy Fineman,Kishen Gowda,Chase Hutton*

Main category: cs.DC

TL;DR: 本文提出了三种并行批量动态算法，用于维护无向图的低出度定向，均实现多对数深度，重点在于最小化工作量。


<details>
  <summary>Details</summary>
Motivation: 在并行批量动态设置中，需要处理边的批量插入或删除，目标是使整个批次的并行处理工作量接近单次顺序更新的每边工作量，同时保持多对数深度。现有算法在时间复杂度上仍有改进空间。

Method: 提出了三种并行批量动态算法：1）第一个实现渐近最优定向和渐近最优期望工作量的算法；2）基于图树宽度的O(c log n)定向算法，每边更新期望最坏情况工作量为O(√log n)；3）O(c + log n)定向算法，每边更新期望最坏情况工作量为O(log² n)。

Result: 1）相比Liu等人的工作改进了对数因子；2）与Berglin和Brodal的最佳顺序最坏情况算法匹配（期望意义）；3）相比Ghaffari和Koo的O(log⁹ n)最坏情况工作量有显著改进。

Conclusion: 本文提出了更高效的并行批量动态低出度定向算法，在保持多对数深度的同时，显著减少了工作量，为图处理提供了更优的并行解决方案。

Abstract: A low out-degree orientation directs each edge of an undirected graph with the goal of minimizing the maximum out-degree of a vertex. In the parallel batch-dynamic setting, one can insert or delete batches of edges, and the goal is to process the entire batch in parallel with work per edge similar to that of a single sequential update and with span (or depth) for the entire batch that is polylogarithmic. In this paper we present faster parallel batch-dynamic algorithms for maintaining a low out-degree orientation of an undirected graph. All results herein achieve polylogarithmic depth, with high probability (whp); the focus of this paper is on minimizing the work, which varies across results.
  Our first result is the first parallel batch-dynamic algorithm to maintain an asymptotically optimal orientation with asymptotically optimal expected work bounds, in an amortized sense, improving over the prior best work bounds of Liu et al.~[SPAA~'22] by a logarithmic factor.
  Our second result is a $O(c \log n)$ orientation algorithm with expected worst-case $O(\sqrt{\log n})$ work per edge update, where $c$ is a known upper-bound on the arboricity of the graph. This matches the best-known sequential worst-case $O(c \log n)$ orientation algorithm given by Berglin and Brodal ~[Algorithmica~'18], albeit in expectation.
  Our final result is a $O(c + \log n)$-orientation algorithm with $O(\log^2 n)$ expected worst-case work per edge update. This algorithm significantly improves upon the recent result of Ghaffari and Koo~[SPAA~'25], which maintains a $O(c)$-orientation with $O(\log^9 n)$ worst-case work per edge whp.

</details>


### [11] [GPU Memory and Utilization Estimation for Training-Aware Resource Management: Opportunities and Limitations](https://arxiv.org/abs/2602.17817)
*Ehsan Yousefzadeh-Asl-Miandoab,Reza Karimzadeh,Danyal Yorulmaz,Bulat Ibragimov,Pınar Tözün*

Main category: cs.DC

TL;DR: 本文系统分析了GPU内存估算的三种范式，评估了Horus、PyTorch FakeTensor和轻量级ML估算器的准确性、泛化性和实际开销，并探讨了GPU利用率估算的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习任务并置可提高GPU利用率，但会导致资源竞争引起的显著减速和内存溢出风险。准确的内存估算对于稳健的并置至关重要，而GPU利用率作为资源竞争的关键代理，可实现干扰感知调度以减少减速并提高吞吐量。

Method: 对三种代表性估算器进行系统分析：Horus（分析模型）、PyTorch FakeTensor（CPU端库）和轻量级ML估算器。构建包含MLP、CNN和Transformer的合成数据集，训练MLP和Transformer基础估算器进行内存预测，并在相同数据集上进行利用率估算实验。

Result: 评估揭示了关键权衡：分析模型依赖硬件，CPU端库带来侵入式集成成本，ML估算器在跨架构泛化方面存在困难。验证了估算器对未见过的真实世界模型的性能。

Conclusion: GPU内存估算面临重大挑战，包括硬件依赖性、集成成本和泛化问题。GPU利用率估算研究相对不足，且存在非加性特性和硬件敏感性。作者发布了所有数据集、工具和工件以支持进一步研究。

Abstract: Collocating deep learning training tasks improves GPU utilization but causes drastic slowdowns due to resource contention and risks Out-of-Memory (OOM) failures. Accurate memory estimation is essential for robust collocation, while GPU utilization -- a key proxy for resource contention -- enables interference-aware scheduling to reduce slowdowns and improve throughput. Existing GPU memory estimators span three paradigms -- analytical models, CPU-side libraries, and ML-based estimators -- each with distinct limitations: dependence on detailed model specifications, intrusive integration, poor generalization, and varying latency overhead. GPU heterogeneity further complicates estimation, as identical tasks can exhibit markedly different memory footprints across hardware generations. GPU utilization remains comparatively understudied, further complicated by the non-additive nature of utilization metrics and hardware sensitivity. We conduct a systematic analysis of representative estimators from each paradigm -- Horus, PyTorch FakeTensor, and our lightweight ML-based estimator -- evaluating accuracy, generalizability, and practical overhead. We construct a synthetic dataset spanning MLPs, CNNs, and Transformers with controlled architectural variations, and train MLP- and Transformer-based estimators for memory prediction. We further experiment with utilization estimation on the same dataset. Our evaluation reveals key tradeoffs and validates estimators against real-world unseen models. Significant challenges remain: analytical models are hardware-dependent, CPU-side libraries impose intrusive integration costs, and ML-based estimators struggle with cross-architecture generalization. We release all datasets, tools, and artifacts to support further research.

</details>


### [12] [Distributed Triangle Enumeration in Hypergraphs](https://arxiv.org/abs/2602.17834)
*Duncan Adamson,Will Rosenbaum,Paul G. Spirakis*

Main category: cs.DC

TL;DR: 该论文首次系统研究分布式超图中的子超图枚举问题，提出了超图计算模型、设计了三角形枚举算法并证明了其最优性，针对稀疏超图开发了高效算法，并提供了通用设计技术。


<details>
  <summary>Details</summary>
Motivation: 过去十年中，子图检测和枚举已成为分布式图算法的核心问题，既有理论挑战又有实际应用。然而，超图中的分布式子超图枚举问题尚未得到系统研究，需要填补这一空白。

Method: 1) 引入多个超图计算模型，推广了图的CONGEST模型并评估其计算能力；2) 在这些模型中设计分布式三角形枚举算法，并在两个模型中证明其最优性；3) 定义稀疏和"处处稀疏"超图类，并描述这些类中三角形枚举的高效分布式算法；4) 提出适用于超图模型的通用算法设计技术。

Result: 建立了超图分布式计算的理论框架，开发了最优的三角形枚举算法，为稀疏超图提供了高效解决方案，并形成了可用于未来算法设计的通用技术工具集。

Conclusion: 该论文开创了分布式超图子超图枚举的系统研究，为这一新兴领域奠定了理论基础，提供了算法设计框架，并展示了在稀疏超图等特殊结构中的高效解决方案。

Abstract: In the last decade, subgraph detection and enumeration have emerged as a central problem in distributed graph algorithms. This is largely due to the theoretical challenges and practical applications of these problems. In this paper, we initiate the systematic study of distributed sub-hypergraph enumeration in hypergraphs. To this end, we (1)~introduce several computational models for hypergraphs that generalize the CONGEST model for graphs and evaluate their relative computational power, (2)~devise algorithms for distributed triangle enumeration in our computational models and prove their optimality in two such models, (3)~introduce classes of sparse and ``everywhere sparse'' hypergraphs and describe efficient distributed algorithms for triangle enumeration in these classes, and (4)~describe general techniques that we believe to be useful for designing efficient algorithms in our hypergraph models.

</details>


### [13] [Joint Training on AMD and NVIDIA GPUs](https://arxiv.org/abs/2602.18007)
*Jon Hu,Thomas Jia,Jing Zhu,Zhendong Yu*

Main category: cs.DC

TL;DR: 提出在AMD-NVIDIA异构环境中进行混合训练的技术方案，包括兼容性导向的CPU转发通信和性能导向的设备直连通信两种方法，后者能达到NVIDIA同构系统98%的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，训练对计算和系统容量的需求快速增长，单一供应商的同构集群已不足够，需要解决AMD-NVIDIA异构环境中的混合训练问题。

Method: 提出两种方法：1) 兼容性导向的CPU转发通信，采用跨并行组的差异化通信后端选择和多NIC并行数据传输；2) 性能导向的设备直连通信，集成CPU卸载的P2P机制，实现无需主机内存中转的直接跨供应商GPU数据传输。

Result: 在LLaMA-8B和Qwen2-7B上的实验表明，提出的设备直连通信方法能达到NVIDIA同构系统98%的吞吐量，同时保持训练稳定性和正确性。

Conclusion: 该技术方案成功解决了AMD-NVIDIA异构环境中的混合训练问题，设备直连通信方法在保持兼容性的同时实现了接近同构系统的性能，为大规模语言模型训练提供了可行的异构解决方案。

Abstract: As large language models continue to scale, training demands on compute and system capacity grow rapidly, making single-vendor homogeneous clusters insufficient. This paper presents a technical solution for heterogeneous mixed training in AMD-NVIDIA environments. We first adopt a compatibility-oriented approach based on CPU-Forwarding Communication, with differentiated communication back-end selection across parallel groups and multi-NIC parallel data transfer. To achieve higher performance, we further propose another Device-Direct Communication approach, integrating a CPU-offloading P2P mechanism to enable direct cross-vendor GPU data transfer without host-memory staging. Experiments on LLaMA-8B and Qwen2-7B demonstrate that the proposed Device-Direct Communication approach achieves up to 98% of the throughput of an NVIDIA homogeneous system, while preserving training stability and correctness.

</details>


### [14] [A reliability- and latency-driven task allocation framework for workflow applications in the edge-hub-cloud continuum](https://arxiv.org/abs/2602.18158)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.DC

TL;DR: 提出一个精确的多目标任务分配框架，用于在边缘-中心-云架构中联合优化工作流应用的可靠性和延迟，通过二进制整数线性规划实现最优解。


<details>
  <summary>Details</summary>
Motivation: 关键工作流应用采用简化的边缘-中心-云架构，但现有方法未能全面解决设备限制和多样化操作条件下的任务分配挑战，特别是可靠性和延迟这两个冲突目标需要精确优化。

Method: 提出精确的多目标任务分配框架，采用二进制整数线性规划（BILP）公式，考虑各目标的相对重要性，整合时间冗余技术，并纳入相关研究中常被忽视的关键约束。

Result: 在真实世界应用中，相比基线策略，平均可靠性提升84.19%，延迟改善49.81%；实验显示方法具有有效性和可扩展性，所有工作流的平均运行时间在0.03到50.94秒之间。

Conclusion: 该框架为简化的边缘-中心-云架构提供了实用的精确任务分配解决方案，能有效平衡关键工作流应用的可靠性和延迟需求，具有实际应用价值。

Abstract: A growing number of critical workflow applications leverage a streamlined edge-hub-cloud architecture, which diverges from the conventional edge computing paradigm. An edge device, in collaboration with a hub device and a cloud server, often suffices for their reliable and efficient execution. However, task allocation in this streamlined architecture is challenging due to device limitations and diverse operating conditions. Given the inherent criticality of such workflow applications, where reliability and latency are vital yet conflicting objectives, an exact task allocation approach is typically required to ensure optimal solutions. As no existing method holistically addresses these issues, we propose an exact multi-objective task allocation framework to jointly optimize the overall reliability and latency of a workflow application in the specific edge-hub-cloud architecture. We present a comprehensive binary integer linear programming formulation that considers the relative importance of each objective. It incorporates time redundancy techniques, while accounting for crucial constraints often overlooked in related studies. We evaluate our approach using a relevant real-world workflow application, as well as synthetic workflows varying in structure, size, and criticality. In the real-world application, our method achieved average improvements of 84.19% in reliability and 49.81% in latency over baseline strategies, across relevant objective trade-offs. Overall, the experimental results demonstrate the effectiveness and scalability of our approach across diverse workflow applications for the considered system architecture, highlighting its practicality with runtimes averaging between 0.03 and 50.94 seconds across all examined workflows.

</details>


### [15] [It does not matter how you define locally checkable labelings](https://arxiv.org/abs/2602.18188)
*Antonio Cruciani,Avinandan Das,Alesya Raevskaya,Jukka Suomela*

Main category: cs.DC

TL;DR: LCL问题在分布式图算法中具有鲁棒性，即使限制为更简单的"节点-边可检查"形式，也能通过局部约简与标准LCL相互转换，仅需对称性打破预言机，在LOCAL模型中最多增加O(log* n)轮开销。


<details>
  <summary>Details</summary>
Motivation: 研究LCL问题的鲁棒性，探索在更严格限制下（如节点-边可检查形式、正则无标签图）的LCL问题是否仍能保持与标准LCL问题的等价性，以验证LCL框架的稳定性。

Method: 采用局部约简方法，在两种形式化之间建立双向转换：从标准LCL到受限的节点-边可检查形式，以及反向转换。这些约简仅需访问对称性打破预言机，在LOCAL模型中实现。

Result: 证明两种形式化之间存在局部约简关系，转换开销最多为O(log* n)轮，表明LCL问题家族对形式化变化具有极强的鲁棒性。

Conclusion: LCL问题框架非常稳定，即使限制到更简单的节点-边可检查形式，仍能保持与标准LCL的等价性，这进一步巩固了LCL作为分布式图算法理论基础的地位。

Abstract: Locally checkable labeling problems (LCLs) form the foundation of the modern theory of distributed graph algorithms. First introduced in the seminal paper by Naor and Stockmeyer [STOC 1993], these are graph problems that can be described by listing a finite set of valid local neighborhoods. This seemingly simple definition strikes a careful balance between two objectives: they are a family of problems that is broad enough so that it captures numerous problems that are of interest to researchers working in this field, yet restrictive enough so that it is possible to prove strong theorems that hold for all LCL problems. In particular, the distributed complexity landscape of LCL problems is now very well understood.
  In this work we show that the family of LCL problems is extremely robust to variations. We present a very restricted family of locally checkable problems (essentially, the "node-edge checkable" formalism familiar from round elimination, restricted to regular unlabeled graphs); most importantly, such problems cannot directly refer to e.g. the existence of short cycles. We show that one can translate between the two formalisms (there are local reductions in both directions that only need access to a symmetry-breaking oracle, and hence the overhead is at most an additive $O(\log^* n)$ rounds in the LOCAL model).

</details>


### [16] [Green by Design: Constraint-Based Adaptive Deployment in the Cloud Continuum](https://arxiv.org/abs/2602.18287)
*Andrea D'Iapico,Monica Vitali*

Main category: cs.DC

TL;DR: 提出一种基于绿色约束的云边应用自动部署规划方法，通过持续分析能耗模式、组件通信和基础设施环境特征，生成环保的部署策略。


<details>
  <summary>Details</summary>
Motivation: 信息技术环境可持续性成为关键问题，云边连续体应用需要能效部署策略，考虑计算需求和节点环境影响，但动态因素（应用行为波动、节点碳强度变化）使规划复杂。

Method: 提出自动生成绿色约束指导的部署规划方法，通过持续分析能耗模式、组件间通信和基础设施环境特征，构建绿色感知约束集，并利用监控数据自动学习和更新约束。

Result: 在云原生应用实际部署场景中验证，证明该方法能有效减少能源消耗和相关排放。

Conclusion: 该方法实现了自适应、能源感知的编排，能够生成环保的部署计划，为云边连续体应用的环境可持续性提供了有效解决方案。

Abstract: The environmental sustainability of Information Technology (IT) has emerged as a critical concern, driven by the need to reduce both energy consumption and greenhouse gas (GHG) emissions. In the context of cloud-native applications deployed across the cloud-edge continuum, this challenge translates into identifying energy-efficient deployment strategies that consider not only the computational demands of application components but also the environmental impact of the nodes on which they are executed. Generating deployment plans that account for these dynamic factors is non-trivial, due to fluctuations in application behaviour and variations in the carbon intensity of infrastructure nodes. In this paper, we present an approach for the automatic generation of deployment plans guided by green constraints. These constraints are derived from a continuous analysis of energy consumption patterns, inter-component communication, and the environmental characteristics of the underlying infrastructure. This paper introduces a methodology and architecture for the generation of a set of green-aware constraints that inform the scheduler to produce environmentally friendly deployment plans. We demonstrate how these constraints can be automatically learned and updated over time using monitoring data, enabling adaptive, energy-aware orchestration. The proposed approach is validated through realistic deployment scenarios of a cloud-native application, showcasing its effectiveness in reducing energy usage and associated emissions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Five Fatal Assumptions: Why T-Shirt Sizing Systematically Fails for AI Projects](https://arxiv.org/abs/2602.17734)
*Raja Soundaramourty,Ozkan Kilic,Ramu Chenchaiah*

Main category: cs.SE

TL;DR: T-shirt sizing等敏捷估算方法在传统软件开发中有效，但在AI项目（特别是LLM和多智能体系统）中会系统性地误导，因为AI开发打破了传统估算的五个基本假设。


<details>
  <summary>Details</summary>
Motivation: AI项目（特别是LLM和多智能体系统）使用传统的T-shirt sizing等敏捷估算方法时，结果会系统性地误导，因为AI开发的特点与传统软件开发不同，需要更合适的估算方法。

Method: 分析T-shirt sizing的五个基本假设在AI开发中的失效情况，基于多智能体系统失败、扩展原则和多轮对话固有不可靠性的研究，提出Checkpoint Sizing方法——一种更以人为本、迭代的方法，使用明确的决策门来重新评估范围和可行性。

Result: AI开发打破了传统估算的五个假设：(1)线性努力扩展，(2)先前经验的可重复性，(3)努力-持续时间可互换性，(4)任务可分解性，(5)确定性完成标准。这表现为非线性性能跳跃、复杂交互表面和"紧耦合"现象。

Conclusion: 提出Checkpoint Sizing作为解决方案，这是一种迭代方法，在开发过程中基于实际学习而非初始假设重新评估范围和可行性，适用于负责AI项目规划和交付的工程经理、技术负责人和产品负责人。

Abstract: Agile estimation techniques, particularly T-shirt sizing, are widely used in software development for their simplicity and utility in scoping work. However, when we apply these methods to artificial intelligence initiatives -- especially those involving large language models (LLMs) and multi-agent systems -- the results can be systematically misleading. This paper shares an evidence-backed analysis of five foundational assumptions we often make during T-shirt sizing. While these assumptions usually hold true for traditional software, they tend to fail in AI contexts: (1) linear effort scaling, (2) repeatability from prior experience, (3) effort-duration fungibility, (4) task decomposability, and (5) deterministic completion criteria. Drawing on recent research into multi-agent system failures, scaling principles, and the inherent unreliability of multi-turn conversations, we show how AI development breaks these rules. We see this through non-linear performance jumps, complex interaction surfaces, and "tight coupling" where a small change in data cascades through the entire stack. To help teams navigate this, we propose Checkpoint Sizing: a more human-centric, iterative approach that uses explicit decision gates where scope and feasibility are reassessed based on what we learn during development, rather than what we assumed at the start. This paper is intended for engineering managers, technical leads, and product owners responsible for planning and delivering AI initiatives.

</details>


### [18] [Examining LLMs Ability to Summarize Code Through Mutation-Analysis](https://arxiv.org/abs/2602.17838)
*Lara Khatib,Micheal Pu,Bogdan Vasilescu,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 该论文提出了一种基于变异测试的方法来评估LLM生成的代码摘要是否准确反映程序实际行为，而非表面意图。通过三个实验共624次评估发现，随着代码复杂度增加，摘要准确性显著下降，且LLM倾向于描述算法意图而非实际行为。


<details>
  <summary>Details</summary>
Motivation: 随着开发者越来越依赖LLM生成的代码摘要进行文档、测试和审查，需要研究这些摘要是否准确反映程序的实际行为。LLM经常自信地描述代码看起来应该做什么（意图），而忽略定义实际行为的细微边界情况或逻辑变化。

Method: 提出基于变异的评估方法：生成代码摘要，在代码中注入针对性变异，检查LLM是否更新其摘要以反映新行为。通过三个实验验证：1）12个受控合成程序的324个变异；2）50个人工编写程序的150个变异样本；3）比较GPT-4和GPT-5.2的性能差异。

Result: 摘要准确性随复杂度急剧下降：单函数76.5% vs 多线程系统17.3%。在人工编写程序中，摘要准确率为49.3%。GPT-5.2相比GPT-4有显著提升（49.3%到85.3%），但两者仍难以区分实现细节与标准算法模式。变异类型和位置影响较弱。

Conclusion: 该工作确立了变异分析作为评估LLM生成摘要是否反映程序行为而非表面文本模式的系统方法。即使最新模型在识别变异为"bug"方面有所改进，但仍需提升对实现细节与算法模式的区分能力。

Abstract: As developers increasingly rely on LLM-generated code summaries for documentation, testing, and review, it is important to study whether these summaries accurately reflect what the program actually does. LLMs often produce confident descriptions of what the code looks like it should do (intent), while missing subtle edge cases or logic changes that define what it actually does (behavior). We present a mutation-based evaluation methodology that directly tests whether a summary truly matches the code's logic. Our approach generates a summary, injects a targeted mutation into the code, and checks if the LLM updates its summary to reflect the new behavior. We validate it through three experiments totalling 624 mutation-summary evaluations across 62 programs. First, on 12 controlled synthetic programs with 324 mutations varying in type (statement, value, decision) and location (beginning, middle, end). We find that summary accuracy decreases sharply with complexity from 76.5% for single functions to 17.3% for multi-threaded systems, while mutation type and location exhibit weaker effects. Second, testing 150 mutated samples on 50 human-written programs from the Less Basic Python Problems (LBPP) dataset confirms the same failure patterns persist as models often describe algorithmic intent rather than actual mutated behavior with a summary accuracy rate of 49.3%. Furthermore, while a comparison between GPT-4 and GPT-5.2 shows a substantial performance leap (from 49.3% to 85.3%) and an improved ability to identify mutations as "bugs", both models continue to struggle with distinguishing implementation details from standard algorithmic patterns. This work establishes mutation analysis as a systematic approach for assessing whether LLM-generated summaries reflect program behavior rather than superficial textual patterns.

</details>


### [19] [Automated LLM-Based Accessibility Remediation: From Conventional Websites to Angular Single-Page Applications](https://arxiv.org/abs/2602.17887)
*Carla Fernández-Navarro,Francisco Chicano*

Main category: cs.SE

TL;DR: 使用大型语言模型自动修复网页可访问性问题，支持静态网站和Angular单页应用，能修复80-86%的问题并生成图像描述。


<details>
  <summary>Details</summary>
Motivation: 网页可访问性问题普遍存在，现有工具只能检测但修复仍需人工，过程缓慢且易出错。特别是单页应用(SPA)的动态特性使传统静态分析方法失效，需要自动化解决方案。

Method: 提出基于大型语言模型(LLM)的模块化工作流，可应用于静态网站和Angular项目。系统直接在静态网页的DOM或SPA的源代码中实施修正，同时生成有意义的图像描述并保持应用设计和稳定性。

Result: 在12个静态网站和6个开源Angular项目上测试，修复了80%的公共网站可访问性问题和86%的Angular应用问题。系统能有效生成图像描述，同时保持应用稳定。

Conclusion: 该工作有助于将可访问性从技术债务转变为日常开发流程的自然组成部分，为静态网站和复杂SPA提供了实用的自动化修复方案。

Abstract: Web accessibility remains an unresolved issue for a large part of the web content. There are many tools to detect errors automatically, but fixing those issues is still mostly a manual, slow, and costly process in which it is easy for developers to overlook specific details. The situation becomes even more complex with modern Single-Page Applications (SPAs), whose dynamic nature makes traditional static analysis approaches inadequate. This work proposes a system that aims to address this challenge by using Large Language Models (LLMs) to automate accessibility fixes. The proposal presents a modular workflow applicable to both static websites and complex Angular projects. The framework actively implements corrections within the DOM of static web pages or the source code of SPAs. The system was tested on 12 static websites and 6 open-source Angular projects, fixing 80% of the accessibility issues on public websites and 86% of the issues on Angular applications. Our proposal also generates meaningful visual descriptions for images while preserving the application's design and stability. This work contributes to ensuring that accessibility stops being a technical debt deferred to the future and becomes a natural part of everyday development workflows.

</details>


### [20] [Mining Type Constructs Using Patterns in AI-Generated Code](https://arxiv.org/abs/2602.17955)
*Imgyeong Lee,Tayyib Ul Hassan,Abram Hindle*

Main category: cs.SE

TL;DR: AI代理在TypeScript项目中比人类更频繁使用'any'关键字和忽略类型检查的高级类型构造，但AI提交的PR接受率却是人类的1.8倍


<details>
  <summary>Details</summary>
Motivation: 尽管AI提升了软件开发效率，但尚不清楚AI在类型相关编程任务（如正确使用类型构造确保类型安全）上是否优于人类，也没有系统研究评估AI代理在复杂类型系统中是否像人类一样过度使用或误用类型构造

Method: 在TypeScript项目领域进行首次实证分析，比较AI代理和人类在类型构造使用上的差异

Result: AI代理使用'any'关键字的倾向是人类的9倍；AI代理更频繁使用忽略类型检查的高级类型构造；令人惊讶的是，尽管存在这些问题，AI代理提交的PR接受率比人类高1.8倍

Conclusion: 软件开发者在与AI代理协作开发过程中应仔细确认代码库的类型安全性

Abstract: Artificial Intelligence (AI) increasingly automates various parts of the software development tasks. Although AI has enhanced the productivity of development tasks, it remains unstudied whether AI essentially outperforms humans in type-related programming tasks, such as employing type constructs properly for type safety, during its tasks. Moreover, there is no systematic study that evaluates whether AI agents overuse or misuse the type constructs under the complicated type systems to the same extent as humans. In this study, we present the first empirical analysis to answer these questions in the domain of TypeScript projects. Our findings show that, in contrast to humans, AI agents are 9x more prone to use the 'any' keyword. In addition, we observed that AI agents use advanced type constructs, including those that ignore type checks, more often compared to humans. Surprisingly, even with all these issues, Agentic pull requests (PRs) have 1.8x higher acceptance rates compared to humans for TypeScript. We encourage software developers to carefully confirm the type safety of their codebases whenever they coordinate with AI agents in the development process.

</details>


### [21] [DeCEAT: Decoding Carbon Emissions for AI-driven Software Testing](https://arxiv.org/abs/2602.18012)
*Pragati Kumari,Novarun Deb*

Main category: cs.SE

TL;DR: 本文提出DeCEAT框架，专门评估小型语言模型在测试生成中的环境可持续性和性能权衡，发现不同SLM在能耗、速度和准确性方面各有优势，且提示设计对结果有重要影响。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在自动化软件测试中的应用日益增加，但现有的可持续性分析主要关注大型语言模型，小型语言模型在测试生成过程中的能源消耗和碳排放特性尚未得到充分研究。

Method: 提出DeCEAT框架，使用HumanEval基准和基于Anthropic模板的自适应提示变体，在受控条件下系统评估SLM的环境和性能权衡。使用CodeCarbon测量能耗和碳排放，使用单元测试覆盖率评估生成测试的质量。

Result: 不同SLM展现出不同的可持续性优势：一些模型优先考虑低能耗和快速执行，而另一些在碳排放约束下保持更高的稳定性或准确性。结果表明SLM驱动的测试生成可持续性是多维度的，且受提示设计强烈影响。

Conclusion: 本文提供了一个专门针对自动化SLM测试生成的可持续性评估框架，阐明了提示结构和模型选择如何共同影响环境和性能结果，填补了小型语言模型在测试生成可持续性研究方面的空白。

Abstract: The increasing use of language models in automated software testing raises concerns about their environmental impact, yet existing sustainability analyses focus almost exclusively on large language models. As a result, the energy and carbon characteristics of small language models (SLMs) during test generation remain largely unexplored. To address this gap, this work introduces the DeCEAT framework, which systematically evaluates the environmental and performance trade-offs of SLMs using the HumanEval benchmark and adaptive prompt variants (based on the Anthropic template). The framework quantifies emission and time-aware behavior under controlled conditions, with CodeCarbon measuring energy consumption and carbon emissions, and unit test coverage assessing the quality of generated tests. Our results show that different SLMs exhibit distinct sustainability strengths: some prioritize lower energy use and faster execution, while others maintain higher stability or accuracy under carbon constraints. These findings demonstrate that sustainability in the generation of SLM-driven tests is multidimensional and strongly shaped by prompt design. This work provides a focused sustainability evaluation framework specifically tailored to automated SLM-based test generation, clarifying how prompt structure and model choice jointly influence environmental and performance outcomes.

</details>


### [22] [Toward Automated Virtual Electronic Control Unit (ECU) Twins for Shift-Left Automotive Software Testing](https://arxiv.org/abs/2602.18142)
*Sebastian Dingler,Frederik Boenke*

Main category: cs.SE

TL;DR: 该研究开发了一个虚拟ECU测试环境原型，通过指令级精确的处理器模型和自动化建模流程，能够在硬件可用前运行真实软件二进制文件，实现测试左移。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发速度超过硬件可用性，导致后期集成和昂贵的硬件在环测试瓶颈。需要能够在物理硬件可用前运行真实软件二进制的虚拟测试环境。

Method: 采用基于SystemC/TLM 2.0的指令级精确处理器模型，通过代理式反馈驱动工作流程，结合GNU调试器与参考模拟器连接，实现自动化差异测试和迭代模型校正。

Result: 原型系统表明，最关键的技术风险——CPU行为保真度——可以通过自动化差异测试和迭代模型校正来降低。实现了虚拟ECU孪生体的可行路径，支持可重复测试、非侵入式追踪和符合安全标准的故障注入。

Conclusion: 虽然云规模部署和完整工具链集成仍需未来工作，但原型展示了虚拟ECU孪生体的可行左移路径，能够在硬件可用前进行早期软件测试和集成，减少硬件在环测试瓶颈。

Abstract: Automotive software increasingly outpaces hardware availability, forcing late integration and expensive hardware-in-the-loop (HiL) bottlenecks. The InnoRegioChallenge project investigated whether a virtual test and integration environment can reproduce electronic control unit (ECU) behavior early enough to run real software binaries before physical hardware exists. We report a prototype that generates instruction-accurate processor models in SystemC/TLM~2.0 using an agentic, feedback-driven workflow coupled to a reference simulator via the GNU Debugger (GDB). The results indicate that the most critical technical risk -- CPU behavioral fidelity -- can be reduced through automated differential testing and iterative model correction. We summarize the architecture, the agentic modeling loop, and project outcomes, and we extrapolate plausible technical details consistent with the reported qualitative findings. While cloud-scale deployment and full toolchain integration remain future work, the prototype demonstrates a viable shift-left path for virtual ECU twins, enabling reproducible tests, non-intrusive tracing, and fault-injection campaigns aligned with safety standards.

</details>


### [23] [Role and Identity Work of Software Engineering Professionals in the Generative AI Era](https://arxiv.org/abs/2602.18190)
*Jorge Melegati*

Main category: cs.SE

TL;DR: 本文探讨生成式AI对软件工程师身份认同的影响，强调不同角色（开发者与测试者）在身份工作上的差异，并提出研究议程以支持GenAI在软件工程中的采用。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明生成式AI的采用会触发软件专业人员的身份工作（身份形成、适应和拒绝），但这些研究未考虑不同角色（如开发者和测试者）之间的差异。本文认为角色是定义软件专业人员身份工作的关键因素，需要深入研究。

Method: 通过回顾关于不同角色的现有研究以及近期关于如何在软件工程中采用生成式AI的研究，提出研究议程。该议程旨在更好地理解角色如何影响由生成式AI采用触发的软件专业人员身份工作，并基于此提出支持这种采用的新工具。

Result: 本文未报告具体实证结果，而是提出了一个研究议程。该议程包括：1）理解角色如何影响身份工作；2）基于此理解开发支持生成式AI采用的新工具；3）讨论潜在实践意义。

Conclusion: 角色是理解生成式AI采用如何影响软件专业人员身份工作的关键因素。需要进一步研究来探索不同角色在身份工作上的差异，并开发相应的支持工具，以促进生成式AI在软件工程中的有效采用。

Abstract: The adoption of Generative AI (GenAI) suggests major changes for software engineering, including technical aspects but also human aspects of the professionals involved. One of these aspects is how individuals perceive themselves regarding their work, i.e., their work identity, and the processes they perform to form, adapt and reject these identities, i.e., identity work. Existent studies provide evidence of such identity work of software professionals triggered by the adoption of GenAI, however they do not consider differences among diverse roles, such as developers and testers. In this paper, we argue the need for considering the role as a factor defining the identity work of software professionals. To support our claim, we review some studies regarding different roles and also recent studies on how to adopt GenAI in software engineering. Then, we propose a research agenda to better understand how the role influences identity work of software professionals triggered by the adoption of GenAI, and, based on that, to propose new artifacts to support this adoption. We also discuss the potential implications for practice of the results to be obtained.

</details>


### [24] [ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation](https://arxiv.org/abs/2602.18306)
*Dongming Jin,Zhi Jin,Zheng Fang,Linyu Li,XiaoTian Yang,Yuanpeng He,Xiaohong Chen*

Main category: cs.SE

TL;DR: 提出ReqElicitGym评估框架，用于系统评估LLM在对话式需求获取中的访谈能力，发现当前LLM在挖掘隐性需求方面仍有局限。


<details>
  <summary>Details</summary>
Motivation: 随着LLM编码能力提升，软件开发的瓶颈转向需求获取。现有评估方法依赖少量场景、真实用户交互和主观评分，缺乏系统化、可量化的比较框架。

Method: 提出ReqElicitGym评估环境，包含101个网站需求获取场景的数据集，设计了交互式模拟用户和任务评估器，支持自动化、可复现的评估。

Result: 对7个代表性LLM的系统评估显示：当前LLM仅能获取不到一半的隐性需求，有效提问多出现在对话后期；能获取交互和内容需求，但难以处理风格相关需求。

Conclusion: ReqElicitGym为自动化对话式需求获取提供了可复现的量化评估框架，有助于推动该领域的发展，当前LLM在访谈能力上仍有提升空间。

Abstract: With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.

</details>


### [25] [VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean](https://arxiv.org/abs/2602.18307)
*Yutong Xin,Qiaochu Chen,Greg Durrett,Işil Dillig*

Main category: cs.SE

TL;DR: VeriSoftBench：一个包含500个Lean 4证明义务的基准测试，专注于软件验证而非数学证明，评估LLM在真实代码库环境中的定理证明能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在定理证明方面的基准测试主要基于Mathlib数学库，但软件验证中的证明通常依赖于项目特定的定义丰富的代码库，需要评估LLM在这种真实环境中的表现。

Method: 创建VeriSoftBench基准测试，包含500个来自开源形式化方法项目的Lean 4证明义务，保持真实的代码库上下文和跨文件依赖关系，评估前沿LLM和专业证明器的表现。

Result: 1) 针对Mathlib数学证明调优的证明器在此代码库中心化环境中表现不佳；2) 成功率与传递性代码库依赖强相关，依赖链越长越难解决；3) 提供经过筛选的依赖闭包上下文比暴露完整代码库表现更好，但仍需改进。

Conclusion: 软件验证中的定理证明需要专门针对代码库环境的评估基准，现有基于数学证明的LLM方法不能很好地迁移到软件验证场景，需要开发新的方法来处理复杂的项目特定依赖关系。

Abstract: Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.

</details>


### [26] [Statistical Confidence in Functional Correctness: An Approach for AI Product Functional Correctness Evaluation](https://arxiv.org/abs/2602.18357)
*Wallace Albertini,Marina Condé Araújo,Júlia Condé Araújo,Antonio Pedro Santos Alves,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 提出SCFC方法，通过统计置信度评估AI系统功能正确性，连接业务需求与统计指标，包含四个步骤：定义规格限、分层概率抽样、自助法估计置信区间、计算能力指数。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统质量评估面临挑战，ISO/IEC 25059等标准缺乏实用且统计稳健的功能正确性评估方法。需要将业务需求与统计置信度测量连接起来。

Method: 提出统计功能正确性置信度（SCFC）方法，包含四个步骤：1) 定义定量规格限；2) 进行分层和概率抽样；3) 应用自助法估计性能指标的置信区间；4) 计算能力指数作为最终指标。

Result: 通过两个工业界真实AI系统的案例研究进行评估，采访AI专家收集了关于方法实用性、易用性和实际采用意向的宝贵见解。方法被证明可行且有价值。

Conclusion: SCFC方法是操作化功能正确性评估的可行且有价值的方式，将评估从点估计转变为统计置信度陈述，填补了现有标准的实践空白。

Abstract: The quality assessment of Artificial Intelligence (AI) systems is a fundamental challenge due to their inherently probabilistic nature. Standards such as ISO/IEC 25059 provide a quality model, but they lack practical and statistically robust methods for assessing functional correctness. This paper proposes and evaluates the Statistical Confidence in Functional Correctness (SCFC) approach, which seeks to fill this gap by connecting business requirements to a measure of statistical confidence that considers both the model's average performance and its variability. The approach consists of four steps: defining quantitative specification limits, performing stratified and probabilistic sampling, applying bootstrapping to estimate a confidence interval for the performance metric, and calculating a capability index as a final indicator. The approach was evaluated through a case study on two real-world AI systems in industry involving interviews with AI experts. Valuable insights were collected from the experts regarding the utility, ease of use, and intention to adopt the methodology in practical scenarios. We conclude that the proposed approach is a feasible and valuable way to operationalize the assessment of functional correctness, moving the evaluation from a point estimate to a statement of statistical confidence.

</details>
